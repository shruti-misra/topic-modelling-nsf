"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1763747","SHF: Medium: Training Sparse Neural Networks with Co-Designed Hardware Accelerators: Enabling Model Optimization and Scientific Exploration","CCF","Special Projects - CCF, Software & Hardware Foundation","07/01/2018","06/22/2020","Keith Chugg","CA","University of Southern California","Continuing Grant","Almadena Chtchelkanova","06/30/2021","$1,199,849.00","Peter Beerel, Leana Golubchik, Panayiotis Georgiou","chugg@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","2878, 7798","075Z, 7924, 7942","$0.00","Machine learning systems are critical drivers of new technologies such as near-perfect automatic speech recognition, autonomous vehicles, computer vision, and natural language understanding.  The underlying inference engine for many of these systems is based on neural networks.  Before a neural network can be used for these inference tasks, it must be trained using a data corpus of known input-output pairs.  This training process is very computationally intensive with current systems requiring weeks to months of time on graphic processing units (GPUs) or central processing units in the cloud.  As more data becomes available, this problem of long training time is further exacerbated because larger, more effective network models become desirable.  The theoretical understanding of neural networks is limited, so experimentation and empirical optimization remains the primary tool for understanding deep neural networks and innovating in the field.   However, the ability to conduct larger scale experiments is becoming concentrated with a few large entities with the necessary financial and computational resources.  Even for those with such resources, the painfully long experimental cycle for training neural networks means that large-scale searches and optimizations over the neural network model structure are not performed.  The ultimate goal of this research project is to democratize and distribute the ability to conduct large scale neural network training and model optimizations at high speed, using hardware accelerators.  Reducing the training time from weeks to hours will allow researchers to run many more experiments, gaining knowledge into the fundamental inner workings of deep learning systems.  The hardware accelerators are also much more energy efficient than the existing GPU-based training paradigm, so advances made in this project can significantly reduce the energy consumption required for neural network training tasks.<br/><br/>This project comprises an interdisciplinary research plan that spans theory, hardware architecture and design, software control, and system integration.  A new class of neural networks that have pre-defined sparsity is being explored.  These sparse neural networks are co-designed with a very flexible, high-speed, energy-efficient hardware architecture that maximizes circuit speed for any model size in a given Field Programmable Gate Array (FPGA) chip.  This algorithm-hardware co-design is a key research theme that differentiates this approach from previous research that enforces some sparsity during the training process in a manner incompatible with parallel hardware acceleration. In particular, the proposed architecture operates on each network layer simultaneously, executing the forward- and back-propagation in parallel and pipelined fully across layers.  With high precision arithmetic, a speed-up of about 5X relative to GPUs is expected.  Using log-domain arithmetic, these gains are expected to increase to 100X or larger. Software and algorithms are being developed to manage multiple FPGA boards, simplifying and automating the model search and training process. These algorithms exploit the ability to reconfigure the FPGAs to trade speed for accuracy, a capability lacking in GPUs.  These software tools will also serve as a bridge to popular Python libraries used by the machine learning community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816887","CSR: Small: Deconstructing Distributed Deep Learning","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2018","08/04/2018","Leana Golubchik","CA","University of Southern California","Standard Grant","Erik Brunvand","09/30/2021","$516,000.00","","leana@cs.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1714, 7354","075Z, 7923","$0.00","Deep learning has made substantial strides in computer vision, speech recognition, natural language processing, and other applications. New algorithms, larger datasets, increased compute power, and machine learning frameworks all contribute to this success. An important missing piece is that it is still challenging for users to effectively provision and integrate deep learning applications into existing datacenters. This project develops novel solutions that enable effective use of cloud resources, which in turn will aid in broadening the population of users capable of discovering new and better deep learning models and applying them in novel settings and applications.<br/><br/>When developing new applications, users experiment with many deep neural networks (DNNs), but have limited knowledge of their computational demand. Due to non-linear scaling, predicting throughput improvements is challenging. Techniques developed in thrust 1 of this project quickly guide provisioning and resource allocation. In such environments, efficient inter-job resource sharing (particularly for similar DNNs) is an open problem, addressed in thrust 2 of the project by developing effective scheduling techniques. The diversity of datacenter workloads (DNNs, web), with different resource ""affinity"", creates opportunities to embrace cloud federations. While promising, there is a lack of techniques to support their sustainable deployment; these are developed in thrust 3 of the project.<br/><br/>This project is committed to diversity in research and education, involving undergraduate and graduate students, coupled with an existing extensive K-12 outreach effort. The developed experimental testbed is utilized for both, research and education. All algorithms, designs, software, and data are made publicly available so that researchers and educators are able to replicate and improve on developed technologies. Solutions to the fundamental problems that are the focus of this project enable the development of new deep learning models and increase the adoption rate of these technologies in novel application domains.<br/><br/>All reports and code are stored in an SVN-based repository. Software and related documents are publicly available on GitHub. All data is kept for at least 7 years beyond the life of the project. Research products are available promptly after publication, including supplemental information, through http://qed.usc.edu. These records are durable, accessible through standard web protocols, and made secure. Appropriate storage media is used, to keep data access current, as needed. Data that supports patents resulting from the project is retained for the duration of the patents. The URL to the repository is http://qed.usc.edu/D3/repository.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816717","CSR: SMALL: Low-Latency Model Inference Using Cellular Batching","CNS","CSR-Computer Systems Research","09/01/2018","06/13/2018","Jinyang Li","NY","New York University","Standard Grant","Erik Brunvand","08/31/2021","$411,325.00","","jinyang@cs.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7354","7923","$0.00","Successful cloud deployment of machine learning services, such as language translation, image search and home assistants require a high performance serving system that can process hundreds of thousands requests per  second.  It is particularly crucial for the serving system to ensure low latency, as even tens of milliseconds increase in delays can annoy users when using a service like the home assistant.  Among the widely-used deep learning models, recurrent neural network (RNN) is an important class of models that incur high latency  when processed by existing serving systems.  This project aims to develop a new serving system that can handle a variety of Artificial Intelligence (AI) tasks using RNN-based deep learning models with significantly improved latency.<br/><br/>To achieve good throughput on modern hardware, one must perform batched computation.  This project develops a new, dynamic approach to batching, called Cellular Batching.  Cellular Batching performs batching and execution at the granularity of a ""cell"" (aka a subgraph with embedded model weights) instead of the entire dataflow graph, as is done in existing systems.  Under Cellular Batching, a new request can immediately join the execution of ongoing requests to minimize queuing delays and increase effective batching.  The project will complete research tasks that make Cellular Batching practical (by developing an efficient scheduler and supporting zero-downtime model upgrading) and generalize it to different models such as search-guided RNNs.<br/><br/>Deep learning models based on RNNs are becoming widely used to accomplish various AI tasks ranging from speech recognition and language translation, to question answering.  As such, there is a pressing demand for a high-throughput and low-latency serving system, in order to improve end-user experience and reduce the cost of deployment.  By demonstrating significant latency and throughput benefits, there is high potential for Cellular Batching to be widely adopted.  This project will also develop a new course component on high performance machine learning systems as part of the graduate-level distributed systems course.<br/><br/>This project will produce data in the form of source code, various serving benchmarks, and experimental results.  The source code and all benchmarks used in the experiments will be distributed via Github.  A local copy of the source code and the publications produced by the project will also be made available at the URL (http://batchmaker.news.cs.nyu.edu) for at least three years beyond the award period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817037","SHF:Small: Tensor-Based Algorithm and Hardware Co-Optimization for Neural Network Architecture","CCF","Special Projects - CCF","10/01/2018","06/19/2018","Zheng Zhang","CA","University of California-Santa Barbara","Standard Grant","Sankar Basu","09/30/2021","$499,998.00","Yuan Xie","zhengzhang@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","2878","075Z, 7923, 7945","$0.00","Machine learning plays an important role in our daily life, including medical data analysis, finance, autonomous driving and computer vision. Many popular machine learning models are both data-intensive and computationally expensive. In order to address this challenge, algorithm and architecture co-design and co-optimizations are required to achieve better performance and energy efficiency. This project aims to develop a more powerful learning architecture by simultaneously optimizing the neural network algorithm and its hardware implementation. The project will have a broad impact. The AI applications will gain a significant performance boost if the computational and storage cost of its algorithm can be reduced significantly.  It will improve many applications such as data mining, medical imaging analysis, computational biology, and financial analysis.  The project will greatly enrich the undergraduate and graduate course curriculum and attract graduate and undergraduate students to participate in this project and related workshops. Finally, this project will develop new AI, VLSI and computer architecture workforce with solid background in several areas including computational math, machine learning, and hardware design.<br/><br/>Tensors are a generalization of vectors and matrices, and they are promising tools to represent and numerically process high-dimensional data arrays. Leveraging the high effectiveness of tensor computation in big-data analysis, this project will investigate three specific topics towards designing high-performance and energy-efficient machine learning hardware. First, theoretically sound and novel tensor numerical algorithms will be developed to significantly reduce the training and inference cost of deep learning. Second, the algorithm framework will be optimized on existing hardware (e.g., GPU and FPGA) to achieve better performance, and to examine the main challenges when running on hardware platforms. Finally, emerging design technologies (e.g., 3-D process-in-memory) will be investigated to design specific hardware libraries to perform fundamental tensor computation and to further boost the performance and energy efficiency of the whole machine learning architecture.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747798","Phase I IUCRC University of Oregon: Center for Big Learning","CNS","IUCRC-Indust-Univ Coop Res Ctr","02/01/2018","02/21/2020","Dejing Dou","OR","University of Oregon Eugene","Continuing Grant","Behrooz Shirazi","01/31/2023","$449,999.00","Joseph Sventek, Daniel Lowd, Allen Malony, Boyana Norris","dou@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","5761","5761","$0.00","Recent advances in machine learning are ushering in a new era in intelligent computing.  The NSF Industry/University Collaborative Research Center for Big Learning (CBL) at the University of Oregon (UO) aims to design novel algorithms and develop efficient systems for deep learning applications in the era of big data.  CBL will catalyze diverse expertise of faculty members, students, industry partners, and federal agencies to create state-of-the-art deep learning methodologies, technologies, and applications across broad domains of business, healthcare, and Internet-of-Things.  CBL will provide training for new scientists and graduate students, as well as a rich environment for cross-disciplinary engagement.<br/><br/>The mission of CBL is to perform pioneering research and development in powerful deep learning algorithms, efficient intelligent systems, and novel applications through unified and coordinated efforts in the CBL consortium. The CBL research team at the UO includes experts in data science, artificial intelligence, machine learning, high-performance computing, health informatics, and bioinformatics. They will explore research projects related to medical record analysis, health behavior prediction, natural language processing, performance optimization, financial forecasting, and computer vision by deploying various deep learning models. <br/><br/>CBL is expected to have broad transformative impacts in computational technologies, education, and society. Through research and applications to address a broad spectrum of real-world challenges, CBL will make significant contributions and impacts to the deep learning community. The discoveries from CBL will amplify opportunities for new products and services of industry in general, while delivering key techniques to CBL industry partners in particular. As the nexus of deep learning research and applications, CBL offers an ideal platform to nurture next-generation talents through world-class mentors from both academia and industry, disseminate the cutting-edge technologies, and facilitate industry/university collaborative research.<br/><br/>The center's repository is hosted at http://nsfcbl.org.  The data, code, and documents will be organized and maintained on the CBL server for the duration of the center plus five years.  The internal code repository will be hosted at GitLab.  After the software packages are well documented and tested, they will be released and hosted at popular public servers, such as GitHub and Bitbucket."
"1854742","AitF: Collaborative Research: A Framework of Simultaneous Acceleration and Storage Reduction on Deep Neural Networks Using Structured Matrices","CCF","Algorithms in the Field","08/15/2018","10/25/2018","Bo Yuan","NJ","Rutgers University New Brunswick","Standard Grant","A. Funda Ergun","08/31/2021","$367,884.00","","bo.yuan@soe.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7239","","$0.00","      Deep neural networks (DNNs) have emerged as a class of powerful techniques for learning solutions in a number of challenging problem domains, including computer vision, natural language processing and bioinformatics.  These solutions have been enabled mainly because we now have computational accelerators able to sift through the myriad of data required to train a neural network.   As the size of DNN models continues to grow, computational and memory resource requirements for training will also grow, limiting deployment of deep learning in many practical applications. <br/><br/>      Leveraging the theory of structured matrices, this project will develop a general framework for efficient DNN training and inference, providing a significant reduction in algorithmic complexity measures in terms of both computation and storage.  <br/>The project, if successful, should fundamentally impact a broad class of deep learning applications.  It will explore accelerating this new structure for deep learning algorithms targeting emerging accelerator architectures, and will evaluate the benefits of these advances across a number of application domains, including big data analytics, cognitive systems, unmanned vehicles and aerial systems, and wearable devices.  The interdisciplinary nature of this project bridges the areas of matrix theory, machine learning, and computer architecture, and will affect education at both Northeastern and CCNY, including the involvement of underrepresented and undergraduate students in the rich array of research tasks.<br/><br/>     The project will: (1) for the first time, develop a general theoretical framework for structured matrix-based DNN models and perform detailed analysis and investigation of error bounds, convergence, fast training algorithms, etc.; (2) develop low-space-cost and high-speed inference and training schemes for the fully connected layers of DNNs; (3) impose a weight tensor with structure and enable low computational and space cost convolutional layers; (4) develop high-performance and energy-efficient implementations of deep learning systems on high-performance parallel platforms, low-power embedded platforms, as well as emerging computing paradigms and devices; (5) perform a comprehensive evaluation of the proposed approaches on different performance metrics in a variety of platforms.  The project will deliver tuned implementations targeting a range of computational platforms, including ASICs, FPGAs, GPUs and cloud servers. The hardware optimizations will focus on producing high-speed and low-cost implementations of deep learning systems."
"1814472","CHS: Small: Multimodal Conversational Assistant that Learns from Demonstrations","IIS","HCC-Human-Centered Computing","08/15/2018","04/09/2020","Brad Myers","PA","Carnegie-Mellon University","Standard Grant","Ephraim Glinert","07/31/2021","$531,019.00","Tom Mitchell","bam@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","075Z, 7367, 7923, 9251","$0.00","Intelligent assistants such as Apple's Siri, Amazon's Alexa and Microsoft's Cortana are rapidly gaining popularity by providing a conversational natural language interface for users to access various online services and digital content.  They allow computing tasks to be performed in contexts where users cannot touch their phones (such as while driving), and on wearable and Internet of Things (IoT) devices (such as Google Home).  However, such conversational interfaces are limited in their ability to handle the ""long-tail"" of tasks and suffer from lack of customizability.  This research will explore a new multi-modal, interactive, programming-by-demonstration (PBD) approach that enables end users to add new capabilities to an intelligent assistant by programming automation scripts for tasks in any existing third-party Android mobile app using a combination of demonstrations and verbal instructions.  The system will leverage state-of-the-art machine learning and natural language processing techniques to comprehend the user's verbal instructions that supply information missing in the demonstration, such as implicit conditions, user intent and personal preferences.  The user's demonstration on the graphical user interface will be used for grounding the conversation and reinforcing the natural language understanding model.  The system will point the way to allowing the general public to more effectively use their smartphones, IoT devices and intelligent assistants, increasing the adoption, efficiency and correctness of uses of these technologies.  The integration of intelligent assistants with PBD will have broad impact by exposing people to programming concepts in an easy-to-learn way, and thereby increasing computational thinking.  <br/><br/>This project will result in several innovations beyond the current state of the art through advances in programming by demonstration (PBD) and intelligent assistants, and especially in their integration.  The work will explore leveraging verbal instructions as an additional modality to address long-standing challenges in PBD research including generalizing the data descriptions and adding control structures.  How to coordinate the two modalities to help the intelligent assistant learn new tasks effectively and efficiently from users will be investigated, and how users utilize the two modalities in multi-modal PBD systems for programming tasks in different situations will also be studied.  New ways to leverage the displayed graphical user interfaces (GUI) of apps to enhance the speech recognition and language understanding by using the strings and other context of the GUI on the smartphone will be developed.  The ability of the conversational assistant to participate in this generalization process will be enhanced, with a focus on having the system ask appropriate and helpful questions so the task automation will fit the user's needs and intentions.  New approaches to representing scripts created by PBD systems that users can read, understand and edit will be explored, as will increasing trust and usefulness of the scripts and supporting error handling, debugging and maintenance.  The new technology will also be able to extract data from and enter data into apps, and to learn, through demonstration and verbal instruction, how to transform the data into appropriate formats.  Finally, how to support sharing of scripts created by PBD systems while ensuring the appropriate levels of privacy and security will also be investigated.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814759","SHF: Small: Collaborative Research: LDPD-Net: A Framework for Accelerated Architectures for Low-Density Permuted-Diagonal Deep Neural Networks","CCF","Software & Hardware Foundation","10/01/2018","04/17/2020","Keshab Parhi","MN","University of Minnesota-Twin Cities","Standard Grant","Sankar Basu","09/30/2021","$291,000.00","","parhi@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","075Z, 7923, 7945, 9251","$0.00","Deep learning has emerged as an important form of machine-learning where multiple layers of neural networks can learn the system function from available input-output data. Deep learning has outperformed traditional machine-learning algorithms based on feature engineering in fields such as image recognition, healthcare, and autonomous vehicles. These are widely used in cloud computing where large amount of computational resources are available. Deep neural networks are typically trained using graphic processing units (GPUs) or tensor processing units (TPUs). The training time and energy consumption grow with the complexity of the neural network. This project attempts to impose sparsity and regularity as constraints on the structure of the deep neural networks to reduce complexity and energy consumption by orders of magnitude, possibly at the expense of a slight degradation in the performance. The impacts lie in the formulation of a new family of structures for neural networks referred to as Low-Density Permuted Diagonal Network or LDPD-Net. The approach will enable the deployment of deep neural networks in energy-constrained and resource-constrained embedded platforms for inference tasks, including, but not limited to, unmanned vehicles/aerial systems, personalized healthcare, wearable and implantable devices, and mobile intelligent systems. In addition, the design methodology/techniques developed in this project can facilitate investigation of efficient computing of other matrix/tensor-based big data processing and analysis approaches. These approaches may also find applications in data-driven neuroscience and data-driven signal processing. In addition to graduate students, the project will involve undergraduates via senior design projects and research experiences for undergraduates. The results of the project will be disseminated to the broader community by publications, presentations, talks at various industries and other academic institutions. <br/><br/>The main barriers to wide adoption of deep learning networks include computational resource constraints and energy consumption constraints. These barriers can be relaxed by imposing sparsity and regularity among different layers of the deep neural network. The proposed low-density permuted-diagonal (LDPD) network can lead to orders of magnitude reduction in computation complexity, storage space and energy consumption. The LDPD-Net will not be retrained by first training a regular network and then only retaining the weights corresponding to the LDPD-Net. Instead, the proposed network will be trained from scratch. The proposed LDPD-Net can enable scaling of the network for a specified computational platform. The proposed research has three thrusts: 1) develop novel resource-constrained and energy-constrained inference and training systems;  2) develop novel efficient hardware architectures that can fully exploit the advantages of the LDPD-Net to achieve high performance; and 3) perform novel software and hardware co-design and co-optimization to explore the design space of the LDPD-Net. Using these, the efficacy of the proposed LDPD-net will be validated and evaluated, via software implementations on high-performance systems, low-power embedded systems, and a hardware prototype on FPGA development boards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815699","SHF: Small: Collaborative Research: LDPD-Net: A Framework for Accelerated Architectures for Low-Density Permuted-Diagonal Deep Neural Networks","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","06/22/2018","Bo Yuan","NY","CUNY City College","Standard Grant","Sankar Basu","11/30/2018","$224,997.00","","bo.yuan@soe.rutgers.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","CSE","7798","075Z, 7923, 7945","$0.00","Deep learning has emerged as an important form of machine-learning where multiple layers of neural networks can learn the system function from available input-output data. Deep learning has outperformed traditional machine-learning algorithms based on feature engineering in fields such as image recognition, healthcare, and autonomous vehicles. These are widely used in cloud computing where large amount of computational resources are available. Deep neural networks are typically trained using graphic processing units (GPUs) or tensor processing units (TPUs). The training time and energy consumption grow with the complexity of the neural network. This project attempts to impose sparsity and regularity as constraints on the structure of the deep neural networks to reduce complexity and energy consumption by orders of magnitude, possibly at the expense of a slight degradation in the performance. The impacts lie in the formulation of a new family of structures for neural networks referred to as Low-Density Permuted Diagonal Network or LDPD-Net. The approach will enable the deployment of deep neural networks in energy-constrained and resource-constrained embedded platforms for inference tasks, including, but not limited to, unmanned vehicles/aerial systems, personalized healthcare, wearable and implantable devices, and mobile intelligent systems. In addition, the design methodology/techniques developed in this project can facilitate investigation of efficient computing of other matrix/tensor-based big data processing and analysis approaches. These approaches may also find applications in data-driven neuroscience and data-driven signal processing. In addition to graduate students, the project will involve undergraduates via senior design projects and research experiences for undergraduates. The results of the project will be disseminated to the broader community by publications, presentations, talks at various industries and other academic institutions. <br/><br/>The main barriers to wide adoption of deep learning networks include computational resource constraints and energy consumption constraints. These barriers can be relaxed by imposing sparsity and regularity among different layers of the deep neural network. The proposed low-density permuted-diagonal (LDPD) network can lead to orders of magnitude reduction in computation complexity, storage space and energy consumption. The LDPD-Net will not be retrained by first training a regular network and then only retaining the weights corresponding to the LDPD-Net. Instead, the proposed network will be trained from scratch. The proposed LDPD-Net can enable scaling of the network for a specified computational platform. The proposed research has three thrusts: 1) develop novel resource-constrained and energy-constrained inference and training systems;  2) develop novel efficient hardware architectures that can fully exploit the advantages of the LDPD-Net to achieve high performance; and 3) perform novel software and hardware co-design and co-optimization to explore the design space of the LDPD-Net. Using these, the efficacy of the proposed LDPD-net will be validated and evaluated, via software implementations on high-performance systems, low-power embedded systems, and a hardware prototype on FPGA development boards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844740","EAGER: Exploring Cognitively Plausible Computational Models for Processing Human Language","IIS","Robust Intelligence","09/01/2018","08/17/2018","Anna Rumshisky","MA","University of Massachusetts Lowell","Standard Grant","D.  Langendoen","08/31/2021","$109,926.00","","arumshisky@gmail.com","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","7495","7495, 7916","$0.00","Despite the recent successes of artificial intelligence techniques designed to process human language, most contemporary solutions are designed to handle very specific language processing tasks.  As a result, human-level language understanding is still out of reach for most current computational approaches, especially when retaining new information and reasoning over the accumulated knowledge is involved.  This exploratory project advances the goal of developing more cognitively realistic computational models that can mimic some of the known properties of human language processing, and as a result, be more robust and better suited as general systems for language understanding, with human-like learning which involves obtaining and updating knowledge over time.<br/><br/>While most contemporary deep learning approaches in natural language processing focus on task-specific end-to-end models, this project prioritizes generalist architectures that would be consistent with the current data on semantic priming, grouping and chunking effects in the formation and use of conceptual systems, and the effects of long- and short-term memory on the storage and retrieval of knowledge. In this project, novel neural network architectures are planned that model a subset of these properties.  The processes that enable learning and memory via strengthening of synaptic connections in the brain will be emulated by a set of representational units (r-units) with bidirectional connections, modeling the interaction between small regions of neocortex during information processing. Memory Store Activation State Model represents the connections between r-units in terms of convolutional filters applied to the memory store. The priming effects will be modeled by a pre-activation pattern produced via a sequence of deconvolutional operation.  Rate-Based Connectivity Network model combines reinforcement learning on per-node basis with a form of Hebbian learning applied to a time-varying system where each r-unit calculates rate of change of its output, allowing node activations to linger through time; it is trained with a discrete global reward signal. The goal of this project is to establish the feasibility of the proposed architectures by developing the initial proof-of-concept prototypes, demonstrating that they are able to converge on simple learning tasks, and applying them to the task of language modeling to ensure that a practically useful representation can be learned.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837986","BIGDATA: IA: Automating Analysis and Feedback to Improve Mathematics Teachers' Classroom Discourse","IIS","ECR-EHR Core Research, Big Data Science &Engineering","10/01/2018","10/03/2018","Tamara Sumner","CO","University of Colorado at Boulder","Standard Grant","Tatiana Korelsky","09/30/2021","$1,998,505.00","James Martin, Wayne Ward, Jennifer Jacobs, Chenhao Tan, William Foland","sumner@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7980, 8083","062Z, 8083, 8817","$0.00","This research project will develop and study an innovative application - TalkBack - for addressing a significant challenge in education: providing teachers with personalized feedback on classroom discussion strategies. The TalkBack application builds on advances in deep learning for natural language processing and speech recognition to automatically analyze classroom discussions and reliably generate information about specific classroom dialog between student and teacher that occur in active learning. This research will significantly extend existing machine learning from simple descriptions, and automated classification, of dialog to more complex descriptions, and automated classification, of dialog using deep learning. TalkBack will be a cloud application available to any teacher who has classroom video and who wants to improve active learning in the classroom.<br/><br/>The TalkBack application will consist of three interrelated components: a cloud-based big data infrastructure for managing and processing classroom recordings, deep learning models that reliably detect the use of talk moves, and an innovative interface that provides teachers with personalized, feedback on their use of discussion strategies during individual teaching episodes and longitudinally over multiple episodes. Two user studies will be conducted to gather information from math teachers related to the design and impact of the application. These user studies will include a pilot study in year 2 (n = 20 teachers) and a field study in year 3 (n = 100 teachers). The TalkBack application will provide an exemplar for a new type of translational activity enabled by big data: the reification of existing, well-researched theoretical frameworks in deep learning models. Building on NSF's investment in research on talk moves, including the Accountable Talk and the IQA frameworks, this work demonstrates how analyses of teaching practices using these frameworks can be fully automated and scaled up to support large numbers of teachers longitudinally over time. Furthermore, this effort will demonstrate how a cloud-based infrastructure supporting the detailed analysis of classroom recordings using speech and language processing can be used to develop next generation learning environments (in this case, personalized feedback on teaching practices) and to uncover new insights into teaching practices at scale. Specifically, this research will provide unprecedented insight into the ways that classroom discussions and student participation changes as teachers develop and expand their use of talk moves over time. This study will develop the big data application, TalkBack, providing immediate and actionable feedback to teachers based on self-recordings of their mathematics lessons.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820827","CDS&E: Structure-Aware Representation Learning Using Deep Networks","DMS","CDS&E-MSS","07/01/2018","06/24/2020","Xiuyuan Cheng","NC","Duke University","Continuing Grant","Christopher Stark","06/30/2021","$200,000.00","Qiang Qiu","xiuyuan.cheng@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","MPS","8069","8083, 9263","$0.00","In recent years, deep neural networks have been widely applied to analyze Big Data produced in various fields of science, industry and society. In the area of signal processing, deep Convolutional Neural Network (CNN) is one of the most successful computational models, with numerous applications including visual object recognition, object detection and localization. Despite the wide success of deep networks, the data representation computed by these models does not necessarily reflect the geometric information in the input data in an understandable way; a gap remains between the remarkable performance of deep models and the interpretability of such performance. In particular, deep networks trained from enormous amounts of data typically have no specific structures in the model parameters, which also leads to significant redundancy in the model. This project will investigate the mathematical foundation for imposing appropriate structures in deep networks, aiming at more analyzable and efficient network models with theoretically guaranteed performance. The results will have direct applications in various machine learning tasks, improving the accuracy, computational efficiency and interpretability of existing models. The theoretical analysis to be developed will deepen the mathematical understanding of deep networks, which is important for the next generation of computational tools for machine learning. Students engaged in the project will be trained in an interdisciplinary environment of mathematics and electrical engineering, developing skills in both analysis and software implementation, which benefits their future careers in academia or industry. The project will also train future mathematicians and electrical engineers through course development, especially courses on machine learning and image sciences with public online repositories. The project explores the new possibility of representation learning using deep networks, a tool with immense potential to address key challenges in today's Big Data analysis and artificial intelligence. <br/><br/>The goal of the project is to develop novel mathematical analysis of the deep Convolutional Neural Network (CNN) model, as well as innovative designs of CNNs with appropriate structures based on the analysis. Specifically, the PIs will study: (1) the geometric structures in the channels of the convolutional layers, with which the CNN representations can collaboratively and explicitly encode geometric information in the data with improved interpretability and robustness; (2) the spatial structures of the convolutional filters, by which the filter regularity can be analytically imposed so that the CNN representations can be provably stable to input variations; and (3) efficient software implementation, which transfers the theoretical results into applications such as object detection and image segmentation in computer vision. The PIs will use tools from harmonic analysis and approximation theory to address the following open problems in the field: the removal of redundancy in trained CNN filters in a principled way while avoiding under-fitting, the more efficient learning of invariant representations with respect to geometrical transforms in the data, and the theoretical guarantees of the deep representations learned by an adaptive network that is trained from data. The new mathematical understanding will guide the design of deep networks to achieve better performance, in accuracy and computational speed, and better interpretability of the learned data representation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750101","CAREER: Machine Learning Assisted Crowdsourcing for Phishing Defense","CNS","Secure &Trustworthy Cyberspace","06/01/2018","09/18/2019","Gang Wang","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Wei-Shinn Ku","06/30/2020","$340,703.00","","gangw@illinois.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8060","025Z, 1045, 7434","$0.00","This project aims to address the growing threat of phishing attacks, messages that try to trick people into revealing sensitive information, by combining human and machine intelligence. Existing detection methods based on machine learning and blacklists are both brittle to new attacks and somewhat lenient, in order to avoid blocking legitimate messages; as a result, widely used email systems are vulnerable to carefully crafted phishing emails. To address this, the project team will develop systems that automatically block obvious scams while forwarding less certain cases to groups of crowd workers trained to detect phishing mails. To support these workers' decision-making, the team will develop novel explanations of the system's decision making that will highlight the aspects of both the message and its algorithm that triggered the need for human judgment. The system will also aggregate these crowd decisions to generate real-time phishing alerts that can be shared to both individual users and to email systems. The project will lead to advances in interpretable machine learning, an important topic given the increasing role that artificial intelligence and machine learning systems play in society, and also increase our ability to characterize the evolution of phishing attacks and the vulnerability of internet platforms and users to those attacks over time. The project team will also use the work as an important component of new courses on usable security and outreach programs to high school teachers and students to both educate them about and increase their participation in cybersecurity research.<br/><br/>The work is organized around three main objectives: empirical characterization of phishing risks, developing accurate and interpretable machine learning models for phishing detection, and developing reliable crowdsourcing systems for phishing alerts. The team will assess phishing risks through developing analytics tools on the effective adoption and configuration of anti-spoofing protocols in email systems, using adversarial machine learning methods to conduct black box testing on existing phishing detectors, and creating reactive honeypots that entice and respond to phishing attacks in order to collect data on not just the initial phishing emails but on attackers' behaviors throughout the course of a successful phishing attack. The data collected on phishing emails will be used to develop the machine learning models, using Convolutional Neural Network and Long Short-Term Memory based deep learning techniques to generate both suspicious features and confidence estimates of individual decisions. The suspicious features will be used to generate interpretable security cues such as text annotations or icons by first creating simpler and more interpretable machine learning models such as decision trees that mimic the local detection boundary near the target emails in the feature space. Rules in the decision tree will be mapped back to interface elements and email content to provide the warnings, and these will be compared to generic email security warnings in a series of user studies that also model people's ability to detect phishing using a variety of cues, features, and media. Those individual models, along with the confidence estimates from the phishing detection model, will then be used to drive a crowdsourcing-based system where the models of individual users' quality will be aggregated to make reliable judgments around emails the models judge as too suspicious to pass but not suspicious enough to automatically filter.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828265","MRI: Acquisition of a Composable Platform as a Service Instrument for Deep Learning & Visualization (COMPaaS DLV)","CNS","Special Projects - CNS, IIS Special Projects","10/01/2018","05/29/2020","Maxine Brown","IL","University of Illinois at Chicago","Standard Grant","Rita Rodriguez","09/30/2021","$1,029,363.00","Robert Kenyon, Andrew Johnson, Georgeta-Elisab Marai","maxine@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","1714, 7484","1189, 9251","$0.00","This project is about acquiring a much-in-demand Graphics Processing Unit (GPU)-based instrument, to develop a Service Instrument for Deep Learning and Visualization called ""COMPaaS DLV: COMposable Platform"".  The project aims to complement available campus computing resources via the campus's research network. It will be able to access local and remote computing and storage facilities funded, including XSEDE, Blue Waters, Deep Learning Instrument and Chameleon. This critical instrumentation will provide a platform to pursue fundamental science and engineering research training in deep learning (data mining and data analytics, computer vision, natural language processing, artificial intelligence), visualization (simulation, rendering, visual analytics, video steaming, image processing), and a combination of deep learning and visualization (e.g., when data is so large that it cannot be easily visualized, then deep learning is used to extract features of interest to be visualized). The instrumentation also enables investigation and contribution to societal issues in disciplines such as anthropology, biology, cybersecurity, data-literacy, fraud detection, healthcare, manufacturing, urban sustainability, and cyber-physical systems (e.g., autonomous cars). <br/><br/>Its design utilizes state-of-the-art computer architecture, known as composable architecture, in which the computer's components (traditional processor, GPU, storage, and networking) form a fluid pool of resources, such that different applications with different workflows can be run simultaneously, with each configuring the resources it requires almost instantaneously, at any time. Given composable infrastructure scalability and agility, it is more beneficial than traditional clouds and clusters that are rigid, overprovisioned, and expensive.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821144","CDS&E: Efficient and Robust Recurrent Neural Networks","DMS","CDS&E-MSS","09/01/2018","06/14/2018","Qiang Ye","KY","University of Kentucky Research Foundation","Standard Grant","Christopher Stark","08/31/2021","$200,000.00","","qye3@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","MPS","8069","8083, 9150, 9263","$0.00","Deep neural networks have emerged over the last decade as one of the most powerful machine learning methods. Recurrent neural networks (RNNs) are special neural networks that are designed to efficiently model sequential data such as speech and text data by exploiting temporal connections within a sequence and handling varying sequence lengths in a dataset. While RNN and its variants have found success in many real-world applications, there are various issues that make them difficult to use in practice. This project will systematically address some of these difficulties and develop an efficient and robust RNN. Computer codes derived in this project will be made freely available. The research results will have applications in a variety of areas involving sequential data learning, including computer vision, speech recognition, natural language processing, financial data analysis, and bioinformatics.<br/><br/>As in other neural networks, training of RNNs typically involves some variants of gradient descent optimization, which is prone to so-called vanishing or exploding gradient problems. Regularization of RNNs, which refers to techniques used to prevent the model from overfitting the raining data and hence poor generalization to new data, is also challenging. The current preferred RNN architectures such as the Long-Short-Term-Memory networks have highly complex structures with numerous additional interacting elements that are not easy to understand. This project develops an RNN that extends a recent orthogonal/unitary RNNs to more effectively model long and short term dependency of sequential data. Through an indirect parametrization of recurrent matrix, dropout regularization techniques will be developed. The network developed in this project will retain the simplicity and efficiency of basic RNNs but enhance some key capabilities for robust applications. In particular, the project will include a study of applications of RNNs to some bioinformatics problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848913","I-Corps: Autonomous Unmanned Vehicle Ecosystem Technology","IIP","I-Corps","09/15/2018","08/16/2018","Michael Lawrence","NY","CUNY Queensborough Community College","Standard Grant","Andre Marshall","02/29/2020","$50,000.00","","mlawrence@qcc.cuny.edu","222-05 56th Avenue","Oakland Gardens","NY","113641497","7186316222","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is the use of autonomous robotic systems to save lives across multiple disaster levels by multiplying human efforts in areas such as disaster response, extending to reduced response times and improved rescue efforts during disaster relief. Using autonomous systems, such as drones, that run software which allows for the use of biosensors and on-board machine learning, natural disaster responders can find survivors faster and reduce casualties. The technology understands mission objectives (intent-driven), coordinates with others (collaborative autonomy) and is voice enabled (natural language conversational interface), allowing it to work well with both humans and machines. These and other features lend this technology to broad commercial applications, offering the best strategy to mitigate multiple disaster levels - at the district or provincial level by assessing strengths and vulnerabilities to various hazards; at the community level like search and rescue during a flood or fire; and at the household level like ambulatory drone service care for the aging.<br/><br/>This I-Corps project will be focused on translating scientific research to commercialization through lessons on the business model canvas and extensive customer discovery. Our research has involved hands-on technical experimentation using deep/machine learning software for prototype development and literature reviews on a variety of software capabilities. Using a camera with machine learning, we were able to identify a human with most of the body obscured. Hardware acts as transport for the optical and acoustical biosensor package which can take action autonomously by utilizing artificial intelligence (AI) technologies extending to: realtime onboard object detection capable of identifying a person in rubble, even when the person is partially obscured (Machine/Deep Learning/edge), offline cloud-enriched datasets for cloud-sourced analysis, voice recognition/conversational natural language processing/speech synthesis. Blockchain enabled autonomous swarm management technology on multiple robotic devices will enable multiple machine collaboration. All sensor data is time-stamped and stored in the cloud where the AI technologies are used to enrich the data. Enriched data is made available offline via the internet to thousands of volunteers for human review. Results from our research show that the technological capability exists, the software and hardware components combined meet a need established through our initial customer discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814631","RI:Small: Dynamic and Statistical Based Invariants on Manifolds for Video Analysis","IIS","Robust Intelligence","09/01/2018","08/19/2018","Octavia Camps","MA","Northeastern University","Continuing Grant","Jie Yang","08/31/2021","$500,000.00","Mario Sznaier","camps@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7495","7495, 7923","$0.00","Computer vision systems can benefit society in many ways.  For example, spatially distributed vision sensors endowed with activity analysis capabilities can prevent crime, help optimize resource use in smart buildings, and give early warning of serious medical conditions.  The most powerful computer vision systems employ an approach called ""deep learning"", in which simulated networks of neurons transform the input video pixels into high-level concepts. For example, in the crime example, the high-level concept might be ""someone breaking into a building"".  A major impediment to building computer vision is that great expertise and trial-and-error is required for a programmer to design a neural network that can teach itself to recognize the goal concepts.  This project will reduce this barrier by creating a set of well-designed neural network modules, or ""layers"", that a programmer can snap together to build a working computer vision system.  Education is proactively integrated into this project, starting with STEM summer camps projects for urban middle school students and continuing at the college level with a multi-disciplinary program that uses the grand challenge of aware environments to link a full range of distinct subjects ranging from computer vision and machine learning to systems theory and optimization. At the graduate level, these activities are complemented by recruitment efforts that leverage the resources at Northeastern's University Program in Multicultural Engineering to broaden the participation of underrepresented groups in research. <br/><br/>Computer vision has made tremendous progress in the era of deep learning. However, training of deep architectures requires learning the optimal value of a very large number of parameters through the numerical minimization of a non-convex loss function. While in practice, using stochastic gradient descent to solve this problem often ""works"", the analysis of what the network learned or why it failed to do so, remains an a-posteriori task requiring visualization tools to inspect which neurons are firing and possibly to look at intermediate results. This research seeks to address this issue by incorporating a set of structured layers to current deep architectures, designed using dynamical systems theory and statistics fundamentals, which capture spatio-temporal information across multiple scales. At its core is a unified vision, invariants on latent space manifolds as information encapsulators, that emphasizes robustness and computational complexity issues. Advantages of the proposed layers include the ability to easily understand what they learn, since they are based on first principles; shallower networks with a reduction of the number of parameters that needs to be learned due to the high expressive power of the new layers; and requiring less annotated data, by providing efficient ways to transfer knowledge between domains and to synthesize realistic data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826540","Statistical Learning for Innovative Assessment","SES","Methodology, Measuremt & Stats, IIS Special Projects","09/01/2018","08/07/2018","Jingchen Liu","NY","Columbia University","Standard Grant","Cheryl Eavey","08/31/2021","$358,991.00","Zhiliang Ying","jcliu@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","SBE","1333, 7484","075Z","$0.00","This research project will develop statistical methods for modern cognitive assessment. With the increasing use of computer-based testing, a variety of high-dimensional and complex structured data sets have been collected. The project will focus on statistical modeling and inference for large-scale data sets of complex structures. Specific topics to be addressed include the analysis of process data, adaptive learning through a reinforcement learning framework, and the development of computational methods for the models to be developed. The results of this research will provide a deeper understanding of the complex data structures collected in technology-rich interactive tasks. The project will shed light on items in learning and assessment environments that are delivered online both in client-server constellations and in stand-alone applications. The project will provide guidelines to improve item quality with a focus on more innovative item types, such as those in scenario-based and simulation-based environments for the assessment of students' knowledge and skills in the STEM fields. Educational researchers will be provided with tools to identify patterns in high-dimensional data and sequence data. Students in instructional and interventional programs will benefit from this research, especially in the STEM fields that are increasingly defined by digital media and technology-based interaction and communication.<br/><br/>Recent large-scale computer-based assessments have developed a number of interactive problem-solving items and collaborative problem-solving items. The investigators will develop statistical methods for the analysis of these new items. The investigators will concentrate on several aspects that are very challenging in the analysis of modern computer-based assessment; specifically, they will focus on: 1) predicting human behavior by means of modern machine learning techniques; 2) extracting latent structure and graphical structure for process data collected by interactive problem-solving items through event history analyses; 3) providing personalized learning material through a reinforcement learning framework; and 4) developing numerical methods to optimize high-dimensional functions either stochastically or deterministically. The models to be developed will combine latent variable and graphical approaches as well as deep-learning techniques for high-dimensional data. For modeling process data, the investigators will employ recent advances in modeling and segmenting techniques for natural language processing. For computation, the investigators will develop adaptive Robbins-Monro stochastic approximation. Optimization algorithms will be developed using recent advances in numerical methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827314","MRI: Acquisition of a GPU Accelerated Vermont Advanced Computing Core","OAC","Major Research Instrumentation","09/01/2018","08/23/2018","Adrian Delmaestro","VT","University of Vermont & State Agricultural College","Standard Grant","Stefan Robila","08/31/2020","$893,120.00","Joshua Bongard, Yolanda Chen, Hugh Garavan, Juan Vanegas","Adrian.DelMaestro@utk.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","CSE","1189","026Z, 062Z, 075Z, 1189, 8089, 8091, 9150","$0.00","This project will enable interdisciplinary science through the acquisition of a high-performance computer cluster, named DeepGreen.  Based on cutting-edge massively parallel graphics processing unit (GPU) technologies, DeepGreen will be utilized by the over 300 users from six Colleges at the University of Vermont, and throughout the Northeast. The unique hybrid architecture was designed to optimize artificial intelligence (AI) applications and will allow for rapid progress on problems of great societal importance. They include: quantum computing, drug discovery and design, safe robotics, control of adaptive crop pests, and new computer vision tools for use in the health care and transportation industries.  As an example, DeepGreen will allow the training of neural networks on the world's largest brain imaging datasets of illicit drug users, yielding novel health and policy strategies to combat the opioid epidemic.  A focus of the scientific and technical team is to broaden the number of personnel able to exploit GPU hardware for problem solving, producing the highly trained and diverse technical workforce required for the current and future AI economy. <br/><br/>DeepGreen was designed by a team of experts from the physical, medical, biological, computational, and agricultural sciences, partnered with an experienced group of information technology professionals.  It will be capable of over 8 petaflops of mixed precision calculations based on the latest NVIDIA Tesla V100 architecture with a hybrid design allowing high bandwidth message passing across heterogeneous compute nodes.  Its extreme parallelism will facilitate research in three interconnected areas: quantum many-body systems, molecular simulation and modeling, and deep learning, artificial intelligence and evolutionary algorithms.  DeepGreen will forge transformative research pipelines. It will enable the study of thousands of quantum entangled atoms, and millions of interacting components in biological systems providing insights into structure-function mechanisms.  Machine learning and deep neural networks will exploit DeepGreen's Tensor Cores to solve diverse problems. These problems include: the development of coarse grained potentials for use in molecular dynamics simulations, real time dynamic processing of crowd sourced decision making for robotics, genomic sequencing of invasive pests, and feature recognition in medical imaging to distinguish cancerous tumors from benign nodules.  Software designed for use on DeepGreen will be released to the public as open source, with other scientists and researchers being able to immediately use and extend it. This project will also support the next generation of data scientists. Training workshops focused on GPU computing and machine learning frameworks, new university courses, and partnerships with existing local NSF-funded graduate training initiatives, will drive broad utilization of DeepGreen.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816701","III: Small: Towards Speech-Driven Multimodal Querying","IIS","Info Integration & Informatics","10/01/2018","07/31/2018","Arun Kumar","CA","University of California-San Diego","Standard Grant","Wei-Shinn Ku","09/30/2021","$500,000.00","Lawrence Saul, Ndapandula Nakashole","arunkk@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7364","075Z, 7364, 7923","$0.00","Modern automatic speech recognition (ASR) tools offer near-human accuracy in many scenarios. This has increased the popularity of speech-driven input in many applications on modern device environments such as tablets and smartphones, while also enabling personal conversational assistants. In this context, this project will study a seemingly simple but important fundamental question: how should one design a speech-driven system to query structured data? Structured data querying is ubiquitous in the enterprise, healthcare, and other domains. Typing queries in the Structured Query Language (SQL) is the gold standard for such querying. But typing SQL is painful or impossible in the above environments, which restricts when and how users can consume their data. SQL also has a learning curve. Existing alternatives such as typed natural language interfaces help improve usability but sacrifice query sophistication substantially. For instance, conversational assistants today support queries mainly over curated vendor-specific datasets, not arbitrary database schemas, and they often fail to understand query intent. This has widened the gap with SQL's high query sophistication and unambiguity. This project will bridge this gap by enabling users to interact with structured data using spoken queries over arbitrary database schemas. It will lead to prototype systems on popular tablet, smartphone, and conversational assistant environments. This could help many data professionals such as data analysts, business reporters, and database administrators, as well as non-technical data enthusiasts. For instance, nurse informaticists can retrieve patient details more easily and unambiguously to assist doctors, while analysts can slice and dice their data even on the move. The research will be disseminated as publications in database and natural language processing conferences. The research and artifacts produced will be integrated into graduate and undergraduate courses on database systems. The PIs will continue supporting students from under-represented groups as part of this project.<br/><br/>This project will create three new systems for spoken querying at three levels of ""naturalness."" The first level targets a tractable and meaningful subset of SQL. This research will exploit three powerful properties of SQL that regular English speech lacks--unambiguous context-free grammar, knowledge of the database schema queried, and knowledge of tokens from the database instance queried--to support arbitrary database schemas and tokens not present in the ASR vocabulary. The PIs will synthesize and innovate upon ideas from information retrieval, natural language processing, and database indexing and combine them with human-in-the-loop query correction to improve accuracy and efficiency. The second version will make SQL querying even more natural and stateful by changing its grammar. This will lead to the first speech-oriented dialect of SQL. The third version will apply the lessons from the previous versions to two state-of-the-art typed natural language interfaces for databases. This will lead to a redesign of such interfaces that exploits both the properties of speech and the database instance queried.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854863","PaySpray","CNS","I-Corps","11/01/2018","10/26/2018","Ali Dehghan","CA","University of California-Riverside","Standard Grant","Pamela McCauley","04/30/2019","$50,000.00","","dehghan.alex@gmail.com","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","8023","","$0.00","The broader impact/commercial potential of this I-Corps project (PaySpray) is to provide a platform in which the members offer their products or services to the network, increase their assets, and use them to reduce their debts. Each member of the system is a network node who is connected to the other members through financial links that represent debt or credit relationships. Once a transaction occurs between two members of a network, either inside a system or outside of it, the link including transaction information will be created between them. PaySpray owns the knowledge over all the networks supplies, demands, and transactions and finds the optimum way to suggest the services from members to members to close a cycle. Members can be individuals or companies, and services can be anything legal to be sold or done. A member of a cycle may be a member of multiple cycles. Services may exceed the amount of the owed money or be less than that, the remaining debt can be paid either by paying money or providing another service or entering into a new cycle. The object is to create a network that permits various type of payments, bartering and financial transactions to manage an individuals or organizations finances. <br/><br/>This I-Corps project utilizes Artificial intelligence on a platform that intelligently connects users for financial transactions.  Members may invite anyone to join their network that they have financial relationships with whether they are the payee  or recipient of the finances.  This process continues until   until a cycle is created.  The system permits multiple cycles, and members can participate in multiple cycles. <br/><br/>In developing the application, a heuristic complex network algorithm  was utilized to identify the possible loops, matching service and demand in the networks. This was also used as a means to improve the efficiency and optimize performance of the model. The research also applied machine learning algorithms (mainly Natural Language Processing) to extract the information from users, inputs and use the output of the ML model to match service and demands. As a future improvement, researchers will apply deep learning RNN models in order to detect the fraud services and demands in the app.  Currently the application is developed in AngularJS with MongoDB Databases and deployed on Amazon Web Service.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751365","CAREER: Physically-Motivated Learning of 3D Shape and Semantics","IIS","Robust Intelligence","04/01/2018","03/31/2020","Manmohan Chandraker","CA","University of California-San Diego","Continuing Grant","Jie Yang","03/31/2023","$356,910.00","","mkchandraker@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","1045, 7495","$0.00","A system that navigates or interacts with the real world must reason about 3D geometric properties such as distances or orientations of objects, as well as semantic properties such as part locations or object types. This project combines physics-based modeling of images and shapes, with the versatility of robust optimization and deep learning, to recover 3D shape and semantic information. The work establishes connections between computer vision, machine learning, computer graphics and perception. Vision is a powerful sensing modality since images encode rich information about shape and semantics. However, image formation is a physical phenomenon that often includes complex factors like shape deformations, occlusions, material properties and participating media. Consequently, practical deployment of autonomous or intelligent vision-based systems requires robustness to the effects of diverse physical factors. Such effects may be inverted by modeling the image formation process, but hand-crafted features and hard-coded rules face limitations for data inconsistent with the model. Recent advances in deep learning have led to impressive performances, but generalization of a purely data-driven approach to handle such complex effects is expensive. To address these challenges, this project develops technologies of handling the diversity of real-world images through incorporation of physical models of image formation within deep learning frameworks. The project creates a cross-disciplinary educational program in vision, graphics, learning and perception through coursework that draws connections across wide areas such as physically-based modeling, deep learning, 3D reconstruction and semantic understanding. The program also develops K-12 educative modules that provide experiential insight into novel technologies such as virtual reality or self-driving, with a focus on outreach to students from under-represented backgrounds.<br/><br/>This research lays the foundations for physically-motivated learning of 3D shape and semantics, with benefits such as higher accuracy, better generalization or greater ease of training. It develops theoretical frameworks that relate unknown material behavior to 3D shape, which allows robust optimization frameworks and convolutional neural network architectures for material-invariant shape estimation. It designs novel network structures that model complex transformations, to generalize recovery of shape or semantics across non-rigid and articulated deformations, or distortions due to refraction and participating media. Further, it uses physical models of appearance or motion to bridge the domain gap between simulations and real images, leading to weakly supervised frameworks that mitigate the expense of data annotation. These advances enable novel applications for light field imaging, augmented reality, self-driving in challenging weather, or underwater robotic exploration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814985","CSR: Small: Collaborative Research: Decentralized Real-Time Machine Learning Systems on Near-User Edge Devices","CNS","CSR-Computer Systems Research","10/01/2018","08/13/2018","Michael Ryoo","IN","Indiana University","Standard Grant","Erik Brunvand","09/30/2021","$250,000.00","","mryoo@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7354","7923","$0.00","The ever-increasing number of Internet of Things (IoT) devices generate large quantities of raw data that need to be processed and analyzed in real time.  Since conducting computationally expensive tasks, such as computer vision and natural language processing, is often a challenge for IoT devices, most of their computations are currently offloaded to cloud servers. However, this offloading leads to an increased risk for privacy as well as a dependency on network connectivity. To solve this challenge, the project utilizes the distributed computing power of already connected IoT devices to perform high computing power applications in real time.<br/><br/>The project is composed of three tasks. First is the development of distributed machine learning (ML) systems for multiple IoT devices.  The project will involve studying how to communicate between nodes with reliable connections and how to dynamically change the job of each node at run-time with little overhead. Second is the development of optimal task assignment and scheduling algorithms. Here, a machine learning approach will be used to generate a recognition model architecture optimal for each distributed system configuration. Third is the development of low-resolution deep neural network (DNN) systems to utilize low-power computing nodes. The development of these DNN systems will involve identifying multiple low-resolution filters that are optimal for varying configurations.<br/><br/>The proposed technical work will advance the state of the art in implementation of parallel and decentralized DNN systems, thereby benefiting all scientific fields of endeavor that rely on computing. The decentralized DNN system will offer new opportunities in power constrained mobile platforms for applications including surveillance and automotive. The research results will lead to new materials/courses for computer architecture and systems. The proposed infrastructure will also be used to guide undergraduate students' research activities. <br/><br/>The software infrastructure will be maintained as an open source project, which can be found at https://github.com/parallel-ml.  It will be updated periodically as new outcomes become available. The results will be published in conferences, journals and technical reports.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838159","BIGDATA: F: Advancing Deep Learning to Monitor Global Change","IIS","Big Data Science &Engineering","11/01/2018","05/21/2020","Vipin Kumar","MN","University of Minnesota-Twin Cities","Standard Grant","Sylvia Spengler","10/31/2021","$1,462,398.00","Michael Steinbach, Philip Pardey, James Wilgenbusch, Senait Senay","kumar@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8083","062Z, 8083, 9251","$0.00","Growth in the world's population and the acceleration of industrialization and urbanization are straining already scarce natural resources and food supplies, which must scale up to keep pace with growing demand. The consequences of the resulting large-scale changes include tremendous stresses on the environment, as well as challenges to our ability to feed the world's population, which could be calamitous at the current rate of change if not managed sustainably. Meeting this challenge will require timely information on global changes; for example, the changing productivity performance of land in agriculture; the conversion of forest to farmland or plantations, and the loss of productive farmland due to urbanization; and soil and water degradation. To address these challenges in monitoring global change, this project will develop advanced machine learning techniques, especially deep learning. The project's primary focus will be on the analysis of remote sensing data, available from a variety of instruments and sensors aboard satellites through United States and international agencies. These rich datasets capture multiple facets of the natural processes and human activities that shape the physical landscape and environmental quality of our planet, and thus offer an opportunity to study and better shape the nature and impact of global changes.<br/><br/>This project seeks to greatly advance the state-of-the-art in machine learning techniques for analyzing the multi-scale, multi-source, spatio-temporal data about earth system processes. Specifically, this project will advance deep learning techniques to meet the challenges involved in using remote sensing data for global change monitoring. Deep learning has been successful in addressing problems in a number of domains involving complex data sets with spatial and temporal (sequential) information such as vision, video, and natural language processing. The promise of deep learning mainly stems from its capacity to exploit complex mapping relationships and extract discriminative features over space and time using large volumes of training data. Consequently, there has been a flurry of activity in applying deep learning techniques to remote sensing data. However, due to challenges that are unique to environmental applications, off-the-shelf deep learning techniques developed for related applications such as computer vision have limited utility. This research will address these challenges and advance the state-of-the-art in deep learning techniques by developing techniques that can make use of the underlying complex, multi-scale, spatio-temporal earth system processes for global change monitoring. Methods developed in this project are expected to also have an impact across many disciplines that deal with large-scale time-varying data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815047","CSR: Small:Collaborative Research: Decentralized Real-Time Machine Learning Systems on Near-User Edge Devices","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2018","05/08/2020","Hyesoon Kim","GA","Georgia Tech Research Corporation","Standard Grant","Erik Brunvand","09/30/2021","$282,000.00","","hyesoon@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1714, 7354","7354, 7923, 9251","$0.00","The ever-increasing number of Internet of Things (IoT) devices generate large quantities of raw data that need to be processed and analyzed in real time.  Since conducting computationally expensive tasks, such as computer vision and natural language processing, is often a challenge for IoT devices, most of their computations are currently offloaded to cloud servers. However, this offloading leads to an increased risk for privacy as well as a dependency on network connectivity. To solve this challenge, the project utilizes the distributed computing power of already connected IoT devices to perform high computing power applications in real time.<br/><br/>The project is composed of three tasks. First is the development of distributed machine learning (ML) systems for multiple IoT devices.  The project will involve studying how to communicate between nodes with reliable connections and how to dynamically change the job of each node at run-time with little overhead. Second is the development of optimal task assignment and scheduling algorithms. Here, a machine learning approach will be used to generate a recognition model architecture optimal for each distributed system configuration. Third is the development of low-resolution deep neural network (DNN) systems to utilize low-power computing nodes. The development of these DNN systems will involve identifying multiple low-resolution filters that are optimal for varying configurations.<br/><br/>The proposed technical work will advance the state of the art in implementation of parallel and decentralized DNN systems, thereby benefiting all scientific fields of endeavor that rely on computing. The decentralized DNN system will offer new opportunities in power constrained mobile platforms for applications including surveillance and automotive. The research results will lead to new materials/courses for computer architecture and systems. The proposed infrastructure will also be used to guide undergraduate students' research activities. <br/><br/>The software infrastructure will be maintained as an open source project, which can be found at https://github.com/parallel-ml.  It will be updated periodically as new outcomes become available. The results will be published in conferences, journals and technical reports.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1754284","Computational approaches to human spoken word recognition","BCS","Perception, Action & Cognition","03/15/2018","09/18/2019","James Magnuson","CT","University of Connecticut","Continuing Grant","Betty Tuller","02/28/2021","$654,529.00","Jay Rueckl","james.magnuson@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","SBE","7252","7252, 9251","$0.00","This project addresses one of the grand challenges facing cognitive science -- how humans understand speech. People recognize words far more easily than even the best computer speech recognition systems, even though the actual sounds we hear as consonants and vowels vary greatly depending on context (what sounds come before or after), who is talking, and the setting (a quiet room versus a crowded airport). Most current models of speech recognition cannot handle the huge variability in real speech because they do not operate on the actual speech signal. Also, they do not learn, so they cannot model how people acquire language. This project addresses these challenges by comparing current models of speech recognition to each other and to human capabilities, with the goal of understanding how human speech processing is so robust and flexible.  In addition, simplified ""deep learning"" networks will be developed and evaluated as models of human speech recognition. Deep learning networks are similar to cognitive models in that they learn abstract representations of the data, not task-specific rules or algorithms. These networks have been used to create accurate commercial speech recognition systems. By comparing them to human performance, the investigators may provide new insights into why human speech recognition is so robust. The results of this project will have technical implications (better understanding of human flexibility may aid in improving computer speech recognition) and health implications (better understanding of human speech recognition will aid in developing better interventions for language disorders). The project will also support the training of a postdoctoral researcher and a PhD student, both of whom will develop skills that can be used to contribute to research and development in academia or industry. <br/><br/>This project focuses on the development of a ""shallow deep network"" model called ""DeepListener"" that will be compared with the behavior of human listeners. A close match in the millisecond-level behavior of the network (for example, in which words are temporarily confusable with each other) and human performance suggests that human speech processing may emerge from similar principles as those in the model. In preliminary work, DeepListener learned to recognize 93% of 2000 real words (200 words produced by 10 talkers). DeepListener will be evaluated by detailed comparison to standard neural network models of cognitive theories and to human performance. The ways in which DeepListener is similar and dissimilar to human performance and competing models will help to advance scientific theories of human speech recognition. This project will follow emerging standards for open science: experiments will be pre-registered and data and computer code will be made freely and publicly available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838193","BIGDATA: IA: Multiplatform, Multilingual, and Multimodal Tools for Analyzing Public Communication in over 100 Languages","IIS","HCC-Human-Centered Computing, Big Data Science &Engineering, Data Infrastructure","09/15/2018","09/07/2018","Margrit Betke","MA","Trustees of Boston University","Standard Grant","Sara Kiesler","08/31/2022","$1,000,000.00","Prakash Ishwar, Lei Guo, Derry Wijaya","betke@cs.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7367, 8083, 8294","062Z, 7433, 8083","$0.00","In today's information age, understanding public communication flows around the world is important to United States policy and diplomacy. The challenge for research is to collect, analyze, and interpret information as it is presented worldwide, creating big data that is flowing at high velocity, in large volumes, with much variety in perspective, language, and platforms. Analytic methods for studying textual and visual public information worldwide are limited by language hurdles. This project aims to solve data analytics problems in the domain of international public information flows by developing methods that effectively leverage natural language processing, machine learning, and computer vision tools.<br/>    <br/>This research will involve collecting multilingual, multiplatform, and multimodal corpora of text and images originating in the U.S. and reported worldwide, developing an interactive budget-efficient methodology for annotation by experts and crowdworkers that scales effectively, using machine learning and deep learning techniques that exploit multilingual and multimodal representations to develop data analytics tools for entity and frame recognition, sentiment analysis of entities and frames, and curating balanced real-time content collections for many languages. This project is expected to generate analytical tools for social scientists and others to better examine the international flow of public communications. The annotated data will provide training and benchmark datasets that can propel research in entity and frame recognition, sentiment analysis, and other related natural language processing tasks for many languages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718970","CCF-BSF:  AF:  Small:  Convex and Non-Convex Distributed Learning","CCF","Algorithmic Foundations","01/01/2018","07/15/2019","Nathan Srebro","IL","Toyota Technological Institute at Chicago","Standard Grant","A. Funda Ergun","12/31/2020","$257,978.00","","nati@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","014Z, 7923, 7926, 7934, 9251","$0.00","Machine learning is an increasingly important approach in tackling many difficult scientific, engineering and artificial intelligence tasks, ranging from machine translation and speech recognition, through control of self driving cars, to protein structure prediction and drug design.  The core idea of machine learning is to use examples and data to automatically train a system to perform some task.  Accordingly, the success of machine learning is tied to availability of large amounts of training data and our ability to process it.  Much of the recent success of machine learning is fueled by the large amounts of data (text, images, videos, etc) that can now be collected.  But all this data also needs to be processed and learned from---indeed this data flood has shifted the bottleneck, to a large extent, from availability of data to our ability to process it.  In particular, the amounts of data involved can no longer be stored and handled on single computers.  Consequently, distributed machine learning, where data is processed and learned from on many computers that communicate with each other, is a crucial element of modern large scale machine learning.<br/><br/>The goal of this project is to provide a rigorous framework for studying distributed machine learning, and through it develop efficient methods for distributed learning and a theoretical understanding of the benefits of these methods, as well as the inherent limitations of distributed learning.  A central component in the PIs' approach is to model distributed learning as a stochastic optimization problem, where different machines receive samples drawn from the same source distribution, thus allowing methods and analysis that specifically leverage the relatedness between data on different machines.  This is crucial for studying how availability of multiple computers can aid in reducing the computational cost of learning.  Furthermore, the project also encompasses the more challenging case where there are significant differences between the nature of the data on different machines (for instance, when different machines serve different geographical regions, or when each machine is a personal device, collecting data from a single user).  In such a situation, the proposed approach to be studied is to integrate distributed learning with personalization or adaptation, which the PIs argue can not only improve learning performance, but also better leverage distributed computation.<br/><br/>This is an international collaboration, made possible through joint funding with the US-Israel Binational Science Foundation (BSF).  The project brings together two PIs that have worked together extensively on related topics in machine learning and optimization."
"1831481","RIDIR: Collaborative Research: Integrated Communication Database and Computational Tools","SMA","Secure &Trustworthy Cyberspace","09/15/2018","09/05/2018","Jennifer Pan","CA","Stanford University","Standard Grant","Sara Kiesler","08/31/2021","$209,486.00","","jp1@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","SBE","8060","025Z, 026Z, 062Z, 065Z, 7433, 7434, 8083, 9178, 9179","$0.00","This project will develop an integrated research framework for sociotechnical cybersecurity research and broader investigations of information provenance by behavioral, information, and computer scientists. Currently, researchers are mainly limited to natural language processing of large bodies of online text. This project will make it possible to analyze larger information worlds, including those from such countries as China, and the flow of information, including video and audio information, in newspapers, TV, and online sources. The project addresses a core goal of cybersecurity research, which is to understand the provenance, flow, and termination of information warfare, and censorship.<br/><br/>The project is aimed at constructing an integrated and unified information database that combines mass communication data from TV and print sources from six locations, with data from two popular online communication platforms. The project will generate a variety of metadata and time series data on topics, actors, events, and sentiments presented in communications by automated multimodal content analysis using text, image, video, and audio. Variables will be linked to identify trajectories of information flow between communication channels through multiple platforms. It will develop a new class of computational models and algorithms that can automatically analyze both verbal and nonverbal communications data by machine learning, computer vision, deep learning, and natural language processing. This project will allow researchers across the computational and social sciences to access the metadata and time series data through a search interface for qualitative research, a statistical package for quantitative research, and various visualization tools. This project will therefore link previously untapped data sources using cutting-edge computational methods to enable scholars to conduct systematic research on large-scale patterns in the emerging information and communication ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755876","CRII: SHF: Enabling Neuroevolution in Hardware","CCF","Software & Hardware Foundation","01/15/2018","01/11/2018","Tushar Krishna","GA","Georgia Tech Research Corporation","Standard Grant","Sankar Basu","12/31/2019","$175,000.00","","tushar@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798","7945, 8228","$0.00","Over the past few years, machine learning algorithms, especially neural networks (NN) have seen a surge of popularity owing to their potential in solving a wide variety of complex problems across image classification and speech recognition. Unfortunately, in order to be effective, NNs need to have the appropriate topology (connections between neurons) for the task at hand and have the right weights on the connections. This is known as supervised learning and requires training the NN by running it through terabytes to petabytes of data. This form of machine learning is infeasible for the emerging domain of autonomous systems (robots/drones/cars) which will often operate in environments where the right topology for the task may be unknown or keep changing, and robust training data is not available. Autonomous systems need the ability to mirror human-like learning, where we are continuously learning, and often from experiences rather than being explicitly trained. This is known as reinforcement learning (RL). The goal of this project will be on enabling RL in energy-constrained autonomous devices. If successful, this research will enable mass proliferation of automated robots or drones to assist human society. The learnings will also be used to develop new courses on cross-layer support for machine learning. <br/><br/>The focus of the research will be on neuroevolution (NE), a class of RL algorithms that evolve NN topologies and weights using evolutionary algorithms.  The idea is to run multiple ""parent"" NNs in parallel, have the environment provide a reward (score) to the actions of all NNs, and create a population of new ""child"" NNs that preserve those nodes and connections from the parents that lead to actions producing the maximum reward. Running NE algorithms over multiple iterations has been shown to evolve complex behaviors in NNs. Unfortunately, NEs are computationally very expensive and have required large scale compute clusters running for hours before converging. A characterization of the computation and memory behavior of NE algorithms will be performed, and opportunities to massively parallelize these algorithms across genes (i.e., nodes and connections in the NN) will be explored. The evolutionary learning steps of crossover and mutation will be performed over specialized hardware engines, and a low-power architectural platform running NE algorithms at the edge will be demonstrated. Furthermore, the proposed research will serve as the foundation for further research in fast and energy-efficient RL algorithms to help realize general-purpose artificial intelligence."
"1831848","RIDIR: Collaborative Research: Integrated Communication Database and Computational Tools","SMA","Secure &Trustworthy Cyberspace, Data Infrastructure","09/15/2018","09/05/2018","Jungseock Joo","CA","University of California-Los Angeles","Standard Grant","Sara Kiesler","08/31/2021","$944,182.00","Tim Groeling, Francis Steen, Zachary Steinert-Threlkeld","jjoo@comm.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","8060, 8294","025Z, 026Z, 062Z, 065Z, 7433, 7434, 8083, 9178, 9179","$0.00","This project will develop an integrated research framework for sociotechnical cybersecurity research and broader investigations of information provenance by behavioral, information, and computer scientists. Currently, researchers are mainly limited to natural language processing of large bodies of online text. This project will make it possible to analyze larger information worlds, including those from such countries as China, and the flow of information, including video and audio information, in newspapers, TV, and online sources. The project addresses a core goal of cybersecurity research, which is to understand the provenance, flow, and termination of information warfare, and censorship. <br/><br/>The project is aimed at constructing an integrated and unified information database that combines mass communication data from TV and print sources from six locations, with data from two popular online communication platforms. The project will generate a variety of metadata and time series data on topics, actors, events, and sentiments presented in communications by automated multimodal content analysis using text, image, video, and audio. Variables will be linked to identify trajectories of information flow between communication channels through multiple platforms. It will develop a new class of computational models and algorithms that can automatically analyze both verbal and nonverbal communications data by machine learning, computer vision, deep learning, and natural language processing. This project will allow researchers across the computational and social sciences to access the metadata and time series data through a search interface for qualitative research, a statistical package for quantitative research, and various visualization tools. This project will therefore link previously untapped data sources using cutting-edge computational methods to enable scholars to conduct systematic research on large-scale patterns in the emerging information and communication ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832737","Workshop: Learning Hidden Linguistic Structure; January 3-7, 2019, New York New York","BCS","Linguistics","09/01/2018","08/02/2018","Gaja Jarosz","MA","University of Massachusetts Amherst","Standard Grant","Tyler Kendall","02/29/2020","$17,922.00","","jarosz@linguist.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","SBE","1311","1311, 7556","$0.00","This workshop will bring together leading language researchers from two largely disjoint scientific communities: Linguistics and Natural Language Processing (NLP). The meeting will create a forum for intellectual exchange on linguistically oriented computational modeling and facilitate building of productive ties between the linguistics and NLP communities. To maximize the accessibility of the workshop to the broader linguistics community and to facilitate cross-disciplinary exchange, the workshop will take place in conjunction with the annual meeting of the Linguistic Society of America and will feature prominent invited speakers and panelists from the NLP community. Cross-disciplinary research that integrates principles of linguistics and findings from human language learning with cutting-edge computational methods from statistical machine learning promises to lead to scientific breakthroughs of mutual benefit to both research communities and to society more generally. Application of computational and mathematical modeling methods from NLP promises to enrich scientific understanding of the human language learning process and how it depends on the information present in children and adults' linguistic environments. The integration also has the potential to lead to new and improved language technologies, such as machine translation and automatic speech recognition systems that are playing an increasingly important role in modern society by facilitating communication between speakers of distinct languages, by increasing multilingual access to information and educational resources on the web, and by producing new tools and resources for people with speech, hearing, and language disabilities. In addition, the accessibility of the workshop to the broader linguistics community creates a pathway into STEM for women in linguistics that would not otherwise exist, providing educational, training and research connections with the NLP community.<br/><br/>The theme for the workshop is ""learning hidden linguistic structure"", which is a topic chosen specifically because there are strong but largely separate research traditions approaching this fundamental learning problem in the two communities. Hidden structure is a common component of linguistic theories and of theories of human language learning, but much is still unknown about how such representations are learned by children from their linguistic input. On the other hand, NLP research has produced a wealth of computational techniques for modeling hidden structure and its learning, but these models rarely incorporate linguistic principles. Integrative research has the potential to lead to improvements in language technologies, especially in low-resource settings where success depends most on the capacity to generalize in linguistically appropriate ways from limited data. It also has potential to lead to scientific breakthroughs in understanding the sorts of computations and hidden representations that underlie human language learning. The workshop will host invited speakers who will present on the theme of ""learning hidden linguistic structure"" from interdisciplinary perspectives. To further increase ties between the linguistics and NLP communities, there will also be a special session with invited panelists on the topic of ""What should linguists know about NLP? "". Finally, to facilitate the training of linguists in computational methods, the workshop will host two entry-level tutorials on machine learning methods in NLP.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1759934","ABI Innovation:  Deep learning methods for protein bioinformatics","DBI","ADVANCES IN BIO INFORMATICS","07/01/2018","03/12/2018","Jianlin Cheng","MO","University of Missouri-Columbia","Standard Grant","Peter McCartney","06/30/2021","$624,247.00","","chengji@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","BIO","1165","9150","$0.00","Protein sequence is a language of a living system, which encodes protein structure and function critical for the survival of any organism. Therefore, understanding how protein sequence describes function and structure is a fundamental problem in biological research. Yet, the traditional interpretation of protein sequences is 3ased on either manual identification of sub-sequence patterns or some arbitrary dissection of a sequence into subsequences of fixed size; neither approach can accurately recognize all of the semantic components in a protein sequence that are relevant to its structure and function. In this project, powerful artificial intelligence methods, based on deep learning, will be designed such that they can automatically map protein sequences into high-level semantic features that are meaningful when related to protein structure and function. This will not only improve the accuracy of predicting protein structural and functional properties, but also provide a new way of representing and interpreting proteins biological function, transforming how protein data are interpreted. The impact of the basic research will be broadened through open source software dissemination to other researchers, seminars on deep learning and bioinformatics, student training, involvement of minority and female students, publications, presentations, workshops, and outreach activities for high school students, as well as thoughtfully crafted communication with the Missouri state legislature, and other members of the general public.<br/><br/>During the research, novel deep one-dimensional (1D), 2D, and 3D convolutional neural networks will be developed to translate protein sequences or structures of arbitrary size into high-level features under the guidance of improving the prediction of multiple residue-wise local structural/functional properties (secondary structures, solvent accessibility, torsion angle, disorder, contact map, disulfide bonds, beta-sheet pairings, and protein functional sites) as well as global properties such as folds. The 1D convolutional neural network for interpreting protein sequence data will also be supplemented by the long- and short-term memory networks. The comprehensive deep learning models will be trained by innovative multi-task learning and transfer learning to enhance prediction performance. The 1D, 2D and 3D convolutional networks will be further integrated to improve the accuracy of analyzing protein sequence, structure and function. The 1D and 3D convolutional neural networks are completely original, and the new 2D convolutional architecture is more comprehensive and versatile than existing approaches. In addition to advancing the classic protein prediction tasks through the novel deep learning architectures, the hidden features automatically extracted by the deep learning models will provide a new semantic representation of proteins, which will likely transform various protein bioinformatics tasks such as classification, clustering, comparison, and ranking. The URL of this project is: http://calla.rnet.missouri.edu/cheng/nsf_deepbioinfo.html .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816039","RI: Small: A Cognitive Framework for Technical, Hard and Explainable Question Answering (THE-QA) with respect to Combined Textual and Visual Inputs","IIS","Robust Intelligence","08/01/2018","03/07/2019","Chitta Baral","AZ","Arizona State University","Standard Grant","Roger Mailler","07/31/2021","$515,999.00","Yezhou Yang","chitta@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7495","7495, 7923, 9251","$0.00","Understanding of visual and textual inputs are important aspects of Artificial Intelligence systems. Often such inputs are presented together to instruct and explain.  As examples, an intelligent robot might learn about its tasks and environment by observing both language and gesture; and an intelligent system addressing scientific questions must interpret figures and diagrams along with text. While there has been a lot of research concerning visual understanding and textual understanding in isolation, there has been very little research that addresses them jointly. This project is developing a framework for answering hard questions about combined visual and textual inputs, and providing supporting explanations. By developing a system that integrates visual and linguistic information for this task, the project could provide the basis for automated tutoring systems in K-12 education, and interpretable interfaces for the workers operating intelligent machines. <br/><br/>The project will employ an integrated approach of deep model-based visual recognition and natural language processing, and knowledge representation and reasoning to develop a question answering engine and its components. It will create a challenge corpus that has visual and textual inputs and questions about those inputs given in natural language. It will provide a baseline for semantic image and text parsing and reasoning-based question answering systems. It will develop semantic parsing of non-continuous text items, such as figures, diagrams, and graphs. It will enhance semantic parsing to various formats of natural language text and questions. It will develop methods to acquire knowledge and reasoning with them for answering questions and providing explanations to the answers. Together these contributions of the project will advance Artificial General Intelligence and allow future service robots and personal mobile applications to understand combined visual and textual inputs. The findings from this project will advance the development of knowledge-driven, reasoning-based question answering by filling the current gap on how to efficiently conduct explainable probabilistic reasoning over deep models.  This helps to overcome the fragility of the trained visual and textual understanding models. It will also uncover the intrinsic connections between deep model-based vision and language understanding algorithms and probabilistic knowledge representation and reasoning by exploring a joint solution for answering the hard questions. In general, this project may result in advances in multiple sub-fields of Artificial Intelligence; namely, computer vision, natural language processing, and question answering; and may impact others such as robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747783","Phase I IUCRC University of Florida:  Center for Big Learning","CNS","IUCRC-Indust-Univ Coop Res Ctr, , , ","02/01/2018","06/11/2020","Xiaolin Li","FL","University of Florida","Continuing Grant","Behrooz Shirazi","01/31/2023","$600,000.00","Dapeng Wu, Jose Principe","andyli@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","5761, R235, S215, U183","5761, 8237","$0.00","This project establishes the NSF Industry/University Collaborative Research Center for Big Learning (CBL). The vision is to create intelligence towards intelligence-driven society. Through catalyzing the fusion of diverse expertise from the consortium of faculty members, students, industry partners, and federal agencies, CBL seeks to create state-of-the-art deep learning methodologies and technologies and enable intelligent applications, transforming broad domains, such as business, healthcare, Internet-of-Things, and cybersecurity. This timely initiative creates a unique platform for empowering our next-generation talents with cutting-edge technologies of societal relevance and significance. <br/><br/>This project establishes the NSF Industry/University Collaborative Research Center for Big Learning (CBL) at University of Florida (UF). With substantial breakthroughs in multiple modalities of challenges, such as computer vision, speech recognition, and natural language understanding, the renaissance of machine intelligence is dawning. The CBL vision is to create intelligence towards intelligence-driven society. The mission is to pioneer novel deep learning algorithms, systems, and applications through unified and coordinated efforts in the CBL consortium. The UF Site will focus on intelligent platforms and applications and closely collaborate with other sites on deep learning algorithms, systems, and applications.<br/><br/>The CBL will have broad transformative impacts in technologies, education, and society. CBL aims to create pioneering research and applications to address a broad spectrum of real-world challenges, making significant contributions and impacts to the deep learning community. The discoveries from CBL will make significant contributions to promote products and services of industry in general and CBL industry partners in particular. As the magnet of deep learning research and applications, CBL offers an ideal platform to nurture next-generation talents through world-class mentors from both academia and industry, disseminates the cutting-edge technologies, and facilitates industry/university collaboration and technology transfer.<br/><br/>The center repository will be hosted at http://nsfcbl.org. The data, code, documents will be well organized and maintained on the CBL servers for the duration of the center for more than five years and beyond. The internal code repository will be managed by GitLab. After the software packages are well documented and tested, they will be released and managed by popular public code hosting services, such as GitHub and Bitbucket."
"1740762","Collaborative Research:  TRIPODS Institute for Optimization and Learning","CCF","TRIPODS Transdisciplinary Rese","01/01/2018","08/26/2018","Francesco Orabona","NY","SUNY at Stony Brook","Continuing grant","Tracy Kimbrel","05/31/2019","$189,872.00","","fo@bu.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","041Y","047Z, 062Z","$0.00","This Phase I project forms an NSF TRIPODS Institute, based at Lehigh University and in collaboration with Stony Brook and Northwestern Universities, with a focus on new advances in tools for machine learning applications.  A critical component for machine learning is mathematical optimization, where one uses historical data to train tools for making future predictions and decisions.  Traditionally, optimization techniques for machine learning have focused on simplified models and algorithms.  However, recent revolutionary leaps in the successes of machine learning tools---e.g., for image and speech recognition---have in many cases been made possible by a shift toward using more complicated techniques, often involving deep neural networks.  Continued advances in the use of such techniques require combined efforts between statisticians, computer scientists, and applied mathematicians to develop more sophisticated models and algorithms along with more comprehensive theoretical guarantees that support their use. In addition to its research goals, the institute trains Ph.D. students and postdoctoral fellows in statistics, computer science, and applied mathematics,  and hosts interdisciplinary workshops and Winter/Summer schools.  <br/><br/>The research efforts in Phase I are on the analysis of nonconvex machine learning models, the design of optimization algorithms for training them, and on the development of nonparametric models and associated algorithms.  The focus is on deep neural networks (DNNs), mostly in general, but also with respect to specific architectures of interest.  The institute's research efforts emphasize the need to develop connections between state-of-the-art approaches for training DNNs and statistical performance guarantees (e.g., on generalization errors), which are currently not well understood. Optimization algorithms development centers on second-order-derivative-type techniques, including (Hessian-free) Newton, quasi-Newton, Gauss-Newton, and their limited memory variants.  Recent advances have been made in the design of such methods; the PIs' work builds upon these efforts with their broad expertise in the design and implementation (including in parallel and distributed computing environments) of such methods.  The development of nonparametric models promises to free machine learning approaches from restrictions imposed by large numbers of user-defined parameters (e.g., defining a network structure or learning rate of an optimization algorithm).  Such models could lead to great advances in machine learning, and the institute's work in this area also draws on the PIs expertise in derivative-free optimization methods, which are needed for training in nonparametric settings.<br/><br/>In this TRIPODS institute, the PIs approach all of these research directions with a unified perspective in the three disciplines of statistics, computer science, and applied mathematics.  Indeed, as machine learning draws so heavily from these areas, future progress requires close collaborations between optimization experts, learning theorists, and statisticians---communities of researchers that, as yet, have tended to operate separately with differing terminology and publication venues.  With an emphasis on deep learning, this institute aims to foster intercollegiate and interdisciplinary collaborations that overcome these hindrances.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740796","Collaborative Research:  TRIPODS Institute for Optimization and Learning","CCF","TRIPODS Transdisciplinary Rese","01/01/2018","09/04/2019","Frank Curtis","PA","Lehigh University","Continuing Grant","Tracy Kimbrel","12/31/2020","$895,670.00","Katya Scheinberg, Frank Curtis, Martin Takac","fec309@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","041Y","047Z, 062Z, 9102","$0.00","This Phase I project forms an NSF TRIPODS Institute, based at Lehigh University and in collaboration with Stony Brook and Northwestern Universities, with a focus on new advances in tools for machine learning applications.  A critical component for machine learning is mathematical optimization, where one uses historical data to train tools for making future predictions and decisions.  Traditionally, optimization techniques for machine learning have focused on simplified models and algorithms.  However, recent revolutionary leaps in the successes of machine learning tools---e.g., for image and speech recognition---have in many cases been made possible by a shift toward using more complicated techniques, often involving deep neural networks.  Continued advances in the use of such techniques require combined efforts between statisticians, computer scientists, and applied mathematicians to develop more sophisticated models and algorithms along with more comprehensive theoretical guarantees that support their use. In addition to its research goals, the institute trains Ph.D. students and postdoctoral fellows in statistics, computer science, and applied mathematics,  and hosts interdisciplinary workshops and Winter/Summer schools.  <br/><br/>The research efforts in Phase I are on the analysis of nonconvex machine learning models, the design of optimization algorithms for training them, and on the development of nonparametric models and associated algorithms.  The focus is on deep neural networks (DNNs), mostly in general, but also with respect to specific architectures of interest.  The institute's research efforts emphasize the need to develop connections between state-of-the-art approaches for training DNNs and statistical performance guarantees (e.g., on generalization errors), which are currently not well understood. Optimization algorithms development centers on second-order-derivative-type techniques, including (Hessian-free) Newton, quasi-Newton, Gauss-Newton, and their limited memory variants.  Recent advances have been made in the design of such methods; the PIs' work builds upon these efforts with their broad expertise in the design and implementation (including in parallel and distributed computing environments) of such methods.  The development of nonparametric models promises to free machine learning approaches from restrictions imposed by large numbers of user-defined parameters (e.g., defining a network structure or learning rate of an optimization algorithm).  Such models could lead to great advances in machine learning, and the institute's work in this area also draws on the PIs expertise in derivative-free optimization methods, which are needed for training in nonparametric settings.<br/><br/>In this TRIPODS institute, the PIs approach all of these research directions with a unified perspective in the three disciplines of statistics, computer science, and applied mathematics.  Indeed, as machine learning draws so heavily from these areas, future progress requires close collaborations between optimization experts, learning theorists, and statisticians---communities of researchers that, as yet, have tended to operate separately with differing terminology and publication venues.  With an emphasis on deep learning, this institute aims to foster intercollegiate and interdisciplinary collaborations that overcome these hindrances.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740735","Collaborative Research: TRIPODS Institute for Optimization and Learning","CCF","TRIPODS Transdisciplinary Rese","01/01/2018","09/06/2019","Han Liu","IL","Northwestern University","Continuing Grant","Tracy Kimbrel","12/31/2020","$300,000.00","","hanliu@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","041Y","047Z, 062Z","$0.00","This Phase I project forms an NSF TRIPODS Institute, based at Lehigh University and in collaboration with Stony Brook and Northwestern Universities, with a focus on new advances in tools for machine learning applications.  A critical component for machine learning is mathematical optimization, where one uses historical data to train tools for making future predictions and decisions.  Traditionally, optimization techniques for machine learning have focused on simplified models and algorithms.  However, recent revolutionary leaps in the successes of machine learning tools---e.g., for image and speech recognition---have in many cases been made possible by a shift toward using more complicated techniques, often involving deep neural networks.  Continued advances in the use of such techniques require combined efforts between statisticians, computer scientists, and applied mathematicians to develop more sophisticated models and algorithms along with more comprehensive theoretical guarantees that support their use. In addition to its research goals, the institute trains Ph.D. students and postdoctoral fellows in statistics, computer science, and applied mathematics,  and hosts interdisciplinary workshops and Winter/Summer schools.  <br/><br/>The research efforts in Phase I are on the analysis of nonconvex machine learning models, the design of optimization algorithms for training them, and on the development of nonparametric models and associated algorithms.  The focus is on deep neural networks (DNNs), mostly in general, but also with respect to specific architectures of interest.  The institute's research efforts emphasize the need to develop connections between state-of-the-art approaches for training DNNs and statistical performance guarantees (e.g., on generalization errors), which are currently not well understood. Optimization algorithms development centers on second-order-derivative-type techniques, including (Hessian-free) Newton, quasi-Newton, Gauss-Newton, and their limited memory variants.  Recent advances have been made in the design of such methods; the PIs' work builds upon these efforts with their broad expertise in the design and implementation (including in parallel and distributed computing environments) of such methods.  The development of nonparametric models promises to free machine learning approaches from restrictions imposed by large numbers of user-defined parameters (e.g., defining a network structure or learning rate of an optimization algorithm).  Such models could lead to great advances in machine learning, and the institute's work in this area also draws on the PIs expertise in derivative-free optimization methods, which are needed for training in nonparametric settings.<br/><br/>In this TRIPODS institute, the PIs approach all of these research directions with a unified perspective in the three disciplines of statistics, computer science, and applied mathematics.  Indeed, as machine learning draws so heavily from these areas, future progress requires close collaborations between optimization experts, learning theorists, and statisticians---communities of researchers that, as yet, have tended to operate separately with differing terminology and publication venues.  With an emphasis on deep learning, this institute aims to foster intercollegiate and interdisciplinary collaborations that overcome these hindrances.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813049","AF: Small: Robust and Secure Learning","CCF","Algorithmic Foundations","10/01/2018","05/24/2018","Gregory Valiant","CA","Stanford University","Standard Grant","A. Funda Ergun","09/30/2021","$500,000.00","","gvaliant@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7796","7923, 7926","$0.00","Machine learning (ML) systems play an increasingly central role in society--from ubiquitous speech recognition systems, to navigation systems, product recommendation systems, and deployed learning systems across manufacturing, industry, and healthcare.  The near future, with complex computer vision systems, self-driving cars, and ML driven medical care and patient monitoring, promises a nearly pervasive presence of ML systems in our society.  Despite promising performance in idealized settings, current ML systems are often brittle--they are sensitive to slight changes in the input data, and often have weaknesses that can be easily exploited by a malicious adversary.  Resolving these current shortcomings is a necessary step in ensuring the stability, safety, and security of a society that relies heavily on machine learning.  The central goal of this project is to develop learning algorithms that are robust, and secure.  These go beyond the traditional goal of developing learning algorithms that achieve high accuracy, and address the broad need for reliability and safety in critical deployed systems.  As an extension of the research component of the project, the investigator will continue education and outreach efforts.  These include disseminating the research publications and code produced by this project, continuing to develop new courses and teaching materials on data-centric algorithms, machine learning, and related topics, and organizing a semi-annual forum for the exchange of ideas between industry and academia.    <br/><br/>The research core of this project addresses the lack of robustness of current learning and optimization algorithms. This lack of robustness takes the following two distinct forms. First, current algorithms are sensitive to changes in even a very small portion of the data-set on which they are trained.  Second, even when trained on legitimate data, the learned models are often susceptible to ""adversarial examples"" in the sense that for the vast majority of data points--even data points in the training set--a small adversarial perturbation of the data point in question will result in the model outputting a completely different label.  The presence of these two types of fragility in current learning systems raises the possibility of vulnerabilities to two new sorts of security threats: 1) the threat that a portion of the training data is either extremely biased and unreliable, or worse--that it has been generated by an adversary whose goal is to mislead the machine learning system, and 2) the threat that deployed machine learning systems can be tricked via minute but carefully generated adversarial modifications in their test points--modifications that are essentially invisible to humans. The project seeks to address these two critical weaknesses of current systems, by : 1) developing new algorithms that are robust to the presence of significant fractions of arbitrary -- including adversarial -- data, which can be applied to a number of fundamental estimation, machine learning, and optimization tasks, and 2) developing a rigorous understanding of why certain training algorithms yield models that are inherently vulnerable to adversarial examples, and develop tools for reducing this vulnerability.  Additionally, this project investigates the computational, and information theoretic aspects of robust and secure learning, including developing an understanding of any potential trade-offs, for example between the amount of training data and computation time, and robustness or security of the resulting trained model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764048","RI:Medium:Collaborative Research: Developing a Uniform Meaning Representation for Natural Language Processing","IIS","Robust Intelligence","08/01/2018","07/23/2018","Martha Palmer","CO","University of Colorado at Boulder","Standard Grant","Tatiana Korelsky","07/31/2021","$399,894.00","J. Andrew Cowell, James Martin","mpalmer@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7495","7495, 7924","$0.00","The use of intelligent agents that can communicate with us in human language has become an essential part of our daily lives.  Today's intelligent agents can respond appropriately to many things we say or text to them, but they cannot yet communicate fully like humans. They lack our general ability to arrive quickly at accurate and relevant interpretations of what others communicate to us and to form appropriate responses, particularly in sustained interactions.  The typical way we teach a machine to acquire such ability is to provide it with approximations of the meanings of utterances in the contexts in which they have occurred in the past.  Over the years these approximations have become increasingly rich and detailed, enabling ever more sophisticated systems for interacting with computers using natural language, such as searching for information, getting up-to-date recommendations for products and services, and translating foreign languages.  The goal of this project is to bring together linguists and computer scientists to jointly develop a practical meaning representation formalism based on these rich approximations that can be applied to a much more diverse set of languages.   This will allow us to use machine learning to develop techniques to automatically translate human utterances into our meaning formalism. In turn, this will enable intelligent agents to acquire more advanced communication capabilities, and for a wider range of languages.  The languages considered for the project include those spoken by large populations such as English, Chinese and Arabic, as well as native tongues of smaller groups such as Norwegian, and Arapaho and Kukama-Kukamira, two indigenous languages of the Americas.  As such, this project will help bring modern technology to smaller groups so that all people can benefit equally from technological advancement.  The project will also contribute to the development of the US workforce by training a new generation of researchers on cutting-edge technologies in artificial intelligence.  <br/><br/>This project brings together an interdisciplinary team of linguists and computer scientists from three institutions to jointly develop a Uniform Meaning Representation (UMR). UMR is a practical, formal, computationally tractable, and cross-linguistically valid meaning representation of natural language that can impact a wide range of downstream applications requiring deep natural language understanding (NLU).  UMR will extend existing meaning representations to include quantifier types and relations, modality, negation, tense and aspect, and be tested on a typologically diverse set of languages.  Methods and techniques for UMR annotation, parsing and generation, and evaluation will be uniform across languages.  The project will also develop novel algorithms and models for UMR-based broad-coverage and general-purpose multilingual semantic parsers.  Students participating in the project will receive training in the full cycle of conceptualizing, producing, processing, and consuming meaning representations at the sites of participating institutions.  This project will help to build a community of NLP researchers that will contribute to the development of UMR-based data and tools and advance the state of the art in Natural Language Processing (NLP) in particular, and Artificial Intelligence (AI) in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763926","RI:Medium:Collaborative Research:Developing a uniform meaning representation for natural language processing","IIS","Robust Intelligence","08/01/2018","07/23/2018","Nianwen Xue","MA","Brandeis University","Standard Grant","Tatiana Korelsky","07/31/2021","$399,237.00","James Pustejovsky","xuen@cs.brandeis.edu","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","7495","7495, 7924","$0.00","The use of intelligent agents that can communicate with us in human language has become an essential part of our daily lives.  Today's intelligent agents can respond appropriately to many things we say or text to them, but they cannot yet communicate fully like humans. They lack our general ability to arrive quickly at accurate and relevant interpretations of what others communicate to us and to form appropriate responses, particularly in sustained interactions.  The typical way we teach a machine to acquire such ability is to provide it with approximations of the meanings of utterances in the contexts in which they have occurred in the past.  Over the years these approximations have become increasingly rich and detailed, enabling ever more sophisticated systems for interacting with computers using natural language, such as searching for information, getting up-to-date recommendations for products and services, and translating foreign languages.  The goal of this project is to bring together linguists and computer scientists to jointly develop a practical meaning representation formalism based on these rich approximations that can be applied to a much more diverse set of languages.   This will allow us to use machine learning to develop techniques to automatically translate human utterances into our meaning formalism. In turn, this will enable intelligent agents to acquire more advanced communication capabilities, and for a wider range of languages.  The languages considered for the project include those spoken by large populations such as English, Chinese and Arabic, as well as native tongues of smaller groups such as Norwegian, and Arapaho and Kukama-Kukamira, two indigenous languages of the Americas.  As such, this project will help bring modern technology to smaller groups so that all people can benefit equally from technological advancement.  The project will also contribute to the development of the US workforce by training a new generation of researchers on cutting-edge technologies in artificial intelligence.  <br/><br/>This project brings together an interdisciplinary team of linguists and computer scientists from three institutions to jointly develop a Uniform Meaning Representation (UMR). UMR is a practical, formal, computationally tractable, and cross-linguistically valid meaning representation of natural language that can impact a wide range of downstream applications requiring deep natural language understanding (NLU).  UMR will extend existing meaning representations to include quantifier types and relations, modality, negation, tense and aspect, and be tested on a typologically diverse set of languages.  Methods and techniques for UMR annotation, parsing and generation, and evaluation will be uniform across languages.  The project will also develop novel algorithms and models for UMR-based broad-coverage and general-purpose multilingual semantic parsers.  Students participating in the project will receive training in the full cycle of conceptualizing, producing, processing, and consuming meaning representations at the sites of participating institutions.  This project will help to build a community of NLP researchers that will contribute to the development of UMR-based data and tools and advance the state of the art in Natural Language Processing (NLP) in particular, and Artificial Intelligence (AI) in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828181","MRI: Acquisition of Artificial Intelligence & Deep Learning (AIDL) Training and Research Laboratory","CNS","IIS Special Projects","10/01/2018","09/08/2018","Xingquan Zhu","FL","Florida Atlantic University","Standard Grant","Rita Rodriguez","09/30/2021","$652,850.00","Hanqi Zhuang, Taghi Khoshgoftaar, Dimitris Pados, Laurent Cherubin","xzhu3@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","7484","1189","$0.00","Researchers in health, biomedical science, and various engineering fields often do not receive sufficient training in using the most powerful approach to machine learning known to date, an approach called ""deep learning"", for analying their data.  Deep learning is based on simulated artificial neural networks, and for large real-world problems requires access to specialized computer hardward and software.  Such hardware and software platforms, however, are rarely part of the information technology resources available for researchers outside of the field of computer science.  This project will overcome this barrier by procurement and development of a deep learning platform at Florida Atlantic University for such research.  The project provides a training hub for industry and university to work closely on advanced artificial intelligence applications, and in turn might benefit economic growth.<br/><br/>This infrastructure project supports creation of a deep learning platform for health, biomedicine, ocean research, and related domains at Florida Atlantic University.  The platform will be shared across campus to service multiple domains. The project brings about a centralized cross campus interdisciplinary platform and augmented deep learning and related artificial intelligence tools for interdisciplinary research.  The former, jointly managed by the College of Engineering and Computer Science, and the FAU office of Information Technology, enables building upon the experience and frameworks of others that eventually results in shared infrastructure savings. The latter is likely to contribute in building/augmenting AI and DL tool kits for interdisciplinary research, including augmentation of existing common machine learning and deep learning algorithms for domain experts to carry out analysis on their data without requiring intensive programming skills. Augmented AI and DL tools should be particularly useful to ocean engineers and health sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764091","RI: Medium: Collaborative Research: Developing a Uniform Meaning Representation for Natural Language Processing","IIS","Robust Intelligence","08/01/2018","07/23/2018","William Croft","NM","University of New Mexico","Standard Grant","Tatiana Korelsky","07/31/2021","$399,821.00","","wcroft@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7495","7495, 7924, 9150","$0.00","The use of intelligent agents that can communicate with us in human language has become an essential part of our daily lives.  Today's intelligent agents can respond appropriately to many things we say or text to them, but they cannot yet communicate fully like humans. They lack our general ability to arrive quickly at accurate and relevant interpretations of what others communicate to us and to form appropriate responses, particularly in sustained interactions.  The typical way we teach a machine to acquire such ability is to provide it with approximations of the meanings of utterances in the contexts in which they have occurred in the past.  Over the years these approximations have become increasingly rich and detailed, enabling ever more sophisticated systems for interacting with computers using natural language, such as searching for information, getting up-to-date recommendations for products and services, and translating foreign languages.  The goal of this project is to bring together linguists and computer scientists to jointly develop a practical meaning representation formalism based on these rich approximations that can be applied to a much more diverse set of languages.   This will allow us to use machine learning to develop techniques to automatically translate human utterances into our meaning formalism. In turn, this will enable intelligent agents to acquire more advanced communication capabilities, and for a wider range of languages.  The languages considered for the project include those spoken by large populations such as English, Chinese and Arabic, as well as native tongues of smaller groups such as Norwegian, and Arapaho and Kukama-Kukamira, two indigenous languages of the Americas.  As such, this project will help bring modern technology to smaller groups so that all people can benefit equally from technological advancement.  The project will also contribute to the development of the US workforce by training a new generation of researchers on cutting-edge technologies in artificial intelligence.  <br/><br/>This project brings together an interdisciplinary team of linguists and computer scientists from three institutions to jointly develop a Uniform Meaning Representation (UMR). UMR is a practical, formal, computationally tractable, and cross-linguistically valid meaning representation of natural language that can impact a wide range of downstream applications requiring deep natural language understanding (NLU).  UMR will extend existing meaning representations to include quantifier types and relations, modality, negation, tense and aspect, and be tested on a typologically diverse set of languages.  Methods and techniques for UMR annotation, parsing and generation, and evaluation will be uniform across languages.  The project will also develop novel algorithms and models for UMR-based broad-coverage and general-purpose multilingual semantic parsers.  Students participating in the project will receive training in the full cycle of conceptualizing, producing, processing, and consuming meaning representations at the sites of participating institutions.  This project will help to build a community of NLP researchers that will contribute to the development of UMR-based data and tools and advance the state of the art in Natural Language Processing (NLP) in particular, and Artificial Intelligence (AI) in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1903951","CAREER: Scaling-up Resistive Synaptic Arrays for Neuro-inspired Computing","CCF","Software & Hardware Foundation, IntgStrat Undst Neurl&Cogn Sys","07/01/2018","03/18/2020","Shimeng Yu","GA","Georgia Tech Research Corporation","Continuing Grant","Sankar Basu","01/31/2021","$373,825.00","","shimeng.yu@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798, 8624","1045, 7945, 8089, 8091, 8551","$0.00","Neuro-inspired deep learning algorithms have demonstrated their power in executing intelligent tasks such as image and speech recognition. However, training of such deep neural networks requires huge amount of computational resources that are not affordable for mobile applications. Hardware acceleration of deep learning, with orders of magnitude improvement in speed and energy efficiency, remains a grand challenge for the conventional hardware based on silicon CMOS technology and von-Neumann architecture. As the learning algorithms extensively involve matrix operations, neuro-inspired architectures that leverage the distributed computing in the neuron nodes and localized storage in the synaptic networks are very attractive. The ultimate goal of this project is to advance the neuro-inspired computing with emerging nano-device technologies towards a self-learning chip. A chip that learns in real-time and consumes low-power can be placed at frontend sensors, bringing broad benefits for a number of current applications. The PI will establish close collaboration with industry through student internships and technology transfer. The plan for integration of research and education will train students with interdisciplinary skills. The cross-layer nature of this project ranging from semiconductor device, circuit design, electronic design automation, and machine learning is expected to provide an ideal platform for this educational goal.<br/><br/>The technical goal of this project is to overcome the challenges that prevent scaling up of the crossbar array size for neuro-inspired architecture. Resistive devices with continuous multilevel states have been proposed to function as synaptic weights in the crossbar architecture. However, with the increase of the array size, issues associated with device yield, device variability, and array parasitics will arise and may degrade the system performance. The PI plans to tackle these challenges by exploiting hierarchical research efforts from devices, circuits and architectures. The outcome of the research includes device compact model, circuit-level benchmark simulator for estimating the area/latency/power of the crossbar array macro, and architectural tool for efficiently mapping the learning algorithms into the crossbar architecture.  The PI has established a custom fabrication channel for tape-out of resistive devices on top of CMOS peripheral circuits via his collaboration with academic partners. The prototype chip with measured data is expected to make a strong impact on this field, which previously relied on the simulations for predicting large-scale array performance."
"1800435","D3SC: CDS&E: Conformer Toolkit: Generating Accurate Small Molecule Conformer Ensembles","CHE","Chem Thry, Mdls & Cmptnl Mthds","08/15/2018","07/27/2018","Geoffrey Hutchison","PA","University of Pittsburgh","Standard Grant","Walter Ermler","07/31/2021","$411,333.00","David Koes","geoffh@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","MPS","6881","026Z, 062Z, 8084, 9216, 9263","$0.00","Geoffrey Hutchison and David Koes of the University of Pittsburgh are supported by the Chemical Theory, Models and Computational Methods Program in the Division of Chemistry, to develop and apply novel statistical machine learning or ""artificial intelligence"" techniques to analyze the structure and flexibility of molecules.  Most molecules are flexible, and at room temperature can exist in multiple interconverting geometries called conformers.  As molecules increase in size, complexity, and flexibility, the number of possible stable conformers increases exponentially. To accurately predict molecular properties, it is important to take into account all geometries---even those lower-probability conformers---if they have some probability of spontaneously forming at a given temperature.  Thus, a challenge is to efficiently identify these conformers despite an extremely complex, multi-dimensional geometric search spaces.  Professors Hutchison and Koes implement a unique grid-based neural network approach to predict conformers, trained on databases of high-quality calculations generated using NSF XSEDE supercomputing resources.  The machine learning techniques are validated against experimental and computational benchmarks.  They are also applied to developing improved methods for identifying drug targets, and for optimizing the design of plastic electronic materials. The databases and software developed for this project are publicly disseminated.  New tutorial resources are developed for Avogadro and 3DMol.js software tools, and new educational components on programming, visualization, and statistical machine learning areincorporated into the ""Mathematics for Chemists"" course taught by Professor Hutchison.  Both investigators give open lectures on data science and chemistry to the public and to local high school students.  They are also actively engaged in outreach to underrepresented groups through the American Chemical Society Project Seed, Pittsburgh Public Schools Science and Technology Academy, and the Drug Discovery, Systems, and Computational Biology Summer Academy for high school students.<br/><br/>The first part of this project draws upon a connection between statistical thermodynamics and Bayesian statistics. Using experimental and computational data, one can estimate distributions of dihedral angles for most molecules. From such probabilities, Bayesian optimization can be used to accurately explore and sample Boltzmann-weighted ensembles of the potential energy surface. The second aim takes advantage of recent improvements in quantum chemical methods for predicting thermochemistry for organic molecules. Using these accurate energies in tandem with experimental and other computational databases, recurrent neural-network thermochemical models are produced for molecules of different sizes and containing a wide range of elements. The resulting large data repositories and open source software tools are disseminated to the community, and used as the foundation for educational materials in a new curriculum for chemistry.  They are providing the basis for ongoing outreach and broadening participation activities to high school and undergraduate students, involving a state-of-the-art, interdisciplinary mix of data science, machine learning, and computational chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819157","Multigrid Methods and Machine Learning","DMS","COMPUTATIONAL MATHEMATICS","07/01/2018","08/09/2019","Jinchao Xu","PA","Pennsylvania State Univ University Park","Continuing Grant","Yuliya Gorb","06/30/2021","$350,000.00","Ludmil Zikatanov","xu@math.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","MPS","1271","079Z, 9263","$0.00","The goal of this project is to merge advanced tools from multigrid (MG) methods and machine learning (ML) towards the development of a novel class of numerical techniques targeting the data intensive applications emerging in physical, biological and social sciences.  Multigrid methods, including both geometric and algebraic multigrid (GMG and AMG) methods, are effective tools for solving linear as well as nonlinear algebraic system of equations arising from scientific and engineering computing.  On the other hand, there is a significant advancement in machine learning (ML) techniques, especially convolutional neural networks (CNN), which have successful applications in many areas such as image classification and processing.  The proposed project is to explore the resemblances and differences between these two different technologies so that more efficient multigrid methods as well more efficient deep learning models are developed.  The existing rich theory of multigrid method is expected to shed new light to the theoretical understanding of deep neural networks whereas the numerous empirical techniques used in the vast and ever-growning deep learning literature can be used to design general multigrid methods with wider range of applications.  This interdisciplinary research project is expected to have a direct impact to both the scientific computing community and the artificial intelligence industry.<br/><br/>More specifically, MG and CNN are similar for the use of multilevel hierarchy and the use of many technical components such as smoothers (MG) versus convolutions (CNN), restriction (MG) versus convolution with stride (CNN). But they also have some major differences: CNN has multiple channels of convolutions to be trained whereas MG often has one single smoother given a priori. Such relationships motivate the design of new multigrid methods with more general smoothers and restrictions that are subject training in different ways and, as a result, multigrid methods will become more adaptive and robust in its application to different practical problems.  The well-understood MG structure and theory can be adapted to understand and improve the existing deep learning model such as residual neural networks.  Furthermore, multilevel iterative techniques used in MG will also be investigated to speed up the stochastic gradient descent method that is now the standard training algorithm for most deep neural networks in machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819733","SBIR Phase I:  Measure What We Treasure: Developing a Structural Cognitive Analytics and Assessment (SCAA) Technology","IIP","SBIR Phase I","06/15/2018","06/13/2018","Laura Cabrera","DC","Plectica LLC","Standard Grant","Rajesh Mehta","10/31/2019","$224,000.00","","lac19@cornell.edu","P O Box 42597","Washington","DC","200152604","6469418822","ENG","5371","5371, 8031, 8032","$0.00","This SBIR Phase I project will address the pedagogical and evaluative problems caused by our current inability to measure core outcomes beyond information recall. The proposed innovation makes important educational outcomes such as deep understanding, cognition (thinking skills) and awareness of thinking (metacognition) measurable. Reliable assessment of these outcomes has broad significance to education and society writ large, as many employers struggle with skills gaps between high school and college graduates and the professional, personal and societal demands on adults in the 21st Century. This research combines cognitive science, epistemology, systems science, and complex systems with new developments in artificial intelligence, including machine learning and neural networks, and aligns well with the NSF's mission to promote progress in science and advance national prosperity. This project not only has the potential to impact core tenets of educational practice - including how teachers teach, how learners learn, and how we measure and understanding knowledge - but also may have impact on science through increased ability to map and analyze patterns and common structures in knowledge and generally for Americans to increase their developmental skills and abilities. Beginning in an educational market valued at $8-15 billion, this project has the potential to catalyze significant job creation and have significant commercial impact across a range of related industries.<br/><br/>This SBIR Phase I project introduces a visual grammar for mapping ideas in a canvas-based environment, and provides a neural-network based mechanism for quantitatively comparing expressions of complex ideas along many dimensions to facilitate a new approach to thinking, learning, and assessment. Maps of ideas will be tokenized and serialized and then fed through a recurrent neural network model to produce encoded numeric vector representations that are meaningfully comparable in their encoded form. Once in this form, vectors will be compared and evaluated for content and structure similarity, and insights offered in many dimensions: depth of detail, understanding of relationships, and numerous other meaningful measures. These vectors enable scalable, consistent, constructive assessment in a way not previously possible. In the mapping environment, teachers and students will create maps either on their own, or collaboratively together in real-time. As users create maps, we use the generated vectors to analyze both content and structure, and then prompt users to think about their subject matter from new perspectives. As a means of evaluation, teachers will create mapping activities for students to complete, and then student maps will be quantitatively compared to the standards and target maps which represent the complete understanding of the topic.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814745","RI:Small: Learning shape features with deep neural networks","IIS","Robust Intelligence","09/01/2018","04/18/2019","Longin Jan Latecki","PA","Temple University","Standard Grant","Jie Yang","08/31/2021","$465,999.00","Haibin Ling","latecki@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7495","075Z, 7495, 7923, 9251","$0.00","This project investigates how to effectively learn shape features with deep neural networks from images. It has been commonly believed that features learned by deep neural networks from images include texture, color, and shape of objects. Although visualizations of learned features demonstrate that contours of objects are extracted in the process of deep learning, our preliminary results provide clear arguments that 2D shape features are not well captured by current deep neural networks. This project develops a framework for effective learning of shape features with deep neural networks. The research brings new insights to a core problem in computer vision: shape understanding, which relates to many subfields in computer vision ranging from low-level tasks, such as segmentation and image statistics, to high-level ones, such as visual retrieval and object detection in images. The project includes plan to deploy the research results directly to applications such as biodiversity study (species recognition). The project also involves high school students and undergraduates in research.<br/><br/>This project conducts both theoretical and experimental research to gain better understanding why shape features are not well captured by current deep neural networks. Then it develops new learning strategies specifically targeted for shape features by following two main alternatives: (1) constraining the filter learning for Convolutional Neural Networks so that they are more contour focused, and (2) designing special structures of Deep Neural Networks for learning shape representation. The project designs circular sequential networks for silhouette-based shape classification, which encode naturally contour context information while implicitly performing contour matching. It also extends these networks to sketches, which are composed of both closed and open contours. Attention models are investigated on shapes to analyze roles of parts in shape representations so as to improve further shape matching and recognition algorithms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838230","BIGDATA: IA: Collaborative Research: Intelligent Solutions for Navigating Big Data from the Arctic and Antarctic","IIS","Polar Cyberinfrastructure, EarthCube, Big Data Science &Engineering","09/01/2018","08/30/2018","Maryam Rahnemoonfar","TX","Texas A&M University Corpus Christi","Standard Grant","Sylvia Spengler","10/31/2019","$612,823.00","","maryam@umbc.edu","6300 Ocean Drive, Unit 5844","Corpus Christi","TX","784125844","3618253882","CSE","5407, 8074, 8083","062Z, 8083, 9102","$0.00","The objective of this research is to investigate artificial intelligence (AI) solutions for data collected by the Center for Remote Sensing of Ice Sheets (CReSIS) in order to provide an intelligent data understanding to automatically mine and analyze the heterogeneous dataset collected by CReSIS. Significant resources have been and will be spent in collecting and storing large and heterogeneous datasets from expensive Arctic and Antarctic fieldwork (e.g. through NSF Big Idea: Navigating the New Arctic). While traditional analyses provide some insight, the complexity, scale, and multidisciplinary nature of the data necessitate advanced intelligent solutions. This project will allow domain scientists to automatically answer questions about the properties of the data, including ice thickness, ice surface, ice bottom, internal layers, ice thickness prediction, and bedrock visualization. The planned approach will advance the broader big data research community by improving the efficiency of deep learning methods and in the investigation of methods to merge data-driven AI approaches with application-specific domain knowledge. Special attention will be given to women and minority involvement in the research and the project will develop new course materials for several classes in AI at a Hispanic and minority serving institute.<br/><br/>In polar radar sounder imagery, the delineation of the ice top and ice bottom and layering within the ice is essential for monitoring and modeling the growth of ice sheets and sea ice. The optimal approach to this problem should merge the radar sounder data with physical ice models and related datasets such as ice coverage and concentration maps, spatiotemporal meteorological maps, and ice velocity. Rather than directly engineering specific relations into the image analysis that require many parameters to be defined and tuned, data-dependent approaches let the machine learn these relationships. To devise intelligent solutions for navigating the big data from the Arctic and Antarctic and to scale up the current and traditional techniques to big data, this project plans several approaches for detecting ice surface, bottom, internal layers, 3D modeling of bedrock and spatial-temporal monitoring of the ice surface: 1) Devise new methodologies based on hybrid networks combining machine learning with traditional domain specific knowledge and transforming the entire deep learning network to the time-frequency domain. 2) Equip the machine with information that is not visible to the human eye or that is hard for a human operator to consider simultaneously, to be able to detect internal layers and 3D basal topography on a large scale. Using the results of the feature tracking of the ice surface in radar altimetry, the research effort will also develop new data-dependent techniques for predicting the ice thickness for following years based on deep recurrent neural networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816851","SaTC: CORE: Small: Adversarial ML in Traffic Analysis","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","08/15/2018","07/28/2020","Matthew Wright","NY","Rochester Institute of Tech","Standard Grant","Wei-Shinn Ku","07/31/2021","$515,840.00","","matthew.wright@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","1714, 8060","025Z, 075Z, 7434, 7923, 9178, 9251","$0.00","Surveillance and tracking on the Internet are growing more pervasive and threaten privacy and freedom of expression. The Tor anonymity system protects the privacy of millions of users, including ordinary citizens, journalists, whistle-blowers, military intelligence, police, businesses, and people living under censorship and surveillance. Unfortunately, Tor is vulnerable to website fingerprinting (WF) attacks in which an eavesdropper uses a machine learning (ML) classifier to identify which website the user is visiting from its traffic patterns. The research team's state-of-the-art WF attack using a deep learning classifier reaches 98% accuracy, which is deeply concerning to Tor and its users. The goal of this project is to explore the new landscape of WF attacks and defenses in light of the team's findings with deep learning. A key aspect of the work is to build upon recent advances in fooling deep learning classifiers and apply these new findings to the context of anonymity systems. Based on this focus on adversarial machine learning, the project will create a new course and an accessible summer camp module on the topic, as well as launch a podcast on Cybersecurity Research featuring interviews with top researchers in the fields of adversarial machine learning and anonymity.<br/><br/>The research has three thrusts. First, the team is exploring the impact that these attacks can have for Tor users by addressing how the attacks can generalize to different network conditions and Tor versions, how they can be better adapted to realistic settings, and how they are impacted by real-world user behaviors in Tor. Second, since recent work has shown that it is possible to reliably fool deep learning classifiers, the team is studying how to adapt these techniques for robust and efficient defense. Prior work has primarily been in the image classification domain, whereas network traffic is more challenging to manipulate, so the team is designing new methods that account for this difference. In the third thrust, recognizing that researchers are actively seeking robust classifiers that are harder to fool, the team aims to understand new ways to build robust classifiers and explore their properties. While this aspect of the project means potentially finding stronger WF attacks against Tor, robust classifiers would be helpful for the myriad applications of deep learning, such as self-driving cars, stylometry, malware detection, processing drone and satellite imagery, and more.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838236","BIGDATA: IA: Collaborative Research: Intelligent Solutions for Navigating Big Data from the Arctic and Antarctic","IIS","Big Data Science &Engineering","09/01/2018","08/30/2018","John Paden","KS","University of Kansas Center for Research Inc","Standard Grant","Sylvia Spengler","08/31/2022","$373,338.00","","paden@ku.edu","2385 IRVING HILL RD","Lawrence","KS","660457568","7858643441","CSE","8083","062Z, 8083, 9150","$0.00","The objective of this research is to investigate artificial intelligence (AI) solutions for data collected by the Center for Remote Sensing of Ice Sheets (CReSIS) in order to provide an intelligent data understanding to automatically mine and analyze the heterogeneous dataset collected by CReSIS. Significant resources have been and will be spent in collecting and storing large and heterogeneous datasets from expensive Arctic and Antarctic fieldwork (e.g. through NSF Big Idea: Navigating the New Arctic). While traditional analyses provide some insight, the complexity, scale, and multidisciplinary nature of the data necessitate advanced intelligent solutions. This project will allow domain scientists to automatically answer questions about the properties of the data, including ice thickness, ice surface, ice bottom, internal layers, ice thickness prediction, and bedrock visualization. The planned approach will advance the broader big data research community by improving the efficiency of deep learning methods and in the investigation of methods to merge data-driven AI approaches with application-specific domain knowledge. Special attention will be given to women and minority involvement in the research and the project will develop new course materials for several classes in AI at a Hispanic and minority serving institute.<br/><br/>In polar radar sounder imagery, the delineation of the ice top and ice bottom and layering within the ice is essential for monitoring and modeling the growth of ice sheets and sea ice. The optimal approach to this problem should merge the radar sounder data with physical ice models and related datasets such as ice coverage and concentration maps, spatiotemporal meteorological maps, and ice velocity. Rather than directly engineering specific relations into the image analysis that require many parameters to be defined and tuned, data-dependent approaches let the machine learn these relationships. To devise intelligent solutions for navigating the big data from the Arctic and Antarctic and to scale up the current and traditional techniques to big data, this project plans several approaches for detecting ice surface, bottom, internal layers, 3D modeling of bedrock and spatial-temporal monitoring of the ice surface: 1) Devise new methodologies based on hybrid networks combining machine learning with traditional domain specific knowledge and transforming the entire deep learning network to the time-frequency domain. 2) Equip the machine with information that is not visible to the human eye or that is hard for a human operator to consider simultaneously, to be able to detect internal layers and 3D basal topography on a large scale. Using the results of the feature tracking of the ice surface in radar altimetry, the research effort will also develop new data-dependent techniques for predicting the ice thickness for following years based on deep recurrent neural networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749917","CAREER: Learning Multi-Level Narrative Structure","IIS","Robust Intelligence","06/01/2018","05/15/2020","Mark Finlayson","FL","Florida International University","Continuing Grant","Roger Mailler","05/31/2023","$455,648.00","","markaf@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7495","1045, 7495, 9251","$0.00","Why do certain stories, but not others, resonate so powerfully with certain populations? Stories (a.k.a. narratives) are powerful: where rational argument fails, a single story can drive home a point, change a mind, and even change a life. What specific structures underlie the power of narrative, and what new artificial intelligence (AI) techniques are needed to learn these structures automatically so we can leverage them in applications? This project seeks to develop these new AI techniques to automatically uncover and confirm the fundamental structures underlying narrative, developing and testing with data drawn from the domains of education and culture. This work will be of broad relevance to developing more intelligent machines, understanding the mind and brain, and improving education. It will produce fundamental insights into a universal form of communication (narrative), providing a potentially transformative new set of tools to researcher and educators.<br/><br/>The project will develop new machine learning and natural language processing approaches to learning key aspects of narrative structure. The basic structure of a narrative involves the plot, a time-ordered sequence of important events, and the plot can be divided into three levels of structure: (1) plot pieces, (2) archetypal characters, and (3) narrative arcs. The PI and his students will first learn to extract these three types of narrative structure, the third of which (narrative arcs) is as-yet untried, using novel combinations of existing grammar learning approaches and Bayesian approaches, specifically the PI's Analogical Story Merging (ASM) algorithm, the Infinite Relational Model (IRM), and iterative learning. Second, the researchers will test hypotheses that reflect why specific stories are persuasive to specific cultures, and apply these insights to improving minority engagement in STEM and computing in middle-school classrooms in Miami Dade County Public Schools. Third, the researchers will seek to uncover systematic regularities in professional education cases (such as business cases, or medical case reports) that will lead to the ability to make computational predictions as to which cases should be most effective in the classroom.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763642","RI: Medium: Recognizing, Mitigating and Governing Bias in AI","IIS","Robust Intelligence","09/01/2018","07/23/2020","Arvind Narayanan","NJ","Princeton University","Standard Grant","Roger Mailler","08/31/2022","$811,016.00","Olga Russakovsky","arvindn@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7495","7367, 7495, 7924, 9251","$0.00","Artificial Intelligence (AI) technologies mediate our interactions with the world and our daily decision making, ranging from shopping to hiring to surveillance. The development of rich AI algorithms able to process and learn from unparalleled amounts of data holds promise for making impartial, well-informed decisions. However, such systems also absorb human biases, such as gender stereotyping of activities and occupations. Left unchecked, they will perpetuate these biases on an unparalleled scale. A steady stream of press confirms that this is a widespread problem in real-world applications. This research brings together an interdisciplinary team to develop the science of AI bias. The findings will impact AI researchers and developers (through novel methodologies), computational social scientists (through a deeper study of human biases at web scale), educators and policy makers (through the comprehensive analysis of bias), and downstream users of AI technology. <br/><br/>Compared to applications such as criminal risk scoring where fairness has traditionally been studied, modern AI systems are characterized by massive datasets, complex deep models and an unprecedented breadth of applications. This results in a wider spectrum of biases with complex propagation pathways, requiring an in-depth scientific investigation. The project develops the tools and techniques for recognizing, mitigating and governing bias in AI by combining expertise in deep learning, crowdsourcing and dataset curation, AI ethics, analyzing inference risk, web measurement, and science and technology studies. The component on recognizing bias includes an application of the Implicit Association Test combined with zero-shot learning to understand the societal bias of web corpora. Mitigating bias includes bridging active learning with research on adversarial examples for AI models. Governing bias includes a qualitative and quantitative study of downstream bias effects. The research is designed to be tightly connected, as for example when the recognition of curation bias in datasets leads to techniques in mitigating bias through enforcing group fairness in deep learning to governing bias in deployed system through developing bias observatories. The study will include advancements in machine learning (decomposing deep architectures, adapting reinforcement learning, exploring domain adaptation), human-computer interaction (developing novel active learning techniques, studying model interpretability), and digital ethnography (studying the effect of AI bias on culture, establishing an AI bias taxonomy). It will serve as a bridge between these fields, establishing tighter connections between them.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854737","SHF: Small: Collaborative Research: LDPD-Net: A Framework for Accelerated Architectures for Low-Density Permuted-Diagonal Deep Neural Networks","CCF","Software & Hardware Foundation","08/15/2018","10/25/2018","Bo Yuan","NJ","Rutgers University New Brunswick","Standard Grant","Sankar Basu","09/30/2021","$224,997.00","","bo.yuan@soe.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798","7923, 7945","$0.00","Deep learning has emerged as an important form of machine-learning where multiple layers of neural networks can learn the system function from available input-output data. Deep learning has outperformed traditional machine-learning algorithms based on feature engineering in fields such as image recognition, healthcare, and autonomous vehicles. These are widely used in cloud computing where large amount of computational resources are available. Deep neural networks are typically trained using graphic processing units (GPUs) or tensor processing units (TPUs). The training time and energy consumption grow with the complexity of the neural network. This project attempts to impose sparsity and regularity as constraints on the structure of the deep neural networks to reduce complexity and energy consumption by orders of magnitude, possibly at the expense of a slight degradation in the performance. The impacts lie in the formulation of a new family of structures for neural networks referred to as Low-Density Permuted Diagonal Network or LDPD-Net. The approach will enable the deployment of deep neural networks in energy-constrained and resource-constrained embedded platforms for inference tasks, including, but not limited to, unmanned vehicles/aerial systems, personalized healthcare, wearable and implantable devices, and mobile intelligent systems. In addition, the design methodology/techniques developed in this project can facilitate investigation of efficient computing of other matrix/tensor-based big data processing and analysis approaches. These approaches may also find applications in data-driven neuroscience and data-driven signal processing. In addition to graduate students, the project will involve undergraduates via senior design projects and research experiences for undergraduates. The results of the project will be disseminated to the broader community by publications, presentations, talks at various industries and other academic institutions. <br/><br/>The main barriers to wide adoption of deep learning networks include computational resource constraints and energy consumption constraints. These barriers can be relaxed by imposing sparsity and regularity among different layers of the deep neural network. The proposed low-density permuted-diagonal (LDPD) network can lead to orders of magnitude reduction in computation complexity, storage space and energy consumption. The LDPD-Net will not be retrained by first training a regular network and then only retaining the weights corresponding to the LDPD-Net. Instead, the proposed network will be trained from scratch. The proposed LDPD-Net can enable scaling of the network for a specified computational platform. The proposed research has three thrusts: 1) develop novel resource-constrained and energy-constrained inference and training systems;  2) develop novel efficient hardware architectures that can fully exploit the advantages of the LDPD-Net to achieve high performance; and 3) perform novel software and hardware co-design and co-optimization to explore the design space of the LDPD-Net. Using these, the efficacy of the proposed LDPD-net will be validated and evaluated, via software implementations on high-performance systems, low-power embedded systems, and a hardware prototype on FPGA development boards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756013","CRII: SHF: Optimizing Deep Learning Training through Modeling and Scheduling Support","CCF","Software & Hardware Foundation","06/01/2018","01/23/2018","Feng Yan","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Almadena Chtchelkanova","05/31/2021","$174,990.00","","fyan@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","7798","7942, 8228, 9150","$0.00","Deep learning models trained on large amounts of data using lots of computing resources have recently achieved state-of-the-art training performance on important yet challenging artificial intelligence tasks. The success of deep learning has attracted significant research interest from hardware and software communities to improve training speed and efficiency. Despite the great efforts and rapid progress made, one important bridge to connect software and hardware support with deep learning domain knowledge is still missing: efficient configuration exploration and runtime scheduling. Both the quality of deep learning models and the training time are very sensitive to many adjustable parameters that are set before and during the training process, including the hyperparameter configurations (such as learning rate, momentum, number and size of hidden layers) and system configurations (such as thread parallelism, model parallelism, and data parallelism). Efficient exploration of hyperparameter configurations and judicious selection of system configurations is of great importance to find high-quality models with affordable time and cost. This is however a challenging problem due to a huge search space, expensive training runtime, sparsity of good configurations, and scarcity of time and resources.<br/><br/>The objective of this research work is to systematically study the unique properties of deep learning systems and workloads, and establish new modeling and scheduling methodologies for improving deep learning training. The PI aims to improve the efficiency of discovering high performing models through a dynamic scheduling methodology driven by a novel hyperparameter configuration classification approach. The PI aims at developing an accuracy- and efficiency-aware hybrid scheduling methodology that makes judicious scheduling decisions based on a global view of both the time dimension (accuracy potential) and spatial dimension (efficiency potential) information. This research work integrates techniques in workload characterization, performance modeling, resource management, and scheduling to dramatically speedup the training process while significantly reducing the cost in time and resources. More broadly, this project will gain foundational knowledge about the interaction between software-hardware support and deep learning domain knowledge. This knowledge can help design next generation deep learning systems and frameworks, making deep learning training handy for researchers and practitioners with limited system and machine learning domain expertise. This research will help enhance curriculum and provide research topics for both undergraduate and graduate students, especially students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832294","Examining groundwater-flood and soil moisture-flood relationships across scales using national-scale data mining, deep learning and knowledge distillation","EAR","Hydrologic Sciences","07/01/2018","07/26/2018","Chaopeng Shen","PA","Pennsylvania State Univ University Park","Continuing Grant","Laura Lautz","12/31/2020","$249,862.00","","cxs1024@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","GEO","1579","","$0.00","In many parts of the United States, it has been shown that groundwater levels and soil moisture, which quantifies the wetness of the soil, are connected via the mechanism of flood production. Water cannot infiltrate into the ground when groundwater is close to the surface and is thus forced to quickly run off to rivers, creating higher flooding risks. However, the relationship between groundwater and floods has been found to be highly diverse and difficult to predict. Depending on terrain, groundwater depth, and many other factors, floods lead groundwater increase in some cases while groundwater can lead floods in others. Previous research from selected experimental watersheds have not resulted in a comprehensive and transferable understanding of the controlling processes. This project will take a big-data, machine learning approach to enhance our understanding of this relationship, allowing us to heuristically exploit previously under-utilized groundwater data for flood predictions and reducing damages. Using learning patterns from national-scale groundwater and streamflow data, the machine learning algorithms will create plausible groundwater-flood relationships. Taking advantage of the big hydrologic data from available satellite missions, this project will create shared undergraduate course modules to enhance student's ability to work with big data and increase their awareness of global water issues.<br/><br/>This research advances hydrologic science by answering the following overarching question: at catchment scales, do groundwater levels in the catchment provide predictive power for flood threshold functions and baseflow? We will address this question in multiple small steps. We will identify the kinds of groundwater-rainfall-runoff (GW-P-Q) relations that can be found over the Continental United States. These relations are quantified by the correlations between water table depths and flood thresholds (and baseflow) at different lags and time scales. We will seek the factors dictate the type of GW-P-Q relations and whether these relations are stable across seasons and years. We will employ two approaches: a human-directed classification analysis, and a knowledge distillation scheme based on deep learning (DL), a rapidly advancing group of techniques supporting the recent surge in artificial intelligence. In the first approach, we will use classification and regression tree to identify factors that could explain the GW-P-Q relations. In the DL-based approach, we will train continental-scale time series DL models using all available data to forecast discharge. This approach addresses the issue with classification trees in which not enough data are available for branch nodes. Through a novel knowledge distillation procedure, we transfer the knowledge gained in the deep network to more interpretable formats, including explicit mathematical formula. Results from the study will provide a comprehensive understanding of GW-P-Q relations where regional patterns and physical controls emerge. Besides gaining new knowledge, a significant by-product is the trained DL models. They can be used as a flood forecasting tool to integrate recent soil moisture and groundwater observations, which have not been exploited until now. The educational activity will mesh with the research activity by engaging undergraduate students in handling, visualizing and interpreting big hydrologic data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763673","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","Special Projects - CCF, Comm & Information Foundations","09/01/2018","09/08/2019","Amir Avestimehr","CA","University of Southern California","Continuing Grant","Phillip Regalia","08/31/2022","$144,293.00","","avestime@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","2878, 7797","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840873","RAPID: Collaborative Research: Machine Learning for Dehazing Unmanned Aerial System Imagery from Volcanic Eruptions","IIS","Robust Intelligence","08/01/2018","08/04/2018","Robin Murphy","TX","Texas A&M Engineering Experiment Station","Standard Grant","David Miller","07/31/2019","$80,742.00","Zhangyang Wang","robin.r.murphy@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7495","7495, 7914","$0.00","The ongoing eruption of the Kilauea volcano in Hawaii is the first reported time that small unmanned aerial systems (UAS) have been used for the emergency response to a volcanic eruption. The Center for Robot-Assisted Search and Rescue (CRASAR) flew 44 small UAS flights for the Hilo Fire Department and Hawaii County Civil Defense. The eruption imagery was partially occluded by plumes of steam carrying toxic gases, something that had not been encountered before. The plumes interfere with responders comprehending the tactical situation because it obscures the ground below and often prevents software from generating useful surface maps. While volcanic eruptions are fairly rare, the same plume problem is likely to occur in other hazardous material events. Machine learning techniques for dehazing were only partially successful because plumes present a very different set of challenges than removing urban haze or smog. This project conducts rapid research to remove or reduce plumes, from stills and video, in near real-time in order to support responses to the ongoing disaster. It will make the datasets available so that they can be used for training and evaluating new machine learning algorithms. The project will host a follow up workshop at the 2019 AAAI Conference on Artificial Intelligence in Hawaii.<br/><br/>This project creates a UAS open-source imagery dataset from the ongoing Leilani, Hawaii, volcanic eruption event. It uses the dataset to expand and refine dehazing algorithms that will help Hawaii public safety agencies and volcanologists see through the plumes of steam and gas that is interfering with mapping the extent and volume of the lava. Plumes of steam mingled with sulfur dioxide interfered with interpreting the boundaries of the lava field and introduced errors into stitching images together or caused details to be averaged out. Smog is a homogeneous, thin visual phenomenon while plumes are heterogeneous and thick, limiting the utility of current techniques and requiring focused research. The dataset offers an opportunity for a corpus of real imagery that can serve as machine learning training data and enable comparison of before and after results. The intellectual merit of the project is twofold. It provides a unique opportunity to explore a new area of machine learning for heterogeneous, thick plumes in images. The comprehensive dataset will enable foundational work in computer vision, machine learning, and emergency informatics.  The research will immediately improve emergency management of the Leilani eruption event and emergency management in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840878","RAPID: Collaborative Research: Machine Learning for Dehazing Unmanned Aerial System Imagery from Volcanic Eruptions","IIS","Robust Intelligence","08/01/2018","08/04/2018","David Merrick","FL","Florida State University","Standard Grant","David Miller","07/31/2019","$6,331.00","","dmerrick@fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7495","7495, 7914","$0.00","The ongoing eruption of the Kilauea volcano in Hawaii is the first reported time that small unmanned aerial systems (UAS) have been used for the emergency response to a volcanic eruption. The Center for Robot-Assisted Search and Rescue (CRASAR) flew 44 small UAS flights for the Hilo Fire Department and Hawaii County Civil Defense. The eruption imagery was partially occluded by plumes of steam carrying toxic gases, something that had not been encountered before. The plumes interfere with responders comprehending the tactical situation because it obscures the ground below and often prevents software from generating useful surface maps. While volcanic eruptions are fairly rare, the same plume problem is likely to occur in other hazardous material events. Machine learning techniques for dehazing were only partially successful because plumes present a very different set of challenges than removing urban haze or smog. This project conducts rapid research to remove or reduce plumes, from stills and video, in near real-time in order to support responses to the ongoing disaster. It will make the datasets available so that they can be used for training and evaluating new machine learning algorithms. The project will host a follow up workshop at the 2019 AAAI Conference on Artificial Intelligence in Hawaii.<br/><br/>This project creates a UAS open-source imagery dataset from the ongoing Leilani, Hawaii, volcanic eruption event. It uses the dataset to expand and refine dehazing algorithms that will help Hawaii public safety agencies and volcanologists see through the plumes of steam and gas that is interfering with mapping the extent and volume of the lava. Plumes of steam mingled with sulfur dioxide interfered with interpreting the boundaries of the lava field and introduced errors into stitching images together or caused details to be averaged out. Smog is a homogeneous, thin visual phenomenon while plumes are heterogeneous and thick, limiting the utility of current techniques and requiring focused research. The dataset offers an opportunity for a corpus of real imagery that can serve as machine learning training data and enable comparison of before and after results. The intellectual merit of the project is twofold. It provides a unique opportunity to explore a new area of machine learning for heterogeneous, thick plumes in images. The comprehensive dataset will enable foundational work in computer vision, machine learning, and emergency informatics.  The research will immediately improve emergency management of the Leilani eruption event and emergency management in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814406","SaTC: CORE: Small: Super-Human Cryptanalysis for Scalable Side-Channel Analysis","CNS","Secure &Trustworthy Cyberspace","09/01/2018","08/30/2018","Berk Sunar","MA","Worcester Polytechnic Institute","Standard Grant","Sandip Kundu","08/31/2021","$500,000.00","Thomas Eisenbarth","sunar@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","8060","025Z, 7434, 7923","$0.00","The project takes the rapidly evolving advances in deep learning and applies them in the context of side-channel analysis (SCA). Finding SCA leakages on real devices can be a tedious process, resulting devices ranging from wearables to embedded Internet of Things (IoT) devices entering the marketplace without proper protection. This project explores ways to automate side-channel security analysis using deep learning techniques. To protect devices against SCA, the project also explores a novel approach to countermeasure design by applying the concept of adversarial learning.<br/><br/>SCA is essentially one complex statistical signal processing problem, which deep learning is ideally suited to solve. The project systematically quantifies the impact of deep learning on SCA by applying deep learning methods to all necessary steps in SCA, namely alignment, noise reduction, feature extraction and model building. Meaningful parameter sets for a representative list of reference targets are explored. The project also adapts adversarial learning techniques to counteract optimized side-channel information recovery, thereby inventing an entirely new class of side-channel countermeasures, where machine learning adaptively shapes leakage signals to prevent correct classification. <br/><br/>The SCA analysis and protection tools explored in this project will be invaluable for the health of our national computing and communications infrastructure. They will be released as an easy-to-use open-source toolbox. Furthermore, the project provides new insights and training for the next generation of experts at the intersection of two critical technologies, i.e. artificial intelligence and security. <br/><br/>More information on the project, including important data and developed code, is available at: http://v.wpi.edu/research/superhuman, until circa 2026.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763702","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","Special Projects - CCF, Comm & Information Foundations","09/01/2018","09/12/2019","Georgios-Alex Dimakis","TX","University of Texas at Austin","Continuing Grant","Phillip Regalia","08/31/2022","$145,871.00","","dimakis@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","2878, 7797","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763657","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","Special Projects - CCF, Comm & Information Foundations","09/01/2018","08/23/2019","Viveck Cadambe","PA","Pennsylvania State Univ University Park","Continuing Grant","Phillip Regalia","08/31/2022","$150,942.00","","VXC12@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","2878, 7797","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763561","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","Special Projects - CCF, Comm & Information Foundations","09/01/2018","08/22/2019","Pulkit Grover","PA","Carnegie-Mellon University","Continuing Grant","Phillip Regalia","08/31/2022","$172,541.00","","pgrover@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878, 7797","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836950","D3SC: EAGER: Deep learning to design selective kinase inhibitors","CHE","Chemistry of Life Processes","09/01/2018","08/15/2018","John Karanicolas","PA","Institute For Cancer Research","Standard Grant","Catalina Achim","08/31/2020","$298,559.00","","john.karanicolas@fccc.edu","333 COTTMAN AVENUE","Philadelphia","PA","191112434","2157282659","MPS","6883","026Z, 062Z, 068Z, 7916, 8084, 9216, 9263","$0.00","John Karanicolas of the Institute for Cancer Research is supported by an award from the Chemical Theory, Models and Computational Methods program and by the Data-Driven Discovery Science in Chemistry initiative in the Division of Chemistry, to develop an artificial intelligence-based strategy for designing small molecule compounds that bind to and probe the functions of human kinases.  Kinases are a family of proteins responsible for controlling virtually all signaling processes in multicellular organisms.  Such processes are fundamental to life itself, and play a critical role in cell division, cell growth and motility, and cellular transport.  Consequently, kinases also represent potential targets for novel cancer drugs.  There are several hundred kinase proteins, organized into complex signaling pathways. Each pathway acts like a set of cascading dominos: once an initial chemical trigger occurs at a key location on the kinase molecule, it sets in motion a series of downstream reactions translating into specialized molecular functions.  Since individual kinases can participate in multiple cellular processes, these signaling pathways are often highly intertwined.  To determine the function of a particular kinase, scientists utilize small molecules as chemical probes that bind to the kinase, suppress its activity, and enable the analysis of downstream impact on cellular function. However, a significant challenge is that most existing chemical probes are not as selective as originally thought: many probes act on multiple kinases at once.  Experimental ""wet lab"" testing of all possible combinations of hundreds of kinases against tens of thousands of potential chemical probes is not practical due to cost.  Professor Karanicolas and his students are utilizing state-of-the-art advances in artificial intelligence and machine learning, publicly-available data on select kinase-probe interactions, and specialized techniques developed in his laboratory for modeling the 3D structure of protein-small molecule binding, to develop a computational model that accurately predicts kinase binding affinities and chemical probe selectivity.  The project's discoveries have the potential for significant impact in facilitating fundamental studies of cell biology, as well as the identification of selective kinase inhibitors for pharmaceutical design.  The new methods are being implemented in software distributed through public repositories and the widely-used Rosetta software suite. The research is providing cross-disciplinary training opportunities fusing chemical biology and data science to students at the high school, undergraduate, and graduate levels. <br/><br/>Modern cell biology leans heavily on kinase inhibitors as chemical probes for analyzing the consequences of deactivating a particular kinase, but the majority of commonly-used chemical probes are not sufficiently target-selective for robust interpretation of observed phenotypes.  By assembling large panels of kinases (corresponding to much of the human kinome), it is possible to experimentally determine selectivity for a given probe: however, these experiments are expensive and impractical to perform at scale.  This project is applying deep learning techniques to predict the binding affinities of individual inhibitor/kinase pairs, using 3D structural descriptors derived from a novel method for modeling inhibitor/kinase complexes, recently developed in the Karanicolas group.  Following careful training and benchmarking, the predictive model is being used in two ways: 1) to evaluate the selectivity of chemical probes that are widely used by cell biologists, and determine which compounds constitute useful tools and which compounds should be deprecated; and 2) to screen a large chemical library of small molecules to identify new candidate probes that are expected to have strong binding affinity and selectivity for a given kinase. The model's predictions in both applications are being tested experimentally through biochemical assays. While it has long been hypothesized that inclusion of 3D structural features would improve existing machine learning approaches for predicting protein/ligand binding affinities, the availability of a rapid and accurate method for building 3D structural models will allow this hypothesis to be tested for the first time. If successful, insights from this project will provide a starting point for developing models to predict binding affinities of other protein-ligand complexes as well, for expanded applications in cell biology and drug discovery.  The results of the project are being disseminated as publicly-available source code through SourceForge, and as modules within the widely-used Rosetta software suite.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817231","RI: Small: Fast and Accurate Natural Language Parsing and Generation by Marrying Deep Learning with Dynamic Programming","IIS","Robust Intelligence","09/01/2018","07/18/2019","Liang Huang","OR","Oregon State University","Continuing Grant","Tatiana Korelsky","08/31/2021","$400,000.00","","huanlian@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","7495, 7923","$0.00","This grant aims to improve automatic understanding, generation, and translation of natural language by machines. Automated natural language processing has already changed the way we interact with digital assistants such as Amazon Echo and Apple Siri on smartphones and other devices.  Automated machine translation reduces information barriers on the Web, where most information is inaccessible to most users because it is in a language they do not understand.  Today's natural language systems, however, are limited to short exchanges and often make errors.  These limitations are due to need for the systems to respond very quickly: current methods for more accurate understanding and generation take too long.  This project will overcome this problem by developing new, fast, principled algorithms for these tasks.  This project also supports STEM education of underrepresented minorities (who do not speak English natively) by recruiting them in machine translation studies.<br/><br/>This grant aims to construct fast (linear-time) and accurate natural language parsers and generators (including translators) that utilize the power of both deep learning (for accurate and automatic feature engineering) and dynamic programming (to speed up the search). This project focuses on Recurrent Neural Network-based models (RNNs) such as Long Short Term Memory (LSTMs). In particular, this project aims to (1) Develop linear-time dynamic programming-based neural parsers by using RNNs to summarize the input text and extend them to joint syntactic-discourse parsing and predictive parsing.  (2) Develop approximate dynamic programming algorithms and principled beam search methods for text generation and machine translation systems that use RNN-based decoders to model output text.  (3) Combine the above two directions with an innovative application of simultaneous translation using predictive parsing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827830","2018 Association for Computational Linguistics (ACL) Student Workshop","IIS","ROBUST INTELLIGENCE","04/01/2018","03/16/2018","Marie-Catherine de Marneffe","OH","Ohio State University","Standard Grant","Tatiana Korelsky","11/30/2018","$18,000.00","","demarneffe.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 7556","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization for computational linguistics and natural language processing. It also is one of the primary application areas for researchers in machine learning and artificial intelligence. The proceedings of its annual meeting provide the foundation of the field; it is the most cited and most respected publication in computational linguistics. Thus, it is also the most important gathering of researchers in computational linguistics and natural language processing. This project is to subsidize travel, conference and housing expenses of students selected to participate in the ACL Student Research Workshop, which will take place during the main ACL conference from July 15-25, 2018 in Melbourne, Australia. The Student Research Workshop helps create a new generation of researchers with a more thorough understanding of their field, with connections and collaborations across institutions, and with innovative and exciting research programs. This contributes to America's pool of researchers with the needed scientific and engineering knowledge and skills. The workshop encourages a spirit of collaborative research and builds a supportive environment for a new generation of computational linguists. <br/><br/>The Student Research Workshop solicits two categories of submissions: research papers and research proposals. The research proposal can have only one author who must be a student. The research papers can have multiple authors, with the first author being a student (at either the graduate or undergraduate level). The workshop is a venue for students to receive constructive critical feedback on their work from experts outside of their institution, and to connect with other students and senior researchers in their field. The students gain exposure by presenting their work earlier than they would otherwise (i.e., in a form not yet ready for the main conference). This is particularly valuable for students from smaller institutions and undergraduate students. In addition, the workshop is organized and run by students. The student organizers gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826218","QRM: Using Visual Information to Quantify Microstructure-Processing-Property Relationships","CMMI","Materials Eng. & Processing, DMREF, ","09/01/2018","07/10/2019","Elizabeth Holm","PA","Carnegie-Mellon University","Standard Grant","Alexis Lewis","08/31/2022","$664,618.00","Bryan Webler","eaholm@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","ENG","088y, 8092, 8292, R210","024E, 062Z, 7556, 8021, 8025, 8400, 9102, 9178, 9231","$0.00","To advance technology for infrastructure and manufacturing, new, high-performance materials must be developed, with tunable properties (for example, strength or electrical conductivity) and predictable performance. The computational tools used to predict material performance have often relied on Materials Engineers examining pictures of a material's microscopic substructure, termed the microstructure, to determine the relationships between microstructural features and material properties. The analysis of microstructural images can be expensive and time consuming, and can produce inconsistent results. This award supports fundamental research to build the tools to enable an automated approach to microstructure image analysis, with the aim of better understanding the microstructural features that control material properties and performance. In this project, researchers will collect a large set of microstructural images and use artificial intelligence (AI) tools including computer vision and machine learning to analyze them. The AI system will autonomously identify different features of the microstructure, measure them, and link them to how the materials was made (processing) and how it performs (properties). The advantages of this system are that it is fast, objective, general, and may perceive information that is not readily visible to humans. To test and validate this approach, it will be applied to understanding scientifically challenging problems with application in advanced manufacturing, including predicting the strength of materials, and determining the limits of microstructural information. This project will additional train Materials Science and Engineering students in the principles of AI. Furthermore, the methods and results of this project will be made publicly available.<br/><br/>The quantitative representation of microstructure is the foundational tool of microstructural science and traditionally involves a human deciding a priori what to measure and how to measure it. However, recent advances in data science, including computer vision (CV) and machine learning (ML) offer new approaches to extracting information from microstructural images. In this project, the PIs will acquire, curate, and publish a diverse collection of microstructural image data sets, including composition, processing, and property metadata, and will build a suite of CV/ML tools for autonomous microstructural image representation and quantification. The CV/ML approach will be applied to finding quantitative composition-microstructure-processing-property relationships, with the goal of materials discovery. Case studies will focus on achieving scientific understanding of additive manufacturing processes; enhancing knowledge of deformation mechanisms; advancing microstructural science to include visual signals that are not perceptible by humans; and assessing the degree of scientific knowledge learned by a black-box ML method. The methods and results developed in this project will be disseminated via open access codes and data repositories, and will contribute to workforce development by educating Materials Science and Engineering students in the principles of computer vision and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815701","III: Small: Automatic Learning-based Services for Distributed Data Management Systems","IIS","Info Integration & Informatics","08/15/2018","08/03/2018","Olga Papaemmanouil","MA","Brandeis University","Standard Grant","Wei-Shinn Ku","07/31/2021","$499,913.00","","olga.papaemmanouil@gmail.com","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","7364","075Z, 7364, 7923","$0.00","The distributed nature of most database management systems (DBMSs) brings about new challenges to the daily tasks of database administrators. Apart from database tuning, today's administrators are often responsible for: (a) adapting data partitioning and replication schemes; (b) managing dynamic workloads to meet query prioritization and performance goals; as well as (c) provisioning computing resources on demand. Unfortunately, the complexity of these tasks often exceeds engineers' abilities to scientifically model them. To address this challenge, this project will couple existing learning-driven theory with distributed data management systems to have significant impact on the design of DBMSs. By leveraging advanced learning algorithms from machine learning and game theory, the project will allow DBMSs to move away from ""hard-coded algorithmic intelligence,"" rigid data structures, and algorithms that are based on informal intuition. Instead, DBMSs will be able to incorporate ""learning-based intelligence"" that provides reasoning on numerous decisions based on pattern recognition capabilities and mathematically proven insights. Leveraging information to learn and adapt could unleash tremendous potential as databases evolve to systems capable of automatically tuning themselves. The results of this project will also reduce the human effort of database administrators as they will be offered predictive models, decision tools and insight to the interplay between data distribution, workload management, and query performance. <br/><br/>The project will provide solutions to some of the key technical challenges that arise when tuning the performance of dynamic workloads on distributed data management systems. This research will lead to the design of new algorithms and learning-based frameworks for supporting data distribution, replication, query dispatching, query scheduling, and performance prediction without human intervention. These techniques will be automatically customized to application-level performance goals and user-defined query priorities and will allow distributed DBMSs to naturally handle dynamic workloads, changing data access patterns, and varying resource availability. By coupling unsupervised/supervised learning, deep learning and game theory to data management tasks the project will transform data management systems to ""database science"" tools through which system administrators will be able to explore and derive insight on the factors that affect the performance of a database and its deployed applications. As a result, the project will deliver complex predictive and correlation models between query, data, resource-related features and system performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828528","MRI: Acquisition of a GPU cluster to support interdisciplinary research in human learning, machine learning, and data science","BCS","Major Research Instrumentation","08/01/2018","07/23/2018","Patrick Shafto","NJ","Rutgers University Newark","Standard Grant","John Yellen","07/31/2021","$99,999.00","Elizabeth Bonawitz, William Graves, Michael Cole, Leslie Michelson","patrick.shafto@gmail.com","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","SBE","1189","1189","$0.00","This Major Instrumentation Grant award supports the Acquisition of a GPU cluster to support interdisciplinary research in human learning, machine learning, and data science at Rutgers University--Newark, a Minority Serving Institution (MSI). It permits purchase of 3 Nvidia dual V100 GPUs to enable theoretical advances and practical applications in interdisciplinary understanding of learning. Rutgers-Newark is undertaking a multiyear effort to build strength in interdisciplinary computer science to support research training, and to address issues of diversity and representation within computer science and data science. These resources would: (1) enable the application of computationally-intensive methods in order to develop new theories and tools to understand human and machine learning; (2) support existing cross-disciplinary training efforts, such as graduate-level courses centered around deep learning and Deep Gaussian Processes; (3) enhance existing funded research by allowing the deployment of advanced data-analytic methods. The GPU cluster will provide a common computational resource for researchers from the Computer Science, Psychology, and Neuroscience departments through which they may collaborate to advance the state-of-the-art in each field. This purchase will complement the existing high-performance computing infrastructure already on campus as well as a recent NSF-supported purchase of a 1.2 petabyte storage system for cataloging the dynamics of human visual experience. Also, it will supplement an NSF-sponsored Mobile Maker Center for community-based data collection and fMRI research. <br/><br/>Humans remain the most powerful and impressive available models of learning, although the roots of these abilities are not fully understood. Although machine learning methods have become exceptionally powerful in recent years, they remain opaque in ways that human learning is not and still require vastly more data, energy and compute power than human learners. Both human and machine learning would benefit from the ability to more tightly connect and study the strengths of each. Gaussian processes provide one such unifying framework. They are an object of interest in machine learning, where they have dual interpretations as regression models and as neural networks, as well as in human learning where they have been proposed as models of cognition and perception. These multiple interpretations of Gaussian processes are key to their interest for bridging human and machine learning. From a theoretical perspective, Gaussian processes are equivalent to (a specific type of) neural network, but much more amenable to mathematical analysis, and can be stacked to obtain Deep Gaussian processes. This Deep learning framework may allow more systematic mathematical analysis than other Deep learning approaches---for example the ability to derive explanations for their inferences. The primary research goal of this project is to use the GPU cluster and the investigators' interdisciplinary expertise to draw deep connections between machine learning and human learning perspectives to advance the state of the art in both, while also improving data analytic capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750082","CAREER: Visual Recognition with Knowledge","IIS","Robust Intelligence","08/15/2018","07/22/2020","Yezhou Yang","AZ","Arizona State University","Continuing Grant","Roger Mailler","07/31/2023","$317,506.00","","yz.yang@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7495","1045, 7495","$0.00","This project will address the problem of Visual Recognition with Knowledge (VR-K): a challenging Artificial Intelligence task to enable a seeing machine to identify unknown visible concepts from previous encounters (annotated data samples) and knowledge (other contextual information). For example, consider such a system that has never encountered a zebra, but which has previous visual encounters with ""horses"" and ""black and white striped"" patterns. Incorporating the linguistic input that, ""A zebra is a horse-like animal with a black and white striped appearance"", the machine's task is to formulate a new recognizer for the visual concept ""zebra"" and to recognize this new concept later. A system that integrates visual and linguistic information in this way can provide the basis for robust personal mobile applications or service robots, such as visual assistants to the vision-impaired, and voice-enable agents for elder care.<br/> <br/>Conventional supervised learning techniques have been perfected to perform increasingly well on narrow performance tasks. To enable satisfactory performance in service robots and mobile multimedia applications, this research will integrate background and commonsense knowledge models to enable higher level reasoning together with such high-performance recognizers. This project will develop the VR-K framework focused on enabling more generalizable computer vision algorithms through integration with natural language understanding and grounding in knowledge-based reasoning. The research program will include 1) developing efficient probabilistic reasoning engines to construct recognition models of unseen concepts (object and attribute) without new annotation through probabilistic semantic parsing; 2) setting up new large-scale visual challenges and testbeds as the basis for rigorous performance evaluation of visual recognition with knowledge models and ablation analysis; and 3) prototyping the proposed framework on service robots and mobile devices for evaluation of the proposed framework's performance in complex real-world applications over a variety of user studies. The project will include education and outreach activities advancing AI in undergraduate research, diversity enhancement, Entrepreneurial Mindset (EM) education, and K-12 classrooms, and will include workshops to introduce AI and deep learning to professionals in non-CS professions such as medical research and pathology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814955","III: Small: Collaborative Research: Building Subjective Knowledge Bases by Modeling Viewpoints","IIS","Info Integration & Informatics","09/01/2018","08/03/2018","Brendan O'Connor","MA","University of Massachusetts Amherst","Standard Grant","Maria Zemankova","08/31/2021","$249,978.00","","brenocon@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7364","7364, 7923","$0.00","This project will develop and empirically evaluate methods for creating subjective knowledge bases: databases of opinions and viewpoints as they are asserted by individuals in books, web forums, and social media. While most knowledge base research seeks to extract real-world truth from text, many factual assertions are either about inherently subjective propositions (such as ""apples are delicious"") or are non-subjective assertions that happen to contradict other belief holders or even consensus reality (such as ""the Earth is flat""). This project pioneers new methods to automatically extract expressions of opinions and viewpoints from a textual corpus and use those assertions to build a subjective knowledge base that can accommodate contradictory and conflicting statements from different authors. Such a subjective knowledge base will help researchers answer a range of questions: What contradictory claims are being made in historical books, or contemporary social media? What propositions does a particular ideological community hold, and are they compatible with, or contradictory to, those held by other communities? This project lays the foundation for understanding a broad range of phenomena that can be seen as conflicts between coherent viewpoints.  The resulting computational models will lay the groundwork for intelligent systems that are robust with respect to the way in which propositions are used in the real world; as applications in artificial intelligence are being deployed more and more in social contexts, this research will inform these methods with more nuanced information about the diversity of human viewpoints.  This work will also include a substantial educational component, incorporating human context into algorithm design in undergraduate STEM education and broadening the use of natural language processing and machine learning across a range of disciplines.<br/><br/>While previous work has focused on the primary task of identifying degrees of certainty (belief, viewpoints) in text, the primary contribution of this project will be modeling the structure of individual extracted viewpoints through the variables of the viewpoint holders and the viewpoint communities to which they belong. Models for building subjective knowledge bases accept subjective claims as fully semantic relational propositions, like recent research in open information extraction. However, instead of relying on the typical assumption of cross-document consensus, these models will embrace the simultaneous presence of contradictory claims across different author groups or even within the writings of the same individual. Major project components include: developing and refining broad-domain part-of-speech and syntactic parsing to be effective across both social media and historical books; using these tools to support author-centric latent-variable models of structured knowledge, which infers latent positions for both propositions and their viewpoint-holders; and improving the model with linguistic analysis of factuality and viewpoint (belief) commitment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813470","III: Small: Collaborative Research: Building Subjective Knowledge Bases by Modeling Viewpoints","IIS","Info Integration & Informatics","09/01/2018","02/11/2019","David Bamman","CA","University of California-Berkeley","Standard Grant","Maria Zemankova","08/31/2021","$266,000.00","","dbamman@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7364","7364, 7923, 9251","$0.00","This project will develop and empirically evaluate methods for creating subjective knowledge bases: databases of opinions and viewpoints as they are asserted by individuals in books, web forums, and social media. While most knowledge base research seeks to extract real-world truth from text, many factual assertions are either about inherently subjective propositions (such as ""apples are delicious"") or are non-subjective assertions that happen to contradict other belief holders or even consensus reality (such as ""the Earth is flat""). This project pioneers new methods to automatically extract expressions of opinions and viewpoints from a textual corpus and use those assertions to build a subjective knowledge base that can accommodate contradictory and conflicting statements from different authors. Such a subjective knowledge base will help researchers answer a range of questions: What contradictory claims are being made in historical books, or contemporary social media? What propositions does a particular ideological community hold, and are they compatible with, or contradictory to, those held by other communities? This project lays the foundation for understanding a broad range of phenomena that can be seen as conflicts between coherent viewpoints.  The resulting computational models will lay the groundwork for intelligent systems that are robust with respect to the way in which propositions are used in the real world; as applications in artificial intelligence are being deployed more and more in social contexts, this research will inform these methods with more nuanced information about the diversity of human viewpoints.  This work will also include a substantial educational component, incorporating human context into algorithm design in undergraduate STEM education and broadening the use of natural language processing and machine learning across a range of disciplines.<br/><br/>While previous work has focused on the primary task of identifying degrees of certainty (belief, viewpoints) in text, the primary contribution of this project will be modeling the structure of individual extracted viewpoints through the variables of the viewpoint holders and the viewpoint communities to which they belong. Models for building subjective knowledge bases accept subjective claims as fully semantic relational propositions, like recent research in open information extraction. However, instead of relying on the typical assumption of cross-document consensus, these models will embrace the simultaneous presence of contradictory claims across different author groups or even within the writings of the same individual. Major project components include: developing and refining broad-domain part-of-speech and syntactic parsing to be effective across both social media and historical books; using these tools to support author-centric latent-variable models of structured knowledge, which infers latent positions for both propositions and their viewpoint-holders; and improving the model with linguistic analysis of factuality and viewpoint (belief) commitment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819477","SBIR Phase I:  A Controversy Detection Signal for Finance","IIP","SBIR Phase I","06/15/2018","01/28/2019","Shiri Dori-Hacohen","MA","Automated Controversy Detection, Inc.","Standard Grant","Peter Atherton","03/31/2019","$225,000.00","","shiri@controversies.info","10 Oak Dr. Apt A","Granby","MA","010339767","6177925067","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to use controversy detection to support financial institutions' ability to reduce their risks and increase profits. This is part of a growing trend towards ""alternative data"" products relying on artificial intelligence and machine learning. Besides the financial industry, there are numerous potential applications of controversy detection technology in a variety of market verticals, such as crisis management, defense applications, and advertising technology. Beyond commercial applications, there is scope for social impact by opening analysis and explanatory power of controversies to individual users. Controversies have a massive impact on civic society and the so-called ""filter bubble"" exacerbates polarization, both political and otherwise; fake news on both sides of the political spectrum has recently captured public attention and generated political concern. Positive impact on society from commercializing this technology includes helping users become better informed and more capable of critically evaluating the often-overwhelming stream of online content. Proving the feasibility of this innovation in a highly quantifiable space such as finance could answer a customer need in that space and create new jobs for the economy, while enabling social good applications that can improve civic society.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project relies on sophisticated machine learning and information retrieval techniques to automatically detect controversial topics. The initial data were collected at the University of Massachusetts Amherst, using NSF-supported research, which recognized controversy by mapping the text of a webpage to algorithmically-identified controversial topics. Research has demonstrated that controversy cannot be detected using existing methods of sentiment analysis, a widely-adopted natural language processing method. This project bridges the gap between the current capabilities of this nascent technology and the clear user need in the financial domain. It will evaluate the feasibility of controversy detection by applying a real-time controversy detection signal to financial data to reduce risk and increase returns.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822986","CRI: CI-SUSTAIN: Collaborative Research: Sustaining Lemur Project Resources for the Long-Term","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2018","08/17/2018","James Allan","MA","University of Massachusetts Amherst","Standard Grant","Wendy Nilsen","08/31/2021","$376,737.00","","allan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7359","7359","$0.00","For more than a decade, the software, datasets, and online services developed and provided by the Lemur Project have supported and enabled a large body of academic and commercial research on search engines, information retrieval, and other areas of computer science that analyze and process human language. This project makes critical enhancements to Lemur Project infrastructure, operates the infrastructure for another three years, and positions it for long-term sustainability. As part of the enhancements, the Galago search engine is enhanced to provide stronger integration of neural networks and other machine learning methods. A new dataset, ClueWeb2020, is developed to replace the widely-used ClueWeb09 and ClueWeb12 datasets. These investments will support advanced research for the next decade. The advanced search capabilities developed for the project's open-source Indri and Galago search engines, which are widely used for research, are added to the open-source Lucene search engine, which is widely used by industry. New software applications are developed to simplify migration between Lemur Project search engines and Lucene. These investments improve the state-of-the-art of software important to industry and enable researchers to migrate research to more widely-used software. The Lemur Project's research infrastructure attracted a substantial research user community because it easily enables leading-edge research. These enhancements enable researchers in information retrieval and related areas to carry out a much broader range of experiments and to share their results. Research and industry development supported by the new Lemur Project software will create a new generation of more capable search engines for a variety of tasks.<br/><br/>The project is organized around three types of activities: Sustaining software, sustaining datasets, and operation. The project achieves long-term software sustainability by adding support for Indri and Galago functionality and creating integration and migration paths with the open-source Lucene search engine, which has large user and volunteer-developer communities. Research done with Galago or Indri will thus be reproducible in Lucene and more accessible to Lucene's industry users. The project also extends the Galago Application Programming Interface to support the newest developments in neural network (deep learning) document ranking technologies, which now are being studied widely and expected in a state-of-the-art research system. It broadens the utility of Ranklib by supporting neural algorithms for better comparison with high quality learning to rank approaches, and broadens the utility of the Sifaka text mining application with support for additional document and machine learning formats. The older ClueWeb09 and ClueWeb12 datsets are superseded by a new ClueWeb2020 dataset that is designed to last a decade and support research on newer learning-to-rank and neural network (deep learning) ranking algorithms. The project maintains and operates the existing infrastructure, in the form of software maintenance and support; dataset licensing and distribution; and operation of online search services. The new Lemur Project infrastructure supports a broad range of Information Retrieval research, for example, research on retrieval models; how to train learned rankers; use of semi-structured knowledge bases; result diversification; query optimization; and distributed search. In particular, it greatly improves support for research on learned and neural (deep learning) ranking algorithms, which have become important research topics in recent years. The ClueWeb datasets are used by a broad human language technologies research community. This project makes enhancements that sustain this infrastructure for the research community for at least the next decade.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1803423","The 2018 NAACL Student Research Workshop","IIS","Robust Intelligence","04/01/2018","03/07/2018","Samuel Bowman","NY","New York University","Standard Grant","Tatiana Korelsky","03/31/2020","$15,000.00","","sb6065@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7495","7495, 7556","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization for computational linguistics and natural language processing, which in turn is one of the primary application areas for research in machine learning and artificial intelligence. The annual meeting of the North American chapter of the ACL (NAACL) is one of the most prestigious and selective international conferences in these fields. The proceedings of this meeting is among the most cited publications in computational linguistics, and the meeting itself is the most important meeting for North American researchers in the field on years when it is held. This project will subsidize travel, conference and housing expenses of students selected to participate in the NAACL Student Research Workshop, which will take place during the main NAACL conference June 1?6, 2018 in New Orleans, LA. The Student Research Workshop helps create a new generation of researchers with a more thorough understanding of their field, with connections and collaborations across institutions, and with innovative and exciting research programs. This contributes to America's pool of researchers with the needed scientific and engineering knowledge and skills. The workshop encourages a spirit of collaborative research and builds a supportive environment for a new generation of computational linguists. <br/><br/>The Student Research Workshop solicits two categories of submissions: research papers and research proposals. The research proposal can have only one author who must be a student. The research papers can have multiple authors, with the first author being a student (at either the graduate or undergraduate level). The workshop is a venue for students to receive constructive critical feedback on their work from experts outside of their institution, and to connect with other students and senior researchers in their field. The students gain exposure by presenting their work earlier than they would otherwise (i.e., in a form not yet ready for the main conference). This is particularly valuable for students from smaller institutions and undergraduate students. In addition, the workshop is organized and run by students. The student organizers gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816753","RI:Small:NSF-BSF: Computational and Statistical Tradeoffs in Inverse Problems using Deep Learning","IIS","Robust Intelligence","08/15/2018","08/14/2018","Joan Bruna Estrach","NY","New York University","Standard Grant","Rebecca Hwa","07/31/2021","$499,772.00","","jb4496@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7495","014Z, 7495, 7923","$0.00","In many real life scenarios one may want to measure some information about a certain object, but cannot measure directly, but only indirectly. For example, if a doctor wants to see the lungs of a patient he cannot do it by just looking at them, but needs to use a special machine such as a CT scanner. This scanner emits radiation on the patient and then measures the reflected radiation to gain information about the shape of the lungs. There are many other examples often encountered in photography, telecommunication, or navigation and more that share the same challenge of inferring signals of interest from indirect, noisy measurements: these are known as inverse problems. While many techniques exist for reconstructing the original object from its measurements, one of the main challenges is to do it in a timely manner. This research aims at providing novel computationally efficient techniques for solving inverse problems. In particular, it explores the ability of deep neural networks to accelerate and improve traditional techniques. This research focuses on both the theoretical aspects of such methods, needed for example to certify that the reconstructed image from a patient CT scan using a deep neural network is close to the underlying original scan, and their applications to tomography and seismic imaging. This research provides transformative multidisciplinary educational opportunities, developed in the Center for Data Science, New York University, bringing together signal processing, statistics and machine learning in new graduate level courses. Thanks to the wide range of relevant applications, this research also provides unique outreach activities to K12 and highschool students, as well as minority students through the BSF collaboration.<br/><br/>Inverse problems occur in many fields ranging from signal processing to machine learning and are relevant to many domains such as medicine (e.g., getting an image in computer tomography) and physics (e.g., calculating the density of the earth from measurements of its gravity fields). Many solvers have been developed for these type of problems, leveraging specific high-dimensional statistical models for the signals of interest. However, there are three main challenges that persist in current existing solutions. First, many of them are computationally demanding and therefore not applicable to many applications that require a solution in a timely manner. Second, many inverse problems pose non-convex optimization problems that do not have an efficient solution at all. Third, current methodology do not make optimal use of available training examples, which, depending on the application, ranges from tens of instances to millions. This project provides novel theoretical foundations for the neural network based solver on general inverse problems, extends this theory to include non-linear problems that do not admit convex solutions, as well as graph-structured problems, and demonstrates its efficiency on challenging applications including low-dose Computed Tomography, Seismic spike de- convolution, and Quantum State Tomography. It specifically builds on the combination of two recent tools developed by the PI and his Israeli collaborator, which provide complimentary insights on the mechanisms underpinning the neural network acceleration. This novel theoretical framework enables a series of important extensions and generalizations, such as non-linear inverse problems, partially known measuring operators, and distributed optimization. As part of this research, several educational outreach activities are conducted to make neural networks more accessible to the broad student community, including K12, highschool and minority students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835309","NCS-FO: Leveraging Deep Probabilistic Models to Understand the Neural Bases of Subjective Experience","DGE","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","08/15/2018","04/10/2019","Jan-Willem van de Meent","MA","Northeastern University","Standard Grant","Gregg Solomon","07/31/2021","$999,375.00","Jennifer Dy, Ajay Satpute, Sarah Ostadabbas, John Hutchinson","j.vandemeent@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","EHR","7980, 8624","8089, 8091, 8551, 8817","$0.00","An interdisciplinary team of researchers at Northeastern University, including experts in deep learning and probabilistic programming, the cognitive neuroscience of emotion, computational methods in neuroimaging, probabilistic modeling, and computer vision, will engage in an integrated computational and neuroscientific research effort to understand the neural bases of subjective experience. Different individuals experience the same events in vastly different ways, owing to their unique histories and psychological dispositions. For someone with social anxieties, the mere thought of leaving the home can induce a feeling of panic. Conversely, an experienced mountaineer may feel quite comfortable balancing on the edge of a cliff. This variation of perspectives is captured by the term subjective experience. Despite its centrality and ubiquity in human cognition, it remains unclear how to model the neural bases of subjective experience. The proposed work will develop new techniques for statistical modeling of individual variation and apply these techniques to a neuroimaging study of the subjective experience of fear. Together, these two lines of research will yield fundamental insights into the neural bases of fear experience. More generally, the developed computational framework will provide a means of comparing different mathematical hypotheses about the relationship between neural activity and individual differences. This will enable investigation of a broad range of phenomena in psychology and cognitive neuroscience. The work has further implications for other fields in which there are individual differences in subject experience, such behavioral medicine and the study of stress in the workplace or STEM education and the study of math or test anxiety. This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE). <br/><br/>The project will develop a new computational framework for modeling individual variation in neuroimaging data and use this framework to investigate the neural bases of one powerful and societally meaningful subjective experience, namely, of fear. Fear is a particularly useful assay because it involves variation across situational contexts (spiders, heights, and social situations), and dispositions (arachnophobia, acrophobia, and agoraphobia) that combine to create subjective experience. In the proposed neuroimaging study, participants will be scanned while watching videos that induce varying levels of arousal. To characterize individual variation in this neuroimaging data, the investigators will leverage advances in deep probabilistic programming to develop probabilistic variants of factor analysis models. These models infer a low-dimensional feature vector, also known as an embedding, for each participant and stimulus. A simple neural network models the relationship between embeddings and the neural response. This network can be trained in a data-driven manner and can be parameterized in a variety of ways, depending on the experimental design, or the neurocognitive hypotheses that are to be incorporated into the model. This provides the necessary infrastructure to test different neural models of fear. Concretely, the investigators will compare a model in which fear has its own unique circuit (i.e., neural signature or biomarker) to subject- or situation-specific neural architectures. More generally, the developed framework can be adapted to model individual variation in neuroimaging studies in other experimental settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822975","CRI: CI-SUSTAIN: Collaborative Research: Sustaining Lemur Project Resources for the Long-Term","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2018","08/17/2018","Jamie Callan","PA","Carnegie-Mellon University","Standard Grant","Wendy Nilsen","08/31/2021","$621,253.00","","callan@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7359","7359","$0.00","For more than a decade, the software, datasets, and online services developed and provided by the Lemur Project have supported and enabled a large body of academic and commercial research on search engines, information retrieval, and other areas of computer science that analyze and process human language. This project makes critical enhancements to Lemur Project infrastructure, operates the infrastructure for another three years, and positions it for long-term sustainability. As part of the enhancements, the Galago search engine is enhanced to provide stronger integration of neural networks and other machine learning methods. A new dataset, ClueWeb2020, is developed to replace the widely-used ClueWeb09 and ClueWeb12 datasets. These investments will support advanced research for the next decade. The advanced search capabilities developed for the project's open-source Indri and Galago search engines, which are widely used for research, are added to the open-source Lucene search engine, which is widely used by industry. New software applications are developed to simplify migration between Lemur Project search engines and Lucene. These investments improve the state-of-the-art of software important to industry and enable researchers to migrate research to more widely-used software. The Lemur Project's research infrastructure attracted a substantial research user community because it easily enables leading-edge research. These enhancements enable researchers in information retrieval and related areas to carry out a much broader range of experiments and to share their results. Research and industry development supported by the new Lemur Project software will create a new generation of more capable search engines for a variety of tasks.<br/><br/>The project is organized around three types of activities: Sustaining software, sustaining datasets, and operation. The project achieves long-term software sustainability by adding support for Indri and Galago functionality and creating integration and migration paths with the open-source Lucene search engine, which has large user and volunteer-developer communities. Research done with Galago or Indri will thus be reproducible in Lucene and more accessible to Lucene's industry users. The project also extends the Galago Application Programming Interface to support the newest developments in neural network (deep learning) document ranking technologies, which now are being studied widely and expected in a state-of-the-art research system. It broadens the utility of Ranklib by supporting neural algorithms for better comparison with high quality learning to rank approaches, and broadens the utility of the Sifaka text mining application with support for additional document and machine learning formats. The older ClueWeb09 and ClueWeb12 datsets are superseded by a new ClueWeb2020 dataset that is designed to last a decade and support research on newer learning-to-rank and neural network (deep learning) ranking algorithms. The project maintains and operates the existing infrastructure, in the form of software maintenance and support; dataset licensing and distribution; and operation of online search services. The new Lemur Project infrastructure supports a broad range of Information Retrieval research, for example, research on retrieval models; how to train learned rankers; use of semi-structured knowledge bases; result diversification; query optimization; and distributed search. In particular, it greatly improves support for research on learned and neural (deep learning) ranking algorithms, which have become important research topics in recent years. The ClueWeb datasets are used by a broad human language technologies research community. This project makes enhancements that sustain this infrastructure for the research community for at least the next decade.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749833","CAREER:Towards Perceptual Agents That See and Reason Like Humans","IIS","Robust Intelligence","06/01/2018","06/01/2018","Subhransu Maji","MA","University of Massachusetts Amherst","Standard Grant","Jie Yang","05/31/2023","$545,586.00","","smaji@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7495","075Z, 1045, 7495","$0.00","Recent advancements in computer vision systems have enabled their widespread deployment in areas like social media, healthcare, robotics, and ecology, among many others. While such applications hold exceptional promise for improving our well-being and advancing scientific discovery, the ubiquity of these intelligent systems presents new technical, social, and cultural challenges for their wide-scale adoption. This project leads an integrated effort of research, teaching, and outreach to address some of these challenges. The project develops architectures that are substantially more accurate and capable of extracting detailed information from perceptual data across different modalities. An emphasis of this work is to develop computer vision systems that can reason about data in ways that are interpretable by humans. This project also promotes diversity, engages high school, undergraduate, and graduate students in research activities, and fosters collaborations with industry and researchers in areas such as ecology and biology through workshops.<br/><br/>This research explores new directions that improve the capabilities of visual perception and reasoning systems for analyzing image data, spatio-temporal data, and depth data. The research develops a novel class of graph-based and factorized architectures for 3D shape and spatio-temporal analysis that provide better tradeoffs between computational cost, memory overhead, and accuracy than existing models. The research develops weakly supervised techniques for learning shape and motion representations from large amounts of unlabeled data. The research also develops a novel class of techniques for transforming visual data to semantic representations such as attributes, natural language, and symbolic programs. These techniques will improve the interpretability of machine learning models and enable collaborative learning and inference between humans and AI agents.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747397","Doctoral Dissertation Research: Information, Public Opinion, and Behavior","SES","Political Science DDRIG","07/15/2018","07/26/2018","Mary Gallagher","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Brian Humes","06/30/2019","$15,733.00","Blake Miller","metg@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","SBE","009Y","1371, 9179","$0.00","This project explores how political information manipulation in authoritarian regimes can affect the behaviors, attitudes, and opinion of individuals. This project is important to our understanding of how authoritarian regimes gain popular support and maintain control over populations using social media and big data. Planned survey experiments will measure the effect of political information manipulation tactics (censorship, astroturfing, and digital repression).   As much of social interaction has moved into a digital space, governments have greater access to personal and social data. At the same time, the fields of artificial intelligence and machine learning have experienced unprecedented growth along with a similarly rapid growth in the power of microprocessors. These advances have given governments better tools to digest, process, and make informed decisions based on these massive stores of personal data. This project is a first step toward understanding how these common data-driven information manipulation tactics work and impact individuals in authoritarian governments, essential knowledge for scholars of modern politics and public opinion in a highly networked and digital age.<br/><br/>In this project, the effects of experiencing political information manipulation on individual behaviors and opinion in authoritarian regimes are explored using both survey experiments and large-N correlational analyses. It is hypothesized that government information manipulation that is covert---when the identity of the government as information manipulator obscured---will increase the magnitude of individual-level effects in a direction favorable to the government. More specifically, when compared to more overt government information manipulation, covert information manipulation will be more persuasive, will more dramatically increase positive perceptions of the state, will more dramatically decrease one's sense of political efficacy, and will decrease one's willingness to express opinions more than overt manipulation. To test these hypotheses, a pseudo experiment will be run using a large database of syndicated articles identified using automated near-duplicate detection. Syndicated state media articles, appearing on different news websites with identical text but different comments, will be compared across a treatment group (articles with comments from astroturfers and ordinary users) and a control group (articles with comments from only ordinary users). Variables targeting each of the hypotheses above will be drawn from the text using statistical and natural language processing methods. Additionally, three survey experiments will be conducted to test, respectively, the effect of censorship, astroturfing, and digital repression in overt and covert forms. This work is critical to understand how modern authoritarian regimes leverage social media to guide public opinion, mobilize regime support, and demobilize opponents.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806415","RUI: Digging Deep for New Physics with the CMS Experiment","PHY","HEP-High Energy Physics","08/15/2018","06/04/2020","Julie Hogan","MN","Bethel University","Continuing Grant","Saul Gonzalez","07/31/2021","$208,260.00","","j-hogan@bethel.edu","3900 Bethel Drive","Saint Paul","MN","551126999","6126386400","MPS","1221","075Z, 7483, 9229","$0.00","A major scientific achievement of the 20th century was the development of the Standard Model of particle physics. The Standard Model is a successful theory, agreeing with decades of experimental observations involving weak, electromagnetic, and strong interactions. The Higgs boson discovery at the Large Hadron Collider (LHC) at CERN completes the suite of predicted Standard Model particles and provides a mechanism for generating elementary particle masses. However, the current formulation of the Standard Model does not account for the observed value of the Higgs boson mass, making discovery of physics beyond the Standard Model a primary goal of the LHC experiments, including those being carried out in this project involving the Compact Muon Solenoid (CMS).<br/><br/>This work will focus on searches for new physics, as well as upgrade developments toward the silicon-based outer tracker for the CMS experiment during the High-Luminosity LHC run in the mid-2020s. This research will also engage undergraduate students in CMS data analysis and detector hardware projects at Bethel University, Fermilab, and CERN. The primary analysis goal is to search for vector-like quark (VLQ) pair production with the record-breaking CMS dataset. Several deep machine learning algorithms will be developed to identify hadronic decays of bottom quarks, top quarks, W, Z, and Higgs bosons, which are all reconstructed as jets with unique internal structure. With these deep learning techniques, VLQ pair production searches can be dramatically improved to include identification and mass reconstruction. Specifically, this work will use image recognition machine learning techniques to develop jet identification algorithms for HL-LHC studies. Additionally, extensive development will be made on design features and assembly procedures for upgraded silicon tracker modules. The Bethel CMS group will characterize silicon sensors with updates to an existing probe station and will participate in module production tests at Fermilab. The Bethel group is also engaged with several outreach and education activities, reaching high school students and the general public through tours of detector exhibits and preparing new students and collaborators for CMS analysis via an annual Data Analysis School and hands-on summer tutorials. The Bethel group will also develop inquiry-based Open Data analysis exercises appropriate for upper-level modern physics courses and/or advanced laboratory courses to be shared with other universities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814105","NeTS: Small: Collaborative Research: Protocol Validation using Minimally Supervised Semantic Interpretation of Text","CNS","Special Projects - CNS","10/01/2018","08/15/2018","Dan Goldwasser","IN","Purdue University","Standard Grant","Ann Von Lehmen","09/30/2021","$250,000.00","","dgoldwas@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1714","075Z, 7923","$0.00","The networks that comprise the Internet are fundamental to our society, facilitating access to medical and financial services, supporting critical infrastructure such as the power grid, and enabling emergent services such as those provided by autonomous cars and IoT (Internet of Things) devices. Network behavior is dictated by a set of instructions, or protocols, developed and tested over time.  Such protocols must operate correctly and comply with requirements that are usually described in a document(s), i.e., in a textual representation.  If they do not operate properly, the performance and security of a network could be compromised.  The goal of this project is to increase assurance in network protocols, specifically in their compliance to specified rules, in their inter-operability and in their functionality.  This project will accomplish this via a novel scheme to perform protocol testing through automated extraction of protocol requirements from their textual specification.  This would mark a significant advance in the field, towards automated mechanisms that assure that network protocols are behaving as we expect them to, making networks more reliable and secure.    <br/><br/>This multidisciplinary project combines expertise from natural language processing and computer networks to create methodologies, frameworks, a knowledge base, and tools for protocol validation for (1) compliance checking, (2) bug finding, and (3) interoperability testing.   The general approach is to apply machine learning, semantic parsing and information extraction  techniques to structured text (RFCs, internet-drafts) and unstructured text (blogs, forums, and bug reports), and create a knowledge base about the protocols, containing formal information such as  message formats, protocol state machine, constraints,  and semi-formal information such as temporal properties, tuning conditions and parameters, changes from one version to another, or known bugs. This information is organized into a knowledge base and used to validate protocol implementations through protocol fuzzying, program analysis, software model checking, and measurement methods, to check whether protocols are compliant with their specifications, to detect semantic bugs dependent on intrinsic protocol properties, or check for interoperability issues between different versions, or protocol stacks.  This work is guided by protocols from three representative domains -- TCP variants, the SDN ecosystem, and IoT smart home environment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815219","NeTS: Small: Collaborative Research: Protocol Validation using Minimally Supervised Semantic Interpretation of Text","CNS","Special Projects - CNS, Networking Technology and Syst","10/01/2018","08/15/2018","Cristina Nita-Rotaru","MA","Northeastern University","Standard Grant","Ann Von Lehmen","09/30/2021","$249,914.00","","c.nitarotaru@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1714, 7363","075Z, 7923, 9102","$0.00","The networks that comprise the Internet are fundamental to our society, facilitating access to medical and financial services, supporting critical infrastructure such as the power grid, and enabling emergent services such as those provided by autonomous cars and IoT (Internet of Things) devices. Network behavior is dictated by a set of instructions, or protocols, developed and tested over time.  Such protocols must operate correctly and comply with requirements that are usually described in a document(s), i.e., in a textual representation.  If they do not operate properly, the performance and security of a network could be compromised.  The goal of this project is to increase assurance in network protocols, specifically in their compliance to specified rules, in their inter-operability and in their functionality.  This project will accomplish this via a novel scheme to perform protocol testing through automated extraction of protocol requirements from their textual specification.  This would mark a significant advance in the field, towards automated mechanisms that assure that network protocols are behaving as we expect them to, making networks more reliable and secure.    <br/><br/>This multidisciplinary project combines expertise from natural language processing and computer networks to create methodologies, frameworks, a knowledge base, and tools for protocol validation for (1) compliance checking, (2) bug finding, and (3) interoperability testing.   The general approach is to apply machine learning, semantic parsing and information extraction  techniques to structured text (RFCs, internet-drafts) and unstructured text (blogs, forums, and bug reports), and create a knowledge base about the protocols, containing formal information such as  message formats, protocol state machine, constraints,  and semi-formal information such as temporal properties, tuning conditions and parameters, changes from one version to another, or known bugs. This information is organized into a knowledge base and used to validate protocol implementations through protocol fuzzying, program analysis, software model checking, and measurement methods, to check whether protocols are compliant with their specifications, to detect semantic bugs dependent on intrinsic protocol properties, or check for interoperability issues between different versions, or protocol stacks.  This work is guided by protocols from three representative domains -- TCP variants, the SDN ecosystem, and IoT smart home environment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751402","CAREER: Toward Spatial-Temporal Architectures with Deformable and Interpretable Convolutions","IIS","Robust Intelligence","04/01/2018","05/11/2020","Fuxin Li","OR","Oregon State University","Continuing Grant","Jie Yang","03/31/2023","$295,443.00","","lif@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","1045, 7495","$0.00","Artificial neural networks have successfully been applied to analyzing visual imagery. The goal of this project is to build a convolutional neural network (CNN) that can scale and deform automatically in order to be able to be invariant to object size and pose.<br/>Currently, CNNs cannot even perform well on an image rescaled twice or half as large, if not trained on the re-scaled image. This leads to a lot of redundancies in the model and unnecessary over-complication of the architecture. This project explores approaches to automatically figure out the correct scaling, as well as other transformations, from visual objects in images and videos. The proposed methods will also make convolutional neural networks easier to interpret, and to reduce the amount of data needed to train a network.<br/>Besides normal computer vision benchmarks, the research team evaluates the approach with collaborations to apply the technologies to different applications, such as forestry and tumor-cell morphology, The educational goal of this project involves developing a new ?what-you-see-is-what-you-get? (WYSIWYG) deep learning toolbox that enables people without much programming and mathematical skills to utilize deep learning for data analysis. The research team also plans to outreach to high schools and community colleges to introduce more than 100 students to deep learning and visual object recognition.  <br/><br/>This research develops spatial-temporal CNNs that scale and deform automatically, hence able to concisely represent object recognition models that generalize better under invariant and equivariant transformations unseen in the training set. The project explores novel auto-scaling and multi-deformable convolutional network architectures that utilize parametric motion fields to automatically locate the correct deformations of a visual object for each convolutional filter. In order to learn the motion fields from video, the research team uses a Siamese convolutional-deconvolutional network predicting boundaries in two consecutive frames, and utilizes an output-to-output feedback loop to deduce boundary motion.  The research team applies this approach to video segmentation and uses it to generate annotations for a weakly supervised learning of the motion fields. The approach is evaluated on several tasks with limited annotations, such as video segmentation, multi-target tracking and object classification and detection in videos under unseen deformations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816380","CHS: Small: Teachable Object Recognizers for the Blind","IIS","HCC-Human-Centered Computing","08/15/2018","08/13/2018","Hernisa Kacorri","MD","University of Maryland College Park","Standard Grant","Ephraim Glinert","07/31/2021","$497,653.00","","hernisa@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7367","075Z, 7367, 7923","$0.00","Identifying objects, from packages of food to items of clothing, is an everyday task that we perform predominantly using our sense of sight.  Blind persons, for whom sighted help is not available, attempt such tasks using a combination of senses, strategies, and ad hoc organizing systems.  In recent years, members of the blind community have been early adopters of mobile apps that use image recognition for identifying objects.   However, such solutions currently have limited use for many objects of interest, because image recognizers cannot provide enough granularity to distinguish among all possible objects of interest across all users.  They also tend to be trained on images taken by sighted people with different background clutter, scale, viewpoints, occlusion, and image quality than in photos taken by blind users.  The goal of this research is to empower blind users to customize the recognition task to their objects of interest, photo-taking strategies, and environment, through a new approach called teachability which holds the promise of allowing end users that are non-experts to provide the training examples for the machine learning models in these applications.  Specifically, a teachable object recognizer (TOR) app will be designed, deployed and publicly released that blind users can train by providing labels along with a few examples of their objects through a smartphone's camera.  If successful, project outcomes will have broad impact by changing the nature of smart assistive technologies by empowering people with disabilities to define the functionality of such technologies, especially for small recognition tasks.<br/><br/>This project addresses the data scarcity issue in accessibility through teachability, where end users teach models pretrained on more available yet less relevant data, with fewer but more pertinent data specific to their needs.  The work will examine the concept of teachability in the context of object recognition for the blind, and will investigate whether accessibility research can leverage advances in computer vision with limited data from blind users.  Questions to be explored will include: How to best explain such intelligent systems to users for higher quality training data? What is the best way to measure their efficacy? What design parameters, sensing modalities, interactions, and algorithms are most influential on their success? The project will include a variety of research methods: surveys, participatory design sessions, prototype usability testing, lab-based user studies, and longitudinal real-world evaluation with blind users.  Using a working prototype mobile application on teachable object recognition, it will investigate accessible interactions on learning-to-train and examine the underlying mechanisms by which robustness and scalability of such teachable assistive technologies can be improved.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823201","CRI: CI-EN: Collaborative Research: mResearch: A platform for Reproducible and Extensible Mobile Sensor Big Data Research","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2018","09/17/2018","James Rehg","GA","Georgia Tech Research Corporation","Standard Grant","Ephraim Glinert","09/30/2021","$225,000.00","","rehg@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1714, 7359","075Z, 7359","$0.00","The Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K) has developed open-source software for smart phones and cloud. Scientists use MD2K software to develop and test algorithms to monitor health, wellness, and work productivity via wearable sensors. The mResearch project is aimed at assisting Computer and Information Science and Engineering (CISE) researchers. The mResearch project will significantly enhance MD2K software and integrate Internet-of-Things (IoT) devices. The enhanced MD2K software will accelerate research in sensors design, mobile computing, privacy, analytics (especially machine learning and deep learning), and visualization. mResearch will enable CISE researchers to easily deploy their contributed software in scientific studies for health, smart homes, and workplace. The resulting discoveries and tools will help individuals improve their health, wellness, and work productivity.<br/><br/>MD2K has developed open-source mobile sensor big data software platforms mCerebrum for smartphones and Cerebral Cortex for the cloud. This scalable and generalizable infrastructure is used for collecting, analyzing, and sharing high-frequency, mobile sensor data and associated labels in the context of scientific field studies. In particular, it supports the development and validation of models and algorithms for inferring markers of health, wellness, and productivity, and their associated risk factors. It has already been used at eleven sites across the country to collect over 300 terabytes of mobile sensor data in the field setting from over 2,000 participants. It has resulted in new computational models for the detection of conversation, smoking, eating, craving, stress, and cocaine use. The mResearch project is making five significant infrastructure enhancements to the MD2K infrastructure to assist CISE researchers in mobile sensor development, mobile computing, privacy, analytics, visualization, and participant engagement. First, it will enable data analytic workflow management across multiple layers of the system to enable reproducible and extensible experimentation. Second, it will allow encapsulation of data sources to provide convenient and responsible access to them in data analytic workflows. Third, it will facilitate cloud-assisted complex, real-time analytics for personalizing mobile interventions and improving engagement. Fourth, simulators will be developed with the ability to feed stored data into the platform at various points to enable research on system components and properties such as data compression, transfer and storage, as well as the scalability of data analytics. Finally, Internet-of-Things (IoT) devices and services will be integrated. With these five enhancements, the MD2K software will provide a complete, open, and modularized architecture. It will include all aspects of sensor data collection, data processing algorithms, cloud-based machine learning, and IoT integration. The enhanced MD2K software will facilitate reproducible and extensible CISE research with high-frequency mobile sensor data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823070","CRI: CI-EN: Collaborative Research: mResearch: A platform for Reproducible and Extensible Mobile Sensor Big Data Research","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2018","09/17/2018","Emre Ertin","OH","Ohio State University","Standard Grant","Ephraim Glinert","09/30/2021","$224,830.00","","ertin.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","1714, 7359","075Z, 7359","$0.00","The Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K) has developed open-source software for smart phones and cloud. Scientists use MD2K software to develop and test algorithms to monitor health, wellness, and work productivity via wearable sensors. The mResearch project is aimed at assisting Computer and Information Science and Engineering (CISE) researchers. The mResearch project will significantly enhance MD2K software and integrate Internet-of-Things (IoT) devices. The enhanced MD2K software will accelerate research in sensors design, mobile computing, privacy, analytics (especially machine learning and deep learning), and visualization. mResearch will enable CISE researchers to easily deploy their contributed software in scientific studies for health, smart homes, and workplace. The resulting discoveries and tools will help individuals improve their health, wellness, and work productivity.<br/><br/>MD2K has developed open-source mobile sensor big data software platforms mCerebrum for smartphones and Cerebral Cortex for the cloud. This scalable and generalizable infrastructure is used for collecting, analyzing, and sharing high-frequency, mobile sensor data and associated labels in the context of scientific field studies. In particular, it supports the development and validation of models and algorithms for inferring markers of health, wellness, and productivity, and their associated risk factors. It has already been used at eleven sites across the country to collect over 300 terabytes of mobile sensor data in the field setting from over 2,000 participants. It has resulted in new computational models for the detection of conversation, smoking, eating, craving, stress, and cocaine use. The mResearch project is making five significant infrastructure enhancements to the MD2K infrastructure to assist CISE researchers in mobile sensor development, mobile computing, privacy, analytics, visualization, and participant engagement. First, it will enable data analytic workflow management across multiple layers of the system to enable reproducible and extensible experimentation. Second, it will allow encapsulation of data sources to provide convenient and responsible access to them in data analytic workflows. Third, it will facilitate cloud-assisted complex, real-time analytics for personalizing mobile interventions and improving engagement. Fourth, simulators will be developed with the ability to feed stored data into the platform at various points to enable research on system components and properties such as data compression, transfer and storage, as well as the scalability of data analytics. Finally, Internet-of-Things (IoT) devices and services will be integrated. With these five enhancements, the MD2K software will provide a complete, open, and modularized architecture. It will include all aspects of sensor data collection, data processing algorithms, cloud-based machine learning, and IoT integration. The enhanced MD2K software will facilitate reproducible and extensible CISE research with high-frequency mobile sensor data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822935","CRI: CI-EN: Collaborative Research: mResearch: A platform for Reproducible and Extensible Mobile Sensor Big Data Research","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2018","09/17/2018","Mani Srivastava","CA","University of California-Los Angeles","Standard Grant","Ephraim Glinert","09/30/2021","$299,993.00","Vivek Shetty","mbs@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","1714, 7359","075Z, 7359","$0.00","The Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K) has developed open-source software for smart phones and cloud. Scientists use MD2K software to develop and test algorithms to monitor health, wellness, and work productivity via wearable sensors. The mResearch project is aimed at assisting Computer and Information Science and Engineering (CISE) researchers. The mResearch project will significantly enhance MD2K software and integrate Internet-of-Things (IoT) devices. The enhanced MD2K software will accelerate research in sensors design, mobile computing, privacy, analytics (especially machine learning and deep learning), and visualization. mResearch will enable CISE researchers to easily deploy their contributed software in scientific studies for health, smart homes, and workplace. The resulting discoveries and tools will help individuals improve their health, wellness, and work productivity.<br/><br/>MD2K has developed open-source mobile sensor big data software platforms mCerebrum for smartphones and Cerebral Cortex for the cloud. This scalable and generalizable infrastructure is used for collecting, analyzing, and sharing high-frequency, mobile sensor data and associated labels in the context of scientific field studies. In particular, it supports the development and validation of models and algorithms for inferring markers of health, wellness, and productivity, and their associated risk factors. It has already been used at eleven sites across the country to collect over 300 terabytes of mobile sensor data in the field setting from over 2,000 participants. It has resulted in new computational models for the detection of conversation, smoking, eating, craving, stress, and cocaine use. The mResearch project is making five significant infrastructure enhancements to the MD2K infrastructure to assist CISE researchers in mobile sensor development, mobile computing, privacy, analytics, visualization, and participant engagement. First, it will enable data analytic workflow management across multiple layers of the system to enable reproducible and extensible experimentation. Second, it will allow encapsulation of data sources to provide convenient and responsible access to them in data analytic workflows. Third, it will facilitate cloud-assisted complex, real-time analytics for personalizing mobile interventions and improving engagement. Fourth, simulators will be developed with the ability to feed stored data into the platform at various points to enable research on system components and properties such as data compression, transfer and storage, as well as the scalability of data analytics. Finally, Internet-of-Things (IoT) devices and services will be integrated. With these five enhancements, the MD2K software will provide a complete, open, and modularized architecture. It will include all aspects of sensor data collection, data processing algorithms, cloud-based machine learning, and IoT integration. The enhanced MD2K software will facilitate reproducible and extensible CISE research with high-frequency mobile sensor data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823283","CRI: CI-EN: Collaborative Research: mResearch: A platform for Reproducible and Extensible Mobile Sensor Big Data Research","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2018","09/17/2018","Benjamin Marlin","MA","University of Massachusetts Amherst","Standard Grant","Ephraim Glinert","09/30/2021","$248,462.00","","marlin@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","1714, 7359","075Z, 7359","$0.00","The Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K) has developed open-source software for smart phones and cloud. Scientists use MD2K software to develop and test algorithms to monitor health, wellness, and work productivity via wearable sensors. The mResearch project is aimed at assisting Computer and Information Science and Engineering (CISE) researchers. The mResearch project will significantly enhance MD2K software and integrate Internet-of-Things (IoT) devices. The enhanced MD2K software will accelerate research in sensors design, mobile computing, privacy, analytics (especially machine learning and deep learning), and visualization. mResearch will enable CISE researchers to easily deploy their contributed software in scientific studies for health, smart homes, and workplace. The resulting discoveries and tools will help individuals improve their health, wellness, and work productivity.<br/><br/>MD2K has developed open-source mobile sensor big data software platforms mCerebrum for smartphones and Cerebral Cortex for the cloud. This scalable and generalizable infrastructure is used for collecting, analyzing, and sharing high-frequency, mobile sensor data and associated labels in the context of scientific field studies. In particular, it supports the development and validation of models and algorithms for inferring markers of health, wellness, and productivity, and their associated risk factors. It has already been used at eleven sites across the country to collect over 300 terabytes of mobile sensor data in the field setting from over 2,000 participants. It has resulted in new computational models for the detection of conversation, smoking, eating, craving, stress, and cocaine use. The mResearch project is making five significant infrastructure enhancements to the MD2K infrastructure to assist CISE researchers in mobile sensor development, mobile computing, privacy, analytics, visualization, and participant engagement. First, it will enable data analytic workflow management across multiple layers of the system to enable reproducible and extensible experimentation. Second, it will allow encapsulation of data sources to provide convenient and responsible access to them in data analytic workflows. Third, it will facilitate cloud-assisted complex, real-time analytics for personalizing mobile interventions and improving engagement. Fourth, simulators will be developed with the ability to feed stored data into the platform at various points to enable research on system components and properties such as data compression, transfer and storage, as well as the scalability of data analytics. Finally, Internet-of-Things (IoT) devices and services will be integrated. With these five enhancements, the MD2K software will provide a complete, open, and modularized architecture. It will include all aspects of sensor data collection, data processing algorithms, cloud-based machine learning, and IoT integration. The enhanced MD2K software will facilitate reproducible and extensible CISE research with high-frequency mobile sensor data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823221","CRI: CI-EN: Collaborative Research: mResearch: A platform for Reproducible and Extensible Mobile Sensor Big Data Research","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2018","09/17/2018","Santosh Kumar","TN","University of Memphis","Standard Grant","Ephraim Glinert","09/30/2021","$751,014.00","","skumar4@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","1714, 7359","075Z, 7359","$0.00","The Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K) has developed open-source software for smart phones and cloud. Scientists use MD2K software to develop and test algorithms to monitor health, wellness, and work productivity via wearable sensors. The mResearch project is aimed at assisting Computer and Information Science and Engineering (CISE) researchers. The mResearch project will significantly enhance MD2K software and integrate Internet-of-Things (IoT) devices. The enhanced MD2K software will accelerate research in sensors design, mobile computing, privacy, analytics (especially machine learning and deep learning), and visualization. mResearch will enable CISE researchers to easily deploy their contributed software in scientific studies for health, smart homes, and workplace. The resulting discoveries and tools will help individuals improve their health, wellness, and work productivity.<br/><br/>MD2K has developed open-source mobile sensor big data software platforms mCerebrum for smartphones and Cerebral Cortex for the cloud. This scalable and generalizable infrastructure is used for collecting, analyzing, and sharing high-frequency, mobile sensor data and associated labels in the context of scientific field studies. In particular, it supports the development and validation of models and algorithms for inferring markers of health, wellness, and productivity, and their associated risk factors. It has already been used at eleven sites across the country to collect over 300 terabytes of mobile sensor data in the field setting from over 2,000 participants. It has resulted in new computational models for the detection of conversation, smoking, eating, craving, stress, and cocaine use. The mResearch project is making five significant infrastructure enhancements to the MD2K infrastructure to assist CISE researchers in mobile sensor development, mobile computing, privacy, analytics, visualization, and participant engagement. First, it will enable data analytic workflow management across multiple layers of the system to enable reproducible and extensible experimentation. Second, it will allow encapsulation of data sources to provide convenient and responsible access to them in data analytic workflows. Third, it will facilitate cloud-assisted complex, real-time analytics for personalizing mobile interventions and improving engagement. Fourth, simulators will be developed with the ability to feed stored data into the platform at various points to enable research on system components and properties such as data compression, transfer and storage, as well as the scalability of data analytics. Finally, Internet-of-Things (IoT) devices and services will be integrated. With these five enhancements, the MD2K software will provide a complete, open, and modularized architecture. It will include all aspects of sensor data collection, data processing algorithms, cloud-based machine learning, and IoT integration. The enhanced MD2K software will facilitate reproducible and extensible CISE research with high-frequency mobile sensor data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822378","Leveraging Heterogeneous Data Across International Borders in a Privacy Preserving Manner for Clinical Deep Learning","OAC","BD Spokes -Big Data Regional I","03/15/2018","03/15/2018","Gari Clifford","GA","Emory University","Standard Grant","Alejandro Suarez","02/28/2021","$300,000.00","Shamim Nemati","gari.clifford@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","024Y","7916","$0.00","There is a growing awareness of the need for multi-center clinical databases and multi-institutional analyses of healthcare data to ensure reproducibility and generalizability of research findings. Single-instance database algorithms are prone to three distinct problems. First, in the context of Big Data science, the size of the data compared to the number of variables makes it difficult to develop complex predictors without overfitting, and more traditional learning algorithms may lead to over-simplified models that do not capture important related influences or interactions between different types of healthcare information. Second, training and testing predictive models on a single database can lead to learning noise or other irrelevant local practices or differences in definitions that are correlated with, but not causally related to, the outcome in question. This leads to models that do not work in other institutions or in the future when practices or the environment changes. Third, sharing data between institutions, and in particular, across borders, is extremely problematic because of trust, legal issues, privacy issues and national policies. The significance of solving these issues is threefold: 1) it would allow the creating of strong generalizable data science models, which leverage enormous pools of data from around the world; 2) it would also allow the identification of rare diseases or patient types, which, as we compile databases, become less rare; and 3) perhaps most importantly, it would allow the free exchange of data science models and generalized approaches to solving medical problems in the cloud.<br/><br/>This project aims to develop a set of distributed deep learning and cloud computation techniques for cross-institution and cross-border machine learning on health and medical data without the need for protected health information to leave the generating institution. The goals are to create demonstration programs which illustrate feasibility and open source the architecture. The scope of this project encompasses the broad set of machine learning-based tasks multiple institutions may want to apply to their healthcare data in the cloud, as well as the technical issues surrounding transfer learning of knowledge across domains (e.g., institutions/demographics) and tasks (e.g., types of classification and prediction problems). The project has three specific aims: 1) develop a cloud-based infrastructure which preserves regional autonomy of data, but allows the sharing of parameters of the partially trained deep neural network (including weights and hyperparameters) between regions, to allow transfer learning across domains and tasks; 2) develop a standardized coded model for deep learning approaches in medical applications; and 3) evaluate the effect of training and testing the model across multiple centers and national boundaries, by comparing improvement in performance with cross-institutional training without loss of privacy protection, using metrics of sensitivity, specificity, positive predictive value, area under the receiver operating characteristic (ROC) curve and model calibration. Aims 1-3 will be achieved by taking four databases (including, a database of intensive care unit patients with sepsis, a free text corpus of nursing progress notes, voice recordings taken from a public corpus classically used for speaker identification, and a public database of full-face images used for classification of facial expressions) and placing them in the cloud (Google, AWS and Azure) at different geopolitical locations (namely US and Europe) and developing a distributed deep learning architecture that learns to improve its performance by sharing weights across borders, but not sensitive patient data. This project has the potential to make several contributions to the field. First, it will demonstrate that medical data across geopolitical boundaries can be made available in an interoperable manner (using the FHIR standard) and can be used for training of deep learning algorithms in a privacy-preserving manner, thus addressing both the concerns of Health Insurance, Portability and Privacy Act (HIPPA) and interoperability. Secondly, it will provide open-source deep learning algorithms for several medical datasets and data types that can be used across institutions to solve similar problems with some fine-tuning (e.g., via transfer learning). Third, it will provide a set of open-source meta algorithms for transfer learning (across domains and tasks) implemented on the cloud in containers (dockers) that can be downloaded for local use or transferred across the different cloud vendors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801446","SaTC: CORE: Medium: Large-Scale Data Driven Anomaly Detection and Diagnosis from System Logs","CNS","Secure &Trustworthy Cyberspace","08/01/2018","07/16/2018","Feifei Li","UT","University of Utah","Standard Grant","Wei-Shinn Ku","07/31/2022","$1,100,000.00","Robert Ricci, Vivek Srikumar","lifeifei@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8060","025Z, 7434, 7924","$0.00","Detecting unusual and anomalous behavior in computer systems is a critical part of ensuring they are secure and trustworthy. System logs, which record actions taken by programs, are a promising source of data for such anomaly detection.  However, existing practices and tools for doing log analysis require deep expertise, as well as heavy human involvement in both defining and interpreting possible anomalies, which limits their scalability and effectiveness.  This project's goal is to improve the state of the art around log-based anomaly detection by developing a framework called DeepLog through (a) advancing natural language processing techniques to extract structured information from a wide variety of log files to support analysis across different data sources and across time, (b) developing new methods to model legitimate workflows and log event sequences over time, (c) adapting machine learning methods to identify deviations from those workflows that represent potential anomalies, and (d) creating tools for system administrators to help them diagnose possible security issues more effectively and efficiently.  The work will be integrated into a freely available software package to benefit both other researchers and practicing system administrators and used to support both classroom and research-based educational activities at the investigators' institutions.<br/><br/>Toward log parsing, the team will adapt named entity recognition methods to parse unstructured logs as well as structured logs where the structure is not pre-defined by, e.g., regular expressions, into structured key-value pairs of log event types and parameters.  This data can be seen as a multi-dimensional feature space whose contents are constrained by the execution of the underlying programs and thus reflects a hidden structure that defines the set of valid, non-anomalous execution sequences.  To help articulate this hidden structure, the team will develop long-short-term-memory (LSTM)-based neural network models that use both the key and value elements to extract semantically meaningful subsequences of program behavior from data extracted from system runs known to be normal.  Once these models are developed using known-good training data, they can be applied to anomaly detection by flagging for consideration new log entries that are unexpected given the current state of the system, logs, and model; they can also be used to infer the underlying workflows and hidden structures described earlier.  These models will be improved through that online learning methods, administrators' feedback about the seriousness of reported anomalies, and generative adversarial training models which create execution sequences that, though anomalous, hew closely to the hidden structures embedded in the logs and the LSTM-based models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833098","RII Track-4: Advancing Machine Learning in Biological Oceanography Through Interdisciplinary Collaborations","OIA","EPSCoR Research Infrastructure","10/01/2018","08/20/2018","Roy Collins","AK","University of Alaska Fairbanks Campus","Standard Grant","Chinonye Whitley","09/30/2020","$187,301.00","","recollins@alaska.edu","West Ridge Research Bldg 008","Fairbanks","AK","997757880","9074747301","O/D","7217","9150","$0.00","Non-Technical Description<br/>Modern scientists in interdisciplinary fields like oceanography and genomics regularly generate complex datasets with billions or trillions of data points. Machine learning is revolutionizing the way these data are observed. As a merger between statistics and computer science, the techniques of machine learning have been used for decades, but recent improvements in high-performance computing, and research investment from the commercial sector have supercharged the utility of these methods. Environmental scientists have been relatively slow to take up these new methods, in part because the rapid pace of discovery makes it difficult to stay at the cutting edge, but as scientific datasets grow ever larger and more complex, it is becoming increasingly important to make use of machine learning as a way to explore and understand the world. In collaboration with colleagues at Woods Hole Oceanographic Institute, the PI aims to stimulate progress in oceanography and genomics by bringing cutting edge tools and techniques from the world of machine learning to Alaska. Acquiring these skills in parallel with an early career scientist, and applying them to ongoing research projects in the Arctic, will enable a substantially improved understanding of ongoing changes in the marine ecosystems of Alaska. Passing on this knowledge to students by designing and teaching a new course on Machine Learning in the Environmental Sciences will ensure that future generations of Alaskans have access to the skills they will need to succeed in science and beyond.<br/><br/>Technical Description<br/>The broad goals of this visit are to advance machine learning in biological oceanography through interdisciplinary collaborations and knowledge transfer. The PI aims to leverage extensive expertise at Woods Hole Oceanographic Institution (WHOI) to advance research on Arctic marine microbial communities using machine learning methods. The WHOI Autonomous Robotics and Perception Laboratory (WARP) Lab specializes in adaptive sampling, marine robotics, computer vision, autonomous robotic exploration, semantic perception, bayesian nonparametrics, deep learning, surprise detection, and data summarization. Objectives of this project include learning the theory and concepts of machine learning in structured and informal settings, which include several common programming languages and toolkits. Using this base, the PI and a graduate trainee will apply the tools and techniques of machine learning to predict spatiotemporal distributions of Arctic marine microbes, their use and transformations of metals, and advance biological oceanographic sampling techniques.  The PI will accomplish this effort by designing methods for adaptive biological sampling using flow-through systems and ocean profilers. Outcomes of the project will include the development of a new course on Machine Learning in the Environmental Sciences to be taught at the University of Alaska Fairbanks and online via the University of the Arctic. Career development and mentorship of the graduate student will include training in computing fluency in the Software Carpentry program and subsequent training as an instructor. As part of the knowledge exchange with WHOI, the PI will host an Oxford Nanopore minION sequencing workshop in Woods Hole to share this cutting edge technology, and lead cross-cutting discussions on the use of machine learning in advancing this technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816568","RI: Small: Computational and Physiological Studies of Complex Neural Codes in the Early Visual Cortex","IIS","Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","04/18/2019","Tai Sing Lee","PA","Carnegie-Mellon University","Standard Grant","Kenneth Whang","09/30/2021","$513,994.00","","tai@cnbc.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495, 8624","7495, 7923, 8089, 8091, 9251","$0.00","In this interdisciplinary project, machine learning approaches are coupled with neurophysiological studies of primate early visual cortex  to investigate the functional, coding and computational benefits of the observed neural representation and computing architecture.  Neural models, with recurrent connections and the proposed dual-code strategy, will be developed to solve multiple vision problems simultaneously and to fit neurophysiological data.  The representations will be studied from both coding perspectives and computational perspectives, based on scene statistics and their relevance for solving vision problems. The research program will be facilitated by international collaboration and tightly integrated with undergraduate and graduate education in neural computation.   The proposed project wide provide new insights to the computations and functions of the biological visual system, as well as new ideas and inspirations for developing machine learning systems that can learn from limited data and function robustly and flexibly in novel complex situations, potentially with broad societal and technological impact.<br/><br/>Current deep learning neural networks utilize tens or hundreds of layers to learn solutions for specific computer vision problems. The mammalian visual system has much fewer layers, and yet can solve many tasks in a variety of novel and complex situations. The nervous system might achieve this feat by having neuronal circuits with loops and recurrent connections, and with order of magnitude more neurons in each ""layer."" Recent neurophysiological findings suggest that neurons in the primary visual cortex (V1) of primates are not simply oriented edge and bar detectors as described in textbooks, but respond strongly to highly specific complex local patterns, although they also respond to many other patterns with much weaker responses.  The PI proposed that the individual neurons are not amorphous entities, functioning facelessly in a large population, but are distinct and unique individuals that serve as specialists for some specific tasks and as generalists in other tasks. They participate in population encoding of  information with strong sparse codes or weak distributed codes respectively, depending on the functional roles they serve.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840751","EAGER: Using Large-scale Web Data for Online Attention Models and Identification of Reading Disabilities","IIS","Info Integration & Informatics","08/15/2018","08/11/2018","Mor Naaman","NY","Cornell University","Standard Grant","Maria Zemankova","07/31/2021","$298,752.00","Shiri Azenkot, Yoav Artzi","mor.naaman@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364","7364, 7916","$0.00","Media websites now capture intricate measures of engagement from millions of readers. These measures, such as in-page scrolling and viewport position, can help us understand patterns of user attention beyond simple measures like time spent on page. This exploratory project will transform the use of this under-utilized attention data by showing how such data, mostly in the context of online news articles, can lead to better understanding of user behavior online, and help capture (and reason about) user attention at scale. However, such data is extremely noisy and challenging to analyze. The researchers will explore developing new techniques for data analysis to reason about the connection between language, text, and attention on the Web. Using these data, and novel analysis techniques, the project will also explore how to identify users with reading difficulties, and potentially support them in online reading tasks. The potential outcomes of this project are algorithms and methods that can be used by news content providers to develop a refined understanding of different types of readers, according to how they interact and read news articles online. More importantly, potential outcomes also include advancements in how such publishers can detect, and support, users with reading difficulties. Such advancement has the potential to lead to improvements in online experiences for the estimated 7% to 15% of the population who suffer from reading disabilities.<br/> <br/>This project directly addresses multiple challenges in making this new type of large-scale data useful and usable in different settings, and is likely to result in a number of key intellectual contributions in large-scale information management, machine learning, and human computer interaction. The first significant challenge is in modeling and processing the raw large-scale data to result in a robust attention signals. The data is extremely noisy and challenging to analyze and understand, with news articles of different formats, different content types, used by different groups of users. Second, the project will make a unique contribution by using novel deep learning techniques to understand the interaction between language and attention, at scale. Such modeling can extend current Natural Language Processing (NLP) techniques to improve methods for the analysis of language and of narrative in the context of news articles, and more broadly for reading tasks. Finally, a combination of field studies and large-scale data analysis will be required to understand the attention patterns of users who may have difficulties in reading. A field study will collect data from users who are known to have reading disabilities, in order to develop a technique that can help estimate a user's reading difficulty from the attention data in a large interaction dataset. This project will thus inform work that combines NLP and Human-Computer Interaction to suggest possible paths for supporting such users in online reading tasks. In order to support further research and reproducibility, the resulting data, software, and models used in this project available to other researchers, as possible. The project results will be disseminated via research papers and talks to be academic and industry audiences, and through the project website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1847853","EAGER: Collaborative Research:Automated Instruction Assistant for Argumentative Essays","IIS","Cyberlearn & Future Learn Tech","09/01/2018","02/21/2019","Smaranda Muresan","NY","Columbia University","Standard Grant","Tatiana Korelsky","08/31/2020","$159,000.00","","smara@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8020","063Z, 7916, 8045, 9251","$0.00","Development of students' writing skills promotes critical thinking across disciplines, and professional success. Yet the past decade of the Nation's Report Card on students' writing points to a long-standing crisis in writing instruction that persists through post-secondary school. Students need more instruction on how to write, and instructors need more support to provide students with comprehensive, targeted feedback. This project will develop an Automated Instruction Assistant (AIA) to provide post-secondary instructors with feedback on essays while they grade them, through the application of natural language processing and machine learning techniques to the analysis of essay content and argumentation. The project will apply an iterative design process to a sequence of two argumentative essay assignments in the context of a freshman course on academic skills in a computer science department, where the enrollment is in the hundreds. It will integrate state-of-the art methods in content analysis and argument mining of text to model text meaning more deeply than in previous work. Automated support for application of educational rubrics to argumentative essays will help instructors to provide more comprehensive, standardized feedback for students, and foster transparent and supportive writing instruction. <br/><br/>This project will develop an AIA that assigns both a total score to an essay, and individual score dimensions, such as how well an argumentative essay articulates a major claim. The scores will be supported by pointers into the text that provide score justification. As a result, the AIA output can be linked directly to a rubric used by the instructor, which facilitates instructor reflection, training for graders, and feedback for students. The technology will integrate and extend the researchers' previous work on content analysis and argument mining. The content analysis will take as input a small number of reference essays to generate a model of the ideas (propositions) in the domain, weighted by importance within and across essays. The argument mining will identify the propositions that play a role in an argument, and their argument relations. It will produce an argument graph in which the nodes are propositions and edges are argument relations. Integration with the content analysis will ground the propositions in the domain ideas, and make it possible to exploit the role of importance of ideas in the domain, and their prominence within an essay. Methods will include novel applications of dynamic programming and integer linear programming combined with deep learning of rich, low dimensional semantic vectors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1847842","EAGER: Collaborative Research: Automated Instruction Assistant for Argumentative Essays","IIS","Cyberlearn & Future Learn Tech","09/01/2018","11/08/2018","Rebecca Passonneau","PA","Pennsylvania State Univ University Park","Standard Grant","Tatiana Korelsky","08/31/2020","$173,000.00","","rjp49@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8020","063Z, 7916, 8045, 9251","$0.00","Development of students' writing skills promotes critical thinking across disciplines, and professional success. Yet the past decade of the Nation's Report Card on students' writing points to a long-standing crisis in writing instruction that persists through post-secondary school. Students need more instruction on how to write, and instructors need more support to provide students with comprehensive, targeted feedback. This project will develop an Automated Instruction Assistant (AIA) to provide post-secondary instructors with feedback on essays while they grade them, through the application of natural language processing and machine learning techniques to the analysis of essay content and argumentation. The project will apply an iterative design process to a sequence of two argumentative essay assignments in the context of a freshman course on academic skills in a computer science department, where the enrollment is in the hundreds. It will integrate state-of-the art methods in content analysis and argument mining of text to model text meaning more deeply than in previous work. Automated support for application of educational rubrics to argumentative essays will help instructors to provide more comprehensive, standardized feedback for students, and foster transparent and supportive writing instruction. <br/><br/>This project will develop an AIA that assigns both a total score to an essay, and individual score dimensions, such as how well an argumentative essay articulates a major claim. The scores will be supported by pointers into the text that provide score justification. As a result, the AIA output can be linked directly to a rubric used by the instructor, which facilitates instructor reflection, training for graders, and feedback for students. The technology will integrate and extend the researchers' previous work on content analysis and argument mining. The content analysis will take as input a small number of reference essays to generate a model of the ideas (propositions) in the domain, weighted by importance within and across essays. The argument mining will identify the propositions that play a role in an argument, and their argument relations. It will produce an argument graph in which the nodes are propositions and edges are argument relations. Integration with the content analysis will ground the propositions in the domain ideas, and make it possible to exploit the role of importance of ideas in the domain, and their prominence within an essay. Methods will include novel applications of dynamic programming and integer linear programming combined with deep learning of rich, low dimensional semantic vectors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816627","RI: Small: From acoustics to semantics:  Embedding speech for a hierarchy of tasks","IIS","Robust Intelligence","08/15/2018","09/13/2018","Karen Livescu","IL","Toyota Technological Institute at Chicago","Continuing Grant","Tatiana Korelsky","07/31/2021","$449,984.00","","klivescu@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7495","7495, 7923","$0.00","There is an increasingly large array of spoken language interfaces available, such as virtual assistants and telephone customer service interfaces.  These technologies both (1) recognize the words spoken by a user and (2) extract actionable information, such as the topic of the user's query and the degree of match between the query and documents in a database.  Such applications are typically treated as a pipeline of automatic speech transcription followed by text processing to extract the meaning.  This project aims to develop technology that directly extracts meaning from speech, while using a variety of linguistic information along the way.  This approach is intended to mitigate the effects of speech recognition errors, as well as to use all of the meaning-bearing information in speech, such as intonation.  This work is expected to have long-term broad impact through technological advances, as well as immediate broad impact through the PI's involvement in local schools and mentoring for a diverse set of visiting students.<br/><br/>The technical goals of this work are (1) to do high-quality natural language processing directly on speech; (2) to seamlessly integrate domain knowledge into end-to-end speech models; (3) improve the performance-vs.-resources tradeoff; and (4) develop models for embedding arbitrary speech signals into meaning-bearing representations.  The process of mapping from speech to meaning can be viewed as a hierarchy of tasks, from the most basic acoustic-phonetic tasks to the deepest semantic tasks.  The experimental work will focus on two task hierarchies:  a ""retrieval"" hierarchy including query-by-example search, keyword spotting, semantic speech search; and a ""recognition"" hierarchy including phonetic recognition, word recognition, parsing, and topic identification.  The main technical approaches to be developed include hierarchical multitask learning methods for incorporating domain knowledge and mitigating low-data settings, as well as new models for acoustic-semantic speech embedding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750383","CAREER: Integrating perceptual models of auditory importance into deep learning-based noise-robust speech recognition","IIS","Robust Intelligence","06/01/2018","07/01/2020","Michael Mandel","NY","CUNY Brooklyn College","Continuing Grant","Tatiana Korelsky","05/31/2023","$289,489.00","","mim@sci.brooklyn.cuny.edu","Office of Research & Sponsored P","Brooklyn","NY","112102889","7189515622","CSE","7495","1045, 7495","$0.00","Hearing is central to human interaction, but the hearing process is not easily observed.  The objective of this project is to train models to identify portions of speech utterances that are important to their being correctly identified by human listeners, and to use predictions from these models to make automatic speech recognition (ASR) systems more noise robust by focusing on those regions.  The ability to identify important regions of an utterance could significantly advance our understanding of healthy and impaired hearing.  Improvements in automatic speech recognition would have broader impacts on the 260 million Americans who use smart phones and the $100 billion ASR industry.  The educational portion of this project utilizes examples from speech, language, audio, and music processing to attract and retain students in Brooklyn College's introductory programming course serving a diverse student body along with similar efforts at affiliated high school programs.<br/><br/>The team's preliminary results have shown that that some regions of an utterance are more important or useful than others in identifying it by measuring the intelligibility of a given utterance in many different noisy mixtures.  This project expands upon these preliminary results in three ways.  First it measures ASR auditory importance using the team's existing slow but accurate technique involving random ""bubble noise"", comparing different ASR variants to each other and to human listeners.  Second, it trains a model to predict ASR auditory importance from clean speech using a novel architecture called the bubble cooperative network (BCN) that allows the recognizer to be trained jointly with the BCN to improve performance.  Third, it adapts the learned importance predictor to human listeners and uses this human-adapted importance predictor to further refine the ASR models.  These tasks should permit the use of utterance-level human responses to directly improve the noise robustness of automatic speech recognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813635","Geometric Numerical Integration of Plasma Physics and General Relativity","DMS","APPLIED MATHEMATICS","09/01/2018","08/10/2018","Melvin Leok","CA","University of California-San Diego","Standard Grant","Victor Roytburd","08/31/2021","$237,632.00","","mleok@math.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","MPS","1266","","$0.00","The accurate and efficient numerical simulation of complex mathematical models is critical to the design and analysis of contemporary engineering, scientific, and medical systems.  Mathematical models of drones, computer vision and graphics, medical imaging, fluid dynamics of plasmas, and gravitational waves are posed on curved spaces, which possess geometric properties that have to be respected by the numerical simulations in order to obtain accurate, robust, and reliable predictions.  The two main motivating applications for this project are to plasma physics and gravitational waves.  Plasmas are highly ionized gases; theyarise in nuclear fusion devices, propulsion systems for space exploration, and during the formation of galaxies.  Gravitational waves are ripples in spacetime that were predicted by Einstein, and they arise from the collision of massive astrophysical bodies like black holes and neutron stars.  The construction of numerical methods for such problems enables scientists to design more stable and efficient nuclear fusion systems, and to more accurately determine the astrophysical events that correspond to gravitational waves that are detected.  In addition, the investigator develops optimization and sensitivity analysis techniques that improve the efficiency of optimization algorithms that underlie deep learning and other machine learning techniques in data science.  Graduate students participate in the research.<br/><br/>The project combines theoretical and computational tools arising from discrete Dirac mechanics and geometry, variational integrators, the relationship between symmetric spaces and the generalized polar decomposition, and embeddings of noncanonical Hamiltonian systems as well as nonvariational equations and their adjoints into degenerate Lagrangian systems.  This provides a systematic method for constructing and analyzing geometric structure-preserving discretizations of degenerate noncanonical Hamiltonian systems, nonvariational equations and their adjoints, and problems that evolve on symmetric spaces.  The resulting methods have implications for plasma physics, which is described by noncanonical Hamiltonian systems, as well as general relativity, which is a degenerate higher-order gauge field theory on a symmetric space.  In addition, adjoint equations arise in many important applications, including optimal control, optimal design, optimal estimation, uncertainty quantification, and sensitivity analysis.  A deeper understanding of the hidden geometric structure underlying an arbitrary system of differential equations and their associated adjoint equations, and variational discretizations that respect that geometric structure, would have profound implications on the broad range of analytical and numerical techniques that rely critically on the solution of adjoint equations.  Graduate students participate in the research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761562","Collaborative Research: Deep learning speech recognition for documenting Seneca, a Native American language, and other acutely under-resourced languages","BCS","Robust Intelligence, DEL","06/01/2018","07/08/2020","Emily Prud'hommeaux","NY","Rochester Institute of Tech","Continuing Grant","Joan Maling","11/30/2021","$196,041.00","Raymond Ptucha","emilypx@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","SBE","7495, 7719","1311, 7495, 7719, 9178, 9179, 9251, SMET","$0.00","The Iroquoian languages of the Six Nations Confederacy, or the Haudenosaunee people, were first encountered when the explorer Jacques Cartier sailed up the St. Lawrence River in the 1530s. Dictionaries and grammars exist, but the basic elements of documentation for every language should also include annotated texts of diverse genres. As currently recognized in The Native American Languages Act, passed by the U.S. Congress in 1990, languages spoken by the indigenous peoples of North America have a unique status and importance. This project will bring together members of the Seneca Nation of Indians with a team of linguistics and computing researchers to record elders speaking Seneca, an Iroquoian language that is particularly endangered. The team will develop software to accurately and efficiently transcribe these recordings using automatic speech recognition (ASR), the technology behind digital personal assistants like Siri or Alexa. Seneca has an exceedingly complex word structure, known as polysynthesis, in which a word is equivalent to a clause or sentence. Such languages challenge ASR systems, which are generally designed to recognize words over a constrained vocabulary. This project will advance scientific knowledge by developing novel methods for generating synthetic text data to augment the existing written resources required to model this complexity. Broader impacts include the availability of the newly documented materials for language revitalization and scientific investigation. The project will provide undergraduates, graduate students, and young adults from the Seneca Nation with valuable STEM experience and broadening participation of Native Americans in the language and computing sciences, including supporting a Seneca doctoral student in computer science. The computational tools and methodologies developed will be accessible to others who are working to document and analyze low-resource languages, many spoken in regions of critical importance for national security.<br/><br/>Spontaneous speech in Seneca contains long, complex words but also many short particles that are essential to understanding the discourse. Crucial for segmenting and annotating spoken Seneca are the prosodic patterns that occur in longer utterances, involving both metrical and tonal components. Most ASR frameworks would be challenged by the large vocabulary size that a polysynthetic morphological system tends to yield. In addition, ASR systems do not typically model high-level prosodic information. Seneca has little available text data derived from spontaneous speech, which is needed to build the predictive language models used in ASR and is invaluable to Seneca learners. Augmenting the available text data will require novel techniques for generating synthetic but plausible text, with a particular focus on neural sequence-to-sequence models. The ability of neural nets to model long-distance and hierarchical relationships will also be exploited to capture utterance-level prosodic patterns required for accurate segmentation of spontaneous speech in Seneca. By bringing together a range of expertise and by involving Seneca community members, key stakeholders in the language, the project bridges traditional linguistic methodology and computational approaches. Each new Seneca recording that is transcribed and annotated through this collaboration across disciplines will support the revitalization of the Seneca language and help to further the state of the art in low-resource language technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761477","Collaborative Research: Deep learning speech recognition for documenting Seneca, a Native American language, and other acutely under-resourced languages","BCS","Robust Intelligence, DEL","06/01/2018","07/24/2018","Karin Michelson","NY","SUNY at Buffalo","Continuing Grant","Joan Maling","11/30/2021","$90,367.00","","kmich@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","SBE","7495, 7719","1311, 7495, 7719, 9178, 9179, 9251, SMET","$0.00","The Iroquoian languages of the Six Nations Confederacy, or the Haudenosaunee people, were first encountered when the explorer Jacques Cartier sailed up the St. Lawrence River in the 1530s. Dictionaries and grammars exist, but the basic elements of documentation for every language should also include annotated texts of diverse genres. As currently recognized in The Native American Languages Act, passed by the U.S. Congress in 1990, languages spoken by the indigenous peoples of North America have a unique status and importance. This project will bring together members of the Seneca Nation of Indians with a team of linguistics and computing researchers to record elders speaking Seneca, an Iroquoian language that is particularly endangered. The team will develop software to accurately and efficiently transcribe these recordings using automatic speech recognition (ASR), the technology behind digital personal assistants like Siri or Alexa. Seneca has an exceedingly complex word structure, known as polysynthesis, in which a word is equivalent to a clause or sentence. Such languages challenge ASR systems, which are generally designed to recognize words over a constrained vocabulary. This project will advance scientific knowledge by developing novel methods for generating synthetic text data to augment the existing written resources required to model this complexity. Broader impacts include the availability of the newly documented materials for language revitalization and scientific investigation. The project will provide undergraduates, graduate students, and young adults from the Seneca Nation with valuable STEM experience and broadening participation of Native Americans in the language and computing sciences, including supporting a Seneca doctoral student in computer science. The computational tools and methodologies developed will be accessible to others who are working to document and analyze low-resource languages, many spoken in regions of critical importance for national security.<br/><br/>Spontaneous speech in Seneca contains long, complex words but also many short particles that are essential to understanding the discourse. Crucial for segmenting and annotating spoken Seneca are the prosodic patterns that occur in longer utterances, involving both metrical and tonal components. Most ASR frameworks would be challenged by the large vocabulary size that a polysynthetic morphological system tends to yield. In addition, ASR systems do not typically model high-level prosodic information. Seneca has little available text data derived from spontaneous speech, which is needed to build the predictive language models used in ASR and is invaluable to Seneca learners. Augmenting the available text data will require novel techniques for generating synthetic but plausible text, with a particular focus on neural sequence-to-sequence models. The ability of neural nets to model long-distance and hierarchical relationships will also be exploited to capture utterance-level prosodic patterns required for accurate segmentation of spontaneous speech in Seneca. By bringing together a range of expertise and by involving Seneca community members, key stakeholders in the language, the project bridges traditional linguistic methodology and computational approaches. Each new Seneca recording that is transcribed and annotated through this collaboration across disciplines will support the revitalization of the Seneca language and help to further the state of the art in low-resource language technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808932","Deep neural networks for multi-channel speaker localization and speech separation","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","12/01/2018","12/04/2018","DeLiang Wang","OH","Ohio State University","Standard Grant","Anthony Kuh","11/30/2021","$300,000.00","","dwang@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","ENG","7607","1653","$0.00","In recent years, there is a dramatic increase in the deployment of the voice-based interface for human-machine communication. Such devices typically have multiple microphones (or channels), and as they are used in homes, cars, and so on, a major technical challenge is how to reliably localize a target speaker and recognize his/her speech in everyday environments with multiple sound sources and room reverberation. The performance of traditional approaches to localization and separation degrades significantly in the presence of interfering sounds and room reverberation. This project investigates multi-channel speaker localization and speech separation from a deep learning perspective. The innovative approach in this project is to train deep neural networks to perform single-channel speech separation in order to identify the time-frequency regions dominated by the target speaker. Such regions across microphone pairs provide the basis for robust speaker localization and separation. <br/><br/>Building on this novel perspective, the proposed research seeks to achieve robust speaker localization and speech separation. For robust speaker localization, time-frequency (T-F) masks will be generated by deep neural networks (DNN) from single-channel noisy speech signals. Across each pair of microphones, an integrated mask will be calculated from the two corresponding single-channel masks and then used to weight a generalized cross-correlation function, from which the direction of the target speaker will be estimated. An alternative method for localization will be based on mask-weighted steered responses. For robust speech separation, masking-based beamforming will be initially performed, where T-F masking and accurate speaker localization are expected to enhance beamforming results substantially. To overcome the limitation of spatial filtering in multi-source reverberant conditions, spectral (monaural) and spatial information will be integrated as DNN input features in order to separate only the target signal with speech characteristics and originating from a specific direction. The proposed approach will be evaluated using automatic speech recognition rate, as well as localization and separation accuracy, on multi-channel noisy and reverberant datasets recorded in real-world environments. This will ensure a broader impact not only in advancing speech processing technology but also in facilitating the design of next-generation hearing aids in the long run.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835270","NCS-FO: Unraveling Cortical Circuits for Auditory Scene Analysis","IIS","IntgStrat Undst Neurl&Cogn Sys","10/01/2018","08/28/2018","Kamal Sen","MA","Trustees of Boston University","Standard Grant","Mitra Basu","09/30/2021","$999,736.00","Xue Han","kamalsen@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","8624","8089, 8091, 8551","$0.00","In everyday social situations, normal hearing humans are able to listen to a speaker in the midst of other people talking and other sound sources. This is an example of a general problem termed auditory scene analysis. The understanding of auditory scene analysis remains a challenging problem in a diverse range of fields such as neuroscience, computer science, speech recognition and engineering, after more than 50 years of research. Although a difficult problem for machines and hearing impaired listeners, humans with normal hearing solve it with relative ease. This suggests the existence of a solution to this problem in the brain, but this solution remains unknown. This project will investigate how the brain solves this problem. By revealing circuits in the brain that contribute to the solution, this project will ultimately improve quality of life through applications in medical devices, e.g., hearing aids and cochlear implants; benefit society through applications in technology, e.g., applications for speech recognition; and create an educational platform to train students to integrate knowledge from a variety of disciplines to address challenging and important societal problems.<br/><br/>The spatial location of different sound sources is an important component of auditory scene analysis. The auditory cortex, with its unique spatial sound processing ability, is thought to play an important role in auditory scene analysis, although the underlying neural network mechanisms remain largely unknown. This project will investigate cortical circuits for auditory scene analysis in the primary auditory cortex of the mouse, employing powerful optogenetic tools to investigate both bottom-up and top-down mechanisms. First, the investigators will examine the influence of behavioral states on cortical spatial representations of sound mixtures across different cortical layers and test the hypotheses that such spatial representations vary across cortical layers and behavioral states of the animal. Second, the investigators will examine the causal role of parvalbumin positive (PV) inhibitory interneurons versus somatostatin positive (SOM) inhibitory interneurons in the primary auditory cortex, using optogenetic manipulations, and test the hypothesis that PV interneurons are critical in mediating bottom-up signaling, whereas SOM interneurons are selectively engaged during active behavioral states. Finally, the investigators will construct a computational model of the primary auditory cortex including excitatory, PV and SOM neurons to explain the experimental results and make predictions for future experiments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813434","SHF: Small: Bitstream Processing","CCF","Special Projects - CCF","10/01/2018","08/23/2018","Mikko Lipasti","WI","University of Wisconsin-Madison","Standard Grant","Yuanyuan Yang","09/30/2021","$449,997.00","","mikko@engr.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","2878","2878, 7923, 7941","$0.00","Embedded computing systems are becoming very common in today's world, ranging from wearable devices, to in-home smart speakers, to autonomous appliances and vehicles. Many of the computational tasks that these systems implement require very high performance hardware for tasks like audio voice recognition, visual object recognition, path optimization, and autonomous control.  Historically, such high-performance hardware relied on binary fixed-point algorithms deployed on low-power microcontrollers or digital signal processors. However, the sensing and control interfaces themselves do not use binary number representations, but instead use bitstreams, which encode numeric input and output values using the density of ones over time. Conventional computing substrates requires conversion of both inputs and outputs to interface with physical systems that utilize bitstreams.  This project is developing novel, biologically-inspired approaches for directly operating on data represented in the native bitstream format. Compute hardware that directly operates on these bitstreams can be seamlessly integrated into systems that sense the real world, process sensory data, and issue control commands based on the processed data as well as learned actions based on rewards from the environment. The capability of these new approaches will be demonstrated through two experimental platforms: a very low power speech recognition system that operates on bitstream audio data, and an autonomous airborne vehicle that learns to navigate its environment. This research has broad industry- and economy-wide impact since it will lead to the discovery and realization of novel, powerful, and energy-efficient approaches for implementing power- and energy-constrained embedded computing systems.<br/><br/>This research advocates development of novel, biologically-inspired approaches for processing data represented as bitstreams. Bitstreams, which encode numeric values using density of ones (unary) or ones and zeroes (binary) are a natural representation for data sensed from the environment (input) as well as robotic control (output), and can be inexpensively generated using low-cost, yet accurate, sigma-delta modulators. The initial phase of this research project focuses on developing the theoretical and algorithmic underpinnings for visual, auditory, and inertial sensory processing, including feature extraction, bandpass filtering, perspective and coordinate transforms, linear optimization, and memory formation, which are grounded in principles from the speech processing, computer vision, spiking neural networks, reinforcement learning, and signal processing domains. The novel sensory processing capabilities are then deployed in two experimental platforms: first, an ultra-low power acoustic model for speech recognition that can demonstrate the suitability of bitstream processing for feature extraction and sequence learning via long short-term memory. Next, bitstream sensory processing technology is coupled directly to a control system that enables an unmanned aerial vehicle to navigate in a controlled indoor environment while learning, with increasing efficiency, to identify and target sources of rewards. Both demonstration platforms rely on concepts from biological spiking neural networks, stochastic computing for arithmetic operations, as well as oversampled sigma-delta modulation theory for data representation and signal processing tasks, and provide unprecedented levels of efficiency in terms of energy consumption, compute density, and autonomous operation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1852744","Doctoral Dissertation Research: Investigating Overt Definite Articles and Grammatical Variation","BCS","DEL DDRIG Document Endangered, DDRI Linguistics","12/01/2018","01/28/2019","Sarah Murray","NY","Cornell University","Standard Grant","Joan Maling","05/31/2021","$18,900.00","Carol-Rose Little, Miloje Despic","sarah.murray@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","SBE","036Y, 8374","1311, 7719, 9179, SMET","$0.00","The questions ""Is the doctor on board?"" and ""Is a doctor on board?"" can signify very different situations. With the first question, a husband may simply be looking for his wife, whereas with the second, a doctor is needed for an emergency. The only difference between these two questions is ""the"" (the definite article) and ""a"" (the indefinite article). Many languages, however, do not have overt definite and indefinite articles. How, then, do speakers of these languages differentiate between these two meanings? One way to differentiate is by context: ""Is doctor on board?"" in languages without overt articles can be either definite or indefinite, and in context speakers understand whether a particular doctor is needed or any doctor. Certain linguistic theories predict that if a language has an overt definite article, then certain grammatical constructions are prohibited. One such prohibited construction is where modifiers appear disconnected from the noun they modify (""Friendly I saw the doctor"" is not grammatical in English, but possibly grammatical in languages without definite articles). This project will investigate variation in two dialects of an underdocumented language, each of which have different ways of expressing definiteness. These dialects, while mutually understandable, also differ with respect to possible grammatical constructions, like the disconnected modifier construction. This project will advance our understanding of crosslinguistic variation in how languages express definiteness and keep track of individuals in discourse, research with implications for the fields of natural language processing, speech recognition and other branches of computational linguistics. Broader impacts include a public repository of recordings and transcriptions, materials for use by community members, promotion of linguistic sustainability in the region and collaboration with local university students as active contributors to knowledge on their heritage language. Other outcomes include the training of a doctoral student.<br/><br/>The CoPI, a doctoral candidate at Cornell University, will document and analyze two dialects of Ch'ol (CTU), an endangered Mayan language of southern Mexico, for her doctoral dissertation. As the two dialects of Ch'ol (Tila and Tumbala) have different ways to express definiteness, the CoPI will connect this research to theories that predict that certain grammatical constructions are prohibited if a language has a definite article. The Tumbala dialect allows for a variety of modifiers to be disconnected from the nouns they modify. The Tila dialect, however, does not allow many modifiers to appear far away to the noun they modify. The hypothesis of this project is that the Tila dialect has more restrictions than the Tumbala dialect because the Tila dialect has a grammaticalized definite article, whereas the Tumbala dialect does not. During fieldwork, the CoPI will record and transcribe narratives, and conduct interviews with speakers. These varied sources of data will create an empirically sound foundation for the dissertation and future work. The outcomes of this project include in-depth documentation of definite articles and nominal constructions in Ch'ol as well as dialectal variation with respect to definiteness and disconnected nominal expressions, both novel contributions to the field. It will also advance linguistic models at the interface of syntax and semantics and inform work on the marking of definiteness crosslinguistically.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822136","Use of Artificial Intelligence towards Automation of Analog Seismogram Digitization","EAR","Geophysics","08/15/2018","08/03/2018","Miaki Ishii","MA","Harvard University","Standard Grant","Paul Raterron","07/31/2021","$154,017.00","","ishii@eps.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","GEO","1574","","$0.00","Seismometers have been recording ground motion since the late 1800s, and about 100 years of the recordings are in analog form (e.g., paper recordings) that cannot be examined using modern techniques.  These data contain information about earthquakes, volcano eruptions, subsurface changes, changes in weather pattern, to name the few, and are vital for understanding various phenomena that affect the Earth and how they evolve over time.  This project aims to make a significant step forward in converting these analog data into usable digital format by introducing artificial intelligence to the conversion process.  Currently, there is a software that takes a record image and generates digital seismograms, but it requires substantial  human interaction making this process slow, impractical, or impossible.  Successful implementation of artificial intelligence will allow more data to be processed quickly for use by the scientific community, which is a significant broader impact. <br/><br/>The project will start by examining the digitized analyses to determine and build the training database to be used for the construction of the neural network. The investigators will also identify steps that will benefit most from implementation of artificial intelligence procedures to decrease human interaction and improve accuracy of the digitization of seismograms.   Neural networks for image classification and object identification are now available and will be examined to find the algorithm that is most suitable for the seismogram digitization process.  The improved digitization software will be openly available to increase users and provide a robust tool to convert analog seismogram images to research-quality digital seismograms.  It will enable the seismological community to retrieve data that for application of modern analyses, and open opportunities for new types of research to be done.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810282","PIC: Hybrid Silicon Electronic-Photonic Integrated Neuromorphic Networks","ECCS","EPMD-ElectrnPhoton&MagnDevices, ","09/01/2018","06/12/2020","Stefan Preble","NY","Rochester Institute of Tech","Standard Grant","Dominique Dagenais","08/31/2021","$523,053.00","Dhireesha Kudithipudi","sfpeen@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","ENG","1517, U181","094E, 095E, 9251","$0.00","Neuromorphic computing is a sub-field of artificial intelligence that implements physical architectures inspired by the learning processes in the brain.  There have been significant efforts to realize neural network architectures using electronic integrated circuit technology.  However, electronic-only hardware is not suitable for high bandwidth applications critical to a modern information world. In contrast, the internet is powered by photonic technologies (lasers, electro-optic modulator and photodetectors) because of light's high bandwidth, speed and low energy consumption. Consequently, this project aims to realize high performance neural networks that utilize light. These photonic neural networks will be integrated on a photonic chip in order to realize scalable and efficient architectures.  However, in order to build neural networks that transcend today's state-of-art, it is necessary to also leverage electronics due to the challenges surrounding photonic memory and amplification, both of which are key to realizing a general purpose neural network. This hybrid approach, where electronics and photonics would be integrated together,  enables the investigation of the broadest class of problems.  In addition to these research aims, this project is an interdisciplinary activity that will provide technical training for future science and engineering professionals.  There will be outreach activities that bring the research to  K-12, undergraduate, and graduate students. Students from underrepresented backgrounds will be actively engaged by providing lab visits with hands-on activities.  Lastly, the education initiatives of AIM Photonics Academy will be leveraged to disseminate the research in the project. Overall this project will impact the broader community with applications in autonomous systems, vision systems, information networks, cybersecurity, robotics and other high bandwidth applications.<br/><br/><br/>This project aims to address two fundamental questions, i) How can photonics maximize functionality in the compute domains?, ii) What neuromorphic algorithms can solve a broad class of problems using photonics?<br/>The overall goal of this project is to demonstrate hybrid silicon electronic-photonic integrated neuromorphic networks. The proposed paradigm leverages the power of optical interference to realize high performance neuromorphic computing networks. Photonic implementations of neural networks offer the inherent advantage that light can easily perform computational tasks that are traditionally challenging to do in electronic-only implementations  (e.g. a Fourier transform can be done optically by simply passing light through a lens). The underlying integrated photonic-electronic network proposed here utilizes a Multimode interference coupler as a neural core (Neuro-MMI) in order to realize interference between multiple inputs and outputs in a compact footprint. The principal investigators propose to realize reconfigurability of the weights in the neural network wrapped around the MMI core. The Neuro-MMI core will be integrated with optoelectronic nonlinear thresholding circuits (along with electronic memory) to realize different classes of neural networks (feed forward neural networks and recurrent neural networks).  Active Neuro-MMI's will be studied to realize on-chip learning and new learning rules will be investigated that are inherent for these topologies. Furthermore, the use of wavelength division multiplexing will be explored  to achieve  dense connectivity and parallelism in order to maximize the performance of the networks. One unique feature of the proposed hybrid photonic-electronic network is the reconfigurability to switch between feed forward and recurrent neural networks on a single chip.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763509","CHS: Medium: Collaborative Research: Activity Recognition for Reducing Delays in Fast-Response Teamwork","IIS","HCC-Human-Centered Computing","10/01/2018","04/15/2020","Aleksandra Sarcevic","PA","Drexel University","Standard Grant","William Bainbridge","09/30/2022","$277,974.00","","aleksandra.sarcevic@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7367","7367, 7924, 9102, 9251","$0.00","Human performance in time-critical teamwork settings relies on appropriate and timely task completion. Time perception, a critical cognitive function that influences team performance, is often skewed in these settings and is impacted by cognitive workload.  This project will address timeliness errors by automatically and unobtrusively modeling and tracking cross-disciplinary task performance through analysis of verbal communication and the use of information artifacts. The resulting model will be used to display alerts about timeliness of critical tasks in a way that supports team members' information needs without increasing their cognitive workload. The benefits and costs of this approach will be evaluated in a simulation setting, measuring its impact on team performance and overall goal accomplishment, as well as its impact on workload and distraction. The application domain for this research is trauma resuscitation, the early evaluation and management of injured patients in the emergency department; the goal is for increased temporal awareness to improve both trauma team efficiency and patient outcomes, saving money and lives.  Further, the project will provide opportunities for interdisciplinary education involving students from computer science and medicine. <br/><br/>This project will develop techniques for monitoring the progress of teamwork and displaying alerts about timeliness of critical tasks. The key system components will include recognizing activities, modeling process deviations and delays, and displaying process information in a way that does not divert attention from the work. Novel techniques for activity recognition in fast-paced and crowded collaborative settings will be based on passive RFID, speech recognition, and computer vision, supplemented by other sensors and digital devices. The proposed research will develop (1) temporal models of verbal communication and digital document interaction during complex activities in fast-paced teamwork for the purpose of automatic activity recognition; (2) approaches for real-time recognition of over a hundred different activities in the presence of up to a thousand RFID tags; and (3) approaches for displaying delay information for rapid assimilation under high task load that learn from workers' responses to improve their usefulness to the team.  Together, the work will provide building blocks for computerized support of teams not just in the trauma domain, but in other domains with complex, interleaved tasks such as surgery, traffic control, and disaster management.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763827","CHS: Medium: Collaborative Research: Activity Recognition for Reducing Delays in Fast-Response Teamwork","IIS","HCC-Human-Centered Computing","10/01/2018","09/11/2018","Ivan Marsic","NJ","Rutgers University New Brunswick","Standard Grant","William Bainbridge","09/30/2022","$700,000.00","","marsic@ece.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7367","7367, 7924, 9102","$0.00","Human performance in time-critical teamwork settings relies on appropriate and timely task completion. Time perception, a critical cognitive function that influences team performance, is often skewed in these settings and is impacted by cognitive workload.  This project will address timeliness errors by automatically and unobtrusively modeling and tracking cross-disciplinary task performance through analysis of verbal communication and the use of information artifacts. The resulting model will be used to display alerts about timeliness of critical tasks in a way that supports team members' information needs without increasing their cognitive workload. The benefits and costs of this approach will be evaluated in a simulation setting, measuring its impact on team performance and overall goal accomplishment, as well as its impact on workload and distraction. The application domain for this research is trauma resuscitation, the early evaluation and management of injured patients in the emergency department; the goal is for increased temporal awareness to improve both trauma team efficiency and patient outcomes, saving money and lives.  Further, the project will provide opportunities for interdisciplinary education involving students from computer science and medicine. <br/><br/>This project will develop techniques for monitoring the progress of teamwork and displaying alerts about timeliness of critical tasks. The key system components will include recognizing activities, modeling process deviations and delays, and displaying process information in a way that does not divert attention from the work. Novel techniques for activity recognition in fast-paced and crowded collaborative settings will be based on passive RFID, speech recognition, and computer vision, supplemented by other sensors and digital devices. The proposed research will develop (1) temporal models of verbal communication and digital document interaction during complex activities in fast-paced teamwork for the purpose of automatic activity recognition; (2) approaches for real-time recognition of over a hundred different activities in the presence of up to a thousand RFID tags; and (3) approaches for displaying delay information for rapid assimilation under high task load that learn from workers' responses to improve their usefulness to the team.  Together, the work will provide building blocks for computerized support of teams not just in the trauma domain, but in other domains with complex, interleaved tasks such as surgery, traffic control, and disaster management.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763355","CHS: Medium: Collaborative Research: Activity Recognition for Reducing Delays in Fast-Response Teamwork","IIS","HCC-Human-Centered Computing","10/01/2018","09/11/2018","Randall Burd","DC","Children's Research Institute","Standard Grant","William Bainbridge","09/30/2022","$230,000.00","","rburd@cnmc.org","111 Michigan Avenue, NW","Washington, D.C.","DC","200102970","3015658483","CSE","7367","7367, 7924","$0.00","Human performance in time-critical teamwork settings relies on appropriate and timely task completion. Time perception, a critical cognitive function that influences team performance, is often skewed in these settings and is impacted by cognitive workload.  This project will address timeliness errors by automatically and unobtrusively modeling and tracking cross-disciplinary task performance through analysis of verbal communication and the use of information artifacts. The resulting model will be used to display alerts about timeliness of critical tasks in a way that supports team members' information needs without increasing their cognitive workload. The benefits and costs of this approach will be evaluated in a simulation setting, measuring its impact on team performance and overall goal accomplishment, as well as its impact on workload and distraction. The application domain for this research is trauma resuscitation, the early evaluation and management of injured patients in the emergency department; the goal is for increased temporal awareness to improve both trauma team efficiency and patient outcomes, saving money and lives.  Further, the project will provide opportunities for interdisciplinary education involving students from computer science and medicine. <br/><br/>This project will develop techniques for monitoring the progress of teamwork and displaying alerts about timeliness of critical tasks. The key system components will include recognizing activities, modeling process deviations and delays, and displaying process information in a way that does not divert attention from the work. Novel techniques for activity recognition in fast-paced and crowded collaborative settings will be based on passive RFID, speech recognition, and computer vision, supplemented by other sensors and digital devices. The proposed research will develop (1) temporal models of verbal communication and digital document interaction during complex activities in fast-paced teamwork for the purpose of automatic activity recognition; (2) approaches for real-time recognition of over a hundred different activities in the presence of up to a thousand RFID tags; and (3) approaches for displaying delay information for rapid assimilation under high task load that learn from workers' responses to improve their usefulness to the team.  Together, the work will provide building blocks for computerized support of teams not just in the trauma domain, but in other domains with complex, interleaved tasks such as surgery, traffic control, and disaster management.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823166","CRI: CI-P: Creating the Largest Speech Emotional Database by Leveraging Existing Naturalistic Recordings","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2018","03/30/2020","Carlos Busso","TX","University of Texas at Dallas","Standard Grant","Tatiana Korelsky","02/28/2021","$115,390.00","","busso@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1714, 7359","7359, 9251","$0.00","This community infrastructure planning project aims to consider the needs from other researchers in the design of the largest publicly available naturalistic speech emotional database, broadening the impact of the corpus across speech processing areas. The project includes a workshop with researchers with relevant but diverse expertise to introduce the current protocol for data collection, and requests their recommendations for improvements. The proposed activity will improve the protocol to address the needs from the community. Affective computing is an important research area aiming to understand, analyze, recognize, and synthesize human emotions. Providing emotion capabilities to current speech-based interfaces can facilitate transformative applications in areas related to Human Computer Interaction (HCI), healthcare, security and defense, education and entertainment. The research infrastructure envisioned in this project will open new opportunities that we cannot address with current speech emotional databases. In the area of affective computing, the proposed corpus will provide suitable training sets to explore learning algorithms that are powerful, but require large amount of labeled data. It is expected that the size, naturalness, and speaker and recording variety in the proposed corpus will allow the community to create robust models that generalize across applications. Improvements on speech emotion recognition systems will facilitate the transition of these algorithms into practical applications, providing unique societal benefits. The proposed infrastructure will also play a key role on other speech processing tasks. For the first time, the community will have the infrastructure to address speaker verification and automatic speech recognition solutions against variations due to emotion.<br/><br/>The proposed infrastructure relies on a novel approach based on emotion retrieval along with crowdsource-based annotations to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. The database considers podcast recordings that are available in audio-sharing websites. Although the approach of building affective databases using media content has been previously explored, the contribution of this study is the use of machine learning algorithms to retrieve audio clips with balanced emotional content, providing natural stimuli with wider spectrum of emotions. The proposed approach relies on automatic algorithms to post-process podcasts and a cost effective annotation process, which make it possible to build large scale speech emotional databases. This approach provides natural emotional renditions that are difficult to obtain with alternative data collection protocols. This project involves the research community from the design of the corpus, which is the key goal in this community infrastructure planning project. The community also play a key role in the selection of target sentences to be emotionally annotated, with novel grand challenges where the goal is to recognize and retrieve target emotional behaviors in unconstrained, unlabeled recordings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1802789","D3SC: CDS&E: Collaborative Research: Development and application of accurate, transferable and extensible deep neural network potentials for molecules and reactions","CHE","Chem Thry, Mdls & Cmptnl Mthds, Leadership-Class Computing","07/15/2018","05/06/2019","Olexandr Isayev","NC","University of North Carolina at Chapel Hill","Standard Grant","Evelyn Goldfield","06/30/2021","$364,365.00","","olexandr@cmu.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","MPS","6881, 7781","026Z, 062Z, 8084, 9216, 9263","$0.00","Adrian Roitberg of the University of Florida and Olexandr Isayev of the University of North Carolina at Chapel Hill are supported by an award from the Chemical Theory, Models and Computational Methods program in the Division of Chemistry. Empirical potentials---also known as force fields---play an essential role in simulating atomic-scale interactions between molecules.  They are used in the computational design of materials and pharmaceuticals.  However, current potentials have been designed to be fast or accurate, but rarely both.  This presents a critical bottleneck for the next phase of predictive chemical computer models.  In this project, Professors Roitberg and Isayev are leveraging state-of-the-art artificial intelligence (AI) to ""learn"" potentials from ultra-large datasets of molecular energies and chemical reactions. The project is creating a new force field, ANI, that is accurate and fast, while also applicable to a broad range of systems in chemistry.  This research has the potential to benefit materials design, renewable energy research, and drug design.  The project is first step toward the use of artificial intelligence techniques to create new materials and molecules beyond what the human imagination can do alone. The research team is engaged in outreach through workshops on molecular simulations, ""Talk science to me"" science for the general public, and the involvement of high school students from the North Carolina School of Science and Math (NCSSM) in the research.<br/> <br/>The objective of this project is to develop a chemically-accurate, extensible, and universal neural network potential, ANI, for use in ""in silico"" organic chemistry experimentation. The range of possible applications for ANI is very broad, from conformational searches to chemical reactions and ligand binding. Through intelligent sampling of new regions of chemical space, the researchers are expanding use cases for ANI to include arbitrary systems containing H, C, N O, F, S, P, Cl and Br atoms. The new design strategy is based on the ANAKIN-ME method, used in implementing the earlier ANI-1 potential. To train ANI-1, a database of wB97x/6-31G* DFT energies for 22 million structural conformations from 60,000 distinct organic molecules was computed through exhaustive, stochastic sampling of conformational and chemical space. Through rigorous benchmarks for organic molecules, biomolecules, and peptides, ANI-1 predicted total and relative energies with RMS errors under 1 kcal/mol, when compared to DFT reference values.  The enhancements being made to ANAKIN-ME are aimed at improving computational efficiency, expanding the range of systems that can be simulated, and achieving < 1 kcal/mol RMS error in comparison to high quality CCSD(T)/CBS quantum chemical energies. These enhancements include reducing the required dataset size for a given set of atom types, to enable inclusion of additional elements and chemistries; expanding training datasets to include data on atomic charges and forces, in addition to energies, and data for charged molecules; and implementing ""query-by-committee"" active learning approaches to facilitate learning of addition, substitution, and elimination chemical reactions by ANI.  The ANI potential is being disseminated through user-friendly open access mechanisms. The implementation of the ANI potential takes advantage of graphics processing unit (GPU) acceleration to run on GPU-enabled workstations and parallel supercomputers.  The ANI software library has a simple Python API and is being integrated with popular molecular modeling and simulation packages such as AMBER, OpenMM and Avogadro.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1802831","D3SC: CDS&E: Collaborative Research: Development and application of accurate, transferable and extensible deep neural network potentials for molecules and reactions","CHE","Chem Thry, Mdls & Cmptnl Mthds","07/15/2018","07/10/2018","Adrian Roitberg","FL","University of Florida","Standard Grant","Evelyn Goldfield","06/30/2021","$435,561.00","","roitberg@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","6881","026Z, 062Z, 8084, 9216, 9263","$0.00","Adrian Roitberg of the University of Florida and Olexandr Isayev of the University of North Carolina at Chapel Hill are supported by an award from the Chemical Theory, Models and Computational Methods program in the Division of Chemistry. Empirical potentials---also known as force fields---play an essential role in simulating atomic-scale interactions between molecules.  They are used in the computational design of materials and pharmaceuticals.  However, current potentials have been designed to be fast or accurate, but rarely both.  This presents a critical bottleneck for the next phase of predictive chemical computer models.  In this project, Professors Roitberg and Isayev are leveraging state-of-the-art artificial intelligence (AI) to ""learn"" potentials from ultra-large datasets of molecular energies and chemical reactions. The project is creating a new force field, ANI, that is accurate and fast, while also applicable to a broad range of systems in chemistry.  This research has the potential to benefit materials design, renewable energy research, and drug design.  The project is first step toward the use of artificial intelligence techniques to create new materials and molecules beyond what the human imagination can do alone. The research team is engaged in outreach through workshops on molecular simulations, ""Talk science to me"" science for the general public, and the involvement of high school students from the North Carolina School of Science and Math (NCSSM) in the research.<br/> <br/>The objective of this project is to develop a chemically-accurate, extensible, and universal neural network potential, ANI, for use in ""in silico"" organic chemistry experimentation. The range of possible applications for ANI is very broad, from conformational searches to chemical reactions and ligand binding. Through intelligent sampling of new regions of chemical space, the researchers are expanding use cases for ANI to include arbitrary systems containing H, C, N O, F, S, P, Cl and Br atoms. The new design strategy is based on the ANAKIN-ME method, used in implementing the earlier ANI-1 potential. To train ANI-1, a database of wB97x/6-31G* DFT energies for 22 million structural conformations from 60,000 distinct organic molecules was computed through exhaustive, stochastic sampling of conformational and chemical space. Through rigorous benchmarks for organic molecules, biomolecules, and peptides, ANI-1 predicted total and relative energies with RMS errors under 1 kcal/mol, when compared to DFT reference values.  The enhancements being made to ANAKIN-ME are aimed at improving computational efficiency, expanding the range of systems that can be simulated, and achieving < 1 kcal/mol RMS error in comparison to high quality CCSD(T)/CBS quantum chemical energies. These enhancements include reducing the required dataset size for a given set of atom types, to enable inclusion of additional elements and chemistries; expanding training datasets to include data on atomic charges and forces, in addition to energies, and data for charged molecules; and implementing ""query-by-committee"" active learning approaches to facilitate learning of addition, substitution, and elimination chemical reactions by ANI.  The ANI potential is being disseminated through user-friendly open access mechanisms. The implementation of the ANI potential takes advantage of graphics processing unit (GPU) acceleration to run on GPU-enabled workstations and parallel supercomputers.  The ANI software library has a simple Python API and is being integrated with popular molecular modeling and simulation packages such as AMBER, OpenMM and Avogadro.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831151","SBIR Phase II:  In-Memory Artificial Neural Network","IIP","SBIR Phase II","09/01/2018","11/19/2019","Wolfgang Hokenmaier","VT","Green Mountain Semiconductor, Inc.","Standard Grant","Steven Konsek","08/31/2021","$760,000.00","","whokenmaier@greenmountainsemi.com","182 Main St., Suite 304","Burlington","VT","054018349","8023438175","ENG","5373","097E, 5373, 8035, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is provided by a novel data processing architecture, utilizing high-parallel in-memory computing for certain recurring and data intensive functions. Traditional computer architecture funnels all data through the central processing unit (CPU). Multiple CPU cores and very high clock frequencies are used to address the issue of ever increasing demands on data processing capability. However, the transportation capacity of data between memory and CPU cores has become a limiting factor creating a 'memory bottleneck'. This limitation is most noticeable in the recent and rapid development of artificial intelligence applications which deploy so called neuromorphic computing techniques, which in turn require a very high parallelism in computation and proportional demands on memory bandwidth. This project performs key repetitive operations within the memory itself, leveraging the inherent parallelism of the memory architecture, thereby avoiding a large percentage of the data transport otherwise required. The resulting elimination of the memory bottleneck provides a path forward for high complexity neuromorphic computing applications such as autonomous navigation used for self-driving cars. Reduced demands on data transport and CPU also significantly reduce power consumption, enabling a wide variety of mobile artificial intelligence applications.<br/><br/>The proposed project investigates the system level integration challenges of a memory-centric neuromorphic computing approach, and aims to demonstrate a seamless integration with existing software platforms currently using traditional neuromorphic computing processors. It is important for a novel hardware platform to be compatible with existing software in order to lower barriers to market entry. This Phase II project also develops the actual semiconductor product which has been investigated in Phase I as a feasibility demonstrator. The Phase II product is based on a non-volatile high density memory architecture, and as such is expected to provide the full capability in terms of both power and operations per second. Once the hardware is available in the second half of the project, these key parameters will be thoroughly characterized and benchmarked against the current state of the art technology. A projection will be made outlining the future scaling potential using ultra high density volatile and non-volatile memory geared towards high complexity neuromorphic computing beyond what is currently possible using existing approaches.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1903577","Exploiting Metal-Insulator-Transition in Strongly Correlated Oxides as Neuron Device for Neuro-Inspired Computing","ECCS","EPMD-ElectrnPhoton&MagnDevices","07/01/2018","11/14/2018","Shimeng Yu","GA","Georgia Tech Research Corporation","Standard Grant","Lawrence Goldberg","07/31/2021","$302,182.00","","shimeng.yu@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1517","100E","$0.00","A radical shift in computing paradigm towards the neuro-inspired computing is attractive for performing data-intensive applications such as image/speech recognitions. The neuro-inspired architecture leverages the distributed computation in the neuron nodes and the localized storage in the synaptic elements. The neuron node today is generally implemented by tens of silicon transistors. Compared to the crossbar array of synaptic elements, the silicon neuron is power-hungry and area-inefficient, thereby reducing the parallelism of computing system. In such context, how to design a single device that can efficiently emulate the neuronal behavior (e.g. integrate-and-fire) is critical to the neuromorphic hardware design. This project aims to exploit the metal-insulator-transition phenomenon in strongly correlated oxides as a compact neuron node that can self-oscillate, namely oxide neuron, to overcome the aforementioned limitations of silicon neuron. The proposed research will have a profound impact on the society that is embracing the artificial intelligence. For instance, a compact design of neuromorphic hardware may enable intelligent information processing on power-efficient mobile platforms, e.g. autonomous vehicle, personalized healthcare, wearable devices, and smart sensors. The objective of the research and education integration is to train undergraduate/graduate students and next-generation workforce with interdisciplinary skills. The cross-layer nature of this project ranging from materials engineering, semiconductor device, circuit-device interaction and artificial neural network provides an ideal platform for this educational goal. The project also plans to engage minority and unrepresentative students in research. Technology transfer will be performed through video or on-site seminars and student internships with industrial collaborators.<br/><br/><br/>The goal of this research is to advance the artificial neuron device design by exploiting the volatile and threshold switching behavior in strongly correlated oxides, with the purpose of significantly reducing the area and energy of the neuron node, and making it compatible for the integration with crossbar array of resistive synaptic elements. The scope of the project is to explore various material systems of the strongly correlated oxides, in particular, NbO2 and SmNiO3 to demonstrate the self-oscillation behavior in the artificial neuron node. When such oxide device is connected with a series synaptic element whose resistance is within the on/off dynamic range of the oxide device, the node voltage between the oxide device and the synaptic element will start self-oscillation, and the oscillation frequency represents the synaptic conductance. This project aims to explore such self-oscillation to emulate the integrate-and-fire neuronal behavior. To achieve the aforementioned research goal, device fabrication, physical and electrical characterization, device modeling, and circuit-device co-design will be performed to demonstrate the feasibility of the concept and further optimize the device performance. The intellectual significance of this project is two folded. From the fundamental science perspective, the physical switching mechanism of metal-insulator-transition in strongly correlated oxides will be investigated. From the applied engineering perspective, the oxide neuron device will be integrated with the resistive crossbar array for demonstration of a neural network for solving a practical problem, i.e. the image pattern classification."
"1813160","CCF-BSF: AF: Small: Algorithms for Interactive Learning","CCF","Algorithmic Foundations","06/01/2018","05/18/2018","Sanjoy Dasgupta","CA","University of California-San Diego","Standard Grant","A. Funda Ergun","05/31/2021","$500,000.00","","dasgupta@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7796","7923, 7926","$0.00","Machine learning classifiers are core components of many of the technologies we use routinely: search engines, speech recognition engines, language translators, assisted driving systems, and so on. These classifiers are typically built by a process of 'supervised learning', in which a computer is given a collection of (input, output) pairs that illustrate a desired behavior (e.g. if the input is this English sentence, the output should be this Spanish sentence) and is told to produce a function that replicates such behavior. This is a rigid form of learning that is known to suffer from a variety of fundamental hurdles; for instance, there are classes of concepts that cannot efficiently be learned in this way. This project will study how such hurdles can be overcome by moving to a more natural learning setup, in which the learning machine is allowed to interact with a human while learning, and receives feedback that is richer than just output values. This research has the potential to influence the way in which machine learning is performed and to broaden its scope of applicability. It is inherently multidisciplinary, and thus part of the project includes community-building activities that will bring together different groups of relevant researchers. There is also an educational component to the project, centered on bringing knowledge of algorithms and machine learning to various student groups that have traditionally been under-represented in STEM disciplines. Interactive learning is a field with great promise in which most of the work to date has consisted of one-off systems geared towards specific applications. This project will aim to bring rigor, formalism, and algorithms with provable guarantees to parts of this field that are currently lacking them.<br/><br/>This project will aim to formalize forms of human feedback (to a learning machine) than are richer than those traditionally studied, such as: simple explanations (e.g. this bird is not a canary because it has the wrong type of beak); attention-focusing; and similarity judgments. The investigators will design algorithms that are able to use these kinds of feedback and have rigorous guarantees, both on correctness and on statistical rates of convergence. The project is particularly focused on overcoming fundamental hardness barriers in learning: learning concept classes that would be intractable to learn in the usual supervised framework; learning with dramatically fewer examples than would normally be needed; adapting to situations in which the distribution of the data is constantly shifting; and improving the results of unsupervised learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750920","CAREER: Advances in Graph Learning and Inference","CCF","Comm & Information Foundations","02/01/2018","01/29/2018","Chinmay Hegde","IA","Iowa State University","Continuing Grant","Phillip Regalia","12/31/2019","$160,413.00","","chinmay.h@nyu.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7797","1045, 7935","$0.00","Graph-based data processing algorithms impact a variety of application domains ranging from transportation networks, artificial intelligence systems, cellphone networks, social networks, and the Web. Nevertheless, the emergent big-data era poses key conceptual challenges: several existing graph-based methods used in practice exhibit unreasonably high running time; several other methods operate in the absence of correctness guarantees. These challenges severely imperil the safety and reliability of higher-level decision-making systems of which they are a part. This research introduces an innovative new computational framework for graph learning and inference that addresses these challenges. Specific applications studied in this project include: better approaches for monitoring roadway congestion and identify traffic incidents in a timely manner; root-cause analysis of complex events in social networks; and design of better personalized learning systems, lowering educational costs and increasing quality nationwide. Activities include integrated programs to increase participation of women and under-represented minorities in the computational sciences. <br/><br/>From a technical standpoint, the investigator pursues three research themes: (i) designing scalable non-convex algorithms for learning the edges (and weights) of an unknown graph given a sequence of independent static and/or time-varying local measurements; (ii) designing new approximation algorithms for utilizing the structure of a given graph to enable scalable post-hoc decision making in complex systems; (iii) developing provable algorithms for training special families of artificial neural networks, and filling gaps between rigorous theory and practice of neural network learning. Progress in each of the above themes will be extensively evaluated using real-world data from engineering applications including social network data, highway monitoring data, and fluid-flow simulation data. Collaborations with domain experts in each of these application areas will ensure that the new theory, tools, and software emerging from this project will lead to meaningful societal benefits."
"1908926","Workshop: Transforming Civil & Environmental Engineering Curriculum: Big Data, Machine Learning, and AI Innovations Impact on CEE Education; Alexandria, Virginia; 10 December 2","CMMI","EngEd-Engineering Education, CIS-Civil Infrastructure Syst","11/15/2018","12/06/2018","Lucio Soibelman","CA","University of Southern California","Standard Grant","Yueyue Fan","04/30/2020","$55,000.00","","Soibelman@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","ENG","1340, 1631","029E, 036E, 1057, 1340, 1631, 7556, 9102","$0.00","Over the last several years, research and development on Machine Learning, Big Data, and Artificial Intelligence has demonstrated many opportunities for creating novel concepts, applications, tools and systems with lasting and transformative impacts on nearly every industry and economic sector. As Civil and Environmental Engineers adopt new computer technologies, computerized data are becoming more and more available. There exist numerous opportunities to exploit and extract knowledge from this vast amount of data through the development of frameworks and algorithms that support the acquisition, modeling, management, and analysis of such diverse infrastructure-oriented data. With sensors everywhere, the field of Civil and Environmental Engineering is undergoing transformative changes in the physical and natural systems we live in and travel through. The massive (big) data generated from these sensors presents tremendous opportunities for a wide range of possibilities that could only be imagined before. Consequently, Civil and Environmental engineers who have traditionally relied upon small data (generated through experiments, surveys, and labs etc.) face novel challenges in utilizing the newly-generated big data to fully exploit opportunities. This workshop will convene a diverse group of participants from universities, industry and stakeholders in civil and environmental engineering, computer science, machine learning, data analytics and artificial intelligence to chart a course of action to enable the transformation of Civil and Environmental Engineering (CEE) curricula and fully prepare the CEE community for both the opportunities and challenges brought by sensors everywhere, internet of things devices, and the massive amount of resulting data. <br/><br/>The workshop will seek to deliver the following specific impacts to CEE departments and to the research community: (1) Provide insight into the current availability of expertise on big data, machine learning, and artificial intelligence within Civil and Environmental Engineering departments; (2) Enhance understanding of the emerging educational needs for different areas of expertise within Civil and Environmental Engineering related to big data, data analytics, machine learning, and artificial intelligence; (3) Guide Civil and Environmental Engineering departments in aligning their curriculum with current needs for the profession; (4) Provide the leadership and guidance for the future development of machine learning, big data, and artificial intelligence tutorial modules specifically aligned to Civil and Environmental Engineering curriculum needs. At a broader level, it is intended that the workshop output will serve as a beacon for bringing advanced computing techniques to Civil and Environmental Engineering, as well as other Engineering disciplines, supporting novel curriculum development. The workshop will provide a roadmap and the leadership for the development of the required tools that will enhance the capabilities of future engineering graduates enhancing competitiveness of the engineering profession and improving engineering education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801426","SaTC: CORE: Medium: Collaborative: Towards Trustworthy Deep Neural Network Based AI: A Systems Approach","CNS","Secure &Trustworthy Cyberspace","08/01/2018","07/20/2018","Suman Jana","NY","Columbia University","Standard Grant","Sandip Kundu","07/31/2021","$300,000.00","","suman@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8060","025Z, 7434, 7924","$0.00","Artificial intelligence (AI) is poised to revolutionize the world in fields ranging from technology to medicine, physics and the social sciences. Yet as AI is deployed in these domains, recent work has shown that systems may be vulnerable to different types of attacks that cause them to misbehave; for instance, attacks that cause an AI system to recognize a stop sign as a speed-limit sign. The project seeks to develop methodologies for testing, verifying and debugging AI systems, with a specific focus on deep neural network (DNN)-based AI systems, to ensure their safety and security. <br/><br/>The intellectual merits of the proposed research are encompassed in four new software tools that will be developed: (1) DeepXplore, a tool for automated and systematic testing of DNNs that discovers erroneous behavior that might be either inadvertently or maliciously introduced; (2) BadNets, a framework that automatically generated DNNs with known and stealthy misbehaviours in order to stress-test DeepXplore; (3) SafetyNets; a low-overhead scheme for safe and verifiable execution of DNNs in the cloud; and (4) VisualBackProp; a visual debugging tool for DNNs. The synergistic use of these tools for secure deployment of an AI system for autonomous driving will be demonstrated.<br/><br/>The project outcomes will significantly improve the security and safety of AI systems and increase their deployment in safety- and security-critical settings, resulting in broad societal impact. The results of the project will be widely disseminated via publications, talks, open access code, and competitions hosted on sites such as Kaggle and NYU's annual Cyber-Security Awareness Week (CSAW). Furthermore, students from under-represented minority groups in science, technology, engineering and mathematics (STEM) will be actively recruited and mentored to be leaders in this critical area.  <br/><br/>The code for this project will be made publicly available via github.com. Preliminary code for the tools that will be developed is already hosted on this website, including DeepXplore (https://github.com/peikexin9/deepxplore) and BadNets (https://github.com/Kooscii/BadNets/). These repositories will be linked to from a homepage that describes the entire project. The project homepage will be hosted on wp.nyu.edu/mlsecproject.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813583","RI: Small: 3D Reconstruction via Differential Rendering and Deep Learning","IIS","Robust Intelligence","09/01/2018","05/21/2020","Matthias Zwicker","MD","University of Maryland College Park","Standard Grant","Jie Yang","08/31/2021","$453,000.00","","zwicker@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7923, 9251","$0.00","Digitally reconstructing the 3D shapes of real-world objects is a core technology that enables a very wide range of applications, such as autonomous robot navigation; 3D printing for personal purposes or reverse engineering; archiving and virtual heritage; creating assets for movies, games, and augmented and virtual reality; or large-scale reconstruction for geographical information systems. This project develops novel computer algorithms to reconstruct the 3D shapes of objects using digital images as inputs. It addresses significant limitations of current techniques that often lead to inaccurate results in real-life applications. To achieve this, the project follows an innovative approach leveraging artificial intelligence techniques to understand 3D shapes based on digital images. The formulation of 3D shape reconstruction using artificial intelligence methods represents an important scientific advancement that promises further advances in the research field. A student-led augmented reality (AR) and virtual reality (VR) club gains first-hand experience with state of the art research and experiments with artificial intelligence-based 3D reconstruction to design innovative AR and VR applications.<br/><br/>This research develops algorithms building on two key techniques, differentiable rendering and deep learning. Combining these two methods leads to synergies that can overcome the limitations of current algorithms. Rendering is the process of algorithmically evaluating an image formation model, which may include sophisticated light transport effects such as non-diffuse surfaces, shadows, and indirect illumination, to compute an image of a virtual 3D object or environment. Using automatic differentiation (AD), a differentiable renderer calculates the partial derivatives of pixel values of rendered images with respect to all unknown model parameters of the virtual 3D model. Leveraging the power and generality of AD and differentiable rendering allows to overcome the overly simplistic image formation models common in previous work. In addition, multi-view reconstruction is often ill-posed because of the large number of unknown parameters and the limited information present in a set of views. Therefore, strong priors and robust error metrics are required. This work obtains these error metrics and priors using large-scale shape and image databases and deep learning techniques, to capture the full complexity of real-world objects. Crucially, it connects deep learning to the unknown 3D model parameters through differentiable rendering, which makes it possible to leverage gradient-based optimization techniques to solve for the desired 3D shapes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801495","SaTC: CORE: Medium: Collaborative: Towards Trustworthy Deep Neural Network Based AI: A Systems Approach","CNS","Secure &Trustworthy Cyberspace","08/01/2018","07/20/2018","Siddharth Garg","NY","New York University","Standard Grant","Sandip Kundu","07/31/2021","$899,990.00","Brendan Dolan-Gavitt, Anna Choromanska","sg175@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","8060","025Z, 7434, 7924","$0.00","Artificial intelligence (AI) is poised to revolutionize the world in fields ranging from technology to medicine, physics and the social sciences. Yet as AI is deployed in these domains, recent work has shown that systems may be vulnerable to different types of attacks that cause them to misbehave; for instance, attacks that cause an AI system to recognize a stop sign as a speed-limit sign. The project seeks to develop methodologies for testing, verifying and debugging AI systems, with a specific focus on deep neural network (DNN)-based AI systems, to ensure their safety and security. <br/><br/>The intellectual merits of the proposed research are encompassed in four new software tools that will be developed: (1) DeepXplore, a tool for automated and systematic testing of DNNs that discovers erroneous behavior that might be either inadvertently or maliciously introduced; (2) BadNets, a framework that automatically generated DNNs with known and stealthy misbehaviours in order to stress-test DeepXplore; (3) SafetyNets; a low-overhead scheme for safe and verifiable execution of DNNs in the cloud; and (4) VisualBackProp; a visual debugging tool for DNNs. The synergistic use of these tools for secure deployment of an AI system for autonomous driving will be demonstrated.<br/><br/>The project outcomes will significantly improve the security and safety of AI systems and increase their deployment in safety- and security-critical settings, resulting in broad societal impact. The results of the project will be widely disseminated via publications, talks, open access code, and competitions hosted on sites such as Kaggle and NYU's annual Cyber-Security Awareness Week (CSAW). Furthermore, students from under-represented minority groups in science, technology, engineering and mathematics (STEM) will be actively recruited and mentored to be leaders in this critical area.  <br/><br/>The code for this project will be made publicly available via github.com. Preliminary code for the tools that will be developed is already hosted on this website, including DeepXplore (https://github.com/peikexin9/deepxplore) and BadNets (https://github.com/Kooscii/BadNets/). These repositories will be linked to from a homepage that describes the entire project. The project homepage will be hosted on wp.nyu.edu/mlsecproject.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761548","Discovering and Demonstrating Linguistic Features for Language Documentation","BCS","IIS Special Projects, Robust Intelligence, DEL","08/15/2018","02/27/2020","Graham Neubig","PA","Carnegie-Mellon University","Standard Grant","D.  Langendoen","01/31/2022","$458,000.00","","gneubig@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","7484, 7495, 7719","075Z, 1311, 7484, 7495, 7719, 9251","$0.00","Documenting endangered languages is matter of great urgency, but is also a time-consuming process.  Annotation and curation of speech and text data, searching for interesting, prototypical, or atypical entries, and marshalling examples for pedagogy or publication are still mostly manual processes.  This project aims to speed these processes by (1) creating better tools for the automated analysis of<br/>smaller and partially-annotated corpora, which have potential to reduce the amount of time required by linguists to manually annotate these corpora, and (2) creating better methods for linguists to browse their collected data and answer questions about the characteristics of the language at hand. The intellectual contribution of this proposal will lie in the development of new computational methods for natural language processing (NLP) for endangered languages, and their evaluation, both in controlled environments and as a tool for linguists in the field. It will also have broader impact in the creation of new tools and standards for linguistic documentation,increased collaboration between linguists and computer scientists, and training of a graduate student in the technologies and practices necessary to move this collaboration forward. The training component will increase the STEM workforce capacity in computational linguistics, important given the need for more advanced tools in working on languages that are underdocumented and spoken in countries that are key to national interests. <br/><br/>As a specific methodology to realize this vision, this project focuses on recent development of massively multilingual NLP models based on neural networks. These methods work by creating NLP using data from a large number of languages, then using the information gleaned from these languages to improve the accuracy of processing on a new language with a paucity of training data. Within this framework, three major research questions will be examined: (1) How can these techniques be efficiently applied to very-low-resource languages,especially those in the early stages of text collection? (2) What methods can be used to move beyond sentence-by-sentence analyses, and synthesize information about the entirety of the language to propose a simple grammatical specification? (3) Is it possible to provide examples that support typological predictions for a linguist to read and learn more about the nuances of the language they are analyzing? All three of these research questions will be examined in a rigorous process of devising methods, testing on existing data sets for well-resourced languages, and finally deployment to field linguists to examine how they improve the efficiency or accuracy of the language documentation process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814522","RI: Small: Applying discrete reasoning steps in solving natural language processing tasks","IIS","Robust Intelligence","08/01/2018","06/29/2020","Gregory Durrett","TX","University of Texas at Austin","Standard Grant","Tatiana Korelsky","07/31/2021","$457,212.00","","gdurrett@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495","075Z, 7495, 7923, 9251","$0.00","Modern natural language processing systems are effective at shallow analysis of unstructured text data, performing tasks such as discovering events, identifying the actors of those events, and grouping events with the same actors. Neural networks help make these systems robust to effects like paraphrasing, but still capture mostly superficial text patterns. To answer deeper questions about things like causal relationships between the events in a text, a system might need to combine several pieces of information, abstract away irrelevant details, and incorporate prior world knowledge to arrive at an answer. This project aims to develop systems that can address these challenges: these systems explicitly model reasoning over text and draw on the power of neural networks to do this reasoning in a nuanced way. Such reasoning is explicitly taught to the systems via ""handholding"" supervision, which encourages the systems to mimic how humans solve a problem and helps them generalize better to new problem instances. This alignment with what humans do also serves to expose the systems' decision-making processes; it provides a form of explanation of their behavior so that one may evaluate them against desired criteria such as equitability.<br/><br/>This proposal's technical innovation is focused on two fronts: designing latent variable models and exploiting new types of handholding supervision during model training. These techniques are explored in the context of three challenging problems requiring complex reasoning: (1) solving mathematical word problems; (2) resolving coreference using world knowledge; (3) answering questions from documents. For each problem, new models are proposed centering around discrete derivations of answers, which draw on state-of-the-art tools like attention-based recurrent neural networks to capture the larger context of the reasoning process. The discreteness of the models' decisions provides an anchor to incorporate auxiliary supervision, which is hard to do in fully end-to-end neural models. The nature of the handholding supervision depends on the task and is a combination of incidental supervision, heuristically identified derivations, and targeted human annotation. Each of the addressed problems tests different aspects of the approach, such as handling complex derivations and incorporating world knowledge, and these problems yield concrete evaluation frameworks to understand the efficacy of the proposed techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827009","IRES Track I: International Research Experience for Students on Non-Volatile Processor Based Self-Powered Embedded Systems","OISE","IRES Track I: IRES Sites (IS)","10/01/2018","07/27/2018","Jingtong Hu","PA","University of Pittsburgh","Standard Grant","Maija Kukla","09/30/2021","$299,999.00","","jthu@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","O/D","7727","5978, 9200","$0.00","The objective of this project is to provide U.S. students with valuable research experience related to battery-less embedded systems in future Internet of Things (IoT) in China, which has one of the world's largest electronic industry and market. The project will select five (5) graduate students and two (2) undergraduate students nation-wide each year and support them to visit Tsinghua University (THU) over a period of eight (8) weeks. <br/><br/>The vision of IoT is to embed small computers into objects around our daily life to improve our economy and societal well-being. It is estimated that the Internet of Things (IoT) will consist of almost 50 billion objects by 2020. However, one of the biggest challenges is how to power these billions of embedded devices since batteries need frequent recharging and impose health and environmental concerns. In this project, the students will work with renowned research groups at THU to develop future IoT sensors that can reply on energy harvested from ambient environment sources, such as solar energy, radio frequency energy, kinetic energy, and thermal energy. Since ambient energy is intrinsically intermittent, the students will conduct research on nano-scale device, circuits, system, and software to realize computer systems that can work with intermittent energy supply. Such systems will dramatically improve our capability of collecting data while reducing the maintenance costs. Potential applications include smart healthcare, smart building and infrastructure monitoring, smart environmental monitoring, etc. This IRES project will not only offer U.S. students an opportunity to work in one of the best engineering research universities in China, but also fully immerse them in a foreign environment and improve their talents in working with a global team. <br/><br/>The project contains well-planned recruitment, preparation, mentoring and post-trip activities. The proposed research targets at nano-scale device, circuits, system, and software issues in battery-less embedded systems that need to be addressed urgently. The first project aims to develop fast, energy efficient, and uniform write operations for nano-scale memory device, which is critical to tolerate intermittency of harvested energy. The project addresses a fundamental inconsistency issue when the NVP is interacting with external volatile peripheral devices, which is one of the most imminent roadblocks towards wide application of NVPs. <br/>The second project addresses a fundamental inconsistency issue when the battery-less device is collecting data and communicating with external devices. The project aims to develop efficient and accurate binarized neural network for individual ultraviolet (UV) exposure pattern recognition system running on nonvolatile IoT platform, which could be generalized to many other applications. <br/> The third project aims to develop efficient and accurate artificial intelligence for individual ultraviolet (UV) exposure pattern recognition system running on battery-less IoT platform, which could be generalized to many other applications. It can be anticipated that with the close interaction with THU and the Beijing Innovation Center for Future Chips, the breakthroughs made from these projects can have a direct impact on future IoT market.<br/><br/>The project will enable U.S. students to conduct high-quality research on realizing battery-less embedded systems, in collaboration with their faculty mentors in THU. Such experiences expose U.S. students to the international research community at a critical early stage in their careers. It is expected that through participating in this program, U.S. students will gain extensive experience on the<br/>research of embedded computer systems, on the culture of China, and on performing and collaborating in an international environment in general. The experience will also be shared to the broader community through the personal social media such as Facebook, Twitter, and Youtube, Web2.0 based forum, carefully integrated activities such as research for undergraduate students, minorities and underrepresented groups, as well as outreach events for local K-12 schools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827427","Collaborative Research: CompCog: Achieving Analogical Reasoning via Human and Machine Learning","BCS","Perception, Action & Cognition, IIS Special Projects","08/15/2018","08/15/2018","Alan Yuille","MD","Johns Hopkins University","Standard Grant","Michael Hout","07/31/2021","$269,869.00","","ayuille1@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","SBE","7252, 7484","075Z, 7252","$0.00","Despite recent advances in artificial intelligence, humans remain unmatched in their ability to think creatively. Intelligent machines can use massive data to learn to identify patterns that are similar to learned examples, but people can use very small amounts of data to discover deep similarities between situations that are superficially very different (e.g., engineers have devised a cooling system for buildings using principles adapted from termite mounds). This type of creative thinking depends on analogy: the ability to find and exploit resemblances based on relations among entities, rather than solely on superficial appearances. The present investigation aims to show how relations can be learned from examples (in the form of either texts or pictures) and then used to reason by analogy. The work integrates recent advances in machine learning with more human-like learning mechanisms. Improved analogy models will increase the power of computer-based information retrieval, allowing both text and pictures to serve as retrieval cues to search large databases for items that are analogous in relational structure. The large analogy datasets generated for the project will be made publically available. More flexible search engines will help to automate creative tasks such as engineering design. Identifying the computational basis for relation learning and analogical reasoning will guide development of artificial intelligence systems by providing more efficient learning mechanisms. The research team is integrating research and education activities by using this project as a training opportunity in interdisciplinary research, encompassing psychology, statistics, computer science and mathematics. <br/><br/>The research will integrate advanced computational approaches with behavioral experiments on human relation learning and analogical reasoning, using both texts and pictures as inputs. The work is guided by cognitive theory on learning and reasoning, and exploits recent advances in the field of machine vision. The project includes the creation and validation of multiple databases of analogy problems. Experiments will be performed to establish human performance levels in a variety of tasks. Computational models will be developed by synergizing big-data learning through deep networks with small-data learning through Bayesian modeling. Models will be evaluated by comparison with human benchmarks. By addressing issues that arise in reasoning from natural inputs such as texts and pictures, the models to be developed will generalize to situations that people encounter in their daily life.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827374","Collaborative Research: CompCog: Achieving Analogical Reasoning via Human and Machine Learning","BCS","Perception, Action & Cognition, IIS Special Projects","08/15/2018","08/15/2018","Keith Holyoak","CA","University of California-Los Angeles","Standard Grant","Michael Hout","07/31/2021","$477,000.00","Hongjing Lu","holyoak@lifesci.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","7252, 7484","075Z, 7252","$0.00","Despite recent advances in artificial intelligence, humans remain unmatched in their ability to think creatively. Intelligent machines can use massive data to learn to identify patterns that are similar to learned examples, but people can use very small amounts of data to discover deep similarities between situations that are superficially very different (e.g., engineers have devised a cooling system for buildings using principles adapted from termite mounds). This type of creative thinking depends on analogy: the ability to find and exploit resemblances based on relations among entities, rather than solely on superficial appearances. The present investigation aims to show how relations can be learned from examples (in the form of either texts or pictures) and then used to reason by analogy. The work integrates recent advances in machine learning with more human-like learning mechanisms. Improved analogy models will increase the power of computer-based information retrieval, allowing both text and pictures to serve as retrieval cues to search large databases for items that are analogous in relational structure. The large analogy datasets generated for the project will be made publically available. More flexible search engines will help to automate creative tasks such as engineering design. Identifying the computational basis for relation learning and analogical reasoning will guide development of artificial intelligence systems by providing more efficient learning mechanisms. The research team is integrating research and education activities by using this project as a training opportunity in interdisciplinary research, encompassing psychology, statistics, computer science and mathematics. <br/><br/>The research will integrate advanced computational approaches with behavioral experiments on human relation learning and analogical reasoning, using both texts and pictures as inputs. The work is guided by cognitive theory on learning and reasoning, and exploits recent advances in the field of machine vision. The project includes the creation and validation of multiple databases of analogy problems. Experiments will be performed to establish human performance levels in a variety of tasks. Computational models will be developed by synergizing big-data learning through deep networks with small-data learning through Bayesian modeling. Models will be evaluated by comparison with human benchmarks. By addressing issues that arise in reasoning from natural inputs such as texts and pictures, the models to be developed will generalize to situations that people encounter in their daily life.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1745463","Convergence HTF: A Research Coordination Network to Converge Research on the  Socio-Technological Landscape of Work in the Age of Increased Automation","IIS","INSPIRE","01/01/2018","08/23/2017","Kevin Crowston","NY","Syracuse University","Standard Grant","Andruid Kerne","12/31/2022","$499,796.00","Jeffrey Nickerson, Ingrid Erickson","crowston@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","8078","060Z, 063Z","$0.00","The landscape of jobs and work is changing rapidly, driven by the development of new technologies. Intelligent, automated machines and services are a growing part of jobs and the workplace. New technologies are enabling new forms of learning, skills assessments, and job training. The potential benefits of these technologies include increased productivity and job satisfaction, and more job opportunities. But technology connected to work can also come with risks. This research coordination network (RCN) addresses the future of work at the human-technology frontier by focusing on the use of intelligent machines in work settings. The RCN supported by this award will promote convergence across computer science, engineering, and social and behavioral science disciplines to define and address key challenges and research imperatives in the future of work at the human-technology frontier with intelligent machines. This convergence RCN will employ deep integration of knowledge, theories, methods, and data from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. The results will include the identification and sharing of new research directions and tools to reinforce positive outcomes and mitigate negative consequences of intelligent machines in work settings. Ultimately this has the power to strengthen the U.S. economy, and improve worker performance and job satisfaction.<br/><br/>This RCN will focus on advancing the knowledge needed to develop actionable design principles that attend to both sides of the human-technology frontier in work settings that use intelligent machines. Such machines include not only autonomous robots and vehicles, but also algorithms and machine learning processes that support all types of autonomous behavior. At present, the technology side of this frontier is advancing more rapidly than the human side: people, organizations, legal frameworks, and social values, to name a few. What is necessary to bring these two side into alignment is a systems design approach that draws on both social and technological requirements as well as their interdependencies. This RCN aims to adopt this goal, thereby developing the knowledge needed to ensure that the benefits of intelligent machines are gained while the negative consequences reduced. <br/><br/>This RCN will bring together investigators from many disciplines including computer science (artificial intelligence, machine learning), robotics, human computer interaction, cognitive science, economics, sociology, law, organizational science, ergonomics, industrial and organizational psychology, engineering, and information systems, to communicate, coordinate, and integrate their research and educational activities across disciplinary and organizational boundaries. Toward this goal, this award will support three primary RCN activities over its five-year term. First, the RCN will organize annual Convergence Conferences that will focus on the contribution of convergent research on topics regarding the socio-technological landscape of work in the age of increased automation. Second, it will support a series of workshops at different disciplinary conferences to expand the reach of the network and to consolidate, test, verify, and evolve research ideas as they develop. Third, the RCN will maintain a set of shared online resources to support the community and its research efforts."
"1763761","SHF: Medium: Time Based Deep Neural Networks: An Integrated Hardware-Software Approach","CCF","Software & Hardware Foundation","05/01/2018","07/28/2020","Chris Kim","MN","University of Minnesota-Twin Cities","Continuing Grant","Sankar Basu","04/30/2022","$900,000.00","Sachin Sapatnekar, Sachin Sapatnekar, Qi Zhao","chriskim@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","7924, 7945","$0.00","Recent advancements in deep learning hardware and algorithms are providing computers with unprecedented levels of human-like intelligence for applications such as self-driving cars, patient diagnosis and treatment, speech processing, strategy games, and education. Traditional deep learning algorithms rely on powerful computers tethered to the cloud which incurs a large communication overhead, requires extensive computing resources, and compromises privacy and security. There is a strong consensus among experts that the next frontier in deep learning will be highly-efficient neural network processors running on mobile platforms. This project aims at developing a compact and low power alternative to conventional deep learning computing hardware, specifically targeted for edge devices. The proposed approach is based on a novel computing concept called time-based circuits, which can deliver a similar level of inference performance at only a fraction of the power consumption compared to traditional methods. Throughout the project, the investigators will consider transferring the new neural network computing methods to industry. The new time-based deep learning computation methods will be incorporated into the graduate and undergraduate curricula, as well as K-12 outreach activities, of the electrical engineering and computer science departments at the University of Minnesota.<br/><br/>This project will focus on both hardware and software techniques for enabling deep learning applications on resource-constrained mobile platforms. On the hardware side, the team will demonstrate a prototype low-power deep neural network processor where internal operations such as convolution, pooling, and activation functions are performed entirely in the time domain. On the software side, the team will develop pruning, approximation, and hybrid approaches that can effectively reduce the complexity of deep neural networks with minimal impact on the overall inference accuracy. A unique aspect of this project is the continual interaction between the hardware and software groups to deliver the first fully time-based deep neural network engine targeted for edge devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831250","SBIR Phase II:  Virtual Learning Assistants for Open Response Assessments","IIP","SBIR Phase II","09/01/2018","07/06/2020","Dharmendra Kanejiya","CA","Cognii, Inc.","Standard Grant","Rajesh Mehta","08/31/2021","$800,000.00","","dharm@cognii.com","169 11th St","San Francisco","CA","941032533","6178991744","ENG","5373","5373, 8031, 8240","$0.00","This SBIR Phase II project focuses on creating scalable Virtual Learning Assistant technology for automatic educational assessments using open response questions. Educational researchers and experts believe that the best pedagogies responsible for improving students' learning outcomes involve (i) open response questions assessments and (ii) one-to-one instructional tutoring. Students learn better when they are given an opportunity to construct answers in their own words instead of selecting from multiple choices and when they receive immediate guidance and coaching. However, these two pedagogies are very time consuming and expensive to implement, making them very difficult to scale. The proposed project will apply the most advanced technologies such as Artificial Intelligence and Natural Language Processing to solve both these problems. Students will benefit from the interactive formative assessment that engages them in a natural language conversation. This innovation is applicable across the grade levels in K-12, higher education, and adult learning and across the subject areas including the sciences. It will facilitate implementation of more rigorous academic standards and make online education more effective. This innovation will improve students' learning outcomes, save teachers' time and reduce the cost of delivering high quality engaging education at a large scale.<br/><br/>This project will create a new type of virtual assistant technology that is exclusively focused on education. The proposed Virtual Learning Assistant (VLA) will advance the conversational AI technology to create pedagogically rich learning and assessment environments for any topic in a content area. The VLA is uniquely distinct from general purpose virtual assistants in its ability to evaluate an open response answer instead of merely serving information. This project will investigate and create various algorithms for processing natural language input arising in an educational setting across different subjects or topics. The resulting mobile and web based product will allow teachers to create new high-quality assessment items with minimal input and assign them to their students. When a student answers a question, the VLA will analyze it instantly for linguistic syntax and semantics using statistical and deterministic knowledge representations. The VLA will generate not only a numerical score reflecting the accuracy of the answer, but also a qualitative feedback that will guide the student towards conceptual mastery of the topic. As part of this Phase II research, a pilot study will be conducted each year involving teachers and students to study the efficacy of the VLA and its scalability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836362","CBMS Conference: Topological Methods in Machine Learning and Artificial Intelligence","DMS","INFRASTRUCTURE PROGRAM","09/01/2018","07/23/2019","Ben Cox","SC","College of Charleston","Standard Grant","Swatee Naik","08/31/2019","$35,960.00","Robert Mignone, Annalisa Calini","coxbl@cofc.edu","66 GEORGE ST","CHARLESTON","SC","294240001","8439534973","MPS","1260","7556, 9150","$0.00","This National Science Foundation award supports the NSF-CBMS regional conference on Topological Methods in Machine Learning and Artificial Intelligence, hosted by the College of Charleston in Charleston, South Carolina, during the week of May 13-17, 2019. The conference will feature Professor Gunnar Carlsson of Stanford University and Ayasdi Inc. as the Principal Lecturer. Professor Carlsson will deliver a series of ten lectures introducing participants to the fast-emerging field of Topological Data Analysis, which employs many of the techniques commonly used in topology, the study of shape, to analyze massive and complex data sets across multiple application domains. The conference will benefit a broad group of participants as data science is rapidly establishing itself as an interdisciplinary discipline with many high-impact applications. Main targets of the lecture series and the ensuing monograph will be applications to the medical sciences, including, e.g., better targeting and prediction of diseases and improved patient care, though the lectures will benefit a far larger constituency. The great majority of the NSF-supported participants will be recruited from amongst early career researchers, graduate students, minorities, and women. <br/> <br/>Topological Data Analysis refers to the use of topology as a tool for understanding and interacting with large and complex data sets. It should be viewed as another step in the development of Machine Learning. Many of the techniques used extensively in topology - including the combinatorial construction of spaces as simplicial complexes, homology and cohomology, local to global methods for computation and application, and the organizing power of the language of category theory and functoriality - all play important roles in the development of this subject. Professor Carlsson?s lecture series and the resulting monograph will introduce students and researchers to this rapidly emerging field. Topics will include topological modeling of data; machine learning; applications of homology to shape analytic tasks, statistics of image patches, and viral evolution; adapting local-to-global methods from topology to point cloud situations; persistence landscapes and persistence images with applications to drug discovery; clustering; and algorithms as data sources. The conference website is at http://math.cofc.edu/CBMS-TDA2019/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824304","Collaborative Research: Deep Inference - Artificial Intelligence for Structural Estimation","SES","Economics","09/15/2018","08/10/2018","Elena Manresa","NY","New York University","Standard Grant","Nancy Lutz","08/31/2020","$84,917.00","","em1849@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","1320","1320, 9179","$0.00","In order to evaluate the effect of economic policies such as extended health care coverage or changes in the minimum wage, economists use structural models which powerfully describe the mechanism at work, but estimating structural models is typically challenging.  A main tool for estimating such structural models is inference via simulation.  For different parametrizations of the model, synthetic data is generated via the model, and the parameters generating data that most closely resembles observed data are used as estimates. Recent modern artificial intelligence methods such as deep learning for image recognition are based on this same principle. These methods have been achieving impressive results over the past years. Therefore, this research takes advantage of such powerful tools in modern pattern recognition for structural estimation in economics. <br/><br/>This research considers a set-up where individual outcomes are a known function of exogenous variables and an error whose distribution is known up to a finite dimensional vector of parameters. The goal is to estimate the finite dimensional parameter. The investigators adopt the generative adversarial network approach (GANs) to find the parameter value such that given a discriminator, a device that can accurately distinguish data generated using the model from real data, is unable to do so when the data is generated according to such parameter value. The method developed in this research differs from other simulation-based minimum distance estimators in that the distance is adaptive. That is, the discriminator learns the features of the data that are best at distinguishing real from synthetic data as opposed to hard-coding what features of the data to match. This adaptability property has proven powerful in pattern recognition tasks. In structural estimation, adaptability can translate into alleviating the curse of dimensionality, and obtaining parameters that are able to more closely match entire distributions of data, as opposed to a set of pre-specified moments. This estimation framework should be useful in applications were distributional effects and heterogeneity are first order to evaluate the effect of a particular policy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824365","Collaborative Research: Deep Inference - Artificial Intelligence for Structural Estimation","SES","Economics","09/15/2018","08/10/2018","Guillaume Pouliot","IL","University of Chicago","Standard Grant","Nancy Lutz","08/31/2020","$82,384.00","","guillaumepouliot@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","SBE","1320","1320, 9179","$0.00","In order to evaluate the effect of economic policies such as extended health care coverage or changes in the minimum wage, economists use structural models which powerfully describe the mechanism at work, but estimating structural models is typically challenging.  A main tool for estimating such structural models is inference via simulation.  For different parametrizations of the model, synthetic data is generated via the model, and the parameters generating data that most closely resembles observed data are used as estimates. Recent modern artificial intelligence methods such as deep learning for image recognition are based on this same principle. These methods have been achieving impressive results over the past years. Therefore, this research takes advantage of such powerful tools in modern pattern recognition for structural estimation in economics. <br/><br/>This research considers a set-up where individual outcomes are a known function of exogenous variables and an error whose distribution is known up to a finite dimensional vector of parameters. The goal is to estimate the finite dimensional parameter. The investigators adopt the generative adversarial network approach (GANs) to find the parameter value such that given a discriminator, a device that can accurately distinguish data generated using the model from real data, is unable to do so when the data is generated according to such parameter value. The method developed in this research differs from other simulation-based minimum distance estimators in that the distance is adaptive. That is, the discriminator learns the features of the data that are best at distinguishing real from synthetic data as opposed to hard-coding what features of the data to match. This adaptability property has proven powerful in pattern recognition tasks. In structural estimation, adaptability can translate into alleviating the curse of dimensionality, and obtaining parameters that are able to more closely match entire distributions of data, as opposed to a set of pre-specified moments. This estimation framework should be useful in applications were distributional effects and heterogeneity are first order to evaluate the effect of a particular policy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829357","Doctoral Dissertation Research: Recovering the Polyvalent Genealogies of Machine Learning, 1948 - 2017","SMA","SciSIP-DDRIG","08/01/2018","07/16/2018","Matthew Jones","NY","Columbia University","Standard Grant","Mark Fiegener","07/31/2021","$26,163.00","Aaron Plasek","mj340@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","SBE","7009","7626, 9179","$0.00","Machine learning techniques currently make ""high-stakes"" judgments in areas as diverse as criminal justice, credit risk, social welfare, hiring, and congressional redistricting. Such techniques make these decisions using patterns learned from historical social data. Emphasis on prediction rather than the circumstances of dataset creation have led to machine learning systems that preferentially target vulnerable populations for disparately adverse social judgments while making it more difficult for those subject to these decisions to protest unfair treatment. This study explores the limitations of such machine learning systems by tracing how technical and non-technical people, including funding agencies, have historically understood what machine learning systems could and should achieve. Particular care is given to the forms of ""learning"" valued by researchers during different moments in the 20th century, and to the emergence of theoretical concepts that were constrained and even defined by the capabilities of the available material devices. This work makes visible the efforts of women and men previously omitted in histories of artificial intelligence and machine learning, and develops a quantitative method to document how the innovations of a discipline are contingent upon interdisciplinary and transdisciplinary research networks. Finally, this project traces how the allocation of resources to particular research communities spurred scientific innovation in adjacent and seemingly unrelated academic research fields in the physical and social sciences. In this sense, the discipline of machine learning provides a useful case study for modeling the propagation of ideas across different subfields.<br/><br/>Both qualitative and quantitative historical research is employed. First, nine university and government archives are perused to reconstruct the institutional organizations, interpersonal research networks, and material computing devices available to machine learning and artificial intelligence researchers. Second, an investigation is conducted using a novel combination of topic modeling and word embeddings on a corpus of millions of full-text Association of Computing Machinery articles from 1950 to 2017 to trace how discursive influence propagates across disparate sub-disciplines. Four research products are generated: (1) a technical machine learning publication detailing the novel method used to analyze the article corpus, (2) a history of science article tracing the early history of machine learning, (3) a general audience ""think piece"" discussing the policy and ethical implications of contemporary machine learning research, (4) and the public release of the project code and the subsequent statistics generated from the article corpus. Digital copies of salient archive records discovered during this research study will be made freely available via Columbia University's Digital Repository, insofar as this is possible given the copyright and access restrictions of holding institutions. Archive materials collected and computational study of the article corpus will be used in the co-PI's doctoral dissertation exploring how machine learning has been used to classify individuals, imagine communities, and legislate forms of social and political evidence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842385","Learning Mathematical Concepts and Computational Thinking through Explainable Artificial Intelligence in a Simulation-based Learning Environment","DRL","STEM + Computing (STEM+C) Part","09/15/2018","10/18/2019","Ning Wang","CA","University of Southern California","Standard Grant","Chia Shen","08/31/2021","$985,438.00","","nwang@ict.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","EHR","005Y","","$0.00","As a result of the powerful innovation and application of computing in STEM disciplines, the STEM+C program supports research and development of interdisciplinary and transdisciplinary approaches to the integration of computing within STEM teaching and learning for preK-12 students in both formal and informal settings. This project will advance the integration of the education of math within the context of artificial intelligence (AI) and computational thinking skills for high school students. AI has become ubiquitous in our everyday lives. From virtual assistants, to self-driving cars, from diagnosing disease, to building houses, many of today's students will go on to work in fields that involve or are influenced by AI. Being proficient in the language of AI is key to a workforce that will be able to continue to innovate and to support the AI-powered technology infrastructure and eco-system. AI builds on the foundation of mathematics. By illustrating how math concepts can be used in powerful AI tools to solve computational problems, learning math through AI and computation can be a motivational and educational vehicle to illustrate the pathway from K-12 STEM education, to post-secondary STEM education, and later to STEM careers. <br/><br/>Most of the AI decision-making process today is a ""black box"" to non-AI experts, and even to some AI experts. Recent advances in explainable AI, an emerging intelligent human-computer interface that makes the decision-making of AI algorithms transparent to users, creates an opportunity to make AI machine learning concepts accessible to high school students. This proposed project employs explainable AI within a simulation-based learning environment where students follow guided human-robot team explorations to learn how to create abstractions of a problem, utilize AI algorithms to automate the process of solution generation, analyze the outcome, and then improve the performance of their solution.  Researchers will iteratively introduce challenge problems and scenarios to encourage students to modify the robot's decision-making where students diagnose, revise, test and analyze, and create new capabilities for the robot. The proposed project partners with three high schools from Virginia and California. The integrated learning content will be developed with high school teachers, focusing on the connections between high school math, AI and computational thinking skills. The proposed project aims to answer the question of to what extent interactive explainable AI design choices and explanations contribute to understanding of math and AI in a use-create-modify framework for high school learners. Research studies will address (1) assessment of the use-create-modify approach and its impact on self-efficacy for computational thinking and use of AI, (2) assessment of understanding of math and AI, and the different uses of explanations to promote learning, and (3) enjoyment and engagement with the human-robot simulation, and to what extent enactment in this human-technology interactive learning environment is able to frame high school level learning of math and its integration with computational thinking.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813341","RI: Small: Collaborative Research: Computational Methods for Argument Mining: Extraction, Aggregation, and Generation","IIS","Robust Intelligence","08/01/2018","03/12/2019","Lu Wang","MA","Northeastern University","Standard Grant","Tatiana Korelsky","07/31/2021","$217,177.00","","wangluxy@umich.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7495","075Z, 7495, 7923, 9251","$0.00","Understanding, evaluating and generating arguments are all crucial elements in the decision-making and reasoning process. Not surprisingly then, a multitude of arguments are encountered and constructed on a daily basis as decisions are made at work and at home, in our social life and in our civic life. In spite of their ubiquity in our lives, most people are not particularly skilled in the interpretation or generation of arguments. At best, making sense of the often massive amount of argumentative online text on a topic of interest remains a daunting task. And while numerous tools exist for representing, modeling and visualizing arguments and argumentative discussions, they are limited by the substantial human effort required to input, organize and annotate arguments for use by the tools. Thus there exists a pressing need for, and this project aims to develop, automated techniques from the field of Natural Language Processing to support all facets of argumentation. This project will have a wide array of broader impacts, including providing other<br/>researchers with annotated datasets and tools for the analysis and generation of arguments, enhancing education through graduate and <br/>undergraduate mentoring, and promoting STEM education diversity through programs for middle and high school girls.  <br/><br/><br/>This project aims to break new ground in the burgeoning area of argument mining. It develops a collection of computational models that comprise the basis of an argumentation toolkit---methods that can be combined and reused to support a range of argumentation applications. The project focuses on inter-related threads of research covering three critical areas of exploration for computational argumentation: (1) argument extraction---making sense of argumentative text. Drawing upon recent developments in structured learning, techniques are developed to identify the components and the structure of an argument within a single document or single turn in an online dialog. (2) Argument aggregation---clustering the components of argumentative text (e.g. sentences, turns) drawn from multiple documents according to the facets of the topic under discussion that they address. Representation learning methods are proposed to better capture topical content and argumentative styles. (3) Argument generation---constructing coherent arguments via rewriting. A neural argument generation framework with key phrase extraction as an intermediate representation is created to improve interpretation of sentences from different sources. A discourse-aware neural generation model is also investigated as an extension to improve the coherence of the generated text.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1748377","CAREER: New Learning-based Algorithms for the Analysis of Very-Large-Scale Neuroimaging Data","IIS","Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","09/08/2018","Mert Sabuncu","NY","Cornell University","Standard Grant","Kenneth Whang","09/30/2023","$581,438.00","","ms3375@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7495, 8624","075Z, 1045, 7495, 8089, 8091","$0.00","Artificial intelligence, fueled by recent advances in machine learning, is poised to transform healthcare and biomedical research. Machine learning algorithms allow researchers to analyze complex patterns in large datasets, in the service of advancing our understanding of biological mechanisms and developing clinical tools. This project considers very large scale brain imaging studies, including, for example, tens of thousands of individuals contributing head MRI scans and other biomedical data such as whole-genome sequences or clinical records. Such data allow researchers to map the effects of genetic, environmental, and other factors on the structure and function of the brain, which in turn advances our knowledge of disorders like Alzheimer's. Today, the primary obstacle in exploiting very large scale brain imaging datasets is computational, because existing software tools don't scale well and lack in quality assurance capabilities. This project will produce a machine-learning based computational pipeline that will fill this gap. In the largest study of its kind, we will showcase the developed software tools to chart the heritability of shapes of brain structures.  In addition, the project will implement a diverse set of educational outreach initiatives, such as a customized research experience for under-represented minority high-school students.   <br/><br/>Neuroimaging is entering a new era of unprecedented scale and complexity. Soon, we will have datasets including more than 100,000 individuals. The fundamental challenge in analyzing and exploiting these data is computational. Today, widely-used neuroimage analysis tools are computationally demanding, produce results that are sensitive to confounds, and are limited in quality control capabilities, making them infeasible at scale. This project will extend recent advances in machine learning to develop an innovative computational pipeline that addresses the drawbacks of existing methods. First, a computationally efficient and flexible brain MRI segmentation framework will be developed that integrates rich neuroanatomical prior models. The segmentation tool will be made robust to confounding effects such as subject motion via the use of an adversarial learning strategy. Learning-based methods will be further investigated to obviate the time-consuming manual quality control of segmentations. Finally, an innovative metric learning approach will be used to study genetic influences on brain morphology in the UK Biobank. The project will also implement an integrated educational plan that is focused on interdisciplinary, hands-on and lifelong learning. The researchers will devote significant effort to developing core educational material that will be adapted and utilized for audiences of various backgrounds and stages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840044","FW-HTF: First Person View and Augmented Reality for Airborne Embodied Intelligent Cognitive Assistants","IIS","FW-HTF-Adv Cogn & Phys Capblty","09/01/2018","03/27/2020","Craig Woolsey","VA","Virginia Polytechnic Institute and State University","Standard Grant","David Miller","08/31/2021","$1,500,000.00","Pratap Tokekar, Matthew Hebdon, Joseph Gabbard","cwoolsey@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>The future of work will involve human operators and semi-autonomous robotic systems. Human workers control the robots, extending their sensing and physical capabilities. This technology-empowered workforce will do remote, dangerous, and increasingly specialized work. An example use of this work might be the inspection of hard-to-access bridge supports. This project will explore the use of intelligent airborne drones to help ground-based operators in the inspection of highway bridges. Through the careful integration of augmented reality (AR) with first-person-view (FPV) operator interfaces this research will enhance worker performance across a wide range of otherwise very difficult tasks. <br/><br/>The research program will address key challenges in the use of embodied intelligent cognitive assistants (e-ICAs) for infrastructure inspection. New principles of shared situation awareness will be developed for human/robot collaboration through AR/FPV user interfaces and these principles will inform interface design guidelines and evaluation measures. The research program will also emphasize collaborative perception and planning, such as peripheral/central computer vision to enhance shared situation awareness, gaze-informed adaptive viewpoint planning for improved image quality, and a global planning method for partially known and uncertain maps that adapts the plan in real-time as the worker discovers new information. Tunable control system performance will allow the worker and the e-ICA to collaborate in managing disturbance energy to optimize mission data quality and flight endurance while ensuring safety of flight. The parallel development of a public repository containing annotated deterioration imagery will support artificial intelligence based defect analytics to provide the human worker with real-time inspection cues. A parallel and coordinated economic and workforce analysis will assess the impact of airborne e-ICAs, controlled using FPV with augmented reality, on the future of work in infrastructure inspection and beyond.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815455","RI: Small: Collaborative Research: Computational Methods for Argument Mining: Extraction, Aggregation, and Generation","IIS","Robust Intelligence","08/01/2018","07/24/2018","Claire Cardie","NY","Cornell University","Standard Grant","Tatiana Korelsky","07/31/2021","$290,823.00","","cardie@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7495","075Z, 7495, 7923","$0.00","Understanding, evaluating and generating arguments are all crucial elements in the decision-making and reasoning process. Not surprisingly then, a multitude of arguments are encountered and constructed on a daily basis as decisions are made at work and at home, in our social life and in our civic life. In spite of their ubiquity in our lives, most people are not particularly skilled in the interpretation or generation of arguments. At best, making sense of the often massive amount of argumentative online text on a topic of interest remains a daunting task. And while numerous tools exist for representing, modeling and visualizing arguments and argumentative discussions, they are limited by the substantial human effort required to input, organize and annotate arguments for use by the tools. Thus there exists a pressing need for, and this project aims to develop, automated techniques from the field of Natural Language Processing to support all facets of argumentation. This project will have a wide array of broader impacts, including providing other<br/>researchers with annotated datasets and tools for the analysis and generation of arguments, enhancing education through graduate and <br/>undergraduate mentoring, and promoting STEM education diversity through programs for middle and high school girls.  <br/><br/><br/>This project aims to break new ground in the burgeoning area of argument mining. It develops a collection of computational models that comprise the basis of an argumentation toolkit---methods that can be combined and reused to support a range of argumentation applications. The project focuses on inter-related threads of research covering three critical areas of exploration for computational argumentation: (1) argument extraction---making sense of argumentative text. Drawing upon recent developments in structured learning, techniques are developed to identify the components and the structure of an argument within a single document or single turn in an online dialog. (2) Argument aggregation---clustering the components of argumentative text (e.g. sentences, turns) drawn from multiple documents according to the facets of the topic under discussion that they address. Representation learning methods are proposed to better capture topical content and argumentative styles. (3) Argument generation---constructing coherent arguments via rewriting. A neural argument generation framework with key phrase extraction as an intermediate representation is created to improve interpretation of sentences from different sources. A discourse-aware neural generation model is also investigated as an extension to improve the coherence of the generated text.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827225","CC* Network Design: Network Upgrades to Improve Engagement in Science Discovery and Education","OAC","Campus Cyberinfrastructure","08/01/2018","07/25/2018","Donna Liss","MO","Truman State University","Standard Grant","Kevin Thompson","07/31/2021","$400,000.00","Jon Beck, Jim McNabb","dliss@truman.edu","100 E. Normal","Kirksville","MO","635014200","6607857245","CSE","8080","9150","$0.00","Truman State University, with the support of the Missouri Research and Education Network (MOREnet) and the University of Missouri, is upgrading the campus network and infrastructure to better enable access to, and use of, scientific data through improved data transfer capabilities for large datasets.  The enhanced infrastructure supports faculty and undergraduate research in understanding star spots, quantifying light pollution in geographical areas, understanding the inhibition mechanisms of drugs to treat global health issues, natural language processing to improve approaches in artificial intelligence, and building low-cost, real-time, soil sensors.  An expanded curriculum that includes hands-on training in cybersecurity and IPv6 technologies is also enabled by utilizing these network resources.  <br/><br/>The improvements include upgrades to the network switch and distribution technologies that result in a ten-fold increase in data transfer and access rates for faculty in STEM-related disciplines, as well as increases in the data transfer bandwidth to the campus observatory, deployment of IPv6 in support of the computer science curriculum, establishment of network performance metrics to inform continued growth, and robust and secure access (through federated identity management) to off-campus research tools and intra-institutional collaboration opportunities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1825535","Investigating the Effectiveness of Machine Learning Paradigms for Supporting Engineering Designers in Rapidly Evolving Digital Manufacturing","CMMI","EDSE-Engineering Design and Sy","08/15/2018","08/07/2018","Christopher McComb","PA","Pennsylvania State Univ University Park","Standard Grant","Georgia-Ann Klutke","07/31/2021","$424,743.00","Timothy Simpson, Nicholas Meisel","uum209@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","072Y","067E, 073E","$0.00","Manufacturing technology is advancing at an unprecedented rate as it become increasingly digital.  Taking advantage of new manufacturing technology is critical for maintaining national security and prosperity, but even dedicated experts and leading-edge companies struggle to keep pace with manufacturing's rapid advancement.  This makes it difficult for engineers to learn about the latest manufacturing technology and design products that take full advantage of new fabrication processes as they become available.  Fortunately, many new manufacturing technologies rely on digital models that produce abundant design data to which machine learning can be applied to derive design knowledge.  While design and manufacturing datasets may be useful for that reason, they are also highly variable in both number and quality of solutions.  This work investigates how the size and quality of these datasets relate to the accuracy and usefulness of machine learning insights, and how this impacts the support provided to engineering designers.  In this work, we focus on additive manufacturing (also called ""3D printing"") as a representative digital manufacturing technology that is rapidly evolving and growing, and which is projected to contribute substantially to the nation?s future manufacturing portfolio. Studies conducted with engineering students as part of this work will be used to provide skill training as well as collect data, helping prepare them for the manufacturing workforce.<br/><br/>The research will combine machine learning, additive manufacturing, and explainable artificial intelligence to evaluate the use of automated design feedback derived from existing crowdsourced additive manufacturing design challenges.  First, part designs will be mined from open, online repositories as well as through curated repositories established in this work via in-class design challenges.  Next, a machine learning pipeline will be implemented to extract design patterns from curated digital repositories.  This will make it possible to test the effect of repository size on the accuracy of design feedback and of repository size on the granularity of feedback.  Finally, a user validation study will be conducted in which students will undertake a design task specific to additive manufacturing technology.  Feedback with varying characteristics will be provided to some participants by extending the machine learning pipeline developed previously with explainable capabilities.  Specific technical deliverables will include (1) a novel dataset of voxelized part designs, (2) a deeper understanding of the impact of repository size and quality on usefulness of machine-generated feedback, and (3) empirical evidence of the impact of real-time additive manufacturing feedback on the solutions generated by engineering designers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800592","D3SC: Machine Learned Free Energies of Compounds","CHE","Chem Thry, Mdls & Cmptnl Mthds, CDS&E","09/01/2018","07/27/2018","Charles Musgrave","CO","University of Colorado at Boulder","Standard Grant","Evelyn Goldfield","08/31/2021","$517,497.00","","charles.musgrave@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","MPS","6881, 8084","026Z, 062Z, 8084, 9216, 9263","$0.00","Charles Musgrave of the University of Colorado Boulder and collaborator Aaron Holder are supported by the Division of Chemistry, and the Division of Chemical, Bioengineering, Environmental, and Transport Systems, to develop and apply machine learning approaches for the discovery of new materials. While the periodic table provides a many possible combinations of elements from which to form materials, only a fraction of these compounds will be stable or have desirable properties for particular applications. Furthermore, of the large number of possible compounds, only about one thousand have known properties at elevated temperatures. For the past fifty years, computational chemists have used equations of quantum mechanics to discover new materials.  However, screening large numbers of candidate materials for a specific technological application remains too computationally demanding to be practical.  Recently, statistical learning approaches have been developed which can extract systematic information from large quantities of data to train highly reliable ""artificial intelligence"" models for predicting properties of a new system.  In this project, Professors Musgrave and Holder are using machine learning approaches applied to predict the stabilities, structures, and chemical reactivity of materials. The predicted properties can then be used to identify candidate materials for catalyzing technologically-important reactions, such as splitting water into oxygen and hydrogen, converting carbon dioxide into useful products, or the 'green' synthesis of ammonia from nitrogen and water.  The models are available on public repositories as machine learning computer codes, and through publicly-accessible databases.  The project is training high school, undergraduate and graduate students in the development and application of state-of-the-art machine learning methods for chemistry and chemical engineering applications.  The researchers participation in the Broadening Opportunity through the Leadership and Diversity (BOLD) Center at University of Colorado.  The incorporation of new concepts in machine learning and chemistry are integrated into courses and through the departmental LearnChemE YouTube platform.<br/><br/>This project combines expertise in electronic structure, thermodynamics, computational science, and machine learning to study one of the most fundamental properties of molecules--the Gibbs free energy, G(T). The data-driven approach takes advantage of results showing that the vibrational entropy and Helmholtz free energy computed in the constant-volume quasiharmonic approximation - quantities that critically contribute to G(T) but are computationally challenging to calculate quantum-mechanically - have systematic temperature dependence and can be accurately and efficiently predicted using machine learning, coupled with knowledge of the chemical composition of the material.  The researchers are extending this observation to apply machine learning methods to model G(T) directly, using experimental data for several hundred molecules for training and descriptor extraction.  The resulting descriptors are being used to predict thermochemical data for ~20,000 unique compositions tabulated in the Inorganic Crystal Structure Database, and in turn, to compute temperature-dependent convex hull phase diagrams and solid-state reaction equilibria.  The models and G(T) data are available on large databases. The new methodology is enabling the discovery of general trends and new chemical knowledge of the effects of temperature and composition on reactivity, synthesizability, stability and metastability. In addition to providing deep insights into the thermochemistry of molecules and reactions, this research is enabling the identification of anomalies that may indicate systems where emerging properties are altering the behavior of the molecule. For example, where temperature-dependent emergent or quantum phenomena create unique materials properties. Despite the technological and economic importance of advanced materials in a broad range of technologies, much is still unknown about the detailed behavior that give rise to their stability and reactivity. Potential applications of the new techniques and thermochemical databases produced include thermochemical water splitting using redox materials, ammonia synthesis by chemical looping, oxidation chemistries, carbothermal reduction of oxides, and reduction of molecules by molecular hydrogen or other reductants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806079","Automated Search for Materials for Ammonia Synthesis and Water Splitting","CBET","EchemS-Electrochemical Systems","09/01/2018","07/09/2018","Charles Musgrave","CO","University of Colorado at Boulder","Standard Grant","Carole Read","08/31/2020","$136,329.00","Aaron Holder","charles.musgrave@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","ENG","7644","","$0.00","The use of renewable electricity and solar energy to convert water, carbon dioxide, and nitrogen into energy-dense fuels and high-valued chemicals can improve the storage and utilization of intermittent solar and wind energy. This technology for ""solar fuels"" has benefits in utilization of renewable energy sources into value added chemicals used to make industrial products. This project supports fundamental research of the discovery of advanced catalysts for a wide range of redox reactions. When conducting new materials discovery, for a given family of promising material compositions, only a fraction of materials will have desirable properties for the targeted reaction applications. For many of these solid-state materials, the chemical equilibria and driving forces for chemical reactions are unknown. Statistical learning approaches have been developed which can extract information from large quantities of data to train highly reliable ""artificial intelligence"" models for predicting properties of a new material system. In this project, the principal investigators are using machine learning approaches applied to experimental data for hundreds of materials to predict the stabilities, structures, and chemical reactivity of hundreds of materials. The predicted properties can then be used to identify candidate materials for catalyzing technologically-important reactions, such as splitting water into oxygen and hydrogen, converting carbon dioxide into useful products, or the 'green' synthesis of ammonia from nitrogen and water. The models are being made available on public repositories such as machine learning computer codes, and through publicly-accessible materials databases. The project is training high school, undergraduate and graduate students in the application of state-of-the-art machine learning methods for chemistry, chemical engineering, and materials science applications. The research is integrated with education and outreach through the PI's participation in the Broadening Opportunity through the Leadership and Diversity (BOLD) Center at University of Colorado, and the incorporation of new concepts in machine learning and chemistry within the PI's courses. <br/><br/>This project will apply machine learning approaches for the discovery of new oxide and oxynitride materials at scale for catalyzing splitting water into oxygen and hydrogen or the 'green' synthesis of ammonia from nitrogen and water. The chemical driving forces for the reactions involved in splitting water and ammonia synthesis depend critically on the energy to create an oxygen vacancy in the oxide or oxynitride material. In this project, The PI is using machine learning approaches trained on a set of oxygen vacancy formation energies that were calculated quantum mechanically. This project is complementary and leverages grant CHE 1800592 that focuses on the development of the machine learning methods and datasets. The predicted properties can then be used to identify candidate oxides and oxynitrides for catalyzing splitting water or ammonia synthesis. This project combines expertise in electronic structure, thermodynamics, computational materials science, and machine learning to study a central property of oxides - their oxygen vacancy formation energies, EV. The data-driven approach takes advantage of results showing that EV depends systematically on various materials properties, such as the electronic band gap and the enthalpy of formation of the material. The researchers will apply machine learning methods to model EV directly, using quantum mechanically calculated EV data for several hundred materials for training and descriptor extraction. The resulting descriptors are being used to predict EV for unique oxide and nitride compositions, and in turn, will enable the computation of millions of reaction equilibria for the oxidation and reduction reactions for water splitting and ammonia synthesis mediated by these materials. Despite the enormous technological and economic importance of advanced oxides and oxynitrides in a broad range of technologies, much is still unknown about the detailed behavior that give rise to their chemical properties. Potential applications of the new techniques and thermochemical databases produced by this project include thermochemical water splitting using redox materials, ammonia synthesis by chemical looping, oxidation of materials, the carbothermal reduction of oxides, oxygen separation membranes, and solid oxide fuel cell electrolytes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824737","CompCog: A Machine Learning Approach to Human Perceptual Similarity","BCS","Information Technology Researc, Perception, Action & Cognition, Robust Intelligence","10/01/2018","08/15/2018","Robert Jacobs","NY","University of Rochester","Standard Grant","Michael Hout","09/30/2021","$400,000.00","","robbie@bcs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","SBE","1640, 7252, 7495","075Z, 7252, 7495","$0.00","Similarity is fundamental to nearly all aspects of human cognition. Perception uses similarity: when viewing a person's face, we (unconsciously) calculate its similarity to the faces of people we know in order to recognize who we are looking at. Categorization uses similarity: when judging whether a building was designed by the architect Frank Lloyd Wright, we calculate its similarity to buildings known to have been designed by Wright in order to make our best estimate. Reasoning and problem solving use similarity: when attempting to solve a calculus problem, we calculate its similarity to previous problems that we have encountered in order to determine a good solution strategy. However, how people calculate the similarity of two items is not yet understood. Which features of items do people use to calculate similarity? And how are the feature values of items compared in order to calculate similarity? This research project will use human experimentation and computational modeling to address these questions when items are viewed or grasped. A long-term benefit of the project is that a greater understanding of people's perceptual similarity judgments will provide a foundation for understanding how people calculate and use similarity in other areas of cognition. While conducting the research, undergraduate and graduate students will be mentored in the cross-disciplinary approach embodied in our investigation through participation in both experimental and computational aspects of the research project. <br/> <br/>This project focuses on developing a new empirical and theoretical foundation for understanding people's notions of similarity, particularly in the domain of perceptual similarity.  The field of cognitive science is well aware that understanding similarity is essential to understanding human cognition. Despite this, the primary motivation for this project is the belief that, to date, cognitive science's approach to the study of similarity judgments is much too simple---the restricted class of similarity metrics considered by cognitive scientists is unlikely to scale to large, realistic settings. The primary hypothesis of this project is that the field of machine learning---especially the study of metric learning---can supply cognitive science with a rich array of complex and sophisticated models, models that will be necessary to accurately characterize people's similarity notions in large, realistic domains. Machine learning has pioneered the study of mathematically rigorous linear and nonlinear similarity metrics. We believe that the time is ripe for the field of cognitive science to make use of machine learning's recent advances. Machine learning's metric learning framework extends and elaborates the cognitive science approach in principled and innovative new directions. Indeed, this framework presents an unparalleled opportunity for cognitive science with the potential for transforming this field. Using the empirical and theoretical findings from machine learning, cognitive scientists can now begin to explore human notions of similarity in more complex and sophisticated ways---and in more realistic domains---than has ever been possible. We regard the research project as an early step for cognitive science towards a more sophisticated understanding of people's notions of similarity. Because the project cannot study similarity in all domains of human cognition, it concentrates on perception. Future work will need to develop further the models proposed and evaluated here. If successful, the program will establish an empirical and theoretical foundation that can subsequently be extended to many other areas of human cognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814825","SaTC: CORE: Small: Collaborative: A Framework for Enhancing the Resilience of Cyber Attack Classification and Clustering Mechanisms","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","10/01/2018","08/07/2019","Shouhuai Xu","TX","University of Texas at San Antonio","Standard Grant","Indrajit Ray","09/30/2021","$499,997.00","","shxu@cs.utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","1714, 8060","025Z, 075Z, 7434, 7923","$0.00","Classification and clustering are two important classes of machine learning techniques that have been widely used for cyber defense purposes. However, these mechanisms can be defeated by intelligent evasion attacks, such as Adversarial Machine Learning. Currently, there are no effective countermeasures against these sophisticated attacks. The objective of the project is to investigate effective countermeasures to make classification and clustering mechanisms robust against intelligent evasion attacks. The scientific contributions of the project include advancing our understanding of the feasibility and impact of evasion attacks, and the design of machine learning algorithms that are robust against such attacks. Since machine learning techniques are widely employed in many other areas such as real-world fraud and crime detection, those areas would benefit from this project too. The project will involve PhD students who will directly contribute to the next-generation workforce and will address diversity by involving female students and students from underrepresented groups.<br/><br/>The project plans to achieve its goal by investigating a more powerful class of attacks, called gray-box attacks, than the traditional black-box attacks investigated in the literature. In the gray-box attack model, the attacker can perform all the activities that a defender would normally perform. The project will build a theoretical model and framework for characterizing the vulnerability and resilience of classification and clustering mechanisms with respect to intelligent evasion attacks under the gray-box model, enhance classification and clustering mechanisms to withstand intelligent evasion attacks with quantifiable resilience gains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829704","CyberTraining: CIC: CyberTraining for Students and Technologies from Generation Z","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Geoffrey Fox","IN","Indiana University","Standard Grant","Alan Sussman","08/31/2021","$492,283.00","Douglas Swany, Gregor von Laszewski","gcf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","Information technology is playing a dramatically increasing role in society, industry, and research. This includes design and use of large databases, simulations and artificial intelligence applications hosted on clouds and supercomputers with convergent technologies. Correspondingly, there is an increasing need for research workforce job skills in these and related areas. This project takes freshly created Indiana University Engineering course material on this cyberinfrastructure and adapts it for training with an emphasis on the needs of under-represented communities. The techniques of the successful open source software movement are used to create sustainable communities around the course curriculum and software. The project is creating new technologies to enable this for today's generation of students. Skills in core cloud computing, big data, supercomputing and artificial intelligence are exemplified by applications in the life science and nanotechnology areas. This project enables the future research workforce to contribute effectively using advanced cyberinfrastructure, promoting the progress of science and advancing the national health, prosperity, and welfare, which serves the national interest, as stated by NSF's mission. <br/><br/>The future economic progress and research leadership of the U.S. is dependent on having a research workforce that is capable of making use of advanced cyberinfrastructure (CI) resources as articulated by the National Strategic Computing Initiative (NSCI). This requires a curriculum that changes and integrates modern concepts and practices for the new generation of students aiming at a ""data-enabled computational science and engineering"" expertise. This project takes what Indiana University has learned from a brand new four-year undergraduate engineering curriculum designed ab initio and taught so far to its first two undergraduate classes, and invests it into developing active training modules. The innovative curriculum integrates big data, simulations, clouds and high performance computing systems presented in a uniform framework. The course material is customized for communities of cyberinfrastructure researchers nucleated, built, and sustained via the dynamic use of GitHub and enhanced by innovative tools to build a novel learning management system optimized for cyberinfrastructure-intensive classes. The project modules include Cloud Computing, Big Data Applications and Analytics, Networking, High-Performance Computing, Artificial Intelligence/Machine Learning, and Information Visualization. There are residential sessions, with a call for participants, and purely online courses and these have both ""teach the student"" and ""teach the teacher"" modes; the latter enables easy spread of the classes. Hands-on learning with research projects built around the class material is fully supported. The project offers CyberTraining with all of the popular approaches used by the Apache Software Foundation, including Meetups and Hackathons. Modules for domain scientists and engineers, e.g., the cyberinfrastructure users that exploit advanced CI methods for research in nanoengineering and bioengineering are included. Both students and teachers contribute to the course improving the text, the software, including a unique set of examples and the project aims to show that one can build both learning and sustainability communities by using the proven techniques of the open source software community. The project uses proactive measures to enhance the involvement of under-represented communities in its activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816891","RI: Small:Comp Cog: Broad-coverage semantic models of human sentence processing","IIS","Robust Intelligence","08/15/2018","08/09/2018","William Schuler","OH","Ohio State University","Standard Grant","D.  Langendoen","07/31/2021","$490,287.00","","schuler.77@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 7923","$0.00","Humans are a successful species in large part because they can pass knowledge about the world to one another using linguistic explanations.  These explanations can be quite complex, involving nested generalizations about multiple classes of objects and events.  Accurate models of how these relationships are decoded from natural language could further our understanding of how the brain works, and may allow non-programmer users to explain their desired products, goals and constraints to machines.  Sentence processing experiments may provide an important window into the mechanisms of idea formation in language comprehension, but the human mind is extraordinarily sensitive to the strangeness of constructed stimuli used in experimentally controlled research designs, yielding potentially confounding effects arising from unexpected words or sentence structures.  A common alternative is to use designs employing naturally-occurring stimuli with statistical controls, usually using one or more probabilistic measures of surprise during sentence processing.  Unfortunately, existing probabilistic measures of surprise are based on overly simple models of sentence processing that are not connected to the nested structure of generalizations that a linguistic explanation may describe, and thus have severe limits as predictors of these kinds of frequency effects.  This project will therefore develop a sentence processing model that decodes sentences into meanings using a human-like incremental probabilistic process.  This model will then be used to control for frequency effects in neural activation, blood oxygenation and reading time data in order to isolate effects that can be attributed to the mechanical process of constructing and storing complex ideas during language comprehension.<br/><br/>This project constructs a model of sentence processing that bases its processing decisions on mental representations of meanings rather than on words only.  This means that the model will be less surprised by repeated nouns or pronouns when these words refer to a common entity which is prominent in a discourse.  The project initially focuses on the development of a statistical sentence processing model which maintains several possible analyses of a sentence after each word is processed, each of which contains explicit representations of each discourse referent involved in a sentence meaning as a set of logical predicates adjacent to that referent in a graphical representation of the meaning.  A subsequent version of the model compresses these context sets into vectors, which are passed through a recurrent neural network.  The predictions of these models are compared against existing neural network language models used in natural language processing applications to ensure that their linguistic predictions are accurate.  Incremental probabilities generated by these models are then used to estimate probabilistic surprise as a frequency control in predicting functional magnetic resonance (fMRI), electroencephalographic (EEG), eye-tracking, and reading-time observations in existing datasets, in order to isolate effects due to memory usage and other mechanistic factors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813567","SHF: Small: Fast Sign-Off of Machine Learning Systems: From Circuit-Level Modeling to Statistical System Validation","CCF","Software & Hardware Foundation","10/01/2018","08/09/2019","Xin Li","NC","Duke University","Standard Grant","Sankar Basu","09/30/2021","$400,000.00","Xin Li","xinli.ece@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7798","075Z, 7923, 7945","$0.00","Machine learning has been adopted by a broad range of emerging applications, including health monitoring, autonomous driving, advanced manufacturing, etc. However, any machine learning system cannot be 100% accurate due to the accuracy limitation posed by machine learning algorithms and the circuit-level non-ideal features associated with its hardware implementation. This project investigates a radically new framework for efficient validation of machine learning systems implemented with nano-scale integrated circuits. It aims to identify and synthesize the critical corner cases for which a machine learning system is likely to fail. The project is expected to initialize a paradigm shift in today's design methodology for complex machine learning systems, thereby leading to an immediate impact on a broad range of industrial sectors relying on machine intelligence. In addition, the proposed education activities create a large number of unique training opportunities for both academic and industrial participants, substantially improving the education infrastructure and generate high-quality researchers and practitioners for the society. <br/><br/>Today, validating a machine learning system with high throughout, low power and complex functionality is an extremely challenging task. This project attacks the grand challenge by developing a novel validation framework composed of two major components: (1) corner-case generation and (2) rare-failure rate estimation. Both physical circuit models and statistical generative models are proposed to synthesize a large amount of test cases, reducing the experimental cost to physically record the corner-cases that are difficult to observe. Furthermore, a novel formulation, based on subset partition and graph embedding, is developed to efficiently inspect the likely-failed test cases and consequently estimate the rare- failure rate that is expensive to capture by random sampling. Built upon these mathematical tools, the project's framework offers a fundamental infrastructure that could facilitate radical breakthroughs over numerous machine learning applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845216","EAGER: Visual Representation Learning Using Mixed Labeled and Unlabeled Data","IIS","Robust Intelligence","09/01/2018","08/13/2018","Hamed Pirsiavash","MD","University of Maryland Baltimore County","Standard Grant","Jie Yang","08/31/2021","$167,041.00","","hpirsiav@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7495","075Z, 7495, 7916","$0.00","Recent advances in deep learning has led to great results in visual recognition and object detection. These deep learning models have various applications from self-driving cars to early disease diagnosis and household robots. However, most such models are supervised, meaning that they need large scale manually annotated datasets to tune the parameters, and obtaining the annotation may be expensive in many applications. This project explores a family of self-supervised learning algorithms where the learning is based on unlabeled data only. The new models can learn visual features that can be used for various visual recognition tasks including object detection and action recognition. This project provides research opportunities for under-represented groups and integrates research outcomes into the course curriculum.<br/><br/>This project studies a family of self-supervised learning algorithms that can learn rich features from unlabeled images and videos. Self-supervised learning algorithms harvest the knowledge from unlabeled data by modeling some regularity in the space of natural images or videos. This project studies novel self-supervised learning algorithms based on constraining the learning by relating transformations of images to transformations of their representations. Moreover, this project studies a novel multi-task learning framework for aggregating the knowledge learned from multiple supervised and self-supervised learning algorithms. This algorithm uses quantization methods to ignore the task specific details of the representation in transferring the knowledge. This algorithm results in a rich set of representations that generalize well across various visual recognition tasks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841471","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Kyle Cranmer","NY","New York University","Standard Grant","William Miller","09/30/2020","$486,879.00","Heiko Mueller","kyle.cranmer@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841456","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Mark Neubauer","IL","University of Illinois at Urbana-Champaign","Standard Grant","William Miller","09/30/2021","$499,872.00","Daniel Katz","msn@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841448","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Michael Hildreth","IN","University of Notre Dame","Standard Grant","William Miller","09/30/2020","$422,981.00","","hildreth.2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839429","EAGER: Real-Time: Reinforcement, Meta, and Episodic Learning for Control under Uncertainty","ECCS","EPCN-Energy-Power-Ctrl-Netwrks, EFRI Research Projects","10/01/2018","09/16/2018","Pramod Khargonekar","CA","University of California-Irvine","Standard Grant","Radhakisan Baheti","09/30/2021","$299,333.00","Pierre Baldi","pramod.khargonekar@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","ENG","7607, 7633","092E, 7916","$0.00","Machine learning and artificial intelligence are among the most important general purpose technologies for the coming decades, with potential to transform all aspects of society from health, to manufacturing, to business, to education, and to security. In the last decade, there have been very important and impressive advances in machine learning driven by the use of deep neural networks, innovative training algorithms, computational resources including specialized hardware (graphics processors, tensor processing units), and large datasets. Some of these developments have connections to emerging understanding from neuroscience on how the human brain learns to makes decisions in real-time. However, there are major challenges in the use of these techniques in real-time control and decision making for engineering systems where stability, reliability, and safety are paramount concerns. This project aims to connect major advances in machine learning and neuroscience to control systems and thereby advance myriad application domains. Modern engineered systems are increasingly complicated. They comprise large heterogeneous distributed networks of (IoT) connected devices, systems, and human/social agents, e.g., transportation, energy, water, manufacturing, health and agriculture. A major challenge is performance, stability and reliability of these systems under large uncertainties. The goal is to expand our understanding and integration of learning and control to derive principles and algorithms for the development of learning-based control systems for a variety of engineering applications.  <br/><br/>While there are significant historical connections between reinforcement learning and stochastic dynamic control, the potential for leveraging ongoing and future advances in machine learning for control remains significantly under- explored. The field of control systems has deep and solid theoretical and mathematical foundations with comprehensive and well-established frameworks for linear, nonlinear, robust, adaptive, stochastic, distributed, and model-predictive control systems. Equally importantly, control systems have applications in multiple domains, such as aerospace, automotive, manufacturing, energy, transportation, agriculture, water, and many other engineered and socio-technical systems. Despite this rich spectrum of theoretical foundations and important applications, the domain of applicability of traditional control techniques is limited to situations where good mathematical models of the underlying systems are available, and where the environmental uncertainty is not too large. This exploratory research project is aimed at overcoming these limitations via novel problem formulations in systems and control inspired by new insights coming from recent developments in machine learning.  A key focus will be on novel control architectures inspired by neuroscience and reinforcement learning. Besides architectural innovations, the project will explore questions of stability, performance, and uncertainty by integrating ideas from rapid (one-shot) learning, meta-learning, and episodic control into control algorithms. The ideas from this project will be at the core of a new graduate level course in learning for control which will be taught at the University of California, Irvine. The resulting course materials will be made available to the research community and will benefit interested graduate students across the nation. In addition, short courses will be offered at major professional conferences, e. g., American Control Conference, IEEE Conference on Decision and Control.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829268","Collaborative Research: Modeling the Invention, Dissemination, and Translation of Scientific Concepts","SMA","Information Technology Researc, CYBERINFRASTRUCTURE, SciSIP-Sci of Sci Innov Policy","09/01/2018","08/02/2018","Xiang Ren","CA","University of Southern California","Standard Grant","Mark Fiegener","08/31/2021","$239,988.00","","xiangren@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","SBE","1640, 7231, 7626","075Z, 7626","$0.00","Scientific advance rests on discovery, dissemination, and translation. And yet, most discoveries fail to get noticed and have practical relevance. Nearly every university, discipline, foundation, government and company would like to know not just how to facilitate the creation of new scientific ideas, but how to get those ideas to spread and relate to practical issues in the world. To address such challenges, this research follows leading philosophers of science who argue that scientific concepts are the basic unit of knowledge, and it is there people observe scientific revolutions and advance. This research identifies scientific concepts using natural language processing techniques. With concepts in hand, it intends to model their interrelation in documents as forming an accumulated knowledge network to which every new publication's conceptual relations can be compared. Through comparison one will identify conceptual discoveries as the arrival of new concepts and conceptual relations. Results from this project will inform a variety of stakeholders concerned with intellectual innovation. The findings will have practical relevance by identifying where and when scholars make creative intellectual contributions, contributions that garner wider attention and reception in fields and domains far afield from academe. It will also identify levers for facilitating more rapid forms of discovery, dissemination and translation. Findings from this research will thus have direct relevance to decisions by academics, government agencies, and non-governmental organizations trying to accelerate scientific advance.  <br/><br/>Specifically, this research conducts three interrelated projects that identify the social conditions giving rise to concept discovery, dissemination, and translation. The first study asks what are the conditions that give rise to scientific discoveries like new concepts and new conceptual relations? Here the project identifies ecological conditions, resources, and demographic compositions that enable forms of conceptual creativity. The second study follows concepts over time and seeks to understand how they garner attention. Notably, only some conceptual discoveries actually spread and get repeatedly used. What are the initial conditions surrounding a concept's creation that predicts its entire career? Here, the project intends to study new concepts and new concept linkages, and it will look at their staging (start) and implementation (preceding moment) practices so as to determine what drives the heightened usage of some concepts over others. The third study will follow concepts as they spread beyond their discipline and to domains outside of academia. There the project analyzes how concepts translate across fields, or how they move from basic research applications to applied ones such as those found in patents. It also asks how scientific concepts get archived as accepted knowledge in encyclopedia such as Wikipedia. What leads an idea to go from theory, to applied use, to public acceptance? Again, are there ecological conditions and socio-cultural resources that enable an idea to move across social domains? This research will generate and share unique linked data, and it will create new empirical models and extensions of theory that will shift the way scholars conceive of scientific advance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1928882","CDS&E: D3SC: The Dark Reaction Project:  A machine-learning approach to exploring structural diversity in solid state synthesis","DMR","CONDENSED MATTER & MAT THEORY, Chem Thry, Mdls & Cmptnl Mthds, Data Cyberinfrastructure","11/01/2018","04/09/2019","Joshua Schrier","NY","Fordham University","Standard Grant","Daryl Hess","08/31/2021","$552,660.00","","jschrier@fordham.edu","441 E. Fordham Road","Bronx","NY","104585149","7188174086","MPS","1765, 6881, 7726","054Z, 7433, 8084, 9177","$0.00","NONTECHNICAL SUMMARY<br/>This award receives funds from the Division of Materials Research, the Chemistry Division and the Office of Advanced Cyberinfrastructure. This award supports research and education that uses data-centric methods to enable the prediction of metal oxide compounds with desired properties. Organically-templated metal oxides have a tremendous degree of structural diversity and compositional flexibility. This allows chemists to tune the structures, properties, and symmetries of these compounds to optimize their performance in specific applications that include catalysis, molecular sieving, gas adsorption, and nonlinear optics.  However, new compounds are typically created by a trial-and-error procedure, and creating novel compounds with specific structures is a grand challenge in solid state chemistry.  This project will develop artificial intelligence techniques for computers called machine learning techniques that can be used to predict the conditions for chemical reactions that will increase structural diversity and lead to specific structural features.  This project will also develop machine learning techniques that generate human-readable explanations about the formation mechanism, which will be tested in the laboratory.<br/> <br/>The primary impact of this project will be to decrease the amount of time and to lower the cost of discovering new materials with specific structural features, which in turn help bring new materials for applications to market more quickly.  This project is an example of a collaboration among synthetic chemists, computational chemists, and computer scientists and as a model it may be directly transferred to a wide range of disciplines and avenues of investigation. Undergraduate student research opportunities and curricular developments will be involved throughout the project, thus contributing to the scientific workforce.<br/> <br/>TECHNICAL SUMMARY <br/>This award receives funds from the Division of Materials Research, the Chemistry Division and the Office of Advanced Cyberinfrastructure. This award supports research and education that uses data-centric methods to enable the prediction of metal oxide compounds with desired properties. Hydrothermal synthesis is widely used to create new metal oxide materials with a wide range of functional properties and applications.  This project will advance the field by developing software infrastructure for associating the results of X-ray diffraction experiments with individual reactions, extracting structural outcome descriptors from this data, and then determining the extent to which these structural outcomes can be predicted from reaction description data.  This will be achieved by developing structural outcome descriptors for geometric properties, non-covalent interaction properties, and electron-density properties, then building machine learning models that correlate these outcomes to reaction conditions, and finally testing the quality of these predictions experimentally.  Active learning and auditable and interpretable models will be incorporated into the workflows to help synthetic chemists select better (more insightful/novel) reactions in an interactive fashion."
"1829240","Collaborative Research: Modeling the Invention, Dissemination, and Translation of Scientific Concepts","SMA","Information Technology Researc, CYBERINFRASTRUCTURE, SciSIP-Sci of Sci Innov Policy","09/01/2018","08/02/2018","Daniel McFarland","CA","Stanford University","Standard Grant","Mark Fiegener","08/31/2020","$313,486.00","","mcfarland@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","SBE","1640, 7231, 7626","075Z, 7626","$0.00","Scientific advance rests on discovery, dissemination, and translation. And yet, most discoveries fail to get noticed and have practical relevance. Nearly every university, discipline, foundation, government and company would like to know not just how to facilitate the creation of new scientific ideas, but how to get those ideas to spread and relate to practical issues in the world. To address such challenges, this research follows leading philosophers of science who argue that scientific concepts are the basic unit of knowledge, and it is there people observe scientific revolutions and advance. This research identifies scientific concepts using natural language processing techniques. With concepts in hand, it intends to model their interrelation in documents as forming an accumulated knowledge network to which every new publication's conceptual relations can be compared. Through comparison one will identify conceptual discoveries as the arrival of new concepts and conceptual relations. Results from this project will inform a variety of stakeholders concerned with intellectual innovation. The findings will have practical relevance by identifying where and when scholars make creative intellectual contributions, contributions that garner wider attention and reception in fields and domains far afield from academe. It will also identify levers for facilitating more rapid forms of discovery, dissemination and translation. Findings from this research will thus have direct relevance to decisions by academics, government agencies, and non-governmental organizations trying to accelerate scientific advance.  <br/><br/>Specifically, this research conducts three interrelated projects that identify the social conditions giving rise to concept discovery, dissemination, and translation. The first study asks what are the conditions that give rise to scientific discoveries like new concepts and new conceptual relations? Here the project identifies ecological conditions, resources, and demographic compositions that enable forms of conceptual creativity. The second study follows concepts over time and seeks to understand how they garner attention. Notably, only some conceptual discoveries actually spread and get repeatedly used. What are the initial conditions surrounding a concept's creation that predicts its entire career? Here, the project intends to study new concepts and new concept linkages, and it will look at their staging (start) and implementation (preceding moment) practices so as to determine what drives the heightened usage of some concepts over others. The third study will follow concepts as they spread beyond their discipline and to domains outside of academia. There the project analyzes how concepts translate across fields, or how they move from basic research applications to applied ones such as those found in patents. It also asks how scientific concepts get archived as accepted knowledge in encyclopedia such as Wikipedia. What leads an idea to go from theory, to applied use, to public acceptance? Again, are there ecological conditions and socio-cultural resources that enable an idea to move across social domains? This research will generate and share unique linked data, and it will create new empirical models and extensions of theory that will shift the way scholars conceive of scientific advance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823366","CRI: CI-NEW: Trainable Reconfigurable Development Platform for Large-Scale Neuromorphic Cognitive Computing","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2018","07/03/2018","Gert Cauwenberghs","CA","University of California-San Diego","Standard Grant","Sankar Basu","07/31/2021","$1,500,000.00","Amitava Majumdar, Emre Neftci","gert@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7359","7359","$0.00","Neuromorphic cognitive computing aims at learning to solve complex cognitive tasks by emulating the principles and physical organization of highly efficient and resilient adaptive information processing in the biological brain.  Despite over 30 years of development and a recent surge of broad interest across all Science, Technology, Engineering and Mathematics (STEM) disciplines, access to neuromorphic cognitive computing remains mostly limited to a small community of highly trained researchers in the field due to high entry barriers and costs associated with the specialized nature and complex operation of currently available systems.  This project will construct and support a general-purpose neuromorphic cognitive computing platform that will be the largest and most versatile realized to date as well as the first to be broadly available and open to the research community at large, for research into new forms of brain-inspired computing that are more effective and more efficient in approaching the cognitive capabilities of the human mind.  Targeting wide adoption by a diverse cross-section of users in the broader STEM research community, the platform will feature a natural user interface that shields novice users from the challenges arising in operating and configuring highly specialized neuromorphic hardware, by providing a set of user-friendly software tools maintained by and shared with the user community.  Building on extensive existing network and storage infrastructure for user access and data sharing at the San Diego Supercomputer Center, the platform will be hosted and maintained through the Neuroscience Gateway (NSG) Portal, which currently serves over 600 active users in the scientific community.<br/><br/>The large-scale neuromorphic platform will serve as a new and unparalleled resource to the Computer and Information Science and Engineering (CISE) research community, addressing a great need for an experimental testbed for research in alternative forms of computing beyond the traditional von Neumann paradigm and the impending physical limits to Moore's Law expansion in the scaling of computing technology.  The reconfigurable platform will feature a hierarchically interconnected network of in-memory computing processing nodes that emulates, in real-time, highly flexible neural dynamics (integrate-and-fire, graded, stochastic binary, etc) of up to 128 million neurons with high flexible connectivity and plasticity (spike-timing dependent plasticity, gradient-based deep learning, etc) of up to 32 billion synapses.  The system will be capable of biophysical detail in computational neuroscience modeling, as well as high performance and efficiency in on-line adaptive pattern recognition, serving and bringing together both computational neuroscience and computational intelligence communities that have traditionally pursued disparate computational approaches.  The user interface of the platform will support software tools and resources for deep learning and run-time optimization in artificial intelligence applications, and for interference of structure and functional connectivity from recorded neural activity in computational neuroscience research, among others.  To facilitate greatest scientific and societal impact, the infrastructure will be made available free of charge, on a time-managed shared basis, to any researcher in return for agreeing to share source code and data necessary to replicate results reported in the literature.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801391","SaTC: CORE: Medium: Collaborative: Using Machine Learning to Build More Resilient and Transparent Computer Systems","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/01/2018","08/20/2018","Ljudevit Bauer","PA","Carnegie-Mellon University","Standard Grant","Wei-Shinn Ku","08/31/2022","$691,625.00","Matthew Fredrikson","lbauer@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1714, 8060","025Z, 075Z, 7434, 7924","$0.00","Machine learning algorithms are increasingly part of everyday life: they help power the ads that we see while browsing the web, self-driving aids in modern cars, and even weather prediction and critical infrastructure. We rely on these algorithms in part because they perform better than alternatives and they can be easy to customize to new applications. Many machine learning algorithms also have a big weakness: it is difficult to understand how and why they compute the answers they provide. This opaqueness means that the answers we get from a machine learning algorithm could be subtly biased or even completely wrong, and yet we might not realize it. This project's goal is to make machine learning algorithms easier to understand, as well as to leverage some of the techniques used by attackers to trick machine learning algorithms into making mistakes to build computer systems that are more resistant to attack. In addition to making fundamental contributions to how machine learning algorithms are designed and used, the project includes outreach efforts that will entice students to gain hands-on experience with machine learning tools.<br/><br/>This project focuses on deep neural networks (DNNs). A groundswell of research within the past five years has demonstrated the propensity of these models to being evaded by inputs created to fool them -- so called ""adversarial examples."" These types of attacks leverage DNNs' opacity: while DNNs can perform remarkably well on some classification tasks, they often defy simple explanations of how they do so, and indeed can leverage features for doing so that humans might find surprising. This project leverages DNNs and the attacks against them to gain insights into how to build more resilient computer systems. Specifically, the project will use DNNs to model adversaries trying to attack computer systems and then ""attack"" these DNNs to learn how to improve these systems' resilience to attack. This modeling will be done using Generative Adversarial Nets (GANs), in which ""generator"" and ""discriminator"" models compete. Central to this vision are the abilities to evade DNNs under constraints and to extract explanations from them about how they perform classification. Consequently, this project will make fundamental advances both in developing better methods to deceive DNNs and in improving this important machine-learning tool.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815011","AF: Small: Foundations for Collaborative and Information-Limited Machine Learning","CCF","Special Projects - CCF, Algorithmic Foundations","10/01/2018","08/28/2018","Avrim Blum","IL","Toyota Technological Institute at Chicago","Standard Grant","A. Funda Ergun","09/30/2021","$324,884.00","","avrim@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","2878, 7796","075Z, 7923, 7926","$0.00","Machine learning increasingly is being used throughout society, and in a wide range of applications.  Businesses use machine learning systems for decision support, internet sites use machine learning to better interact with users, our personal devices use machine learning to adapt to our needs, and our cars are beginning to use data-trained systems to improve safety.  These applications bring up new opportunities as well as new concerns. Opportunities include the potential for systems to more rapidly learn and adapt through collaboration, and concerns include privacy and the fairness of algorithmically-made decisions.  This project is aimed at developing new foundational understanding of these opportunities and concerns, to help guide the development of more efficient, more adaptive, and fairer, machine learning methods.  This project additionally will support educational workshops on these issues, and more broadly will support the education and training of young scientists on these topics.<br/><br/>Specifically, this project has the following four main thrusts: (1) Collaborative Machine Learning. How can devices with related learning tasks best collaborate to learn efficiently from only a modest amount of data, and how can privacy and related concerns be addressed?  (2) Property Testing and Error Extrapolation.  This thrust aims to develop methods that, from a small amount of labeled data, can reliably estimate how well a given learning algorithm or representation class would perform if given a much larger labeled data sample.  (3) Semi-Supervised Learning. Semi-supervised learning refers to methods that combine labeled and unlabeled data, to learn well even when labeled data is limited. This work aims to develop theoretical foundations for an approach based on explicitly learning regularities within the unlabeled data and then using these to guide how learning is performed over the labeled data. (4)  Fairness in Learning. There has recently been substantial concern about algorithmic decisions (such as whether to offer an applicant a loan) that could unfairly discriminate against certain classes of people. This work aims to develop improved theoretical understanding, tools, and guarantees for tackling these kinds of problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848840","Conference on Cognitive Computational Neuroscience (CCN): September 2018, Philadelphia, PA","BCS","Cognitive Neuroscience, Robust Intelligence","09/01/2018","08/23/2018","Alyson Fletcher","CA","University of California-Los Angeles","Standard Grant","Kurt Thoroughman","08/31/2021","$50,000.00","Thomas Naselaris","akfletcher@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","1699, 7495","1699, 7556, 8089, 8091","$0.00","This project will provide three-years of support for the Conference on Cognitive Computational Neuroscience (CCN). This conference provides an annual scientific meeting for neuroscientists whose goal is to develop computationally defined models of brain information processing that explain rich measurements of brain activity and behavior. Historically, different disciplines have met subsets of these goals: Cognitive science has developed computational models at the cognitive level; computational neuroscience has developed neurobiologically plausible computational models at lower levels; cognitive neuroscience has mapped processes onto brain regions; and artificial intelligence has developed synthetic systems.  CCN is unique in its focus on the intersection between these fields.  In addition to advancing research, CCN seeks to contribute to the growing commercial use of biologically inspired hardware and software in Artificial Intelligence as well as being a vehicle for broadly impacting education and society.  One particular focus of CCN is increasing the visibility of women and scientists from underrepresented populations via speaking opportunities. This award will partially support travel grants for this purpose.  The conference will also include hands-on tutorials, and materials from these will propagate to various university curricula.  The award will support video recordings of the tutorials and talks.  These recordings will be made publicly available on the website to increase the broader impact of the conference to the wider community and those unable to attend. <br/><br/>A central goal of neuroscience is to understand how vast populations of neurons give rise to complex behavior. Today, advances in various domains offer tangible possibilities to make fundamental conceptual breakthroughs. Modern neural recording technologies now provide opportunities to observe neural activity at unprecedented resolution and scale. At the same time, research in cognitive science has become increasingly sophisticated in identifying computational principles that may serve as the basis for human cognition, and machine learning and artificial intelligence have made great strides in building models to autonomously solve complex cognitive tasks. However, interactions among these distinct disciplines remain rare. This new conference may stimulate unifying frameworks that fully realize the cross-disciplinary potential of these individual advances. Concretely, the goal of CCN is to create and foster a community that will develop models of brain information processing with several key features. These models should (1) be fully computationally defined and implemented in computer simulations; (2) be neurobiologically plausible; (3) explain measurements of brain activity (and continue to do so as spatiotemporal resolution and scale improve); (4) explain behavior in the context of naturalistic stimuli and tasks; and (5) perform feats of intelligence such as recognition, internal modelling and representation of the environment, decision-making, planning, action, and motor control. Such models currently do not exist and are unlikely to emerge without greatly improved cross-disciplinary engagement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801494","SaTC: CORE: Medium: Collaborative: Using Machine Learning to Build More Resilient and Transparent Computer Systems","CNS","Special Projects - CNS","09/01/2018","08/20/2018","Michael Reiter","NC","University of North Carolina at Chapel Hill","Standard Grant","Wei-Shinn Ku","08/31/2022","$333,320.00","","michael.reiter@duke.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1714","025Z, 075Z, 7924","$0.00","Machine learning algorithms are increasingly part of everyday life: they help power the ads that we see while browsing the web, self-driving aids in modern cars, and even weather prediction and critical infrastructure. We rely on these algorithms in part because they perform better than alternatives and they can be easy to customize to new applications. Many machine learning algorithms also have a big weakness: it is difficult to understand how and why they compute the answers they provide. This opaqueness means that the answers we get from a machine learning algorithm could be subtly biased or even completely wrong, and yet we might not realize it. This project's goal is to make machine learning algorithms easier to understand, as well as to leverage some of the techniques used by attackers to trick machine learning algorithms into making mistakes to build computer systems that are more resistant to attack. In addition to making fundamental contributions to how machine learning algorithms are designed and used, the project includes outreach efforts that will entice students to gain hands-on experience with machine learning tools.<br/><br/>This project focuses on deep neural networks (DNNs).  A groundswell of research within the past five years has demonstrated the propensity of these models to being evaded by inputs created to fool them -- so called ""adversarial examples."" These types of attacks leverage DNNs' opacity: while DNNs can perform remarkably well on some classification tasks, they often defy simple explanations of how they do so, and indeed can leverage features for doing so that humans might find surprising. This project leverages DNNs and the attacks against them to gain insights into how to build more resilient computer systems. Specifically, the project will use DNNs to model adversaries trying to attack computer systems and then ""attack"" these DNNs to learn how to improve these systems' resilience to attack. This modeling will be done using Generative Adversarial Nets (GANs), in which ""generator"" and ""discriminator"" models compete. Central to this vision are the abilities to evade DNNs under constraints and to extract explanations from them about how they perform classification. Consequently, this project will make fundamental advances both in developing better methods to deceive DNNs and in improving this important machine-learning tool.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823015","SPX: Collaborative Research: Automated Synthesis of Extreme-Scale Computing Systems Using Non-Volatile Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","07/05/2018","Nathaniel Cady","NY","SUNY Polytechnic Institute","Standard Grant","Anindya Banerjee","09/30/2022","$500,000.00","","ncady@sunypoly.edu","257 Fuller Rd.","Albany","NY","122033603","5184378689","CSE","042Y","026Z","$0.00","The project investigates the design of a scalable computing infrastructure that uses nanoscale non-volatile memory (NVM) devices for both storage and computation. The project's novelties are (i) the use of multiple parallel flows of current through naturally occurring sneak paths in NVM crossbars for computation; (ii) the replacement of slow organic expert-driven discovery of flow-based computing designs by automated synthesis techniques for accelerated discovery of novel NVM crossbar designs; and (iii) a pervasive focus on fault-tolerance throughout the design of exact, approximate and stochastic flow-based computing designs. The project's impacts are (i) the design of an end-to-end framework that maps compute-intensive kernels written in a high-level programming language onto nanoscale NVM crossbar designs and (ii) the creation of a new scalable capability to perform exact and approximate in-memory digital computations on fault-prone nanoscale NVM crossbars. The team of computer scientists and nanoscience researchers is creating flow-based computing designs for four benchmark problems: the Feynman grand prize problem, computer vision, basic linear algebra, and simulation of dynamical systems. The automatically synthesized NVM crossbar designs are being evaluated using high-performance simulations and experimental benchmarking in a modern nanotechnology laboratory. <br/><br/><br/>Computing using multiple parallel flows of current through data stored in nanoscale crossbars is often fast and more energy-efficient, but the design of such crossbars is highly unintuitive for human designers. The project explores a combination of formal methods for checking satisfiability of Boolean formulae, and artificial intelligence techniques such as best-first search, to automatically synthesize NVM crossbar designs from specifications written in a high-level programming language. The team of computer scientists and nanoscience researchers is pursuing a transformative agenda for extreme-scale computing by leveraging memory devices in NVM crossbars as structurally-constrained fault-prone distributed nano-stores of data, and exploiting the natural parallel flow of current through NVM crossbars for computing over data stored in the distributed nano-stores. The NVM crossbar designs generated from OpenCV, LAPACK, and ODEINT programs are evaluated using the Xyce circuit simulation software and subsequently fabricated for experimental benchmarking. By combining storage and computation on the same device, the project circumvents the von Neumann barrier between the processor and the memory and creates scalable solutions for extreme-scale computing on fault-prone NVM crossbars without introducing substantial changes to the programming model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758684","SBIR Phase II:  Predicting Healthcare Fraud, Waste and Abuse by Automatically Discovering Social Networks in Health Insurance Claims Data through Machine Learning","IIP","SBIR Phase II","04/01/2018","07/06/2020","PARTHA DATTA RAY","CA","Albeado, Inc","Standard Grant","Alastair Monk","03/31/2021","$1,049,842.00","","partha.dattaray@albeado.com","5201 Great America Parkway","Santa Clara","CA","950541157","4083167831","ENG","5373","096Z, 169E, 5373, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will usher new Artificial Intelligence/Machine Learning (AI/ML) products delivering high accuracy with explainability. Without rationale behind predictions, decision makers can't trust and effectively use AI/ML solutions.  Outcome of R&D through this project would lead to more accurate and faster detection with appropriate explanation of anomalous interactions and recommend effective controls to 1) eliminate billions of dollars of fraud, waste and abuse (FWA) in Health Insurance  markets;  2) lower costs, improve quality and speed of Health Care delivery to consumers; and 3) promote new markets in Personalized Health and Smart Health sector for  emerging Medical Internet-of-Things (IOT) devices and systems, enabling economic growth.  The results of this research are expected to enable the discovery of medical anomaly together with advancing the detection of new types of FWA. The boost in detection accuracy with explanation will save hundreds of millions of dollars. Societal impact includes reduced costs to consumers and taxpayers through better FWA control and advance health outcome through early medical IOT anomaly detection. More broadly, the system is expected to detect possible opioid or substance abuse epidemic cohorts, under/over-medication, advanced alerts for community health anomalies.<br/><br/>The proposed project will extend and generalize a novel machine learning method to solve the Fraud, Waste, and Abuse (FWA) problem in health insurance, coupled with explanatory capability providing rational behind predictions and operationalized in a distributed parallel computing framework for scaling. The technical problem is how to combine relations between entities (e.g., doctors) with their attribute (e.g., a doctor's prescription history). This project advances the state of the art by combining relations between rows in the training data (e.g. doctors) with standard machine learning to improve prediction accuracy while facilitating local explanation. The result is vastly improved prediction accuracy with explainability. Thus, the method uses network information to fill in the gaps of entity information alone and vice versa while facilitating explanation for a test case. This method is expected to significantly improve the ability to detect FWA and pave ways for multi Billion dollars savings, call out IOT-based medical anomaly in advance to improve health outcome and build trust in the predictions for the decision makers through the explanations provided. The team intends to deliver not only the accuracy boost with explainability, but a fully operational system with automated data pipeline, parallel and distributed algorithmic processing framework which can be deployed on a SaaS basis or an enterprise solution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815882","SHF: Small: Deep Neural Network Inference on Energy-Harvesting Devices","CCF","Special Projects - CCF","10/01/2018","08/23/2018","Nathan Beckmann","PA","Carnegie-Mellon University","Standard Grant","Yuanyuan Yang","09/30/2021","$450,000.00","Brandon Lucia","beckmann@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878","2878, 7923, 7941","$0.00","Intermittently powered, energy-harvesting computers are sophisticated computing, sensing, and communicating systems that do not need a battery or tethered power source.  These energy-harvesting devices will form the foundation of the next generation of internet-of-things (IoT) applications, ranging from wearable and implantable medical devices, to environmental and atmospheric monitoring, to tiny ChipSat-scale satellites in deep space.  Realizing the value of these applications requires intelligent devices that can frequently make decisions locally and autonomously (i.e., without help from other nearby computers). For example, a device may need to decide whether to turn on a battery-draining camera to detect a person of interest, to decide which sensors embedded in concrete to enable to collect the most useful data about an aging bridge, and to decide when and how much data to send from these sensors back to the cloud.  In recent years, statistical inference and machine learning using deep neural networks has proven the most successful method for such decision-making. Machine learning is a crucially important feature for future IoT devices, but today's resource-constrained energy-harvesting systems do not support the high-intensity computations required by deep neural network inference. This project builds the software computer systems and hardware computer architectures required by future, intermittent IoT devices to enable autonomous, intelligent decision-making using machine learning.  This project produces software systems with novel algorithms that enable today's energy-harvesting IoT devices to efficiently make intelligent decisions.  This project will then design novel parallel computer architectures that are designed specifically for efficient operation of machine learning computations with intermittent input power.  These architectures further increase the efficiency of intermittent decision-making by 10s or 100s of times, enabling a new class of intelligent IoT applications that are not possible using today's architectures. The sum of these software and hardware components addresses the existential question of deep machine learning on intermittent systems, demonstrating its viability and realizing its benefits to academia, industry, and in applications important to society, such as defense, healthcare, and civil infrastructure.  This project contributes towards a diverse future workforce, through integration with course curricula, mentoring of students from under-represented minority groups, and technical high school outreach programs.<br/><br/>The key challenge overcome by this project is to make deep neural network inference viable on a resource-constrained, intermittent device. This task requires architecture and software support to tolerate frequent, intermittent power interruptions and to operate with hundreds of microwatts of power instead of the tens or hundreds of milliwatts required by today's machine learning accelerators.  This project develops approximate, intermittent partial re-execution techniques to efficiently tolerate interruptions without the need to unnecessarily checkpoint and restore software state.  The project develops the first intermittence-safe data-parallel architecture, integrating non-volatile memory with an array of simple compute elements. The project includes an immediate path to software and hardware prototypes and lays the groundwork for a future silicon hardware implementation of the architecture.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822976","SPX: Collaborative Research: Automated Synthesis of Extreme-Scale Computing Systems Using Non-Volatile Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","07/05/2018","Sumit Jha","FL","The University of Central Florida Board of Trustees","Standard Grant","Anindya Banerjee","09/30/2022","$500,000.00","","jha@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","042Y","026Z","$0.00","The project investigates the design of a scalable computing infrastructure that uses nanoscale non-volatile memory (NVM) devices for both storage and computation. The project's novelties are (i) the use of multiple parallel flows of current through naturally occurring sneak paths in NVM crossbars for computation; (ii) the replacement of slow organic expert-driven discovery of flow-based computing designs by automated synthesis techniques for accelerated discovery of novel NVM crossbar designs; and (iii) a pervasive focus on fault-tolerance throughout the design of exact, approximate and stochastic flow-based computing designs. The project's impacts are (i) the design of an end-to-end framework that maps compute-intensive kernels written in a high-level programming language onto nanoscale NVM crossbar designs and (ii) the creation of a new scalable capability to perform exact and approximate in-memory digital computations on fault-prone nanoscale NVM crossbars. The team of computer scientists and nanoscience researchers is creating flow-based computing designs for four benchmark problems: the Feynman grand prize problem, computer vision, basic linear algebra, and simulation of dynamical systems. The automatically synthesized NVM crossbar designs are being evaluated using high-performance simulations and experimental benchmarking in a modern nanotechnology laboratory. <br/><br/>Computing using multiple parallel flows of current through data stored in nanoscale crossbars is often fast and more energy-efficient, but the design of such crossbars is highly unintuitive for human designers. The project explores a combination of formal methods for checking satisfiability of Boolean formulae, and artificial intelligence techniques such as best-first search, to automatically synthesize NVM crossbar designs from specifications written in a high-level programming language. The team of computer scientists and nanoscience researchers is pursuing a transformative agenda for extreme-scale computing by leveraging memory devices in NVM crossbars as structurally-constrained fault-prone distributed nano-stores of data, and exploiting the natural parallel flow of current through NVM crossbars for computing over data stored in the distributed nano-stores. The NVM crossbar designs generated from OpenCV, LAPACK, and ODEINT programs are evaluated using the Xyce circuit simulation software and subsequently fabricated for experimental benchmarking. By combining storage and computation on the same device, the project circumvents the von Neumann barrier between the processor and the memory and creates scalable solutions for extreme-scale computing on fault-prone NVM crossbars without introducing substantial changes to the programming model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813709","RI: Small: Learning Dynamics and Evolution towards Cognitive Understanding of Videos","IIS","Robust Intelligence","09/01/2018","04/22/2019","Chenliang Xu","NY","University of Rochester","Standard Grant","Jie Yang","08/31/2021","$465,990.00","Jiebo Luo","chenliang.xu@rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7495","075Z, 7495, 7923, 9251","$0.00","A fundamental capability of human intelligence is being able to learn to act by watching instructional videos. Such capability is reflected in abstraction and summarization of the instructional procedures as well as in answering questions such as ""why"" and ""how"" something happened in the video. This project aims to build computational models that are able to perform well in above tasks, which require, beyond the conventional recognition of objects, actions and attributes in the scene, the higher-order inference of any relations therein. Here, the higher-order inference refers to inference that cannot be answered immediately by direct observations and thus requires stronger semantics. The developed technology will enable many applications in other fields, e.g., multimedia (video indexing and retrieval), robotics (reasoning capability of why and how questions), and healthcare (assistive devices for visually impaired people). In addition, the project will contribute to education and diversity by involving underrepresented groups in research activities, integrating research results into teaching curriculum, and conducting outreach activities to local K-12 communities. <br/><br/>The research will develop a framework to perform higher-order inference in understanding web instructional videos, such that models devised in this framework are capable of not only discovering and captioning procedures that constitute the instructional event but also answering questions such as why and how something happened. The framework is built on a video story graph that models the dynamics (the composition of actions at different scales) and evolution (the change in object states and attributes), and it supports higher-order inference upon deep learning units and incorporation of external knowledge graph in a unified framework. Methodologies to extract such video story graphs and use them to discover, caption procedures and perform question-answering will be explored. Expected outcomes of this project include: a software package for constructing and performing inference on video story graphs and incorporating external knowledge; a web-deployed system to process user-uploaded instructional videos; and a large video dataset with procedure and question-answering annotations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746511","STTR Phase I:  Microscope-based Technology For Automatic Brain Cell Counts Using Unbiased Methods","IIP","STTR Phase I","01/01/2018","12/27/2017","Peter Mouton","FL","Stereology Resource Center, Inc.","Standard Grant","Ruth Shuman","12/31/2018","$224,417.00","Dmitry Goldgof","peter@disector.com","1810 West Kennedy Blvd","Tampa","FL","336061645","4107391667","ENG","1505","124E, 1505, 7236, 8042","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is in automating the process of unbiased stereology, the state-of-the-method used in the life sciences for counting stained cells on tissue sections.  Unbiased stereology allows neuroscientists to accurately analyze the size and number of brain cells, which are altered in many neurological disorders and mental illnesses. For reasons that are currently unknown, Alzheimer's disease, Parkinson's disease and Amyotrophic Lateral Sclerosis are all associated with a progressive loss of brain cells. In contrast, children with autism are born with too many brain cells, which leads to life-long problems in processing complex streams of information.  Stereology plays an important role in investigating many conditions affecting the brain and assessing the efficacy and safety of possible treatments. Though the proposed technology will initially target research studies to understand and treat all neurological conditions, it can be useful for automatic assessments of cells in all tissues, including cancer screening and diagnosis from biopsies. <br/><br/><br/>The proposed project will develop and optimize an algorithm to help brain scientists identify causes and treatments for neurological disease and mental illness. The proposed technology will use deep learning (artificial intelligence) systems to automatically recognize, count and size brain cells on tissue sections. An important use of this technology will be to analyze brains and nerve tissue from mice and rats treated to show similar neurological diseases as those found in humans. These animal models provide a powerful tool for testing treatments to cure brain disease in humans. Currently stereology studies of tissues from these animals require a trained technician to sit before a computer screen making tedious manual counts of hundreds and thousands of microscopic cells. This outdated approach fails to take advantage of powerful deep learning methods proposed for the proposed software that could complete these tasks 10 times faster and with fewer errors and human biases. Therefore, the proposed technology will use deep learning technology to accelerate basic research and drug development in the U.S., thereby establishing a long-term economic engine and bringing significant benefits to society through scientific breakthroughs and medical discoveries."
"1824257","Learning Depends on Knowledge: Using Interaction Designs and Machine Learning to Contrast the Testing and Worked Example Effects","BCS","Science of Learning, IIS Special Projects","08/15/2018","09/18/2019","Ken Koedinger","PA","Carnegie-Mellon University","Standard Grant","Soo-Siang Lim","07/31/2021","$736,416.00"," Paulo Carvalho","Koedinger@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","004Y, 7484","059Z, 075Z, 9251","$0.00","A distinctive characteristic of human learning is its capability to flexibly acquire a wide range of rich and complex forms of knowledge (e.g., first and second languages) and acquiring new and accumulated knowledge (e.g., learning physics is easier after having learned algebra). An adequate explanation of human learning must address how existing knowledge changes the way we learn so that we achieve knowledge goals and in specific contexts. This project aims to discover and specify how human learning processes operate differently under different contexts, depending upon what content is being learned. This research contributes to improved educational practices by specifying how learning processes are influenced by knowledge acquisition in a systematic and replicable way. This research will enhance our understanding of successful learning and optimal performance.<br/><br/>The project explores learning processes by contrasting learning from retrieval practice and learning from studying examples. The goal of this project is to resolve and clarify how these processes compete for cognitive resources, including attention and working memory, in ways that depend on the knowledge content to be learned. This research examines (1) the learning processes involved in learning from retrieval practice and from worked examples, (2) how these learning processes work differently when applied to different knowledge content, and (3) the computational mechanisms of learning that give rise to learning different content. The researchers use a combination of experiments in which the learning approach is varied along with the materials being studied and machine learning models. It will demonstrate knowledge-learning dependence by showing that one learning process (e.g., retrieval practice) produces better learning outcomes than another (e.g., example encoding) in some knowledge contexts but the reverse occurs in other knowledge contexts. By implementing the studies as part of a machine learning architecture, this research will provide computational evidence and theoretical insight into the hypothesized knowledge-learning dependence framework. The machine learning architecture developed may be used as an educational and research tool in learning sciences, and the project involves training new learning scientists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848029","Convergence: RAISE Integrating machine learning and biological neural networks","CBET","GCR-Growing Convergence Resear, SSA-Special Studies & Analysis, Information Technology Researc, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","09/13/2018","Xue Han","MA","Trustees of Boston University","Standard Grant","Leon Esterowitz","09/30/2021","$999,999.00","Bobak Nazer","xuehan@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","ENG","062Y, 1385, 1640, 8624","049Z, 7236, 8089, 8091","$0.00","The field of neuroscience is undergoing a rapid transformation, and within the next decade, it may become possible to capture data from millions of individual neurons at the same time.  Such a technological advancement would allow scientists to record and analyze a significant fraction of the brain's neural network at unprecedented spatial and time resolutions.  The goal of this research project is to advance our understanding of brain activity through the integration of bioengineering, systems neuroscience and data science and their application to the study of networks of neurons.  The research team will engineer new sensors designed to image the activity of individual neurons within a large network and then apply this method to the study of functioning neural systems.  The team will also develop computational methods to extract information from the resulting, extremely large datasets.  This research will have broader impact through training STEM students in a convergent science area and through deepening our understanding of the science underlying neurological disease and thereby improving mental health treatment.<br/><br/>This research project aims to create novel protein sensors to acquire single-neuron-resolution imaging data.  This methodology could serve as the basis for ultra-large-scale neural network imaging.  The researchers will establish the architectural principles and fundamental limits for fluorescence imaging systems and inference algorithms that extract underlying neural activity.  They will then develop machine learning techniques to extract network-level phenomena from high-dimensional neural data.  Finally, the researchers will study large networks of neurons during behavior and learning via carefully-designed experiments and machine learning techniques.  The technologies developed in this work, to acquire and to analyze, single-neuron-resolution imaging data, will facilitate the understanding of brain's neural network computation at an ultra-large scale, directly confronting challenging societal problems related to the human brain.  The project participants will also educate the next generation of engineers and scientists in the convergent area of neuroscience with data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839704","EAGER: Real-Time: Ultrasonic Reconstruction and Localization with Deep Helmholtz Networks","ECCS","CCSS-Comms Circuits & Sens Sys, EFRI Research Projects","10/01/2018","09/17/2018","Joel Harley","FL","University of Florida","Standard Grant","Lawrence Goldberg","09/30/2020","$273,118.00","","joel.harley@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","ENG","7564, 7633","152E, 1653, 7916","$0.00","Ultrasonic Reconstruction and Localization with Deep Helmholtz Networks<br/><br/>This project studies physics-informed neural networks to characterize and monitor materials and engineered systems with ultrasound. Ultrasound is studied because it is a wireless, high resolution, medically safe, and inherently secure technology that is driving innovations in wearables, medical implants, secure / encrypted communication systems, and imaging. The project's neural network algorithms can characterize materials and engineered systems from the micro-level (e.g., micro-electrical-mechanical systems) to the macro-level (e.g., pipelines, airplanes, or rail lines). The neural networks characterize the materials by learning the general behavior of ultrasound from simulated data, physical constraints, and measured data. That learned behavior is then compared with the true measured behavior.  To achieve our goal, three significant challenges of applying neural networks (and machine learning generally) to many engineered systems are studied: (1) experimental training data is often scarce or unavailable, (2) data diversity and variability is typically high, and (3) purely data-driven approaches offer few engineering assurances. Data scarcity is addressed by training neural networks with simulations rather than experimental data. Data diversity and variability is addressed by using transfer learning theory to transfer generalized knowledge from the simulation data into the analysis of the experimental data. Engineering assurances are improved by incorporating physics-based constraints into the neural networks. The resulting neural networks are referred to as Helmholtz networks, named for the time-independent wave equation.<br/><br/>The objective of the project is to establish the foundation for Helmholtz networks, which are deep, generative, physics-informed neural networks that reconstruct ultrasonic wave propagation and locate ultrasonic sources. The Helmholtz networks are based on the fact that each frequency of a wave can be represented as the sum of a sparse number of spatial modes. The modes are constrained by the Helmholtz equation and this physical constraint ensures that the machine learning algorithm is trustworthy for system-critical engineered systems (e.g., health monitoring of an aircraft). Such physics-informed machine learning is an important (albeit not widely studied) topic for integrating advanced computation tools into real-time engineered systems. <br/>The research thrusts of this proposal are to initiate and explore the foundations for three new types of neural networks: (1) generative Helmholtz networks to learn modal representations of waves and reconstruct wavefields, (2) localization networks to locate ultrasonic sources under uncertainties, and (3) localization Helmholtz networks to locate sources from learned modal representations. Thrust 1 explores the creation of generative models (i.e., generative Helmholtz networks) that learn the spatial modal characteristics of a medium from simulations. These generative models then reconstruct wavefields from undersampled test data. Thrust 2 studies localization networks to locate ultrasonic sources under simulated uncertainties, such as velocity, delay, and/or amplitude uncertainty. Thrust 3 investigates the use of transfer learning to combine Thrust 1 and Thrust 2 and use the learned modes to locate sources in geometrically complex media.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839169","I-Corps: Neuromorphic device derived from resistive switching system","IIP","I-Corps","07/01/2018","06/18/2018","Min Hwan Lee","CA","University of California - Merced","Standard Grant","Andre Marshall","12/31/2019","$50,000.00","","mlee49@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092012039","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is in facilitating innovations in a wide range of industries working on advanced computing-based applications as the result of a reliable neuromorphic chip (where neural networks are etched into silicon). This technology may accelerate the advance of machine learning and ultimately realize a breakthrough in computing architecture by overcoming the Von Neumann bottleneck -- where the intrinsic latency originated from the separation of processor and memory. Empowered by neuromorphic features of this project's technology, machine learning may be carried out with unprecedented speed and efficiency in design and energy consumption. Hence, intensive machine learning based technologies such as face recognition, autonomous driving and other areas of artificial intelligence will be advanced by a significant margin. A successful execution of the project will also facilitate widespread commercialization of emerging applications utilizing artificial intelligence and/or neuromorphic computing. <br/><br/>This I-Corps project leverages the recent innovative research related to highly controllable neuromorphic devices. The technology is based on the oxide-based resistive random access memory (ReRAM), a non-volatile memory based upon electrical stimuli-induced resistance changes, mostly by the formation and rupture of so-called nanoscale conducting filaments in each cell. The intrinsic resemblance of ReRAM cells to neuromorphic systems triggered a recent boom in related research. However, there are still considerable technical issues including wide cell-to-cell variations in operation voltage, cell current and operation speed. Unclear understanding of physical switching mechanisms precludes a rationale design of reliable cells for a commercial device. The proposed novel scheme, in which the uncontrollability is significantly minimized, is expected to achieve a breakthrough in realizing commercializable neuromorphic devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824854","Synthesis and design workshop: Designing Scalable Advanced Learning Ecosystems","DRL","STEM + Computing (STEM+C) Part","09/01/2018","05/17/2019","Robert Kadel","GA","Georgia Tech Research Corporation","Standard Grant","Tatiana Korelsky","08/31/2019","$93,953.00","Ashok Goel, Stephen Harmon, Yakut Gazi","rob.kadel@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","EHR","005Y","7556","$0.00","The U.S. is currently at a crossroads in higher education where access to quality post-secondary education is a prerequisite for entry into the middle class but the ability to afford such an education is becoming increasingly limited. Georgia Tech proposes the Scalable Advanced Learning Ecosystem to address this mismatch. Building on successful online education programs that have offered Master?s degrees in computer science and analytics, Georgia Tech hosts a workshop for higher education administrators and researchers to establish components of Scalable Advanced Learning Ecosystems that can be applied across colleges and universities. The goal of Scalable Advanced Learning Ecosystems is to create educational experiences that can be scaled to growing populations of students while providing the personalized attention that has typically been the domain of unscalable individual, face-to-face teaching and advising.<br/><br/>Technical description<br/><br/>Georgia Tech hypothesizes that with growing demand for higher education, but with a limited supply of professors, advisors, and budgets to meet such demand, Scalable Advanced Learning Ecosystems can be used to provide personalization and customization for large populations of students. Human activity and expertise are used to guide the creation of this ecosystem's innovations, but day-to-day operations are machine-driven. The innovations include: Personalized Learning Systems that use artificial intelligence and machine learning to establish customized educational plans for each student; Intelligent Tutoring Systems that modify the presentation and sequence of materials in response to student performance; Data Mining and Learning Analytics that extract useful and actionable information to understand student uses of learning resources and outcomes; Scalable Online Environments that deliver high-quality, high-engagement learning experiences; and Immersive Learning Environments that simulate a physical presence in real or imagined worlds, overlay content on those worlds, and merge real and virtual worlds to produce new learning opportunities. The workshop is  held on November 29 and 30, 2018 at the Global Learning Center on the Georgia Tech campus in Midtown Atlanta, and participating institutions are selected by proposals submitted that describe each institution?s interest in Scalable Advanced Learning Ecosystems and the institution?s commitment to action to improve educational access.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837131","FMitF: A Novel Framework for Learning Formal Abstractions and Causal Relations from Temporal Behaviors","CCF","FMitF: Formal Methods in the F, Special Projects - CCF","11/01/2018","09/07/2018","Jyotirmoy Deshmukh","CA","University of Southern California","Standard Grant","Nina Amla","10/31/2022","$1,000,000.00","Yan Liu, Paul Bogdan","jdeshmuk@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","094Y, 2878","062Z, 071Z, 075Z, 8206","$0.00","Formal methods consist of a collection of techniques that help developers rigorously reason about the behaviors of software and hardware systems with the help of mathematical logic. While formal logic has been used for articulating system specifications for the purpose of verification or software synthesis, this project introduces a logic-based framework to address machine-learning problems such as classification (which category should a new datum be put into?), clustering (how should a collection of data points be grouped together into categories?) and discovery of causal relations (when should an earlier data observation be deemed to cause the appearance of a later data observation?) for time-series data, in which repeated observations are made over time. The use of formal logic opens new avenues such as enhancing the interpretability of machine-learning models, the explainability of learning results, and articulation of formal guarantees on the behavior of learning algorithms. The societal impact of this work targets discovery of latent information in time-series data in diverse domains such as healthcare, autonomous systems, and security. The research impacts education by providing cross-disciplinary training of undergraduate and graduates students in areas of data science, machine learning, formal methods, and introducing students to methods from statistical physics on a number of real-world systems.<br/><br/>This project explores the intersection between the logical inference based on real-time temporal logics and statistical inference prevalent in machine learning. The algorithms developed in this project allow users to express domain knowledge in the form of signal predicates or chance constraints, and output the results of classification, clustering or causal discovery as formulas in specific real-time temporal logics. This allows the results of the machine-learning algorithms to be human-interpretable, and also improves the explainability of learning algorithms by answering the question of why a particular time-series datum is classified or clustered in a specific fashion.  These techniques are able to model uncertainty in time-series data by creating a new class of non-parametric learning methods that combine concepts from statistical physics, information theory, and statistical inference. The use of a logic-based framework allows providing formal guarantees on the learning process itself by applying ideas such as probably-approximately-correct learning (from computational learning theory) to the inference of real-time temporal logic formulas from data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822683","CRCNS Research Proposal: Collaborative Research: Evaluating Machine Learning Architectures Using a Massive Benchmark Dataset of Brain Responses to Natural Scenes","IIS","CRCNS-Computation Neuroscience","10/01/2018","09/10/2018","Kendrick Kay","MN","University of Minnesota-Twin Cities","Standard Grant","Kenneth Whang","09/30/2021","$652,405.00","","kay@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7327","7327, 8089, 8091","$0.00","Machine learning technologies have the potential to radically transform the study of the human brain, but require far more data than is typically collected during conventional neuroscience experiments. The goal of this project is to drive the application of ML techniques to neuroscience research by generating a massive dataset of brain responses from the human visual system. The resulting dataset will be freely available to scientists, educators, and students. Through a yearly modeling competition, neuroscientists will gain experience in the application of advanced computational methods and ML researchers will gain a deeper understanding of the challenges and complexities of the human brain. Results of the modeling competition will be presented at an annual conference attended by both machine learning and neuroscience researchers and students, providing an opportunity for the two groups to interact and discuss approaches. This project will foster open collaboration between neuroscientists and artificial intelligence researchers and a culture of sharing data, ideas, and progress. <br/><br/>The long-term goal of this work is to generate data that will lead to the development of experimentally validated and computationally powerful models of the human visual system. The project leaders will use high-field (7 Tesla) functional magnetic resonance imaging (fMRI) to measure brain responses to a broad sampling of natural images in human observers. The specific objectives are as follows: (1) Acquire, pre-process, and distribute a massive, high-resolution fMRI dataset that exploits state-of-the-art imaging techniques. The dataset will include multiple samples of brain responses to roughly eighty thousand photographs drawn from an image collection that is widely used by the ML community. (2) Establish and host an annual competition for modeling this rich dataset at the conference on Cognitive Computational Neuroscience. (3) Bridge the gap between ML architectures and the human brain by testing new ML-inspired architectures as models of the visual system. The project leaders will focus specifically on recent developments in ML that suggest new hypotheses about the dorsal visual stream.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813935","III: Small: Transfer Learning using Transformation among Models and Samples","IIS","Info Integration & Informatics","08/15/2018","06/25/2019","C.S. George Lee","IN","Purdue University","Standard Grant","Wei Ding","07/31/2021","$515,938.00","","csglee@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7364","075Z, 7364, 7923, 9251","$0.00","As huge volumes of unlabeled data are generated and made available in many domains, annotating data in these domains becomes burdensome and creates a major bottleneck in maintaining machine-learning databases.  This project will investigate a family of transfer-learning methods as an automatic annotation tool, without human involvement, in annotating data for various machine-learning settings.  The novelty of the project's transfer-learning approach is based on the common concept of matching-based optimization technique to solve the different forms of transfer learning.  The optimization will be carried out using transformations at different levels for different forms.  The planned transfer-learning framework will exploit lots of unlabeled data or a few labelled data in the target domain and prior knowledge in the form of labelled source data, source models or other auxiliary information in the source domain.  Using this common matching-based optimization framework, this will bring out a natural transition from low-level, sample-based matching to high-level, model-based matching for the different forms of transfer learning.  The family of transfer learning methods will have promising ramifications in diverse areas such as intelligent robots and self-driving cars so that they operate efficiently in new and changing environments without the need of large amount of annotated data in the new environments.<br/><br/>This project will investigate two major forms of transfer learning -- domain adaptation and few-shot learning.  The research will focus on studying the effect of the proposed matching-based optimization technique to solve the different forms of transfer learning.  The project will focus on three major tasks, depending on what information is available in each task: (Task 1) Unsupervised domain adaptation, where the source-domain data is labelled while the target-domain data is unlabeled.  In this case, the project team will investigate the optimization based on matching each source-domain sample with each target-domain sample to learn a generalizable target model; (Task 2) Hypothesis transfer learning, where the source and the target domain tasks are different, and only source models and sparsely labelled target domain data will be used to learn a generalizable target model.  The model will be learned using matching between source models and target-domain samples; (Task 3) Few-shot learning, where the goal is to learn a generalizable target model from a few labelled samples in the target domain by utilizing auxiliary source knowledge.  The project team will study whether transformation between source-model parameters can be substituted as useful auxiliary source-domain knowledge.  Hence, the planned research will minimize the requirement of obtaining lots of labelled samples used in machine learning, and it will realize robust learning systems that are generalizable across tasks and domains.  Furthermore, since the matching is carried out among each individual sample/model of information locally and explicitly, the results are expected to be better than previous methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1905558","CAREER: Adversarial Artificial Intelligence for Social Good","IIS","Robust Intelligence","08/01/2018","05/04/2020","Yevgeniy Vorobeychik","MO","Washington University","Continuing Grant","Erion Plaku","02/28/2022","$346,240.00","","yvorobeychik@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7495","1045, 7495, 9251","$0.00","The success of AI technologies has resulted in their widespread deployment, with algorithms for reasoning under uncertainty, such as machine learning, having a particularly high impact.  A challenge that is often ignored, however, is the adversarial nature of many domains, in which social, economic, and political interests may try to manipulate intelligent systems into making costly mistakes.  While AI has a long history in playing adversarial games, such as chess and poker, the approaches have not been appropriate for many real-world situations.  The goal of the proposed research is to develop a general framework for adversarial AI that is far broader in scope and applicability, building on insights from game theory, AI planning, and cybersecurity.<br/><br/>A key modeling insight of the proposed research is that attacks across a broad array of settings can be modeled as planning problems, so that robust algorithms can be fundamentally viewed as interdicting attack plans.  Our research will develop new foundational techniques for scalable plan interdiction under uncertainty, building off of the framework of Stackelberg games. Proposed techniques will leverage a combination of abstraction, factored representation of state, and value function approximation.  In addition, novel scalable algorithms will be developed for multi-stage interdiction problems, modeled as sequential stochastic games, considering both perfect and imperfect information. Moreover, the research will make novel modeling and algorithmic contributions in multi-defender and multi-attacker interdiction games.  Finally, in the more applied arena, the research will make significant intellectual contributions in applying advances in adversarial AI to model problems exhibiting important adversarial aspects, such as privacy-preserving data sharing, access control and audit policies, and vaccine design.<br/>"
"1822929","CRCNS Research Proposal: Collaborative Research: Evaluating Machine Learning Architectures Using a Massive Benchmark Dataset of Brain Responses to Natural Scenes","IIS","CRCNS-Computation Neuroscience, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","09/10/2018","Thomas Naselaris","SC","Medical University of South Carolina","Standard Grant","Kenneth Whang","09/30/2021","$422,619.00","Kendrick Kay","tnaselar@musc.edu","171 ASHLEY AVE","CHARLESTON","SC","294258908","8437923838","CSE","7327, 8624","7327, 8089, 8091, 9150","$0.00","Machine learning technologies have the potential to radically transform the study of the human brain, but require far more data than is typically collected during conventional neuroscience experiments. The goal of this project is to drive the application of ML techniques to neuroscience research by generating a massive dataset of brain responses from the human visual system. The resulting dataset will be freely available to scientists, educators, and students. Through a yearly modeling competition, neuroscientists will gain experience in the application of advanced computational methods and ML researchers will gain a deeper understanding of the challenges and complexities of the human brain. Results of the modeling competition will be presented at an annual conference attended by both machine learning and neuroscience researchers and students, providing an opportunity for the two groups to interact and discuss approaches. This project will foster open collaboration between neuroscientists and artificial intelligence researchers and a culture of sharing data, ideas, and progress. <br/><br/>The long-term goal of this work is to generate data that will lead to the development of experimentally validated and computationally powerful models of the human visual system. The project leaders will use high-field (7 Tesla) functional magnetic resonance imaging (fMRI) to measure brain responses to a broad sampling of natural images in human observers. The specific objectives are as follows: (1) Acquire, pre-process, and distribute a massive, high-resolution fMRI dataset that exploits state-of-the-art imaging techniques. The dataset will include multiple samples of brain responses to roughly eighty thousand photographs drawn from an image collection that is widely used by the ML community. (2) Establish and host an annual competition for modeling this rich dataset at the conference on Cognitive Computational Neuroscience. (3) Bridge the gap between ML architectures and the human brain by testing new ML-inspired architectures as models of the visual system. The project leaders will focus specifically on recent developments in ML that suggest new hypotheses about the dorsal visual stream.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841800","CAREER: Teaching Machines to Design Self-Assembling Materials","DMR","CONDENSED MATTER & MAT THEORY","06/01/2018","07/17/2018","Andrew Ferguson","IL","University of Chicago","Continuing Grant","Daryl Hess","05/31/2020","$90,001.00","","andrewferguson@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","1765","1045, 8400, 9216","$0.00","TECHNICAL SUMMARY<br/><br/>This CAREER award supports theoretical and computational research and education in the understanding and design of self-assembling biomaterials. Self-assembly of structured aggregates by the spontaneous organization of their constituent building blocks is prevalent in the natural world, and is an attractive route to fabricate artificial materials with desirable properties that cannot be easily produced by other means. The design of building blocks programmed to self-assemble custom materials is a grand challenge in materials science.<br/><br/>In this work, the PI will integrate statistical mechanics theory with nonlinear machine learning algorithms to establish a new theoretical and computational approach to understand and program the self-assembly of nanostructured biomaterials. Using these tools, the PI will extract from molecular simulations the pathways and mechanisms by which building blocks self-assemble into structured aggregates. This methodology overcomes a key scientific challenge by integrating thermodynamics and kinetics in a unified framework that identifies both what stable aggregates form (thermodynamics) and how they assemble (kinetics and mechanisms). <br/><br/>The collective order parameters unveiled by this approach are good descriptors of the slow dynamical motions driving assembly, and present a natural parameterization for kinetically meaningful free energy landscapes that link building block properties to collective assembly behavior. By ""sculpting"" the landscape topography through rational manipulation of building block structure and chemistry the PI's group will program the assembly of desired structures that are thermodynamically stable and kinetically accessible (design).<br/><br/>The PI will apply a new approach to three technologically important self-assembling biomaterials: 1) ""patchy colloid"" polyhedral clusters for small molecule encapsulation, 2) ultra-short peptide mineralization templates for silica nanotubes for controlled drug release, heavy metal ion adsorption, and catalysis, and 3) antimicrobial peptide amphiphile nanostructures for antibiotic resistant bacteria. This work will establish new basic understanding and control of materials assembly, and accelerate development of new structural and functional biomaterials. <br/><br/>The integrated education and outreach plan incorporates the scientific outcomes into education and outreach, and supports graduate training, undergraduate research, and mentoring of underrepresented minority groups. The PI will create a new materials science course to equip the next generation workforce with computational tools, support undergraduate students in performing portions of the work, and promote the recruitment, retention, and success of students of color through mentorship of minority students and high school outreach.<br/><br/>NONTECHNICAL SUMMARY<br/><br/>This CAREER award supports a theoretical and computational research program to design microscopic building blocks with the ability to spontaneously self-organize into materials with desirable properties. This way of making materials is known as ""bottom-up self-assembly"", as opposed to more familiar ""top-down"" manufacturing. Imagine if it will be possible one day to design molecules with just the right shape and properties so that shaking them in a flask spontaneously self-assembled a solar cell! In this work, the PI will combine ideas from the fields of thermodynamics and machine learning (sometimes known as artificial intelligence) to establish a new tool to allow computers to learn both what structures can be formed by a particular building block, and how they assemble. The PI will then flip this problem to use our tool to help reverse-engineer building blocks to assemble custom materials. <br/><br/>The PI's group will apply these tools to the design of three useful biological materials: 1) micron-sized particles possessing directional sticky patches that assemble polyhedral clusters to hold and deliver small molecules, 2) short peptides that assemble networks to template the synthesis of silica nanotubes for drug delivery, cleanup of heavy metal pollutants, and catalysis of chemical reactions, and 3) longer peptides that assemble into nanometer sized rods that can kill antibiotic resistant bacteria such as the MRSA ""superbug"".<br/><br/>This award also supports an integrated research and education program in which the scientific results from this work will enrich and enhance undergraduate and graduate classes, and high school outreach activities. Undergraduate students will directly participate in the scientific research by working with the PI during the summer months. The PI will also design and teach a new class providing hands-on experience in the computational materials modeling, analysis, and design, and maintain his commitment to promote the recruitment and success of students of color through mentorship of undergraduate and graduate minority students."
"1816148","RI: Small: Uncovering Dynamics from Internet Imagery","IIS","Robust Intelligence","08/15/2018","07/22/2019","Jan-Michael Frahm","NC","University of North Carolina at Chapel Hill","Standard Grant","Jie Yang","07/31/2021","$450,000.00","","jmf@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7495","075Z, 7495, 7923","$0.00","Virtual- and augmented reality (VR/AR) technologies have the promise to enable new and exciting ways of perceiving the world from the comfort of our homes and desks. Among the current applications of 3D VR visualizations, obtaining realistic depictions of actual real-world environments is highly desired in educational experiences. This project will develop scalable algorithms for computing ""living 3D models"" that can represent elements such as people and cars moving around the scene, water flowing in fountains, or chairs outside cafes being placed in different places on different days. To overcome the need for dedicated capture, the project targets publicly available Internet photo collections, which have the requisite data diversity to drive large-scale, cost-effective VR/AR content generation. The research not only supports the field of VR/AR but also provides improved analysis methods for a broad range of other applications, including forensic analysis, cultural heritage conversation, city planning, virtual training, and education, with particularly potential impact in enhancing social study experiences for economically disadvantaged students. <br/><br/>This project will aggregate object instances in the individual 2D images of the photo collection to infer the motion dynamics of the entire class of objects in the scene, e.g., all cars or all people. The method will thus infer and model the motion dynamics without ever seeing the motion of these objects, since there is typically only one observation per object instance available due to the uncontrolled, crowd-sourced capture. The key information for the inference will be the observation of the varying densities of the dynamic scene elements in the scene. The novel scene representation stores the accumulated dynamics in object class scene occupancy maps, as well as object class motion flows for the scene, e.g., the information where pedestrians move to in the scene and how they move within the scene. The developed methodology will open new and exciting avenues for research on jointly recovering semantic labels and 3D geometry in the wild, a task that is one of the currently most challenging problems in 3D computer vision.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1906230","I-Corps: A Deep Learning Toolbox for Subsurface Imaging in Exploration for Oil and Gas","IIP","I-Corps","12/01/2018","12/12/2018","Xuqing Wu","TX","University of Houston","Standard Grant","Andre Marshall","05/31/2021","$50,000.00","","xwu8@central.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to meet the increasing demand of energy consumption and maintain a healthy oil and gas output in the US. Machine vision and automatic data picking are viable approaches to keep up with the data multiplication of seismic surveys. The proposed product is the first one that is aiming at minimizing labor burden in seismic data processing by using a deep neural network with a novel and efficient transfer learning strategy. If successful, the novel deep transfer learning approach will build the foundation for adopting advanced deep neural network technologies by many other industrial applications. In addition to the Oil and Gas industry, the product can also be used in other important areas such as geothermal energy exploration.<br/><br/>This I-Corps project is to explore the market potential for a product to automatically identify unique patterns embedded in the seismic data. The intellectual merit of this proposal lies in the theme of a novel deep transfer learning approach, which will ease the training burden of the deep neural network and solve the training data shortage problem by utilizing discriminative unsupervised feature learning to learn high-level representations that are more invariant to variations between the synthetic and real data. The innovativeness of our proposal is that it is aimed to build a reliable system to perform various pattern recognition tasks required by the seismic data processing and be adaptive to different datasets acquired through seismic surveys. This product has the potential to save up to 20% to 50% of the total seismic processing time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817048","SHF: Small: Algorithms and Software for Scalable Kernel Methods","CCF","Leadership-Class Computing, Software & Hardware Foundation","07/01/2018","05/06/2019","George Biros","TX","University of Texas at Austin","Standard Grant","Almadena Chtchelkanova","06/30/2021","$485,562.00","","gbiros@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781, 7798","7923, 7942","$0.00","Scientists and engineers are increasingly interested in using machine learning methods on huge datasets that cannot be processed on a single workstation.  At the same time public and private institutions are making significant investments on high-performance computing (HPC) clusters equipped with thousands of leading edge processors and network connectivity. However, despite the availability of such HPC systems, data analysis tasks are mostly restricted to a single or a few workstations. The reason is that, with few exceptions, existing machine learning software does not scale efficiently on HPC systems. The need to process in-situ large scientific and engineering datasets is not met with current software and significant downsampling is required in order to use existing tools. A serious bottleneck in current artificial intelligence (AI) workflows is the significant cost of training for large scale problems. The slow convergence of existing methods and the large number of calibration hyper-parameters (learning rate, batch size, and other knobs that control the performance of the AI system) make training extremely expensive. Design and analysis of scalable optimization algorithms for faster training, that is the fitting of the machine learning (ML) model parameters to the data, are needed for analytics in real time  and at scale, which is the goal of this project.<br/><br/>The proposed research will introduce novel numerical methods and parallel algorithms for second-order/Newton methods that will be tailored to machine learning (ML) models and will be many orders of magnitude faster than the existing state of-the-art (first-order methods like steepest descent). The researchers plan to design, analyze, and implement robust approximations for covariance matrices, a class of matrices in AI and computational statistics, used in statistical analysis (e.g., sampling, risk assessment, and uncertainty quantification). The investigators plan to design, analyze, and implement scalable fast algorithms in the context of high-performance computing for the so called nearest-neighbor problem, a particular method in ML, data analysis, and information retrieval. The resulting software library will provide a means for end-to-end tools for discovery and innovation and provide new capabilities in the NSF XSEDE infrastructure project. Along with  research activities, an educational and dissemination program is designed to communicate the results of this work to both students and researchers, as well as a more general audience of computational and application scientists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815948","III: Small: Collaborative Research: Explainable Natural Language Inference","IIS","Info Integration & Informatics","09/01/2018","06/22/2020","Peter Jansen","AZ","University of Arizona","Standard Grant","Maria Zemankova","08/31/2021","$262,463.00","Mihai Surdeanu","pajansen@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7364","075Z, 7364, 7923, 9251","$0.00","Natural language inference (NLI) can support decision-making using information contained in natural language texts (e.g, detecting undiagnosed medical conditions in medical records, finding alternate treatments from scientific literature). This requires gathering facts extracted from text and reasoning over them. Current automated solutions for NLI are largely incapable of producing explanations for their inferences, but this capacity is essential for users to trust their reasoning in domains such as scientific discovery and medicine where the cost of making errors is high. This project develops natural language inference methods that are both accurate and explainable. They are accurate because they build on state-of-the-art deep learning frameworks which use powerful, automatically learned, representations of text. They are explainable because they aggregate information in units that can be represented in both a human readable explanation and a machine-usable vector representation. This project will advance methods in explainable natural language inference to enable the application of automated inference methods in critical domains such as medical knowledge extraction. The project will also evaluate the explainability of the inference decisions in collaboration with domain experts.<br/><br/>This project reframes natural language inference as the task of constructing and reasoning over explanations. In particular, inference assembles smaller component facts into a graph (explanation graph) that it reasons over to make decisions. In this view, generating explanations is an integral part of the inference process and not a separate post-hoc mechanism. The project has three main goals: (a) Develop multiagent reinforcement learning models that can effectively and efficiently explore the space of explanation graphs, (b) Develop deep learning based aggregation mechanisms that can prevent inference from combining semantically incompatible evidence, and (c) Build a continuum of hypergraph based text representations that combine discrete forms of structured knowledge with their continuous embedding based representations. The techniques will be evaluated on three application domains: complex question answering, medical relation extraction, and clinical event detection from medical records. The results of the project will be disseminated through the project website, scholarly venues, and the software and datasets will be made available to the public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822650","CRCNS Research Proposal:  Collaborative Research: New Dimensions of Visual Cortical Organization","IIS","CRCNS-Computation Neuroscience, Robust Intelligence","10/01/2018","09/07/2018","Steven Zucker","CT","Yale University","Standard Grant","Kenneth Whang","09/30/2022","$625,115.00","","steven.zucker@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7327, 7495","075Z, 7327, 8089, 8091","$0.00","The visual system of the mouse is now widely studied as a model for developmental neurobiology, as well as for the understanding of human disease, because it can be studied with the most powerful modern genetic and optical tools.  This project aims to discover how neurons in the visual cortex of the mouse allow it to see well by measuring how the cortex represents ecologically-relevant properties of the visual world.  Quantitative studies of neurons in the mouse's primary visual cortex to date reveal only very poor vision, but their behavior indicates that mice can see much better than that -- they avoid predators and catch crickets in the wild. To understand mouse vision, the investigators will study responses to novel, mathematically tractable stimuli resembling the flow of images across the retina as the mouse moves through a field of grass.  Studies based on these new stimuli indicate that most V1 neurons respond reliably to fine details of the visual scene.  A mathematical understanding of how the brain takes in the visual world should have real implications for how we see, and should have great benefits for artificial vision by computers and robots.  Bringing these ideas into the classroom will provide the foundation for new technologies, and will expose students to both real and artificial vision systems.<br/><br/>Analyses of the brain's visual function are limited by the stimuli used to probe them. Conventional quantitative approaches to understanding biological vision have been based on models with linear kernels in which only the output might be subject to a nonlinearity, all derived from responses of neurons in the brain to gratings of a range of spatial frequencies.  This analysis fails to capture relevant features of natural images, which can not be constrained to linearity. The goal of this project is to probe the mouse visual system beyond the linear range but below the barrier posed by the complexity of arbitrary natural images. The investigators have identified an intermediate stimulus class--visual flow patterns--that formally approximate important features of natural visual scenes, resembling what an animal would see when running through grass. Flow patterns have a rich geometry that is mathematically tractable.  This project will develop such stimuli and test them on awake-behaving mice, while recording the resultant neural activity in the visual cortex.  Studying the mouse opens up the possibility of applying the entire range of powerful modern neuroscience tools-- genetic, optical, and electrophysiological. Visual responses will be analyzed using a novel variety of machine learning algorithms, which will allow the investigators to model the possible neural circuits and then test predictions from those model circuits.  Such an understanding of the brain will inform both primate vision and the next generation of artificially-intelligent algorithms which, as a result, should benefit from being more ""brain-like.""<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830479","NRI: INT: COLLAB: Manufacturing USA: Intelligent Human-Robot Collaboration for Smart Factory","CMMI","AM-Advanced Manufacturing, NRI-National Robotics Initiati","09/15/2018","06/07/2019","Zhaozheng Yin","MO","Missouri University of Science and Technology","Standard Grant","Bruce Kramer","02/29/2020","$683,656.00","Ming Leu","zhaozheng.yin@stonybrook.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","ENG","088Y, 8013","062Z, 063Z, 075Z, 116E, 7632, 8086, 9102, 9150, 9178, 9231, 9251, MANU","$0.00","This National Robotics Initiative (NRI) collaborative research project addresses the NSF Big Idea of Work at the Human-Technology Frontier by targeting human-robot collaboration in manufacturing.  Recent advances in sensing, computational intelligence, and big data analytics have been rapidly transforming and revolutionizing the manufacturing industry towards robot-rich and digitally connected factories. However, effective, efficient and safe coordination between humans and robots on the factory floor has remained a significant challenge. To meet the need for safe and effective human-robot collaboration in manufacturing, the investigators will research an integrated set of algorithms and robotic test beds to sense, understand, predict and control the interaction of human workers and robots in collaborative manufacturing cells.  It is expected that these methods will  significantly improve the safety and productivity of hybrid human-robot production systems, thereby promoting their deployment in future ""smart factories"".  To broaden the impact of this project, a partnership with Manufacturing USA Institute(s) and professional societies will be established to provide human-robot collaboration learning modules for inclusion in robotics and smart manufacturing-related curricula.  These learning modules, together with annual events aimed at community college and pre-college students, and workshops for the dissemination of research results will raise public awareness and attract new entrants into the manufacturing and robotics industries, creating truly synergetic education opportunities in science, technology, engineering and mathematics, as well as accelerating the adoption of smart factory-enabling technologies.  <br/><br/>The project will address fundamental challenges in human-robot collaboration in the manufacturing environment, such as the limitation of one-to-one sensing between humans and robots, the lack of adaptive and stochastic modeling methods for reliable recognition and prediction of human actions and motions in different manufacturing scenarios, and multi-scale human-robot coordination. To address these challenges, multi-disciplinary research involving sensing, machine learning, stochastic modeling, robot path planning, and advanced manufacturing will be performed.  Specific tasks include algorithm development and deployment on lab-scale and real-world test beds to: (1) sense and recognize where objects (e.g., robots, humans, parts or tools) are located and what each worker is doing; (2) predict what the next human action will be; and (3) plan and control safe and optimal robot trajectories for individualized on-the-job assistance for humans, proactively avoiding worker injury. The outcomes from the project will be evaluated on the shop-floor at the collaborating company COsorizio MAcchine Uensili (COMAU) in Michigan, and the Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing of the National Research Council of Italy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808752","Collaborative Research: Reinforcement learning based adaptive optimal control of powered knee prosthesis for human users in real life","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2018","08/14/2018","Jennie Si","AZ","Arizona State University","Standard Grant","Anthony Kuh","07/31/2021","$250,925.00","","si@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","7607","1653","$0.00","The proposed research aims at designing robust, real time learning controllers for powered lower limb prosthesis worn by above-knee amputees. It centers on adaptive optimal tuning of prosthetic knee joint impedance parameters with an ultimate goal of achieving human-prosthesis symbiosis. Current state-of-the-art approaches rely on a predetermined collection of knee joint impedance parameters, resulted from tedious manual tuning in a clinic. In addition to a lack of adaptability to different users, current impedance controls do not adapt to different use environments. One of the key design challenge is due to the constant interaction between the human user and the robotic leg. As such, advanced robotics including those employing latest artificial intelligence technologies, control system theory and design, and existing biomechanics based controls cannot meet the needs of real time learning control of a powered prosthetic leg in a human-prosthesis system. Given the nature of the problem, reinforcement learning based adaptive optimal control, also referred to as adaptive dynamic programming (ADP), holds great promise to delivering the next generation of prosthesis control solutions. <br/><br/>Intellectual Merit: The design challenge requires innovative approaches of real time reinforcement learning control. The learning controller has to be designed without knowing an explicit dynamic system model describing the human-prosthesis system, while assuring human user safety and system stability, and being scalable and adaptable to different users and use conditions. Putting it all together, the success of this project will be an important milestone for machine learning, control engineering, and rehabilitation engineering. <br/><br/>Broader Impacts: This research has a direct impact on improving the lives of above-knee amputees. Also of great societal impact is the potential of reducing health care cost. New knowledge gained from human-robot interaction will not only aid amputees but also stroke patients who use exoskeleton as assistive devices. The proposed research will also benefit several research communities such as wearable robots, machine learning, and rehabilitation to develop new technologies addressing real applications. To excite and educate future leaders and researchers in science and engineering, the project will provide an opportunity for integration of our research work into graduate education and postdoc training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830383","NRI: INT: COLLAB: Manufacturing USA: Intelligent Human-Robot Collaboration for Smart Factory","CMMI","NRI-National Robotics Initiati","09/15/2018","09/11/2018","Gloria Wiens","FL","University of Florida","Standard Grant","Bruce Kramer","08/31/2022","$377,930.00","","gwiens@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","ENG","8013","062Z, 063Z, 075Z, 7632, 8086, 9102, MANU","$0.00","This National Robotics Initiative (NRI) collaborative research project addresses the NSF Big Idea of Work at the Human-Technology Frontier by targeting human-robot collaboration in manufacturing.  Recent advances in sensing, computational intelligence, and big data analytics have been rapidly transforming and revolutionizing the manufacturing industry towards robot-rich and digitally connected factories. However, effective, efficient and safe coordination between humans and robots on the factory floor has remained a significant challenge. To meet the need for safe and effective human-robot collaboration in manufacturing, the investigators will research an integrated set of algorithms and robotic test beds to sense, understand, predict and control the interaction of human workers and robots in collaborative manufacturing cells.  It is expected that these methods will  significantly improve the safety and productivity of hybrid human-robot production systems, thereby promoting their deployment in future ""smart factories"".  To broaden the impact of this project, a partnership with Manufacturing USA Institute(s) and professional societies will be established to provide human-robot collaboration learning modules for inclusion in robotics and smart manufacturing-related curricula.  These learning modules, together with annual events aimed at community college and pre-college students, and workshops for the dissemination of research results will raise public awareness and attract new entrants into the manufacturing and robotics industries, creating truly synergetic education opportunities in science, technology, engineering and mathematics, as well as accelerating the adoption of smart factory-enabling technologies.  <br/><br/>The project will address fundamental challenges in human-robot collaboration in the manufacturing environment, such as the limitation of one-to-one sensing between humans and robots, the lack of adaptive and stochastic modeling methods for reliable recognition and prediction of human actions and motions in different manufacturing scenarios, and multi-scale human-robot coordination. To address these challenges, multi-disciplinary research involving sensing, machine learning, stochastic modeling, robot path planning, and advanced manufacturing will be performed.  Specific tasks include algorithm development and deployment on lab-scale and real-world test beds to: (1) sense and recognize where objects (e.g., robots, humans, parts or tools) are located and what each worker is doing; (2) predict what the next human action will be; and (3) plan and control safe and optimal robot trajectories for individualized on-the-job assistance for humans, proactively avoiding worker injury. The outcomes from the project will be evaluated on the shop-floor at the collaborating company COsorizio MAcchine Uensili (COMAU) in Michigan, and the Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing of the National Research Council of Italy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830295","NRI: INT: COLLAB: Manufacturing USA: Intelligent Human-Robot Collaboration for Smart Factory","CMMI","NRI-National Robotics Initiati","09/15/2018","09/11/2018","Robert Gao","OH","Case Western Reserve University","Standard Grant","Bruce Kramer","08/31/2022","$453,549.00","","Robert.Gao@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","ENG","8013","062Z, 063Z, 075Z, 7632, 8086, MANU","$0.00","This National Robotics Initiative (NRI) collaborative research project addresses the NSF Big Idea of Work at the Human-Technology Frontier by targeting human-robot collaboration in manufacturing.  Recent advances in sensing, computational intelligence, and big data analytics have been rapidly transforming and revolutionizing the manufacturing industry towards robot-rich and digitally connected factories. However, effective, efficient and safe coordination between humans and robots on the factory floor has remained a significant challenge. To meet the need for safe and effective human-robot collaboration in manufacturing, the investigators will research an integrated set of algorithms and robotic test beds to sense, understand, predict and control the interaction of human workers and robots in collaborative manufacturing cells.  It is expected that these methods will  significantly improve the safety and productivity of hybrid human-robot production systems, thereby promoting their deployment in future ""smart factories"".  To broaden the impact of this project, a partnership with Manufacturing USA Institute(s) and professional societies will be established to provide human-robot collaboration learning modules for inclusion in robotics and smart manufacturing-related curricula.  These learning modules, together with annual events aimed at community college and pre-college students, and workshops for the dissemination of research results will raise public awareness and attract new entrants into the manufacturing and robotics industries, creating truly synergetic education opportunities in science, technology, engineering and mathematics, as well as accelerating the adoption of smart factory-enabling technologies.  <br/><br/>The project will address fundamental challenges in human-robot collaboration in the manufacturing environment, such as the limitation of one-to-one sensing between humans and robots, the lack of adaptive and stochastic modeling methods for reliable recognition and prediction of human actions and motions in different manufacturing scenarios, and multi-scale human-robot coordination. To address these challenges, multi-disciplinary research involving sensing, machine learning, stochastic modeling, robot path planning, and advanced manufacturing will be performed.  Specific tasks include algorithm development and deployment on lab-scale and real-world test beds to: (1) sense and recognize where objects (e.g., robots, humans, parts or tools) are located and what each worker is doing; (2) predict what the next human action will be; and (3) plan and control safe and optimal robot trajectories for individualized on-the-job assistance for humans, proactively avoiding worker injury. The outcomes from the project will be evaluated on the shop-floor at the collaborating company COsorizio MAcchine Uensili (COMAU) in Michigan, and the Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing of the National Research Council of Italy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821894","Collaborative Research: Multimodal Affective Pedagogical Agents for Different Types of Learners","IIS","Cyberlearn & Future Learn Tech","08/01/2018","07/23/2018","Nicoletta Adamo-Villani","IN","Purdue University","Standard Grant","Amy Baylor","07/31/2021","$498,823.00","Bedrich Benes","nadamovi@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8020","063Z, 8045","$0.00","While most research on embodied pedagogical agents (adaptive virtual agents that guide and mentor learners) explores cognitive features, this project investigates the role of agent affect/emotion. This project examines how the agent's affective state (e.g., seeming interested or concerned) impacts different types of students (e.g., differing by knowledge level, gender, underrepresented group status, interest in STEM fields, and personality profile) when learning from online statistics lessons. The project integrates several areas of research: a) computer graphics research on life-like and believable representation of emotion in embodied agents, b) advanced methods and techniques from artificial intelligence and computer vision for real-time recognition of emotions, c) cognitive psychology research on learning from affective agents, and d) education research on the efficacy of affective agents for improving student learning of STEM concepts. Through experimental research the project will advance the state of the art in agent design and implementation by integrating findings on effective emotion regulation with algorithms that support life-like expression of emotions in embodied agents. <br/><br/>To investigate the multimodal design features of affective pedagogical agents, the project has two main objectives: (1) research and develop novel algorithms for emotion recognition and for life-like emotion representation in embodied animated agents, and (2) develop an empirically grounded research base to guide the design of affective pedagogical agents for different types of learners. In one series of experiments the project will determine evidence-based design principles to guide the development of agents that demonstrate emotion/affect, including which kinds of affective states are most effective for which kinds of learners. In a second series of experiments, the project will implement a web-camera system to detect the emotional state of the learner (e.g., confused, interested, content, or bored), adapting the emotional state displayed by the agent in response. Of interest is whether students learn the statistics lesson better when the pedagogical agent is sensitive to the learner's emotional state than when it is not. In addition to its scientific merit, the project will develop and make available a toolkit of affective animated pedagogical agents that adapt to learner characteristics to be used by learners of all ages, for education and training in a variety of subject matters and settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821833","Collaborative Research: Multimodal Affective Pedagogical Agents for Different Types of Learners","IIS","Cyberlearn & Future Learn Tech","08/01/2018","07/23/2018","Richard Mayer","CA","University of California-Santa Barbara","Standard Grant","Amy Baylor","07/31/2021","$250,000.00","","mayer@psych.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","8020","063Z, 8045","$0.00","While most research on embodied pedagogical agents (adaptive virtual agents that guide and mentor learners) explores cognitive features, this project investigates the role of agent affect/emotion. This project examines how the agent's affective state (e.g., seeming interested or concerned) impacts different types of students (e.g., differing by knowledge level, gender, underrepresented group status, interest in STEM fields, and personality profile) when learning from online statistics lessons. The project integrates several areas of research: a) computer graphics research on life-like and believable representation of emotion in embodied agents, b) advanced methods and techniques from artificial intelligence and computer vision for real-time recognition of emotions, c) cognitive psychology research on learning from affective agents, and d) education research on the efficacy of affective agents for improving student learning of STEM concepts. Through experimental research the project will advance the state of the art in agent design and implementation by integrating findings on effective emotion regulation with algorithms that support life-like expression of emotions in embodied agents. <br/><br/>To investigate the multimodal design features of affective pedagogical agents, the project has two main objectives: (1) research and develop novel algorithms for emotion recognition and for life-like emotion representation in embodied animated agents, and (2) develop an empirically grounded research base to guide the design of affective pedagogical agents for different types of learners. In one series of experiments the project will determine evidence-based design principles to guide the development of agents that demonstrate emotion/affect, including which kinds of affective states are most effective for which kinds of learners. In a second series of experiments, the project will implement a web-camera system to detect the emotional state of the learner (e.g., confused, interested, content, or bored), adapting the emotional state displayed by the agent in response. Of interest is whether students learn the statistics lesson better when the pedagogical agent is sensitive to the learner's emotional state than when it is not. In addition to its scientific merit, the project will develop and make available a toolkit of affective animated pedagogical agents that adapt to learner characteristics to be used by learners of all ages, for education and training in a variety of subject matters and settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810005","Synaptic dynamics in ferroelectric devices and their application to deep neural networks","ECCS","EPMD-ElectrnPhoton&MagnDevices","08/15/2018","06/22/2019","Asif Khan","GA","Georgia Tech Research Corporation","Standard Grant","Paul Lane","07/31/2021","$458,000.00","Saibal Mukhopadhyay","asif.khan@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1517","094E, 100E, 9102, 9251","$0.00","Nontechnical:<br/>One of the grand visions of the modern computing era has been to mimic the cognitive capabilities of the human-brain, and even to rival them. This vision is becoming possible due to recent advances in machine learning and artificial intelligence. Current computing technologies are still far from creating a digital entity that is as capable and as energy efficient as a biological brain. Digital learning systems are notoriously power hungry and can require a room-full of digital computer clusters. Compare that to the human brain which performs all its feats at a meager power budget of twenty watts and a weight of less than two kilograms. One reason for this inefficiency is that transistors, the basic building blocks of digital computers, do not function in the same way as synapses, the basis of biological computing. The proposed research aims at overcoming this barrier by making a relatively basic change to the structure of the transistor. An emerging material with ferroelectric properties, doped hafnium oxide, will be introduced into transistors. The new device is called a ferroelectric field effect transistor and can emulate the properties of biological synapses. In this project, the unique properties of the synaptic ferroelectric transistor will be used to design and optimize artificial intelligence cores such as deep neural networks that vastly exceed the performance and efficiency of the current state-of-the-art. The project will train participating students in an interdisciplinary setting that involves material science, circuit design, computer architecture, and neuro-science. The STEM outreach and education programs will help participating undergraduates, high school students and high school teachers to broaden their experience in computer science and novel semiconductor devices.<br/><br/>Technical:<br/>The project will explore the rich domain dynamics in ferroelectric hafnia-zirconia alloy gated silicon transistors to build synaptic units for vector matrix multiplication crossbars. The architecture and system level work will entail the design and optimization of full-blown deep neural networks based on these ferroelectric crossbar kernels. Physics based compact models of ferroelectric transistors that account for the important details of domain dynamics will be developed which will tie the material-device level work and the architecture-system level work. A key feature of the project is its vertically integrated approach that involves different levels in the computing hierarchy from materials to systems. Innovations at all these different levels will ensure that the interesting properties of the emerging ferroelectric device technology can be fully leveraged to create an energy efficient and high-performance hardware platform for advanced machine learning and data intensive cognitive applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824198","Research Coordination Network: Cognitive Functions in the Learning of Symbolic Signals & Systems","BCS","Science of Learning, M3X - Mind, Machine, and Motor","09/01/2018","07/19/2019","Cornelia Fermuller","MD","University of Maryland College Park","Standard Grant","Soo-Siang Lim","08/31/2021","$516,000.00","Shihab Shamma, Ralph Etienne-Cummings","fer@cfar.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","SBE","004Y, 058Y","059Z, 070E","$0.00","The objective of this Research Coordination Network (RCN) is to advance understanding of how biological systems learn complex symbolic signals, and create artificial systems with similar capabilities. By defining a common framework to describe these signals, and their variability across space and time, the RCN will develop methods and tools applicable to a wide range of domains, including language, music, action, perception, and navigation. The RCN will build upon research in Neuromorphic Engineering and its development of bio-inspired, low-power computing platforms, sensors, and signal processing. Using these tools, the RCN will focus on high-level cognitive functions, to create complex, bio-inspired systems that learn through engagement in tasks. The network will bring together neuroscience, cognitive science, applied mathematics, computer science, and engineering, with emphasis on machine learning and artificial intelligence. Network members will participate in a yearly three-week, hands-on workshop, that will develop and test new tools and ideas, stimulate new collaborations, and educate students on unique interdisciplinary skills. <br/><br/>The RCN will facilitate interactions and collaborative projects among participating researchers employing a wide range of paradigms that specifically deal with three thrusts: the role of neural plasticity for learning symbolic systems; the adaptive mechanisms underlying the learning of sensory-motor tasks; and transitioning to real-world applications such as automatic speech and dynamic scene understanding, neuromorphic hardware implementations, cognitive computational algorithms, and databases acquisition. Specific examples of such diverse projects include brain process models that assess learning and expertise; algorithms, based on physiological or abstract events, that process input from neuromorphic hardware; and development of software and neuromorphic hardware for signal interpretation and action execution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815358","III: Small: Collaborative Research: Explainable Natural Language Inference","IIS","Info Integration & Informatics","09/01/2018","06/26/2020","Niranjan Balasubramanian","NY","SUNY at Stony Brook","Standard Grant","Maria Zemankova","08/31/2021","$252,537.00","","niranjan@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7364","075Z, 7364, 7923, 9251","$0.00","Natural language inference (NLI) can support decision-making using information contained in natural language texts (e.g, detecting undiagnosed medical conditions in medical records, finding alternate treatments from scientific literature). This requires gathering facts extracted from text and reasoning over them. Current automated solutions for NLI are largely incapable of producing explanations for their inferences, but this capacity is essential for users to trust their reasoning in domains such as scientific discovery and medicine where the cost of making errors is high. This project develops natural language inference methods that are both accurate and explainable. They are accurate because they build on state-of-the-art deep learning frameworks which use powerful, automatically learned, representations of text. They are explainable because they aggregate information in units that can be represented in both a human readable explanation and a machine-usable vector representation. This project will advance methods in explainable natural language inference to enable the application of automated inference methods in critical domains such as medical knowledge extraction. The project will also evaluate the explainability of the inference decisions in collaboration with domain experts.<br/><br/>This project reframes natural language inference as the task of constructing and reasoning over explanations. In particular, inference assembles smaller component facts into a graph (explanation graph) that it reasons over to make decisions. In this view, generating explanations is an integral part of the inference process and not a separate post-hoc mechanism. The project has three main goals: (a) Develop multiagent reinforcement learning models that can effectively and efficiently explore the space of explanation graphs, (b) Develop deep learning based aggregation mechanisms that can prevent inference from combining semantically incompatible evidence, and (c) Build a continuum of hypergraph based text representations that combine discrete forms of structured knowledge with their continuous embedding based representations. The techniques will be evaluated on three application domains: complex question answering, medical relation extraction, and clinical event detection from medical records. The results of the project will be disseminated through the project website, scholarly venues, and the software and datasets will be made available to the public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838730","SCH: INT: Collaborative Research: Data-driven Stratification and  Prognosis for Traumatic Brain Injury","IIS","Smart and Connected Health","09/15/2018","09/06/2018","Chandan Reddy","VA","Virginia Polytechnic Institute and State University","Standard Grant","Wendy Nilsen","08/31/2022","$695,600.00","Brandon Foreman","reddy@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8018","075Z, 8018","$0.00","Traumatic Brain Injury (TBI) is a global health problem affecting over 10 million people worldwide and is a leading cause of death and disability among children and young adults in the United States. While the understanding of biological mechanisms related to acquired brain injuries has improved significantly in the past two decades, none of these advances have translated to a successful clinical trial and therefore, there has been no substantial improvement in treating such critical conditions. The heterogeneity of TBI and the ability to reliably stratify critically-ill patients who will likely have better outcomes for a certain intervention are amongst the major challenges in clinical research. To address these challenges, this project develops a comprehensive set of machine learning methods that can be broadly applied to a variety of problems. Data sources include both in-patient bedside data as well as remotely monitored telemedicine data, thus connecting data at multiple levels for specific patient populations. This research is crucial to support the development of pilot computational models for stratification of critical care patients and potentially inform ways to reduce the overall healthcare and societal costs for this patient population.<br/><br/>The project aims to develop novel computational algorithms for reliably stratifying brain injury patients and predicting their short-term and long-term outcomes from multi-modal physiologic and clinical data. Specifically, the research objectives of this project are: (i) Develop a scalable and effective algorithm for personalized subgroup identification for any given patient using an efficient subcluster model that groups patients using only a subset of coherently relevant variables. Discriminative subspace models will also be built to distinguish subgroups of patients. (ii) Propose a new machine learning paradigm called 'Label-Bag learning' to identify and predict changes in TBI Patients. The goal of label-bag learning is to learn a group of labels and their corresponding outcome variable in the data. The project includes a new framework based on Bayesian correlations that can adaptively transform any existing machine learning algorithm and implicitly handle this label-bag problem formulation through constrained modeling. (iii) Develop a novel approach to long-term outcome prediction through differential subset modeling framework. Through outreach and educational activities, the project will promote computational and systems thinking among high school, undergraduate, and graduate students along with clinical trainees. Methods developed in this project will be integrated into courses and tutorials that have both computational and biomedical emphases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838745","SCH: INT: Collaborative Research: Data-driven Stratification and Prognosis for Traumatic Brain Injury","IIS","Smart and Connected Health","09/15/2018","09/06/2018","Vignesh Subbian","AZ","University of Arizona","Standard Grant","Wendy Nilsen","08/31/2022","$503,567.00","Jonathan Ratcliff","vsubbian@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8018","075Z, 8018","$0.00","Traumatic Brain Injury (TBI) is a global health problem affecting over 10 million people worldwide and is a leading cause of death and disability among children and young adults in the United States. While the understanding of biological mechanisms related to acquired brain injuries has improved significantly in the past two decades, none of these advances have translated to a successful clinical trial and therefore, there has been no substantial improvement in treating such critical conditions. The heterogeneity of TBI and the ability to reliably stratify critically-ill patients who will likely have better outcomes for a certain intervention are amongst the major challenges in clinical research. To address these challenges, this project develops a comprehensive set of machine learning methods that can be broadly applied to a variety of problems. Data sources include both in-patient bedside data as well as remotely monitored telemedicine data, thus connecting data at multiple levels for specific patient populations. This research is crucial to support the development of pilot computational models for stratification of critical care patients and potentially inform ways to reduce the overall healthcare and societal costs for this patient population.<br/><br/>The project aims to develop novel computational algorithms for reliably stratifying brain injury patients and predicting their short-term and long-term outcomes from multi-modal physiologic and clinical data. Specifically, the research objectives of this project are: (i) Develop a scalable and effective algorithm for personalized subgroup identification for any given patient using an efficient subcluster model that groups patients using only a subset of coherently relevant variables. Discriminative subspace models will also be built to distinguish subgroups of patients. (ii) Propose a new machine learning paradigm called 'Label-Bag learning' to identify and predict changes in TBI Patients. The goal of label-bag learning is to learn a group of labels and their corresponding outcome variable in the data. The project includes a new framework based on Bayesian correlations that can adaptively transform any existing machine learning algorithm and implicitly handle this label-bag problem formulation through constrained modeling. (iii) Develop a novel approach to long-term outcome prediction through differential subset modeling framework. Through outreach and educational activities, the project will promote computational and systems thinking among high school, undergraduate, and graduate students along with clinical trainees. Methods developed in this project will be integrated into courses and tutorials that have both computational and biomedical emphases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808898","Collaborative Research: Reinforcement learning based adaptive optimal control of powered knee prosthesis for human users in real life","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2018","08/14/2018","He Huang","NC","North Carolina State University","Standard Grant","Anthony Kuh","07/31/2021","$149,075.00","","hhuang11@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","ENG","7607","1653","$0.00","The proposed research aims at designing robust, real time learning controllers for powered lower limb prosthesis worn by above-knee amputees. It centers on adaptive optimal tuning of prosthetic knee joint impedance parameters with an ultimate goal of achieving human-prosthesis symbiosis. Current state-of-the-art approaches rely on a predetermined collection of knee joint impedance parameters, resulted from tedious manual tuning in a clinic. In addition to a lack of adaptability to different users, current impedance controls do not adapt to different use environments. One of the key design challenge is due to the constant interaction between the human user and the robotic leg. As such, advanced robotics including those employing latest artificial intelligence technologies, control system theory and design, and existing biomechanics based controls cannot meet the needs of real time learning control of a powered prosthetic leg in a human-prosthesis system. Given the nature of the problem, reinforcement learning based adaptive optimal control, also referred to as adaptive dynamic programming (ADP), holds great promise to delivering the next generation of prosthesis control solutions. <br/><br/>Intellectual Merit: The design challenge requires innovative approaches of real time reinforcement learning control. The learning controller has to be designed without knowing an explicit dynamic system model describing the human-prosthesis system, while assuring human user safety and system stability, and being scalable and adaptable to different users and use conditions. Putting it all together, the success of this project will be an important milestone for machine learning, control engineering, and rehabilitation engineering. <br/><br/>Broader Impacts: This research has a direct impact on improving the lives of above-knee amputees. Also of great societal impact is the potential of reducing health care cost. New knowledge gained from human-robot interaction will not only aid amputees but also stroke patients who use exoskeleton as assistive devices. The proposed research will also benefit several research communities such as wearable robots, machine learning, and rehabilitation to develop new technologies addressing real applications. To excite and educate future leaders and researchers in science and engineering, the project will provide an opportunity for integration of our research work into graduate education and postdoc training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808692","Model Reduction of High Dimensional Hidden Markov Models and Markov Decision Processes","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2018","08/21/2018","Munther Dahleh","MA","Massachusetts Institute of Technology","Standard Grant","Anthony Kuh","08/31/2021","$360,000.00","Yury Polyanskiy","dahleh@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","7607","1653","$0.00","Intellectual Merit: We currently live in an era where data is a major currency promising a transformative change to our society. Consequently, there has been a surge in the use of machine learning (ML) algorithms on high dimensional data producing unstructured stochastic models. Such models tend to be of very high dimensions limiting their utility in various applications involving optimization or decision systems. This proposal focuses on developing a foundational theory for model reduction applied to classes of stochastic models, in particular, Hidden Markov Models (HMMs); these are stochastic models that are described by underlying finite dimensional state space. <br/><br/>Broader Impact: Ultimately, a model reduction theory will impact many fundamental aspects related to complex stochastic models including simulation, prediction, coding, robust learning, decision design and reinforcement learning. This research will develop new insights to address similar questions for other stochastic models including jump linear systems, and graphical models with latent variables and will have a direct impact on problems related to artificial intelligence and reinforcement learning. The latter is emerging as a popular approach for many decision-systems applications involving social behavior-- where simple mechanistic models do not exist. Examples of such problems are critical infrastructures and smart services where high dimensional unstructured data is available in real time. Models emerging in such approaches tend to have very high dimensions. <br/><br/>A foundational theory for model reduction will affect the way we learn and utilize complex stochastic models. As a result, this development will enter our courses at MIT in a fashion similar to how model reduction theory impacted courses in linear system theory. The development should affect classes in stochastic models, machine learning, and statistical learning theory, reinforcement learning, and AI.  We also intend to incorporate the connection between model reduction and statistical learning in our new MIT micromasters in statistics and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840458","Planning Grant: Engineering Research Center for Safe and Secure Artificial Intelligence Solutions (SAIS)","EEC","ERC-Eng Research Centers","09/01/2018","08/31/2018","Hongyi Wu","VA","Old Dominion University Research Foundation","Standard Grant","Sarit Bhaduri","08/31/2020","$100,000.00","Cong Wang, Brian Payne, Chunsheng Xin","h1wu@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","ENG","1480","112E, 132E","$0.00","The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program.  Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>The planning grant enables the principal investigators (PIs) to work with a full spectrum of stakeholders to prepare the groundwork for establishing an Engineering Research Center (ERC) for Safe and Secure Artificial Intelligence Solutions (SAIS). The proposed center focuses on development of fundamental, theoretically-grounded, and systematic approaches to enable safe and secure AI and establishment of an interdisciplinary community that engages all stakeholders. Scholars from multiple disciplines and practitioners from multiple fields will work together to fully understand the problems and explore the design space. The success of this endeavor can potentially lead to enabling technologies for secure machine learning systems, thereby accelerating their development and widening their adoption in different application domains. Regional educators and government/industrial employers will be engaged to train cybersecurity workforce. Overall the proposed ERC SAIS contributes significantly to the protection of future cyber and physical world and safeguarding the human society. <br/><br/>The PIs will identify and engage the SAIS stakeholder community, including partners and participants from academics, government and military agencies, research centers, and various industries. The interdisciplinary team will plan a long-term vision and direction for SAIS and identify key research and development thrusts, based on the inputs from all stakeholders. The team will develop marketing plan, future member recruiting plan, and cooperative membership agreement (including membership eligibility, commitment, and intellectual property ownership). The team will decide the center's administration structure and management policies and guidelines. The team also will identify a set of research, education, workforce development, and outreach projects to be carried out during the first two years should the center be established. Finally, the team will create plans to work with university admission and K-12 educators to inspire and improve the participation of underrepresented groups in the proposed ERC SIAS and introduce cybersecurity to school counselors, teachers, students, and parents.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755898","CRII: RI: Learning a Timely Semantic Resource from Social Media Data","IIS","Robust Intelligence","06/01/2018","02/19/2020","Wei Xu","OH","Ohio State University","Standard Grant","Tatiana Korelsky","08/31/2020","$183,000.00","","xu.1265@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 8228, 9251","$0.00","One key challenge in text mining and natural language processing research is that a single meaning can be expressed in many different ways, i.e., paraphrases. There has been steady progress towards large paraphrase resources, and a significant increase in its applications: from information retrieval, information extraction, and natural language generation to IBM's Watson, Google's Knowledge Graph, and many more. This research aims to create better paraphrase acquisition techniques and larger scale semantic resources, which could be of great use in various natural language processing tasks and social media data analytics in social science, national security, and other related fields. One example of potential applications is text simplification, which automatically rephrases complex texts into simpler language for children or people with reading disabilities.<br/><br/>The technical innovation of this study focuses on joint modeling of word- and phrase-level alignments between sentence pairs to address the challenges of extracting semantic knowledge from informal data sources (such as social media), which exist in very large quantities rather than just formal sources, such as newswire as per previous work. The model design extends multiple instance learning via two methods, a graphical model and neural network, and can flexibly permit the exploration of different assumptions and models the importance of words or phrases. The modeling advancements can be generalized to other natural language understanding tasks, which require analyzing sentences based on word-level composition or word meaning in a given context, and natural language generation tasks that benefit from learning what words and phrases to remove or rephrase.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824119","I-Corps:  Machine Learning Approach for Microbial Process Control and Management","IIP","I-Corps","04/01/2018","03/11/2020","Hong Liu","OR","Oregon State University","Standard Grant","Andre Marshall","10/31/2020","$50,000.00","","liuh@engr.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will directly affect the treatment and management of the over 80 km3 of wastewater and solid wastes produced each year in the US. Waste treatment processes are directly tied to environmental and human health in both developed and developing countries but are subject to high costs due to significant energy and maintenance requirements. Municipal wastewater treatment alone accounts for about 3% of electrical energy consumed in the U.S and other developed countries. The successful implementation of the proposed technology, which integrates artificial intelligence (AI) into existing waste treatment infrastructure, has the potential to greatly improve the management of microbial communities associated with treatment processes thereby improving overall effectiveness and sustainability. This solution represents a new way for customers to cut energy and operational costs and improve effectiveness of their treatment processes without large investments in infrastructure. Potential markets for this technology include public, municipal facilities in addition to treatment facilities in the industrial/agricultural sector.  Development of the proposed technology will likely spur additional applications of AI systems in other fields centered around microbial community based engineered systems like bioproduct production, biosensing, and bioremediation.<br/><br/>This I-Corps project is based on our recent development of a machine learning based approach used to accurately predict microbial community structure, process stability, and reactor performance for wastewater treatment. This novel approach, which incorporates genomic data along with environmental and operational parameters into data-mining datasets has demonstrated significant increases in accurately predicting process stability and performance of small-scale wastewater systems compared to models developed without consideration of microbial community dynamics. The predictive models developed through the construction of artificial neural networks have potential to inform engineering decisions for optimized performance and stability of environmental biotechnologies in full-scale systems. Development and implementation of this approach will not only progress the understanding and control of microbial communities that inhabit environmental biotechnologies but may be expanded to other microbiomes such as those associated with human health and biogeochemical cycles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815899","SHF: Small: Energy Efficient Learning on Chip with Quantized Representations","CCF","Software & Hardware Foundation","10/01/2018","07/08/2019","Diana Marculescu","PA","Carnegie-Mellon University","Standard Grant","Sankar Basu","09/30/2021","$466,000.00","Ronald Blanton","dianam@utexas.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","075Z, 7923, 7945, 9102, 9251","$0.00","Machine learning models, especially deep neural networks, have been adopted in many real-time applications, such as speech and image recognition or object detection. These applications typically must be fast and energy efficient so they are usable in the field. This project develops quantized representations and algorithmic transformations for neural networks that represent large models with significantly less storage, energy and area, and with little or no accuracy degradation - in other words, enabling the use of efficient neural networks on mobile devices. The results of this project are poised to change how designers approach modeling, analysis, and optimization methodologies for large-scale deep neural networks, and more generally, any statistical learning applications that rely on expensive compute operations, with large memory footprint. The project will have not only a technical impact, but also an important educational and mentoring component by potentially changing how engineers are trained in a multidisciplinary fashion for dealing with next generation technological advances in general, and the problem of energy efficient machine learning in silicon in particular. The extensive experience of the Carnegie Mellon team in outreach to underrepresented groups involving training a diverse student body will be leveraged in this work, while further expanding the project's outreach to high-school and middle-school students.<br/><br/>This project exploits the fact that using a quantized representation for on-chip training and inference can reduce the energy consumption and storage requirements of the associated hardware implementation. This is a crucial feature in achieving higher throughput and lower latency for real-time learning systems. This project addresses these challenges by developing novel quantization approaches for machine learning models that reduce both data movement and computation, and therefore reduce overall energy consumption. Furthermore, the research pursued herein relies on new algorithmic approaches for training quantized learning models so as to accelerate their training process without harming their accuracy, and learning quantized models on hardware (i.e., field programmable gate arrays) to verify their benefits and accelerate their adoption.  To demonstrate feasibility, the project features a hardware accelerator-based testbed for inference and training models in a quantized fashion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747043","SBIR Phase I:  Mia Learning Independent Reading Choice Support System","IIP","SBIR Phase I","01/01/2018","10/15/2018","Darren Cambridge","DC","Mia Learning LLC","Standard Grant","Rajesh Mehta","01/31/2019","$225,000.00","","darren@mialearning.com","1140 Third St NE","Washington","DC","200023406","2022705224","ENG","5371","5371, 8031","$0.00","This Small Business Innovation Research Phase I project will build children's motivation to read and help them access books best suited to their individual interests, purposes, and abilities. Even though robust research demonstrates both intrinsic motivation to read and print book ownership strongly shape reading achievement, few existing educational software products address them. The project will develop a voice chatbot app elementary students will use in class to receive personalized book recommendations and coaching on choosing well. An associated book subscription service will allow kids to own books they choose, including in very low income schools through partnerships with non-profits. Together, these offerings will tap a combined U.S. children's book and literacy educational software market for grades 2-5 that tops $1.4 billion dollars annually. Unlike most other ""personalized"" or ""adaptive"" learning systems, the app will use machine learning and artificial intelligence to increase the agency of students and teachers. It will focus on helping students improve their ability to make their own choices rather than making those choices for them.  <br/><br/>The intellectual merit of this project lies in its innovative combination of a recommender system and a pedagogical agent to simultaneously assist students in completing an authentic task (choosing books to read independently) quickly and well while also teaching them to complete the task increasingly effectively and independently over multiple performances. The voice conversational interface will provide this combined task support and coaching through an emotionally engaging narrative experience accessible to struggling readers. The research will yield a field-tested prototype of the system, constructing a domain model, authoring conversational content, developing machine learning technology, and iteratively improving the system through usability and pilot testing in elementary school classrooms. Technical challenges include tuning automated voice recognition in naturalistic classroom environments, overcoming the cold start problem to generate high quality initial recommendations, and supporting acquisition of both cognitive and metacognitive skills within an ill-structured domain where measurement of successful performance has complex dependencies with student identity and social context."
"1840052","FW-HTF:  Collaborative Research:  Augmenting and Advancing Cognitive Performance of Control Room Operators for Power Grid Resiliency","CNS","FW-HTF-Adv Cogn & Phys Capblty","10/01/2018","09/17/2018","Gautam Biswas","TN","Vanderbilt University","Standard Grant","David Corman","09/30/2023","$323,081.00","Abhishek Dubey","gautam.biswas@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","082Y","","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by the National Science Foundation. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. Effective decision making by power grid operators in extreme events (e.g., Hurricane Maria in Puerto Rico, the Ukraine cyber attack) depends on two factors: operator knowledge acquired through training and experience, and appropriate decision support tools. Decision making in electric grid operation during extreme adverse events directly impacts the life of citizens. This project will augment the cognitive performance of human operators with new, human-focused decision support tools and better, data-driven training for managing the grid especially under highly disruptive conditions. The development of new generation of tools for online knowledge fusion, event detection, cyber-physical-human analysis in operational environment can be applied during extreme events and provide energy to critical facilities like hospitals, city halls and essential infrastructure to keep citizens safe and avoid economic loss for the Nation. Higher performance of operators will improve worker quality of life and will enhance the economic and social well-being of the country. The project's training objectives will leverage existing educational efforts and outreach activities and we will publicize the multidisciplinary outcomes through multiple venues.<br/><br/><br/>The proposed project will integrate principles from cognitive neuroscience, artificial intelligence, machine learning, data science, cybersecurity, and power engineering to augment power grid operators for better performance. Two key parameters influencing human performance from the dynamic attentional control (DAC) framework are working memory (WM) capacity, the ability to maintain information in the focus of attention, and cognitive flexibility (CF), the ability to use feedback to redirect decision making given fast changing system scenarios. The project will achieve its goals through analyzing WM and CF and performance of power grid operators during extreme events; augmenting cognitive performance through advanced machine learning based decision support tools and adaptive human-machine system; and developing theory-driven training simulators for advancing cognitive performance of human operators for enhanced grid resilience. A new set of algorithms have been proposed for data-driven event detection, anomaly flag processing, root cause analysis and decision support using Tree Augmented naive Bayesian Net (TAN) structure, Minimum Weighted Spanning Tree (MWST) using the Mutual Information (MI) metric, and unsupervised learning improved for online learning and decision making. Additionally, visualization tools have been proposed using cognitive factor analysis and human error analysis. We propose a training process driven by cognitive and physiometric analysis and inspired by our experience in operators training in multiple domain: the power grid, aircraft and spacecraft flight simulators. A systematic approach for human operator decision making is proposed using quantifiable human and engineering analysis indices for power grid resiliency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1850040","I-Corps: A Wearable, Biomarker-Tracking Device Platform Using Machine Learning and Predictive Technology for Positive Behavior Change","IIP","I-Corps","09/15/2018","09/12/2018","Christina Mair","PA","University of Pittsburgh","Standard Grant","Andre Marshall","02/29/2020","$50,000.00","","cmair@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project revolves around the human brain?s proclivity towards substances detrimental to human health and wellbeing, such as dangerous amounts of sugar, salt, and drugs, which is causing global health and safety risks. Inability to avoid these temptations shortens lifespans, increases healthcare costs, and strains resources. This team will use machine learning and pattern recognition AI to identify and react to factors that result in destructive human behaviors. While the initial intent of our product is to provide potential customers with a tool to detect and prevent addiction overdose and relapse, offer interventions of supervised individuals, conserve emergency response resources, and save taxpayer money, this unsupervised learning AI could, in subsequent iterations, be used to identify and react to any behavior -- such as unsafe driving, binge eating, or anger management. Helping people identify the precursors to their behaviors could serve as a type of biofeedback that gives them and their support networks timely and individualized insights, preventions, and interventions -- some of which could be implemented by Internet of Things enabled devices (e.g. locking a car?s ignition; calling a sponsor; playing calm music; etc.), resulting in cost and health benefits across many populations.<br/><br/>This I-Corps project is focused on a patent-pending platform to predict and prevent addiction relapses and overdoses by treating those in recovery with timely interventions using wearable devices and artificial intelligence.  Detecting craving states is supported by past and current research, and there is scientific justification for gathering smartphone usage and biometric wearable data on drug users. Researchers have used the combination of smartphone usage and wearables data to predict binge drinking behavior and we plan to apply this research to opioid use. Additionally, cocaine and opioid research uses the same wearable device as our studies. We expand upon current research by combining just-in-time smartphone-based interventions for people in recovery for opioid use disorders with pattern-detection predictive models that will be trained at the population level and refined at the individual level. We will gather data on all relevant factors that determine drug relapse risk state: physiology, smartphone usage, location data, self-reports, and support-network-reports. Once a high-risk state is detected, a smartphone app-based intervention focused on behavior change will then be implemented in a timely and customizable manner.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840192","FW-HTF:  Collaborative Research:  Augmenting and Advancing Cognitive Performance of Control Room Operators for Power Grid Resiliency","CNS","FW-HTF-Adv Cogn & Phys Capblty, Special Projects - CNS","10/01/2018","03/30/2020","Anurag Srivastava","WA","Washington State University","Standard Grant","David Corman","09/30/2023","$1,394,337.00","Paul Whitney, Anjan Bose, Adam Hahn, Saeed Lotfifard","asrivast@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","082Y, 1714","063Z, 9251","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by the National Science Foundation. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim.  Effective decision making by power grid operators in extreme events (e.g., Hurricane Maria in Puerto Rico, the Ukraine cyber attack) depends on two factors: operator knowledge acquired through training and experience, and appropriate decision support tools. Decision making in electric grid operation during extreme adverse events directly impacts the life of citizens. This project will augment the cognitive performance of human operators with new, human-focused decision support tools and better, data-driven training for managing the grid especially under highly disruptive conditions. The development of new generation of tools for online knowledge fusion, event detection, cyber-physical-human analysis in operational environment can be applied during extreme events and provide energy to critical facilities like hospitals, city halls and essential infrastructure to keep citizens safe and avoid economic loss for the Nation. Higher performance of operators will improve worker quality of life and will enhance the economic and social well-being of the country. The project's training objectives will leverage existing educational efforts and outreach activities and we will publicize the multidisciplinary outcomes through multiple venues.<br/><br/><br/>The proposed project will integrate principles from cognitive neuroscience, artificial intelligence, machine learning, data science, cybersecurity, and power engineering to augment power grid operators for better performance. Two key parameters influencing human performance from the dynamic attentional control (DAC) framework are working memory (WM) capacity, the ability to maintain information in the focus of attention,  and  cognitive flexibility (CF), the ability to use feedback to redirect decision making given fast changing system scenarios. The project will achieve its goals through analyzing WM and CF and performance of power grid operators during extreme events; augmenting cognitive performance through advanced machine learning based decision support tools and adaptive human-machine system; and developing theory-driven training simulators for advancing cognitive performance of human operators for enhanced grid resilience. A new set of algorithms have been proposed for data-driven event detection, anomaly flag processing, root cause analysis and decision support using Tree Augmented naive Bayesian Net (TAN) structure, Minimum Weighted Spanning Tree (MWST) using the Mutual Information (MI) metric, and unsupervised learning improved for online learning and decision making.  Additionally, visualization tools have been proposed using cognitive factor analysis and human error analysis. We propose a training process driven by cognitive and physiometric analysis and inspired by our experience in operators training in multiple domain: the power grid, aircraft and spacecraft flight simulators. A systematic approach for human operator decision making is proposed using quantifiable human and engineering analysis indices for power grid resiliency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830965","SBIR Phase II:  Fast Creation of Photorealistic 3D Models using Consumer Hardware","IIP","SBIR Phase II","09/01/2018","09/04/2018","Jeevan Kalanithi","CA","Openspace","Standard Grant","Peter Atherton","08/31/2020","$750,000.00","","zoinks@gmail.com","3802 23rd St","San Francisco","CA","941143321","4159947035","ENG","5373","5373, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be substantial: a successful project would transform the construction industry, making it far more efficient by reducing legal conflicts, schedule slips and poor decision making. The proposed work will enable the fast and easy creation of 100% complete visual documentation of a physical space; this documentation can be generated many times throughout the course of construction. In so doing, the proposed project will allow professionals in the construction industry to track progress and communicate with their teams far more efficiently than ever before. A second outcome of the project will be the creation of vast, detailed, never before seen datasets of construction projects and real estate, allowing technical innovations in artificial intelligence and computer vision to impact one of the largest industries in the nation and the world. For example, systems could be trained to automatically spot safety concerns, augmenting the efforts of safety managers and keeping workers safer than ever before.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project will develop a fast, easy to use and cheap method to create photorealistic immersive models using off the shelf consumer hardware. Technical hurdles include validating the quality and efficacy of models generated with consumer hardware and automatic creation of routes through the 3D space without human annotation. Technical milestones involve using various sensor streams as well as other prior data to build these routes. With these hurdles cleared, advanced work may include automated analytics between and among 3D models of the same site captured over time. Because of the system's ease of use, it will enable the collection of large, totally novel datasets. The goal of the research is to produce a prototype that a layperson can use to create an immersive model of a physical site in order to document it with no annotation effort. The plan to reach these goals includes iterative software development against the hurdles listed above, as well as continuous user feedback to guide and refine development.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763618","RI: Medium: Extreme Clustering","IIS","Robust Intelligence","09/01/2018","08/23/2018","Andrew McCallum","MA","University of Massachusetts Amherst","Standard Grant","Rebecca Hwa","08/31/2022","$1,103,937.00","Akshay Krishnamurthy","mccallum@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7495","075Z, 7495, 7924","$0.00","Clustering is a fundamental tool for data science, hypothesis discovery, pattern discovery, and information integration.  Given a collection of objects, clustering is the task of automatically grouping the objects so that objects within a group (called a cluster) are more similar to each other than to objects in other clusters.  Clustering is widely used in medicine, engineering, science and commerce. Most modern clustering methods scale well to a large number of objects, but not to a large number of clusters.  Furthermore currently widely used clustering methods excessively assign objects to clusters, suffer in accuracy, and do not represent uncertainty in the clustering.  All of these weaknesses limit analysis capabilities in many scientific, engineering, and other high-impact applications. This project is developing new machine learning and algorithms for large-scale clustering that scales to both massive number of objects and massive number of clusters.  This project will build on the recent preliminary success with a family of algorithms that build hierarchical clustering, which supports efficient re-assignment of data to new clusters, and which naturally represents uncertainty.  The new research aims to further increase accuracy and scalability. The project team will demonstrate its new research in multiple domains relevant to national priorities, including clustering chemical compounds for material science discovery, clustering single cell genome data, and entity resolution on scientific metadata (such as paper authors, patent authors, papers, institutions, etc)--- creating tools that advance scientific discovery, collaboration and scientific peer review.  All of the software developed as part of this project will be released as open source software in order to facilitate experimentation and adoption of our methods in research and practice. The project team will develop a tutorial at the intersection of machine learning and algorithms, and will additionally teach a course on efficient clustering methods to researchers beyond computer scientists.<br/><br/>This project will develop new research on machine learning and algorithms for hierarchical clustering that scales to both massive number of input objects, N, and massive number of clusters, K---a problem setting termed ""extreme clustering,"" named after its similarly- motivated supervised cousin, ""extreme classification."" The project builds on the successes of recent preliminary work on PERCH, a family of algorithms for large-scale, incremental-data, non-greedy, hierarchical clustering that has achieved remarkable new state-of-the-art results. The method efficiently routes new data points to the leaves of an incrementally-built tree. Motivated by the desire for both accuracy and speed, the approach performs tree rotations both for the sake of enhancing subtree purity and encouraging balanced trees. Experiments demonstrate that PERCH constructs more accurate trees than other tree-building clustering algorithms and scales well with both N and K, achieving a higher quality clustering than the strongest at clustering competitor in nearly half the time. The project will perform new research (a) improving flexibility through alternative clustering cost functions and data representations, (b) further improving scalability and accuracy through new tree routing functions, (c) developing new tree-cut methods for determining the best clusterings and distributions over clusterings, and (d) inventing new methods for joint clustering of multiple inter-related data instance types. Evaluation and application of the research will be conducted on multiple broad-impact, large-data domains, including biomedicine, material science, image analysis, and scientific information integration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840083","FW-HTF:  Collaborative Research:  Augmenting and Advancing Cognitive Performance of Control Room Operators for Power Grid Resiliency","CNS","FW-HTF-Adv Cogn & Phys Capblty, CPS-Cyber-Physical Systems","10/01/2018","09/17/2018","Alexandra von Meier","CA","University of California-Berkeley","Standard Grant","David Corman","09/30/2023","$299,325.00","","vonmeier@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","082Y, 7918","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by the National Science Foundation. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. Effective decision making by power grid operators in extreme events (e.g., Hurricane Maria in Puerto Rico, the Ukraine cyber attack) depends on two factors: operator knowledge acquired through training and experience, and appropriate decision support tools. Decision making in electric grid operation during extreme adverse events directly impacts the life of citizens. This project will augment the cognitive performance of human operators with new, human-focused decision support tools and better, data-driven training for managing the grid especially under highly disruptive conditions. The development of new generation of tools for online knowledge fusion, event detection, cyber-physical-human analysis in operational environment can be applied during extreme events and provide energy to critical facilities like hospitals, city halls and essential infrastructure to keep citizens safe and avoid economic loss for the Nation. Higher performance of operators will improve worker quality of life and will enhance the economic and social well-being of the country. The project's training objectives will leverage existing educational efforts and outreach activities and we will publicize the multidisciplinary outcomes through multiple venues.<br/><br/><br/>The proposed project will integrate principles from cognitive neuroscience, artificial intelligence, machine learning, data science, cybersecurity, and power engineering to augment power grid operators for better performance. Two key parameters influencing human performance from the dynamic attentional control (DAC) framework are working memory (WM) capacity, the ability to maintain information in the focus of attention, and cognitive flexibility (CF), the ability to use feedback to redirect decision making given fast changing system scenarios. The project will achieve its goals through analyzing WM and CF and performance of power grid operators during extreme events; augmenting cognitive performance through advanced machine learning based decision support tools and adaptive human-machine system; and developing theory-driven training simulators for advancing cognitive performance of human operators for enhanced grid resilience. A new set of algorithms have been proposed for data-driven event detection, anomaly flag processing, root cause analysis and decision support using Tree Augmented naive Bayesian Net (TAN) structure, Minimum Weighted Spanning Tree (MWST) using the Mutual Information (MI) metric, and unsupervised learning improved for online learning and decision making. Additionally, visualization tools have been proposed using cognitive factor analysis and human error analysis. We propose a training process driven by cognitive and physiometric analysis and inspired by our experience in operators training in multiple domain: the power grid, aircraft and spacecraft flight simulators. A systematic approach for human operator decision making is proposed using quantifiable human and engineering analysis indices for power grid resiliency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814557","SaTC: CORE: Small: FIRMA: Personalized Cross-Layer Continuous Authentication","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/15/2018","06/11/2020","Daniela Oliveira","FL","University of Florida","Standard Grant","Nina Amla","08/31/2021","$516,000.00","Dapeng Wu, Natalie Ebner","daniela@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1714, 8060","025Z, 075Z, 7434, 7923, 9102, 9178, 9251","$0.00","An important problem in computer security is verifying that people using computing devices are authorized to use them, not just when they first sign on to the device but during the whole time they are using them.  Most existing continuous authentication schemes impose burdens on users, for instance, when systems quickly log users out and require frequent re-entry of passwords.  This project will build and evaluate FIRMA, a user-transparent, continuous authentication software framework that collects usage data, targeted at corporate security contexts where such monitoring can be done.  To the extent that people have unique but recurrent patterns of use -- itself an interesting research question -- FIRMA can estimate the likelihood that the current user is still an authorized, authenticated user based on how current use patterns compare to historical ones.  Doing this might both reduce the burden of frequent re-authentication and provide early warning signs of malicious activity by malware or insider attacks.  Further, by leveraging the unique way people use computers, FIRMA will be diverse by design -- adversaries will not be able to predict how specific individuals use their devices and their attacks will fail in many devices -- thereby ""herd-protecting"" security by making it difficult for malware to automatically spread across many devices.  If successful, the project could have real impact on corporate security, reducing data breaches and downtime while improving the usability of these systems.  The work will also have educational and training impacts through interdisciplinary collaboration and education between computer engineering and psychology, involvement of undergraduate researchers, and efforts to recruit female and minority students to participate in the project. <br/><br/>FIRMA will be composed of a kernel module, which will continuously record at the operating system level all events related to user activities: user events (mouse clicks, keystrokes, and timestamps), processes, and the files and network events created as a consequence of user-driven activity. These events, recorded during a training period that represents a user's typical computer usage, will be applied to create a user profile using a novel Generative Adversarial Network (GAN)-based deep learning approach called AttenGAN/P-GAN, which will be composed of a user profile generator and a runtime classifier.  AttenGAN/P-GAN will both provide new deep learning tools for processing sequences of unknown length as well as improved ability to train classifiers for anomaly detection without negative samples. The runtime classifier will continuously observe events generated by FIRMA's extractor, leverage the user profile to classify the current window of events being observed as normal or anomalous, and update the current user confidence score. This classifier will be resilient to benign profile changes caused by fluctuations in a user's activity pattern caused by external factors, such as travel (change of time zone) or change of groups or projects. FIRMA's evaluation will comprise four-week captures of natural computer usage data from recruited computer users. This evaluation will consider usability, classification accuracy, and false positives in the presence of various types of anomalies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750978","CAREER: Structured Scientific Evidence Extraction: Models and Corpora","IIS","Info Integration & Informatics","07/01/2018","06/01/2020","Byron Wallace","MA","Northeastern University","Continuing Grant","Maria Zemankova","06/30/2023","$565,933.00","","b.wallace@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7364","1045, 7364, 9251","$0.00","Scientific evidence is primarily disseminated in free-text journal articles. Drawing upon this evidence to make decisions or inform policies therefore requires perusing relevant articles and manually extracting the findings of interest. Unfortunately, this process is time-consuming and has not scaled to meet the demands imposed by the torrential expansion of the scientific evidence base. This work seeks to design novel Natural Language Processing (NLP) methods that can automatically ""read"" and make sense of unstructured published scientific evidence. This is critically important because decisions by policy-makers, care-givers and individuals should be informed by the entirety of the relevant published scientific evidence; but because evidence is predominantly unstructured -- and hence not directly actionable -- this is currently impossible in practice. Consider clinical medicine, an important example which serves as the target domain of this proposal (although the framework and models will generalize to other scientific areas). Roughly 100 articles describing trials were published every single day in 2015. Healthcare professionals cannot possibly make sense of this, and thus treatment decisions must be made without full consideration of the available evidence. Methods that can automatically infer from this torrential mass of unstructured literature which treatments are actually supported by the evidence would facilitate better, evidence-based decisions. Toward this end, this research seeks to design NLP models capable of mapping from natural language scientific articles describing studies or trials to structured ""evidence frames"" that codify the interventions and outcomes studied, and the reported findings concerning these. NLP technology is not presently up to this task. Therefore, this project will support core methodological contributions that will advance systems for data extraction and machine reading of lengthy articles; these will have impact beyond the present motivating application. <br/><br/>From a technical perspective, the focus of this work concerns developing novel, interpretable (transparent) neural network models for extraction from and inference over lengthy articles. Specifically, this project aims to design models that can automatically identify treatments and associated outcomes from free-texts, and then infer the reported comparative effects of the former with respect to the latter. This pushes against limits of existing language technology capabilities. In particular, this necessitates models that perform deep analysis of individual, potentially lengthy, technical documents. Furthermore, model transparency is critical here, as domain experts must be able to recover from where in documents evidential claims were inferred. New corpora curated for this project (to be shared with the broader community) will facilitate core NLP research on such models. To realize the aforementioned methodological aims, the researchers leading this project will develop conditional and dynamic ""attentive"" neural models. Specific methodological lines of research to be explored include: (i) Models equipped with conditional, sparse attention mechanisms over textual units that reflect scientific discourse structure to achieve accurate and transparent extraction of, and inference concerning, reported evidence. (ii) Neural sequence tagging models that take multiple 'reads' of a text, exploiting iteratively adjusted conditional document representations as global context to inform local predictions. A project website (http://www.byronwallace.com/evidence-extraction) provides access to papers, datasets and other project outputs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746232","SBIR Phase I:  A Hybrid Brain-Computer Interface for Virtual and Augmented Reality","IIP","SBIR Phase I","01/01/2018","03/05/2019","Jay Jantz","MA","Neurable Inc.","Standard Grant","Henry Ahn","03/31/2019","$224,915.00","","jayj@neurable.com","25 1st Street","Cambridge","MA","021411802","9173124551","ENG","5371","108E, 5371, 8018, 8042, 8089, 8091","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project addresses the need for non-invasive brain-computer interfaces (BCIs) and hands-free control of technologies, including artificial and virtual reality (AR/VR) and smart devices. The proposed multi-purposed BCI is expected to have immediate applications for several industries, including manufacturing and medicine. Currently, existing systems are either too expensive or limited for real-time control. The proposed BCI is specifically designed for 3 dimensional environments, and is intended to leverage multiple ('hybrid') signals from the human body to allow increased performance using affordable hardware. It is also designed for and expected to allow AR/VR control, which can enable productivity applications, as well as model BCI use in real-world scenarios. The long term goal is to enable users to scroll menus, select objects, and even type using their brain activity. The platform uses the existing form-factor of AR/VR headsets to incorporate brain-sensing electrodes, and will be compatible with popular devices, independently or in parallel with their existing controllers. The electrodes are designed to be safe, non-invasive, and dry (requiring conductive gel or saline). The high-risk, high-reward research to be conducted under this project will significantly advance the applications of BCI systems in general, with an emphasis on AR/VR technologies.<br/><br/>The proposed project concerns a novel hybrid BCI by combining oculomotor and electroencephalography (EEG) signals via a custom machine learning platform. BCIs detect and interpret neural signals enabling control over a variety of technologies. However, current BCIs remain extremely limited in their applicability. They either require expensive equipment, invasive surgery, or have too low performance when using affordable noninvasive hardware. This BCI aims to provide real-time control in 3-dimensional scenarios, (e.g., AR/VR/real-world smart devices), while using affordable hardware. This SBIR Phase I project seeks to combine three distinct innovations: high-performance EEG signal analysis, high-speed eye movement classification, and custom multi-signal ensemble classification techniques. Specifically, the project seeks to use a custom machine learning and artificial intelligence approach informed by physiology to combine oculomotor and EEG signals to specifically enable3D AR/VR control. The ultimate goal is to develop a high performance BCI system that affords flexible user control across hardware, software, and mobile applications."
"1839909","CICI: SSC: SciTrust: Enhancing Security for Modern Software Programming Cyberinfrastructure","OAC","Cybersecurity Innovation","10/01/2018","08/24/2018","Yanfang Ye","WV","West Virginia University Research Corporation","Standard Grant","Robert Beverly","10/31/2019","$649,156.00","Brian Woerner, Xin Li","yanfang.ye@case.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","8027","9150","$0.00","Software plays a vital role supporting scientific communities. Modern software programming cyberinfrastructure (CI), consisting of online discussion platforms (such as Stack Overflow) and social coding repositories (such as Github), offers an open-source and collaborative environment for distributed scientific communities to expedite the process of software development. Within the ecosystem, researchers and developers can reuse code snippets and libraries, or adapt existing ready-to-use software to solve their own problems. Despite the apparent benefits of this new social coding paradigm, its potential security-related risks have been largely overlooked; insecure or malicious codes could be easily embedded and distributed, which could severely damage the scientific credibility of CI. Therefore, there is an urgent need for developing scalable techniques and tools to automatically detect these open-source insecure or malicious codes. To address this issue, this proposed project seeks to explore innovative links between Artificial Intelligence (AI) and cybersecurity to enhance the security of modern software programming CI. <br/><br/>The key components of the proposed research are three-fold: (1) a novel AI-based solution (iTrustSO) utilizing social coding properties is developed to automatically identify suspicious insecure code snippets on Stack Overflow; (2)  a cross-platform model is constructed to represent the complex interplay between GitHub and Stack Overflow; deep learning techniques are then utilized to build a predictive model (iTrustGH) for automatic detection of malicious codes on GitHub; and (3) a user-friendly tool (SciTrust) is developed to enhance code security for software development. The broader impacts of this work include benefits to scientific communities and the whole society by promoting the efficiency of cyber-enabled software development without sacrificing the security. The establishment of a Cybersecurity Lab through this project enhances the education and workforce training in cybersecurity. The project integrates research with education through curriculum development and student mentoring activities for the newly-established cybersecurity degree program. It is also expected to increase the participation of underrepresented groups including minority and women.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747785","Phase II IUCRC UNC Charlotte Site: Center for Visual and Decision Informatics (CVDI)","CNS","IUCRC-Indust-Univ Coop Res Ctr","03/01/2018","05/28/2020","Mirsad Hadzikadic","NC","University of North Carolina at Charlotte","Continuing Grant","Ann Von Lehmen","02/28/2022","$301,670.00","Zbigniew Ras, Mohamed Shehab, Jean-Claude Thill, Wenwen Dou","mirsad@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","5761","5761","$0.00","The Center for Visual and Decision Informatics (CVDI) focuses on developing advanced visual and decision support tools and techniques that enable decision makers to improve the way organizations' information is interpreted and exploited. This award will create a CVDI Site at the University of North Carolina, Charlotte, which will focus on data science and big data analytics research related to risk mitigation in financial services, healthcare, and social good. Financial services and healthcare, along with energy, retail, and logistics are the key sectors of Charlotte's economy. On the other hand, social good is an important issue currently facing Charlotte community. Social good analytics can aid the community's effort to improve living conditions of its citizens, which will positively influence the overall workforce readiness for employers. This contribution will be measured in novel data collection mechanisms and long-term outcomes of implemented policies. The net effect will be more resilient industry, increased economic development, increased social capital, and increased economic mobility in all segments of the community.<br/><br/>The researcher team on this award is uniquely focused on developing long-term capability for researching, understanding, and documenting risk, as well as creating tools and applications for explaining and predicting such risks. This will be accomplished through the application of data- and analytics-driven technologies, including, but not limited to, data and analytics platform, advanced visualization techniques, artificial intelligence and deep-learning algorithms, agent-based modeling and simulation, network science analysis, and systems dynamics methods. Over the next 5 years, this Site will seek to transform UNC Charlotte into the national hub of innovation and creativity for the benefit of industry and the society.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815535","CIF: RI: Small: Information-theoretic measures of dependencies and novel sample-based estimators","IIS","Robust Intelligence","08/15/2018","09/14/2018","Sewoong Oh","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Rebecca Hwa","07/31/2019","$450,000.00","","sewoong@cs.washington.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7495","7495, 7923","$0.00","Measures of dependencies play central roles in discovering associations between variables that leads to scientific discoveries. In practice, analysts need to compute these measures from data, which can be challenging. The standard estimators can fail when, for example, the data has a mixture of continuous and discrete variables, or when the data lies on a complex space with abundant boundaries. The aim of this project is to address practical issues in estimating measures of dependencies, and provide novel estimators to overcome these challenges. The success of the proposed work will result in novel estimators for discovering new aspects of data. The immediate impact is in two specific contexts: discovering correlations in biological datasets and analyzing the inner-workings of deep neural networks; the lasting impact will be in diverse fields including genomic, biology, machine learning, and artificial intelligence. This project also integrates research with education through the creation of a graduate course on statistical learning. In addition, the project will offer undergraduates the opportunity to be involved in research.<br/><br/>This proposal addresses two fundamental questions: designing novel estimators for information theoretic measures and designing novel estimators for modern measures of correlation that is defined as a solution of optimization problems. In the former, two major challenges are addressed: variables of mixed type (continuous and discrete) and boundary biases. Borrowing techniques from local log-likelihood density estimators, nearest neighbor methods, and order statistics, this leads to a new estimator that can adapt to the local geometry of the distributions in a principled way, that improves significantly over existing estimators. In modern data analysis, several measures of correlations are naturally defined as solutions of optimization problems, making them challenging to estimate. This proposal aims to provide a principled approach and propose a new estimator borrowing insights from importance sampling and nearest neighbor methods. The proposed framework is applied to estimate hypercontractivity ratio, an information theoretic quantity that captures hidden correlations in the data and is naturally defined as a solution of an infinite dimensional optimization. The proposed measure of hypercontractivity  is shown to discover potential correlations that other standard measures are not able to, in canonical synthetic examples and real datasets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829071","NRT-HDR: Modeling and Understanding Human Behavior: Harnessing Data from Genes to Social Networks","DGE","NSF Research Traineeship (NRT), Project & Program Evaluation","09/01/2018","06/09/2020","Wei Wang","CA","University of California-Los Angeles","Standard Grant","Vinod Lohani","08/31/2023","$3,000,000.00","Wei Wang, Weizhe Hong, Sean Young, Junghoo 'John' Cho, Andrea Bertozzi","weiwang@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","EHR","1997, 7261","062Z, 7361, 9179, SMET","$0.00","A confluence of technologies is transforming the biological, environmental, and social sciences into data-intensive sciences. Indeed, with the data now produced every day, there exists an unprecedented opportunity to revolutionize the journey of scientific discovery.  By harnessing these data, one can advance the understanding of human conditions, behaviors, and their underlying mechanisms and social outcomes, enabling a spectrum of new and transformative research and practice.  Fundamental new approaches across computing, mathematics, engineering, and sciences are critically needed, and future scientists must be accordingly trained in these emergent cutting-edge methods. This National Science Foundation Research Traineeship (NRT) award to the University of California, Los Angeles will address this demand by training graduate students at the intersections of data science, mathematics, cryptography, artificial intelligence, genomics, behavior science, and social science. The traineeship program anticipates training one hundred twenty (120) PhD students, including fifty (50) funded trainees, from the social, biological, mathematical and computational sciences and engineering, through a unique and comprehensive training opportunity. <br/><br/>This cross-disciplinary traineeship program has four research areas: genomics and genetics; brain imaging and image analysis; mobile sensing and individual behaviors; and social networks. These areas are interconnected through three core themes: mathematical modeling and network analysis, scalable machine learning and big data analytics, and biomedical applications and social outcomes. At the nexus of these research areas and core themes, this traineeship program provides novel interdisciplinary graduate education to advance both graduate student training and scientific research.  Key features of the traineeship include novel curricula; cross-disciplinary laboratory rotations between engineering, life science, and social science; new foundational classes at the intersections of data science, mathematics, artificial intelligence, behavior science, and social science; summer internships at research institutes, big data firms, and hospitals and translational clinical settings; career, ethics, and technical communication skills development; and outreach to minority, women, and high school students with a distinct focus on groups traditionally underrepresented in STEM PhD programs. <br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training. The program is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas through comprehensive traineeship models that are innovative, evidence-based, and aligned with changing workforce and research needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818969","A Two-Way Research Street: Geometric Algorithms in Optimization and Computer-Based  Discrete Geometry","DMS","COMPUTATIONAL MATHEMATICS","07/01/2018","06/07/2018","Jesus De Loera","CA","University of California-Davis","Standard Grant","Leland Jameson","06/30/2021","$306,817.00","","deloera@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1271","9263","$0.00","At the foundation of the progress in artificial intelligence and data science that is changing society (e.g., driverless cars) is the mathematical theory of optimization. For example, convex and non-linear optimization is the engine at the core of the very successful deep neural-networks. The first part of the project develops the mathematics necessary to solve optimization theory challenges (e.g., larger amounts of data, uncertain data) and to create faster, more accurate optimization algorithms. Computers are changing the nature of mathematical research and discovery too. For instance, computers can derive formulas and proofs automaticaly, computers can search for examples, and now they can more easily extract patterns thanks to machine learning. The second part of the project investigates the use of algorithms from artificial intelligence and algorithms to attack problems in mathematics, especially in geometry.<br/><br/>This project in computational mathematics has two interacting components: The first component is to apply methods from convex geometry, algebraic geometry, geometry of numbers, and combinatorics to develop new algorithms for mixed-integer optimization problems arising in data science, especially the clustering of data with special conditions. The project also studies augmentation (primal) algorithms for integer and mixed-integer variables, these are algorithms that generalize the pivoting used for the simplex method. The second component of the project investigates geometric and combinatorial problems amenable to be investigated with computers. The computation of a number of fundamental combinatorial quantities in convex geometry, including the exact value of integer Caratheodory numbers for cones, quantitative Helly numbers, and integral Radon-Tverberg numbers, will be emphasized. The project presents a computer-based approach to prove or disprove several theorems in<br/>discrete geometry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817083","CIF:Small:Machine Learning Based Turbo Detection for Two and Three Dimensional Magnetic Recording","CCF","Comm & Information Foundations","10/01/2018","04/02/2020","Benjamin Belzer","WA","Washington State University","Standard Grant","Phillip Regalia","09/30/2021","$516,000.00","Krishnamoorthy Sivakumar","belzer@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7797","7923, 7935, 9251","$0.00","This project investigates two and three dimensional magnetic recording (TDMR and 3DMR) for next generation hard disk drives. In TDMR, bits are written on two-dimensional patches of a magnetic storage disk, whereas in 3DMR bits are written on multiple disk layers. TDMR is an emerging technology that promises up to an order of magnitude increase in information bits per unit of disk area, without requiring radical redesign of the disk. 3DMR is an even newer technology that has the promise of significant areal information density increases over TDMR. A key problem in TDMR and 3DMR is that, at high densities, some bits are not written to any of the magnetic grains on the disk. Moreover, one must contend with signal dispersion: along-track, across-tracks, and between layers. This project investigates machine learning for turbo detection of TDMR and 3DMR channels at high densities of between 1 and 4 magnetic grains per coded bit. The considered machine learning topics include design of local area influence probabilistic model detectors, recently introduced by the investigators, and design of deep neural network detectors for TDMR and 3DMR. Through established collaborations, the investigators will validate the developed techniques with realistic waveforms and will facilitate technology transfer. The project also includes educational and outreach components. The investigators will work with the Voiland College of Engineering Diversity Programs office to identify potential undergraduate researchers from underrepresented groups to participate in the project.<br/><br/>The specific technical objectives of this project are: (i) developing information-theoretic design techniques for deep neural networks, (ii) designing machine learning based media noise predictors for TDMR turbo-detectors, (iii) designing deep neural network detectors that handle both two-dimensional intersymbol interference and media noise, (iv) generalizing the machine learning based detector designs for 3DMR, and (v) evaluating the developed designs with TDMR and 3DMR waveforms derived from realistic micromagnetic simulations, obtained from international collaborators. This work is expected to provide significant progress toward the industry's information density target of 10 Terabits per square inch.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1748897","Assessing the changes in the brain representations of individual STEM concepts in the course of learning","BCS","Science of Learning, Information Technology Researc","09/01/2018","08/22/2018","Robert Mason","PA","Carnegie-Mellon University","Standard Grant","Soo-Siang Lim","08/31/2021","$549,377.00","Marcel Just","rmason@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","004Y, 1640","059Z, 075Z, 8089","$0.00","Discovering how STEM concepts are represented in the brain will allow us to both assess student learning of science and to teach STEM concepts more effectively. This project makes use of previous work by the investigators that demonstrated the remarkable new ability to determine the neural correlates (brain signature) of an individual concept, using fMRI brain imaging. Knowing the brain signature of physics concepts like gravity and velocity makes it possible to observe how the brain representations develop in a learner's brain, how they are organized, and how they depend on that person's learning background and on the teaching method. If we know the brain's way of organizing STEM concept knowledge, we can design curricula that teach to that organizational system and measure student learning at the brain level. This approach will not replace conventional tests, but it will provide a new type of basis for those tests such that they can assess alternative instructional methods. <br/><br/> The investigators had previously shown that neural signatures can be decomposed into meaningful underlying dimensions that are remarkably similar across people, making it possible to precisely compare the neural representations of student learners to the representations of instructors or of other successful learners. What has not been attempted before, and is the major goal of this project, is relating the neural signatures of concept learning to various learning outcomes.  Behavioral tests will assess the students? acquisition of the concepts and fMRI scans will assess the acquisition of the concomitant brain representations of the individual concepts.  The students' neural representations will be analyzed for their intrinsic integrity, measured by a machine learning classifier's accuracy in identifying a concept from its fMRI signature.  These will be compared to the neural representations of people with demonstrated mastery of the concepts (such as advanced students and the class instructors). Additionally, the project will assess changes in brain tissues (gray and white matter) that co-occur with concept learning, as well as changes in synchronization (functional connectivity) between brain regions involved in the concept representations. A central contribution of this project will be a brain-based understanding of how individual scientific concepts are learned and how this learning can be related to behavioral measures and individual differences.  The long-term goal is to build the foundation for neuroscience findings to inform teaching techniques and assessments, and as inspiration for additional strategies to promote successful learning in both the general student population and students at risk for failure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815971","CIF: Small: Learning on Graphs","CCF","Special Projects - CCF","06/15/2018","06/11/2018","Francois Meyer","CO","University of Colorado at Boulder","Standard Grant","Phillip Regalia","05/31/2021","$426,527.00","","fmeyer@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","2878","075Z, 7923, 7935, 8089, 8091","$0.00","Not unlike road networks, significant disruptions in the pattern of connections between brain regions have profound effect on brain function. International projects, such as the Human Connectome Project and the Autism Brain Imaging Data Exchange initiative, provide open access to massive datasets that can be used to learn the association between brain connectivity and psychiatric or neurological diseases. This award responds to the current lack of analytical and computational methods to quantify changes in the organization of brain functional networks. This project proposes to design novel machine learning algorithms that will lead the way toward precision medicine in psychiatry and neurology. The award will train several graduate students to work on big-data challenges in precision medicine. The source code that will implement the algorithms will be made publicly available in the form of open source toolboxes.<br/><br/>The availability of large datasets composed of graphs creates an unprecedented need to invent tools in statistical learning to study ""graph-valued random variables"". The first goal of this project is to develop theory and algorithms to estimate the mean and variance of a set of graphs. This basic problem is at the core of several statistical inference problems about population of graph ensembles. The second aim is to develop statistical methods that can detect significant structural changes (e.g., alteration of the topology and connectivity, etc.) in a time series of graphs. The third aim is concerned with the question of learning functions defined on a graph ensemble. The proposed approach relies on the ability to equip a graph ensemble with a metric, effectively turning the learning problem into the question of extending functions defined on a metric space.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814369","NSF-BSF:  SHF: Small: Certifiable Verification of Large Neural Networks","CCF","Software & Hardware Foundation","10/01/2018","05/23/2018","Clark Barrett","CA","Stanford University","Standard Grant","Nina Amla","09/30/2021","$480,924.00","","barrett@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7798","7923, 8206","$0.00","Software systems play important roles in almost every area of modern life.  In order to reduce the difficulty of developing new software, research in the field of artificial intelligence (AI) has been promoting a new model of programming: instead of having a human engineer design and code algorithms, a set of training examples are used together with machine-learning algorithms to automatically extrapolate software implementations. In classical programing, because such code is written by humans, we can persuade others that it is correct. In machine-learned systems, however, the program amounts to a highly complex mathematical formula for transforming inputs into outputs. The key difficulty, however, is that it is not possible currently to reason about correctness in such systems.<br/><br/>This project addresses this issue by developing an algorithm, called Reluplex, capable of proving properties of deep neural networks (DNNs) or providing counter-examples if the properties fail to hold.  The project has three main objectives. First, the investigators develop algorithmic techniques to greatly reduce the number of states that need to be explored by a verification tool.  Second, they develop a strategy for producing checkable verification proofs.  Checkable correctness proofs make it unnecessary to rely on correctness of the verification tool; one can instead rely only on the correctness of a small trusted proof-checker.  Finally, the investigators implement this approach in an open-source tool and evaluate it on real-world industrial DNNs. Given that AI components are becoming ubiquitous in safety-critical systems, such as autonomous vehicles, this research will increase trust in these systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813877","CIF: Small: Precise Computational and Statistical Tradeoffs for Iterative Signal Estimation and Supervised Learning","CCF","Special Projects - CCF, Comm & Information Foundations","07/01/2018","06/18/2018","Mahdi Soltanolkotabi","CA","University of Southern California","Standard Grant","Phillip Regalia","06/30/2021","$490,691.00","","soltanol@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","2878, 7797","075Z, 7923, 7935","$0.00","Due to the proliferation of new sensors and acquisition devices, massive data sets are gathered in many modern applications ranging from medical imaging to online advertisement. Conventional statistical signal estimation and machine learning theory aims to understand how the predictive ability or accuracy of data analysis algorithms scales with data sizes. However, given the sheer size of modern data sets, these classical statistical theories are insufficient as algorithms have to operate effectively under a variety of new constraints such as, limited processing time, fixed computational budget and communication constraints. This project aims to develop the fundamental limits of how statistical accuracy tradeoffs with these resource constraints together with algorithms that nearly achieve such performance limits. The resulting signal estimation and learning algorithms are deployed in novel applications aimed at decreasing the acquisition time in medical imaging devices and speeding up parallelized algorithms in other data processing domains. Parts of this project are integrated into an advanced graduate class and select results will serve to motivate K-12 students to pursue careers in STEM (Science, Technology, Engineering and Math).<br/> <br/>In this project, the team of researchers study a family of convex optimization algorithms used for signal estimation and unsupervised learning. The main goal of this project is to understand how the statistical performance of iterative convex optimization algorithms tradeoffs with various statistical and computational resources such as run time, data size, communication, etc. The theoretical investigations utilizes techniques from convex analysis, probability, and information theory. The theoretical analysis can be used to establish fundamental performance bounds for popular convex optimization problems and guide the design of new algorithms that achieve optimal trade-offs between competing resource constraints.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813537","RI:Small:Robust Performance Models","IIS","GVF - Global Venture Fund, Robust Intelligence, EPSCoR Co-Funding","09/01/2018","06/24/2020","Lars Kotthoff","WY","University of Wyoming","Standard Grant","Roger Mailler","08/31/2021","$447,402.00","","larsko@uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","CSE","054Y, 7495, 9150","5947, 7495, 7923, 9150, 9251","$0.00","Algorithms are ubiquitous in modern society and integral to the economy. Whether finding optimal assignments of packages and operators to planes, trucks, and ships, or translating between different languages, the problems solved become larger and more challenging every day. Crucially enabling such developments are advances in artificial intelligence. There are often different approaches for solving the same type of problem, and they are often synergistic -- where one fails, another performs well.  AI techniques in this project allow the best approach for a given problem to be chosen automatically.  This research will allow for such choices to be made more robustly even in difficult circumstances, resulting in improved performance and reduced effort to deploy AI in practical systems. Ultimately, the project will make it easier for humans to develop high-performance AI systems.<br/><br/>Algorithm selection is the process of automatically matching synergistic algorithmic choices to the specific properties of a problem in order to achieve optimal performance.  Current methods for making such choices over available algorithms are often limited in applicability by the hardware on which the algorithms were benchmarked, the resource limits imposed on runs, and subject to bias caused by performance fluctuations in randomized algorithms.  In many cases, these issues are caused by reliance on brittle performance measures, limiting practical application in academia and industry.  This project aims to address these limitations in three ways. First, it will define a notion of robustness to guide algorithm selection, and identify properties of algorithms, experimental setups, and computational environments that affect robustness. Second, it will develop specific performance measures informed by this definition of robustness, and which are portable across different hardware platforms. Third, it will mitigate the impact of brittle performance measures through new approaches to building performance models based on machine learning.  The project will result in the dissemination of shared data and benchmarks to the broader AI community, for example through the Algorithm Selection Library (ASlib).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806738","NSF-BSF: Elementary Particle Physics with ATLAS","PHY","HEP-High Energy Physics","09/01/2018","07/16/2020","Allen Mincer","NY","New York University","Continuing Grant","James Shank","08/31/2021","$1,940,000.00","Kyle Cranmer, Andrew Haas","allen.mincer@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","MPS","1221","014Z, 075Z, 7483","$0.00","This award will provide support for the NYU group working on the ATLAS experiment at the Large Hadron Collider (LHC) at CERN, a particle physics laboratory in Geneva, Switzerland. The LHC machine and ATLAS, a large particle detector facility, were built as basic science tools using funds from NSF and other agencies around the world. One of their primary objectives was to find the Higgs Boson, the last particle in the historically successful ""Standard Model"" (SM) that accounts for so much of the existence of, and forces between, known particles forming the matter in the universe. This effort has been successful. The next step in the experiments is to look for evidence for physics Beyond the Standard Model (BSM) that might, for instance, account for the presence of the mysterious ""Dark Matter"" that makes up so much of the mass of the universe. The LHC is currently in the midst of Run 2, at almost twice the energy explored earlier and with significantly increased event samples. It is possible that evidence for BSM physics could emerge at this higher energy and with the higher event statistics from the current run and by means of further upgrades of the LHC and ATLAS to follow.<br/><br/>Analytically, the NYU group will be studying Higgs decays, searching for evidence of supersymmetry and exotic new states of matter through several approaches, including the reaction process known as Vector Boson Fusion, and will be developing novel approaches to the reconstruction and study of high energy Jets.  NYU will partner with the Weizmann Institute of Science through support from the US-Israel Binational Science Foundation program to search for the decays of Higgs boson decays to charm quark pairs, through heavy flavor tagging.   Here the joint NYU-Weizmann effort will exploit advanced machine learning techniques to identify and select such challenging final states in Higgs decay. <br/><br/>Technical projects include improvements and refinements of the ATLAS trigger for missing energy, a key variable in the searches for new physics, and track triggering, an essential technique for dealing with reconstruction of events in very high rate collisions, which are endemic at the LHC.  The group also helps to lead a smaller experimental effort called milliQan, whose objective is to search for fractionally charged particles that might be produced at the LHC.<br/><br/>The NYU group's broader impacts efforts are extensive.  For general audiences, the NYU exhibit on the LHC is estimated to reach large numbers of school-age children, and the ""Higgs Hunters"" project has engaged users from a variety of age groups in significant numbers.  For secondary school educators, workshops are also planned at NYU to familiarize and educate teachers with some of the state of the art tools used in the particle physics domain, to provide them with background to bring new knowledge to the classroom.  Additionally, the analytical projects of the group, which include statistical applications, machine learning, and analysis preservation tools and techniques, can be extended to fields outside of physics, and young researchers supported under this program will have training in such innovative and cross-cutting techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841099","Community-Building Workshop on Programmable System Security in a Software-Defined World","CNS","CSR-Computer Systems Research","08/01/2018","08/06/2018","Guofei Gu","TX","Texas A&M Engineering Experiment Station","Standard Grant","Erik Brunvand","07/31/2020","$1,485.00","","guofei@cse.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7354","073Z, 7556","$0.00","This workshop brings together networking, systems, and security researchers to discuss and establish a vision for programmable security in modern software-defined infrastructures (e.g., cloud, Internet-of-Things, or edge computing environments). The output of the workshop will be a public report documenting the discussions and a set of recommendations on research directions and frontiers. The workshop and the report will stimulate research collaboration and the creation of a common research vision between disparate communities.<br/><br/>We increasingly live in a software-defined world where systems that were once implemented as rigid control systems or fixed-function hardware systems are now highly programmable through software interfaces that decouple underlying hardware details and offer remote control and centralized management. Early examples of software-defined systems (SD-X) include multi-tenant clouds, software-defined networking (SDN), network functions virtualization (NFV), software-defined infrastructure (SDI), and software-defined radios (SDR). While SD-X technologies have rapidly proliferated within industry and received considerable systems research attention, the paradigm has not been fully exploited in approaching a wide array of important security challenges. The objective of this workshop is to identify those research challenges and opportunities to exploit SD-X approaches in making system security more programmable, agile, orchestrated, and intelligent.  This workshop creates a much-needed opportunity for a cross-cutting group of researchers to fill out the vision of what programmable security based on SD-X could be, including research challenges, long-term visions, and key issues. In the process, this workshop will promote a more focused community and vision where traditionally disparate communities previously worked in isolation and without a more ambitious system security vision within the context of complex software-defined infrastructures.  The workshop report will be made available to the public via the workshop website.<br/> <br/>Broad directions to be considered by the workshop attendees include, but are not limited to: (1) new abstractions for data/control planes aimed specifically at security, (2) new architectures that integrate diverse SD-X domains (networking, processing, storage, etc.) for a more powerful and comprehensive security framework, (3) new programming and language paradigms for programmable security, (4) a better understanding of attack surfaces and adversarial methods within modern software-defined infrastructures, (5) new formal and experimental methodologies for reasoning about software-defined security, (6) the integration of emerging Artificial Intelligence/Machine Learning and data-driven capabilities into programmable system security, (7) new applications paradigms that exploit programmable paradigms in innovative ways, and (8) the application of programmable security approaches to emerging platforms and infrastructure domains. Overall, workshop participants will help to build community and define the vision of a new generation of security technologies in the rapidly expanding world of software-defined infrastructures and devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822207","Planning IUCRC University of Arizona: Center for Networked Embedded, Smart and Trusted Things (NESTT)","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/01/2018","07/29/2018","Frederic Zenhausern","AZ","University of Arizona","Standard Grant","Behrooz Shirazi","12/31/2019","$15,000.00","","fzenhaus@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","5761","5761","$0.00","Most public health systems around the world are investing into initiatives promoting and supporting the convergence of medicine and technology for improving the healthcare delivery systems. These efforts rely on integrating sensing, logistics, computing, actuation and communication functionalities in everyday practices. The University of Arizona will work with Arizona State University, University of Southern California, University of Connecticut, and Southern Illinois University to form a new IUCRC for Networked Embedded, Smart and Trusted Things (NESTT). The vision of NESTT is to contribute to the development of an equitable, safe and secure connected world. NESTT will achieve this vision by focusing on creating holistic IoT solutions, integrating technology disciplines with expertise in medicine, law, business, and humanities.<br/><br/>In order to make these transformative approaches and technologies a reality, machine learning and artificial intelligence are some of the emerging tools for processing the massive amounts of IoT data and information that will shift the medical decision making processes from simple binary modes to complex ""nuanced"" and ""near real-time"" models. UofA NESTT site will co-organized workshops to design industry specifications and prototype health logistics appliances, wearable sensor systems, ambient intelligence and A.I. data algorithms that can be integrated effectively into the regulatory framework and processes of the healthcare delivery system by enabling the IoT advanced technology solutions planned at the other NESTT nodes.<br/><br/>There is trend in digitalization of medicine and healthcare, especially with the emergence of the Internet of Things which will disrupt the healthcare industry in this decade. UA NESTT site will investigate this digital transformation through the entire value chain, from discovery to bedside. Health platforms designed to address unmet needs and improve medical access to underserved large populations will provide useful solutions to the global healthcare challenges. UA NESTT academic, clinical and business partners will contribute their expertise to provide contextualization and standardization of information to offer real-world insights in digital transformation of the healthcare sector.<br/><br/>The agenda and documentation of activities for the NESTT IUCRC planning meetings will be made available on UofA's website for the Center for Applied Nanobioscience and Medicine (https://phoenixmed.arizona.edu/anbm ). It will be maintained until the establishment of NESTT, and then merged with the NESTT site.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808064","Deep Learning-based Detection of Stealth False Data Injection Attacks in Large-Scale Power Grids","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2018","08/15/2018","Katherine Davis","TX","Texas A&M Engineering Experiment Station","Standard Grant","Anil Pahwa","08/31/2021","$360,000.00","Erchin Serpedin, Thomas Overbye","katedavis@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","7607","155E","$0.00","The proposed research aims to strengthen national security by improving the resilience of power grid critical infrastructure with respect to data manipulation attacks. A comprehensive methodology referred to as DEFENDA - DEtection of FalsE and uNexpected Data Attacks - is proposed to quantify the integrity of data and to characterize the impact of false data on power systems. These attacks are referred to as unobservable or stealth false data injection (FDI) attacks, and they are crafted to bypass traditional bad data detection.  DEFENDA's vision is to quickly detect sensor manipulation attacks and correct the false data. The project aims to contribute enhanced state-of-the-art cyber-physical security strategies for transmission system operation, where results will inform solution of similar problems including cyber-physical attack detection at generation, transmission, and distribution levels as well as in communication networks, banking systems, cloud computing and storage, and other critical infrastructures. <br/><br/>DEFENDA will contribute attack detection strategies for real-world power grids via deep neural network (DNN) architectures known to offer superior representational power and improved detection performance. Specifically, DEFENDA aims to develop an efficient and robust FDI attack detection mechanism based on a deep long-short-term-memory (LSTM) recurrent neural network (RNN) that captures the time series nature of the status and measurement data and learns their respective normal and malicious patterns. To ensure detection efficiency, DEFENDA investigates optimal selection of the deep architecture and underlying hyper-parameters. Furthermore, DEFENDA ensures detection robustness through three measures. First, DEFENDA enables replacement of any missing status and measurement data via a deep LSTM auto-encoder (LSTM-AE) to enhance detection performance even in presence of jamming attacks. Second, using a deep variational LSTM auto-encoder (V-LSTM-AE) DEFENDA is capable to detect attacks that have not been characterized via an anomaly detector. Finally, DEFENDA carries out detection decision fusion based on centralized, semi-centralized, and decentralized detection architectures.  DEFENDA will also create and make available synthetic cases with scenarios designed to promote research in cyber-physical analysis and attack detection. By demonstrating the importance of cyber security and data integrity though the scenarios developed, the project will prepare a generation to solve the problems facing society.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812963","CSR: Small: Collaborative Research: Safety Guard: A Formal Approach to Safety Enforcement in Embedded Control Systems","CNS","CSR-Computer Systems Research","08/01/2018","06/14/2018","Haibo Zeng","VA","Virginia Polytechnic Institute and State University","Standard Grant","Erik Brunvand","07/31/2021","$250,000.00","","hbzeng@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7354","7923","$0.00","Ensuring the safety of embedded control systems, such as the ones used in cars, is remarkably difficult as shown by the many recalled cars in recent years, causing tens of billions of financial loss each year. This project aims to improve the safety of critical components in embedded control systems by developing safety guards, a reactive component generated automatically from safety requirements and attached to the original system, to ensure the combined system is safe even if the original system violates the safety requirements. <br/><br/>The intellectual merit of this project lies in the set of methods and tools to be developed for synthesizing safety guards of black-box systems. Specifically, this project consists of three research tasks: (a) building a benchmark suite of critical components and their safety requirements; (b) developing synthesis algorithms for constructing the finite-state machines (FSMs) of the safety guards; and (c) developing software synthesis tools for automatically generating software code that implements these FSMs. <br/><br/>This research project will benefit a wide range of application domains, including automotive and avionics, which will be investigated through collaborations with industry. It will help improve the safety of critical components, including those based on machine learning and artificial intelligence techniques. It will simplify certification since the relatively simple safety guard can be certified against safety requirements in place of the detailed model of a critical component. And last but not the least, it will simplify the development process by allowing people to focus on functionality and performance without worrying about safety violations at the same time. <br/><br/>The resulting software tools, together with evaluation benchmarks and experimental data, will be made available to the public. To facilitate dissemination and sharing, the project will maintain online documentations, tutorials, slides, and source code of the tool and benchmark repositories. Besides the research websites of participants at Virginia Tech and University of Southern California, the following website will be dedicated to disseminate the project results broadly: http://chaowang-vt.github.io/safetyguard/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821819","Phase I IUCRC The Georgia Institute of Technology: Center for Fiber-Wireless Integration and Networking for Heterogeneous Mobile Data Communications (FiWIN)","CNS","IUCRC-Indust-Univ Coop Res Ctr","09/01/2018","09/15/2019","Gee-Kung Chang","GA","Georgia Tech Research Corporation","Continuing Grant","Behrooz Shirazi","08/31/2021","$300,000.00","Umakishore Ramachandran, John Barry, Xiaoli Ma, Hua Wang","gkchang@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","5761","5761","$0.00","The Fiber-Wireless Integration and Networking (FiWIN) Center for Heterogeneous Mobile Data Communications is an NSF Industry/University Cooperative Research Program (I/UCRC) led by the Georgia Institute of Technology (GT) since 2015. Based on recognized research leadership and accumulated expertise in integrated fiber-wireless access technologies, FiWIN is favorably positioned to make substantial contributions to access network architectures, functional designs of network elements, and optimized system interfaces and operations for the globally competitive next generation mobile communication networks. FiWIN has successfully completed the first three years of operation. The center expands its scope with the addition of Auburn University (AU) as a Member Site in 2018.<br/><br/>GT has been and continues to be a leading global player in the development of fiber wireless integration and networking technologies for delivering high data rate, large capacity and low latency multi-band 5G services. The combined expertise of AU and GT will expand FiWIN's scope to develop radio frequency (RF) integrated circuits for beamforming in coordinated multi-point transmission, in mission-critical sensors for industrial and health Internet of Things (IoT) applications and in heterogeneous cell coverage with mobile edge computing. Mutual synergy will foster advances in data compression algorithms, bandwidth efficient waveform and modulation, and machine learning to autonomously mitigate nonlinear behaviors of 5G network elements.<br/><br/>An affordable and reliable advanced communication network will drive academic innovation and business agility for providing a vital gateway for economic and societal growth. The continuing rise of social media and artificial intelligence will revolutionize on how we interact with the internet, the machines and with one another. It will enable new kinds of data analytics to be harnessed for tangible business, education and everyday life benefits. Outcomes of our research will be innovative technologies to benefit US competitive industrial offerings for 5G systems and cloud/fog computing by teaching and fostering students to be ready for a versatile and skilled workforce. <br/><br/>All FiWIN Center research products are published in archival journals or conference proceedings in compliance according to the Center's Membership agreement and Bylaws. A reference library containing archival research products (Annual reports, Industrial Advisory Board (IAB) presentations, student project proposals and outcome presentations, and a list of publications) will be maintained and updated and be accessible to FiWIN Center Members, FiWIN research team members, and general public with security protection for a period of 5 years after the Center has graduated. An interactive web-based server http://fiwin.org will be used for sharing scientific data products with IAB members and the public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841097","Community-Building Workshop on Programmable System Security in a Software-Defined World","CNS","CSR-Computer Systems Research, Networking Technology and Syst","08/01/2018","08/06/2018","Kun Sun","VA","George Mason University","Standard Grant","Erik Brunvand","07/31/2020","$48,500.00","","ksun3@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7354, 7363","073Z, 7556","$0.00","We increasingly live in a software-defined world where systems that were once implemented as rigid control systems or fixed-function hardware systems are now highly programmable through software interfaces that decouple underlying hardware details and offer remote control and centralized management. Early examples of software-defined systems (SD-X) include multi-tenant clouds, software-defined networking (SDN), network functions virtualization (NFV), software-defined infrastructure (SDI), and software-defined radios (SDR). While SD-X technologies have rapidly proliferated within industry and received considerable systems research attention, the paradigm has not been fully exploited in approaching a wide array of important security challenges. The objective of this workshop is to identify those research challenges and opportunities to exploit SD-X approaches in making system security more programmable, agile, orchestrated, and intelligent. This workshop creates a much-needed opportunity for a cross-cutting group of researchers to fill out the vision of what programmable security based on SD-X could be, including research challenges, long-term visions, and key issues. In the process, this workshop will promote a more focused community and vision where traditionally disparate communities previously worked in isolation and without a more ambitious system security vision within the context of complex software-defined infrastructures. The workshop report will be made available to the public via the workshop website.<br/><br/>Broad directions to be considered by the workshop attendees include, but are not limited to: (1) new abstractions for data/control planes aimed specifically at security, (2) new architectures that integrate diverse SD-X domains (networking, processing, storage, etc.) for a more powerful and comprehensive security framework, (3) new programming and language paradigms for programmable security, (4) a better understanding of attack surfaces and adversarial methods within modern software-defined infrastructures, (5) new formal and experimental methodologies for reasoning about software-defined security, (6) the integration of emerging Artificial Intelligence/Machine Learning and data-driven capabilities into programmable system security, (7) new applications paradigms that exploit programmable paradigms in innovative ways, and (8) the application of programmable security approaches to emerging platforms and infrastructure domains. Overall, workshop participants will help to build community and define the vision of a new generation of security technologies in the rapidly expanding world of software-defined infrastructures and devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827139","CC* Networking Infrastructure: The Roadrunner High-Performance Science, Engineering, and Business DMZ","OAC","Campus Cyberinfrastructure","07/01/2018","09/15/2018","Brent League","TX","University of Texas at San Antonio","Standard Grant","Kevin Thompson","12/31/2020","$500,000.00","Harry Millwater, Bernard Arulanandam, Brent League","BRENT.LEAGUE@UTSA.EDU","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","8080","","$0.00","World-class research in cyber security, bioinformatics, cloud computing, machine learning, artificial intelligence, real-time computing and other related areas requires efficient and often near-immediate access to large data sets in order to reduce the time from theory to discovery. In addition, discoveries are often interdisciplinary and multi-institutional. As a result, a critical enabling feature for impactful, collaborative research is a dedicated high-speed network that is omnipresent across a campus. The University of Texas at San Antonio (UTSA), a Hispanic Serving Institution, is implementing a dedicated research network (DMZ) to facilitate data-intensive computation and research collaboration endeavors. This infrastructure fills a gap that currently exists in the ""last mile"" bottleneck from a research lab to the DMZ. <br/><br/>In particular, the project calls for installation of 10 Gb/s switches across campus that, in concert with the dedicated research network, provide 5-10X faster data transfer rates. These improvements foster access by UTSA faculty and students to the campus' high-performance computing facility, high-speed data storage, and visualization laboratory as well as the Texas Advanced Computing Center (TACC) in Austin, Texas. The new network also enhances experiential learning activities such as UTSA's annual CyberPatriot competition, undergraduate research projects in cloud and high-performance computing, cloud computing ELab training for undergraduates, as well as certification programs such as the Master's level certification in cloud computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931078","CAREER:  A Dynamic Program Monitoring Framework Using Neural Network Hardware","CCF","Software & Hardware Foundation","09/01/2018","04/03/2020","Abdullah Muzahid","TX","Texas A&M Engineering Experiment Station","Continuing Grant","Yuanyuan Yang","03/31/2022","$222,278.00","","abdullah.muzahid@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7798","1045, 7941","$0.00","Software bugs and security attacks cripple US economy by costing more than $150 billion a year. However, there has been no major innovation in this context. This research project aims to change that fact with the help of neural network based hardware. If the project is successful, it will significantly affect current industry <br/>practices and spur a new trend. It will encourage companies to invest in new techniques for debugging and security attack analysis using neural network hardware and make a compelling use case for the hardware implementation, thereby influencing continuous investment in neural network hardware. In addition, the project will contribute to the research and educational activities of a minority serving institution. Students will be tightly integrated into the project through dissertation, thesis work, and undergraduate research work. The PI will incorporate emerging architecture design and its programming in undergraduate and graduate coursework. Moreover, the PI will involve local high school students in computer science related projects through summer internships.<br/><br/>Neural network is a machine learning technique that mimics human brain. Therefore, neural network hardware provides some unique capabilities that can be utilized in many different ways. This project proposes to utilize neural network hardware for ""program monitoring"". Program execution monitoring is often used to detect software bugs, performance issues, security attacks etc. Neural network hardware will learn the normal ""behavior"" of the program. Then it will detect any deviation of such behavior. Such deviation can be attributed to software bugs, performance issues or security attacks. The proposed approach provides a general framework for handling these issues. Due to online learning and testing capability of neural network hardware, the framework will be adaptive to any change in program inputs, code, and platforms."
"1757632","REU Site: Undergraduate Research in Smart Environments","CNS","RSCH EXPER FOR UNDERGRAD SITES, Special Projects - CNS","03/01/2018","07/12/2019","Lawrence Holder","WA","Washington State University","Standard Grant","Harriet Taylor","02/28/2021","$370,000.00","","holder@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","1139, 1714","9250","$0.00","This Research Experiences for Undergraduates site will enable 10 undergraduates each year to conduct hands-on research in the field of smart environments for a period of 10 weeks each summer. Research participants from a wide variety of institutions and demographic backgrounds will explore the links between multiple applications including artificial intelligence, machine learning, data mining, high-performance computing, pervasive computing, networking, distributed systems, health, medicine, psychology, gerontechnology, and energy sustainability. Within this context, participants will gain an appreciation for how to collect large amounts of data from these environments, analyze the data for new knowledge, and take actions to effect changes in the environments in order to improve health, security, efficiency and sustainability. <br/><br/>This REU site facilitates research and training in multiple complementary disciplines including computer science, electrical engineering, psychology, and health care. The team partners with other REU programs on campus, and participants will interact regularly with other REU students from around the country, with faculty from multiple REU programs, and with graduate students from the related ongoing research programs in smart environments. The program will enable interdisciplinary research that links design of technology with health-based sustainable energy applications, and creates new opportunities for undergraduates to study health, energy and human behavior using the technology. Results of this program, including descriptions of student projects, lessons learned, and quantitative and qualitative feedback, will be disseminated via the program website. This program will also make a contribution to a generation whose workforce is trained in multiple, complementary disciplines. Finally, our planned recruitment effort and strong mentoring component will spread knowledge of the benefits of the REU program among the targeted populations, resulting in broadening participation in engineering and related disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838124","BIGDATA: F: Collaborative Research: Mining for Patterns in Graphs and High-Dimensional Data: Achieving the Limits","IIS","Big Data Science &Engineering","10/01/2018","09/10/2018","Jiaming Xu","IN","Purdue University","Standard Grant","Sylvia Spengler","08/31/2019","$345,168.00","","jx77@duke.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8083","062Z, 8083","$0.00","While modern datasets are very large, the amount of information per variable is often relatively small. This includes datasets from genomics, social networks, and many applications in machine learning and artificial intelligence. For instance, in genomics we often track hundreds of thousands of genes, but only have a few hundred independent samples for each one. Similarly, online social networks are massive, but the structure of friendships only gives us a relatively small amount of data per individual. This kind of data is called ""high-dimensional"", and poses new challenges for mathematics, statistics, and computer science, especially when (as with all real data) they are noisy or incomplete. This project will identify exactly when and how it is mathematically possible to find patterns in these massive but noisy datasets, giving scientists across many fields a useful guide to how much data they need to draw reliable conclusions, and to develop new algorithms that will solve modern data science problems efficiently and optimally.<br/><br/>Through the study of community detection, noisy graph isomorphism, and matrix/tensor factorization, this project will develop a general framework to 1) locate the information-theoretic limit below which the observation is too noisy to detect the underlying pattern, or even to tell if a pattern exists; 2) devise efficient algorithms that succeed all the way down to the lowest possible signal-to-noise ratio; 3) prove that important classes of algorithms need super-polynomial time in certain hard regimes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819331","SBIR Phase I:  Platform to Elucidate the Causal Mutations Behind Human Inherited Diseases","IIP","SBIR Phase I","07/01/2018","07/12/2018","Bertrand Adanve","NY","GENETIC INTELLIGENCE","Standard Grant","Ruth Shuman","06/30/2019","$225,000.00","","bertrand@geneticintelligence.com","153E W 110th St","New York","NY","100290000","9178931659","ENG","5371","5371, 8038","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) project is the development of an artificial intelligence-based software platform to elucidate inherited diseases by identifying the causative genetic factors in the whole genome at large, not just the tiny portion of the genome called the exome. This technology will help solve the longstanding problems of multigenic inherited diseases and help unlock the full potential of gene therapy modalities such as CRISPR, which require knowledge of the causal mutation for targeting. With a defined genetic target, truly curative therapeutics and early, accurate diagnostics will be made possible. Through this platform, pharmaceutical companies will benefit from a shortened drug development cycle and a lower risk of clinical trial failure while diagnostics companies will be able to develop fast and accurate molecular diagnostics targeting the mutations identified by the platform. <br/><br/>This SBIR Phase I project proposes to build a build a proof-of-concept software platform that employs a combination of supervised and unsupervised machine learning algorithms to process, sort, and analyze human whole genome sequencing data from an Amyotrophic Lateral Sclerosis (ALS) cohort from a set genetic background. ALS is an incurable, debilitating disease whose genetic causes remain largely unknown. Identifying these mutations will enable the development of efficacious treatments for the conditions. Three main objectives will be accomplished for the platform. One, the ability to process full-size, high coverage human whole genome data automatically through the pipeline in a scalable manner. Two, the ability to identify the genetic background of a test subject with a top-5 error rate of <1%, an important verification step to minimize incorrect cohort stratification from false ancestry self-reports. Three, the ability to rapidly identify at least one genetic feature known to be associated with ALS (e.g., SOD1, C9ORF72), which will help provide early validation for the platform.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838251","BIGDATA: F: Collaborative Research: Mining for Patterns in Graphs and High-Dimensional Data: Achieving the Limits","IIS","Big Data Science &Engineering","10/01/2018","09/10/2018","Cristopher Moore","NM","Santa Fe Institute","Standard Grant","Sylvia Spengler","09/30/2021","$737,645.00","","moore@santafe.edu","1399 HYDE PARK ROAD","SANTA FE","NM","875018943","5059462727","CSE","8083","062Z, 8083","$0.00","While modern datasets are very large, the amount of information per variable is often relatively small. This includes datasets from genomics, social networks, and many applications in machine learning and artificial intelligence. For instance, in genomics we often track hundreds of thousands of genes, but only have a few hundred independent samples for each one. Similarly, online social networks are massive, but the structure of friendships only gives us a relatively small amount of data per individual. This kind of data is called ""high-dimensional"", and poses new challenges for mathematics, statistics, and computer science, especially when (as with all real data) they are noisy or incomplete. This project will identify exactly when and how it is mathematically possible to find patterns in these massive but noisy datasets, giving scientists across many fields a useful guide to how much data they need to draw reliable conclusions, and to develop new algorithms that will solve modern data science problems efficiently and optimally.<br/><br/>Through the study of community detection, noisy graph isomorphism, and matrix/tensor factorization, this project will develop a general framework to 1) locate the information-theoretic limit below which the observation is too noisy to detect the underlying pattern, or even to tell if a pattern exists; 2) devise efficient algorithms that succeed all the way down to the lowest possible signal-to-noise ratio; 3) prove that important classes of algorithms need super-polynomial time in certain hard regimes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813117","CSR: Small: Collaborative Research: Safety Guard: A Formal Approach to Safety Enforcement in Embedded Control Systems","CNS","CSR-Computer Systems Research","08/01/2018","06/14/2018","Chao Wang","CA","University of Southern California","Standard Grant","Erik Brunvand","07/31/2021","$250,000.00","","wang626@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7354","7923","$0.00","Ensuring the safety of embedded control systems, such as the ones used in cars, is remarkably difficult as shown by the many recalled cars in recent years, causing tens of billions of financial loss each year. This project aims to improve the safety of critical components in embedded control systems by developing safety guards, a reactive component generated automatically from safety requirements and attached to the original system, to ensure the combined system is safe even if the original system violates the safety requirements. <br/><br/>The intellectual merit of this project lies in the set of methods and tools to be developed for synthesizing safety guards of black-box systems. Specifically, this project consists of three research tasks: (a) building a benchmark suite of critical components and their safety requirements; (b) developing synthesis algorithms for constructing the finite-state machines (FSMs) of the safety guards; and (c) developing software synthesis tools for automatically generating software code that implements these FSMs. <br/><br/>This research project will benefit a wide range of application domains, including automotive and avionics, which will be investigated through collaborations with industry. It will help improve the safety of critical components, including those based on machine learning and artificial intelligence techniques. It will simplify certification since the relatively simple safety guard can be certified against safety requirements in place of the detailed model of a critical component. And last but not the least, it will simplify the development process by allowing people to focus on functionality and performance without worrying about safety violations at the same time. <br/><br/>The resulting software tools, together with evaluation benchmarks and experimental data, will be made available to the public. To facilitate dissemination and sharing, the project will maintain online documentations, tutorials, slides, and source code of the tool and benchmark repositories. Besides the research websites of participants at Virginia Tech and University of Southern California, the following website will be dedicated to disseminate the project results broadly: http://chaowang-vt.github.io/safetyguard/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821815","Theory-Guided Discovery of Tin-Based Materials","DMR","CONDENSED MATTER & MAT THEORY","09/01/2018","05/27/2020","Alexey Kolmogorov","NY","SUNY at Binghamton","Continuing Grant","Daryl Hess","08/31/2021","$354,105.00","","kolmogorov@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","MPS","1765","054Z, 8084, 8396, 8399, 9216, 9263","$0.00","NON-TECHNICAL SUMMARY<br/>This award supports computational and theoretical research to advance theory-guided discovery of new materials through rapid search of the large space of chemical compositions. The central aims of this project are to perform a systematic screening of tin alloys and develop a library of neural network-based interatomic potentials for an extended set of chemical elements. The neural network models will be used to accelerate the search over possible structures at the level of atoms. Research into tin alloys has attracted renewed interest due to their potential to display novel physics and next-generation functional features. Finely tuned tin-based topological insulators could find future use in spintronics, quantum computing, and thermoelectric materials which can generate electricity from heat. High-capacity tin-based electrodes with improved durability could make batteries cheaper and safer. Carefully optimized tin-based solders may reduce the use of toxic lead-containing materials. <br/><br/>This project includes educational activities to attract young students and members of underrepresented groups to scientific research. The PI will contribute a new theme to Binghamton University's Physics Outreach Program for middle school students, develop a set of hands-on presentations on neural networks for undergraduate students, and recruit students from different majors enrolled in Binghamton University's Evolutionary Studies program to carry out interdisciplinary undergraduate research.<br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports computational and theoretical research to advance theory-guided discovery of new materials through rapid search of the large space of chemical compositions. The PI will focus on the systematic study of tin-based materials. The work is motivated by the remarkable richness of the tin alloys' structural and electronic properties enabling their use as topological insulators, battery anodes, solders, and more. The main challenges associated with the study and development of tin alloys lie in the complexity of their structures and the diversity of their bonding mechanisms. These factors limit the scope of ab initio-based study and the accuracy of classical potential-based modeling. <br/><br/>The PI aims to demonstrate that the efficiency and reliability of materials prediction can be improved considerably by: (i) screening a large materials class with a suite of diverse search strategies, and (ii) using emerging neural network methodology for modeling interatomic interactions to accelerate the search. For the comprehensive sampling of the configuration space of structures and compositions, the research team will rely on a combination of high-throughput, evolutionary, and rational design searches. Identification of new tin-based wide-gap topological insulators, durable battery anodes, and stable lead-free solders will advance knowledge in several areas of basic and application-driven research. For the systematic construction of reusable neural network models, the research team will use a recently developed stratified training procedure enabling a natural extension of libraries to larger sets of chemical systems. The neural network models will be freely available as a part of the group's open-source MAISE package. This effort will promote the development and application of emerging machine learning methods in materials research. The scientific work will be integrated with educational and outreach activities which will foster the interest of the next generation in science, technology, engineering, and mathematics disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830478","LEAP-HI/GOALI: Engineering Crops for Genetic Adaptation to Changing Enviroments","CMMI","LEAP-HI Leading Engineering fo, GOALI-Grnt Opp Acad Lia wIndus, COVID-19 Research, EPSCoR Co-Funding","09/01/2018","04/15/2020","Lizhi Wang","IA","Iowa State University","Standard Grant","Bruce Kramer","02/29/2024","$2,165,142.00","William Beavis, Sotirios Archontoulis, Guiping Hu, Jack Kloeber","lzwang@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","068Y, 1504, 158Y, 9150","019Z, 062Z, 068Z, 075Z, 096Z, 1504, 5514, 8024","$0.00","This Leading Engineering for America's Prosperity, Health, and Infrastructure (LEAP-HI) Grant Opportunities for Academic Liaison with Industry (GOALI) project addresses the NSF Big Ideas of Understanding the Rules of Life and Harnessing the Data Revolution in targeting the need to provide food, fiber and fuel for a growing population using fewer resources (land, water, pesticides and fertilizers) in uncertain and rapidly changing environments. It is widely recognized that current agricultural technologies, from crop genetic improvement to field crop production, will not meet future agricultural demands, due to their heavy reliance on expensive, time-consuming, trail and error field trials to develop improved plant breeds. Emerging mathematical optimization and machine learning methods for analyzing high-dimensional data provide opportunities to speed up plant breeding to achieve rapid and efficient adaptation of crops to changing environments. The approaches in this project will take advantage of engineering techniques that have been used to remarkably improve the efficiency and resiliency of communication, manufacturing, transportation and energy systems.  The research requires the synthesis of multiple disciplines, including agronomy, crop modeling, machine learning, operations research, optimization and plant breeding and aims to demonstrate the leadership role of engineering in addressing agricultural challenges.<br/><br/>Three technical issues, which represent a small but highly visible subset of agronomic systems, will be addressed: (1) accurately predicting plant phenotypes based on genetic, agronomic management and environmental data and their interactions; (2) design of genetic improvement systems to efficiently develop cultivars with superior phenotypes; and (3) design of crop management strategies to assure that crops achieve superior phenotypes under changing environments, while balancing reward, time, and risk in the decision-making process. The research team will first translate the technical issues into engineering objectives and then identify existing methods and design new ones to achieve the objectives. The corresponding engineering objectives are: (1) identify a small subset of variables associated with synergistic effects in addition to their additive effects; (2) design a set of algorithms for genomic selection, which is a special type of nonlinear, non-convex, high-dimensional, and dynamic optimization problem constrained by resource availability and laws of reproductive biology; and (3) create a set of multi-objective and multi-level optimization models and algorithms for balancing reward, time, and risk, subject to genetic, environmental, and logistical constraints. Achieving these objectives will demonstrate the power of engineering approaches in improving the efficiency and resiliency of agronomic systems, with the aim of establishing plant breeding as an engineering discipline.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807069","RUI: Improving LIGO Optics and Data Quality to Increase the Rate and Accuracy of Gravitational-Wave Observations","PHY","LIGO RESEARCH SUPPORT","08/15/2018","07/17/2020","Joshua Smith","CA","California State University-Fullerton Foundation","Continuing Grant","Pedro Marronetti","07/31/2021","$299,538.00","","josmith@fullerton.edu","1121 North State College Blvd.","Fullerton","CA","928313014","6572782106","MPS","1252","075Z, 9229","$0.00","A century after the prediction of gravitational waves by Albert Einstein, the NSF-funded Advanced Laser Interferometer Gravitational-Wave Observatory (Advanced LIGO) opened a new window on the Universe by discovering gravitational waves from merging binary systems of black holes and neutron stars. This award will extend the reach of Advanced LIGO, helping it to achieve its full promise. In particular, it will support research to reduce the amount of light that is ""lost"" when it hits the optics in gravitational-wave detectors and to apply Machine Learning techniques to find and remove disturbances in the detector data. This research will involve and provide transferable skills to a diverse team of undergraduate and Master's students at California State University Fullerton, a primarily undergraduate and Hispanic-serving institution. The results of this research will improve upon the state-of-the art in optics and deepen our understanding of the Universe through new gravitational-wave discoveries.     <br/><br/>This award renews support for California State University Fullerton's experimental gravitational-wave physics program. The PI will lead a team of undergraduate and master's students in research focused on two areas critical to extending the reach of Advanced LIGO. First, they will adapt techniques from Machine Learning to identify the physical processes that cause time-varying noise in the instruments and work to ameliorate them. This will increase the rate of gravitational-wave observations and the accuracy of the information extracted from them. Second, they will conduct experiments to identify the mechanisms behind light scattering in gravitational-wave detector optics, a key to the quantum noise that directly limits LIGO's sensitivity. A new instrument will be developed to test whether the long mysterious point-like scatter from LIGO optics is related to micro-crystals formed during annealing of the optics. This work will lead to higher quality optics and improve quantum-noise reduction techniques such as squeezed light in future gravitational-wave observatories.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813091","Theory and Applications of Localized Kernel Bases to Meshfree Methods","DMS","APPLIED MATHEMATICS","09/01/2018","08/19/2018","Joseph Ward","TX","Texas A&M University","Standard Grant","Victor Roytburd","08/31/2021","$230,812.00","Francis Narcowich","jward@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1266","","$0.00","The need for analyzing and modeling data taken from scattered, irregularly placed sites arises frequently in diverse fields: atmospheric science, artificial intelligence, computer-aided design graphics, data mining, medical imaging, learning networks, and many other areas.  For example, weather prediction or climate modeling is based on geophysical data collected at scattered sites, by sensors on satellites, ground stations, or stations at sea.  Carrying out such tasks presents difficulties for traditional methods, which are based on collecting data at uniformly placed sites or which require constructing ""meshes"" (think wire fence) that must be carefully tailored to deal with the data sites involved.  Newer methods, the so-called kernel methods, are meshfree and can handle scattered data.  The investigators further develop the theory of kernel methods, on the basis of which algorithms can be developed that are easier to use, faster, less expensive to implement, and capable of handling data from a hundred thousand or more sites.<br/><br/>Scattered data problems present a challenge for any method, including the traditional kernel-type algorithms based on translates of one (conditionally) positive definite function.  Scattered data occur naturally in meshfree methods, machine learning, neural nets, and other situations.  Dealing with such data, ideally, requires local, stable bases, preconditioners, and other similar tools.  In recent work on the sphere and rotations in space where no boundary is present, the investigators have developed novel bases related to certain classes of kernels.  For problems where boundaries are inherent, their bases need further development.  One key part of this project involves a novel idea of extrapolating data slightly beyond the boundary of a compact domain to enhance the approximation power of the method.  Another key area of investigation involves local approximation orders for data that is far more general than the typical quasi-uniform assumptions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1932630","BIGDATA: F: Collaborative Research: Mining for Patterns in Graphs and High-Dimensional Data: Achieving the Limits","IIS","Big Data Science &Engineering","10/01/2018","01/15/2020","Jiaming Xu","NC","Duke University","Standard Grant","Sylvia Spengler","09/30/2021","$345,168.00","","jx77@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8083","062Z, 8083","$0.00","While modern datasets are very large, the amount of information per variable is often relatively small. This includes datasets from genomics, social networks, and many applications in machine learning and artificial intelligence. For instance, in genomics we often track hundreds of thousands of genes, but only have a few hundred independent samples for each one. Similarly, online social networks are massive, but the structure of friendships only gives us a relatively small amount of data per individual. This kind of data is called ""high-dimensional"", and poses new challenges for mathematics, statistics, and computer science, especially when (as with all real data) they are noisy or incomplete. This project will identify exactly when and how it is mathematically possible to find patterns in these massive but noisy datasets, giving scientists across many fields a useful guide to how much data they need to draw reliable conclusions, and to develop new algorithms that will solve modern data science problems efficiently and optimally.<br/><br/>Through the study of community detection, noisy graph isomorphism, and matrix/tensor factorization, this project will develop a general framework to 1) locate the information-theoretic limit below which the observation is too noisy to detect the underlying pattern, or even to tell if a pattern exists; 2) devise efficient algorithms that succeed all the way down to the lowest possible signal-to-noise ratio; 3) prove that important classes of algorithms need super-polynomial time in certain hard regimes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1734910","NCS-FO: Connecting Spikes to Cognitive Algorithms","IIS","IntgStrat Undst Neurl&Cogn Sys","01/01/2018","08/07/2017","Il Memming Park","NY","SUNY at Stony Brook","Standard Grant","Kenneth Whang","12/31/2021","$715,232.00","","memming.park@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","8624","8089, 8091, 8551","$0.00","Experimental neuroscientists can record the signals communicated among the neurons that are collectively involved in producing meaningful behaviors, but making sense of these patterns of activity in terms of specific mental functions is challenging. This research project aims to discover the unseen mental processes that underlie such meaningful behavior from those recordings. The technology developed in this endeavor will uncover new ways of understanding mental processes hidden deep in the noisy signals collected from multiple neurons and will be used to derive new theoretical models (cognitive algorithms) to explain how populations of neurons work together. Such models will contribute to the development of diagnostic tools and neural prosthetics for cognitive dysfunctions in perception, working memory, and decision making, and can also inspire advances in machine learning and artificial intelligence.<br/> <br/>The technical goal of this project is to develop a data-driven framework amenable to visualization and interpretation of neural activity underlying cognition. The core of the project is the identification and recovery of an interpretable low-dimensional nonlinear continuous dynamical system that underlies observed neural time series, and its validation through experimental perturbations. This will answer two key scientific questions: (1) How are task and cognitive variables represented in low-dimensional neural trajectories; and (2) What are the laws that govern the time evolution of the neural states. Answering these questions will help us understand how subjects implement and switch between different cognitive strategies, and more importantly, will provide a means for testing previously proposed theoretical models of the neural computations underlying cognition. This project will develop a number of statistical methods that can (i) extract private and shared noise from single-trial electrophysiological observations, (ii) combine recordings from multiple sessions to infer a common cognitive neural dynamics model, and (iii) design control stimulation to perturb the current neural state. Specifically, these tools will be applied to recordings from cortical areas involved in visuomotor decision-making to discover (1) how the co-variability in a population of sensory neurons encodes decision variables, (2) how the cognitive strategy changes when sensory evidence statistics change, and (3) the underlying dynamics that sustain spatial working memory. The success of this project could transform how the field analyzes population activity with low-dimensional structure in the context of cognitive tasks and beyond."
"1734944","Collaborative Research: NCS-FO: Connecting Spikes to Cognitive Algorithms","IIS","IntgStrat Undst Neurl&Cogn Sys","01/01/2018","08/07/2017","Alexander Huk","TX","University of Texas at Austin","Standard Grant","Kenneth Whang","12/31/2021","$234,752.00","","huk@mail.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8624","8089, 8091, 8551","$0.00","Experimental neuroscientists can record the signals communicated among the neurons that are collectively involved in producing meaningful behaviors, but making sense of these patterns of activity in terms of specific mental functions is challenging. This research project aims to discover the unseen mental processes that underlie such meaningful behavior from those recordings. The technology developed in this endeavor will uncover new ways of understanding mental processes hidden deep in the noisy signals collected from multiple neurons and will be used to derive new theoretical models (cognitive algorithms) to explain how populations of neurons work together. Such models will contribute to the development of diagnostic tools and neural prosthetics for cognitive dysfunctions in perception, working memory, and decision making, and can also inspire advances in machine learning and artificial intelligence.<br/> <br/>The technical goal of this project is to develop a data-driven framework amenable to visualization and interpretation of neural activity underlying cognition. The core of the project is the identification and recovery of an interpretable low-dimensional nonlinear continuous dynamical system that underlies observed neural time series, and its validation through experimental perturbations. This will answer two key scientific questions: (1) How are task and cognitive variables represented in low-dimensional neural trajectories; and (2) What are the laws that govern the time evolution of the neural states. Answering these questions will help us understand how subjects implement and switch between different cognitive strategies, and more importantly, will provide a means for testing previously proposed theoretical models of the neural computations underlying cognition. This project will develop a number of statistical methods that can (i) extract private and shared noise from single-trial electrophysiological observations, (ii) combine recordings from multiple sessions to infer a common cognitive neural dynamics model, and (iii) design control stimulation to perturb the current neural state. Specifically, these tools will be applied to recordings from cortical areas involved in visuomotor decision-making to discover (1) how the co-variability in a population of sensory neurons encodes decision variables, (2) how the cognitive strategy changes when sensory evidence statistics change, and (3) the underlying dynamics that sustain spatial working memory. The success of this project could transform how the field analyzes population activity with low-dimensional structure in the context of cognitive tasks and beyond."
"1816512","SHF: Small: Fault Model Evaluation and Discovery","CCF","Software & Hardware Foundation","06/01/2018","07/08/2019","Ronald Blanton","PA","Carnegie-Mellon University","Standard Grant","Sankar Basu","05/31/2021","$462,330.00","","blanton@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7945, 9102, 9251","$0.00","Integrated electronic circuits (ICs) today are ubiquitous in that they affect every aspect of our daily lives, ranging from use within all forms of mobile devices (smart phones, laptops, etc.) to all modes of transportation (cars, aircraft, etc.). Advances in electronics also form the foundation for the recent and forthcoming advances in machine learning, artificial intelligence, autonomous vehicles, etc.  The research work stemming from the project will aid the continued advancement of the electronic semiconductor industry by helping leading companies ensure that electronics work properly and reliably over long time periods. Additionally, the research conducted will include undergraduate student researchers, and the results will be disseminated to the research community, and incorporated into post-secondary curriculum.<br/><br/>The research will specifically examine how electronic failures are modeled, and for any shortcomings discovered, new models that are more accurate will be developed. While it is widely known that model effectiveness greatly depends on the IC and its fabrication, it is not known which models will be proven ineffective beforehand. In addition, especially for cutting-edge technologies, it is expected that there also will be a need for new, more accurate fault models for state-of-the-art ICs. Therefore, the objective of this research is to develop a comprehensive data-driven methodology for the continuous evaluation and development of fault models. Data from failed ICs will be analyzed to understand the likelihood and types of failures. For the expected mismatch between existing models and measured data, new models will be proposed and evaluated. Successfully meeting the objective of this research would will enable IC manufacturers to produce more reliable ICs at much lower cost.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839567","ATE 2.0: Preparing Technicians for the Future of Work","DUE","Advanced Tech Education Prog","09/01/2018","08/23/2018","Ann-Claire Anderson","TX","CORD","Standard Grant","Virginia Carter","08/31/2022","$3,471,157.00","Richard Gilbert, Michael Lesiecki, Hope Cotner","anderson@cord.org","4901 Bosque Boulevard","Waco","TX","767105841","2547418334","EHR","7412","1032, 9178, SMET","$0.00","The workplace of today is undergoing a major transformation driven by machine learning, artificial intelligence, the internet-of-things, robotics, and systems-integrated process control. NSF's focus on the Future of Work at the Human Technology Frontier recognizes that technology advances are changing industries at an unprecedented pace.  These technological advances promise benefits to the nation by creating new enterprises, occupations, and opportunities for innovation and global leadership while drastically altering the workplace as we know it.  As technology evolves, so will tasks and occupations, creating a demand for an expanding array of knowledge, skills, and services. The demand for positions involving tasks that can be automated will decline and, in some cases, disappear, while entirely new occupations will emerge. This transformation is already affecting America's technicians. This project proposes strategies and collaborative regional activities with industry that will enable the NSF-ATE community to prepare technicians for the changing workplace by transforming technician education at the secondary and post-secondary educational levels.<br/><br/>This project will convene academic partners, industry leaders, and economic development professionals.  These individuals will serve as collaborative thought partners in framing, testing, refining, and supporting strategies that transform technician education to assure regional competitiveness in the evolving workplace. Technological education today generally focuses on industry segments and single sectors. Yet, soon technicians will need skill sets that cross industries and support both core and advanced STEM skills. This project will identify key cross-disciplinary and new disciplinary knowledge and skills needed by technicians in industries that are responding to the changing workplace. Regional networks of academic partners will actively collaborate with industry to strengthen ATE efforts to improve technician education across the US. It is expected that bringing the relevant stakeholders together will facilitate the needed paradigm shift in technician education, and coalesce support around industry expectations for technician education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816372","AF: Small: Parallels in Approximability of Discrete and Continuous Optimization Problems","CCF","Algorithmic Foundations","10/01/2018","05/22/2018","Madhur Tulsiani","IL","Toyota Technological Institute at Chicago","Standard Grant","Tracy Kimbrel","09/30/2021","$497,239.00","","madhurt@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926","$0.00","Optimization problems arise naturally in many areas such as scheduling, artificial intelligence, software engineering, control of robotic systems, statistics and machine learning. Many of these problems require too long to solve exactly - a common approach for dealing with this has been to design techniques which can efficiently find approximate solutions that are 'good enough' for the task at hand. The study of what approximations are best possible, as well as methods for achieving them, has also led to many new ideas in theoretical computer science, leading to a rich mathematical theory. This project considers several such problems (arising in different areas) which represent challenges to our current understanding. The goal of the project is to develop unified techniques for solving and analyzing them. The project includes several opportunities for training and mentoring of graduate and undergraduate students. Another aim of the project is to develop a collaborative forum for theoretical computer science students in the Chicago area, which can be used to discuss technical ideas and develop expository material.<br/><br/>This project considers various problems in discrete and continuous optimization, which represent bottlenecks for algorithmic techniques for designing approximation algorithms, as well as for techniques proving hardness of approximation. The difficulty of understanding many of these problems arises from the fact that many of them only impose a relatively weak global constraint on the solutions, which is hard to exploit algorithmically and also not amenable to techniques for proving inapproximability. The project considers several continuous optimization problems which offer an ideal testbed for the development of new algorithmic techniques, while still capturing the bottlenecks in proving inapproximability of related discrete problems. The aim of this project is to examine such problems from the following perspectives: (1) average-case hardness and lower bounds for the Sum-of-Squares hierarchy of convex relaxations; (2) techniques and barriers for proving inapproximability; and (3) conditions under which good approximations are achievable.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747341","SBIR Phase I:  High Fidelity EUV PhotoMasks","IIP","SBIR Phase I","01/01/2018","12/18/2017","Supriya Jaiswal","CA","Astrileux Corporation","Standard Grant","Rick Schwerdtfeger","06/30/2019","$225,000.00","","supriya@astrileux.com","4225 Executive Sq Ste 490","La Jolla","CA","920378411","8585312432","ENG","5371","5371, 8034","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to drive the next generation of advanced computing power and performance, by manufacturing integrated circuits, the fundamental units of electronic systems, at length scales of 7 nm and smaller. Today?s central processing units (CPUs) each contain 7.2 Bn chips and over 1.2 sextillion chips are manufactured per year to meet computing demands. Next generation technology is expected to enable artificial intelligence and machine learning through both conventional computing and potentially neuromorphic paradigms, bringing to reality transformative applications such as self-driving cars and smart buildings. As Moore?s law continues to set the pace of technological advancement, chipmakers will deploy new EUV (Extreme Ultraviolet) lithography tools, using light of 13.5 nm to pattern integrated circuits or chip architecture into silicon wafers, for the next three generations of technology. Chipmakers strive to bring about the readiness of EUV technology in 2019. The global demand for next generation electronics is forever increasing as the population grows above 7 Bn. However, the global supply of electronics constantly faces challenges to reduce costs and deliver technology beyond Moore?s Law.<br/><br/>The proposed project addresses the challenges related to high volume manufacturing at the 7 nm node for lithography tools and its components. For example, an EUV photomask, a high commodity component, patterns and replicates integrated circuit design into silicon wafers. Current EUV photomasks have a sub-optimal manufacturing yield of ~60% and suffer from defectivity which arises during fabrication of its architecture. During operational use the photomask sustains damage from the debris generated by the EUV plasma light source that implants in the mask and inevitably replicates in the wafer, destroying the integrated chip pattern. In high volume manufacturing, these issues manifest in the wafer yield, the reusability of a mask, and drive the need for high cost real-time inspection and metrology. A new EUV photomask which promises greater robustness to defects, a higher manufacturing yield, more reusability of masks in operations and a longer lifetime is presented. The goals of the project are to evaluate new integrated architecture for the EUV mask design, develop a higher yield fabrication process and characterize the EUV performance. More robust photomasks reduce the capital outlay required for in-situ metrology and inspection and ultimately bring down the cost of next generation electronics."
"1831524","RIDIR: A Big Data Approach to Understanding American Growth","SES","Economics, Data Infrastructure","09/01/2018","11/14/2019","Konstantinos Arkolakis","MA","National Bureau of Economic Research Inc","Standard Grant","Joseph Whitmeyer","08/31/2021","$1,004,899.00","Sun Kyoung Lee","costas.arkolakis@yale.edu","1050 Massachusetts Avenue","Cambridge","MA","021385398","6178683900","SBE","1320, 8294","","$0.00","This project uses a big data approach to find out what lies behind the tremendous growth in the American economy in the nineteenth and twentieth centuries.  This award supports a team of researchers, Costas Arkolakis and Michael Peters at Yale University, and Sun Kyoung Lee at Columbia University, who employ artificial intelligence and machine learning technologies to link and then analyze massive amounts of historical US federal census, Department of Labor, and Bureau of Labor Statistics data. The transformation of the US economy during this time period was remarkable, from a rural economy at the beginning of the 19th century to an industrial nation by the end. More strikingly, after lagging behind the technological frontier for most of the nineteenth century, the United States entered the twenty-first century as the global technological leader and the richest nation in the world.  Results from this project will allow Americans to understand how people lived and how business operated.  It will reveal the past that led us to where we are now in terms of people, geography, prices and wages, wealth, revenue, output, capital, numbers and types of workers, urbanization, migration, and industrialization.<br/><br/>In this project, multiple data sets, involving variables such as family status, age, ethnicity, occupation, literacy, and income, are linked. This enables researchers to build a robust picture of the factors that shaped the American economic geography and to illuminate major aspects of America's economic growth during the ""Second Industrial Revolution"" (1850-1940), a period characterized by an influx of people of European origin.  One analysis constructs pairs of parent-child data and investigates, over time, how a child's chance of moving up relative to the child's parents has evolved. Another reveals how economic shocks and policies affected trends in intergenerational mobility. A third uses digitized plant-level Census of Manufactures data to explore the factors that contributed to manufacturing growth at the firm level and how this, in turn, affected America's aggregate economic growth. The data will be shared with other researchers in fields such as economics, history and sociology, enabling them to uncover the underlying forces of America's remarkable economic transition and growth.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810758","NSF-BSF: AF: Small: An Algorithmic Theory of Brain Networks","CCF","Algorithmic Foundations","06/01/2018","05/18/2018","Nancy Lynch","MA","Massachusetts Institute of Technology","Standard Grant","A. Funda Ergun","05/31/2021","$450,000.00","","lynch@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7923, 7934, 8089, 8091, 9102","$0.00","Understanding how the brain works, as a computational device, is a central challenge of modern neuroscience and AI.  Different research communities approach this challenge in different ways, including examining neural network structure as a clue to computational function, using functional imaging to study neural activation patterns, developing theory based on simplified models of neural computation, and engineering of neural-inspired machine learning  architectures. This project will approach the problem using techniques from distributed computing theory and other branches of theoretical computer science. This project has the potential to improve our understanding of computation in the brain, by identifying key problems that are solved in the brain and key mechanisms that may be used to solve them.  This work can also have impact on theoretical computer science, by contributing a new and fruitful direction for theoretical study. This collaboration between MIT and the Weizmann Institute in Israel will increase the participation of women and minority participants in this field and will seek to bridge the gap between computer scientists and biology researchers.<br/><br/>Specifically, the project develops an algorithmic theory for brain networks, based on novel stochastic Spiking Neural Network models with general interconnection patterns. It defines a collection of abstract problems to be solved by these networks, inspired by problems that are solved in actual brains, such as problems of focus, recognition, learning, and memory.  The project designs algorithms (networks) that solve the problems, and analyze them in terms of static costs such as network size, and dynamic costs such as time to converge to a correct solution.  The investigators consider tradeoffs between the various costs, and will prove corresponding lower bound results. The models, problems, and solutions should be simple enough to enable theoretical analysis, yet realistic enough to provide insight into the behavior of real neural networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824005","Improving and Integrating Global Diversity Estimates Using Transparent Methods","SES","Political Science","08/15/2018","07/16/2018","Avital Livny","IL","University of Illinois at Urbana-Champaign","Standard Grant","Brian Humes","07/31/2021","$315,583.00","Scott Althaus","alivny@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","SBE","1371","040Z","$0.00","Understanding the socio-political impact of ethnic, religious, and linguistic diversity is a vital concern for both social scientists and policy-makers. Using several different datasets measuring diversity around the world, researchers have found that it often has a negative impact on democracy, development and political stability. However, these findings rely on fundamentally flawed datasets: their demographic estimates come from questionable sources and do not vary within countries or over time. This makes them ill-suited to studying outcomes like war and development, which often change rapidly and cluster in space. In addition, the existing datasets cannot be directly compared, making it difficult to assess their accuracy. Rather than investing time and money in the collection of new data, this project builds on an existing resource: 9 million survey responses from 175 countries around the world. By using Artificial Intelligence (AI) methods to compare these surveys to official statistics and generate improved diversity estimates, other researchers will be able to apply this method in other areas like health and inequality where government statistics are missing or questionable. Further, this project will make our results directly comparable to existing datasets. These data and methods will be made available through a user-friendly online portal, where scientists, policy-makers, and members of the public will be able to explore the data. They will also be able to create their own datasets and use visualization tools to see how the world's demographics are changing and consider what this means for our future.<br/><br/>Existing estimates of ethnic, religious, and linguistic diversity are correlated cross-sectionally with a number of socio-political and economic outcomes including development, conflict, and social capital. Close examination of these data raises validity concerns: few are based on high-quality official statistics, the majority coming from questionable secondary sources. Further, criteria for group inclusion (i.e., ontologies) are opaque and inconsistently applied. Even where they appear accurate, data are static and aggregated at the country level, although they are often used to explain time-varying and spatially disaggregated outcomes. Ontologies in extant datasets are also incompatible, making comparison and integration difficult. This proposal improves existing measures by applying machine learning methods to compare 9 million responses across 175 countries with a new database of census results. An algorithm will identify survey design features that maximize accuracy, to define a compensatory weighting scheme across these features. The result is a set of survey-based demographic estimates with improved validity, even for countries lacking reliable census data. This method of triangulating surveys and official statistics is generalizable to research areas that use either source and can also inform improved survey design. The project will also develop tools for linking surveys, censuses, and existing datasets based on explicit and transparent decision rules to facilitate their comparison and integration. An online portal will provide access to datasets and code, supporting customized data manipulation and visualization. The methods and tools proposed here -- emphasizing accuracy, transparency, and cross-resource integration -- should serve as a model for future data collection.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832831","Summer 2018 Causal Inference Workshops","DMS","STATISTICS","05/15/2018","05/22/2018","Alexander Volfovsky","NC","Duke University","Standard Grant","Gabor Szekely","04/30/2019","$30,000.00","","alexander.volfovsky@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","MPS","1269","7556","$0.00","This award provides partial support for new researchers to attend two conferences on causal inference in summer 2018: The Atlantic Causal Inference Conference (ACIC) held in Pittsburgh, PA on May 22-23, 2018 and the Causal Workshop at the Conference on Uncertainty in Artificial Intelligence (UAI)  held in Monterey, CA on August 6-10, 2018. Both conferences serve as flagship gatherings of leaders in reasoning about causes of observed effects.  In recent years, causal inference has seen important advances, especially through a dramatic expansion in its theoretical and practical domains. Machine learning methods have focused on ultra-high-dimensional models and scalable stochastic algorithms, whereas more classical causal inference has been guiding policy in complex domains involving economics, social and health sciences, and business. Through such advances, a powerful cross-pollination has emerged as a new set of methodologies promising to deliver more robust data analysis than each field could individually.<br/><br/>The primary purpose of the two conferences is to advance the study of causal inference and to bridge gaps between different communities within causal inference, by promoting interaction and networking among new researchers. At both conferences, the supported participants will present their research via invited and contributed talks or via poster presentations. Since the conferences draw on causal inference communities in statistics, biostatistics, computer science, and other disciplines, these conferences present a great opportunity for new researchers to see the breadth of opportunities for research and collaboration in this area.  More information is available at the conference web sites:<br/><br/>ACIC: https://www.cmu.edu/acic2018/ <br/><br/>UAI: https://sites.google.com/view/causaluai2018<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849087","I-Corps: Mobile, Smart Gait Assessment System","CNS","I-Corps","10/01/2018","09/15/2018","Ou Bai","FL","Florida International University","Standard Grant","Behrooz Shirazi","03/31/2020","$50,000.00","","obai@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is the product of a mobile, smart gait assessment system allowing for continuous gait monitoring outside the clinical settings, which can be used for online, remote clinical management for patients with gait disorders including patients with Parkinson Disease (PD). PD alone affects over one million people in the United States, a number that increases by 60,000 each year. This technology will support the prevention of re-admission by fall tracking and prevention, lower hospital visits and length of stay by allowing at-home monitoring of disease progression, implement monitored wellness programs, differentiate disease types by pattern recognition, and provide timely and efficient clinical management by tracking interventional outcomes. The patient customers will benefit by receiving recommendations on gait and posture habits, leading to a decrease in fall occurrences. Moreover, they will benefit from activity monitoring by receiving suggestions on exercise routines, which have been shown to increase health and fitness. On the other hand, clinics and hospitals will benefit from reducing the volume of patients at their facilities, and insurance agencies will benefit by decreasing patients' length of stay.  <br/><br/>This I-Corps project proposes a system capable of detecting asymmetry between kinetic and kinematic gait parameters, which consists of embedded inertial measurement units and insoles with piezoresistive pressure sensor arrays. With machine learning algorithms, the system can detect and classify the disease progression, which will support treatment modification along with normal disease course and symptomatic changes. This smart, non-invasive, non-pharmaceutical approach for disease prognosis could also lead to an increase in participation in clinical trials, as the low-risk involvement required could encourage persons with and without PD to participate. The developed technology allows for continuous gait recordings outside the clinical setting. This smart, wireless gait monitoring system has the potential of decreasing the burden associated with the numerous clinic visits. The intellectual merits lie in the use of emerging technologies, including cloud computing and artificial intelligence for feature extraction, classification, and rare-event detection. These features will support medical professionals' decision-making regarding the course of treatments from its early stages without having to add more effort to their already busy schedules. In addition, the designed Internet-of-Things (IoT) architecture will allow mobile health monitoring, including treatment compliance and categorization for treatment improvement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817120","CSR: Small: Redshift: An Operating System for Pervasive Hardware Acceleration","CNS","CSR-Computer Systems Research","06/15/2018","07/08/2019","Anton Burtsev","CA","University of California-Irvine","Standard Grant","Matt Mutka","05/31/2021","$468,000.00","","aburtsev@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7354","7923, 9251","$0.00","In contrast to today's systems centered around general-purpose processors also known as central processing units (CPUs), the next generation of high-performance computers will inherently rely on diverse, heterogeneous hardware ranging from many-core processors like Intel Xeon Phi that contains up to 72 processor cores and graphical processing units (GPUs) to specialized hardware accelerators, like specialized machine-learning chips and field-programmable gate arrays (FPGAs) re-programmed on demand for a specific task. In a hardware-accelerated environment that consists of many diverse execution units, the execution of a program is no longer a conventional thread tied to a single CPU, but a graph of small computations scheduled on a set of hardware accelerators each implementing a part of the program logic. Redshift is a new operating system for developing applications that leverage performance of a heterogeneous hardware-accelerated system.<br/><br/>At the core of Redshift is a dataflow programming model that enables execution of commodity programs on a network of heterogeneous hardware execution units with only minimal modifications. Redshift implements programs as collections of asynchronous invocations that transparently move execution between hardware functions. A novel runtime maps computations to execution units, balances load among them, and scales the hardware graph of computation in response to load. <br/><br/>The problem of efficient computing environments has large impacts on society as a whole: a rapidly growing share of scientific discoveries is done in silico. As this trend continues, we as a society depend on computational capacity of modern computers. Redshift will provide a foundation for developing the next generation of computation intensive applications in the areas of artificial intelligence, e.g., speech and image recognition, personal digital assistance, big-data analytical applications, genomic and personalized medicine, drug discovery, and many more.<br/><br/>Redshift will be implemented as a practical layer compatible with de facto research and industry standard Linux operating system, and will be open source, directly benefiting the broader community. To make our approach widely available, Redshift will be hosted in a publicly-readable repository, and will be available to anyone (https://github.com/mars-research/redshift). Additionally, as parts of Redshift will be developed in the openly-available National Science Foundation-funded CloudLab and Emulab testbeds, Redshift will be available for a test drive via a CloudLab profile (https://www.cloudlab.us/p/redshift/testdrive) that automatically instantiates a collection of nodes running Redshift).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826506","Designing Materials to Revolutionize and Engineer our Future March 26th & 27th 2018 Meeting","DMR","DMREF","02/15/2018","02/13/2018","William Bentley","MD","University of Maryland College Park","Standard Grant","John Schlueter","01/31/2019","$99,997.00","Gregory Payne","bentley@eng.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","MPS","8292","054Z, 7556, 8400","$0.00","Non-technical description: This grant funds a workshop to bring together Materials Genome Initiative (MGI) grantees and program managers from the National Science Foundation (NSF) and the Department of Energy (DOE), along with those from other federal agencies including the National Institute of Standards and Technology (NIST) and the Department of Defense (DoD). As NSF's response to the President's Materials Genome Initiative, the Designing Materials to Revolutionize and Engineer our Future (DMREF) program seeks to foster tight collaborations between materials researchers in experiment, theory, and computation. These collaborations are founded on highly iterative feedback loops in which experimental results directly inform theory and computation, and vice versa, with the goal of accelerating the discovery and development of new materials. The workshop provides researchers a forum to share their research results and discuss cross-cutting topics related to establishing and sustaining research collaborations, managing digital data, and supporting long-term simulation software development. <br/><br/>Technical description: Researchers that are participating in Material Genome Initiative projects provide leadership to the broader materials research community by implementing strategies to achieve the goals set out by this Initiative, including reducing both the cost and time it takes to bring a new material to market. The goals of the workshop are to: 1) Identify successes of the MGI, 2) Promote data-driven research, artificial intelligence, and machine learning for materials discovery and development, 3) Transition fundamental research along the Materials Development Continuum toward eventual deployment, and 4) Identify mechanisms for MGI principal investigators (PIs) to partner with those at other laboratories. Overarching themes identified through presentations, break-out sessions, and discussions will be presented in a report for dissemination to the broader materials research community. MGI projects are developing tools that are made available to aid research of the larger community and this workshop presents the most effective ways to implement this task. The workshop, which will be held in College Park, Maryland on March 26-27, 2018, follows in a series of successful workshops, the latest being held in Bethesda, Maryland January 11-12, 2016."
"1747365","SBIR Phase I:  Grading 10x Faster with AI Assistance","IIP","SMALL BUSINESS PHASE I","01/01/2018","12/20/2017","Sergey Karayev","CA","Gradescope Inc","Standard Grant","Rajesh Mehta","06/30/2018","$224,700.00","","sergeyk@gradescope.com","2030 Addison St Suite 500","Berkeley","CA","947042658","7029857442","ENG","5371","5371, 8031","$0.00","This SBIR Phase I project will show feasibility of a method for instant and accurate grading of student answers to a previously unseen question, after observing an instructor grade no more than 10% of the answers to that question. This project will use instructor-defined grading rubrics, which ensure consistency and provide helpful feedback to the student. Initial work will evaluate the method on multiple choice and fill-in-the-blank questions from a variety of Science, Technology, Engineering, and Math (STEM) subjects. The same method will be general enough to allow extension to short-answer and diagram questions. This work is important because although constructed-response questions are far more effective at assessing student knowledge and guiding student learning, they are significantly more time-consuming and onerous to grade than multiple-choice questions, and are therefore underused in many courses. Giving instructors the ability to grade constructed-response questions 10 times faster will increase the prevalence of this type of assessment and thereby improve STEM education outcomes. It will also increase educator effectiveness, because time that instructors currently spend grading can be better spent in personalized interaction with students.<br/><br/>This project will apply and extend recently developed deep neural network methods for few-shot learning to the task of using a small sample of graded student answers to automatically grade the rest. Deep neural networks have recently emerged as the best approach to most machine learning problems. However, unlike humans, who can often learn a new concept after just a few examples, deep neural networks typically need thousands of examples to do the same. Recent developments have enabled deep neural networks to learn new concepts from just a few examples, after seeing millions of examples of other concepts. The first objective of this project is to evaluate these methods using millions of answers to a range of multiple-choice and fill-in-the-blank questions from a diverse set of STEM subjects graded on an existing service. This few-shot learning problem is significantly more complex than problems studied in the research literature. The second objective is to extend these methods, and to develop new methods, in order to reach human-level accuracy after observing the grading of no more than 10% of student answers. The last objective is to ensure that the proposed system is useful to real instructors by implementing a prototype user interface."
"1812431","New Directions in High Energy Nuclear Physics","PHY","NUCLEAR THEORY","07/01/2018","07/27/2020","Rainer Fries","TX","Texas A&M University","Continuing Grant","Bogdan Mihaila","06/30/2021","$349,945.00","","rjfries@comp.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1285","075Z, 7569","$0.00","Among the four fundamental forces in nature the strong nuclear force stands out for being the least understood one. It drives many processes in the universe that are crucial for our existence. One particularly important aspect of the strong nuclear force is the existence of quark gluon plasma. If ordinary matter is heated up to temperatures of about 1,000,000,000,000 degrees, hotter than the core of the sun, atoms and molecules cease to exist and even protons and neutrons inside atomic nuclei melt. The remaining primordial soup of quarks and gluons filled the very early universe. We can recreate quark gluon plasma in our largest particle colliders by smashing heavy nuclei into each other. Experimental programs at the Large Hadron Collider in Europe and the Relativistic Heavy Ion Collider in the USA study the properties of quark gluon plasma. This project will support research that will improve our understanding of the formation and properties of quark gluon plasma in nuclear collisions. The PI and his collaborators will use computer simulations and advanced statistical methods to reach this goal. Funding is provided to support training for a graduate students and junior scientists in nuclear science.<br/><br/>Quark gluon plasma in nuclear collisions emerges from the highly complex gluon fields that are initially created in nuclear collisions. The PI and his group will investigate these fields and their properties by studying how angular momentum of the droplets of quark gluon plasma is related to the initial angular momentum of the colliding nuclei. The same gluon fields will also be studied through their interaction with fast quarks. The PI and his group will be able to use the JETSCAPE framework for large scale computational simulations of nuclear collisions. They will study various aspects of jets and heavy quarks being quenched in quark gluon plasma. They will then extract properties of quark gluon plasma using multiple constraints. For example,  the strength of quenching of quarks and heavy quarks, and the viscosity of quark gluon plasma can be independently measured and are mutually related. Testing these relations will be an important step towards a deeper understanding of the dynamics of quark gluon plasma. The PI and his group will also use advanced statistical methods and machine learning applied to data to understand the mechanisms of hadron formation from quarks and gluons. These results will be used to improve the state-of-the-art modeling of the hadronization process that is a crucial input to many calculations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763307","AF: Medium: Collaborative Research: Foundations of Fair Data Analysis","CCF","Special Projects - CCF","07/01/2018","06/12/2018","AARON ROTH","PA","University of Pennsylvania","Continuing Grant","Tracy Kimbrel","06/30/2022","$439,621.00","Rakesh Vohra, Sampath Kannan","aaroth@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","2878","075Z, 7924, 7926","$0.00","Machine learning algorithms increasingly make or inform critical decisions that affect peoples' every day lives.  For instance, algorithms make decisions pertaining to hiring, college admissions, credit card and mortgage approvals, sentencing and parole of the incarcerated, first-responder deployment, and what advertisements and search results a user sees on the internet.  An attractive feature is that these algorithms can efficiently process large amounts of data in making these decisions, thus hopefully improving economic and social efficiency.  Because such decisions are so consequential, their fairness has become a matter of increasing concern.  It has been argued that automation, by removing the human element, guarantees fairness, but this is not so -- several empirical studies have demonstrated that automation is no panacea.  Further, the reasons for unfairness and discrimination can be complex and non-obvious.  This project will study the frictions that may cause unfairness in algorithmic decision making, and the costs of mitigating unfairness -- that is, quantitative trade-offs between fairness and other desiderata, including accuracy, computational efficiency, and economic efficiency.<br/><br/>Specifically, this project will study frictions to fairness arising from several factors.  There may not be sufficient data about minority populations.  There can be feedback loops arising from the fact that observations can only be made on an individual if a risky action is taken, e.g., the person is granted a loan, or hired.  Decision makers can be myopic, choosing to maximize short-term gains rather than exploring riskier options that may pay off in the long run.  Economic frictions include self-confirming equilibria---differing subjective perceptions of opportunities leading to choices by individuals and communities which sustain those perceptions, and competition among classifiers (for example, credit agencies) leading to less accurate qualifiers in equilibrium.  Finally, the problem of finding fair and accurate classifiers can be computationally intractable.  This project will seek ways to mitigate the unfairness arising from these frictions.  It will study the cost of incentivizing myopic agents to explore and examine the short-term costs of such incentives, and their long-term impact on fairness.  It will also seek to design computationally tractable classifiers that achieve provably good approximations for fairness and accuracy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806630","Advanced LIGO Search for the Stochastic Gravitational Wave Background","PHY","LIGO RESEARCH SUPPORT","08/15/2018","06/08/2020","Vuk Mandic","MN","University of Minnesota-Twin Cities","Continuing Grant","Pedro Marronetti","07/31/2021","$396,000.00","","vuk@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1252","075Z","$0.00","Recent observation by the Advanced Laser Interferometer Gravitational-wave Observatory (aLIGO) and by Advanced Virgo (aVirgo) detectors of mergers of binary black hole and binary neutron star systems have opened the field of multi-messenger astrophysics. They have sparked a very broad range of studies, from new tests of General Relativity and the measurement of the Hubble constant to constraints on the equation of state in neutron stars and studies of the r-process of heavy element production in the binary neutron star merger. Adding the gravitational wave signals from all such binaries in the universe leads to a stochastic gravitational wave background (SGWB). The above discoveries have enabled a robust estimate of this background, indicating that it is within reach of the upcoming observation runs of aLIGO and aVirgo. The SGWB may well be the next new type of gravitational-wave signal to be discovered by these detectors, with the discovery coming potentially as early as 2019.  This project aims to measure (and detect) the SGWB using data from upcoming observation runs of aLIGO and aVirgo, improving the sensitivity by up to 100x relative to the most recent results. The project will support involvement of graduate and undergraduate students in research at the frontier of the nascent field of multi-messenger astrophysics, as well as activities designed to share the excitement of this new field with broader audience.<br/><br/>More specifically, using cross-correlation techniques applied to aLIGO and aVirgo data, the frequency content and temporal structure of the SGWB will be measured. Bayesian parameter estimation framework will then use this information to estimate the contributions of various astrophysical and cosmological SGWB models, as well as to identify and remove environmental contamination, such as due to the Schumann magnetic resonances. The results of this analysis are expected to place stringent constraints on the formation and evolution of compact binary systems, hence illuminating the evolution of the observed structure in the universe. Furthermore, these results will have the potential to constrain cosmological SGWB models, such as inflationary or cosmic (super)string models, and therefore probe the physics of fundamental interactions at very high energies, unachievable in laboratories. Similar cross correlation techniques will also be used to develop new searches for long transient signals, lasting from hours to days or weeks, which are expected in multiple models of neutron stars and are of particular interest for studying the remnants of binary neutron star mergers. If detected, such a signal would provide information about the physics processes driving Gamma Ray Bursts and about the high-density state of nuclear matter in neutron stars. To further improve the sensitivity of these searches, deep machine learning techniques will be used to remove the environmental contamination from the aLIGO and aVirgo gravitational-wave data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806612","Physics at the LHC with CMS","PHY","HEP-High Energy Physics","08/01/2018","07/21/2020","Paul Sheldon","TN","Vanderbilt University","Continuing Grant","Saul Gonzalez","07/31/2021","$1,548,698.00","Will Johns, Alfredo Gurrola","Paul.Sheldon@Vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","MPS","1221","075Z, 7483","$0.00","The Standard Model of particle physics has been a remarkably successful theory, agreeing with several decades of experimental observations involving weak, electromagnetic, and strong interactions. The LHC discovery of the Higgs boson in 2012 was further confirmation of this success. However, the Standard Model remains an incomplete theory as it fails to explain dark matter and it does not provide an explanation for the mass of light neutrinos. Interestingly, these insufficiencies appear to be part of an intriguing overlap in the sciences describing the largest and smallest scales of the universe: proposed extensions of the Standard Model also shed light on our understanding of the evolution of our universe and the dark matter relic density measured by astronomers. This work exploits the rich particle physics program at the Large Hadron Collider at CERN in Geneva, Switzerland to search for dark matter and other physics beyond the Standard Model.<br/><br/>The expanding LHC data set collected by the CMS experiment offers significant opportunity for new discovery, providing greater sensitivity for new physics signatures which would address questions in both particle physics and cosmology. This research program is aimed at questions motivated by the synergy between these two disciplines, for example, by using events produced through vector boson fusion processes to search for dark matter and supersymmetry. In addition, this effort leverages searches for TeV-scale particles using tau lepton reconstruction and identification, which combined with other technicques, enables targeted searches for interesting new physics such as dark matter, compressed spectra SUSY, heavy neutrinos, leptoquarks, and new heavy neutral gauge bosons such as Z-prime. This research is also enabled by hardware operations and development work on the CMS pixel detector and on development of advanced machine learning data analysis techniques. In addition, the group will undertake multiple education and outreach activities, including the national initiative Quarknet, research experiences for Vanderbilt undergraduates, hosting intern days for local secondary school students, and hosting a ""Dark Matter Fest"" for students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1753165","CAREER: Coherent Laser Control for Compact Accelerators","PHY","PLASMA PHYSICS, PHYSICS-BROADEN PARTICIPATION","06/01/2018","06/01/2020","Franklin Dollar","CA","University of California-Irvine","Continuing Grant","Vyacheslav (Slava) Lukin","05/31/2023","$400,000.00","","fdollar@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","MPS","1242, 7621","075Z, 1045, 1062, 1242, 7433, 7621, 8084, 8990","$0.00","This CAREER award will support a study of how to enhance performance of laser-driven particle accelerators by controlling laser properties such as the focal spot quality.  Using techniques that enable the laser light to be manipulated and coherently controlled, high brightness compact accelerators may become achievable on a single tabletop, with immediate applications in medical isotope production and radiography. The research is highly integrated with mentoring, professional development, and outreach, which are employed to train students practical skill sets to prepare them for a range of technical careers. The education plan will utilize culturally relevant frameworks for engaging underrepresented populations, providing hands-on skill set development, and empowering the students themselves to increase visibility of accelerator science as a discipline. <br/> <br/>Femtosecond lasers are ideally suited for use as tabletop particle accelerators since their short pulse duration enables relativistic intensities to be generated at high repetition rates. With sufficient intensity, a laser can redistribute electrons within optical cycles, generating enormous acceleration gradients which can drive advanced accelerators. Though early analytical models showed promise of GeV ions and hundreds of GeV electrons, experiments have yet to produce such beams.  This CAREER award will enable experiments shaping the wavefront and modifying the polarization of femtosecond lasers towards development of practical tabletop accelerators.  Influence of higher order modes for increased charge and decreased beam divergence will be studied, with machine learning providing insight into the respective influence of each mode in these highly nonlinear regimes. Numerical modeling of the interactions will be performed utilizing large data samples and exactly measured beam properties.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763349","AF: Medium: Collaborative Research: Foundations of Fair Data Analysis","CCF","Special Projects - CCF, Algorithmic Foundations","07/01/2018","09/17/2019","Mallesh Pai","TX","William Marsh Rice University","Continuing Grant","Tracy Kimbrel","06/30/2022","$187,345.00","","Mallesh.Pai@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","2878, 7796","075Z, 7924, 7926, 7932","$0.00","Machine learning algorithms increasingly make or inform critical decisions that affect peoples' every day lives. For instance, algorithms make decisions pertaining to hiring, college admissions, credit card and mortgage approvals, sentencing and parole of the incarcerated, first-responder deployment, and what advertisements and search results a user sees on the internet. An attractive feature is that these algorithms can efficiently process large amounts of data in making these decisions, thus hopefully improving economic and social efficiency. Because such decisions are so consequential, their fairness has become a matter of increasing concern. It has been argued that automation, by removing the human element, guarantees fairness, but this is not so -- several empirical studies have demonstrated that automation is no panacea. Further, the reasons for unfairness and discrimination can be complex and non-obvious. This project will study the frictions that may cause unfairness in algorithmic decision making, and the costs of mitigating unfairness -- that is, quantitative trade-offs between fairness and other desiderata, including accuracy, computational efficiency, and economic efficiency. <br/><br/>Specifically, this project will study frictions to fairness arising from several factors. There may not be sufficient data about minority populations. There can be feedback loops arising from the fact that observations can only be made on an individual if a risky action is taken, e.g., the person is granted a loan, or hired. Decision makers can be myopic, choosing to maximize short-term gains rather than exploring riskier options that may pay off in the long run. Economic frictions include self-confirming equilibria---differing subjective perceptions of opportunities leading to choices by individuals and communities which sustain those perceptions, and competition among classifiers (for example, credit agencies) leading to less accurate qualifiers in equilibrium. Finally, the problem of finding fair and accurate classifiers can be computationally intractable. This project will seek ways to mitigate the unfairness arising from these frictions. It will study the cost of incentivizing myopic agents to explore and examine the short-term costs of such incentives, and their long-term impact on fairness. It will also seek to design computationally tractable classifiers that achieve provably good approximations for fairness and accuracy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816936","SHF: Small: SMT Reasoning for Tensors and Data","CCF","Special Projects - CCF","10/01/2018","06/21/2018","Dejan Jovanovic","CA","SRI International","Standard Grant","Nina Amla","09/30/2021","$499,452.00","","dejan@csl.sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","CSE","2878","075Z, 7923, 8206","$0.00","Our society is increasingly more dependent on computing systems that include components built from tensor models and rely on them to make critical decisions, like for example airplane controllers built from MATLAB code, or neural networks in self-driving cars. The surge in size and complexity of these systems has made their development, testing, and verification extremely costly and time consuming. To address this challenge, this project develops novel automated reasoning techniques that can support the development cycle of tensor-based systems, at scale, from design to verification, with the potential to increase both performance and reliability of the resulting systems, while broadening the applicability of symbolic reasoning to new domains.<br/><br/>The goal of the project is to develop new automated reasoning and symbolic optimization techniques needed for reasoning about complex systems that rely on tensors and data as the underlying model of<br/>computation. The current generation of automated reasoning techniques focuses solely on scalar domains, and they are inadequate for reasoning about large tensor systems with data. This project develops new reasoning techniques, in the context of satisfiability modulo theories, that can operate at the level of tensors. The key novel ideas explored in this project include high-level modeling and representation of tensor models, data-aware reasoning and optimization techniques for both linear and non-linear tensor models, and combination of numerical optimization and symbolic reasoning techniques. The project integrates the new techniques into machine learning and software analysis tools and evaluates their effectiveness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816870","CSR: Small: Development of Distributed Neural Processing Electronics for Whole-Body Computing and Biomedical Sensor Fusion","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2018","06/11/2020","Jie Gu","IL","Northwestern University","Standard Grant","Erik Brunvand","09/30/2022","$507,800.00","","jgu@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","1714, 7354","7354, 7923, 9251","$0.00","Wearable electronics and health assistive devices have become commonly used for daily activity tracking and medical care.   However, the existing devices can no longer satisfy the growing demand on computing power and energy efficiency especially when a large number of sensors are utilized to improve the performance of such devices. This work will develop a new generation of wearable biomedical devices that utilize the emerging computing architecture and state-of-art networking techniques to boost the data processing capability of such devices. The developed techniques will be demonstrated in the state-of-art prostheses for rehabilitation application.  <br/> <br/>Several key intellectual merits are delivered.  First of all, this project will develop novel distributed neuromorphic computing architecture featuring a scalable, reconfigurable, and multi-chip neural network system for sensor fusion enabled biomedical devices.  Heterogeneous sensor data from physiological signals will be efficiently processed through reconfigurable neural processors.  Moreover, to form a highly efficient network and remove routing congestions, body channel communication will be developed and integrated with the neural processors to achieve a high-bandwidth, low-latency, inter-connected network around the human body.  <br/><br/>The work brings significant advancement and benefits to modern biomedical devices by creating highly efficient sensing, computing and networking solutions.  The proposed development promotes technology fusion across fields of computer, electrical, biomedical engineering. The sub-tasks of the project will be delivered to undergraduate and graduate classrooms for hands-on experience of integrated circuits design, biomedical instrumentation, machine learning techniques.  New courses on computing techniques for sensor fusion enabled biomedical system will be created to disseminate the learned knowledge to broader audience. <br/><br/>All publications, and experimental data, source codes, design files from this project will be retained on institutional servers for 3 years after the completion of the project.  All publications along with important data, codes, and supporting design files will be made accessible through PI's data repository, http://nu-vlsi.eecs.northwestern.edu/data_repository.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750483","CAREER:Designing Robots that Learn: Closing the Gap Between Machine Learning and Engineering","IIS","Robust Intelligence","06/01/2018","06/20/2019","Byron Boots","GA","Georgia Tech Research Corporation","Continuing Grant","Erion Plaku","05/31/2023","$188,141.00","","bboots@cs.washington.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","1045, 7495","$0.00","Robotics has enjoyed great success when a robot's interaction with its environment can be precisely defined. However, if the models that robots use to predict, plan, and control their behaviors are inaccurate, it can lead to suboptimal or even dangerous behaviors.  Unfortunately, as robots and their environments become more complex, it is increasingly difficult to accurately specify robot behavior.  An alternate approach is to learn the models from experience, but most such approaches need large amounts of data, in a variety of situations, which is practically infeasible to collect.  This project attempts to combine the strengths of hand-crafted, physics-based models and machine learning algorithms to tackle such problems.  Such a combined approach will better position engineers to design robots that can operate in less-structured real-world environments. The project also addresses educational goals related to this vision by providing cross-disciplinary experiences that combine machine learning and robotics. <br/><br/>The goal of this project is to develop new theory and algorithms that close the gap between machine learning and engineering approaches to robotics, which have traditionally been studied separately. While engineering uses physics knowledge to provide interpretability, transparency, and guarantees about the reliability and robustness of engineered systems, machine learning studies data and information, and provides guarantees that focus on the expressivity of models, computational cost, and sample efficiency of learning algorithms.  The current project consists of three research initiatives to bring these disciplines closer together. The first aims to develop new semi-parametric models for robotics that combine parametric physical models and non-parametric statistical models. The second will tackle the problem of learning state space models from data when given incomplete information about dynamics and state. The third will investigate how structural knowledge from engineering can be used to constrain the hypothesis space of nonparametric learning algorithms and deep neural network models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820773","I-Corps: Machine Learning Algorithms and Tools for Analysis and Optimization of Infrastructure","IIP","I-Corps","01/01/2018","01/04/2018","Roman Lubynsky","MA","Massachusetts Institute of Technology","Standard Grant","Pamela McCauley","06/30/2019","$50,000.00","","rml@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to develop algorithmic methods and computational systems that will provide new information, cheaply, quickly, consistently, and at a large scale, to inform investments in building, energy, and green infrastructure for engineering consultancies, construction companies, power utilities, real-estate developers, and cities. Currently, studies for built, energy, and green infrastructure investments are typically made case-by-case and employ infrequent, manual, and expensive audits performed by trained professionals. In addition, the ground-truth data (site surveys and other urban asset databases) used to inform these decisions are siloed across stakeholders and are often expensive and difficult to collect. The ecosystem of simulation and analysis tools is fragmented into a variety of highly complex software packages that require considerable effort and customization. This results in much repeated effort, inconsistent results across different projects, and high costs for surveys and instrumentation.<br/><br/>This I-Corps project will investigate commercial applications of a machine learning system that ingests, aggregates, processes and learns mappings between, remote-sensing, model simulation, and local sensor and survey data on buildings and their environment. The data used will be best-available open source and commercial satellite data, as well as Internet of Things data from buildings and energy infrastructure. The project further develops neural-network based generative machine learning models that can emulate complex physical and engineering systems but are much cheaper and faster to deploy and maintain in practice. This system would allow users to make inferences (physical parameter recovery, forecasts) about built, green, and energy infrastructure and their surroundings (micro-climates) where this data is not available, in a low-cost, large-scale, automated way, removing the need for certain expensive sensor deployments or manual surveys. The system develops a top-down, machine-learning based simulation capability that would allow to perform fast scenario and impact analysis of investments and interventions."
"1819808","SBIR Phase I: Automating Conversational Systems For Enterprise Customer Support","IIP","SMALL BUSINESS PHASE I","07/01/2018","06/19/2018","Yanjia Yao","NY","TOPBOTS Inc","Standard Grant","Peter Atherton","12/31/2019","$224,479.00","","mariya@topbots.com","106 B Warwick Place","Ithaca","NY","148501731","6366751088","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will result from the development of a scalable, enterprise-ready conversational AI platform that enables businesses to automate customer support requests, optimize call routing efforts to get customers to the right human support agent, generate customer self-service resources like smart FAQs, augment human staff with bot capabilities, and enable personalized recommendations to support sales and purchasing decisions for customers. Such technology is beneficial to any enterprise organization, B2C (business-to-consumer) and B2B (business-to-business), who needs to communicate to customers, employees, and the public. With rising customer service and communication costs, automation is critical for businesses to maintain profitability and competitive advantages. On a societal level, conversational interfaces democratize access to information and resources worldwide. A significant portion of the world's population lives in regions where internet access is poor and device ownership is rare. Many also suffer from physical limitations, such as impaired vision or limited motor control. Accelerating technology access through conversational interfaces such as SMS and voice has already been shown to transform quality of life for disadvantaged groups through applications in telemedicine, personal finance, and education.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project aims to enable human-level conversational ability in machines. Teaching computers to communicate like humans is a critical step to achieving human-level machine intelligence (HLMI), also called artificial general intelligence (AGI). Current state-of-the-art approaches to automated dialog systems use a combination of supervised learning and reinforcement learning implemented with deep neural network architectures. These approaches have shown promise on toy problems in academic research, but have mixed performance in enterprise settings. This Phase 1 project extends current deep learning approaches by developing a repeatable methodology for annotating and preparing enterprise data for use in dialog systems and a scalable, grounded neural network architecture that can reference external knowledge sources with minimal manual engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831263","SBIR Phase II:  Efficient Custom Machine Learning for Embedded Intelligence in the Internet of Things","IIP","SBIR Phase II","09/15/2018","08/29/2019","Kyle Rupnow","IL","Inspirit IoT, Inc.","Standard Grant","Benaiah Schrag","02/28/2021","$909,704.00","","kjrupnow@gmail.com","2510 Hallbeck Dr.","Champaign","IL","618226879","2177786116","ENG","5373","169E, 5373, 8033, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will result in a significant improvement in the performance, power, and cost of deploying machine learning (ML) solutions through horizontal platform technologies that enable many vertical applications. This improvement will accelerate deployment of intelligent systems and improve scalability through localized intelligence. Our technology automates hardware design, implementation, and deployment to Field-Programmable Gate Array (FPGA) platforms. Our initial target verticals: Security and Surveillance, Predictive Maintenance, and Healthcare represent hundreds of billions USD in growth markets for IoT devices and substantially more economic impact through improved efficiency in deployment and operations and reduced societal costs. Improved performance, power consumption and scalability of these key technologies will lead to improved public safety, improved intelligence in home healthcare services, and more efficient manufacturing and energy systems through deployment of Predictive Maintenance technologies on key industrial equipment. Wide deployment of these technologies will lead to substantial energy savings and a corresponding reduction in carbon emissions, reduced economic loss due to negative events, improved scalability and response time to predicted or active negative events, and lower cost in deployment and operations due to low cost, low power, and physically small sensor systems.<br/><br/>The proposed project focuses on design of high performance, energy-efficient platforms for ML applications, and associated design tools and libraries. Neural networks are heavily used for many machine learning problems but optimizing for efficient deployment currently requires extensive trial-and-error for the large design space of options. Our deep neural network (DNN) optimization framework applies bit-width optimizations, weight sharing and pruning automatically to reduce computation and weight storage demands by more than 10X, while analyzing quality of results impact and using fine-tuned retraining to minimize or eliminate accuracy degradation. Our high level synthesis (HLS) tool then translates optimized networks to hardware while applying pipelining, functional unit parallelism, resource sharing, and platform-specific optimizations. Together these tools automate and accelerate the process of analyzing, optimizing and implementing ML for hardware deployment, reducing time and required expertise for hardware design. Our deployment platforms are modular, composable platforms for small, low-cost deployments of audio/video signal processing, feature extraction and classification, systems control (e.g. pan-tilt-zoom cameras), and communications to decision-making or cloud services. We will extend competitive advantages from our Phase I project with features for solutions in the security/surveillance, predictive maintenance, and healthcare verticals, and tight integration of platforms, tools and IP libraries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752934","CAREER: Geometry and Learning for Manifold-Structured Data in 3D and Beyond","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","07/01/2018","06/22/2020","Rongjie Lai","NY","Rensselaer Polytechnic Institute","Continuing Grant","Leland Jameson","06/30/2023","$276,256.00","","lair@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1271, 8048","1045, 8091, 9263","$0.00","With the advance of modern technology and computing power, processing and analyzing of data in three and higher dimensions becomes a ubiquitous task in diverse fields such as medical imaging, computational chemistry, computational biology, social networks and many others. For many problems in practice, data is commonly associated with certain coherent and nonlinear structure. Mathematically, this allows us to model data sets as points sampled on manifolds usually of low dimensions embedded in a high dimensional ambient space. Different from image and signal processing which handle functions on flat domains with well-developed tools for processing and learning, manifold-structured data is far more challenging due to their complicated geometry. For example, the same geometric object can take very different coordinate representations due to the variety of embeddings, transformations or representations (imagine the same human body shape can have different poses as its nearly isometric embedding ambiguities). These ambiguities form an infinite dimensional isometric group and make higher-level tasks in manifold-structured data analysis and learning even more challenging. To overcome these challenges, it becomes increasingly important to develop new tools in both theoretical and computational point of views for processing manifold structured data. <br/><br/>This project proposes to investigate analyzing and learning of manifold-structured data by bridging connection from geometric partial differential equations (PDEs) and learning theory to intrinsic data analysis. The major objectives of this project contain three components. The first part is to investigate a framework of geometric-PDEs-based methods to a data structure for manifolds represented as incomplete inter-point distance. The second part is to overcome the challenge of poor performance using intrinsic descriptors to handling not nearly isometric manifolds. In the third part, a new method of defining geometric convolution on manifolds is considered. This provides a building block of constructing the proposed geometric convolutional neural network for conducting deep learning on manifold-structured data. By collaborating with biomedical engineers, applications such as human brain mappings will also be explored. The new methodologies and research findings resulting from the proposed work will lead to new ways of tackling problems in manifold-structured data analysis and will be integrated into my future teaching and course projects in appropriate ways. The education plan is to provide unique opportunities to train undergraduate and graduate students interested in exploring geometry and learning on manifold-structured data, and to reach out the general public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746824","STTR Phase I:  Deep Transfer Learning Enabled Machine Vision Inspection and Its Applications in Exploration Geophysics","IIP","STTR Phase I","01/01/2018","12/27/2017","Wenyi Hu","TX","Advanced Geophysical Technology Inc.","Standard Grant","Peter Atherton","06/30/2019","$225,000.00","Xuqing Wu","wenyi.hu@agtgeo.com","14100 Southwest Freeway","Sugar Land","TX","774784566","2818886789","ENG","1505","1505, 8032","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project will result from a direct benefit to the energy sector of the U.S. economy, since seismic exploration will play an increasingly important role in meeting increasing energy demands and maintaining healthy oil and gas output. The goal of this project is to develop a software package for automated pattern recognition that can be used by seismic processing companies to automatically pick geological features from seismic data. Seismic data volumes have grown exponentially over the last three decades as the seismic exploration industry increases its survey coverage. Manual picking and geological pattern identification jobs, which depend on visual inspection, are labor intensive and cannot keep up with the growth in data generated by seismic surveys. In this project, the company will develop a machine vision enabled picking and identification tool trained by a deep learning network. Lessons learned in training an efficient deep learning network for pattern recognition have wide applications in other areas such as medical image analysis. This project will support the training of both graduate and undergraduate students in the areas of seismic exploration, machine learning and high-performance computing. <br/><br/>This Small Business Technology Transfer (STTR) Phase I project aims to develop a deep learning network model to recognize unique patterns embedded in seismic data, which patterns are characteristic of the associated geological structures. Specifically, the project will demonstrate the feasibility of delivering a machine vision enabled inspection tool to relieve domain experts from labor-intensive visual examination activities. Various automatic picking approaches currently exist, with differing degrees of success. Nonetheless, the uncertainty involved in these tools is still too high for them to be widely adopted by the industry. Recent advances in the area of deep learning make it possible to surpass human-level visual recognition performance in some applications. High performance deep learning network models, however, require a large amount of high quality training data. In this project, the company proposes to use a novel self-taught deep transfer learning approach to overcome the data shortage problem resulting from proprietary rights associated with the data. The new training workflow is adaptive to the domain of seismic data processing. It will also minimize the training effort and deliver a robust system with guaranteed performance for new and unseen datasets."
"1839234","TRIPODS+X:RES: Collaborative Research: Creating Inference from Machine Learned and Science Based Generative Models","DMS","TRIPODS Transdisciplinary Rese","10/01/2018","09/10/2018","Joshua Agar","PA","Lehigh University","Standard Grant","A. Funda Ergun","09/30/2021","$199,953.00","","joshagar@gmail.com","Alumni Building 27","Bethlehem","PA","180153005","6107583021","MPS","041Y","047Z, 062Z, 8037","$0.00","In many scientific disciplines computational simulations are used to enhance our understanding of physical processes of complex systems. In all such simulations simplifications are required to make the problem tractable, limiting the scope of the questions that can be addressed. Generally, computational simulations of large systems with many interacting components, based on the governing physics, requires complex and time consuming computations. This project will apply deep learning neural networks (NN) with geometric transformations based on the physics of the system to accurately approximate traditional physics-based computational simulations in a highly efficient manner. The increased efficiency imparted by the  NN model will facilitate the asking of scientific questions which are currently computationally intractable. While the proposed work will focus on using this method to discover new strain-induced polar phases and phase competition, and to understand the large-scale structure in the universe, the concepts developed in this work can be applied to computational simulations in other scientific disciplines.<br/><br/>The proposed work will focus on the development of foundational data science methods and the application of these methods to augment computationally-expensive science-based generative models in a way that is principled and efficient, thereby enabling improved data-driven scientific inference. The work will place specific emphasis on the design of neural network models, which through physically-significant domain architectures can approximate N-body and highly-correlated phenomena with minimal loss of information. The work will develop tools to guide the discovery and experimental synthesis of new strain-induced polar phases and phase competition, which exhibit enhanced electromechanical responses; and it will expand our simulation capabilities and understanding of the large-scale structure in the universe. Ultimately, this work will provide both domain specific advances, as well as a framework for other domain areas to augment computationally intensive, highly-correlated, N-body problems with data-driven models, which respect the physics of the problem and lead to increased computational efficiency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839217","TRIPODS+X:RES: Collaborative Research: Creating Inference from Machine Learned and Science Based Generative Models","DMS","TRIPODS Transdisciplinary Rese, DMR SHORT TERM SUPPORT","10/01/2018","09/10/2018","Uros Seljak","CA","University of California-Berkeley","Standard Grant","A. Funda Ergun","09/30/2021","$399,431.00","Michael Mahoney","useljak@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","041Y, 1712","047Z, 062Z, 8037","$0.00","In many scientific disciplines computational simulations are used to enhance our understanding of physical processes of complex systems. In all such simulations simplifications are required to make the problem tractable, limiting the scope of the questions that can be addressed. Generally, computational simulations of large systems with many interacting components, based on the governing physics, requires complex and time consuming computations. This project will apply deep learning neural networks (NN) with geometric transformations based on the physics of the system to accurately approximate traditional physics-based computational simulations in a highly efficient manner. The increased efficiency imparted by the  NN model will facilitate the asking of scientific questions which are currently computationally intractable. While the proposed work will focus on using this method to discover new strain-induced polar phases and phase competition, and to understand the large-scale structure in the universe, the concepts developed in this work can be applied to computational simulations in other scientific disciplines.<br/><br/>The proposed work will focus on the development of foundational data science methods and the application of these methods to augment computationally-expensive science-based generative models in a way that is principled and efficient, thereby enabling improved data-driven scientific inference. The work will place specific emphasis on the design of neural network models, which through physically-significant domain architectures can approximate N-body and highly-correlated phenomena with minimal loss of information. The work will develop tools to guide the discovery and experimental synthesis of new strain-induced polar phases and phase competition, which exhibit enhanced electromechanical responses; and it will expand our simulation capabilities and understanding of the large-scale structure in the universe. Ultimately, this work will provide both domain specific advances, as well as a framework for other domain areas to augment computationally intensive, highly-correlated, N-body problems with data-driven models, which respect the physics of the problem and lead to increased computational efficiency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804540","Collaborative Proposal: Understanding Motor Cortical Organization through Engineering Innovation to TMS-Based Brain Mapping","CBET","Engineering of Biomed Systems","09/01/2018","11/27/2019","Wasim Malik","MA","Massachusetts General Hospital","Standard Grant","Aleksandr Simonian","08/31/2021","$300,000.00","","wmalik@partners.org","Research Management","Somerville","MA","021451446","8572821670","ENG","5345","8089, 8091","$0.00","This project addresses a question that has vexed scientists for more than a century: how does the motor cortex, i.e. the part of the brain where nerve impulses initiate voluntary muscular activity, represent and coordinate multiple muscles in order to produce a vast range of movements? To answer this question, this project will harness the unique strengths of non-invasive, navigated, transcranial magnetic stimulation (TMS) mapping to establish causal links between brain physiology and behavior. TMS is achieved by placing a coil of wires near the scalp, which when activated with an electrical current will create a magnetic field across the scalp and skull.  TMS is the only non-invasive method available to stimulate the brain as effectively as invasive stimulation for mapping the motor cortex. However, innovations in three areas are critically needed to use TMS-based motor mapping to understand multi-muscle physiology and control: 1) drastically improving the efficiency, efficacy and reliability of the TMS-based motor cortex mapping processes, 2) characterizing and validating TMS-based mapping as a probe for understanding the relationship between multi-muscle activation and voluntary movement, and 3) applying a neural network computational method to improve understanding of motor control and organization. Enhanced understanding of motor cortex physiology through TMS mapping of motor representations has the potential to better map the brain in applications such as surgical removal of tumors, assessing brain injury due to concussions or stroke, and identifying cortical networks needed for successful brain-machine interactions for controlling prostheses. Students involved with this project will be trained to address multidisciplinary challenges at the intersection of neuroscience, non-invasive brain stimulation, software design, control theory, machine-learning, statistical signal processing, data dimensionality reduction and visualization. Partnership with Boston-based leaders in the technology industry will provide state-of-the-art training to undergraduate, graduate, and post-graduate trainees. Through cooperative educational programming at Northeastern University and internships with Mass General Hospital, STEM-based learning opportunities will be provided for middle- and high-school students, inspiring a diverse body of students to pursue STEM careers. To promote STEM careers and demonstrate impact, the team will reach out to local venues that promote public awareness and appreciation of science, such as science fairs and the Boston Museum of Science.<br/><br/>The goal of this collaborative project is to develop a deeper mechanistic understanding of the role of the motor cortex (M1) in controlling single muscles and synergies in producing complex movements. This will be accomplished by developing several innovations in the use of non-invasive transcranial magnetic stimulation (TMS) to map the spatial distribution of synergies and single muscles. Transformative computational advances will be used to extract more accurate information about brain interaction with other physiological systems outside the motor domain and increase the rigor of analysis and data visualization to enhance interpretability, and repeatability. An enhanced understanding of corticomotor organization of complex movement will pave the way to studying motor system development across the lifespan, the basis of human performance enhancement, and the basis and characterization of neuromotor diseases. The research plan is organized under 3 aims. AIM 1 is to accelerate acquisition of TMS-based maps by developing an active learning process based on a Gaussian Process Model (GPM) of Muscle Evoked Potentials (MEPs) as a function of 2D spatial coordinates on the scalp. The developed Active-GMP learning algorithm is expected to speed up the mapping process by diverting time spent on loci with null data to loci where the model needs more samples to improve certainty. The efficacy and the accuracy of the new algorithm will be compared to three existing alternatives. AIM 2 is to test the behavioral relevance of synergies derived from human multi-muscle TMS mapping, i.e., to biologically validate the technical methods developed in Aim 1. Specifically, TMS and Voluntary (VOL) EMG data will be collected from 16 hand-arm muscles in healthy participants while subjects mimic hand postures for static letters and numbers of the American Sign Language alphabet. Non-negative matrix factorization-extracted synergies from VOL data and TMS data will be compared to determine if the TMS-elicited synergies match those utilized during movement production and if the adaptive Active-GMP and user-guided approaches more closely match synergies derived from VOL data compared to other approaches. AIM 3 is to develop generative and inverse topographic imaging models that allow forward modeling of M1 control and reverse mapping of M1 organization, respectively, of muscles and synergies. Hybrid models combining subject-specific FE modeling of TMS-induced cortical electric fields with neural network models trained to predict evoked muscle responses will be used to answer key questions: Q1) Are synergies dominant features of motor control? Q2) Do direct M1 motorneuron projections augment a synergy model of control? and Q3) Are muscles and synergies discretely organized in M1?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815287","SHF: Small: Open-domain, Data-driven Code Synthesis from Natural Language","CCF","Software & Hardware Foundation","10/01/2018","05/22/2018","Graham Neubig","PA","Carnegie-Mellon University","Standard Grant","Sol Greenspan","09/30/2021","$499,726.00","Bogdan Vasilescu","gneubig@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7944","$0.00","One of the major hurdles in programming is turning ideas into code; all programmers, even experts, frequently reach points in a program where they know what they want to do but cannot easily turn it into a concrete implementation. In this case, it is common to turn to the web, e.g. enter a natural language query, search, browse results, copy-and-paste appropriate code, and modify it to the desired shape. However, this process is still time-consuming. This research aims to automate and enhance this process, by creating new data-driven methods for code synthesis from natural language, which allow developers to go directly from natural language description to code. Specifically, this project's goal is to bring code synthesis to the open domain, moving from highly engineered methods that work on only a single programming language or task, to methods that have the flexibility and scalability to answer most of the questions asked by programmers, in many different programming languages. The intellectual merit of this cross-disciplinary project lies in its potential to contribute to software engineering through the examination of developer's interaction with natural language productivity tools, and its potential to contribute to natural language processing through new models to understand procedural texts. This project will have broader impact through the development of tools and data linking together programs and natural language, potential to improve STEM education by lowering the barriers to programming, and training of graduate and undergraduate research assistants who will be able to straddle and act as bridges between the fields of natural language processing and software engineering.<br/><br/>There are three technical pillars to the work. First, it will focus on methods to mine data consisting of natural language and corresponding code at scale, necessary for training. The mining will be performed over existing online data sources, such as community question answering sites (Stack Overflow) and open-source software repositories (GitHub), using machine learning models that consider both content matches and available meta-data, and crowd-sourcing-based verification of the extracted data. Second, the project will develop code synthesis methods that have the flexibility to handle the wide variety of expressions expected across a variety of software projects and developer needs. This will be done by developing models using neural networks, which have recently shown impressive ability to interpret a wide variety of expressions in other natural language processing tasks. We will expand these models to condition on project context, which will ensure handling of the various constraints necessary to create well-formed programs and allow for adaptation to project-specific conventions and needs. Third, the project will develop methods for learning and improving the models from developer behavior, by feeding back corrections to the generated code into the system and learning from the differences between the pre- and post-correction code. These methods will all be integrated into developer support tools that can be used in a development environment, or through an online API. The utility of these methods will be examined in both controlled and in-the-wild studies. Controlled studies will examine the subjective accuracy of the mined data and generated code, as well as the effect of the tools on the efficiency and ease of development, for programmers from novice to expert level. This project will also create and release tools for general consumption, solicit feedback from a wide variety of developers, and examine how developers use the proposed tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804550","Collaborative Proposal: Understanding Motor Cortical Organization through Engineering Innovation to TMS-Based Brain Mapping","CBET","Engineering of Biomed Systems","09/01/2018","08/28/2018","Eugene Tunik","MA","Northeastern University","Standard Grant","Aleksandr Simonian","08/31/2021","$300,000.00","Dana Brooks, Deniz Erdogmus","e.tunik@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","ENG","5345","8089, 8091","$0.00","This project addresses a question that has vexed scientists for more than a century: how does the motor cortex (the part of the brain where nerve impulses initiate voluntary muscular activity) represent and coordinate multiple muscles in order to produce a vast range of movements? To answer this question, this project will harness the unique strengths of non-invasive, navigated, transcranial magnetic stimulation (TMS) mapping to establish causal links between brain physiology and behavior. TMS is achieved by placing a coil of wires near the scalp, which when activated with an electrical current will create a magnetic field across the scalp and skull to stimulate the brain.  TMS is the only non-invasive method available to stimulate the brain like invasive stimulation. However, to use TMS-based motor mapping to understand multi-muscle physiology and control, innovations in three areas are critically needed: 1) drastically improving the efficiency, efficacy and reliability of the TMS-based motor cortex mapping processes, 2) characterizing and validating TMS-based mapping as a probe for understanding the relationship between multi-muscle activation and voluntary movement, and 3) applying a neural network computational method to improve understanding of motor control and organization. Enhanced understanding of motor cortex physiology through TMS mapping of motor representations has the potential to better map the brain in applications such as surgical removal of tumors, assessing brain injury due to concussions or stroke, and identifying cortical networks needed for successful brain-machine interactions for controlling prostheses. Students involved with this project will be trained to address multidisciplinary challenges at the intersection of neuroscience, non-invasive brain stimulation, software design, control theory, machine-learning, statistical signal processing, data dimensionality reduction and visualization. Partnership with Boston-based leaders in the technology industry will provide state-of-the-art training to undergraduate, graduate, and post-graduate trainees. Through cooperative educational programming at Northeastern University and internships with Mass General Hospital, STEM-based learning opportunities will be provided for middle- and high-school students, inspiring a diverse body of students to pursue STEM careers. To promote STEM careers and demonstrate impact, the team will reach out to local venues that promote public awareness and appreciation of science, such as science fairs and the Boston Museum of Science.<br/><br/>The goal of this collaborative project is to develop a deeper mechanistic understanding of the role of the motor cortex (M1) in controlling single muscles and synergies in producing complex movements. This will be accomplished by developing several innovations in the use of non-invasive transcranial magnetic stimulation (TMS) to map the spatial distribution of synergies and single muscles. Transformative computational advances will be used to extract more accurate information about brain interaction with other physiological systems outside the motor domain and increase the rigor of analysis and data visualization to enhance interpretability, and repeatability. An enhanced understanding of corticomotor organization of complex movement will pave the way to studying motor system development across the lifespan, the basis of human performance enhancement, and the basis and characterization of neuromotor diseases. The research plan is organized under 3 aims. AIM 1 is to accelerate acquisition of TMS-based maps by developing an active learning process based on a Gaussian Process Model (GPM) of Muscle Evoked Potentials (MEPs) as a function of 2D spatial coordinates on the scalp. The developed Active-GMP learning algorithm is expected to speed up the mapping process by diverting time spent on loci with null data to loci where the model needs more samples to improve certainty. The efficacy and the accuracy of the new algorithm will be compared to three existing alternatives. AIM 2 is to test the behavioral relevance of synergies derived from human multi-muscle TMS mapping, i.e., to biologically validate the technical methods developed in Aim 1. Specifically, TMS and Voluntary (VOL) EMG data will be collected from 16 hand-arm muscles in healthy participants while subjects mimic hand postures for static letters and numbers of the American Sign Language alphabet. Non-negative matrix factorization-extracted synergies from VOL data and TMS data will be compared to determine if the TMS-elicited synergies match those utilized during movement production and if the adaptive Active-GMP and user-guided approaches more closely match synergies derived from VOL data compared to other approaches. AIM 3 is to develop generative and inverse topographic imaging models that allow forward modeling of M1 control and reverse mapping of M1 organization, respectively, of muscles and synergies. Hybrid models combining subject-specific FE modeling of TMS-induced cortical electric fields with neural network models trained to predict evoked muscle responses will be used to answer key questions: Q1) Are synergies dominant features of motor control? Q2) Do direct M1 motorneuron projections augment a synergy model of control? and Q3) Are muscles and synergies discretely organized in M1?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820240","SBIR Phase I:  A System for Automating End-to-End Creation of Natural Language Interfaces","IIP","SBIR Phase I","06/15/2018","06/19/2019","Ajay Patel","CA","PLASTICITY INC.","Standard Grant","Peter Atherton","07/31/2019","$224,679.00","","ajay@plasticityai.com","2562 Amethyst Dr","Santa Clara","CA","950511155","4157356358","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to enable more efficient access to information, goods, and services from organizations and businesses through conversational interfaces. With an estimated 33 million voice-activated devices installed in the U.S. by the end of 2017, consumers are beginning to see the benefits of the rise in business services on these devices. Interacting with computers in natural language is more efficient than ever before, and business are urgently seeking to include their services on this new modality. Yet, while advances in machine learning have increased the speed and accuracy with which conversational interfaces are built, today's solutions still result in major bottlenecks in the development process. Addressing these bottlenecks would result in significant savings of time and resources for businesses, in addition to increased convenience for end users.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project advances the development of machine learning and natural language processing technologies that will enable rapid creation of conversational interfaces and increase the efficiency with which individuals interact with computers. Despite major recent advances in conversational interface tooling, current bottlenecks in building robust conversational interfaces still exist, as portions of the development process go from highly-automated to manually-programmed. Instead, the proposed approach will achieve end-to-end automation using an innovative LSTM sequence-to-sequence machine learning model to learn and simulate human behavior on a web page, as well as a new system for open-domain question answering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822768","Teachers are the Learners: Providing Automated Feedback on Classroom Inter-Personal Dynamics","IIS","ECR-EHR Core Research, Cyberlearn & Future Learn Tech","08/01/2018","03/25/2020","Jacob Whitehill","MA","Worcester Polytechnic Institute","Standard Grant","Amy Baylor","07/31/2021","$759,969.00","Erin Ottmar, Lane Harrison","jrwhitehill@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","7980, 8020","063Z, 7218, 8045","$0.00","The quality of teacher-student interactions in school classrooms both predicts and impacts students' learning outcomes. Training teachers to perceive subtle interactions and interpersonal classroom dynamics more accurately can help them to implement more effective interactions in their own classrooms. Contemporary methods of training teachers to understand classroom interactions are based mostly on watching classroom observation videos of other teachers, which have been annotated for different dimensions (""positive climate"", ""teacher sensitivity"", etc.). Only rarely do teachers receive personalized feedback on their own classroom interactions captured in video, and when they do, it is sparse - typically one comment for every 15-minute video segment without any details. This project will automate classroom observations using a system called Automatic Classroom Observation Recognition neural Network (ACORN). This system will integrate multimodal features consisting of facial expression, eye gaze, auditory emotion, speech, and language in order to assess classroom dynamics automatically. As a complement to ACORN, the researchers will also develop a Classroom Observation Interactive Learning System (COILS) that trains teachers to perceive classroom dynamics more precisely.<br/><br/>ACORN will be trained and tested on two coded classroom observation datasets of hundreds of pre-school and elementary school teachers across the USA. Moreover, based on the ACORN prototype, COILS will be developed. COILS will then be evaluated in a study on 50 pre-service teachers. The research questions are: 1) Will the observation training with COILS help them perceive classroom interactions more precisely? 2) How well will ACORN perform vs human coders? and 3) How well can the machine  learned automated subjective activity perform in the new domain of classroom dynamics?  The researchers will also explore different machine learning computational architectures that can utilize modest-sized data sets to accurately learn from multi-modal data. If successful, both ACORN and COILS can be extended from pre-service teachers to train in-service teachers in understanding classroom dynamics to improve their teaching.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810508","Deep Optical Learning Devices and Architectures","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","07/01/2018","05/28/2020","Kelvin Wagner","CO","University of Colorado at Boulder","Standard Grant","Anthony Kuh","06/30/2021","$366,000.00","","kelvin@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","ENG","7607","1653, 9251","$0.00","In this proposal the authors introduce a new concept for an optical device, the optical Rectifying Linear Unit or ReLU, and motivate it as the key enabling technology required for constructing an optical deep learning system. The ReLU neuron was introduced to alleviate the vanishing gradient problem in deep multi-layer neural networks, and it has allowed the training of state-of-the-art deep neural networks.<br/><br/>The authors present an efficient optical implementation of the ReLU in which a bidirectional transmissive optical switch that is controlled by an interferometric differential detector part of the forward propagating field implements both the rectifying linear  forward response but also the derivative needed for gating the backwards propagating error needed for deep learning.  The optical ReLU is the key component necessary to realize the vision of a physically implemented trainable optical machine learning technology.<br/><br/>Such an optical deep learning system has the potential to scale up in performance to a level far in excess even the most optimistic projections for the further development of massively parallel super computers and will use much less energy by harnessing the efficient analog computational capabilities of coherent photons. The research team will design, fabricate, test, and demonstrate large arrays of these new optical ReLU devices using liquid-crystal-on-Silicon smart-pixel technology. A proof-of-concept laboratory demonstration of a self-aligning deep learning optical system will then be developed.<br/><br/>This project will train a graduate student in the fields of machine learning and deep neural networks as well as CMOS device design and fabrication, liquid crystal chemistry and physics, coherent and nonlinear optics, lasers, and computer controlled experimental technology.  Such a cross-disciplinary background will produce a nimble research leader capable of advancing the frontiers of both optical and machine learning science and technology. During this program the PI will continue to develop a Massively Open Online Course (MOOC)  in the areas of Fourier optics and holography that will help to train a new generation of optical scientists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815300","RI: Small: Feature Encoding for Reinforcement Learning","IIS","Robust Intelligence","08/01/2018","09/14/2018","Ronald Parr","NC","Duke University","Continuing Grant","Rebecca Hwa","07/31/2021","$499,968.00","Lawrence Carin","parr@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495","7495, 7923","$0.00","This project focuses on the subfield of machine learning referred to as Reinforcement Learning (RL), in which algorithms or robots learn by trial and error. As with many areas of machine learning, there has been a surge of interest in ""deep learning"" approaches to reinforcement learning, i.e, ""Deep RL.""  Deep learning uses computational models motivated by structures found in the brains of animals. Deep RL has enjoyed some stunning successes, including a recent advance by which a program learned to play the Asian game of Go better than the best human player. Notably, this level of performance was achieved without any human guidance. Given only the rules of the game, the program learned by playing against itself. Although games are intriguing and attention-grabbing, this feat was merely a technology demonstration. Firms are seeking to deploy Deep RL methods to increase the efficiency of their operations across a range of applications such as data center management and robotics. To realize fully the potential of Deep RL, further research is required to make the training process more predictable, reliable, and efficient. Current techniques require massive amounts of training data and computation, and subtle changes in the configuration of the system can cause huge differences in the quality of the results obtained. Thus, even though RL systems can learn autonomously by trial and error, a large amount of human intuition, experience and experimentation may be required to lay the groundwork for these systems to succeed. This proposal seeks to develop new techniques and theory to make high quality deep RL results more widely and easily obtainable. In addition, this proposal will provide opportunities for undergraduates to be involved in research through Duke's Data+ initiative.<br/><br/>The proposed research is partly inspired by past work on feature selection and discovery for reinforcement learning.  Much of that work focused primarily on linear value function approximation.  Its relevance to deep reinforcement learning is that methods such as Deep Q-learning have a linear final layer.  The preceding, nonlinear layers can, therefore, be interpreted as performing feature discovery for what is ultimately a linear value function approximation process.  Sufficient conditions on the features that were specified for successful linear value function approximation in earlier work can now be re-interpreted as an intermediate objective function for the penultimate layer of a deep network. The proposed research aims to achieve the following objectives: 1) Develop a theory of feature construction that explains and informs deep reinforcement learning methods, 2) develop improved approaches to value function approximation that are applicable to deep reinforcement learning, 3) develop improved approaches to policy search that are applicable to deep reinforcement learning, and 4) develop new algorithms for exploration in reinforcement learning that take advantage of learned feature representations, and 5) perform computational experiments demonstrating the efficacy of the new algorithms developed on benchmark problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854400","A Natural Language Based Data Retrieval Engine for Automated Digital Data Extraction for Civil Infrastructure Projects","CMMI","CIS-Civil Infrastructure Syst","09/01/2018","10/22/2018","Hyungseok Jeong","TX","Texas A&M University","Standard Grant","Yueyue Fan","02/29/2020","$166,684.00","","djeong@tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","ENG","1631","029E, 036E, 039E","$0.00","This research project will create new knowledge and resources to significantly enhance the reusability of digital data during the lifecycle of civil infrastructure assets. The rapid development of digital technologies is transforming how civil infrastructure asset data and information is produced, exchanged, and managed throughout its life cycle. Despite growing digital data availability, such data cannot be fully exploited without the ability to infer meaning from the varying data terminologies entered by practitioners. The lack of common understanding of the same data, or similar data given in different terms, preclude data exchange or can lead to extraction of the wrong data and misinterpretation. This research project will leverage the advancements in linguistics and computer science to develop a novel approach that can recognize users' intention from their natural language input and automatically extract the desired data from heterogeneous datasets. The results of this research will benefit the construction industry by accelerating the industry's transition to digital data-based project delivery and asset management. The research will also broaden engineering education by creating advanced course materials both at undergraduate and graduate levels.<br/><br/>Diversity in data terminology creates an important hurdle for computer-to-computer communication, creating a big burden to end users who must perform the role of middleware in digital data exchange. This issue exists throughout the life cycle of a civil infrastructure asset. This project will develop a computational theory and a platform for its implementation to analyze users' plain English data requirements, and automatically match their intention to the data entities in heterogeneous source datasets based on semantic equivalence. To accomplish this goal, the research team will: a) utilize Natural Language Processing and machine learning techniques to recognize user's intention from their natural language queries, b) translate text-based domain knowledge into an extensive civil engineering machine-readable dictionary that defines meanings of technical terms using a text-based automated ontology learning method, c) design an algorithm that finds the most semantic-relevant data entities in digital data sets for a given keyword input, and d) test the performance of the algorithm in terms of its accuracy using civil infrastructure text documents such as technical specifications, design manuals, and guidelines. The research outcomes will provide fundamental tools and resources for other researchers and industry professionals for various text-mining and intelligence-inference systems. It will facilitate seamless data exchange between various proprietary software applications used during the life cycle of civil infrastructure assets, including applications involving design evaluation and selection, digital model construction, and regulation compliance checking."
"1809234","Deep Learning for Passive RF Imaging","ECCS","CCSS-Comms Circuits & Sens Sys","08/15/2018","08/15/2018","Birsen Yazici","NY","Rensselaer Polytechnic Institute","Standard Grant","Lawrence Goldberg","07/31/2021","$360,000.00","","yazici@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","ENG","7564","153E","$0.00","Deep Learning for Passive Radio Frequency Imaging<br/><br/>Deep Learning has enjoyed a spectacular success in wide range of machine learning applications in recent years. However, its potential as a mathematical tool in imaging is yet to be explored. This project develops a Deep Learning based theory, methods and algorithms for passive Radio Frequency (RF) imaging in complex environments using illuminators of opportunity. With the proliferation of wireless communications and broadcasting signals, passive RF imaging has emerged as a potential alternative to active imaging with several advantages. Passive imaging does not require spectrum allocation. It is environmentally friendly and capable of stealth operations. Passive RF systems are lightweight, small, inexpensive, and easy to build and operate, making them suitable for deployment on small uninhabited aerial vehicles (UAVs). These attributes make passive synthetic aperture radar (SAR) technology suitable for a vast array of everyday civilian applications ranging from agriculture to infrastructure monitoring.<br/>While UAV-based passive SAR has the potential to revolutionize imaging across many domains of applications, one of the fundamental bottlenecks in the deployment of these systems is the challenges in image formation. Unlike active imaging, passive SAR image reconstruction involves many unknowns and uncertainties including transmitter locations, transmitted waveforms, multiply scattering and dynamically changing wave propagation environments and limited communication and computational resources. These challenges rule out the use of existing methods such as the usual Fourier transform based or iterative ones.<br/><br/>This project takes a radically different approach to imaging and interprets physics-based modeling and image reconstruction as machine learning tasks. Deep Learning excels in extracting features from data automatically bypassing hand-crafting process of modeling and feature engineering. Conventional approach to imaging involves physics based and statistical modeling, estimation, tomography and optimization. Central to this project is to remove this separation between different domains of expertise and learn and refine models and perform optimization within Deep Learning framework from training data. This takes advantage of Deep Learning's ability to generate complex, non-linear functions to jointly learn wave propagation and prior models and hyperparameters to improve accuracy and robustness. The network designs range from entirely data-driven model-free approaches to ones that are guided by Bayesian inference and optimization theory. The resulting image reconstruction methods are expected to be more robust and accurate with respect to uncertain and dynamically changing environments and unknown imaging parameters and computationally more efficient than state-of-the-art alternatives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839478","I-Corps: Brain Health Monitoring via Phenotyping Electroencephalogram Data","CNS","I-Corps","09/01/2018","08/23/2018","Jimeng Sun","GA","Georgia Tech Research Corporation","Standard Grant","Behrooz Shirazi","08/31/2020","$50,000.00","","jimeng@illinois.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8023","","$0.00","This I-Corps project is based on deep learning algorithms developed using data from patients with sleep disorders and other neurological conditions. We created a unique deep neural network, composed of both a convolutional and recurrent  modules, and trained the model on a unique set of 10,000 clinical polysomnography (PSG) results, the world's largest collection of clinical PSGs assembled to date.<br/><br/>The broader impact/commercial potential of this I-Corps project revolves around its simplicity and accuracy. The benefit for patients is to enable them and their family members to perform accurate home testing with comfort. Meanwhile, physicians would receive real time diagnostic information about their patients, leading to earlier treatment. For insurance providers and hospitals, costs are drastically reduced as outcomes improve and fewer resources and staff are required. Apart from the healthcare industry, our project would also translate well into the general population, where wearables and health tracking devices are becoming commonplace. Being able to review the framework of one's sleep, with an accurate depiction of multiple characteristics including sleep staging, appeals to the consumer.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820469","SBIR Phase I:  A CVML platform for intelligent machines","IIP","SMALL BUSINESS PHASE I","06/15/2018","06/15/2018","BINU MATHEW","CA","KRAENION LABS LLC","Standard Grant","Peter Atherton","11/30/2018","$224,996.00","","BINU@SATVAD.COM","17094 LON RD","LOS GATOS","CA","950330000","6502839142","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is that it will enable small and mid-size robotics and industrial vehicle manufacturers to rapidly deploy computer vision and machine learning in their products and make their machines competitive in the global market place. The project will create a multi-vendor real-time vision stack for industrial machines that enables multiple types of machines to be developed using a uniform software interface. Combined with an app store model, such interfaces will enable a developer community to create application-specific solutions with ease, add features to machines via software and essentially create a new market. When deployed by equipment manufacturers, it will have societal impact by reducing the number of forklift and other industrial and construction machine related accidents, deaths and property damage. There are secondary benefits such as reducing the amount spent on worker compensation. The scientific impact will be the enhanced understanding of principled approaches to stereoscopic depth estimation combined with machine learning based object detectors to create integrated vision systems that function in real-time on commodity hardware.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project addresses the fact that current solutions for autonomous vehicles and machines use expensive LIDARs and RADARs in conjunction with cameras. Such systems are cost-prohibitive and ill-suited for industrial applications that operate in structured warehouse and manufacturing environments rather than highways. Manufacturers can benefit from a cheaper integrated vision-based system where all the necessary algorithms, software and hardware engineering has already been done for them. The intellectual merit of the project lies in achieving the following research goals. 1) Develop an algorithmic approach to stereoscopic depth estimation that combines quick-to-generate classic features (e.g., edges and corners) with machine learning. 2) Combine machine learning based object detectors with stereoscopic depth to create an integrated vision pipeline that functions in real-time on commodity hardware. 3) Devise methods to train the system more easily by relying on depth and motion features. 4) Address critical operational design considerations such as thermal and power management and understand requirements to maintain mechanical and structural integrity through periods of intense use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764089","CHS: Medium: Scaling Qualitative Inductive Analysis through Computational Methods","IIS","HCC-Human-Centered Computing","08/01/2018","08/01/2019","Danielle Szafir","CO","University of Colorado at Boulder","Continuing Grant","William Bainbridge","07/31/2021","$705,394.00","Jed Brubaker, Michael Paul, Casey Fiesler","danielle.szafir@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7367","7367, 7924, 9102","$0.00","This project focuses on the integration of people and computation in the context of qualitative inductive methods (QIMs), in which experts deeply engage with text corpora such as open-ended surveys, transcribed interviews, or collections of social media content. This engagement can produce insights, but constraints on expertise and time make these methods hard to scale to large datasets. Technologies like machine learning and natural language processing (NLP), which can mine certain kinds of patterns from text data at scales not feasible for even large teams of humans, may offer a way forward; however, machines make mistakes understanding the nuances of language, lack the context and expertise of human analysts, and may fail to detect interesting small-scale patterns necessary to solve particular problems. The goal of this project is to scale up the use of QIMs by inverting traditional models where humans are used to verify computational results (""human-in-the-loop""), starting instead with human insights that can be amplified through computational models, support, and suggestions for analysis (""computer-in-the-loop""). Working with collaborators in domains including mental health, public health, disaster response, policy making, and philanthropy, the team will conduct qualitative studies of their QIM practices and needs, then develop and evaluate systems with the goal of improving both the quality and scale of the insights experts can generate.  The team will produce publicly available versions of the tools and disseminate them through an online community for interested researchers from all fields.  The project activities will also inform courses on information visualization, human computer interaction, and applied machine learning, along with workshops aimed at recruiting high school women to careers in computing.<br/><br/>The work is organized around three main threads.  The first thread is to conduct a deep analysis of qualitative work processes, using both existing accounts of qualitative work in the literature and participant observation methods with at least 10 teams who approach QIM from a range of disciplines, domains, and scales. Through analysis of interviews, logs, and artifacts, the team will generate rich descriptions of these work processes that will further both understanding of QIMs as a method and identify open problems amenable to computational support. The second thread is to develop computational models of QIMs that align with analysts' processes and judgments around qualitative data, using a variety of datasets from the research team and their partners. Because many QIM methods label specific text passages as relevant to a specific concept, taking those as positive examples and nearby, unlabeled data as negative examples may allow QIMs to be modeled as a series of binary classification problems.  This will allow the team to use NLP methods guided by domain knowledge and insights from the first thread to generate features for machine learning-based models. The third thread involves connecting these models to analysts' processes through a series of passage-level, document-level, and theme-level visualizations that leverage the models' predictions to suggest other passages relevant to a concept in a given document, hierarchical aggregation of patterns across documents to support the extraction of higher-level themes along with ways to merge and divide concepts, and statistical analysis of the prevalence of themes in corpora-level analysis. The algorithms and tools will be evaluated through a series of offline tests against existing analyzed datasets, short analysis challenge contests in workshop settings to evaluate usability and reactions to the tool, longitudinal three-month deployments with partners that involve weekly semi-structured questionnaires about their usability in practice, and in an online community to both provide support for and collect feedback about the system while growing a methodological community of practice around this style of big data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817183","RI: Small: Modeling Multiple Modalities for Knowledge-Base Construction","IIS","Robust Intelligence","08/01/2018","06/25/2018","Sameer Singh","CA","University of California-Irvine","Standard Grant","Roger Mailler","07/31/2021","$448,001.00","","sameer@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","7495, 7923","$0.00","Information-rich documents are prevalent in many domains such as news articles, social media posts, online retail pages, healthcare records, financial reports, and scientific papers. Automatically extracting knowledge from such documents is useful in many applications, such as for answering questions, searching the web, automated dialogs, and analyzing trends. Existing machine learning methods focus only on the text in the documents, and ignore other information sources such as images, tables, and numbers. Thus, much of the information is not extracted, leading to incomplete knowledge and incorrect conclusions. This research advances research in machine learning and natural language processing to address these problems. With support for accurate extraction and reasoning, this project will pave the way for novel applications to domains with unstructured, multimodal documents.<br/><br/>The specific aim of the project is to investigate a novel construction pipeline for knowledge bases, taking the first steps in combining textual and relational evidence with numerical, image, and tabular data. To address the many interconnected challenges therein, the project focuses on two sub-tasks. First, the team will extract new facts about an entity from a document, such as its attributes, by combining the different parts (text, images, and tables). Second, the team will develop models to identify missing relations in graphs that contain multimodal facts. For each task, the project includes plans to introduce new datasets, propose benchmark evaluations, and develop appropriate baselines. Further, the team will build upon recent advances in deep neural encoders to investigate machine learning approaches that learn unified, semantic embeddings to model multimodal data. With these contributions, the project will initiate a body of research in machine learning and natural language processing that uses unstructured multimodal data in all its forms for accurate knowledge extraction.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808582","Collaborative Research: CDS&E: Theoretical Foundations and Algorithms for L1-Norm-Based Reliable Multi-Modal Data Analysis","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/27/2018","Panagiotis Markopoulos","NY","Rochester Institute of Tech","Standard Grant","Tevfik Kosar","08/31/2021","$323,973.00","Andreas Savakis","pxmeee@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","8069, 8084","026Z, 8084, 9263","$0.00","In modern applications of science and engineering, large volumes of data are collected from diverse sensor modalities, commonly stored in the form of high-order arrays (tensors), and jointly analyzed in order to extract information about underlying phenomena. This joint tensor analysis can exploit inherent dependencies across data modalities and allow for markedly enhanced inference. Standard methods for tensor analysis rely on formulations that are sensitive to heavily corrupted points among the processed data (outliers). To counteract the destructive impact of outliers in modern data analysis (and thereto relying applications), this project will investigate new theory and robust algorithmic methods. The performance benefits of the developed tools will be evaluated in applications from the fields of data analytics, machine learning and computer vision. Thus, this research aspires to increase significantly the reliability of data-enabled research across science and engineering. Combining theoretical explorations, with practical algorithmic solutions for data analysis and experimental evaluations, this project has the potential to build significant future capacity not only for U.S. academic institutions but also for the U.S. government and industry. Thus, apart from promoting the progress of science, this project could contribute to advances in the national prosperity and welfare. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities and supports diversity in STEM by involving students from underrepresented groups.<br/><br/>In this project, the theoretical underpinnings of L1-norm tensor analysis will be investigated, with a focus on its computational hardness and exact solution. Then, based on these new foundations, efficient/practical algorithms for L1-norm tensor analysis will be explored, together with scalable and distributed software implementations. These theoretical and algorithmic investigations are expected to advance significantly the knowledge in the currently under-explored area of L1-norm tensor analysis and deliver highly impactful methodologies for outlier-resistant multimodal data processing. Next, the PIs will employ the newly developed algorithmic tools in key problems from the fields of data analytics, machine learning and computer vision. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities?and supports diversity in STEM by involving?students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838627","SCH: INT: New Machine Learning Framework to Conduct Anesthesia Risk Stratification and Decision Support for Precision Health","IIS","Smart and Connected Health","10/01/2018","09/08/2018","Heng Huang","PA","University of Pittsburgh","Standard Grant","Wendy Nilsen","09/30/2022","$1,182,305.00","Dan Li, fei Zhang","heng.huang@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","8018","8018, 8062","$0.00","With advances in anesthesia techniques, surgery has become increasingly applicable to a wider range of diseases and patients. Worldwide more than 230 million major surgical procedures are carried out each year. In terms of patient safety and medical economics, an important issue is how to reduce the incidence of postoperative complications and mortality. At least half of postoperative complications can be prevented, while improvements in anesthesia-associated factors contribute greatly to the prevention of complications. Anesthesia information management system is a specialized type of electronic health record that allow the automatic and reliable collection and storage of patient data during the perioperative period. The electronic anesthesia data not only provide a rich data set to assist both anesthesia providers and hospitals with their goals to improve patient safety during the fast-paced intra-operative period, but also capture detailed data to allow end users to access information for management, quality assurance, and research purposes. This project addresses the computational challenges in large-scale electronic anesthesia data mining, develops and validates an automated anesthesia risk prediction and decision support system to identify risk factors and detect patients at risk of postoperative complications and in-hospital mortality. <br/><br/>This project develops novel large-scale machine learning framework to integrate the emerging key computational techniques, such as semi-supervised generative adversarial learning, interpretable deep learning, large-scale optimization, and unsupervised hashing, to analyze large-scale electronic anesthesia data for enhancing anesthesia risk stratification and improving the quality of care for precision health. Specifically, the PIs investigate: 1) new computational tools to automate electronic anesthesia data processing, 2) novel semi-supervised generative adversarial network for anesthesia risk stratification, 3) interpretable deep learning model for clinical markers discovery, 4) scale up deep learning models for big data computation via new large-scale optimization algorithms, 5) new unsupervised deep generative adversarial hashing network for fast and accurate clinical case retrieval, and 6) evaluate the proposed methods and system using real large-scale anesthesia data. It is innovative to integrate large-scale machine learning and data-intensive computing for electronic anesthesia data mining that holds great promise for predicting postoperative outcomes using the comprehensive preoperative and intra-operative patient profiles. The developed methods and tools impact other public health research and enable investigators working on electronic health data to effectively test risk prediction hypothesis. This project facilitates the development of novel educational tools to enhance several current courses at University of Pittsburgh.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763233","III: Medium: Collaborative Research: Guiding Exploration of Protein Structure Spaces with Deep Learning","IIS","Info Integration & Informatics","07/01/2018","08/01/2019","Amarda Shehu","VA","George Mason University","Continuing Grant","Sylvia Spengler","06/30/2021","$499,074.00","","ashehu@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7364","7364, 7924, 9102","$0.00","Decades of scientific enquiry beyond molecular biology have demonstrated just how fundamental form is to function. All chemical reactions in the living cell involve molecules bumping and sticking to one another, and molecular form or structure is a central determinant of complementarity and strength of molecular interactions. In particular, by assuming specific structures, proteins are able to regulate diverse processes that maintain and replicate the living cell. With structure determination in laboratories lagging desperately behind the rapidly-growing number of protein-encoding gene sequences by high-throughput sequencing technologies, computational approaches to the problem of protein structure prediction now have a central role in molecular biology. This project advances algorithmic research to address the current impasse in form-function related problems in molecular biology. In particular, the project develops advanced  optimization methods to explore the vast protein structure space and leverages information-integration techniques under the deep learning framework to effectively guide the exploration towards biologically-active structures. This project will benefit researchers of diverse sub-communities in computational and biological sciences, result in open source, publicly-available software packages, and provide excellent training and mentoring opportunities for under-represented students at the interface of computational science and computational biology.<br/><br/><br/>This project advances algorithmic research in information integration and informatics to address the current impasse in structure-function related problems in computational structural biology. The main focus is on the de-novo protein structure prediction problem, which is central to inferring biological activities of a rapidly-growing number of protein-encoding gene sequences. The proposed research generalizes the problem of exploring and obtaining a comprehensive view of a protein's structure space as that of computing a diverse ensemble of constraint-satisfying structures and then leveraging information-integration techniques to guide the exploration to regions of the structure space relevant for biological activity. The research proposes hybrid stochastic optimization algorithms for comprehensive exploration of protein structure spaces, deep convolutional neural networks for better assessment of structure nativeness, and combines the two in an information-integration algorithmic framework to guide the exploration of a structure space towards native structures. By doing so, the proposed research investigates a direction complementary to physics-based treatments, proposing to supplant such treatments with machine-learned models of nativeness. The research will benefit researchers in machine learning, stochastic optimization, and information integration with application-driven interests in molecular modeling, protein structure prediction, and modeling of complex, dynamic systems. The research will be disseminated via various venues, including an open-source software package, and will provide training opportunities for under-represented students of all levels at the interface of optimization, deep learning, and computational biology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817094","CSR: Small: Collaborative Research: Tuning Extreme-scale Storage Stack through Deep Reinforcement Learning","CNS","CSR-Computer Systems Research","10/01/2018","08/03/2018","Yong Chen","TX","Texas Tech University","Standard Grant","Erik Brunvand","09/30/2021","$240,026.00","Dong Dai","yong.chen@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","7354","7923","$0.00","Many research domains, such as high-energy physics, climate science, astrophysics, combustion science, and computational biology, need to process large amounts of data. Such domains are heavily relying on the capabilities of high performance computing (HPC) systems to manage and efficiently process massive amounts of data. Consequently, applications in the aforementioned research domains require highly optimized performance on the HPC storage systems that store, manage, and manipulate data. This project aims to utilize deep reinforcement learning methods to fine-tune the HPC storage system for optimized performance.<br/><br/>This research explores the feasibility of leveraging deep reinforcement learning to optimize HPC storage systems by: (a) Creating a deep learning based HPC storage stack model; (b) Remodeling existing HPC storage stack to support automated configuration and tuning; (c) Collecting training datasets and training the storage stack model; and (d) utilizing the model as a responsive and playable virtual environment to learn the best policy to tune parameters. <br/><br/>As a collaborative project, this research aims to advance the domain knowledge of both HPC storage systems and machine learning. The enhanced performance on the HPC storage stack will in turn benefit scientific discovery and thus our society. The investigators will integrate research, education, and outreach efforts during the course of this project, including recruiting and retaining of underrepresented students, mentoring graduate and undergraduate students, integrating research findings into curriculum, and publishing and disseminating results.<br/><br/>The data collected to train the storage stack model will be shared at https://discl.cs.ttu.edu/tuningstorage while the code of machine learning at https://github.com/forrestbao/DL4SC. Results and data will be made available by the time of publication. The data will be annotated as appropriate to facilitate interpretation. The principal investigators will strive to maintain the repositories as long as possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808591","Collaborative Research: CDS&E: Theoretical Foundations and Algorithms for L1-Norm-Based Reliable Multi-Modal Data Analysis","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/27/2018","Evangelos Papalexakis","CA","University of California-Riverside","Standard Grant","Tevfik Kosar","08/31/2021","$175,263.00","","epapalex@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","8069, 8084","026Z, 8084, 9263","$0.00","In modern applications of science and engineering, large volumes of data are collected from diverse sensor modalities, commonly stored in the form of high-order arrays (tensors), and jointly analyzed in order to extract information about underlying phenomena. This joint tensor analysis can exploit inherent dependencies across data modalities and allow for markedly enhanced inference. Standard methods for tensor analysis rely on formulations that are sensitive to heavily corrupted points among the processed data (outliers). To counteract the destructive impact of outliers in modern data analysis (and thereto relying applications), this project will investigate new theory and robust algorithmic methods. The performance benefits of the developed tools will be evaluated in applications from the fields of data analytics, machine learning and computer vision. Thus, this research aspires to increase significantly the reliability of data-enabled research across science and engineering. Combining theoretical explorations, with practical algorithmic solutions for data analysis and experimental evaluations, this project has the potential to build significant future capacity not only for U.S. academic institutions but also for the U.S. government and industry. Thus, apart from promoting the progress of science, this project could contribute to advances in the national prosperity and welfare. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities and supports diversity in STEM by involving?students from underrepresented groups.<br/><br/>In this project, the theoretical underpinnings of L1-norm tensor analysis will be investigated, with a focus on its computational hardness and exact solution. Then, based on these new foundations, efficient/practical algorithms for L1-norm tensor analysis will be explored, together with scalable and distributed software implementations. These theoretical and algorithmic investigations are expected to advance significantly the knowledge in the currently under-explored area of L1-norm tensor analysis and deliver highly impactful methodologies for outlier-resistant multimodal data processing. Next, the PIs will employ the newly developed algorithmic tools in key problems from the fields of data analytics, machine learning and computer vision. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities?and supports diversity in STEM by involving students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1908220","III: Small: Deep Learning for Gene Expression Pattern Image Analysis","IIS","Info Integration & Informatics","10/16/2018","12/18/2018","Shuiwang Ji","TX","Texas A&M Engineering Experiment Station","Standard Grant","Amarda Shehu","07/31/2021","$499,957.00","","sji@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7364","7364, 7923","$0.00","Biological image informatics is an emerging frontier in computational biology, as there is an urgent need to move beyond the manual inspection of images to computational analysis for accelerating scientific discoveries. Conventional methods commonly employ shallow machine learning models in which handcrafted image representations are computed and used in model construction. These approaches heavily rely on prior knowledge on the data and problems to compute appropriate image representations. Motivated by the recent success of deep learning methods in image-related domains, the objective of this project is to develop advanced deep learning models for automated representation learning from biological images. This project also facilitates the development of new courses and laboratory infrastructure for attracting graduate, undergraduate, and high school students, with an emphasis on those from underrepresented groups.<br/><br/>Specifically, this project focuses on the analysis of spatiotemporal gene expression pattern images in fruit fly and mouse. The key challenges lie in how to capture the intrinsic structures of biological problems and how to enable effective model training on small, manually labeled biological data sets. This project develops multi-instance, multi-task, hierarchical, and regularized deep learning models for incorporating the structures of biological problems. The multi-instance and multi-task models capture the complex relationships among inputs and outputs, respectively. The hierarchical and regularized models explicitly encode problem structures and make the results interpretable. In addition, transfer and unsupervised learning methods are developed to enable effective model training on small labeled data sets. These are achieved by integrating both labeled and unlabeled data sets across multiple domains. Altogether, this project is expected to result in a set of advanced deep learning methods for the efficient and effective analysis of biological images.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815538","III: Small: A New Machine Learning Approach for Improved Entity Identification","IIS","Info Integration & Informatics","09/01/2018","08/15/2018","Theodoros Rekatsinas","WI","University of Wisconsin-Madison","Standard Grant","Sylvia Spengler","08/31/2021","$320,377.00","","thodrek@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7364","7364, 7923","$0.00","Modern analytics rely on data integration to combine heterogeneous data into a unified repository they can tap into for insights, services, and scientific knowledge. The typical goal of data integration is to combine heterogeneous data about the same real-world entity into a canonical representation of that entity. Traditionally, entity canonicalization methods focus on structured data and leverage the semantics of the schema accompanying the data to come up with canonical entity representations. This dependency on data semantics makes existing entity canonicalization methods inapplicable to dark data, i.e., operational data that corresponds to unstructured, noisy, and incomplete data. This project will develop entity canonicalization methods that focus on unstructured and semi-structured data and are suitable for large-scale integration applications. This work will help ease the currently challenging procedure of heuristically consolidating matching information about the same entity into unified representations and thus enable dark data to be more effectively used in downstream analytics applications.<br/><br/>The emphasis of this work is on entity canonicalization techniques that leverage representation learning (a.k.a. feature learning) and deep learning. The combination of distributed representations with deep architectures has emerged as the de facto standard for analyzing and processing unstructured data. This project will develop new deep learning architectures for: (1) record linkage, i.e., clustering unstructured data records that provide information about the same entity; and (2) data fusion, i.e., combining matching unstructured records into a canonical representation of the underlying entity. For record linkage, this work will introduce new deep learning techniques that capture multi-context domain-specific knowledge to learn the semantic similarity between records. For data fusion, this project will design new multi-sequence to one-sequence encoder-decoder recurrent neural networks for data fusion with a particular focus on incomplete data. The outcomes of this project have the potential to advance the state-of-the-art in large scale data integration methods as well as machine learning methods for high-dimensional, sparse, and noisy data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816732","CHS:Small: Improved Cross-Subject Cognitive and Emotional State Classification  Using Functional Near-Infrared Spectroscopy Data for Deep Learning","IIS","HCC-Human-Centered Computing","08/15/2018","10/31/2019","Leanne Hirshfield","NY","Syracuse University","Standard Grant","Ephraim Glinert","07/31/2021","$494,374.00","Michael Kalish, Senem Velipasalar","leanne.hirshfield@colorado.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7367","7367, 7923, 8089, 8091, 9102","$0.00","New advances in bio-technology suggest devices to wear and measure the brain will be available to support many different activities. Future technologies may use brain activity data to adapt or customize educational software in real-time. Activity support based on interpreted brain activity could be used to reduce mental workload, modify emotional states, or help someone with post-traumatic stress disorder. However, brain activity data is complex and difficult to interpret. This project will use deep machine learning methods to overcome the challenge of classifying and interpreting brain activity data using real-time data from participants. The objective is to harness the tremendous potential of cognitive sensors and computational methods to help individuals function more effectively.  <br/><br/>Although many early successes were achieved using machine learning on brain data, several notable challenges have arisen, which significantly limit the impacts of these early successes. The technical approach in this project has three research thrusts. The investigators will develop models specifically for use on high density functional-near infrared spectroscopy (fNIRS) data. Thrust 1 involves the development of advanced deep learning techniques that are particularly well-suited for fNIRS data, and address spatial and temporal inter-relations. Thrust 2 involves development and adaptation of algorithm transparency (AT) techniques that are well-suited to shed light on brain dynamics embedded within the deep learning model structures. This will help the research team interpret the underlying structure of the models, with respect to brain spatial and temporal dynamics at the individual and group level. Thrust 3 collates the model and AT techniques developed in the prior thrusts and evaluates them using an extensive cross-subject and cross-participant fNIRS dataset. Using this data for evaluation purposes, the research team will work together to interpret results to improve upon classifier performance and model generalizability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763246","III: Medium: Collaborative Research: Guiding Exploration of Protein Structure Spaces with Deep Learning","IIS","Info Integration & Informatics","07/01/2018","05/24/2018","Jianlin Cheng","MO","University of Missouri-Columbia","Standard Grant","Sylvia Spengler","06/30/2021","$448,040.00","","chengji@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","7364","7364, 7924","$0.00","Decades of scientific enquiry beyond molecular biology have demonstrated just how fundamental form is to function. All chemical reactions in the living cell involve molecules bumping and sticking to one another, and molecular form or structure is a central determinant of complementarity and strength of molecular interactions. In particular, by assuming specific structures, proteins are able to regulate diverse processes that maintain and replicate the living cell. With structure determination in laboratories lagging desperately behind the rapidly-growing number of protein-encoding gene sequences by high-throughput sequencing technologies, computational approaches to the problem of protein structure prediction now have a central role in molecular biology. This project advances algorithmic research to address the current impasse in form-function related problems in molecular biology. In particular, the project develops advanced  optimization methods to explore the vast protein structure space and leverages information-integration techniques under the deep learning framework to effectively guide the exploration towards biologically-active structures. This project will benefit researchers of diverse sub-communities in computational and biological sciences, result in open source, publicly-available software packages, and provide excellent training and mentoring opportunities for under-represented students at the interface of computational science and computational biology.<br/><br/><br/>This project advances algorithmic research in information integration and informatics to address the current impasse in structure-function related problems in computational structural biology. The main focus is on the de-novo protein structure prediction problem, which is central to inferring biological activities of a rapidly-growing number of protein-encoding gene sequences. The proposed research generalizes the problem of exploring and obtaining a comprehensive view of a protein's structure space as that of computing a diverse ensemble of constraint-satisfying structures and then leveraging information-integration techniques to guide the exploration to regions of the structure space relevant for biological activity. The research proposes hybrid stochastic optimization algorithms for comprehensive exploration of protein structure spaces, deep convolutional neural networks for better assessment of structure nativeness, and combines the two in an information-integration algorithmic framework to guide the exploration of a structure space towards native structures. By doing so, the proposed research investigates a direction complementary to physics-based treatments, proposing to supplant such treatments with machine-learned models of nativeness. The research will benefit researchers in machine learning, stochastic optimization, and information integration with application-driven interests in molecular modeling, protein structure prediction, and modeling of complex, dynamic systems. The research will be disseminated via various venues, including an open-source software package, and will provide training opportunities for under-represented students of all levels at the interface of optimization, deep learning, and computational biology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835390","Collaborative Research: NCS-FO: Discovering Dynamics in Massive-Scale Neural Datasets Using Machine Learning","IIS","IntgStrat Undst Neurl&Cogn Sys","10/01/2018","08/28/2018","Matthew Kaufman","IL","University of Chicago","Standard Grant","Todd Leen","09/30/2021","$189,049.00","","mattkaufman@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8624","8089, 8091, 8551","$0.00","For decades, neuroscientists have recorded from single brain cells (neurons) to understand how the brain senses, makes decisions, and controls movements. We can now record from hundreds of neurons simultaneously but are still at an early stage in developing tools for determining how networks of neurons work together to perceive the world and to generate the control signals needed to produce coordinated movement. Focusing on movement, this project brings to bear the power of deep learning --- powerful new machine learning algorithms --- on the problem of understanding neural activity. Because deep learning thrives on big data, the investigators can leverage massive-scale brain recordings.  These include month-long recordings chronicling the activity of 100 neurons as a monkey goes about its daily business, or recording from thousands of neurons for hours in the mouse, each identified with an exact location in the brain and tied to the mouse's on-going behaviors. These approaches will open new windows on how neurons act together moment-by-moment to produce movement. The investigators will develop simple descriptions of the underlying processes to be shared with the public through venues including online tutorials, a new open course that will be developed at Emory University and Georgia Tech, the Atlanta Science Festival, and Atlanta's Brain Awareness Month. They will also make their data sets publicly available, and host data tutorial and modeling competitions at key scientific meetings, to accelerate progress by engaging the broader scientific community.<br/><br/>In the fifty years since Ed Evarts first recorded single neurons in M1 of behaving monkeys, great effort has been devoted to understanding the relation between these individual signals and movement-related signals collected during highly constrained motor behaviors performed by over-trained monkeys. In parallel, theoreticians posited that the computations performed in the brain depend critically on network-level phenomena: dynamical laws in brain circuits that constrain the activity and dictate how it evolves over time. The goal of this project is to develop a powerful new suite of tools, based on deep learning, to analyze these dynamics at unprecedented temporal and spatial scales. The investigators will leverage recordings with month-long M1 electrophysiology, EMG, and behavioral data during natural behaviors from monkeys, and vast numbers of neurons recorded with two-photon imaging from behaving mice. Novel machine learning techniques using sequential auto-encoders will enable the investigators to learn the dynamics underlying these data. This combination will provide windows into the brain's control of motor behavior that have never before been possible. The novel analytical framework developed here will be extensible from motor behaviors to higher level problems of error processing, decision making, and learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835364","Collaborative Research: NCS-FO: Discovering Dynamics in Massive-Scale Neural Datasets Using Machine Learning","IIS","IntgStrat Undst Neurl&Cogn Sys","10/01/2018","10/17/2019","Chethan Pandarinath","GA","Emory University","Standard Grant","Todd Leen","09/30/2021","$621,897.00","","chethan.pandarinath@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","8624","8089, 8091, 8551","$0.00","For decades, neuroscientists have recorded from single brain cells (neurons) to understand how the brain senses, makes decisions, and controls movements. We can now record from hundreds of neurons simultaneously but are still at an early stage in developing tools for determining how networks of neurons work together to perceive the world and to generate the control signals needed to produce coordinated movement. Focusing on movement, this project brings to bear the power of deep learning --- powerful new machine learning algorithms --- on the problem of understanding neural activity. Because deep learning thrives on big data, the investigators can leverage massive-scale brain recordings.  These include month-long recordings chronicling the activity of 100 neurons as a monkey goes about its daily business, or recording from thousands of neurons for hours in the mouse, each identified with an exact location in the brain and tied to the mouse's on-going behaviors. These approaches will open new windows on how neurons act together moment-by-moment to produce movement. The investigators will develop simple descriptions of the underlying processes to be shared with the public through venues including online tutorials, a new open course that will be developed at Emory University and Georgia Tech, the Atlanta Science Festival, and Atlanta's Brain Awareness Month. They will also make their data sets publicly available, and host data tutorial and modeling competitions at key scientific meetings, to accelerate progress by engaging the broader scientific community.<br/><br/>In the fifty years since Ed Evarts first recorded single neurons in M1 of behaving monkeys, great effort has been devoted to understanding the relation between these individual signals and movement-related signals collected during highly constrained motor behaviors performed by over-trained monkeys. In parallel, theoreticians posited that the computations performed in the brain depend critically on network-level phenomena: dynamical laws in brain circuits that constrain the activity and dictate how it evolves over time. The goal of this project is to develop a powerful new suite of tools, based on deep learning, to analyze these dynamics at unprecedented temporal and spatial scales. The investigators will leverage recordings with month-long M1 electrophysiology, EMG, and behavioral data during natural behaviors from monkeys, and vast numbers of neurons recorded with two-photon imaging from behaving mice. Novel machine learning techniques using sequential auto-encoders will enable the investigators to learn the dynamics underlying these data. This combination will provide windows into the brain's control of motor behavior that have never before been possible. The novel analytical framework developed here will be extensible from motor behaviors to higher level problems of error processing, decision making, and learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815491","RI: Small: Learning to Discover Structure for 3D Vision","IIS","Robust Intelligence","09/01/2018","09/14/2018","Zihan Zhou","PA","Pennsylvania State Univ University Park","Continuing Grant","Jie Yang","08/31/2021","$449,223.00","","zzhou@ist.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7495","7495, 7923","$0.00","People are able to spontaneously perceive structure, that is, orderly, regular, or coherent patterns and relationships, in images. When walking along a city street, a human can instantly identify parallel lines, rectangles, rotational symmetries, repetitive patterns, and many other types of structure. This project develops a novel data-driven framework for structure discovery in computer vision, leveraging the availability of massive data and recent advances in machine learning techniques. The techniques developed in this project can be applied to a wide spectrum of real-world applications such as 3D reconstruction of man-made environments, virtual and augmented reality, and indoor rescue robots. Further, the ability to understand 3D perceptual organization as humans do can benefit other fields including (i) cognitive science, as it produces new computational models which can be used to test and develop existing theories, and to explore new details and aspects of the brain, (ii) human-robot interaction, as it enables robots to reason in terms of geometric shape, physics, and dynamics, and (iii) architectural engineering, as it facilitates interactions with existing standards for construction and management of buildings.<br/><br/>The project is built upon a formal definition of structure, consisting of (i) the constituent patterns, (ii) the domain of replication or continuation of the patterns, and (iii) any change of the pattern and domain over space and time (Witkin and Tenenbaum, 1983). The research has three aims. The first aim lays the computational foundation by innovating machine learning methods to detect the domain of the structure and its constituent patterns, respectively. The second aim further establishes a unified framework for structure discovery, going beyond the bottom-up scheme and sequential processing. Here, the challenge lies in that structure often spans large spatial extent, and is typically subject to pattern deformation and domain distortion. In the last aim, the researchers incorporate the structure in complex vision systems to demonstrate its advantage in real-world applications. Ultimately, the project strives to significantly improve the effectiveness and efficiency of 3D vision systems, and to enrich the general computer vision principles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835345","Collaborative Research: NCS-FO: Discovering Dynamics in Massive-Scale Neural Datasets Using Machine Learning","IIS","IntgStrat Undst Neurl&Cogn Sys","10/01/2018","08/28/2018","Lee Miller","IL","Northwestern University at Chicago","Standard Grant","Todd Leen","09/30/2021","$189,045.00","","lm@northwestern.edu","750 N. Lake Shore Drive,","Chicago","IL","606114579","3125037955","CSE","8624","8089, 8091, 8551","$0.00","For decades, neuroscientists have recorded from single brain cells (neurons) to understand how the brain senses, makes decisions, and controls movements. We can now record from hundreds of neurons simultaneously but are still at an early stage in developing tools for determining how networks of neurons work together to perceive the world and to generate the control signals needed to produce coordinated movement. Focusing on movement, this project brings to bear the power of deep learning --- powerful new machine learning algorithms --- on the problem of understanding neural activity. Because deep learning thrives on big data, the investigators can leverage massive-scale brain recordings.  These include month-long recordings chronicling the activity of 100 neurons as a monkey goes about its daily business, or recording from thousands of neurons for hours in the mouse, each identified with an exact location in the brain and tied to the mouse's on-going behaviors. These approaches will open new windows on how neurons act together moment-by-moment to produce movement. The investigators will develop simple descriptions of the underlying processes to be shared with the public through venues including online tutorials, a new open course that will be developed at Emory University and Georgia Tech, the Atlanta Science Festival, and Atlanta's Brain Awareness Month. They will also make their data sets publicly available, and host data tutorial and modeling competitions at key scientific meetings, to accelerate progress by engaging the broader scientific community.<br/><br/>In the fifty years since Ed Evarts first recorded single neurons in M1 of behaving monkeys, great effort has been devoted to understanding the relation between these individual signals and movement-related signals collected during highly constrained motor behaviors performed by over-trained monkeys. In parallel, theoreticians posited that the computations performed in the brain depend critically on network-level phenomena: dynamical laws in brain circuits that constrain the activity and dictate how it evolves over time. The goal of this project is to develop a powerful new suite of tools, based on deep learning, to analyze these dynamics at unprecedented temporal and spatial scales. The investigators will leverage recordings with month-long M1 electrophysiology, EMG, and behavioral data during natural behaviors from monkeys, and vast numbers of neurons recorded with two-photon imaging from behaving mice. Novel machine learning techniques using sequential auto-encoders will enable the investigators to learn the dynamics underlying these data. This combination will provide windows into the brain's control of motor behavior that have never before been possible. The novel analytical framework developed here will be extensible from motor behaviors to higher level problems of error processing, decision making, and learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839353","TRIPODS+X:VIS: The DISC Institute Workshop Series on Machine Learning + X.","DMS","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","07/17/2020","Katya Scheinberg","PA","Lehigh University","Standard Grant","Tracy Kimbrel","09/30/2021","$199,353.00","Hector Munoz-Avila, Katya Scheinberg","katyas@cornell.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","MPS","041Y, 1253","047Z, 062Z","$0.00","This project encompasses the planning and organization of several specialized workshops that will bring together top experts in multiple areas to shape new and emerging multidisciplinary fields, tapping the tremendous recent surge in the adoption of machine learning tools in various areas of science and engineering. The premise of this project is the need for sophisticated computational tools  to analyze data and improve our ability to understand and harness phenomena  associated with complex domains such as chemical processes, autonomous robots operating in open and dynamic environments, supply chain optimization involving large organizations with multiple and competing objectives, and cognitive neuroscience bridging electrical brain impulses and high-level functions such as problem solving. Towards this end it is necessary to foster interdisciplinary collaborations and to promote convergent research and develop fertile space for collaborations among industrial, academic, and governmental partners to attack some of the most pressing problems in technology and society. <br/><br/>Under the umbrella of the new Institute for Data, Intelligent Systems, and Computation (I-DISC) at Lehigh University, which builds upon the foundation of Lehigh research expertise in areas such as machine learning, optimization, and data-driven decision making, four workshops will be organized that will bring together leading researchers from different research communities that otherwise may not interact. All of these workshops are on newly emerging topics which are expected to gain significant traction in the near future. These topics are as follows: (1) Chemistry, chemical engineering, materials science, and related disciplines where machine learning is used  to elucidate and design complex processes (chemical/biological, engineered/natural) or material systems with wide ranging applications addressing grand challenges in energy, health, environment, and water. (2) Robotics, where applications of machine learning, also known as robot learning, has been rapidly growing in recent years, where the main focus has been to develop algorithms to assist robots to acquire novel skill or adapt to their environment through sensing. (3) Supply chain management with the specific focus on applying machine learning models for prescriptive analytics, such as optimization, in contrast to already popular use of machine learning (deep learning) models for predictive and descriptive analytics, such as predicting customer demands. (4) Cognitive Neuroscience with the focus on understanding the brain-cognition-behavior interface, which requires expertise in neuroscience as well as computational modeling, machine learning and big data science in order (a) to enable sophisticated analyses of complex patterns in brain data and (b) to provide insight into how hypothesized brain-level implementations could in fact produce observed behavioral outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815221","SaTC: CORE: Small: Towards Adversarially Robust Machine Learning","CNS","Secure &Trustworthy Cyberspace","08/15/2018","08/10/2018","Aleksander Madry","MA","Massachusetts Institute of Technology","Standard Grant","Wei-Shinn Ku","07/31/2021","$500,000.00","","madry@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8060","025Z, 7434, 7923","$0.00","Machine learning has witnessed tremendous progress over the last decade and is rapidly becoming a critical part of key aspects of our lives, from health care and financial services, to the way we evaluate job applications, commute to work, or even use media. This gives rise to a fundamental question: how will all these machine learning solutions fare when applied in real-world settings that are safety-sensitive or even security-critical? Will these solutions be sufficiently reliable and resistant to malicious tampering? These concerns are hardly unjustified. In fact, the current state-of-the-art machine learning toolkit was tailored to optimize for the ""average case"" performance and turns out to be catastrophically vulnerable to more ""worst case"" inputs and manipulation. A particularly prominent problem is the existence of so-called adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet cause the model to act incorrectly.  This project's overarching goal is to re-think machine learning methodology to make it deliver solutions that are robust and secure. Much of the focus will be on building a principled and holistic understanding of adversarial robustness, i.e., resistance to adversarial examples, as a phenomenon both in machine learning and in security.  As part of this goal, the team will conduct community-building and outreach efforts including hosting challenge competitions to test machine learning robustness, disseminating datasets and code, and developing course materials and research projects suitable for a variety of undergraduate and high school curricula.<br/><br/>The planned work will pursue three main thrusts: exploring the complexity landscape of adversarial robustness, analyzing the power and limitations of adversarial training (the current dominant approach to producing adversarially robust models), and designing classifiers whose adversarial robustness can be rigorously verified. The intent is to develop the theoretical foundations of the studied concepts but also to leverage them to engage the practical aspects of the problem.  More specifically, on one hand, the project aims to establish formal threat models and provide ultimate limits on what kind of security can and cannot be achieved for them. The directions to be explored here span complexity analyses of adversarially robust generalization and model stealing, design of new regularization and optimization techniques for these contexts, and adapting the existing -- mostly continuous -- methods to discrete domains. On the other hand, the plan is to use the developed techniques to deploy adversarially robust models and then validate their robustness via a mix of public security challenges and benchmarks as well as formal verification tools. Design of these tools is a part of this project too and it involves exploring methods for reducing the number of non-linearities deep learning models use as well as applying convex programming approaches.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1825709","Understanding Pedestrian Dynamics for Seamless Human-Robot Interaction","CMMI","Science of Learning, M3X - Mind, Machine, and Motor","09/01/2018","08/22/2018","Yi Guo","NJ","Stevens Institute of Technology","Standard Grant","Robert Scheidt","08/31/2021","$358,535.00","","yguo1@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","ENG","004Y, 058Y","059Z, 063Z, 070E, 6856, 7632, 9102","$0.00","In the near future, robots will navigate alongside people in busy, crowded and unconstrained environments such as shopping malls, airports and elder-care facilities. Current mobile robot motion planners are inadequate because current models of pedestrian dynamics do not fully capture the complexity of human motion behavior in crowds. As a result, robots can ""freeze"" in places where crowd density is high, which further impedes pedestrian traffic flow. This project will use novel machine learning techniques to develop a robot motion planner with human-like navigations features. The project team will use objective and subjective performance measures to evaluate the predictability and acceptability of human-robot interactions with robots and their novel control algorithms deployed in shopping malls and campus buildings. This project serves the national interest because the resulting pedestrian dynamics modeling methods and robot controllers may result in public safety applications such as emergency evacuations and crowd planning/management. The project will involve an educational component that provides engineering and research methods training to graduate and undergraduate students, as well as STEM outreach to high-school and middle-school students. Additional efforts will be made to attract and retain women into careers in science and engineering. <br/><br/>This research investigates new control methodologies that promise improved pedestrian/mobile-robot navigation through crowded and unconstrained environments. Methods include the extraction of features related to pedestrian behavior from existing datasets and their use in training a deep neural network (DNN) to model pedestrian dynamics; the use of inverse reinforcement learning to generate a cost map that humans follow to navigate; the implementation of the map within a robot motion controller; and the experimental testing and validation of the controller in real-world human environments using objective and subjective performance criteria.  The experimental and evaluation data will be made available for use by the research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817089","CSR: Small: Collaborative Research: Tuning Extreme-Scale Storage System Through Deep Learning","CNS","CSR-Computer Systems Research","10/01/2018","07/30/2019","Forrest Sheng Bao","IA","Iowa State University","Standard Grant","Erik Brunvand","09/30/2021","$230,767.00","","fsb@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7354","7923","$0.00","Many research domains, such as high-energy physics, climate science, astrophysics, combustion science, and computational biology, need to process large amounts of data. Such domains are heavily relying on the capabilities of high performance computing (HPC) systems to manage and efficiently process massive amounts of data. Consequently, applications in the aforementioned research domains require highly optimized performance on the HPC storage systems that store, manage, and manipulate data. This project aims to utilize deep reinforcement learning methods to fine-tune the HPC storage system for optimized performance.<br/><br/>This research explores the feasibility of leveraging deep reinforcement learning to optimize HPC storage systems by: (a) Creating a deep learning based HPC storage stack model; (b) Remodeling existing HPC storage stack to support automated configuration and tuning; (c) Collecting training datasets and training the storage stack model; and (d) utilizing the model as a responsive and playable virtual environment to learn the best policy to tune parameters. <br/><br/>As a collaborative project, this research aims to advance the domain knowledge of both HPC storage systems and machine learning. The enhanced performance on the HPC storage stack will in turn benefit scientific discovery and thus our society. The investigators will integrate research, education, and outreach efforts during the course of this project, including recruiting and retaining of underrepresented students, mentoring graduate and undergraduate students, integrating research findings into curriculum, and publishing and disseminating results.<br/><br/>The data collected to train the storage stack model will be shared at https://discl.cs.ttu.edu/tuningstorage while the code of machine learning at https://github.com/forrestbao/DL4SC. Results and data will be made available by the time of publication. The data will be annotated as appropriate to facilitate interpretation. The principal investigators will strive to maintain the repositories as long as possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839733","EAGER: Real-Time: Decision and Control of Complex Engineered Systems Enabled by Machine Learning and High-performance Computing","CMMI","Special Initiatives","09/01/2018","08/21/2018","Mario Rotea","TX","University of Texas at Dallas","Standard Grant","Robert Landers","08/31/2021","$299,941.00","Stefano Leonardi","rotea@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","ENG","1642","030E, 034E, 7916, 8024, 9102","$0.00","In recent years, there have been significant advances in machine learning - statistical techniques that enable computers to ""learn"" using available data.  Machine learning methods have demonstrated great success in image recognition, language translation, speech processing, and other consumer applications. This has led to great interest globally in academia, industry, and government. The drawback in purely machine learning methods is that it does not use the knowledge of physical properties of specific system which could significantly improve the performance of these methods. This EArly-concept Grant for Exploratory Research (EAGER) project will lead to fundamental results and methods that combine the advantages of machine learning techniques and knowledge of physical attributes of the system to enable decision making and control of complex engineered systems. The research will be conducted in the context of control of large wind energy plants. Maximizing power production despite variable and uncertain operating conditions in large wind plants is an unsolved problem that is ripe for transformative approaches and innovation. The research from this project is likely to transition to industry by leveraging connections with the NSF I-UCRC for Wind Energy Science, Research and Technology (WindSTAR) as wind plant owners and operators constantly seek new ways to improve annual energy production and reduce the cost of electricity from wind.<br/><br/>The main idea of this EAGER project is to leverage advances in deep learning and high performance computing simulations for the control of complex engineered systems. Our hypothesis is that techniques from (semi-supervised) machine learning can be tailored to extract information from high performance simulation data to deal with the joint problem of identifying control system architectures and control algorithms for real-time decision making in complex engineered systems. The research goals of this project have great potential to contribute to the convergence of high performance computing simulations and data, machine learning, and controls to advance the state-of-art tools for controlling complex engineered systems. The testbed for the project is a wind plant. As turbines become larger, and are placed closer to one another, the aerodynamic coupling amongst turbines will increase resulting in a truly large-scale complex engineered system that must perform despite environmental uncertainty and variability of turbine components. Specific goals of this project include: Advanced learning algorithms for extracting control system architecture and training algorithms from large eddy simulation data of the wind farms; Real-time decision algorithms to select architecture and algorithms from site-specific libraries discovered in the first goal; and Real-time algorithms for tuning key parameters of the control solutions for additional improvements in the overall energy production.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822830","Putting Teachers in the Driver's Seat: Using Machine Learning to Personalize Interactions with Students (DRIVER-SEAT)","IIS","ECR-EHR Core Research, Cyberlearn & Future Learn Tech","09/01/2018","08/01/2019","Neil Heffernan","MA","Worcester Polytechnic Institute","Standard Grant","Amy Baylor","08/31/2021","$764,317.00","Korinn Ostrow, Neil Heffernan, Jacob Whitehill","nth@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","7980, 8020","063Z, 7218, 8045","$0.00","This is a project that will use machine learning to personalize messages about student homework. The project will apply technologies used by Google's Smart Reply, a functionality that uses machine learning to generate and suggest human-like email responses, to provide teachers a quick and effective way to respond to student online homework. An important part of many State standards is the need for math students to communicate their ideas through writing. With the number of schools digitizing their classrooms on the rise, teachers are inundated with student data. Teachers are often unable to review and provide feedback in an effective and timely manner. This project will help teachers be more efficient and at the same time cause more effective student learning. The Dialogue Reinforcement Infrastructure for Volitional Exploratory Research - Soliciting Effective Actions from Teachers (DRIVER-SEAT) will be designed to help teachers more efficiently and effectively communicate with students in a way that feels personalized, while supported by advances in computer science. By applying a feature similar to Google's Smart Reply in an educational setting, DRIVER-SEAT offer teachers suggestions of automated messages that can be used for more personalized feedback, thereby revolutionizing digital learning by re-incorporating teachers in an efficient and productive way. <br/><br/>The project will enlist teachers to create DRIVER-SEAT. These teachers will use a prototype equivalent to Google's Smart Reply, to establish a library of trusted messages that teachers choose to provide their students. The methodology behind Google's Smart Reply utilizes standard sequence-to-sequence machine learning techniques to automatically generate responses, grouping them into 100 clusters (with each cluster representing a specific semantic intent), and selecting messages from these clusters to suggest to users. In a similar fashion, sequence-to-sequence deep learning techniques are used to generate and suggest messages. However, instead of communicating via email, teachers will be using these messages to provide feedback for their students' math homework. Based on student performance and system-detected affect and behavior, three appropriate feedback responses are selected to initiate interaction with each student. Cooperating teachers will help craft the library by piloting the prototype system and selecting feedback to send their students. Library development will enable machine learning to discover how to help teachers efficiently reply to their students. By implementing this technology, there is great potential to narrow the achievement gap in mathematics classrooms across the nation. This effect could then extend to science, technology, and engineering classrooms in a similar fashion. The transformative aspects of the proposed work will lead to adjustments in the way teachers and students interact in online learning environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813662","III: Small: Mirador: Explainable Computational Models for Recognizing and Understanding Controversial Topics Encountered Online","IIS","Info Integration & Informatics","09/01/2018","08/13/2018","James Allan","MA","University of Massachusetts Amherst","Standard Grant","Maria Zemankova","08/31/2021","$499,682.00","","allan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7364","7364, 7923","$0.00","This project aims to develop algorithms and tools that allow a person to recognize that a web page or other document discusses one or more topics that are controversial -- that is, about which there is strong disagreement within some sizeable group of people. The project will develop algorithms and tools that explain the controversy surrounding the topic, identifying the populations that disagree, the stances that they take, and how those stances conflict with each other. The advances in these algorithms will broaden the research community's understanding of how discussions and disagreements on topics can be modeled computationally and how that resulting information can be conveyed to a general user. The project will assist people in critical evaluation of on-line material and help them understand why a page is educative or why it is not. <br/><br/>The aim of this project is to provide users with tools that illuminate the broader context of the topic or topics of a single page or document that someone finds. Previous work has shown that it is possible to recognize with reasonable accuracy that a document is part of a controversial topic, but that work is fragmented across different genres, demands more robust modeling and more thorough evaluation, and lacks explanatory power that can help a reader understand why and how a text is contentious. In this project, the researchers explore fundamental questions about how controversy can be modeled computationally so that it can be recognized ""in the wild"". The project also explores model variations that allow an algorithm to extract an explanation of the nature of the controversy. The project applies and extends text analysis and comparison techniques. It leverages powerful statistical language modeling methods as well as recent neural network (deep learning) approaches to represent text, its controversial nature, its stances, and their relationships, all extracted from Web pages and other documents. The modeling will be initially used offline to identify collections of topics known to be controversial and then adapt that collection by monitoring slowly-changing news sources and blog postings as well as ephemeral microblog sources of data to capture rapid changes in controversy. The researchers will make the resulting techniques available by providing an open-source example server.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751636","CAREER: A Flexible Optimal Control Framework for Efficient Training of Deep Neural Networks","DMS","COMPUTATIONAL MATHEMATICS","06/01/2018","07/09/2020","Lars Ruthotto","GA","Emory University","Continuing Grant","Leland Jameson","05/31/2023","$221,186.00","","lruthotto@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","MPS","1271","1045, 9263","$0.00","One of the most transformative technologies of our time is deep learning, a form of machine learning that uses neural networks containing many hidden layers. Recent success has led to breakthroughs in applications such as speech and image recognition, nourishing public interest. However, more theoretical insight is needed to create a rigorous scientific basis for designing and training deep neural networks, increasing their scalability, and providing insight into their reasoning. This project develops a new mathematical framework that simplifies designing, training, and analyzing deep neural networks. Due to the broad applicability of deep learning, advances made in this project will benefit a wide range of technologies of high economic and societal impact, e.g., driverless cars, drug discoveries, and web searches. The mathematical theory supporting the new algorithms will increase the acceptance of deep learning for delicate tasks, e.g., cancer prediction and cyber-security. <br/><br/>This project develops a new mathematical framework for deep learning based on the interpretation of deep learning as a dynamic optimal control problem involving nonlinear time-dependent differential equations. This interpretation offers a new way to analyze the successes and failures of deep learning. Advances in deep learning will be made by adapting the wide array of tools available for related optimal control problems, including numerical optimization, partial and ordinary differential equations, inverse problems theory, and parallel processing. Using these currently untapped resources provides rigorous new ways to design and train very deep neural networks that generalize well. Advances in optimal control will be achieved through new hybrid regularization methods, adaptive time discretizations, and large-scale splitting-based optimization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747751","Phase I IUCRC University of Missouri-Kansas City: Center for Big Learning (CBL)","CNS","IUCRC-Indust-Univ Coop Res Ctr","02/01/2018","03/11/2020","Zhu Li","MO","University of Missouri-Kansas City","Continuing Grant","Behrooz Shirazi","01/31/2023","$449,990.00","Praveen Rao, Yugyung Lee, Sejun Song, Reza Derakhshani","lizhu@umkc.edu","5100 Rockhill Road","Kansas City","MO","641102499","8162355839","CSE","5761","5761, 9150","$0.00","This project establishes the NSF Industry/University Collaborative Research Center (I/UCRC) for Big Learning (CBL) to accelerate innovation and impact of Deep Learning in various embedded applications. The vision is to create intelligence towards intelligence-driven society. Through catalyzing the fusion of diverse expertise from the consortium of faculty members, students, industry partners, and federal agencies, CBL seeks to create state-of-the-art deep learning methodologies and technologies and enable intelligent applications, transforming broad domains, such as business, healthcare, Internet-of-Things, and cybersecurity. This timely initiative creates a unique platform for empowering our next-generation talents with cutting-edge technologies of societal relevance and significance. <br/><br/>The University of Missouri at Kansas City (UMKC) site focuses on the deep learning in embedded systems for mobile and IoT applications. It is based on a framework called DeepLite for deep learning model compression and acceleration that can fit cutting edge deep learning capabilities in embedded systems with very limited computing, storage, communication and power capabilities. DeepLite allows embedded deep learning model training and compression for power, storage, computation complexity tradeoffs with learning performances for targeted embedded applications like immersive content capture, depth and action sensing, visual surveillance, next gen image and video compression and communication. <br/><br/>CBL is expected to make wide ranging and long lasting impact to machine learning algorithm, system and application research, accelerating deep learning technology innovation and adoption in the real world, enable transformative new capabilities and new applications in all aspect of society, from education, medicine, media, to security and defense. CBL seamlessly integrates innovation, engineering education, technology business incubation, and community engagement. It facilitates closer interactions and cross pollination of ideas between academia and industry, broaden the research horizon for faculties and students,  while help shrink the time to impact and time to market of new technology. <br/><br/>The center repository will be hosted at http://nsfcbl.org. The data, code, documents will be well organized and maintained on the CBL servers for the duration of the center for more than five years and beyond. The internal code repository will be managed by GitLab. After the software packages are well documented and tested, they will be released and managed by popular public code hosting services, such as GitHub and Bitbucket."
"1811675","III: Small: Deep Learning for Gene Expression Pattern Image Analysis","IIS","Info Integration & Informatics","08/15/2018","08/02/2018","Shuiwang Ji","WA","Washington State University","Standard Grant","Aidong Zhang","01/31/2019","$499,957.00","","sji@tamu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7364","7364, 7923","$0.00","Biological image informatics is an emerging frontier in computational biology, as there is an urgent need to move beyond the manual inspection of images to computational analysis for accelerating scientific discoveries. Conventional methods commonly employ shallow machine learning models in which handcrafted image representations are computed and used in model construction. These approaches heavily rely on prior knowledge on the data and problems to compute appropriate image representations. Motivated by the recent success of deep learning methods in image-related domains, the objective of this project is to develop advanced deep learning models for automated representation learning from biological images. This project also facilitates the development of new courses and laboratory infrastructure for attracting graduate, undergraduate, and high school students, with an emphasis on those from underrepresented groups.<br/><br/>Specifically, this project focuses on the analysis of spatiotemporal gene expression pattern images in fruit fly and mouse. The key challenges lie in how to capture the intrinsic structures of biological problems and how to enable effective model training on small, manually labeled biological data sets. This project develops multi-instance, multi-task, hierarchical, and regularized deep learning models for incorporating the structures of biological problems. The multi-instance and multi-task models capture the complex relationships among inputs and outputs, respectively. The hierarchical and regularized models explicitly encode problem structures and make the results interpretable. In addition, transfer and unsupervised learning methods are developed to enable effective model training on small labeled data sets. These are achieved by integrating both labeled and unlabeled data sets across multiple domains. Altogether, this project is expected to result in a set of advanced deep learning methods for the efficient and effective analysis of biological images.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809936","Dense Polarization-Keyed Fiber Optic Communication System","ECCS","CCSS-Comms Circuits & Sens Sys","08/15/2018","06/16/2020","Kam Wai Chan","OK","University of Oklahoma Norman Campus","Standard Grant","Lawrence Goldberg","07/31/2020","$118,517.00","","cliff.chan@oamphotonics.com","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","ENG","7564","153E, 9150","$0.00","Full Exploitation of Polarization Modulation for Increasing Data Speed in an Integrated Fiber Optic Communication System<br/><br/>Optical fibers form the workhorse of the telecommunications industry today. Their information carrying capacity has risen spectacularly. Even so, demand for higher capacity will continue at an increasingly rapid rate due to the rise of cloud computing, distributed data centers, and the Internet of Things (IoT). The objective of this proposal is to address such a need by fully exploiting the polarization property of light for information transmission that can significantly surpass the capability of the technologies employed in current optical fiber communication systems. The outcomes of the research are expected to give a significant boost to the channel capacity in a single fiber without the need of fiber infrastructure upgrade, hence pushing the capacity limit of existing optical fiber networks. The proposed work is compatible with current and future fiber-optic technologies, making it readily deployable and future-proof. Aside from technology advancement, the project offers undergraduate and graduate students the opportunity to gain practical as well as state-of-the-art technology experiences that are indispensable for the telecommunication industry. The proposal will help train students and equip them with the knowledge of technology and its commercialization leading to creation of new businesses. In turn, this will increase the competitive edge of the United States.<br/><br/>Polarization of light has been used in optical communications for carrying or switching information. Current applications mostly focus on utilizing the two independent polarization channels to double the data transmission capacity of a single-mode optical fiber. The goal of this research is to significantly increase the capacity by exploiting the very large number of possible states of polarization (SOPs) to encode information. This is accomplished by maximizing the number of distinguishable SOPs with the use of optimal quantum receivers for SOP estimation and a machine-learning-based optimal polarization modulation format in an integrated fiber optic communication system. The research will explore this capacity in single-mode fibers through theory as well as experimental results. Research tasks will include the design of optimal polarization receivers for the optical communication link, the combination of polarization shift keying (POLSK) with phase-shift keying (PSK) and quadrature amplitude modulation (QAM), deep neural network-based optimizations for the polarization modulation format, and an experimental testbed incorporating the optimal modulation and detection for the entire optical link composed of the transmitter, channel and receiver. Specifically, the optimal polarization modulation format obtained by machine learning over the integrated transmitter-channel-receiver architecture will give new insights to system optimization that incorporates data-driven information on the optical link impairments. This is in contrast to the conventional approach to optimizing the components of the communication system individually at the expense of optimality of the composite system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835769","Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning","OAC","Data Cyberinfrastructure, EarthCube","10/01/2018","08/13/2018","Pierre Gentine","NY","Columbia University","Standard Grant","Amy Walton","09/30/2021","$307,426.00","","pg2328@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7726, 8074","062Z, 077Z, 7923","$0.00","This project targets a difficult problem in weather and climate prediction -- the representation of convection.  Accurate representation of convection is important, since a majority of current model predictions depend on it.  Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.<br/><br/>The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target.  The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions.  The neural networks will be trained on high-fidelity simulations that explicitly resolve convection.  Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution.  The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835863","Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning","OAC","Climate & Large-Scale Dynamics, Data Cyberinfrastructure","10/01/2018","08/13/2018","Michael Pritchard","CA","University of California-Irvine","Standard Grant","Amy Walton","09/30/2021","$289,409.00","","mspritch@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","5740, 7726","062Z, 077Z, 4444, 7923","$0.00","This project targets a difficult problem in weather and climate prediction -- the representation of convection.  Accurate representation of convection is important, since a majority of current model predictions depend on it.  Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.<br/><br/>The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target.  The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions.  The neural networks will be trained on high-fidelity simulations that explicitly resolve convection.  Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution.  The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816908","NeTS: Small: Machine Learning Meets Wireless Network Optimization: Exploring the Latent Knowledge","CNS","Networking Technology and Syst","10/01/2018","08/20/2018","Yu Cheng","IL","Illinois Institute of Technology","Standard Grant","Alexander Sprintson","09/30/2021","$410,737.00","","cheng@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7363","7923","$0.00","Machine learning has been widely applied in various areas including wireless networking. While the capability of machine learning in classification and pattern recognition has been widely accepted, the role it can play on fundamental research issues in wireless networks is yet to be explored. With the proliferation of heterogeneous networking, wireless network optimization has seen a tremendous increase in problem size and complexity, calling for a paradigm of efficient computation. This project aims at a pioneering study on how to exploit deep learning for significant performance gain in wireless network optimization. Innovative techniques are to be developed for extracting latent knowledge from historical optimization instances, and such knowledge will be leveraged to greatly mitigate the computation complexity in solving new optimization problems. The proposed research seamlessly integrates studies in the areas of optimization, machine learning, graph theory, and wireless networking. This interdisciplinary research will not only provide various training projects to undergraduate and graduate students, but also inspire students to pursue high-quality research with an open-minded and cross-disciplinary perspective. Outcomes from this project may directly benefit the industry with low-complexity yet efficient resource allocation algorithms in practical wireless networks. <br/><br/>This project is expected to contribute a series of new insights and innovative methods in integrating machine learning with wireless network optimization. This study will reveal that properly trained machine learning algorithms can smartly identify critical features (in terms of a small set of critical links or transmission patterns) that lead to optimal or close-to-optimal solutions. The traditional learning framework for data classification cannot be easily tailored for exposing the latent knowledge in wireless network optimization. This project will conduct a systematic study including learning method selection, input/output design, cost function design, training set construction, and parameter tuning, to accommodate the unique needs and requirements for learning from historical optimization instances. This study will demonstrate how the learned knowledge can be exploited to significantly mitigate the computation cost in both centralized optimization and online scheduling. This study will enable people, possibly for the first time, to understand the complex relationship among the input data traffic, internal network features (link or pattern activation), and optimal resource allocation (scheduling or routing).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823276","CRI: II-New: Cognitive Mechanisms and Computational Modeling of Gaze Control During Scene Free Viewing, Visual Search, and Daily Tasks","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/15/2018","08/22/2018","Ali Borji","FL","The University of Central Florida Board of Trustees","Standard Grant","Wendy Nilsen","07/31/2021","$249,277.00","Mark Neider, Ulas Bagci, Mubarak Shah","ali.borji@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7359","7359","$0.00","This project is to build an infrastructure to study visual attention and its applications at the University of Central Florida. The infrastructure will also enable larger-scale inquiries into high-level visual recognition problems in the vision sciences. The infrastructure of fixed and portable eye trackers will be used by several faculty and researchers across the University (e.g., Computer Science, Engineering and Computer Engineering, Psychology, Biomedical engineering and UCF Institute for Simulation and Training) to study human perception, cognition, learning, and motor control, as well as exploring applications in activity recognition, surveillance, and data summarization. <br/><br/>The core goal of this infrastructure is to utilize gaze and eye tracking technology to understand mechanisms of attention from behavioral, neurophysiological, and computational perspectives and explore its applications in a wide range of problems in computer vision and psychology. The team utilizing the infrastructure has complementary expertise to address research encompassing computer vision, machine learning, human vision, imaging, and psychology. In particular, this infrastructure will be used to explore in what ways current attention models fail, how to remedy them, and discover new cues that attract gaze. The approach is a combination of cognitive and computational studies utilizing machine learning and computer vision techniques. New large-scale eye movement datasets and benchmarks will be constructed as part of this proposal.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751720","CAREER: Predictive kinetic modeling of halogenated hydrocarbon combustion","CBET","CFS-Combustion & Fire Systems","07/01/2018","03/09/2018","Richard West","MA","Northeastern University","Standard Grant","Harsha Chelliah","06/30/2023","$503,888.00","","R.West@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","ENG","1407","1045","$0.00","Halogenated hydrocarbons (HHCs) are widely used as both refrigerants and fire suppressants. Driven by environmental and economic considerations, there is rapid innovation in the industry, but the next generation of HHC compounds raise fire safety concerns. Predicting the combustion behavior of these novel HHCs earlier in the design process will save much time, effort, and expense. The chemical kinetic models for describing HHC combustion are highly complex, comprising thousands of elementary reactions involving hundreds of chemical species. To effectively predict these combustion behaviors, we must automate the construction of kinetic models. This project will use a computational approach known as machine learning to help model these complex reacting systems. This breakthrough will enable us to develop an automated reaction mechanism generation tool to create detailed kinetic models for combustion of HHCs. The methodology proposed in this work are not only novel and necessary, but will be widely applicable in other aspects of automated mechanism generation. The integrated educational objective of this CAREER project is to develop a series of computational modules teaching students to solve problems throughout their chemical engineering curriculum.<br/><br/>The research approach is to extend and apply automated Reaction Mechanism Generator (RMG) software to create detailed kinetic models for combustion of any mix of hydrocarbons containing any combination of halogen atoms. Optimized decision-tree and novel convolutional neural network algorithms from the field of machine learning will be extended to enable the necessary restructuring of parameter estimation codes. Quantum chemistry calculations will be automated to supplement literature searches to generate the necessary training data. The model-generating tool will be validated against available experimental data from key example compounds, and used to explain the remarkable combustion behavior of these compounds. The educational program is aligned with the research, developing a series of computational modules that will be integrated into existing classes. These modules will teach students to use Python and SciPy to solve chemical engineering problems. The integration of teaching modules for scientific computing throughout the undergraduate chemical engineering curriculum will help prepare a generation of graduate engineers for a workplace in which data analysis, processing, and computation are increasingly important.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834597","EAGER: SaTC: Tracking Semantic Change in Medical Information","SES","Secure &Trustworthy Cyberspace","05/15/2018","05/22/2018","Ritwik Banerjee","NY","SUNY at Stony Brook","Standard Grant","Sara Kiesler","04/30/2021","$298,823.00","","rbanerjee@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","SBE","8060","025Z, 065Z, 7434, 7916","$0.00","Changes in the meaning of information as it passes through cyberspace can mislead those who access the information. This project will develop a new dataset and algorithms to identify and categorize medical information that remains true to the original meaning or undergoes distortion. Instead of imposing an external true/false label on this information, this project looks into a series of changes within the news coverage itself that gradually lead to a deviation from the original medical claims. Identifying important differences between original medical articles and news stories is a challenging, high risk-high reward venture. Broader impacts of this work include benefits to the research community by making novel contributions to understanding temporal changes in natural language information, as well as social benefits in the form of improved informational tools like question-answering. For the medical domain in particular, understanding temporal distortions and deviations from actual medical findings can reduce occurrences of harmful health choices, for instance, by embedding the research outcomes in news, social media, or search engines. <br/><br/>This project will develop a large dataset of medical scientific publications, and record their characteristics as they change over time across news by designing and developing discrete time-series representations of entities and their attributes and relations. This task will provide the basis for designing and implementing machine learning tasks that exploit stylometric features in natural language in conjunction with temporal distributions to identify and categorize such changes. This research will go beyond current approaches limited to true/false classification of individual articles, and hence be able to identify and analyze information change in narratives, including semantic changes and nuances, or selective emphasis of related information. The research entails an unsupervised and a semi-supervised machine learning approach with bootstrapping, and exploring a binary labeling task to distinguish distorted pieces of information from those that are faithful to the scientific finding, and a multi-label categorization to learn the type of semantic change occurring through time. The dataset will be disseminated via an archival location for natural language processing resources such as the Linguistic Data Consortium (https://www.ldc.upenn.edu/) to facilitate long-term availability to other researchers, and BitBucket or GitHub will be used to ensure the development, maintenance, sharing, and archiving of code.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848608","EAGER: Automatic Identification of Bug Description Elements","CCF","Software & Hardware Foundation","10/01/2018","08/28/2018","Andrian Marcus","TX","University of Texas at Dallas","Standard Grant","Sol Greenspan","09/30/2020","$200,000.00","Vincent Ng","amarcus@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7798","7916, 7944, 8206","$0.00","When an application does not behave the way it is meant to or as expected by the users, they often communicate the problem via a bug-report, which is then used by developers to identify the problem and fix it. To submit a bug-report, users utilize issue-trackers, which allows them to write in natural language a description of the problem they encountered. One problem in bug reporting is the perception gap that exists between bug reporters and developers. Those who report a bug typically only have functional knowledge of an application, even if they have development experience themselves, whereas the software developers have intimate code-level knowledge. Consequently, information in bug-reports are often incomplete, potentially incorrect, or hard to comprehend, which leads to excessive manual effort spent by developers in trying to identify the real source of the problem. This project aims to automatically analyzing bug descriptions in natural language and identifying parts that correspond to the observed behavior of the application, the expected behavior, and the steps that describe what the user did when encountering the problem. The ability to automatically identify these parts of a bug description is important as it allows further analysis which will determine the quality of the reported information and supports developers in solving the problem. In the long run, this award will lead to a new type of bug reporting system that is able to automatically enable users to better describe the problem behaviors that they notice, and in turn, help developers address software problems more productively. The project will also support defining best practices in bug reporting, to be used by software users across the world.<br/><br/>The project combines well-established and highly innovative research solutions from natural language processing, automated discourse analysis, and machine learning. Specifically, the project addresses discourse semantics at statement level, rather than bug report level, and solves the difficult challenge of bug content disambiguation. In addition, it also addresses the problem of identifying relationships between bug description elements, which is essential in supporting future work on automated bug reproduction. The main solution relies on the use of neural networks, which require a substantial amount of manual coding of bug reports. The resulting set of annotated bug reports could be used to support research beyond this project, such as, the translation of natural language test sequences or scenarios into fully automated test cases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1759487","Collaborative Research: ABI Innovation: Interpretable Machine Learning to Identify Molecular Markers for Complex Phenotypes","DBI","ADVANCES IN BIO INFORMATICS","06/01/2018","06/06/2019","Su-In Lee","WA","University of Washington","Continuing Grant","Peter McCartney","05/31/2023","$987,333.00","Matt Kaeberlein","suinlee@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","BIO","1165","","$0.00","Biologists are now able to gather complete sets of gene expression data and protein concentrations for particular targets from specific tissues. The presence and concentrations of these molecules serve as features when determining a diagnostic pattern for specific states of development or disease. The approach to biomarker identification taken in this research attempts to find a set of features (here, gene expression levels) that best predict an outcome (protein levels occurring in the condition). The identified features, biomarkers, can help determine the molecular basis for the condition. Unfortunately, false positive biomarkers are very common, as evidenced by low success rates of replication in independent data sets and therefore low success in such markers becoming important in applications such as diagnostics in clinical practice. We seek to radically shift the current paradigm in biomarker discovery by resolving fundamental problems with the current approach by using novel, theoretically well-founded machine learning (ML) methods to learn interpretable models from data, and follow this up with a systematic experimental validation system in model organisms.  The disease model we are using is for Alzheimer's disease (AD), an urgent national and international research priority. Amyloid plaques and neurofibrillary tangles are the hallmark of AD, and their building blocks are Amyloid-alpha and tau proteins, respectively. These proteins can be measured accurately from human brain tissues, as can global gene expression values. At present, we lack an understanding of the set of genes that affect formation of plaques and tangles, or any protective or pathological responses to these toxic peptides.  <br/><br/>Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data indicate three fundamental problems with this approach. <br/>First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. <br/>To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data by 1) selecting interpretable features, 2) making interpretable predictions, and 3) validating and refining predictions through interventional experiments. This approach has the following aims:<br/>1. Develop NEBULA (network-based unsupervised feature learning) framework to learn interpretable features that will likely provide meaningful phenotype explanations from publicly available multi-omic data sets. <br/>2. Develop a unified framework, called SHAP (Shapley additive explanation), to interpret the predictions of complex models by estimating the importance of each feature to a particular prediction.<br/>3. Validate and refine predictions through interventional experiments using high-throughput assays of gene knockdown on powerful nematode models of proteotoxicity.    <br/>For further information see the project website at: http://suinlee.cs.washington.edu/projects/im3.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750192","CAREER: Fundamental Intelligent Building Blocks of the Intensive Care Unit (ICU) of the Future","IIS","Info Integration & Informatics","04/01/2018","04/12/2019","Parisa Rashidi","FL","University of Florida","Continuing Grant","Sylvia Spengler","03/31/2023","$366,752.00","","parisa.rashidi@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7364","1045, 7364, 9102","$0.00","In the United States, intensive care units (ICUs) costs exceed 4% of national health costs, and ICU mortality rates can be as high as 29%. Precise assessment and prediction of patient status in the ICU can enable early interventions, and can result in improved patient outcomes. However, today's ICUs still face many barriers for assessing and predicting patient status. First, essential information such as pain and functional status are not captured automatically, but rather are repetitively measured by overburdened ICU nurses, with new assessments added each year. Second, existing methods for predicting patient status have limited accuracy and are used infrequently (e.g. every 24 hours as opposed to in real-time). This leads to missing opportunities for early interventions. Finally, existing models cannot automatically incorporate family caregiver feedback for improved patient status prediction. Together, these challenges point to the critical need for developing several fundamental intelligent building blocks of future ICUs. These building blocks should address: (a) how to learn new patient status prediction models without compromising performance on previous prediction models, (b) how to handle the complex clinical data for precise prediction of patient outcome, and (c) how to incorporate family caregiver feedback into the prediction models.<br/><br/>This project will pursue three specific research objectives that will address these issues: (1) Lifelong Multi-Task Learning: Novel multi-task deep learning models will be developed for recognizing clinical expressions and functional activities related to pain and functional status assessments. These models will be customized in an innovative manner to maximize information sharing among related tasks. (2) Multi-Scale and Dynamic Learning: Novel multi-scale recurrent neural networks will be developed to predict precise patient outcomes while handling multiple temporal scales and implicit input changes over time. (3) Continual Opportunistic Learning: Novel active deep learning models will be developed to query the labels of the most informative data points for improving the models over time, with minimum burden on users. The proposed project will bring together novel elements of machine learning algorithms and critical care medicine. This will represent the first attempt to autonomously assess pain and functional status in the ICU, to predict precise patient trajectory from high-resolution data, and to improve predictive clinical models through user feedback. In addition, the research will be impactful because what is learned here, will contribute to a broader understanding of future design considerations for the next generation of lifelong learning systems and intelligent hospitals. The PI will also provide a highly-integrated research and educational program for Florida high school teachers and students, and University of Florida (UF) undergraduate students in the context of the intelligent ICU. The PI proposes to: (1) sponsor summer internships for math teachers, (2) organize an Intelligent Machines workshop on coding and machine intelligence for the high school students, and (3) develop focused research and training activities for undergraduate students. These outreach and training programs will be used to promote interest in science, technology, engineering, and mathematics (STEM) fields among Florida high school students and UF undergraduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836575","EAGER: Collaborative Research: An Unified Learnable Roadmap for Sequential Decision Making in Relational Domains","IIS","Robust Intelligence","09/01/2018","08/18/2018","Ronald Parr","NC","Duke University","Standard Grant","Rebecca Hwa","08/31/2020","$100,000.00","","parr@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495","7495, 7916","$0.00","This project seeks to develop new algorithms and data structures for learning and planning in situations where the environment is represented with a set of relations between objects. Relational representations capture interactions between objects in a succinct and easily interpretable representation. Examples of domains that are well-suited to relational representations includes intelligent drones assisting soldiers, activities in a supply chain management, communication and friendship connections in a social network, and tracking individuals and activities in video.  Most recent advances in machine learning and planning, such as so-called ""deep neural networks"", however, employ simple ""flat"" representations, where the state of the world is an uninterpreted string of bits.  This project will make machine learning and planning methods easier to use and more robust by generalizing them so that they explicitly work with relational models and data.   The methods, theory, and data resulting from this proposal will impact the scientific community in several positive ways and will be made publicly available through an appropriate website. The research will be disseminated through refereed journals and conference proceedings and made available to researchers. Code for the proposed algorithms and descriptions of new benchmark problems will also be made publicly available. The investigators will work on organizing workshops and tutorials based on the challenges and findings arising from this project. <br/><br/>Many special purpose solutions have been developed to address small parts of these problems, but there are no <br/>general purpose tools that harness recent advances in machine learning to tackle this family of problems. This proposal seeks to develop such tools, drawing upon the investigators' prior experience in learning relational regression trees and experience in value function approximation for reinforcement learning. In addition, this project seeks to build a bridge between recent advances in deep learning, which generally has not been compatible with relational representations, and recent advances in relational learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854435","RI: Small: Inverse Rendering by Co-Evolutionary Learning","IIS","Robust Intelligence","09/01/2018","09/19/2018","Jia Deng","NJ","Princeton University","Continuing Grant","Jie Yang","05/31/2020","$230,860.00","","jiadeng@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7495","7495, 7923","$0.00","This project addresses the problem of inverse rendering: recovering 3D shape, material, and lighting from a single image. Inverse rendering is a fundamental problem in computer vision; it recovers the basic properties of a visual scene, and serves as a foundation for higher-level scene understanding such as recognizing objects, actions, and functionalities. Despite its fundamental importance, inverse rendering remains difficult. Solving inverse rendering can significantly advance computer vision and benefit a wide variety of applications from autonomous driving to assisting the visually impaired. This project develops new machine learning algorithms to advance the state of the art of inverse rendering. In addition, the project contributes to education and diversity by integrating research results into courses at various levels and by recruiting underrepresented groups to participate in this research. <br/><br/>This research advances inverse rendering technologies using computer graphics and machine learning. In particular, the research team develops two machine learning systems that co-evolve as adversaries: a rendering system that learns to compose 3D scenes and renders images using a graphics engine, and an inverse rendering system that learns to recover shape, material, and lighting from the rendered images. To develop the rendering system, the research team investigates new learning algorithms for adaptive, automatic scene composition. To develop the inverse rendering system, the research team investigates new learning algorithms that integrate neural networks and physics-based vision."
"1818201","NSF Workshop on Real-time Learning and Decision Making of Dynamical Systems. To Be Held at NSF, February 12-13, 2018.","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","02/15/2018","02/15/2018","Le Xie","TX","Texas A&M Engineering Experiment Station","Standard Grant","Anthony Kuh","01/31/2019","$99,670.00","","le.xie@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","7607","155E, 1653, 7556","$0.00","The goal of the workshop is to have a group of leading experts that have complementary background (in the area of control, signal processing, machine learning, communication, power and energy, transportation, etc.) to cross the bridge among various research areas and shape the research paradigm that arises from real-time learning for data-driven dynamical systems. Specifically, we have identified the following data-rich dynamical engineering systems: power and energy systems, transportation systems, as well as signal and information processing systems. Topics for discussions include various real time learning approaches for the engineering systems (such as deep learning architectures, model-based learning, model-free learning, reinforcement learning, etc.), data representation for engineering systems (including the research problems in feature extraction, graphical models, real time unsupervised learning, etc.), and the potential solutions for closing the loop around data. The workshop will examine control, signal processing, machine learning, communication, power and energy, and transportation systems. <br/><br/>Intellectual Merit: <br/><br/>The workshop will address key questions in real-time learning and decision making that arises from dynamic data-driven engineering systems. Topics of interest include different perspectives of learning and decision making from control, signal processing, machine learning, computational intelligence, and domain applications' viewpoint. They will synergistically contribute towards two of the ten big ideas from NSF; ``Harnessing Data for 21st Century Science and Engineering'' and ""Growing Convergent Research at NSF''. <br/><br/><br/>Broader Impacts: <br/><br/>This workshop will engage and promote convergent research centered around real-time decision making and data in engineering systems. The discussions will have significant impact on the research and education of machine learning and large-scale data-driven engineering systems. The workshop will also generate opportunities for collaborative research between academia and industry. Ideas will also be generated for possible innovation and entrepreneurship. The workshop report will serve as an important resource for the scientific community and the general public."
"1755701","CRII: RI: Learning with Low-Quality Visual Data: Handling Both Passive and Active Degradations","IIS","Robust Intelligence","08/15/2018","08/03/2018","Zhangyang Wang","TX","Texas A&M Engineering Experiment Station","Standard Grant","Jie Yang","07/31/2021","$173,043.00","","atlaswang@utexas.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7495","7495, 8228","$0.00","This project is focused on effectively and robustly exploiting low-quality (LQ) visual data for computer vision tasks. While most current computer vision systems are designed for high-quality visual data, collected from ""clear"" environments where subjects are well observable without significant attenuation or alteration, a dependable vision system must reckon with the entire spectrum of degradations from unconstrained environments. With various degradations arising from the visual data acquisition and processing pipeline, the ubiquitous LQ visual data can dramatically deteriorate the model performance in practice. The project outcome can broadly benefit a variety of real-world applications, such as video surveillance, autonomous/assisted driving, robotics and medical image analysis, where LQ visual data has constituted major performance and reliability bottlenecks. <br/><br/>This research categorizes common degradations into the two types: ""passive degradations"" that are caused by uncontrollable environment factors (such as bad weather and low light); and ""active degradations"" that are intentionally introduced in a controllable way to meet certain budget requirements (such as lossy compression). The project will mainly addresses two important technical questions: i) how to overcome passive degradations and achieve more robust high-level task performance on LQ video data, using end-to-end deep learning models; and ii) how to properly introduce and control active degradations to generate the desired form of LQ data, that both satisfies certain budget requirements and maintains the target task utility, using deep adversarial learning models. The resulting new techniques are to be verified on application examples such as video recognition, video annotation, video compression, and de-identified video data sharing for recognition purpose.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750326","CAREER: Interpretable Deep Modeling of Discrete Time Event Sequences","IIS","Info Integration & Informatics","07/01/2018","06/01/2020","Fei Wang","NY","Joan and Sanford I. Weill Medical College of Cornell University","Continuing Grant","Wei Ding","06/30/2023","$425,920.00","","few2001@med.cornell.edu","1300 York Avenue","New York","NY","100654805","6469628290","CSE","7364","1045, 7364","$0.00","Discrete Time Event Sequences (DTES) are ordered event sequences with a concrete timestamp associated with each event. DTES are ubiquitous in our daily life. One representative example is patient electronic health records. Computational modeling of DTES can reveal the hidden event evolving mechanisms and improve the performance of endpoint analytical tasks such as sequence forecasting and grouping. Conventional approaches for analyzing DTES are typically based on strong statistical assumptions and may not work well in practice. Motivated by the recent empirical success of deep learning methods in various application domains, the objective of this project is to develop interpretable deep learning approaches for modeling DTES. This project validates the utility of the developed algorithms in various medical applications. It incorporates the resulting research outcomes into curriculum development and courses, to train a new generation of machine learning and data mining practitioners. In addition, special training opportunities are provided to high school students and community college students for a broader education of modern data analysis techniques.<br/><br/>This project consists of three synergistic research thrusts. First, it develops a series of approaches for integrating external domain knowledge into the modeling process. This guarantees the learned models align well with the domain knowledge and at the same time provides effective regularizations to avoid overfitting. Second, it devises approaches based on mimic learning and pattern dissection to interpret the knowledge hidden in the learned models. This makes the learned models much more practical and reusable. Third, effective model and data sharing mechanisms are developed to transfer the knowledge across similar learning tasks. This maximizes the utilizations of the available samples for each task by leveraging the task relationships. Two key problems in medical domain, hospital readmission and disease phenotyping, are used as the target applications for validating the proposed approaches based on several real-world large-scale patient electronic health record data sets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836565","EAGER: Collaborative Research: An Unified Learnable Roadmap for Sequential Decision Making in Relational Domains","IIS","Robust Intelligence","09/01/2018","08/18/2018","Sriraam Natarajan","TX","University of Texas at Dallas","Standard Grant","Rebecca Hwa","08/31/2021","$98,522.00","","sriraam.natarajan@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","7495, 7916","$0.00","This project seeks to develop new algorithms and data structures for learning and planning in situations where the environment is represented with a set of relations between objects. Relational representations capture interactions between objects in a succinct and easily interpretable representation. Examples of domains that are well-suited to relational representations includes intelligent drones assisting soldiers, activities in a supply chain management, communication and friendship connections in a social network, and tracking individuals and activities in video.  Most recent advances in machine learning and planning, such as so-called ""deep neural networks"", however, employ simple ""flat"" representations, where the state of the world is an uninterpreted string of bits.  This project will make machine learning and planning methods easier to use and more robust by generalizing them so that they explicitly work with relational models and data.   The methods, theory, and data resulting from this proposal will impact the scientific community in several positive ways and will be made publicly available through an appropriate website. The research will be disseminated through refereed journals and conference proceedings and made available to researchers. Code for the proposed algorithms and descriptions of new benchmark problems will also be made publicly available. The investigators will work on organizing workshops and tutorials based on the challenges and findings arising from this project. <br/><br/>Many special purpose solutions have been developed to address small parts of these problems, but there are no <br/>general purpose tools that harness recent advances in machine learning to tackle this family of problems. This proposal seeks to develop such tools, drawing upon the investigators' prior experience in learning relational regression trees and experience in value function approximation for reinforcement learning. In addition, this project seeks to build a bridge between recent advances in deep learning, which generally has not been compatible with relational representations, and recent advances in relational learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811745","Methodology for Multi Time-Scale Nonlinear Dynamical Spatio-Temporal Statistical Models","DMS","STATISTICS","08/01/2018","07/20/2020","Christopher Wikle","MO","University of Missouri-Columbia","Continuing Grant","Gabor Szekely","07/31/2021","$225,000.00","","wiklec@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","MPS","1269","9251","$0.00","Scientists and engineers are increasingly aware of the importance of accurately characterizing various sources of uncertainty when trying to understand complex systems such as those that vary across time and space.  Examples of such systems include how ocean heating influences convective clouds in the tropics, which in turn, can influence severe weather and habitat conditions over North America; or, how a migratory species interacts with its environment and competitive pressures from both predators and prey.   When performing statistical modeling on such complex spatio-temporal phenomena, the scientific goal is typically either inference, prediction, or forecasting, all of which require some measure of uncertainty.  To accomplish these goals through modeling, one must synthesize information from a variety of sources, including direct observations, indirect (remotely sensed) observations, surrogate observations (mechanistic model output), previous empirical results, expert opinion, and scientific knowledge.  This information must then integrate into a process model that can represent the complexity of the interacting processes, and account for uncertainty.  This research is concerned with building these models in a way that can account for complex interactions across different time scales.<br/><br/><br/>This project concerns the development of a methodological framework for parsimonious  and computationally efficient models for multi time-scale nonlinear dynamical spatio-temporal processes that accounts for the interaction across processes and time scales in such a way as to accommodate uncertainty in data, processes, and parameters.  In particular, the project will focus on a hybrid model that combines elements of a generalized quadratic nonlinear spatio-temporal dynamical model with a recurrent neural network model.  However, this methodology will focus on models for processes that involve multiple time scales of variability.  This will include the development of computationally efficient algorithms that can deal with the extreme curse of dimensionality in the state and parameter spaces associated with complex interacting nonlinear phenomena by adapting, extending and combining approaches from both statistics and machine learning. Not only will the proposed modeling and computational methodology be an advancement in statistics, but it will be useful across a broad range of disciplines that deal with complex multi time-scale dynamical processes such as brain science, climatology, demography, econometrics, fisheries, ecology, meteorology, oceanography, and wildlife biology.  In addition, the project will contribute to STEM education through training a graduate research assistant, who will gain inter-disciplinary experience.  In addition, the project will foster undergraduate interest in the STEM disciplines by employing undergraduate research assistants to help with the development of visualization tools for spatio-temporal data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836060","SCH: EXP: RadiOptiMeter: Long-Term and Fine-Grained Breathing Volume Monitoring for Sleep Disordered Breathing (SDB)","IIS","Smart and Connected Health","01/01/2018","08/04/2018","Tam Vu","CO","University of Colorado at Boulder","Standard Grant","Wendy Nilsen","08/31/2020","$420,480.00","","tam.vu@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8018","8018, 8061","$0.00","Sleep disordered breathing (SDB) in children is considered to be a public health problem with serious consequences such as decreased cognitive function, poor school performance, daytime sleepiness and increased cardiovascular risk. Current diagnosis of SDB is performed in hospital sleep laboratories by monitoring patients with a host of cardiorespiratory sensors attached at various positions on the patient?s body. This obtrusive form of monitoring is inconvenient to patients and require tremendous amount of attention from technicians to ensure study quality. Children, especially, tolerate the study very poorly; often removing sensors, having trouble sleeping, and necessitating repeat investigations. This project aims to develop a new method to remotely and continuously monitor breathing volume and breathing patterns of human subjects during sleep studies using optical signals assisted by radio frequency signals. We introduce RadiOptiMeter, a hybrid radio-optical breath volume monitoring approach that couple the unique characteristics of radio frequency (RF) signals with image stream captured by a depth-CO2-thermal camera to accurately estimate breathing volume of sleeping patients from afar. This study is the first step in the development of non-invasive respiratory monitoring. Utilizing this novel and non-invasive device to measure breathing during sleep will begin a research program to promote future diagnosis of SDB in the comfort of the child?s home, with no in-hospital laboratory expenses or the expense of the disposable medical equipment which equates to thousands of dollars a year in each sleep laboratory.<br/> <br/>This project will investigate a new method to continuously monitor breathing volume and breathing patterns of humans during in-hospital sleep studies using radio frequency and optical signals. We introduce RadiOptiMeter, a hybrid radio-optical breath volume monitoring approach that couple the unique characteristics of radio frequency (RF) signals with image stream captured by a depth-CO2-thermal camera to accurately and continuously estimate breathing volume of sleeping patients from afar. We propose techniques to address challenges brought about by the body movement during sleep, environmental wireless signal noises, and the diversity of patient populations. An expected outcome is a robust and accurate breathing volume monitoring system for SDB studies. The cooperation between each of the proposed devices allows us to exploit the synergistic effects of the devices to cover the limitations imposed by each device type and provides system redundancies. These redundancies ensure reliability for long-term monitoring tasks, which are critical for clinical applications. Our proposed research will make the following key contributions to enable new non-contact vital signal monitoring system: (1) Analytical models, experimental tools, and evaluation results of a breathing volume estimation method using a vision-based system (VVE) which include 4D volumetric model and skeletal structure analysis from depth- CO2-thermal (DCT) camera outputs. (2) Analytical models, experimental hardware and software components, and evaluation results of a RF-based breathing volume estimation (RVE) system, that uses neural-network-based machine learning for chest displacement- to-volume matching. (3) A hybrid radio-optical breathing volume estimation system (RadiOptiMeter) that synergistically combines the VVE and RVE to perform continuous and fine-grain monitoring. RadiOptiMeter includes body movement tracking, automatic antenna steering, and a set of controlling and synchronizing algorithms for a harmonic integration of the whole system. This collaborative effort between researchers at the Department of Computer Science and Engineering and medical doctors at Sleep Medicine Research at the Children?s Hospital Colorado is the first step in the development of non-invasive respiratory monitoring. A novel non-invasive device to measure breathing during sleep will be the first step in the necessary foundational research to promote future diagnosis of SDB in the comfort of the child?s home, with no in-hospital laboratory expenses or the expense of the disposable medical equipment which equates to thousands of dollars a year in each sleep laboratory. This project also provides an excellent methodd to train graduate students to conduct this vision-based research project. The RadiOptiMeter concepts serves as an exciting and appealing tool for structuring a variety of educational activities. Moreover, the project results will be disseminated through scholarly publications and active outreach through our existing and potential industrial partners."
"1839387","EAGER/Collaborative Research: Real-Time: Hybrid Control Architectures Combining Physical Models and Real-time Learning","CMMI","Special Initiatives","09/01/2018","08/20/2018","Luis Duffaut Espinosa","VT","University of Vermont & State Agricultural College","Standard Grant","Robert Landers","08/31/2020","$149,879.00","","lduffaut@uvm.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","ENG","1642","030E, 034E, 7916, 8024, 9102, 9150","$0.00","Machine learning has become a focus of many researchers as effective solution to many complex engineering problems. At its core machine learning are the methods that provide computers ways to learn using available data. Artificial neural networks (ANN) have traditionally been the backbone of machine learning methods. While these learning systems certainly have their strengths, they also have limitations in the context of control engineering. For example, physics based models often provide key physical insight into the design of control systems for power grids, autonomous vehicles, and robots. So completely discarding such models in the context of learning based control systems is often counterproductive. This EArly-concept Grant for Exploratory Research (EAGER) project aims to develop a new, foundational and innovative control architecture which combines the advantages of model based design methods with those of real-time learning. The architecture is based on recent advances in the mathematical modeling of dynamical systems. While well suited for a variety of applications in engineering, biology, and ecology, the target application is the safe and reliable control of smart grids.  The latter are clearly of vital importance for future economic development and the security of the nation's constantly evolving energy distribution system. Project outcomes will provide practical solutions to complex energy management problems involving uncertain power demands, energy limits, and use of renewable resources while at the same time maintaining grid stability and reliability.<br/><br/>The hybrid control architecture involves a given system and an assumed physical model both driven by the same control input. The measured difference between their outputs defines an error system. The key idea is to use a generic input-output representation known as a Chen-Fliess functional series to describe this unknown error system. The series coefficients are estimated in real-time via a minimum mean-square error estimator. Effectively, the conventional artificial neuron is replaced here by this new type of learning unit to approximate the error system. The control problem is solved via predictive control using the assumed model and the learned error system. The enabling technology is recent advances in the numerical approximation of Chen-Fliess series which make it possible to implement the scheme in discrete-time. The specific objectives of the project are to (1) advance the theoretical foundations that underpin real-time learning for control applications, including the cascading of these new learning units for deep learning (2) optimize and adapt the novel theoretical results for real-time control of smart grids to provide a priori performance guarantees. The main problem here lies in the uncertainty coming from the over-simplified/poorly modeled dynamics of the grid in addition to the action of renewable resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839378","EAGER/Collaborative Research: Real-Time: Hybrid Control Architectures Combining Physical Models and Real-time Learning","CMMI","Special Initiatives","09/01/2018","08/20/2018","W. Steven Gray","VA","Old Dominion University Research Foundation","Standard Grant","Robert Landers","08/31/2020","$148,556.00","","sgray@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","ENG","1642","030E, 034E, 7916, 8024","$0.00","Machine learning has become a focus of many researchers as effective solution to many complex engineering problems. At its core machine learning are the methods that provide computers ways to learn using available data. Artificial neural networks (ANN) have traditionally been the backbone of machine learning methods. While these learning systems certainly have their strengths, they also have limitations in the context of control engineering. For example, physics based models often provide key physical insight into the design of control systems for power grids, autonomous vehicles, and robots. So completely discarding such models in the context of learning based control systems is often counterproductive. This EArly-concept Grant for Exploratory Research (EAGER) project aims to develop a new, foundational and innovative control architecture which combines the advantages of model based design methods with those of real-time learning. The architecture is based on recent advances in the mathematical modeling of dynamical systems. While well suited for a variety of applications in engineering, biology, and ecology, the target application is the safe and reliable control of smart grids.  The latter are clearly of vital importance for future economic development and the security of the nation's constantly evolving energy distribution system. Project outcomes will provide practical solutions to complex energy management problems involving uncertain power demands, energy limits, and use of renewable resources while at the same time maintaining grid stability and reliability.<br/><br/>The hybrid control architecture involves a given system and an assumed physical model both driven by the same control input. The measured difference between their outputs defines an error system. The key idea is to use a generic input-output representation known as a Chen-Fliess functional series to describe this unknown error system. The series coefficients are estimated in real-time via a minimum mean-square error estimator. Effectively, the conventional artificial neuron is replaced here by this new type of learning unit to approximate the error system. The control problem is solved via predictive control using the assumed model and the learned error system. The enabling technology is recent advances in the numerical approximation of Chen-Fliess series which make it possible to implement the scheme in discrete-time. The specific objectives of the project are to (1) advance the theoretical foundations that underpin real-time learning for control applications, including the cascading of these new learning units for deep learning (2) optimize and adapt the novel theoretical results for real-time control of smart grids to provide a priori performance guarantees. The main problem here lies in the uncertainty coming from the over-simplified/poorly modeled dynamics of the grid in addition to the action of renewable resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747360","SBIR Phase I:  Addressing the memory bottleneck in deep neural networks in cloud platforms","IIP","SMALL BUSINESS PHASE I","01/01/2018","12/27/2017","Farnood Merrikh Bayat","CA","Mentium Technologies Inc.","Standard Grant","Peter Atherton","09/30/2018","$224,586.00","","farnoodmb@mentiumtech.com","2208 Pacific Coast Dr","Goleta","CA","931175494","8056176245","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will consist in defining the way toward an ultra-fast and energy efficient accelerator for Machine Learning applications deployed on cloud computing. The merging of cloud computing and Machine Learning is shaping our everyday life experience. Examples of applications running on the cloud and exploiting Machine Learning algorithms include data mining, natural language processing and pattern recognition. These three together represent cognitive computing and, due to a vast and growing number of APIs for developers, it is becoming easier to access the computational power of the cloud and develop new applications. This new computation potential is used by businesses to connect data and find patterns valuable for commerce or to improve cybersecurity.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will define a new kind of hardware accelerator, able to speed up cognitive computation by orders of magnitude while reducing energy consumption compared with state-of-the-art processors. The proposed technology is fast and energy efficient, but can be prone to low precision and temperature variation sensitivity. During Phase I, the company will define the hardware accelerator at the system level, optimizing the design for ultra-high speed and sufficient precision to carry out the cognitive computation required. At the same time, the effect of temperature variation and noise will be minimized through improved design. Finally, the energy consumption of the new designs will be estimated and compared with the overall performance of state-of-the-art competitive architectures."
"1755676","CRII: III: Statistical Learning and Inference Methods for Automated Data Cleaning","IIS","Info Integration & Informatics","09/01/2018","03/28/2018","Theodoros Rekatsinas","WI","University of Wisconsin-Madison","Standard Grant","Wei Ding","07/31/2021","$175,000.00","","thodrek@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7364","7364, 8228","$0.00","The recent investments in information retrieval, natural language processing, and AI have enabled computers to interpret what they see; read and analyze unstructured data; answer complex questions; and interact with their environment. There is a hidden catch, however: the reliance of all these state-of-the-art systems on high-effort tasks like data preparation and data cleaning. It is estimated that 70% to 80% percent of the time devoted on analytics projects is spent on checking and organizing data. The challenge is that data collection often introduces incomplete, erroneous, replicated, or conflicting data records. The burden of data preparation has led to many efforts in automating isolated tasks related to data cleaning, such as record de-duplication. However, success with end-to-end data cleaning has been limited, especially in the presence of critical data driven applications. Here, human engagement is normally required to guide and evaluate the impact of data cleaning. This project investigates the design of partly-automated, interactive data cleaning systems that are efficient for large-scale applications and come with formal accuracy guarantees. <br/><br/>The emphasis of this work is on data cleaning methods that combine human expertise with statistical learning and probabilistic inference to model the inherent noise of raw data; and repair incomplete, inconsistent or erroneous records. The main hypothesis driving this work is that statistical learning allows us to reason about heterogeneous signals that are indicative of the correct latent value of a data record. This project will develop a formal statistical framework for data cleaning and weakly supervised machine learning solutions for interactive data cleaning over structured or semi-structured data. The outcomes of this project will have the power to significantly ease the currently challenging procedure of manually inspecting data to be used in downstream analytical tasks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1726188","MRI: High Performance Digital Pathology Using Big Data and Machine Learning","CNS","Major Research Instrumentation, Information Technology Researc, Special Projects - CNS","01/01/2018","09/15/2017","Joseph Picone","PA","Temple University","Standard Grant","Rita Rodriguez","12/31/2020","$400,000.00","Iyad Obeid, Tunde Farkas, Yuri Persidsky","joseph.picone@gmail.com","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","1189, 1640, 1714","1189","$0.00","This project, developing a digital imaging system, aims to automatically characterize an enormous archive of digital images from a pathology lab. Based on open source programming language packages and using deep learning technologies, the effort entails developing software that will automatically annotate and classify these images. Thus the system consists of an annotated archive tool for high performance digital pathology involving digital images from pathology slides produced in clinical operations. This tool, presently unavailable, enables observation, annotation, and classification of images from tissues in pathology slides in order to create a very large data base that may be analyzed with algorithms that are designed to process and interpret the image data. By applying state of the art machine learning, the effort is expected to generate a sustainable facility to rapidly collect large amounts of data automatically. This facility enables deep learning systems to systematically address many operational challenges, such as ingestion of large, complex images.<br/><br/>Broader Impacts:<br/>The instrumentation provides a useful technology capability.  The work builds on the researchers' history of providing unencumbered resources for fields including human language technology and neuroscience. Several large, comprehensive databases of pathology slides will be released in an unencumbered manner; no comparable databases currently exist in terms of the quantity of data proposed. The urban setting of the project, as well as the diverse nature of the institution's client population, make it ideal for collecting this type of clinical data.  A new generation of healthcare professionals will be trained using these resources to validate their knowledge in the longer term."
"1750656","CAREER: Algorithm-Centric High Performance Graph Processing","CCF","Software & Hardware Foundation","02/01/2018","05/18/2020","Xuehai Qian","CA","University of Southern California","Continuing Grant","Almadena Chtchelkanova","01/31/2023","$254,524.00","","xuehai.qian@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","1045, 7941","$0.00","With the advent of big data, large amounts of data are collected from numerous sources, such as social media, sensor feeds, and scientific experiments. Graph analytics has emerged as an important way to understand the relationships between heterogeneous types of data, allowing data analysts to draw valuable insights from patterns in the data for a wide range of applications, including machine learning tasks, natural language processing, anomaly detection, clustering, recommendation, social influence analysis, bioinformatics. Due to the broad applications, the research community tackled graph processing from multiple angles, including distributed, disk-based systems and in-memory graph processing. There are four key problems of today's graph processing research: 1) the gap between programming model and algorithm; 2) the lack of diversity in applications studied; 3) insufficient research on dynamic graphs and graph database; and 4) architectural supports focus only on classical problems. This proposal attempts to advance the graph processing systems by solving these major challenges.<br/><br/>This research proposes a novel approach ALCHEM, algorithm-centric high performance graph processing, which involves the collaborative designs of algorithms, programming model, systems, and architecture. This interdisciplinary research program takes the opportunity to explore or enhance the interactions between different layers, with the emphasis on algorithm efficiency. It contains four research thrusts: (1) Using graph abstraction as a bridge between programming model and algorithm to speed up the convergence; (2) Developing efficient execution model with specialization; (3) Building a graph database as a unified engine for relational and dynamic graph data; (4) Enhancing architecture with novel features to support new graph algorithms (e.g., random walk). The research will trigger close interactions between researchers in theory, system, and architecture. The project will engage women, minorities and undergraduates. Uniquely, it will not only train the students' system building skills, but also strengthen their algorithm understanding. The research outcomes will benefit the society by improving everyday life with better and faster recommendations, enhanced security, and better social relationships."
"1746192","SBIR Phase I:  Machine Assisted Comparative Policy Analysis in Public Health","IIP","SBIR Phase I","01/01/2018","12/27/2017","Michael Korostelev","PA","Legal Science Partners LLC","Standard Grant","Peter Atherton","12/31/2018","$224,355.00","","mike@legalscience.io","401 Woodside Ave.","Narberth","PA","190722332","2679941749","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to enhance the capabilities of a research tool for companies, legal experts and researchers undertaking nationwide comparative policy analysis in the public health domain. The tool will assist experts in identifying relevant policy documents by determining and scoring the significance of statutory provisions in context of specific legal questions. The quantitative approach can enable novel policy tracking. Rather than experts setting up alerts for updates to a specific set of documents, this tool learns from the legal text used to answer legal questions to allow for real time tracking and discovery of updates and other relevant documents. This approach to policy tracking can present experts with timely information on updates, along with revealing new documents as they are introduced.  Timely analysis can inform policy-makers, facilitating the crafting of optimized evidence-based public health legislation.  Reducing the cost and effort of the most time-consuming aspects of legal research can make precise scientific policy analysis affordable and accessible commercially and in real time. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project will decrease the time required to produce timely analysis of public health policy across 50 states. This research will apply machine learning, natural language processing and graph theory techniques to extract logical legal ontologies by computing similarities of public health provisions in statutory text. In domain specific problems, large sets of examples of annotated text are required. In the legal domain there is little available expert-labeled legal corpora and purposefully curating this kind of dataset is prohibitively expensive.  To address this challenge, the proposed solution integrates transparently into legal experts' workflow while generating ontology that mirrors the approach of a domain expert. The second challenge is that searching for patterns in the relations of a very large network of documents can be very expensive computationally. The proposed solution addresses this by extracting clues from the expert workflow to identify shortcuts that simplify and constrain the larger problem. These clues, combined with sparse expert labeled data can produce a more accurate baseline for optimization of scoring and similarity comparison of larger sets. By being integrated into more workflows, the transparent annotation process and algorithm could be applied to other policy domains."
"1851588","CAREER: Towards an Intermittent Learning Framework for Smart and Efficient Cyber-Physical Autonomy","CNS","CPS-Cyber-Physical Systems","08/01/2018","08/14/2019","Kyriakos G Vamvoudakis","GA","Georgia Tech Research Corporation","Continuing Grant","Sandip Roy","04/30/2023","$287,252.00","","kyriakos@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7918","1045, 7918","$0.00","This project expands how reinforcement learning frameworks can be used for Cyber-Physical Systems (CPS) for autonomy. The research utilizes intermittent reinforcement, where a reward is not given every time the desired response is performed. This differs from traditional reinforcement learning mechanisms, in which a reward is given for each point during online training. What is novel in this framework is that it can demonstrate how reinforcement learning can be used when rare events, or noisy and adversarial data, can affect the training and performance of these algorithms. The work will be validated on collaborative road freight transport and collaborative robotics testbeds, through international partnerships with Sweden and the United Kingdom. The project includes activities that integrate high-school students into challenging problems in machine learning areas, motivated through drone racing competitions.<br/><br/>The goal of this research is to expand foundational knowledge through deepened ties between the learning, control, game theory, and CPS communities. The approach is to, (i) unify new perspectives of learning in engineering with respect to resiliency, bandwidth efficiency, robustness, and other aspects that cannot be achieved with the state-of-the-art approaches; (ii) develop intermittent deep learning methods for CPS that can mitigate sensor attacks and can handle cases of limited sensing capabilities; (iii) incorporate nonequilibrium game-theoretic learning in CPS with components whose decision-making, rationality, and information usage are fundamentally different; and (iv) investigate ways to transfer learning to new platforms. The project's education and outreach component includes internships that will lead to technology transfer, summer camps with a special focus on reaching out to underrepresented minorities and women, and collaboration with institutions in Sweden and the United Kingdom through student exchange programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730574","Collaborative Research: Computational Photo-Scatterography: Unraveling Scattered Photons for Bio-Imaging","IIS","Special Projects - CCF, Robust Intelligence, Expeditions in Computing","03/01/2018","05/06/2020","Ashutosh Sabharwal","TX","William Marsh Rice University","Continuing Grant","Jie Yang","02/28/2023","$2,077,348.00","Rebecca Richards-Kortum, Richard Baraniuk, Lin Zhong, Ashok Veeraraghavan","ashu@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","2878, 7495, 7723","7723, 9251","$0.00","Much of the success of today's healthcare is due to rapid advances in our ability to collect and analyze high-resolution data about the human body. However, current methods to achieve cellular resolution are invasive (e.g., blood test or tissue biopsy), and non-invasive imaging modalities do not achieve cellular resolution. The principal goal of this Expeditions project is to develop computational imaging systems for non-invasive bio-imaging, deep beneath the skin, and at cellular-level resolutions. This project has the potential to fundamentally impact healthcare and medicine, by enabling live views of cross sections of human anatomy, simply by pointing a camera at any part of the body. This would put individual users at the center of their healthcare experience and make them true partners in their healthcare delivery. The health imaging devices that result from this project will act as an important pillar in the personalized medicine revolution. This research expedition also holds the potential to launch new healthcare paradigms for chronic disease management, pediatrics, low-resource healthcare, and disaster medical care. Beyond healthcare, making progress on the problem of cellular-scale deep-tissue imaging using light will push the frontiers of the fundamental problem of inverse scattering, which impacts numerous areas of science and engineering. The order of magnitude advances made in inverse scattering and imaging through scattering media will have significant cross-cutting applications in diverse areas such as basic science, consumer imaging, automotive navigation, robotics, surveillance, atmospheric science, and material science. Finally, projects with a single, easy-to-appreciate, and high-impact goal have the potential to inspire the next generation of scientists, attract diverse set of students driven by humanitarian and social causes, and become a platform for inclusion and innovation.<br/><br/>The overarching goal of this project is to develop, test, and validate new computational imaging systems, to non-invasively image below the skin at tunable depths, in highly portable form-factors such as wearables or point-of-care devices. The main challenge is that light scatters as it travels through the human body, and in this process, the spatial information from different points within the body gets mixed up. A new concept, Computational Photo-Scatterography (CPS), is being applied in this project in order to computationally unravel the scattered photons in an imaging system, and allow creation of sharp images and accurate inferences. Recognizing that the brute-force complexity of unraveling scattered photons is prohibitively high, the project uses a computational co-design framework that leverages advances by team members from multiple domains: programmable illumination and optics, image sensors, machine learning, inverse graphics, and hybrid analog-digital computing.  The project will use machine learning (ML) instead of physics-based de-scattering to speed up the solution of the underlying inverse problem. A combination of physics-based inverse graphics algorithms, and ML algorithms combining deep learning and generative modeling will be used to estimate tissue scattering parameters -  motion due to blood flow induces time-variation in tissue parameters, which makes solving the inverse scattering problem more difficult. The project will use ML to create fast but approximate estimators, which will serve as accelerators for inverse scattering. The development of new sensors, able to capture the data necessary to reconstruct the structure of the tissue deep below the skin, constitutes the most important contribution of the project. These systems and algorithms will have the potential to break the current resolution limits of noninvasive bio-imaging by nearly two orders of magnitude, enabling cellular-level imaging at depths far beyond currently possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730147","Collaborative Research: Computational Photo-Scatterography: Unraveling Scattered Photons for Bio-Imaging","IIS","Special Projects - CCF, Expeditions in Computing","03/01/2018","06/25/2019","Srinivasa Narasimhan","PA","Carnegie-Mellon University","Continuing Grant","Jie Yang","02/28/2023","$1,195,899.00","Artur Dubrawski, Ioannis Gkioulekas","srinivas@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878, 7723","7723, 9251","$0.00","Much of the success of today's healthcare is due to rapid advances in our ability to collect and analyze high-resolution data about the human body. However, current methods to achieve cellular resolution are invasive (e.g., blood test or tissue biopsy), and non-invasive imaging modalities do not achieve cellular resolution. The principal goal of this Expeditions project is to develop computational imaging systems for non-invasive bio-imaging, deep beneath the skin, and at cellular-level resolutions. This project has the potential to fundamentally impact healthcare and medicine, by enabling live views of cross sections of human anatomy, simply by pointing a camera at any part of the body. This would put individual users at the center of their healthcare experience and make them true partners in their healthcare delivery. The health imaging devices that result from this project will act as an important pillar in the personalized medicine revolution. This research expedition also holds the potential to launch new healthcare paradigms for chronic disease management, pediatrics, low-resource healthcare, and disaster medical care. Beyond healthcare, making progress on the problem of cellular-scale deep-tissue imaging using light will push the frontiers of the fundamental problem of inverse scattering, which impacts numerous areas of science and engineering. The order of magnitude advances made in inverse scattering and imaging through scattering media will have significant cross-cutting applications in diverse areas such as basic science, consumer imaging, automotive navigation, robotics, surveillance, atmospheric science, and material science. Finally, projects with a single, easy-to-appreciate, and high-impact goal have the potential to inspire the next generation of scientists, attract diverse set of students driven by humanitarian and social causes, and become a platform for inclusion and innovation.<br/><br/>The overarching goal of this project is to develop, test, and validate new computational imaging systems, to non-invasively image below the skin at tunable depths, in highly portable form-factors such as wearables or point-of-care devices. The main challenge is that light scatters as it travels through the human body, and in this process, the spatial information from different points within the body gets mixed up. A new concept, Computational Photo-Scatterography (CPS), is being applied in this project in order to computationally unravel the scattered photons in an imaging system, and allow creation of sharp images and accurate inferences. Recognizing that the brute-force complexity of unraveling scattered photons is prohibitively high, the project uses a computational co-design framework that leverages advances by team members from multiple domains: programmable illumination and optics, image sensors, machine learning, inverse graphics, and hybrid analog-digital computing.  The project will use machine learning (ML) instead of physics-based de-scattering to speed up the solution of the underlying inverse problem. A combination of physics-based inverse graphics algorithms, and ML algorithms combining deep learning and generative modeling will be used to estimate tissue scattering parameters -  motion due to blood flow induces time-variation in tissue parameters, which makes solving the inverse scattering problem more difficult. The project will use ML to create fast but approximate estimators, which will serve as accelerators for inverse scattering. The development of new sensors, able to capture the data necessary to reconstruct the structure of the tissue deep below the skin, constitutes the most important contribution of the project. These systems and algorithms will have the potential to break the current resolution limits of noninvasive bio-imaging by nearly two orders of magnitude, enabling cellular-level imaging at depths far beyond currently possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831795","SCC: Building Safe and Secure Communities through Real-Time Edge Video Analytics","CNS","S&CC: Smart & Connected Commun","10/01/2018","09/09/2018","Hamed Tabkhi","NC","University of North Carolina at Charlotte","Standard Grant","David Corman","09/30/2022","$1,897,466.00","Shannon Reid, Douglas Shoemaker, Arun Ravindran, Srinivas Pulugurtha","htabkhiv@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","033Y","042Z","$0.00","The emergence of intelligent technologies is enabling a new era of connection between community residents and the surrounding environments, both in the United States and around the world. With the new wave of growth in urban areas, ensuring public safety is an essential precursor toward ""smart"" cities and communities. This project proposes a novel ""intelligent"" policing technology as a transformative solution to efficiently enhance law enforcement, while minimizing unnecessary interactions and maintaining resident privacy. The proposed technology offers a network of smart cameras that do not require continuous monitoring, but instead are trained to generate alerts on the spot in real-time. Since the cameras identify behaviors and not identities, they can reduce biases, minimize false alarms, and protect personal privacy. The intelligent policing technology will be co-designed and co-created with the direct help of community residents, neighborhood leaders, and local business owners, as well as agencies including the City of Charlotte, and local law enforcement agencies in Charlotte-Mecklenburg and Gaston counties. <br/><br/>The proposed research makes fundamental advances in multiple areas from computer vision, computer architecture, and real-time edge computing, as well as criminology and community-technology interaction. It paves the path for bringing the recent advances in deep learning and data analytics to enhance the safety and security of communities without jeopardizing the privacy of residents. To this end, this project formulates social-technical advances to efficiently analyze and assist communities and governing agencies in making real-time, smart reactions. The project enables real-time vision processing near the cameras (edge nodes) and cooperative processing over the edge network. At the same time, the proposed research interprets, formalizes, and models public safety and security events to be machine detectable, reducing biases, and enabling broad-based community support and trust. By demonstrating the use of powerful emerging edge computing technologies, the project will highlight the applicability and adaptability of such technologies to tackle many community challenges and broader smart cities and cyber-physical systems (CPS) applications, including smart transportation and pedestrian safety. Additionally, the proposed community-based pilots will serve as exemplars to other communities across the nation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823547","Civil Rights Violations and the Democratic Rule of Law","SES","LSS-Law And Social Sciences","08/01/2018","07/16/2018","Emily Gade","WA","University of Washington","Standard Grant","Reggie Sheehan","07/31/2021","$248,692.00","Michael McCann, Noah Smith","emily.gade@emory.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","SBE","1372","9179","$0.00","In recent years, states among the world's oldest industrialized democracies have selectively surrendered their commitments to liberal democracy in order to safeguard against (real or perceived) national security threats. How do state actors discuss, arrive at, and justify such authorization of extralegal violence against targeted citizens? The United Kingdom wrestled with this balance between national security and democratic protections during the period of its ""Troubles"" with Northern Ireland. During this time, the U.K. government interned 1,874 un-convicted Irish Catholic citizens and subjected as many as fourteen to the ""five techniques"" of torture. This research leverages an original archival resource -- 8,430 recently-declassified pages from the British Prime Minister's security-related correspondence files (1969-74) -- to analyze the processes by which the United Kingdom internally justified its extralegal internment and torture policies. It identifies the government's internal decision-making processes, compares these internal motivations to the external justifications it publicly presented to support these violent policies, and seeks to identify themes and patterns which may apply to other cases of states? deliberations about using extralegal violence during times of perceived national insecurity. This analysis advances conflict studies and socio-legal scholarship by 1) providing a nuanced understanding of how robust democracies erode rule-of-law protections; 2) identifying the internal-external dimensions of democratic states? authorization of violence; and 3) contributing historical knowledge of the U.K. ""Troubles."" This project increases public-facing scholarship on the complexities of protecting both democracies' national security interests and their rule-of-law principles. Its focus on using detention and torture to quell terrorist threats are particularly relevant to contemporary political debates the world over.<br/><br/>This project combines deductive and inductive qualitative social science methods with computational science techniques to identify, analyze, and generalize state justifications for violence, as follows. First, it uses scientific process-tracing methods to identify a series of hypothesized internal motivations and justifications that lead state agents to authorize violence against Irish Catholic citizens. Second, it collects and selectively analyzes additional data on the U.K. Government's public statements. This allows for the identification of (in)consistencies between the government's internal and external justifications for violence. Third, it uses novel natural language processing (NLP) computational science techniques to: build models that detect justifications' text patterns, cross-validate qualitative conclusions, generalize justifications to other cases of state authorizations of violence, and evaluate the efficacy of NLP for this type of research. This research lays the groundwork for future text-as-data machine-learning analysis of government documents. It also applies and advances the frontiers of NLP by examining whether and how these techniques can be tailored to identify recent or unfolding threats to democratic principles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1729931","Collaborative Research: Computational Photo-Scatterography: Unraveling Scattered Photons for Bio-Imaging","IIS","Expeditions in Computing","03/01/2018","07/13/2020","Ramesh Raskar","MA","Massachusetts Institute of Technology","Continuing Grant","Jie Yang","02/28/2023","$444,000.00","","raskar@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7723","7723, 9251","$0.00","Much of the success of today's healthcare is due to rapid advances in our ability to collect and analyze high-resolution data about the human body. However, current methods to achieve cellular resolution are invasive (e.g., blood test or tissue biopsy), and non-invasive imaging modalities do not achieve cellular resolution. The principal goal of this Expeditions project is to develop computational imaging systems for non-invasive bio-imaging, deep beneath the skin, and at cellular-level resolutions. This project has the potential to fundamentally impact healthcare and medicine, by enabling live views of cross sections of human anatomy, simply by pointing a camera at any part of the body. This would put individual users at the center of their healthcare experience and make them true partners in their healthcare delivery. The health imaging devices that result from this project will act as an important pillar in the personalized medicine revolution. This research expedition also holds the potential to launch new healthcare paradigms for chronic disease management, pediatrics, low-resource healthcare, and disaster medical care. Beyond healthcare, making progress on the problem of cellular-scale deep-tissue imaging using light will push the frontiers of the fundamental problem of inverse scattering, which impacts numerous areas of science and engineering. The order of magnitude advances made in inverse scattering and imaging through scattering media will have significant cross-cutting applications in diverse areas such as basic science, consumer imaging, automotive navigation, robotics, surveillance, atmospheric science, and material science. Finally, projects with a single, easy-to-appreciate, and high-impact goal have the potential to inspire the next generation of scientists, attract diverse set of students driven by humanitarian and social causes, and become a platform for inclusion and innovation.<br/><br/>The overarching goal of this project is to develop, test, and validate new computational imaging systems, to non-invasively image below the skin at tunable depths, in highly portable form-factors such as wearables or point-of-care devices. The main challenge is that light scatters as it travels through the human body, and in this process, the spatial information from different points within the body gets mixed up. A new concept, Computational Photo-Scatterography (CPS), is being applied in this project in order to computationally unravel the scattered photons in an imaging system, and allow creation of sharp images and accurate inferences. Recognizing that the brute-force complexity of unraveling scattered photons is prohibitively high, the project uses a computational co-design framework that leverages advances by team members from multiple domains: programmable illumination and optics, image sensors, machine learning, inverse graphics, and hybrid analog-digital computing.  The project will use machine learning (ML) instead of physics-based de-scattering to speed up the solution of the underlying inverse problem. A combination of physics-based inverse graphics algorithms, and ML algorithms combining deep learning and generative modeling will be used to estimate tissue scattering parameters -  motion due to blood flow induces time-variation in tissue parameters, which makes solving the inverse scattering problem more difficult. The project will use ML to create fast but approximate estimators, which will serve as accelerators for inverse scattering. The development of new sensors, able to capture the data necessary to reconstruct the structure of the tissue deep below the skin, constitutes the most important contribution of the project. These systems and algorithms will have the potential to break the current resolution limits of noninvasive bio-imaging by nearly two orders of magnitude, enabling cellular-level imaging at depths far beyond currently possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757017","REU Site: EXERCISE - Explore Emerging Computing in Science and Engineering","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","07/22/2019","Enyue Lu","MD","Salisbury University","Standard Grant","Joseph Maurice Rojas","01/31/2021","$369,995.00","","ealu@salisbury.edu","1101 Camden Avenue","Salisbury","MD","218016837","4105436066","CSE","1139","9250","$0.00","The project is a renewal of the Research Experiences for Undergraduates (REU) EXERCISE (Explore Emerging Computing in Science and Engineering) site at Salisbury University (SU) for the next three years. EXERCISE is an interdisciplinary project that explores emerging paradigms in parallel computing with data and compute intensive applications in science and engineering. The goal of the project is to offer student participants, particularly from primarily undergraduate institutions (PUIs), a valuable research experience in parallel computing. The project will promote ""parallel thinking"", an important computational thinking skill guiding current generation students into the twenty-first century computing era. The site will prioritize recruiting under-represented students and females, and attract students from local historically black college and universities (HBCUs), PUIs, and community colleges on Maryland's Eastern Shore into computational science and engineering majors and the general Science, Technology, Engineering, and Mathematics (STEM) fields. The Principal Investigator, together with faculty mentors, will supervise a 10-week REU program that gives a diverse cohort of students a taste of computational thinking in the domain of parallel computing and also an understanding of the graduate school experience. The host institution SU will collaborate with the University of Maryland Eastern Shore, an HBCU, and the University of Maryland College Park for multi-disciplinary faculty expertise and diverse summer activities including field trips, social activities, high school outreach, and graduate school application information sessions.<br/><br/>Processing complex information and large data in conventional von Neumann computer architectures is becoming increasingly difficult. Computers are undertaking a fundamental turn toward concurrency architectures such as hyperthreading, multi-core, and many-core architectures. Emerging parallel and distributed computing paradigms adapted to these concurrent architectures have begun to demonstrate the power of solving problems with large datasets and high computational complexity in a wide range of applications. However, there are fundamental difficulties in program semantics related to process interleaving: a parallel program can yield inconsistent answers, or even crash, due to unpredictable interactions between simultaneous tasks. Secondly, communication, memory access, and I/O overhead may result in run-time delays. Finally, it is difficult to ensure that programs consume resources in a manner that simultaneously achieves efficiency and meets performance goals. The REU Site will focus on four aspects of parallel computing, namely: algorithms, software, architecture and applications to address these parallel computing challenges. Students will work with faculty mentors in completing cutting-edge research projects to tackle data and compute intensive applications that emphasize the above four aspects. By the end of program, students will acquire valuable skills, gain a broader and deeper understanding of research, and develop greater confidence in their abilities. In particular, they will be exposed to emerging paradigms in parallel computing such as Map-Reduce and Graphical Processing Unit computing, and will have opportunities to explore concurrent software and multiprocessor architectures, and design efficient parallel algorithms, and to tackle data and compute intensive problems in computer and social networks, image and natural language processing, pattern recognition and machine learning."
"1850725","SaTC: CORE: Medium: Collaborative: Understanding and Discovering Illicit Online Business Through Automatic Analysis of Online Text Traces","CNS","Secure &Trustworthy Cyberspace","07/20/2018","09/17/2019","Xiaojing Liao","IN","Indiana University","Continuing Grant","Sara Kiesler","08/31/2022","$347,018.00","","xliao@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8060","025Z, 065Z, 7434, 7924","$0.00","Unlawful online business often leaves behind human-readable text traces for interacting with its targets (e.g., defrauding victims, advertising illicit products to intended customers) or coordinating among the criminals involved. Such text content is valuable for detecting various types of cybercrimes and understanding how they happen, the perpetrator's strategies, capabilities and infrastructures and even the ecosystem of the underground business.  Automatic discovery and analysis of such text traces, however, are challenging, due to their deceptive content that can easily blend into legitimate communication, and the criminal's extensive use of secret languages to hide their communication, even on public platforms (such as social media and forums). The project aims at systematically studying how to automatically discover such text traces and intelligently utilize them to fight against online crime. The research outcomes will contribute to more effective and timely control of online criminal activities, and the team's collaboration with industry also enables the team to get feedback and facilitate the transformation of new techniques to practical use. <br/> <br/>This project focuses on both criminals' communication with their targets and the underground communications among miscreants. To discover and understand illicit online activities, the research looks for any semantic inconsistency between text content and its context (such as advertisements for selling illegal drugs on an .edu domain) and for inappropriate operations being triggered (such as a malware download). Inconsistencies are captured by the Natural Language Processing (NLP) techniques customized to various security settings. Further, based upon crime-related content discovered, the project will study various machine learning techniques that support automatic extraction and analysis of threat intelligence and criminal activities. The techniques are evaluated using data collected from various sources (public datasets, underground forums and others), and the findings they make are validated through a process that involves manual labeling, communication with affected parties, and collaborations with industry partners. This work will help create in-depth knowledge about underground ecosystems and lead to more effective control of illicit operations of these online businesses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801652","SaTC: CORE: Medium: Collaborative: Understanding and Discovering Illicit Online Business Through Automatic Analysis of Online Text Traces","CNS","Secure &Trustworthy Cyberspace","09/01/2018","09/17/2019","ChengXiang Zhai","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Sara Kiesler","08/31/2022","$225,960.00","","czhai@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8060","025Z, 065Z, 7434, 7924","$0.00","Unlawful online business often leaves behind human-readable text traces for interacting with its targets (e.g., defrauding victims, advertising illicit products to intended customers) or coordinating among the criminals involved. Such text content is valuable for detecting various types of cybercrimes and understanding how they happen, the perpetrator's strategies, capabilities and infrastructures and even the ecosystem of the underground business.  Automatic discovery and analysis of such text traces, however, are challenging, due to their deceptive content that can easily blend into legitimate communication, and the criminal's extensive use of secret languages to hide their communication, even on public platforms (such as social media and forums). The project aims at systematically studying how to automatically discover such text traces and intelligently utilize them to fight against online crime. The research outcomes will contribute to more effective and timely control of online criminal activities, and the team's collaboration with industry also enables the team to get feedback and facilitate the transformation of new techniques to practical use. <br/> <br/>This project focuses on both criminals' communication with their targets and the underground communications among miscreants. To discover and understand illicit online activities, the research looks for any semantic inconsistency between text content and its context (such as advertisements for selling illegal drugs on an .edu domain) and for inappropriate operations being triggered (such as a malware download). Inconsistencies are captured by the Natural Language Processing (NLP) techniques customized to various security settings. Further, based upon crime-related content discovered, the project will study various machine learning techniques that support automatic extraction and analysis of threat intelligence and criminal activities. The techniques are evaluated using data collected from various sources (public datasets, underground forums and others), and the findings they make are validated through a process that involves manual labeling, communication with affected parties, and collaborations with industry partners. This work will help create in-depth knowledge about underground ecosystems and lead to more effective control of illicit operations of these online businesses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838139","BIGDATA: F: Privacy in Unsupervised Learning","IIS","Big Data Science &Engineering","10/01/2018","09/11/2018","Raman Arora","MD","Johns Hopkins University","Standard Grant","Ralph Wachter","09/30/2021","$911,398.00","","arora@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8083","062Z, 8083","$0.00","Modern data sets are largely unlabeled. Unsupervised learning of useful representations to better understand the structure in data is a critical challenge in data science and machine learning; it finds application in computational and social science, including information retrieval, web mining, and recommendation systems. As we progress further into the age of Big data, and the amount of data to be processed grows faster than the growth in our computational resources, better and faster ways for performing unsupervised learning and data analysis on such big data sets become ever more necessary. Furthermore, with the advent of the internet of things, private data is collected rather ubiquitously and seamlessly through devices such as smartphones, cameras, microphones, radio-frequency identification (RFID) readers, and social networks, raising serious concerns about an individual's privacy. Therefore, in this project, we initiate a formal investigation into privacy-aware unsupervised learning for Big data applications.<br/><br/>Taking a stochastic optimization view of unsupervised learning, we capture more general learning problems than previously studied in the privacy literature. One such class of learning problems is non-convex problems, such as matrix learning, tensor factorization, deep learning, and many more. While most of these problems are NP-hard, in practice we find that we can efficiently find solutions to these problems. We conjecture that noisy stochastic gradient descent updates that have recently been shown to efficiently find local minima for a large class of non-convex problems also guarantees privacy implicitly. Finally, we consider extensions of the privacy model from that of a single curator to those to distributed learning, continual release model, streaming model, and a novel sliding window model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750789","CAREER: Towards an Intermittent Learning Framework for Smart and Efficient Cyber-Physical Autonomy","CNS","CPS-Cyber-Physical Systems","05/01/2018","04/10/2018","Kyriakos G Vamvoudakis","VA","Virginia Polytechnic Institute and State University","Continuing grant","Jonathan Sprinkle","10/31/2018","$89,442.00","","kyriakos@gatech.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7918","1045","$0.00","This project expands how reinforcement learning frameworks can be used for Cyber-Physical Systems (CPS) for autonomy. The research utilizes intermittent reinforcement, where a reward is not given every time the desired response is performed. This differs from traditional reinforcement learning mechanisms, in which a reward is given for each point during online training. What is novel in this framework is that it can demonstrate how reinforcement learning can be used when rare events, or noisy and adversarial data, can affect the training and performance of these algorithms. The work will be validated on collaborative road freight transport and collaborative robotics testbeds, through international partnerships with Sweden and the United Kingdom. The project includes activities that integrate high-school students into challenging problems in machine learning areas, motivated through drone racing competitions.<br/><br/>The goal of this research is to expand foundational knowledge through deepened ties between the learning, control, game theory, and CPS communities. The approach is to, (i) unify new perspectives of learning in engineering with respect to resiliency, bandwidth efficiency, robustness, and other aspects that cannot be achieved with the state-of-the-art approaches; (ii) develop intermittent deep learning methods for CPS that can mitigate sensor attacks and can handle cases of limited sensing capabilities; (iii) incorporate nonequilibrium game-theoretic learning in CPS with components whose decision-making, rationality, and information usage are fundamentally different; and (iv) investigate ways to transfer learning to new platforms. The project's education and outreach component includes internships that will lead to technology transfer, summer camps with a special focus on reaching out to underrepresented minorities and women, and collaboration with institutions in Sweden and the United Kingdom through student exchange programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801432","SaTC: CORE: Medium: Collaborative: Understanding and Discovering Illicit Online Business Through Automatic Analysis of Online Text Traces","CNS","Secure &Trustworthy Cyberspace","09/01/2018","07/22/2020","XiaoFeng Wang","IN","Indiana University","Continuing Grant","Sara Kiesler","08/31/2022","$357,115.00","","xw7@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8060","025Z, 065Z, 7434, 7924","$0.00","Unlawful online business often leaves behind human-readable text traces for interacting with its targets (e.g., defrauding victims, advertising illicit products to intended customers) or coordinating among the criminals involved. Such text content is valuable for detecting various types of cybercrimes and understanding how they happen, the perpetrator's strategies, capabilities and infrastructures and even the ecosystem of the underground business.  Automatic discovery and analysis of such text traces, however, are challenging, due to their deceptive content that can easily blend into legitimate communication, and the criminals' extensive use of secret languages to hide their communication, even on public platforms (such as social media and forums). The project aims at systematically studying how to automatically discover such text traces and intelligently utilize them to fight against online crime. The research outcomes will contribute to more effective and timely control of online criminal activities, and the team's collaboration with industry also enables the team to get feedback and facilitate the transformation of new techniques to practical use. <br/> <br/>This project focuses on both criminals' communication with their targets and the underground communications among miscreants. To discover and understand illicit online activities, the research looks for any semantic inconsistency between text content and its context (such as advertisements for selling illegal drugs on an .edu domain) and for inappropriate operations being triggered (such as a malware download). Inconsistencies are captured by the Natural Language Processing (NLP) techniques customized to various security settings. Further, based upon crime-related content discovered, the project will study various machine learning techniques that support automatic extraction and analysis of threat intelligence and criminal activities. The techniques are evaluated using data collected from various sources (public datasets, underground forums and others), and the findings they make are validated through a process that involves manual labeling, communication with affected parties, and collaborations with industry partners. This work will help create in-depth knowledge about underground ecosystems and lead to more effective control of illicit operations of these online businesses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801365","SaTC: CORE: Medium: Collaborative: Understanding and Discovering Illicit Online Business Through Automatic Analysis of Online Text Traces","CNS","Secure &Trustworthy Cyberspace","09/01/2018","07/19/2018","Xiaojing Liao","VA","College of William and Mary","Continuing grant","Sara Kiesler","09/30/2018","$134,661.00","","xliao@indiana.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","8060","025Z, 065Z, 7434, 7924","$0.00","Unlawful online business often leaves behind human-readable text traces for interacting with its targets (e.g., defrauding victims, advertising illicit products to intended customers) or coordinating among the criminals involved. Such text content is valuable for detecting various types of cybercrimes and understanding how they happen, the perpetrator's strategies, capabilities and infrastructures and even the ecosystem of the underground business.  Automatic discovery and analysis of such text traces, however, are challenging, due to their deceptive content that can easily blend into legitimate communication, and the criminal's extensive use of secret languages to hide their communication, even on public platforms (such as social media and forums). The project aims at systematically studying how to automatically discover such text traces and intelligently utilize them to fight against online crime. The research outcomes will contribute to more effective and timely control of online criminal activities, and the team's collaboration with industry also enables the team to get feedback and facilitate the transformation of new techniques to practical use. <br/> <br/>This project focuses on both criminals' communication with their targets and the underground communications among miscreants. To discover and understand illicit online activities, the research looks for any semantic inconsistency between text content and its context (such as advertisements for selling illegal drugs on an .edu domain) and for inappropriate operations being triggered (such as a malware download). Inconsistencies are captured by the Natural Language Processing (NLP) techniques customized to various security settings. Further, based upon crime-related content discovered, the project will study various machine learning techniques that support automatic extraction and analysis of threat intelligence and criminal activities. The techniques are evaluated using data collected from various sources (public datasets, underground forums and others), and the findings they make are validated through a process that involves manual labeling, communication with affected parties, and collaborations with industry partners. This work will help create in-depth knowledge about underground ecosystems and lead to more effective control of illicit operations of these online businesses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730202","Collaborative Research: Computational Photo-Scatterography: Unraveling Scattered Photons for Bio-Imaging","IIS","Special Projects - CCF, Expeditions in Computing","03/01/2018","07/23/2020","Alyosha Molnar","NY","Cornell University","Continuing Grant","Jie Yang","02/28/2023","$504,000.00","","am699@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","2878, 7723","7723, 9251","$0.00","Much of the success of today's healthcare is due to rapid advances in our ability to collect and analyze high-resolution data about the human body. However, current methods to achieve cellular resolution are invasive (e.g., blood test or tissue biopsy), and non-invasive imaging modalities do not achieve cellular resolution. The principal goal of this Expeditions project is to develop computational imaging systems for non-invasive bio-imaging, deep beneath the skin, and at cellular-level resolutions. This project has the potential to fundamentally impact healthcare and medicine, by enabling live views of cross sections of human anatomy, simply by pointing a camera at any part of the body. This would put individual users at the center of their healthcare experience and make them true partners in their healthcare delivery. The health imaging devices that result from this project will act as an important pillar in the personalized medicine revolution. This research expedition also holds the potential to launch new healthcare paradigms for chronic disease management, pediatrics, low-resource healthcare, and disaster medical care. Beyond healthcare, making progress on the problem of cellular-scale deep-tissue imaging using light will push the frontiers of the fundamental problem of inverse scattering, which impacts numerous areas of science and engineering. The order of magnitude advances made in inverse scattering and imaging through scattering media will have significant cross-cutting applications in diverse areas such as basic science, consumer imaging, automotive navigation, robotics, surveillance, atmospheric science, and material science. Finally, projects with a single, easy-to-appreciate, and high-impact goal have the potential to inspire the next generation of scientists, attract diverse set of students driven by humanitarian and social causes, and become a platform for inclusion and innovation.<br/><br/>The overarching goal of this project is to develop, test, and validate new computational imaging systems, to non-invasively image below the skin at tunable depths, in highly portable form-factors such as wearables or point-of-care devices. The main challenge is that light scatters as it travels through the human body, and in this process, the spatial information from different points within the body gets mixed up. A new concept, Computational Photo-Scatterography (CPS), is being applied in this project in order to computationally unravel the scattered photons in an imaging system, and allow creation of sharp images and accurate inferences. Recognizing that the brute-force complexity of unraveling scattered photons is prohibitively high, the project uses a computational co-design framework that leverages advances by team members from multiple domains: programmable illumination and optics, image sensors, machine learning, inverse graphics, and hybrid analog-digital computing.  The project will use machine learning (ML) instead of physics-based de-scattering to speed up the solution of the underlying inverse problem. A combination of physics-based inverse graphics algorithms, and ML algorithms combining deep learning and generative modeling will be used to estimate tissue scattering parameters -  motion due to blood flow induces time-variation in tissue parameters, which makes solving the inverse scattering problem more difficult. The project will use ML to create fast but approximate estimators, which will serve as accelerators for inverse scattering. The development of new sensors, able to capture the data necessary to reconstruct the structure of the tissue deep below the skin, constitutes the most important contribution of the project. These systems and algorithms will have the potential to break the current resolution limits of noninvasive bio-imaging by nearly two orders of magnitude, enabling cellular-level imaging at depths far beyond currently possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730326","Collaborative Research: Computational Photo-Scatterography: Unraveling Scattered Photons for Bio-Imaging","IIS","Special Projects - CCF, Robust Intelligence, Expeditions in Computing","03/01/2018","07/20/2020","Latanya Sweeney","MA","Harvard University","Continuing Grant","Jie Yang","02/28/2023","$452,000.00","","latanya@fas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","2878, 7495, 7723","7723, 9251","$0.00","Much of the success of today's healthcare is due to rapid advances in our ability to collect and analyze high-resolution data about the human body. However, current methods to achieve cellular resolution are invasive (e.g., blood test or tissue biopsy), and non-invasive imaging modalities do not achieve cellular resolution. The principal goal of this Expeditions project is to develop computational imaging systems for non-invasive bio-imaging, deep beneath the skin, and at cellular-level resolutions. This project has the potential to fundamentally impact healthcare and medicine, by enabling live views of cross sections of human anatomy, simply by pointing a camera at any part of the body. This would put individual users at the center of their healthcare experience and make them true partners in their healthcare delivery. The health imaging devices that result from this project will act as an important pillar in the personalized medicine revolution. This research expedition also holds the potential to launch new healthcare paradigms for chronic disease management, pediatrics, low-resource healthcare, and disaster medical care. Beyond healthcare, making progress on the problem of cellular-scale deep-tissue imaging using light will push the frontiers of the fundamental problem of inverse scattering, which impacts numerous areas of science and engineering. The order of magnitude advances made in inverse scattering and imaging through scattering media will have significant cross-cutting applications in diverse areas such as basic science, consumer imaging, automotive navigation, robotics, surveillance, atmospheric science, and material science. Finally, projects with a single, easy-to-appreciate, and high-impact goal have the potential to inspire the next generation of scientists, attract diverse set of students driven by humanitarian and social causes, and become a platform for inclusion and innovation.<br/><br/>The overarching goal of this project is to develop, test, and validate new computational imaging systems, to non-invasively image below the skin at tunable depths, in highly portable form-factors such as wearables or point-of-care devices. The main challenge is that light scatters as it travels through the human body, and in this process, the spatial information from different points within the body gets mixed up. A new concept, Computational Photo-Scatterography (CPS), is being applied in this project in order to computationally unravel the scattered photons in an imaging system, and allow creation of sharp images and accurate inferences. Recognizing that the brute-force complexity of unraveling scattered photons is prohibitively high, the project uses a computational co-design framework that leverages advances by team members from multiple domains: programmable illumination and optics, image sensors, machine learning, inverse graphics, and hybrid analog-digital computing.  The project will use machine learning (ML) instead of physics-based de-scattering to speed up the solution of the underlying inverse problem. A combination of physics-based inverse graphics algorithms, and ML algorithms combining deep learning and generative modeling will be used to estimate tissue scattering parameters -  motion due to blood flow induces time-variation in tissue parameters, which makes solving the inverse scattering problem more difficult. The project will use ML to create fast but approximate estimators, which will serve as accelerators for inverse scattering. The development of new sensors, able to capture the data necessary to reconstruct the structure of the tissue deep below the skin, constitutes the most important contribution of the project. These systems and algorithms will have the potential to break the current resolution limits of noninvasive bio-imaging by nearly two orders of magnitude, enabling cellular-level imaging at depths far beyond currently possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823766","I-Corps: Embedded Machine Vision for Accurate Gait Analyses and Body Movement Measurements","IIP","I-Corps","04/01/2018","03/07/2018","Yan Luo","MA","University of Massachusetts Lowell","Standard Grant","Andre Marshall","09/30/2020","$50,000.00","","Yan_Luo@uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to revolutionize conventional rehabilitation assistant and body movement assessment tools with low-cost machine vision based solutions by maximizing the capability of ubiquitous tablets and 2D cameras, which have significant cost and accessibility advantage over more expensive 3D cameras. A set of tablet/smartphone Apps can be developed base on the embedded machine vision technology, and the team plan to create related services using a subscription model. This innovation will benefit broad healthcare market, which includes rehab facilities, surgery/neurology outpatient clinics, physical therapy clinics, stroke centers, and individuals. Overall, the innovation will make embedded machine vision technology accessible to and serve many population groups in the society. <br/><br/>This I-Corps project is based on preliminary results that we obtained on a PC platform to identify and track human body joint points in close to real-time. The prototype leverages novel machine learning algorithms and high performance processors (graphics processing units). Our next phase of research is to generate depth map from a 2D camera, recognize and track body joint points, and score body movements, all on a tablet platform. The challenges include depth generation with 2D images and efficient models and algorithms to execute on a resource constraint embedded platform. The team will leverage embedded deep learning techniques to optimize the algorithms towards an embedded processor architecture on modern tablets so that the proof-of-concept can deliver satisfactory performance. The team also plan to work with a medical doctor and a rehabilitation facility advisor to review and refine the software features and validate the early prototype.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749050","CAREER: Multimodal Photodetectors","ECCS","EPMD-ElectrnPhoton&MagnDevices","03/01/2018","03/01/2018","Zongfu Yu","WI","University of Wisconsin-Madison","Standard Grant","Dominique Dagenais","02/28/2023","$500,000.00","","zyu54@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","1517","091E, 1045","$0.00","Machine intelligence has acquired unprecedented power with the recent progress of deep learning. When paired with sensory functions, autonomous machines with even rudimentary intelligence are expected to revolutionize the world's economy. Today, the vision that most machines have is based on traditional intensity pictures of a scene, just as humans use. This vision modality has many limitations: it is impaired by fog and rain, and it offers no spectral information other than combinations of three fundamental colors. These issues greatly limit the practical use of autonomous machines due to their stringent safety and reliability requirements. As a result, expensive optical instruments are being used to assist conventional vision in accomplishing special tasks. The proposed project has the potential to overcome the fundamental issues of traditional imaging technologies. It is based on a new type of light-sensing pixels that can measure multimodal information of light, such as incident angle, wavelength, and phase. They could offer unprecedented scene awareness for pervasive use in future machines.<br/>Light-sensitive pixels used in today's camera can only detect the intensity of light. The intensity information is sufficient for conventional applications such as photography, its limitations become apparent in advanced vision tasks. This project will develop a new class of photodetectors to measure multimodal information of light waves. They are compact and can form high density arrays as imaging chips. Although multimodal information can be measured through conventional optical components, such as lenses, prisms, and gratings, these components are expensive to integrate. They also degrade spatial resolution and decrease operational speed. This project uses novel nanostructures to exploit unique optical interactions. Multi-modal pixels will be designed using full wave simulation and fabricated with photo-lithography. The multimodal pixels are completely compatible with existing semiconductor fabrication facilities and could potentially be mass-produced at the cost of consumer electronics. The project will also develop new machine learning algorithms to exploit multimodal information to perform vision tasks far beyond those possible with today's intensity-only approach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828629","MRI: Acquisition of a high-performance GPU-based computer for advanced multiscale materials modeling","DMR","MPS DMR INSTRUMENTATION","10/01/2018","08/20/2018","Juan De Pablo","IL","University of Chicago","Standard Grant","Guebre Tessema","09/30/2021","$999,491.00","Aaron Dinner, Hakizumwami B. Runesha, Margaret Gardel, Imre Kondor","depablo@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","1750","054Z, 062Z, 8084","$0.00","The funds from the Division of Materials Research support the University of Chicago to acquire a modern, Graphics Processor Unit (GPU) based computer that will allow researchers from a wide variety of disciplines to pursue new materials research directions that are based on the use of machine learning strategies and that could not be investigated without access to the necessary computational resources. Four focus areas are envisaged, namely (1) new materials discovery, (2) development of new molecular models, (3) machine-learning enabled interpretation of experiments, and (4) development of new simulation algorithms that rely on concepts from machine learning. A series of workshops and other outreach activities will be carried out to engage the broader materials research community in advanced computational research on GPU-accelerated computers.<br/> <br/><br/>The Division of Materials Research supports the University of Chicago with the project ""Acquisition of a high-performance GPU-based computer for advanced multiscale materials modeling"" . The instrument is a high-performance GPU (Graphics Processing Unit) cluster tailored for fast and efficient simulations, including molecular dynamics (MD) simulations, hybrid particle-continuum simulations, mesoscale simulations and continuum simulations. This cluster will thrust forward a wide array of multidisciplinary research projects anchored in molecular engineering, physics, chemistry, and biology, and builds upon the investigators' involvement in developing the fastest, most powerful suite of simulation software for particle-based simulations on GPUs, including molecular dynamics simulations, evolutionary optimization algorithms, and powerful deep learning-assisted advanced sampling algorithms. That software is now distributed freely, and it is used throughout the world.  The acquisition of a large GPU cluster at The University of Chicago will provide a unique computational resource and drive new collaborative efforts in algorithm and software development at the interface between molecular engineering, physics, chemistry, biology, computer science and materials science. The system will operate as a facility to serve four interrelated objectives, namely (i) support the local research community and their collaborators to achieve significant scientific advances on an array of collaborative research projects touching on proteins, membranes, cytoskeleton, fluids, colloids, nanoparticles, polymers, and mechanical metamaterials; (ii) create a vibrant interdisciplinary intellectual research community of theoreticians and experimentalists around the facility, and promote the creative development of novel and more effective advanced sampling algorithms, centered on the creation of a unified set of software tools supporting state-of-the-art methodologies; (iii) organize workshops to train undergraduate, graduate, and postgraduate students from a wide array of disciplines to disseminate knowledge about the utilization of distributed computational environments; (iv) contribute to diversifying the scientific workforce by organizing training and educational programs for undergraduate students from institutions serving under-represented minorities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763481","AF: Medium: Collaborative Research: Beyond Sparsity: Refined Measures of Complexity for Linear Algebra","CCF","Comm & Information Foundations","03/15/2018","04/28/2020","Atri Rudra","NY","SUNY at Buffalo","Continuing Grant","Phillip Regalia","02/28/2022","$378,582.00","","atri@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7797","7924, 7926, 7935, 9251","$0.00","Modern data science applications exploit structure in real life data using machine learning (including deep learning) algorithms. At the core of most of these systems are algorithms for a branch of mathematics called linear algebra. In particular, a large portion of these algorithms utilize the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. This project studies new, more powerful linear algebraic structures and algorithms that exploit these new structures. Given the fundamental importance of these algorithms, ideas generated from this project are expected to be implemented in widely deployed machine learning systems. The outreach component of this project involves (1) a technical workshop for researchers from diverse areas and (2) outreach events for K-12 students.<br/><br/>A variety of problems in modern data science have been successfully characterized using a width. For example, one of the most common widths, the rank of a matrix, has a near-ubiquitous use across many applications. This project significantly expands the understanding of several recently proposed widths and extracts their full potential for positive practical outcomes. Furthermore, it contributes to the recently growing work on beyond worst-case analysis in linear algebra, machine learning and coding theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763315","AF: Medium: Collaborative Research: Beyond Sparsity: Refined Measures of Complexity for Linear Algebra","CCF","Comm & Information Foundations","03/15/2018","09/08/2019","Christopher Re","CA","Stanford University","Continuing Grant","Phillip Regalia","02/28/2022","$552,118.00","","chrismre@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7797","7924, 7926, 7935","$0.00","Modern data science applications exploit structure in real life data using machine learning (including deep learning) algorithms. At the core of most of these systems are algorithms for a branch of mathematics called linear algebra. In particular, a large portion of these algorithms utilize the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. This project studies new, more powerful linear algebraic structures and algorithms that exploit these new structures. Given the fundamental importance of these algorithms, ideas generated from this project are expected to be implemented in widely deployed machine learning systems. The outreach component of this project involves (1) a technical workshop for researchers from diverse areas and (2) outreach events for K-12 students.<br/><br/>A variety of problems in modern data science have been successfully characterized using a width. For example, one of the most common widths, the rank of a matrix, has a near-ubiquitous use across many applications. This project significantly expands the understanding of several recently proposed widths and extracts their full potential for positive practical outcomes. Furthermore, it contributes to the recently growing work on beyond worst-case analysis in linear algebra, machine learning and coding theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839457","International Conference on Mathematics of Data Science","DMS","COMPUTATIONAL MATHEMATICS","11/01/2018","10/29/2018","Yuesheng Xu","VA","Old Dominion University Research Foundation","Standard Grant","miao-jung ou","10/31/2019","$15,000.00","Hideaki Kaneko, Fang Hu, Katherine Smith","y1xu@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","MPS","1271","7556","$0.00","Data science is an emerging interdisciplinary field of science and technology. It aims at developing theory, methods and techniques for extraction of useful knowledge or insights from raw data in various structured or unstructured forms such as signal, radar, sounds, images, videos and texts to make smart decisions. Data science employs theories and methods drawn from many fields within the broad areas of mathematics, statistics, information science, and computer science. Mathematics plays an indispensable role in data science. To bring together active researchers in various fields related to data science and practitioners in industry to identify mathematical and statistical challenges in data science, the International Conference on Mathematics of Data Science is being held on the campus of Old Dominion University on November 3-4, 2018. The conference website is http://icmds2018.org. The conference invited speakers are internationally known researchers in the field of data science and will address critical mathematical issues of the field. The conference will promote research collaboration among different areas, cultivate research partnership between academia and industry and, in particular, encourage young talents to work in the field of data science. The funds will solely be used to support junior researchers and graduate students in related fields at US universities and research institutions to attend the conference. This supports the recruitment of young talent to the field of data science and preparation of the next generation researchers to meet the scientific challenges in the big data era. Special efforts are made to recruit graduate students from underrepresented groups such as African American students and female students for the conference participants.<br/><br/>The conference will cover mathematical topics crucial to data science. In the field of data science, mathematics has provided functional spaces to represent data sets, approximation approaches to characterize similarity and difference of data sets, optimization methods to extract information from raw data, and analytical, geometrical tools to describe insightful relationships among various concepts in data and their statistical analysis. Further development of data science demands that mathematics play a leading role. For example, it is not yet fully understood that why deep learning is very efficient for certain applications while less efficient in other scenarios. This requires mathematical understanding of the fundamental issues in deep learning. All these issues will be the focus of the conference. Specifically, its scope covers sparse representation of big data sets, functional spaces suitable for big data analysis, mathematical foundation of machine learning, signal image processing, statistical analysis for big data, convex or non-convex sparse optimization for data analysis, scalable algorithms for big data and applications of data science. Scientific and societal broader impacts of this project lie in the aspects that the conference will promote interaction of mathematics, statistics, computer science, engineering and industrial applications, which support the interdisciplinary field of data science, and it will provide a platform for young scholars to learn about and discuss challenging mathematical issues in the field.<br/><br/>Website: https://sites.wp.odu.edu/icmds2018/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1741317","BIGDATA: F: Collaborative Research: Taming Big Networks via Embedding","IIS","Big Data Science &Engineering","01/01/2018","08/30/2017","Jiawei Han","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sylvia Spengler","12/31/2021","$400,001.00","","hanj@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8083","7433, 8083","$0.00","In the Internet Age, information entities and objects are interconnected, thereby forming gigantic information networks. Recently, network embedding methods, that create low-dimensional feature representations that preserve the structure of data points in their original space, have been shown to be greatly beneficial for many data mining and machine learning problems over networks. Despite significant research progress, we are still lacking powerful network embedding techniques with theoretical guarantees to effectively deal with massive, heterogeneous, complex and dynamic networks. The PIs aim to develop a new generation of network embedding methods for analyzing massive networks. The research project has the potential to significantly transform graph mining and network analysis. The PIs also plan to develop open course materials and open source software tools that integrate information network analysis and machine learning. <br/><br/>This project consists of four synergistic research thrusts. First, it develops model-based network embedding to leverage the first-order and second-order proximity of networks. Second, it devises a family of inductive network embedding methods that are able to leverage both linkage information and side information. Third, it develops both local clustering and deep learning based network embedding methods to attack the complex structure of networks such as locality and non-linearity. Fourth, it develops online and stochastic optimization algorithms for different network embedding methods to tackle the fast growth and evolution of modern massive networks. The new methods developed in this project enjoy faster rates of convergence in optimization, lower computational complexities, and statistical learning guarantees. The targeted applications include but are not limited to semantic search and information retrieval in social/information network analysis, expert finding in bibliographical database, and recommendation systems."
"1855099","BIGDATA: F: Collaborative Research: Taming Big Networks via Embedding","IIS","Big Data Science &Engineering","07/01/2018","10/23/2018","Quanquan Gu","CA","University of California-Los Angeles","Standard Grant","Sylvia Spengler","12/31/2021","$499,879.00","","qgu@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","8083","7433, 8083","$0.00","In the Internet Age, information entities and objects are interconnected, thereby forming gigantic information networks. Recently, network embedding methods, that create low-dimensional feature representations that preserve the structure of data points in their original space, have been shown to be greatly beneficial for many data mining and machine learning problems over networks. Despite significant research progress, we are still lacking powerful network embedding techniques with theoretical guarantees to effectively deal with massive, heterogeneous, complex and dynamic networks. The PIs aim to develop a new generation of network embedding methods for analyzing massive networks. The research project has the potential to significantly transform graph mining and network analysis. The PIs also plan to develop open course materials and open source software tools that integrate information network analysis and machine learning. <br/><br/>This project consists of four synergistic research thrusts. First, it develops model-based network embedding to leverage the first-order and second-order proximity of networks. Second, it devises a family of inductive network embedding methods that are able to leverage both linkage information and side information. Third, it develops both local clustering and deep learning based network embedding methods to attack the complex structure of networks such as locality and non-linearity. Fourth, it develops online and stochastic optimization algorithms for different network embedding methods to tackle the fast growth and evolution of modern massive networks. The new methods developed in this project enjoy faster rates of convergence in optimization, lower computational complexities, and statistical learning guarantees. The targeted applications include but are not limited to semantic search and information retrieval in social/information network analysis, expert finding in bibliographical database, and recommendation systems."
"1759644","I-Corps Team: New Tool for Sleep Apnea Screening","IIP","I-Corps","01/15/2018","01/24/2018","Janet Roveda","AZ","University of Arizona","Standard Grant","Pamela McCauley","06/30/2019","$50,000.00","","meilingw@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to decrease health care costs for sleep apnea monitoring and diagnosis. It is estimated that a large proportion of potential sleed apnea patients are undiagnosed. Sleep apnea is a potentially remedial risk factor for hypertension, type II diabetes, stroke, coronary artery disease, and heart failure. Sleep apnea also causes learning disability among children. The proposed sleep apnea screening tool is accurate and easy to use. It can potentially be broadly employed in clinics and homes, and reduce the need for both overnight laboratory-based polysomnograms and home sleep studies.<br/><br/>This I-Corps project targets to provide an easy-to-use screening tool for sleep apnea that combines mobile technology, internet of things (IoT) technology, and machine learning technology into one system.  The core technology of this screening tool is the state-of-the-art machine learning algorithms with IoT sensors. The solution is expected to deliver a high accuracy and sensitivity sleep apnea screening methodology using deep learning methods. Preliminary results are consistent with six types of apnea-hypopnea index threshold."
"1758556","Collaborative Research: Using Computer Vision to Measure Neighborhood Variables Affecting Health","SES","Sociology, Methodology, Measuremt & Stats","05/15/2018","05/16/2018","Jackelyn Hwang","CA","Stanford University","Standard Grant","Joseph Whitmeyer","04/30/2020","$138,913.00","","jackelyn.hwang@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","SBE","1331, 1333","9179","$0.00","This project will develop an automated method, using advances in computer science, to observe and record systematically the physical conditions of neighborhood environments at a large-scale.  Neighborhood environments play a significant role in shaping the health of individuals and communities, consequently contributing to inequality in the U.S.  Past research suggests that the presence of physical disorder, poorly maintained properties, and vacant lots in neighborhoods can negatively affect physical and mental health, attract more crime and disorder, and lead to neighborhood disinvestment.  The measures resulting from this study will facilitate examination of these processes and provide a powerful resource for the scientific research community.  They will be more broadly beneficial as well by allowing policymakers, practitioners, and the public to track neighborhood progress and target improvements.<br/><br/>The project takes advantage of Google Street View imagery--the largest publicly available longitudinal dataset of visual appearance of street blocks--and will use Amazon's Mechanical Turk--a crowdsourcing platform--and existing field survey data to identify indicators of physical disorder and maintenance, such as trash and blighted buildings, on a sample of street segments across three distinct cities: Boston, Detroit, and Los Angeles. These data will be used to train an algorithm that draws on recent advances in machine learning and computer vision. Reliability and validity of the method for identifying characteristics and measures will be tested throughout each step of the process. The resulting longitudinal measures of the physical conditions of neighborhoods will be linked to longitudinal health surveys conducted in each of the three cities to analyze the relationship between physical neighborhood conditions and health. In addition, the new measures will be released as a publicly available database of longitudinal measures of physical neighborhood conditions across multiple cities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758751","Collaborative Research: Using Computer Vision to Measure Neighborhood Variables Affecting Health","SES","Sociology, Methodology, Measuremt & Stats","05/15/2018","05/16/2018","Nikhil Naik","MA","Harvard University","Standard Grant","Joseph Whitmeyer","04/30/2019","$21,091.00","","NAIK@mit.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","SBE","1331, 1333","","$0.00","This project will develop an automated method, using advances in computer science, to observe and record systematically the physical conditions of neighborhood environments at a large-scale.  Neighborhood environments play a significant role in shaping the health of individuals and communities, consequently contributing to inequality in the U.S.  Past research suggests that the presence of physical disorder, poorly maintained properties, and vacant lots in neighborhoods can negatively affect physical and mental health, attract more crime and disorder, and lead to neighborhood disinvestment.  The measures resulting from this study will facilitate examination of these processes and provide a powerful resource for the scientific research community.  They will be more broadly beneficial as well by allowing policymakers, practitioners, and the public to track neighborhood progress and target improvements.<br/><br/>The project takes advantage of Google Street View imagery?the largest publicly available longitudinal dataset of visual appearance of street blocks?and will use Amazon?s Mechanical Turk? a crowdsourcing platform?and existing field survey data to identify indicators of physical disorder and maintenance, such as trash and blighted buildings, on a sample of street segments across three distinct cities: Boston, Detroit, and Los Angeles. These data will be used to train an algorithm that draws on recent advances in machine learning and computer vision. Reliability and validity of the method for identifying characteristics and measures will be tested throughout each step of the process. The resulting longitudinal measures of the physical conditions of neighborhoods will be linked to longitudinal health surveys conducted in each of the three cities to analyze the relationship between physical neighborhood conditions and health. In addition, the new measures will be released as a publicly available database of longitudinal measures of physical neighborhood conditions across multiple cities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814991","Variational Problems and Partial Differential Equations on Discrete Random Structures: Analysis and Applications to Data Science","DMS","APPLIED MATHEMATICS","08/01/2018","07/28/2018","Dejan Slepcev","PA","Carnegie-Mellon University","Standard Grant","Pedro Embid","07/31/2021","$245,566.00","","slepcev@math.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1266","062Z","$0.00","The investigator studies variational and partial differential equation (PDE) approaches to problems of data science.  Modern technology enables us to obtain large amounts of data about virtually any aspect of the world we live in.  The goal of data science is to extract and interpret the information the data contain.  Achieving this leads to machine learning tasks such as regression, clustering, classification, dimensionality reduction, semi-supervised learning, and learning data representation (e.g.  deep learning).  These tasks are regularly cast as optimization problems where one minimizes an objective functional that models the desired properties of the object sought.  The objective functionals and the resulting minimization are often posed on the available data sample, which leads to discrete variational problems on graphs and related structures representing the data.  The goal of this project is to develop a mathematical framework to study variational and PDE-based problems on random data samples.  The investigator uses insights from the continuum-based variational problems and PDEs to improve existing approaches in the discrete setting and introduce new models and algorithms for pertinent problems of data science. Graduate students are engaged in the research of the project.<br/><br/>The investigator adapts tools of analysis to the discrete random setting in order to show the fundamental properties of the variational problems and PDEs on such structures.  He works on establishing and using the connection between problems on random discrete structures and the continuum problems that arise in the large-sample limit.  In particular, he investigates the behavior of Laplacian-based and p-Laplacian-based regularizations in semi-supervised learning; studies stable ways to detect the boundaries of the data sets and impose the desired boundary conditions; and develops accurate graph-based discretizations for the continuum problems that the data sets approximate in the limit.  The second part of the project is devoted to gradient flows on random discrete structures.  Here the investigator studies stability and asymptotic properties of such problems, as well as the properties of the nonlocal continuum problems that they represent. Graduate students are engaged in the research of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1741345","BIGDATA: IA: Collaborative Research: Domain Adaptation Approaches for Classifying Crisis Related Data on Social Media","IIS","Big Data Science &Engineering","01/01/2018","09/14/2017","Doina Caragea","KS","Kansas State University","Standard Grant","Sylvia Spengler","12/31/2021","$500,000.00","Daniel Andresen","dcaragea@ksu.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","8083","7433, 8083, 9102","$0.00","The project investigates the use of big-data analysis techniques for classifying crisis-related data in social media with respect to situational awareness categories, such as caution, advice, fatality, injury, and support, with the goal of helping emergency response teams identify useful information. A major challenge is the scale of the data, where millions of short messages are continuously posted during a disaster, and need to be analyzed. The use of current technologies based on automated machine learning is limited due to the lack of labeled data for an emergent target disaster, and the fact that every event is unique in terms of geography, culture, infrastructure, technology, and the people involved. To tackle the above challenges, domain adaptation techniques that make use of existing labeled data from prior disasters and unlabeled data from a current disaster are designed. The resulting models are continuously updated and improved based on feedback from crowdsourcing volunteers. The research will provide real, usable solutions to emergency response organizations and will enable these organizations to improve the speed, quality and efficiency of their response. <br/><br/>The research provides novel solutions based on domain adaptation and deep neural networks to tackle the unique challenges in applying machine learning for crisis-related data analysis, specifically the volume and velocity challenges of big crisis data. Domain adaptation approaches enable the transfer of information from prior source disasters to an emergenet target disaster. Deep learning approaches make it possible to employ large amounts of labeled source data and unlabeled target data, and to incrementally update the models as more labeled target data becomes available. Large-scale analysis across combinations of source and target crises will help identify patterns of transferable situational awareness knowledge. The resulting technical and social solutions will be blended together for use in data management and emergency response."
"1741353","BIGDATA: IA: Collaborative Research: Domain Adaptation Approaches for Classifying Crisis Related Data on Social Media","IIS","Big Data Science &Engineering","01/01/2018","09/14/2017","Cornelia Caragea","TX","University of North Texas","Standard Grant","Aidong Zhang","01/31/2018","$400,000.00","","cornelia@uic.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","8083","7433, 8083","$0.00","The project investigates the use of big-data analysis techniques for classifying crisis-related data in social media with respect to situational awareness categories, such as caution, advice, fatality, injury, and support, with the goal of helping emergency response teams identify useful information. A major challenge is the scale of the data, where millions of short messages are continuously posted during a disaster, and need to be analyzed. The use of current technologies based on automated machine learning is limited due to the lack of labeled data for an emergent target disaster, and the fact that every event is unique in terms of geography, culture, infrastructure, technology, and the people involved. To tackle the above challenges, domain adaptation techniques that make use of existing labeled data from prior disasters and unlabeled data from a current disaster are designed. The resulting models are continuously updated and improved based on feedback from crowdsourcing volunteers. The research will provide real, usable solutions to emergency response organizations and will enable these organizations to improve the speed, quality and efficiency of their response. <br/><br/>The research provides novel solutions based on domain adaptation and deep neural networks to tackle the unique challenges in applying machine learning for crisis-related data analysis, specifically the volume and velocity challenges of big crisis data. Domain adaptation approaches enable the transfer of information from prior source disasters to an emergenet target disaster. Deep learning approaches make it possible to employ large amounts of labeled source data and unlabeled target data, and to incrementally update the models as more labeled target data becomes available. Large-scale analysis across combinations of source and target crises will help identify patterns of transferable situational awareness knowledge. The resulting technical and social solutions will be blended together for use in data management and emergency response."
"1750162","CAREER: Automated Analysis and Design of Optimization Algorithms","CCF","Special Projects - CCF, Comm & Information Foundations","02/15/2018","09/08/2019","Laurent Lessard","WI","University of Wisconsin-Madison","Continuing Grant","Phillip Regalia","01/31/2023","$467,312.00","","laurent.lessard@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","2878, 7797","1045, 7935","$0.00","Iterative optimization algorithms lie at the heart of modern data-intensive applications such as machine learning, computer vision, and data science. Society has become increasingly reliant on such algorithms for commerce, transportation, healthcare, emergency response, and national security. Despite their critical role in society, algorithms are typically designed and tuned using insight from experts, extensive numerical simulations, and other heuristics. This research develops a more principled understanding and approach to algorithm design that automatically accounts for sensitivity to parameter choice, robustness to noise, and other sources of uncertainty. This approach enables algorithms to be engineered in a way that guarantees performance and safety, which is similar to how airplanes, skyscrapers, and computer hardware are built.<br/><br/>Iterative algorithms may be viewed as dynamical systems with feedback. In gradient-based descent methods, for example, gradients are evaluated at each step and used to compute subsequent iterates. By treating algorithms as control systems, this research leverages tools from robust control (specifically: integral quadratic constraints, graphical methods, and semidefinite representation) to analyze and ultimately synthesize a variety of algorithms under different assumptions in an efficient, scalable, and systematic manner. This research also involves collaborative efforts in the areas of graph structure learning of gene regulatory networks and interactive machine learning, which serve to test and validate new algorithm designs."
"1819131","A General and Efficient Framework for Computational Shape Analysis Through Geometric Distributions","DMS","COMPUTATIONAL MATHEMATICS","07/01/2018","06/06/2018","Nicolas Charon","MD","Johns Hopkins University","Standard Grant","Leland Jameson","10/31/2020","$215,032.00","","charon@cis.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1271","062Z, 9263","$0.00","The analysis of shapes and their variability has become an increasingly central problem in multiple areas of data science. In the field of computer vision, shape recognition and classification is often a crucial component of machine learning systems such as self-driving cars. In natural sciences, the recent development of computational anatomy, that is the automatic analysis of anatomical structures by numerical algorithms, provides a fruitful approach in understanding and diagnosing a wide range of pathologies and disorders. Along these different scientific questions, the amount and variety of available data has never ceased to grow. As a result, the concept of shape itself has considerably expanded and may refer to various types of geometric objects, which poses the important challenge of constructing and computing relevant similarity metrics between shapes across all these different modalities. The purpose of this research project is to develop an integrated mathematical model and associated numerical pipeline that allows for morphological analysis of geometric structures in a flexible and efficient way, and explore its possible applications to computational anatomy and computer vision. It will also include a substantial educational component with the training of a graduate student, support for presentations in conferences and workshops, and dissemination of an open-source code to the scientific community.<br/><br/>The primal challenge of statistical shape analysis is the rather non-standard and disparate mathematical spaces in which objects belong, whether the shapes in question are raw images, manually or automatically extracted landmarks, curves, surfaces, vector fields or multi-modal objects. While the seminal model proposed by Grenander introduced the idea of comparing any two shapes through the estimation of an optimal deformation (measured by a metric on a certain diffeomorphism group), this model's generality falls short in many real applications where a certain amount of residual dissimilarity is necessary to account for other sources of variability (like noise). This project intends to fill this current gap by introducing a flexible approach to quantify shape similarity which relies on a unified embedding of shape spaces as generalized distributions, following the principles of geometric measure theory. Beyond the past success of these representations for curve and surface registration problems, the objective will be to demonstrate on a mathematical and computational level how it extends to a much wider class of geometric data structures and allows for cross-modality analysis, while pushing the scope of applications to other problems like clustering, classification and sparse representations on shapes. Fast numerical methods for this new framework is also an important aspect of the project, with the objective of making implementations scalable to the current dimensionality of datasets e.g. in medical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1741342","BIGDATA: F: Collaborative Research: Taming Big Networks via Embedding","IIS","Big Data Science &Engineering","01/01/2018","08/30/2017","Quanquan Gu","VA","University of Virginia Main Campus","Standard Grant","Sylvia Spengler","11/30/2018","$500,000.00","","qgu@cs.ucla.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8083","7433, 8083","$0.00","In the Internet Age, information entities and objects are interconnected, thereby forming gigantic information networks. Recently, network embedding methods, that create low-dimensional feature representations that preserve the structure of data points in their original space, have been shown to be greatly beneficial for many data mining and machine learning problems over networks. Despite significant research progress, we are still lacking powerful network embedding techniques with theoretical guarantees to effectively deal with massive, heterogeneous, complex and dynamic networks. The PIs aim to develop a new generation of network embedding methods for analyzing massive networks. The research project has the potential to significantly transform graph mining and network analysis. The PIs also plan to develop open course materials and open source software tools that integrate information network analysis and machine learning. <br/><br/>This project consists of four synergistic research thrusts. First, it develops model-based network embedding to leverage the first-order and second-order proximity of networks. Second, it devises a family of inductive network embedding methods that are able to leverage both linkage information and side information. Third, it develops both local clustering and deep learning based network embedding methods to attack the complex structure of networks such as locality and non-linearity. Fourth, it develops online and stochastic optimization algorithms for different network embedding methods to tackle the fast growth and evolution of modern massive networks. The new methods developed in this project enjoy faster rates of convergence in optimization, lower computational complexities, and statistical learning guarantees. The targeted applications include but are not limited to semantic search and information retrieval in social/information network analysis, expert finding in bibliographical database, and recommendation systems."
"1818751","Analysis and Recovery of High-Dimensional Data with Low-Dimensional Structures","DMS","COMPUTATIONAL MATHEMATICS","06/15/2018","06/26/2020","Wenjing Liao","GA","Georgia Tech Research Corporation","Continuing Grant","Yuliya Gorb","05/31/2021","$215,385.00","","wliao60@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","1271","9263","$0.00","Nowadays massive, high dimensional data sets arise in many fields of contemporary science and introduce new challenges. In machine learning, the well-known curse of dimensionality implies that, in order to achieve a fixed accuracy in prediction, a large number of training data is required. In image and signal recovery, a large number of measurements are needed to recover a high-dimensional vector, unless further assumptions are made. Fortunately, many real-world data sets exhibit low-dimensional geometric structures due to rich local regularities, global symmetries, repetitive patterns, or redundant sampling. The PI will explore low-dimensional geometric structures in data sets for feature extraction, data prediction and signal recovery. Dimension reduction and function approximation given a set of training data are of central interest in machine learning and data science. When data are concentrated near a low-dimensional set or the function has low complexity, the PI will develop new and fast machine learning algorithms whose performance depends on the complexity of the data or the function, instead of the dimension of the data sets.  In image and signal recovery, an interesting problem is to recover a high-dimensional, sparse vector from a small number of structured measurements. This problem is challenging since sensing matrices arising from imaging and signal processing are often deterministic, structured and highly coherent (some columns are highly correlated), which does not allow one to apply standard theory and algorithms. The PI will utilize the structures of sensing matrices, develop efficient algorithms, and prove performance guarantees. The theory and fast algorithms developed in this project can be applied to a wide range of problems in data compression, image analysis, computer vision, and signal recovery.<br/> <br/> <br/>High dimensional data arise in many fields of contemporary science and introduce new challenges. Fortunately, many real-world data sets exhibit low-dimensional geometric structures. This project focuses on exploiting these low-dimensional geometric structures of the data sets, and developing novel methods for dimension reduction, function approximation, and signal recovery. The PI will work on two sets of problems. In the first one, a data set is modeled as point clouds in a D-dimensional space but concentrating near a d-dimensional manifold, where d is much smaller than D. She plans to exploit the geometric structures of the data sets to build low-dimensional representations of data and approximate functions on data. Function approximations in Euclidean spaces have been well studied; however, classical estimators converge to the true function extremely slowly in high dimensions. When data are concentrated near a d-dimensional manifold, or the function has low complexity, the PI aims at constructing estimators that converge to the true function at a faster rate depending on the intrinsic dimension d. The proposed approach is based on the PI's recent work on adaptive geometric approximations for intrinsically low-dimensional data, where a data-driven, fast and robust scheme was developed to construct low-dimensional geometric approximations of data. The second set of problems arise from imaging and signal processing where the goal is to recover a high-dimensional, sparse vector from its noisy low-frequency Fourier coefficients. It is related with super-resolution in imaging, as the missing high-frequency Fourier coefficients correspond to the high-resolution components of the vector. Many existing methods fail since some columns in the sensing matrix are highly correlated. The PI will utilize the structure of the sensing matrix, develop efficient algorithms and prove performance guarantees. A mathematical theory will be developed to explain the fundamental difficulty of super-resolution, as well as the resolution limit of superior subspace methods, such as MUSIC, ESPRIT, and the matrix pencil method.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826258","Achieving Autonomy by Learning from Sensor-Assisted Control in a Wheelchair-Based Human-Robot Collaborative System","CMMI","M3X - Mind, Machine, and Motor","09/01/2018","12/06/2019","Rajiv Dubey","FL","University of South Florida","Standard Grant","Robert Scheidt","08/31/2021","$528,383.00","Kyle Reed, Redwan Alqasemi, Sudeep Sarkar","dubey@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","ENG","058Y","070E, 116E, 7632, 9178, 9231, 9251","$0.00","Individuals with sensorimotor impairment must often rely on others to help them perform common activities of daily living. The goal of this project is to improve independence and quality of life by creating an adaptive human-robot collaborative system (a wheelchair mounted robotic arm) that learns from example to assist its user perform instrumental activities in a way that requires minimal user guidance. The project has a novel intention recognition framework that learns user goals despite imprecision of telemanipulation cues provided by the user during object interactions. The project also implements and tests a novel form of shared control authority that adaptively allocates workload between the human and robot to optimally leverage the physical capabilities and cognitive resources of the user. By doing so, this project will improve the independence of individuals with sensorimotor impairment and increase the autonomy of a wheelchair mounted assistive robot, thereby advancing NSF's mission by promoting the progress of science and advancing the national health and welfare.  The project will involve an educational component that provides training to graduate students in conducting research. The project also develops a hands-on exhibit in collaboration with the Tampa Museum of Science and Industry, allowing visitors to explore the field of rehabilitation engineering.<br/><br/>This research investigates new control methodologies that promise improved autonomy in the control of a wheelchair mounted robotic arm operated by individuals with sensorimotor impairment.  The project addresses key steps in the dexterous telemanipulation of objects: object detection, classification, and affordance modeling; user intention estimation; user / robot workload distribution; and algorithm training through teleoperation. Methods include computer vision, machine learning, probabilistic graphical modeling, and human subject experimentation to develop and test the collaborative human / robot system during performance of activities such as opening/closing doors with pull and knob style handles, and fetching objects in an unstructured populated environment.  Effective application of the technology promises persons with physical disabilities opportunity to achieve a high level of independence, dignity, and quality of life.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1912887","BIGDATA: IA: Collaborative Research: Domain Adaptation Approaches for Classifying Crisis Related Data on Social Media","IIS","Big Data Science &Engineering","08/26/2018","12/21/2018","Cornelia Caragea","IL","University of Illinois at Chicago","Standard Grant","Sylvia Spengler","12/31/2021","$395,502.00","","cornelia@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","8083","7364, 7433, 8083","$0.00","The project investigates the use of big-data analysis techniques for classifying crisis-related data in social media with respect to situational awareness categories, such as caution, advice, fatality, injury, and support, with the goal of helping emergency response teams identify useful information. A major challenge is the scale of the data, where millions of short messages are continuously posted during a disaster, and need to be analyzed. The use of current technologies based on automated machine learning is limited due to the lack of labeled data for an emergent target disaster, and the fact that every event is unique in terms of geography, culture, infrastructure, technology, and the people involved. To tackle the above challenges, domain adaptation techniques that make use of existing labeled data from prior disasters and unlabeled data from a current disaster are designed. The resulting models are continuously updated and improved based on feedback from crowdsourcing volunteers. The research will provide real, usable solutions to emergency response organizations and will enable these organizations to improve the speed, quality and efficiency of their response. <br/><br/>The research provides novel solutions based on domain adaptation and deep neural networks to tackle the unique challenges in applying machine learning for crisis-related data analysis, specifically the volume and velocity challenges of big crisis data. Domain adaptation approaches enable the transfer of information from prior source disasters to an emergenet target disaster. Deep learning approaches make it possible to employ large amounts of labeled source data and unlabeled target data, and to incrementally update the models as more labeled target data becomes available. Large-scale analysis across combinations of source and target crises will help identify patterns of transferable situational awareness knowledge. The resulting technical and social solutions will be blended together for use in data management and emergency response."
"1741370","BIGDATA: IA: Collaborative Research: Domain Adaptation Approaches for Classifying Crisis Related Data on Social Media","IIS","Big Data Science &Engineering","01/01/2018","09/18/2018","Andrea Tapia","PA","Pennsylvania State Univ University Park","Standard Grant","Sylvia Spengler","12/31/2021","$400,000.00","Jessica Kropczynski","atapia@ist.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8083","7433, 8083","$0.00","The project investigates the use of big-data analysis techniques for classifying crisis-related data in social media with respect to situational awareness categories, such as caution, advice, fatality, injury, and support, with the goal of helping emergency response teams identify useful information. A major challenge is the scale of the data, where millions of short messages are continuously posted during a disaster, and need to be analyzed. The use of current technologies based on automated machine learning is limited due to the lack of labeled data for an emergent target disaster, and the fact that every event is unique in terms of geography, culture, infrastructure, technology, and the people involved. To tackle the above challenges, domain adaptation techniques that make use of existing labeled data from prior disasters and unlabeled data from a current disaster are designed. The resulting models are continuously updated and improved based on feedback from crowdsourcing volunteers. The research will provide real, usable solutions to emergency response organizations and will enable these organizations to improve the speed, quality and efficiency of their response. <br/><br/>The research provides novel solutions based on domain adaptation and deep neural networks to tackle the unique challenges in applying machine learning for crisis-related data analysis, specifically the volume and velocity challenges of big crisis data. Domain adaptation approaches enable the transfer of information from prior source disasters to an emergenet target disaster. Deep learning approaches make it possible to employ large amounts of labeled source data and unlabeled target data, and to incrementally update the models as more labeled target data becomes available. Large-scale analysis across combinations of source and target crises will help identify patterns of transferable situational awareness knowledge. The resulting technical and social solutions will be blended together for use in data management and emergency response."
"1815267","CDS&E: Machine Learning for Star Cluster Classification","AST","EXTRAGALACTIC ASTRON & COSMOLO","11/01/2018","10/18/2018","Daniela Calzetti","MA","University of Massachusetts Amherst","Standard Grant","Nigel Sharp","10/31/2020","$251,741.00","Subhransu Maji","calzetti@astro.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","MPS","1217","1206, 8084","$0.00","This is a pilot project to compare and test human visual inspection versus machine learning (ML) and computer vision (CV) methods for identifying young star clusters (YSC) in high resolution images of galaxies.  It is the first step towards exploring and optimizing the ML methods so as to build a tool capable of automatic search, classification, and shape measurement of YSC.  This study will provide a launch-pad for the full project.  It seems likely that ML tools will be the only viable way to handle the vast 'Big Data' databases becoming common in astronomy.  An integrated educational component includes summer research for undergraduate students, including a valuable introduction to 'Big Data' issues.<br/><br/>Initial tests will be performed on the two closest galaxies to our own Milky Way (M31 and M33), and then extended to M51 and NGC628, which are further away from us.  These are well-studied galaxies for which high-fidelity catalogs already exist, which are available for comparison and calibration.  The chosen test galaxies have very different cluster populations, and thus represent key testbeds to validate both the standard (human-based) approach and the future ML approach being developed.  ML/CV algorithms to be explored and tested on these images include very deep convolutional neural networks, which will be adapted to provide collective classifications of star clusters.  The human-based approach is currently the 'industry standard', and its validation will provide a more secure footing for future investigations of the physics of star formation in external galaxies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763569","CHS: Medium: Collaborative Research: Scalable Integration of Data-Driven and Model-Based Methods for Large Vocabulary Sign Recognition and Search","IIS","HCC-Human-Centered Computing","08/01/2018","07/21/2018","Matt Huenerfauth","NY","Rochester Institute of Tech","Standard Grant","Ephraim Glinert","07/31/2021","$209,896.00","","matt.huenerfauth@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7367","7367, 7924","$0.00","It is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would not know how to find it. ASL lacks a written form or intuitive ""alphabetical sorting"" based on such a writing system. Although some dictionaries make available alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find a match to the unfamiliar sign (if it is present at all in that dictionary). This research will create a framework that will enable the development of a user-friendly, video-based sign-lookup interface, for use with online ASL video dictionaries and resources, and for facilitation of ASL annotation.  Input will consist of either a webcam recording of a sign by the user, or user identification of the start and end frames of a sign from a digital video. To test the efficacy of the new tools in real-world applications, the team will partner with the leading producer of pedagogical materials for ASL instruction in high schools and colleges, which is developing the first multimedia ASL dictionary with video-based ASL definitions for signs. The lookup interface will be used experimentally to search the ASL dictionary in ASL classes at Boston University and RIT. Project outcomes will revolutionize how deaf children, students learning ASL, or families with deaf children search ASL dictionaries. They will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. And they will lay the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor. The new linguistically annotated video data and software tools will be shared publicly, for use by others in linguistic and computer science research, as well as in education. <br/><br/>Sign recognition from video is still an open and difficult problem because of the nonlinearities involved in recognizing 3D structures from 2D video, and the complex linguistic organization of sign languages. The linguistic parameters relevant to sign production and discrimination include hand configuration and orientation, location relative to the body or in signing space, movement trajectory, and in some cases, facial expressions/head movements. An additional complication is that signs belonging to different classes have distinct internal structures, and are thus subject to different linguistic constraints and require distinct recognition strategies; yet prior research has generally failed to address these distinctions. The challenges are compounded by inter- and intra- signer variations, and, in continuous signing, by co-articulation effects (i.e., influence from adjacent signs) with respect to several of the above parameters. Purely data-driven approaches are ill-suited to sign recognition given the limited quantities of available, consistently annotated data and the complexity of the linguistic structures involved, which are hard to infer. Prior research has, for this reason, generally focused on selected aspects of the problem, often restricting the work to a limited vocabulary, and therefore resulting in methods that are not scalable. More importantly, few if any methods involve 4D (spatio-temporal) modeling and attention to the linguistic properties of specific types of signs. A new approach to computer-based recognition of ASL from video is needed. In this research, the approach will be to build a new hybrid, scalable, computational framework for sign identification from a large vocabulary, which has never before been achieved. This research will strategically combine state-of-the-art computer vision, machine-learning methods, and linguistic modeling. It will leverage the team's existing publicly shared ASL corpora and Sign Bank - linguistically annotated and categorized video recordings produced by native signers - which will be augmented to meet the requirements of this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800322","Banach Spaces and Graphs: Geometric Interactions and Applications","DMS","ANALYSIS PROGRAM","07/01/2018","06/01/2020","Florent Baudier","TX","Texas A&M University","Continuing Grant","Marian Bocea","06/30/2021","$112,771.00","","florent@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1281","","$0.00","It can arguably be said that the world one lives in is geometric in nature. Numerous practical everyday-life issues, as well as fundamental scientific mysteries, can be expressed in geometric terms. For instance, networks are ubiquitous in our modern society. From the World Wide Web and its powerful search engines to social networks, from telecommunication networks to economic systems, networks represent a wide range of real world systems. They can naturally be seen as geometric objects by considering the number of edges of the shortest path connecting two nodes as a quantity measuring their proximity. The study of physical laws has led as well to the development of a refined mathematical framework where elaborate geometric structures are able to depict and model the interactions of elementary particles and the symmetries underlying quantum physics. The notion of a metric space is a central concept that is pivotal in mathematical models of optimization problems in networks, and in a vast range of application areas, including computer vision, computational biology, machine learning, statistics, and mathematical psychology, to name a few. This extremely useful abstract concept generalizes the classical notion of an Euclidean space, where the distance from point A to point B is computed as the length of an imaginary straight line connecting them. The heart of the matter usually boils down to understanding whether a given metric space, in particular a graph equipped with its shortest path distance, can be faithfully represented in a more structured space, typically a Banach space with some desirable properties. Our ability to do so usually has tremendous applications.<br/><br/>The problems investigated in this project are motivated by their potential applications in theoretical physics and theoretical computer science. Most of the problems considered find their origins either in practical issues (e.g. the design of efficient approximation algorithms), or in fundamental mathematical problems in topology or noncommutative geometry (e.g. the Novikov conjecture(s), the Baum-Connes conjecture(s)). Embedding problems that arise in connection to these problems have been considered independently by several groups of mathematicians (Banach space geometers, geometric group theorists, computer scientists...). An underlined aspect of this proposal is to consider these embeddings problems from a unified and global standpoint. Fundamental and long-standing open problems in quantitative metric geometry (e.g. Enflo's problem, a metric reformulation of uniform smoothability, the coarse embeddability of groups and expander graphs into super-reflexive Banach spaces...) will be tackled from a different angle with new and innovative techniques. In particular, the project will develop a certain asymptotic theory of Banach spaces and explore its connections to the geometry of infinite graphs. The approach here to solve the local problems above, is to study asymptotic counterparts of the local properties involved, in order to gain new insights and to devise new approaches. This approach is motivated by the fact that the asymptotic setting usually provides a finer picture, is on some occasion better understood, and requires completely different techniques. A general outline of the research methodology of this project is to utilize powerful tools from surrounding fields (graph theory, probability theory, Ramsey theory...), and cross-over techniques (e.g. techniques from theoretical computer science to solve geometric group theoretic problems).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761320","Kinetics-Driven Drug Discovery Using Persistent Homology, Rare-Event Molecular Dynamics and Experimental Data","DMS","NIGMS","07/15/2018","08/09/2019","Alex Dickson","MI","Michigan State University","Continuing Grant","Junping Wang","06/30/2022","$844,463.00","Guowei Wei, Kin Sing Lee","alexrd@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","8047","4075","$0.00","In this project a team of investigators from mathematics, molecular biology and medicinal chemistry will develop mathematical and computational tools to predict the efficacy of compounds that may help treat neuropathic pain in diabetics.  Pharmaceutical drugs are mostly made up of very small molecules, which take their effect by binding to target biomolecules in our bodies and perturbing their functions. A major challenge in designing new drugs - such as treatments for cancer, diabetes and Alzheimer's disease - is figuring out how to bind a particular target both accurately (with little off-target binding), and effectively (a high percentage of targets occupied by drug molecules). A key quantity to maximize the effectiveness of a drug is its ""residence time,"" the average amount of time the drug will remain in the binding site after each binding event. However, little is known about how the structure of a drug molecule determines its residence time, and this hinders our ability to incorporate residence time predictions in the drug design process.  This research will predict the residence times of compounds binding to a pharmaceutical target molecule that affects diabetic neuropathic pain, as well as test those predictions experimentally.  This study will potentially result in new treatments for diabetic neuropathic pain and also serve as a blueprint for future drug discovery efforts focused on residence time.  To facilitate adoption of this approach the team of investigators will disseminate their results via a dedicated website, online servers and participation in world-wide competitions for predicting drug binding properties. This project also involves the training of graduate students with unique interdisciplinary backgrounds, and will inform the development of graduate courses at the intersection of mathematics and biological sciences. <br/><br/>This project will develop a pipeline of mathematical and computational tools to enable kinetics-based drug discovery. The studies will be conducted on soluble epoxide hydrolase (sEH), an established pharmaceutical target for diabetic neuropathic pain for which only limited drugs have yet been approved. This project will use an integrated approach that encompasses topological modeling, machine learning, virtual screening, molecular simulation, as well as in vitro and in vivo assessment of compound efficacy. In PI Wei's laboratory, persistent homology will be used together with deep learning to abstract topological information from protein-ligand complexes and predict stable binding poses, binding affinities, and binding kinetics.  It is believed that the combination of topological analysis and deep learning will be transformative: it will bring a surge in similar approaches in 3D biomolecular data predictions in the near future, as well as applications to other fields, such as chemistry, and material science.  PI Dickson will use rare-event techniques in molecular modeling to simulate ligand release events, and identify the rate-limiting transition states of the ligand binding and release process. This project will also examine the robustness of ligand binding transition states for the first time, which is the key quantity to enable kinetics-based drug design.  Thirdly, PI Lee will continually assess the binding affinity and residence times of the predicted compounds. Selected compounds will be tested in vivo with a novel mouse model, to determine the limits of the benefits of long in vitro residence times. This collaborative project will achieve synergistic benefits by bringing together expertise from advanced mathematics, computational biophysics, and molecular pharmacology. The collaborative tools for sampling and prediction resulting from this work can then be applied to the discovery of novel long residence time compounds for other targets of interest.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829167","WORKSHOP: Doctoral Consortium at the IEEE International Conference on Automatic Face and Gesture Recognition (FG 2018)","IIS","HCC-Human-Centered Computing, Robust Intelligence","04/01/2018","03/21/2018","Yan Tong","SC","University of South Carolina at Columbia","Standard Grant","Ephraim Glinert","03/31/2019","$10,000.00","","tongy@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","7367, 7495","7367, 7495, 7556","$0.00","This is funding to support a Doctoral Consortium (workshop) of approximately 6 graduate students from U.S. educational institutions, along with unpaid senior members of the research community as mentors, to be held in conjunction with the thirteenth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2018), which will take place May 15-19, 2018, in Xi'an, China.  The IEEE FG conference is the premier international forum for research in image- and video-based face, gesture, and body movement recognition.  Their broad scope includes new algorithms for computer vision, pattern recognition, and computer graphics, as well as machine learning techniques relevant to face, gesture and body motion for a variety of applications.  The conferences present research that advances the state of the art in these and related areas, leading to new capabilities in interactive systems, biometrics, surveillance, healthcare, and entertainment, and they play an important role in shaping related scientific, academic, and educational programs.  More information is available online at http://www.fg2018.org/.  The Doctoral Consortium will provide an opportunity for Ph.D. students whose dissertations are on topics related to automatic face and gesture recognition to present their proposed research, and receive constructive feedback from an invited committee of faculty and industry researchers, as well as from other students working in these areas.  The event will give students valuable exposure to outside perspectives of their work, and provide a comfortable forum in which to discuss and fine-tune their career objectives with members of the international research community, and identify areas that need further development. The workshop will also enable these young researchers to develop a network of contacts at a critical stage of their careers, and will foster a supportive community of scholars and spirit of collaborative research.  The organizers will make a particular effort to recruit and include students from underrepresented groups (women and underrepresented minorities) and students from smaller schools or schools with less established computer vision research.  They will also try to recruit a demographically diverse (in terms of region, type of employment and stage in career) group of mentors to advise these students.<br/><br/>The Doctoral Consortium will be a half-day event during the conference.  This year, there will be four distinct aspects to the event.  First, each participant will be paired up with an invited faculty or industrial researcher who works in the related area and will act as their mentor both in the Doctoral Consortium and throughout the FG conference.  Second, there will be a career panel during a working lunch where participants will have an opportunity to discuss their research and career objectives with other participants and mentors in an informal setting.  Third, there will be a dedicated oral session for the students to present their research to the invited committee.  Fourth, there will be a poster session for the students to present their work to all conference attendees.  These four activities, taken together, will afford an excellent and structured way for students to communicate with other students as well as with established researchers of their related research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763486","CHS: Medium: Collaborative Research: Scalable Integration of Data-Driven and Model-Based Methods for Large Vocabulary Sign Recognition and Search","IIS","HCC-Human-Centered Computing","08/01/2018","07/21/2018","Carol Neidle","MA","Trustees of Boston University","Standard Grant","Ephraim Glinert","07/31/2021","$300,023.00","","carol@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7367","7367, 7924","$0.00","It is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would not know how to find it. ASL lacks a written form or intuitive ""alphabetical sorting"" based on such a writing system. Although some dictionaries make available alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find a match to the unfamiliar sign (if it is present at all in that dictionary). This research will create a framework that will enable the development of a user-friendly, video-based sign-lookup interface, for use with online ASL video dictionaries and resources, and for facilitation of ASL annotation.  Input will consist of either a webcam recording of a sign by the user, or user identification of the start and end frames of a sign from a digital video. To test the efficacy of the new tools in real-world applications, the team will partner with the leading producer of pedagogical materials for ASL instruction in high schools and colleges, which is developing the first multimedia ASL dictionary with video-based ASL definitions for signs. The lookup interface will be used experimentally to search the ASL dictionary in ASL classes at Boston University and RIT. Project outcomes will revolutionize how deaf children, students learning ASL, or families with deaf children search ASL dictionaries. They will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. And they will lay the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor. The new linguistically annotated video data and software tools will be shared publicly, for use by others in linguistic and computer science research, as well as in education. <br/><br/>Sign recognition from video is still an open and difficult problem because of the nonlinearities involved in recognizing 3D structures from 2D video, and the complex linguistic organization of sign languages. The linguistic parameters relevant to sign production and discrimination include hand configuration and orientation, location relative to the body or in signing space, movement trajectory, and in some cases, facial expressions/head movements. An additional complication is that signs belonging to different classes have distinct internal structures, and are thus subject to different linguistic constraints and require distinct recognition strategies; yet prior research has generally failed to address these distinctions. The challenges are compounded by inter- and intra- signer variations, and, in continuous signing, by co-articulation effects (i.e., influence from adjacent signs) with respect to several of the above parameters. Purely data-driven approaches are ill-suited to sign recognition given the limited quantities of available, consistently annotated data and the complexity of the linguistic structures involved, which are hard to infer. Prior research has, for this reason, generally focused on selected aspects of the problem, often restricting the work to a limited vocabulary, and therefore resulting in methods that are not scalable. More importantly, few if any methods involve 4D (spatio-temporal) modeling and attention to the linguistic properties of specific types of signs. A new approach to computer-based recognition of ASL from video is needed. In this research, the approach will be to build a new hybrid, scalable, computational framework for sign identification from a large vocabulary, which has never before been achieved. This research will strategically combine state-of-the-art computer vision, machine-learning methods, and linguistic modeling. It will leverage the team's existing publicly shared ASL corpora and Sign Bank - linguistically annotated and categorized video recordings produced by native signers - which will be augmented to meet the requirements of this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841188","EAGER: Closed-loop Silicon-biomolecular Systems with Integrated Synthesis-fluidics-nanopore Interfaces","CCF","EPMD-ElectrnPhoton&MagnDevices, Software & Hardware Foundation, Computational Biology","10/01/2018","07/27/2018","Luis Ceze","WA","University of Washington","Standard Grant","Mitra Basu","09/30/2020","$199,906.00","Jeffrey Nivala","luisceze@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1517, 7798, 7931","7916, 7945, 7946","$0.00","Given the slowing down of cost-performance gains from Moore's law and the scalability limits of digital storage technology, biomolecules are attractive alternatives for information storage and processing -- DNA data storage in particular is especially interesting due to its density, durability, and path to feasibility. At the same time, electronics will likely continue to be an integral part of computing systems, due to its high performance and the ability to be engineered. Hence, it is natural to consider hybrid biomolecular-electronic systems. Towards this end, this project is focused on building a fully integrated, closed-loop system that includes DNA synthesis, molecular sensors (nanopore), and fluidics for novel applications in biomolecular information processing. If successful, the scientific community will be provided with new and easy-to-use methods to accelerate molecular data storage-processing-computing and synthetic biology experimentation. This will make molecular computing and synthetic biology applications more accessible to the broader community. Integrating in-vivo and in-vitro biomolecular components with silicon systems can lead to innovations in a range of areas from health diagnostics and therapies, new materials, food, to information technology.<br/><br/>The investigators will engineer a toolbox of new molecular parts that can be used to store and transmit information in the form of nanopore-addressable molecular barcoding of synthetic DNA and proteins. A digital fluidics system will employ computer vision techniques for reliable control of droplet movements. The investigators will develop machine learning techniques to analyze raw nanopore sensor data for low-cost and high-throughput identification of molecular outputs. Demonstration of these components within the integrated system to be developed as part of this project will show proof-of-principle applications that advance molecular information processing capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763523","CHS: Medium: Collaborative Research: Scalable Integration of Data-Driven and Model-Based Methods for Large Vocabulary Sign Recognition and Search","IIS","HCC-Human-Centered Computing","08/01/2018","07/21/2018","Dimitris Metaxas","NJ","Rutgers University New Brunswick","Standard Grant","Ephraim Glinert","07/31/2021","$689,999.00","","dnm@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7367","7367, 7924","$0.00","It is surprisingly difficult to look up an unfamiliar sign in American Sign Language (ASL). Most ASL dictionaries list signs in alphabetical order based on approximate English translations, so a user who does not understand a sign or know its English translation would not know how to find it. ASL lacks a written form or intuitive ""alphabetical sorting"" based on such a writing system. Although some dictionaries make available alternative ways to search for a sign, based on explicit specification of various properties, a user must often still look through hundreds of pictures of signs to find a match to the unfamiliar sign (if it is present at all in that dictionary). This research will create a framework that will enable the development of a user-friendly, video-based sign-lookup interface, for use with online ASL video dictionaries and resources, and for facilitation of ASL annotation.  Input will consist of either a webcam recording of a sign by the user, or user identification of the start and end frames of a sign from a digital video. To test the efficacy of the new tools in real-world applications, the team will partner with the leading producer of pedagogical materials for ASL instruction in high schools and colleges, which is developing the first multimedia ASL dictionary with video-based ASL definitions for signs. The lookup interface will be used experimentally to search the ASL dictionary in ASL classes at Boston University and RIT. Project outcomes will revolutionize how deaf children, students learning ASL, or families with deaf children search ASL dictionaries. They will accelerate research on ASL linguistics and technology, by increasing efficiency, accuracy, and consistency of annotations of ASL videos through video-based sign lookup. And they will lay the groundwork for future technologies to benefit deaf users, such as search by video example through ASL video collections, or ASL-to-English translation, for which sign-recognition is a precursor. The new linguistically annotated video data and software tools will be shared publicly, for use by others in linguistic and computer science research, as well as in education. <br/><br/>Sign recognition from video is still an open and difficult problem because of the nonlinearities involved in recognizing 3D structures from 2D video, and the complex linguistic organization of sign languages. The linguistic parameters relevant to sign production and discrimination include hand configuration and orientation, location relative to the body or in signing space, movement trajectory, and in some cases, facial expressions/head movements. An additional complication is that signs belonging to different classes have distinct internal structures, and are thus subject to different linguistic constraints and require distinct recognition strategies; yet prior research has generally failed to address these distinctions. The challenges are compounded by inter- and intra- signer variations, and, in continuous signing, by co-articulation effects (i.e., influence from adjacent signs) with respect to several of the above parameters. Purely data-driven approaches are ill-suited to sign recognition given the limited quantities of available, consistently annotated data and the complexity of the linguistic structures involved, which are hard to infer. Prior research has, for this reason, generally focused on selected aspects of the problem, often restricting the work to a limited vocabulary, and therefore resulting in methods that are not scalable. More importantly, few if any methods involve 4D (spatio-temporal) modeling and attention to the linguistic properties of specific types of signs. A new approach to computer-based recognition of ASL from video is needed. In this research, the approach will be to build a new hybrid, scalable, computational framework for sign identification from a large vocabulary, which has never before been achieved. This research will strategically combine state-of-the-art computer vision, machine-learning methods, and linguistic modeling. It will leverage the team's existing publicly shared ASL corpora and Sign Bank - linguistically annotated and categorized video recordings produced by native signers - which will be augmented to meet the requirements of this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827598","Collaborative research: An integrated model of phonetic analysis and lexical access based on individual acoustic cues to features","BCS","Linguistics, Perception, Action & Cognition","09/01/2018","06/23/2020","Stefanie Shattuck-Hufnagel","MA","Massachusetts Institute of Technology","Standard Grant","Betty Tuller","08/31/2021","$354,605.00","Jeung-Yoon Choi","sshuf@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","1311, 7252","1311, 7252, 9251","$0.00","One of the greatest mysteries in the cognitive and neural sciences is how humans achieve robust speech perception given extreme variation in the precise acoustics produced for any given speech sound or word. For example, people can produce different acoustics for the same vowel sound, while in other cases the acoustics for two different vowels may be nearly identical. The acoustic patterns also change depending on the rate at which the sounds are spoken.  Listeners may also perceive a sound that was not actually produced due to massive reductions in speech pronunciation (e.g., the ""t"" and ""y"" sounds in ""don't you"" are often reduced to ""doncha""). Most theories assume that listeners recognize words in continuous speech by extracting consonants and vowels in a strictly sequential order. However, previous research has failed to find evidence for invariant cues in the acoustic signal that would allow listeners to extract the important information. This project uses a new tool for the study of language processing, LEXI (for Linguistic-Event EXtraction and Interpretation), to test the hypothesis that individual acoustic cues for consonants and vowels can in fact be extracted from the signal and can be used to determine the speaker's intended words. When some acoustic cues for speech sounds are modified or missing, LEXI can detect the remaining cues and evaluate them as evidence for the intended sounds and words. This research has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns seen in speech disorders or accented speech. This project supports training of 1-2 doctoral students and 8-10 undergraduate students through hands-on experience in experimental and computational research. All data, including code for computational models, the LEXI system, and speech databases labeled for acoustic cues will be publicly available through the Open Science Framework; preprints of all publications will be publicly available at PsyArxiv and NSF-PAR.<br/><br/>This interdisciplinary project unites signal analysis, psycholinguistic experimentation, and computational modeling to (1) survey the ways that acoustic cues vary in different contexts, (2) experimentally test how listeners use these cues through distributional learning for speech, and (3) use computational modeling to evaluate competing theories of how listeners recognize spoken words. The work will identify cue patterns in the signal that listeners use to recognize massive reductions in pronunciation and will experimentally test how listeners keep track of this systematic variation. This knowledge will be used to model how listeners ""tune in"" to the different ways speakers produce speech sounds. By using cues detected by LEXI as input to competing models of word recognition, the work provides an opportunity to examine the fine-grained time course of human speech recognition with large sets of spoken words; this is an important innovation because most cognitive models of speech do not work with speech input directly. Theoretical benefits include a strong test of the cue-based model of word recognition and the development of tools to allow virtually any model of speech recognition to work on real speech input, with practical implications for optimizing automatic speech recognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814385","AF: Small: Collaborative Research: Matrix Signings and Algorithms for Expanders and Combinatorial Nullstellensatz","CCF","Algorithmic Foundations","09/01/2018","05/18/2018","Alexandra Kolla","CO","University of Colorado at Boulder","Standard Grant","A. Funda Ergun","08/31/2021","$250,000.00","","alexkolla@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7796","7923, 7926, 9102","$0.00","This project will investigate spectral properties of graph-related matrices and their signings, which have become fundamental tools in computer science. The spectra of such matrices have had a tremendous impact in numerous areas including machine learning, data mining, web search and ranking, game theory, scientific computing, and computer vision and have influenced several algorithmic innovations. The project will have significant technical as well as educational impacts. The inherent mathematical and algorithmic nature of the project together with the plethora of potential practical applications will bring together researchers from varied areas such as mathematics and network design. The investigators will organize a workshop on Spectral Graph Theory to bring together experts in these areas. The project will support graduate students who will receive mentoring and extensive training in the design and analysis of algorithms. The investigators will also direct special efforts towards fostering diversity through educational activities targeting under-represented groups in STEM disciplines.<br/><br/>In this project, the investigators will design efficient algorithms for constructing various combinatorial structures that are guaranteed to exist through suitable signings of matrices. The combinatorial structures to be studied include expander graphs and several other applications of the algebraic method. Notably, the algorithmic problem of efficiently constructing of expander graphs is at the core of spectral graph theory. This project will develop a comprehensive understanding of the inherent difficulties, as well as propose algorithms for efficiently constructing expander graphs via signed adjacency matrices. Combinatorial Nullstellensatz is a powerful algebraic tool often used to show the existence of certain combinatorial structures. However, the non-constructive nature of its proof has been a barrier towards finding these structures efficiently. Existential proofs based on the algebraic method have resisted progress on the constructive front (unlike those based on probabilistic method). In this project, the investigators will break ground along this direction by obtaining efficient constructive proofs for restricted applications of Combinatorial Nullstellensatz.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838061","BIGDATA: F: Multiaffine Constrained Optimization for High-Dimensional Big Data Models","IIS","Big Data Science &Engineering","10/01/2018","09/11/2018","Donald Goldfarb","NY","Columbia University","Standard Grant","Sylvia Spengler","09/30/2021","$699,952.00","John Wright","goldfarb@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8083","062Z, 8083","$0.00","This project addresses fundamental questions about properties of optimization models involving multiaffine functions and algorithms for solving them. Multiaffine functions are functions of variables, or blocks of variables, that are linear in them when all other variables, or blocks of variables, are held fixed. Optimization problems of this type arise in a wide variety of big data applications in science, engineering, medicine, statistics and social media, including machine learning, computer vision, medical and hyperspectral imaging, and tensor models to name just a few. Because these problems often involve massive amounts of data and huge numbers of variables, this project will attempt to develop efficient distributed and probabilistic approaches, enabling solutions to be obtained more rapidly than is currently possible. It is expected that the methodology that is developed will have a pervasive influence on practice in the interdisciplinary field of data science. The project will demonstrate this methodology on data sets from specific applications from a wide range of fields and disseminate the results through websites, code release and conference talks and tutorials, as well as through interactions with faculty and students from various applied disciplines in Columbia University's Data Science Institute, female students through the Society of Women Engineers at Columbia, and the hosting of high school students from under-represented minorities through Columbia University's Young Scholars Program.<br/><br/>While providing important tools for solving real world problems, the project is also expected to have a major impact on the theoretical underpinnings of the Alternating Direction Method of Multipliers (ADMM) and an understanding of the optimization landscape of multiaffine problems arising in data analysis. ADMM has become a major algorithmic approach for solving problems in both parallel and distributed computational settings, because of its ability to transform the computationally intensive process of solving a difficult problem into an iterative procedure that involves solving simpler problems that are coupled by a system of linear equations. The project will expand ADMMs applicability by enabling these problems to be coupled by multiaffine constraints. The project will combine this multiaffine ADMM (M-ADMM) approach with stochastic and/or distributed approaches that are provably efficient and scalable.  For stochastic M-ADMM methods, how to reduce variance and importance sampling will be studied. For distributed settings, how to incorporate both centralized and decentralized concensus constraints into an M-ADMM framework will be investigated, as will asynchronous variants. The project will empirically study how to distribute the the computational effort of M-ADMM, both according to blocks of data and groups of parameters in high dimensional models. Because multiaffine problems are highly nonconvex, the solutions obtained by M-ADMM are in general only guaranteed to be local optima. It is known however, that for certain  multiaffine optimization problems, every local minimum is a global minimum and every saddle point has a direction of strict negative curvature under reasonable assumptions. The project will attempt to expand these kinds of results to more general multiaffine constrained problems and study the ability of M-ADMM for avoiding stagnating near saddle points.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827591","Collaborative research: An integrated model of phonetic analysis and lexical analysis based on individual acoustic cues to features","BCS","Linguistics, Perception, Action & Cognition","09/01/2018","04/07/2020","Rachel Theodore","CT","University of Connecticut","Standard Grant","Betty Tuller","08/31/2021","$202,981.00","James Magnuson, Paul Allopenna","rachel.theodore@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","SBE","1311, 7252","1311, 7252","$0.00","One of the greatest mysteries in the cognitive and neural sciences is how humans achieve robust speech perception given extreme variation in the precise acoustics produced for any given speech sound or word. For example, people can produce different acoustics for the same vowel sound, while in other cases the acoustics for two different vowels may be nearly identical. The acoustic patterns also change depending on the rate at which the sounds are spoken.  Listeners may also perceive a sound that was not actually produced due to massive reductions in speech pronunciation (e.g., the ""t"" and ""y"" sounds in ""don't you"" are often reduced to ""doncha""). Most theories assume that listeners recognize words in continuous speech by extracting consonants and vowels in a strictly sequential order. However, previous research has failed to find evidence for invariant cues in the acoustic signal that would allow listeners to extract the important information. This project uses a new tool for the study of language processing, LEXI (for Linguistic-Event EXtraction and Interpretation), to test the hypothesis that individual acoustic cues for consonants and vowels can in fact be extracted from the signal and can be used to determine the speaker's intended words. When some acoustic cues for speech sounds are modified or missing, LEXI can detect the remaining cues and evaluate them as evidence for the intended sounds and words. This research has potentially broad societal benefits, including optimization of human-machine interactions to accommodate atypical speech patterns seen in speech disorders or accented speech. This project supports training of 1-2 doctoral students and 8-10 undergraduate students through hands-on experience in experimental and computational research. All data, including code for computational models, the LEXI system, and speech databases labeled for acoustic cues will be publicly available through the Open Science Framework; preprints of all publications will be publicly available at PsyArxiv and NSF-PAR.<br/><br/>This interdisciplinary project unites signal analysis, psycholinguistic experimentation, and computational modeling to (1) survey the ways that acoustic cues vary in different contexts, (2) experimentally test how listeners use these cues through distributional learning for speech, and (3) use computational modeling to evaluate competing theories of how listeners recognize spoken words. The work will identify cue patterns in the signal that listeners use to recognize massive reductions in pronunciation and will experimentally test how listeners keep track of this systematic variation. This knowledge will be used to model how listeners ""tune in"" to the different ways speakers produce speech sounds. By using cues detected by LEXI as input to competing models of word recognition, the work provides an opportunity to examine the fine-grained time course of human speech recognition with large sets of spoken words; this is an important innovation because most cognitive models of speech do not work with speech input directly. Theoretical benefits include a strong test of the cue-based model of word recognition and the development of tools to allow virtually any model of speech recognition to work on real speech input, with practical implications for optimizing automatic speech recognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1912051","CAREER: Fast algorithms via a spectral theory for graphs with a prescribed cut structure","CCF","Algorithmic Foundations","10/22/2018","12/20/2018","Ioannis Koutis","NJ","New Jersey Institute of Technology","Continuing Grant","A. Funda Ergun","06/30/2020","$46,055.00","","ikoutis@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7796","1045, 7933, 9150","$0.00","Critical applications involving very large data sets require algorithms that run fast and provide strong performance guarantees. Among the numerous examples are the analysis of medical scans and the acquisition -via imaging- of connectivity in neural systems, an important task in current computational neuroscience. These problems are very often approached by first modeling the data as networks -also called graphs- and then applying graph-specific algorithms to solve them. Among many possibilities, algorithms that rely on certain algebraic representations of graphs have become very appealing due to recent theoretical progress that renders them very time-efficient.  However, efficiency appears to come at the cost of an occasionally inferior quality in the generated solutions. Via the proposed extensions of the theory studying these algebraic representations, the project will design new algorithms with strong guarantees and wide applicability.<br/><br/>Spectral graph theory studies the connections between algebraic and combinatorial properties of graphs. It is well known that these connections can be far from tight. For example, two given graphs may have approximately the same cuts, but significantly different eigenvalues and eigenvectors. As a result, spectral algorithms for cut problems on graphs, albeit very fast, do not provide good approximation guarantees. This project will extend aspects of spectral graph theory to a spectral theory for cut structures, defined as sets of graphs with approximately prescribed cuts. The central question of the new theory is:  What kind of spectral properties can be realized by graphs within a given cut structure?<br/><br/>A goal of the project is to show that any cut structure contains graphs whose eigenvectors provide tight information about its cuts.  The project will also study algorithms for the efficient computation of these special graphs, by essentially modifying the spectrum of an input graph without significantly altering its cuts. Then, the combination of spectral modification and classical spectral algorithms will yield fast algorithms with enhanced approximation guarantees.  The project will draw from connections of spectral graph theory with graph decompositions discovered in the context of oblivious routing algorithms. In turn, it is expected that the project will have an impact on routing problems too. In later stages the project will study the theoretical limits of spectral modification. It will also examine the descriptive quality of the developed theory in the performance of algorithms and other phenomena on interesting classes of graphs, such as social or biological networks.<br/><br/>The project will freely disseminate prototype implementations of the new algorithms and will apply them to computer vision and machine learning problems in industry and academia. Applications will be pursued via selected interdisciplinary collaborations. The balance between theoretical and applied work will serve a broader educational effort at both the undergraduate and graduate level, which will also include the introduction of new courses. A significant part of the research will be carried out at the University of Puerto Rico, and so the project is expected to have a significant impact in the education of underrepresented minorities."
"1814613","AF: Small: Collaborative Research: Matrix Signings and Algorithms for Expanders and Combinatorial Nullstellensatz","CCF","Algorithmic Foundations","09/01/2018","05/18/2018","Karthekeyan Chandrasekaran","IL","University of Illinois at Urbana-Champaign","Standard Grant","A. Funda Ergun","08/31/2021","$249,986.00","","karthe@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7923, 7926","$0.00","This project will investigate spectral properties of graph-related matrices and their signings, which have become fundamental tools in computer science. The spectra of such matrices have had a tremendous impact in numerous areas including machine learning, data mining, web search and ranking, game theory, scientific computing, and computer vision and have influenced several algorithmic innovations. The project will have significant technical as well as educational impacts. The inherent mathematical and algorithmic nature of the project together with the plethora of potential practical applications will bring together researchers from varied areas such as mathematics and network design. The investigators will organize a workshop on Spectral Graph Theory to bring together experts in these areas. The project will support graduate students who will receive mentoring and extensive training in the design and analysis of algorithms. The investigators will also direct special efforts towards fostering diversity through educational activities targeting under-represented groups in STEM disciplines.<br/><br/>In this project, the investigators will design efficient algorithms for constructing various combinatorial structures that are guaranteed to exist through suitable signings of matrices. The combinatorial structures to be studied include expander graphs and several other applications of the algebraic method. Notably, the algorithmic problem of efficiently constructing of expander graphs is at the core of spectral graph theory. This project will develop a comprehensive understanding of the inherent difficulties, as well as propose algorithms for efficiently constructing expander graphs via signed adjacency matrices. Combinatorial Nullstellensatz is a powerful algebraic tool often used to show the existence of certain combinatorial structures. However, the non-constructive nature of its proof has been a barrier towards finding these structures efficiently. Existential proofs based on the algebraic method have resisted progress on the constructive front (unlike those based on probabilistic method). In this project, the investigators will break ground along this direction by obtaining efficient constructive proofs for restricted applications of Combinatorial Nullstellensatz.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750716","CAREER: Faster and Smaller Sketches for Bigger Data","CCF","Algorithmic Foundations","02/01/2018","03/12/2020","Huy Nguyen","MA","Northeastern University","Continuing Grant","A. Funda Ergun","01/31/2023","$306,207.00","","hu.nguyen@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7796","1045, 7926","$0.00","The advent of new sensing and tracking technologies and expansive use of social networks detailing every walk of life have generated enormous new datasets. The difficulty in dealing with new datasets arises from not only the sheer volume but also the speed required for the analysis and the complex and heterogeneous nature of the data. Underlying these challenges is the need for suitable representations of the data that facilitate efficient computation and are sufficiently compact for storage and communication. This project aims to address fundamental gaps in our understanding of these representations (so-called sketches) and develops both new data representations and new algorithms for massive datasets in a holistic fashion. The project builds on techniques from a wide variety of areas including mathematical analysis, information theory, coding theory, combinatorics, and optimization, and enriches the deep connections among them. Undergraduate and graduate students will be trained and equipped with technical tools to work in these areas. The PI and the students involved in the project will also distill new findings into general audience surveys and give talks at workshops in different technical areas for broadest possible dissemination of information.<br/><br/>This project aims to study sketching algorithms by focusing on three main thrusts:(a) Study time complexity of sketches in streaming algorithms in both upper and lower bounds.(b) Develop new forms of sketches for distributed environments. The project focuses on sketching for submodular functions, a popular model for machine learning, computer vision, economics, etc. Problems in these applications are modeled as submodular maximization subject to various types of constraints. (c) Study space complexity of linear sketches in sparse recovery with respect to different recovery guarantees."
"1801976","Advances in Moduli Spaces and Algebraic Stacks","DMS","ALGEBRA,NUMBER THEORY,AND COM","08/01/2018","07/29/2018","Jarod Alper","WA","University of Washington","Standard Grant","Michelle Manes","07/31/2021","$150,001.00","","jarod@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1264","","$0.00","The investigator will research various problems within the field of algebraic geometry and related fields.  Algebraic geometry is one the most ancient subjects in mathematics with its origins traced to the algebraic study of curves by Greek mathematicians more than 2400 years ago.  Algebraic geometry drastically transformed around 60 years ago with the seminal work of Grothendieck.  This transformation led to more rigorous and much broader foundations with immensely powerful techniques which were employed to resolve a number of long-lasting conjectures not only within algebraic geometry but also other mathematical fields such as number theory, topology and statistics.  Most recently, the last few decades have seen the reach of algebraic geometry expand even further with an astounding array of practical applications.  Algebraic geometry now provides the technical backbone to cryptosystems governing online transactions, to computer graphics used in movies, games and virtual reality, to the computational methods in the study of biological systems  and the effectiveness of drug treatments, and to many other fields including data science, machine learning and computer vision. <br/><br/>This project aims to study a wide collection of ideas encircling the concept of moduli spaces.  In algebraic geometry, there is a large disparity between geometric intuition and the technical tools needed to prove results, and this disparity is perhaps no greater than in the study of moduli spaces.  A fundamental yet highly technical tool used to study moduli spaces is the theory of algebraic stacks.  The investigator will further the development of the foundations of algebraic stacks by in particular developing an intrinsic method to construct projective moduli spaces parameterizing objects that may have positive dimensional automorphism groups. The investigator will pursue applications of these foundational results to study the birational geometry of moduli spaces such as the D-equivalence conjecture motivated by mirror symmetry.  Finally, the investigator will attempt to apply sophisticated algebro-geometric techniques to tackle questions in algebraic complexity theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763681","SHF: Medium: Embracing Architectural Heterogeneity through Hardware-Software Co-design","CCF","Software & Hardware Foundation","06/01/2018","09/09/2019","Chitaranjan Das","PA","Pennsylvania State Univ University Park","Continuing Grant","Yuanyuan Yang","05/31/2021","$1,000,000.00","Anand Sivasubramaniam, Mahmut Kandemir","das@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7798","7924, 7941","$0.00","The last decade has witnessed a proliferation of heterogeneity across diverse application domains spanning from high-end datacenters to low-cost embedded systems, because they are capable of better performance and energy efficiency compared to homogeneous multicore architectures. These systems typically include a subset of CPUs, GPUs, FPGAs and ASICs as compute engines and hence, present unique programming/resource management challenges. However, the lack of required compiler and runtime support, present a barrier to the widespread adoption of heterogeneous systems. Furthermore, the design of the underlying heterogeneous architecture in terms of number and placement of various compute engines, memory subsystems and interconnects for a given area/power budget to satiate various application demands, is not fully explored. Therefore, it is imperative to investigate the entire system stack in a cohesive manner spanning applications, system software and underlying hardware for providing the required support for efficient application executions.  Thus, the main goal of this research project is to enable dynamic mapping of an application to different computing engines for improving performance/power efficiency and system utilization. The outcomes of this project are poised to change the way the programmers and users perceive heterogeneity and interact with it. The research on heterogeneous computing will be integrated with the educational activities and student training at Penn State for nurturing the future workforce in science and engineering, with active participation of female graduate students and undergraduates (Honors) students. <br/> <br/>The project consists four tasks. Task-I aims at conducting a profile-based workload characterization for various application domains including deep learning, cloud computing and high-performance computing on diverse hardware platforms to understand their performance/power utility. This will be used to develop a machine-learning (ML) based model for initial assignment of tasks to different compute engines.  Task-II is aimed at exploring compiler/programming support to transform application code into suitable device-agnostic 'codelets', that serve as the granularity for seamless scheduling and execution across different hardware units. Task-III investigates runtime support to optimally schedule and seamlessly move the codelets across the hardware units for improving system performance. Finally, Task-IV explores design of heterogeneous platforms by analyzing issues such as degree of heterogeneity, placement and integration of various computing engines on a chip and across chips, the underlying communication support.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750994","CAREER: Ultrasound Brain-Computer Interface","IIS","HCC-Human-Centered Computing","05/01/2018","03/23/2020","Brett Byram","TN","Vanderbilt University","Continuing Grant","Ephraim Glinert","04/30/2023","$329,997.00","","brett.c.byram@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","7367","1045, 7367","$0.00","Brain-computer interfaces (BCIs) are an emerging research area that might profoundly benefit people with severe motor loss and have potential applications in other situations where physical and voice interfaces are impractical.  This project will develop techniques for using ultrasound imaging to measure and interpret brain activity and integrate ultrasound derived information with existing electroencephalography (EEG)-based techniques.  Doing this will provide faster and more precise measures of brain activity compared to existing techniques.  It will also allow for the integration of sub-cortical information into BCIs, information that EEG systems cannot provide.  To achieve this goal, portable ultrasound helmet prototypes will be developed and solutions to transcranial ultrasound image quality problems will be resolved, which will also prove useful for medical ultrasound more generally.  The helmet will advance the feasibility of mobile brain activity measurements.  Along with the development and prototyping of an ultrasound BCI device, the development of new algorithms to integrate ultrasound and EEG signals will lead to new techniques for both understanding brain activity and using it to interact with computers.  The work will also provide the basis for modernizing and creating several new courses around the intersection of machine learning and biomedical signal processing, as well as providing research opportunities for outreach programs that involve high school students from underrepresented groups in STEM.<br/><br/>The project will be organized around several main activities.  The first activity is to develop high-dynamic range ultrasound imaging methods for transcranial ultrasound.  This will include the integration of multiple ultrasound transducer arrays and development of a new beamforming strategy utilizing iterative multi-stage regularization supported by deep learning methods to remove high-amplitude noise while preserving small signal variations corresponding to blood flow.  The second activity is to use the imaging methods to support an adaptive demodulation technique enabling non-contrast functional ultrasound analyses without contrast agents at low transducer frequencies appropriate for human transcranial imaging.  The third activity is to develop a wearable helmet that captures both ultrasound and EEG signals together.  The ultrasound and EEG will be physically registered using a new probe that will serve as both an EEG and an ultrasonic point source.  The helmet data will be used to develop both ultrasound-only and integrated (EEG plus ultrasound) algorithms for detecting brain states that computer systems can use either as direct commands or as input to adapt their behavior to a user's current brain state.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828576","MRI: Development of Reconfigurable Environmental Intelligence Platform","CNS","Information Technology Researc","10/01/2018","09/11/2018","Claudio Silva","NY","New York University","Standard Grant","Rita Rodriguez","09/30/2021","$600,000.00","Kaan Ozbay, Semiha Ergan, Juan Bello","csilva@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","1640","1189","$0.00","This project, developing a Reconfigurable Environmental Intelligence Platform (REIP) aims to alleviate many complex aspects of remote sensing, including sensor node design, software stack implementation, privacy issues, bandwidth, and centralized compute limitations, bringing down start up times from years to weeks. REIP will be a plug-and-play remote sensing infrastructure with advanced edge processing capabilities for in situ- insight generation. Sensor networks have dynamically expanded our ability to monitor and study our world Sensor networks have already deployed specialized sensor networks for many applications, including monitoring pedestrian traffic and outdoor noise monitoring and the need for sensing networks keeps increasing as the use cases for sensor networks expands and becomes more complex. Sensors no longer simply record data, they process and transforms it into something useful before sending it to central servers. <br/><br/>At the core of REIP is a set of hardware modules that connect together to form a sensing solution. Each sensing module will come in a number of variants allowing the user to find the proper tradeoff between complexity/ ost and power/features. The REIP infrastructure will expand the use of audio-visual sensing architectures beyond the highly specialized research groups that are able to design, build, and purchase all the necessary components and make it available to a wider community as a research infrastructure. REIP will be tested on real-world applications, including observation, integrated sensing transportation networks, and indoor sensing for reducing waste in HVAC (Heating, Ventilating, and Air Conditioning) systems. Experts will be able to customize each application domain.<br/> <br/>Led by a team of researchers with expertise in sensor networks, machine learning, deep learning, visualization, data analysis, human computing interface, engineering, and occupational therapy, this work will contribute to a variety of projects and is bound to have significant broader impacts. REIP and this research will directly impact a diverse population of students and foster education in science, technology, engineering, and math (STEM). Mentoring opportunities will be provided for all the involved graduate and undergraduate students<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816311","RI: Small: Print and Probability - A Statistical Approach to Analysis of Clandestine Publication","IIS","Robust Intelligence","09/01/2018","07/31/2018","Taylor Berg-Kirkpatrick","PA","Carnegie-Mellon University","Standard Grant","Tatiana Korelsky","08/31/2019","$499,770.00","Max G'Sell, Christopher Warren","tberg@eng.ucsd.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923","$0.00","This Print and Probability project develops novel machine learning and computer vision techniques to infer thousands of book and pamphlet printers whose identities have eluded scholars for roughly 500 years.  Before the modern era, the book trade was often dangerous and secretive. For fear of persecution and punishment, printers between 1473-1800 declined to attach their names to about a quarter of all known books and pamphlets. However, now that over 130,000 books have been digitized by the Early English Books Online (EEBO) project, defects and variations in the printing tools of this era may hold the key to identifying these printers. Once an individual piece of metal type is damaged, it creates unique stamps. Since typesets belonged to specific printers, impressions of damaged type can thus serve as the fingerprints to identify the printers of tens of thousands of clandestine publications. The Print and Probability project automatically detects and tracks these unique pieces of damaged type in order to uncover new information about the history of books. The methods developed in this project could be generalizable to other important tasks and domains - for example, digital forensics and authorship attribution. In addition, the Print and Probability project will train students in a multidisciplinary way, engaging them in collaboration across multiple fields.<br/><br/>By developing new techniques for visual anomaly detection, the Print and Probability project detects damaged letterforms that create consistent aberrations. Based on these damaged type extractions, the project develops probabilistic models of both printer and damaged letter form identifications that allow direct inference of printers at scale. This framework also incorporates other sources of evidence into the identification model - most significantly, the spelling, punctuation, and spacing habits of individual press-house compositors, whose distinctive practices lend themselves to clustering and automatic attributions across all pages of text in the collection.  Integrating a new method for automatic compositor attribution, this project develops a statistical model for printer identification that leverages the same sources of evidence compiled manually by scholars of rare books, but at a scale and speed never before possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756313","Architecture and plasticity of auditory lexical representations in the human brain","BCS","Cognitive Neuroscience","08/01/2018","07/05/2018","Maximilian Riesenhuber","DC","Georgetown University","Standard Grant","Kurt Thoroughman","07/31/2021","$651,953.00","Josef Rauschecker, Xiong Jiang","mr287@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","SBE","1699","1699","$0.00","Considering the diversity and richness of languages spoken around the globe, speech perception is arguably one of the main achievements of the human brain. While there is now broad support among cognitive neuroscientists for the concept of a hierarchy of cortical areas subserving auditory and speech processing, there is substantial disagreement about the roles different brain areas play in speech recognition, how the speech system learns and represents spoken words, and how learning words through one sense (e.g., through reading) can allow the brain to recognize the same words through another sense (e.g., through hearing). With support from the National Science Foundation, we will use advanced functional magnetic resonance imaging (fMRI) to investigate these questions. Specifically, the project is designed to resolve a major ongoing controversy regarding how and where words are represented in the brain by, for the first time, applying fMRI techniques to this field that have been used successfully to answer related questions in the domain of written words. A second study will probe how the brain learns new spoken words and adds them to the auditory lexicon.  In the final set of studies, the project will break new ground in our understanding of how the auditory and visual systems are linked in reading, and how this interaction is enabled by cross-sensory learning. Understanding the neural bases of speech processing and cross-sensory learning of language are areas of great interest not only for basic science but also other areas ranging from education and language learning to engineering (by elucidating effective learning algorithms for deep multisensory hierarchies, e.g., for automatic speech recognition) as well as biomedical fields (by building a foundation to study a range of disorders, including dyslexia and language comprehension deficits). The research project will form an opportunity to train the next generation of scientists, at the graduate, undergraduate, and high school levels. <br/><br/>In more scientific detail, the overarching goal of the proposed project is to study the existence and plasticity of auditory lexica in the human brain, and to understand coupling of written and auditory speech representations that permit cross-modal transfer of lexical learning. The project has three Aims: Aim 1 addresses the current controversy regarding the existence and location of auditory word representations in the brain. Translating techniques we previously developed to identify a ""visual lexicon"" in the ""Visual Word Form Area"", we will use sensitive fMRI rapid adaptation (fMRI-RA) and other advanced fMRI techniques to test the hypothesis of a (receptive) auditory lexicon within an analogous ""Auditory Word Form Area"" in left anterior superior temporal cortex. At the same time, we will test whether another lexicon exists in motor-related speech areas of the auditory dorsal stream representing articulatory word forms that are automatically activated by speech perception via an ""inverse model"". Aim 2 is designed to probe the plasticity invoked by the addition of novel words to the auditory lexicon. Aim 3 studies the interaction of written language with the auditory system. Prior studies have shown that reading activates phonological representations in proficient readers, and that this phonological recoding is crucial for reading acquisition. We will test the novel hypothesis that written words cause widespread activation of the auditory system and that training on novel written words can drive word-selective rewiring in the auditory lexical system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827361","Temporal dynamics of phonetic perceptual organization","BCS","Perception, Action & Cognition","09/01/2018","07/06/2019","Robert Remez","NY","Barnard College","Standard Grant","Betty Tuller","08/31/2021","$545,797.00","","rremez@barnard.edu","3009 BROADWAY","New York","NY","100276598","2128542708","SBE","7252","7252, 9251","$0.00","Scientists and engineers have studied speech to understand why this form of communication is so effective. They have also sought to create speaking and listening devices that approach the accuracy and ease of everyday communication, with modest success. The research problem has been easy to define: English is composed of more than 100,000 words created from over 16,000 different syllables and syllables are composed from a small inventory of several dozen consonants and vowels. Automatic speech recognition would be remarkably easy if these linguistic properties - words, syllables, consonants, vowels -produced uniformity in the sounds that talkers actually make. In fact, each utterance is also physically unique, whether in its sound pattern or in the visible movements of the speaker's face.  Different vocal anatomy in men, women and children causes complex variation in sound production even when the linguistic message is the same. Moreover, aspects of a talker's productions may express the dialect and speaking style of their family and linguistic community. Human listeners readily attend to the acoustic hints of these individual and social markers while also listening for the message. This project will examine how these different sources of perceptual information for speech are organized, how they are integrated over time, and how they allow perceptual tuning to the speech of individual talkers. Ultimately, a more complete account of the perception of speech can lead to improvement in recognition technology and to the creation of assistive devices.<br/><br/><br/>Three experimental projects will be performed: 1) to estimate the temporal dynamics of auditory sensory integration; 2) to determine the dimensions of exposure-based perceptual tuning to the characteristics of individual talkers; and, 3) to describe and model the intrinsic differences in auditory and visual temporal sensitivity and persistence that affect audiovisual speech perception. In each instance, the perceptual sensitivity to linguistic properties, talker characteristics, and language general features of spoken language will be assayed using discriminating and robust measures of auditory and audiovisual resolution. The studies explore the versatility of perceptual faculties applied to speech and provide an opportunity to identify the principles underlying the remarkably robust perceptual abilities that support and sustain communication. The overall goal is a formal and functional characterization of the cognitive resources that insure the perceptual stability of spoken communication in natural environments, whether the source of speech is visible or not, whether the talker is familiar or not, and whether the quality of the sensory samples of speech is natural or not.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1902395","US-German Data Sharing Proposal: CRCNS Data Sharing: REvealing SPONtaneous Speech Processes in Electrocorticography (RESPONSE)","IIS","CRCNS-Computation Neuroscience, IntgStrat Undst Neurl&Cogn Sys","09/01/2018","01/30/2019","Dean Krusienski","VA","Virginia Commonwealth University","Standard Grant","Kurt Thoroughman","07/31/2021","$388,060.00","","djkrusienski@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","7327, 8624","7327, 8089, 8091","$0.00","The uniquely human capability to produce speech enables swift communication of abstract and substantive information. Currently, nearly two million people in the United States, and far more worldwide, suffer from significant speech production deficits as a result of severe neuromuscular impairments due to injury or disease. In extreme cases, individuals may be unable to speak at all. These individuals would greatly benefit from a device that could alleviate speech deficits and enable them to communicate more naturally and effectively. This project will explore aspects of decoding a user's intended speech directly from the electrical activity of the brain and converting it to synthesized speech that could be played through a loudspeaker in real-time to emulate natural speaking from thought. In particular, this project will uniquely focus on decoding continuous, spontaneous speech processes to achieve more natural and practical communication device for the severely disabled.<br/><br/>The complex dynamics of brain activity and the fundamental processing units of continuous speech production and perception are largely unknown, and such dynamics make it challenging to investigate these speech processes with traditional neuroimaging techniques. Electrocorticography (ECoG) measures electrical activity directly from the brain surface and covers an area large enough to provide insights about widespread networks for speech production and understanding, while simultaneously providing localized information for decoding nuanced aspects of the underlying speech processes. Thus, ECoG is instrumental and unparalleled for investigating the detailed spatiotemporal dynamics of speech. The research team's prior work has shown for the first time the detailed spatiotemporal progression of brain activity during prompted continuous speech, and that the team's Brain-to-text system can model phonemes and decode words. However, in pursuit of the ultimate objective of developing a natural speech neuroprosthetic for the severely disabled, research must move beyond studying prompted and isolated aspects of speech. This project will extend the research team's prior experiments to investigate the neural processes of spontaneous and imagined speech production. In conjunction with in-depth analysis of the recorded neural signals, the researchers will apply customized ECoG-based automatic speech recognition (ASR) techniques to facilitate the analysis of the large amount of phones occurring in continuous speech. Ultimately, the project aims to define fundamental units of continuous speech production and understanding, illustrate functional differences between these units, and demonstrate that representations of spontaneous speech can be synthesized directly from the neural recordings. A companion project is being funded by the Federal Ministry of Education and Research, Germany (BMBF)"
"1827744","RIDIR:  Collaborative Research: Enabling Access to and Analysis of Shared Daylong Child and Family Audio Data","SMA","CYBERINFRASTRUCTURE, Data Infrastructure","03/01/2018","03/09/2018","Anne Warlaumont","CA","University of California-Los Angeles","Standard Grant","John Yellen","02/28/2021","$360,683.00","","warlaumont@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","7231, 8294","7433, 8004, 9178, 9179","$0.00","A child's language development in the first few years of life predicts long-term cognitive development, academic achievement, and expected income as an adult. Early language development in turn depends on linguistic interactions with adults. Increasingly, researchers are using daylong audio recordings to study child language development and child-caregiver interactions. Compared to short language samples, daylong recordings capture the full range of experiences a child has over the course of a day. Daylong audio recordings are also being used in applied settings. For example, studies show that by the time they enter First Grade, children from higher socioeconomic backgrounds hear tens of millions more words than children from lower socioeconomic backgrounds, perpetuating social inequalities. Multiple large-scale intervention projects targeting low socioeconomic households, including the Thirty Million Words Initiative in Chicago and the Providence Talks program, are using daylong audio recordings to provide automated, personalized feedback to parents on when and how often their child hears adult words and experiences conversational turns. The features of daylong recordings that are advantageous for researchers and practitioners also pose unique challenges. For one, their long durations are ideal for studying the temporal dynamics of child-adult interaction, but taking advantage of the long durations requires the enlistment of automated speech recognition technology. Current automatic speech recognition systems have difficulties with child speech and are challenged by the noisy and varied acoustic environments represented in the recordings. Another challenge is that the recordings capture private moments that require long hours of human listening to remove. This makes it difficult for researchers to share the recordings publicly, so that the potential value of the recordings collected by individual research labs is not fully realized.<br/><br/>This project will create a new resource, called HomeBank, that will have three key components: (1) a public dataset containing daylong audio recordings that have had private information removed by human listeners, (2) a larger dataset containing about ten to one hundred times as many hours of recording that have not had private information removed and will be free but restricted to those who have demonstrated training in human research ethics, and (3) an open-source repository of computer programs to automatically analyze the daylong audio recordings. HomeBank will take advantage of an existing cyberinfrastructure for sharing linguistic data called TalkBank. The daylong audio recordings included in the datasets will represent both typically developing and clinical groups, a range of ages from newborn infants to school age children, and a range of language and socioeconomic backgrounds. We expect the primary users to be basic and applied child development researchers as well as engineers developing automatic speech recognition technologies. The free-to-access database and the open source computer programs will ultimately improve both the data on which early interventions are based and the tools available for providing parents with feedback on the linguistic input they provide their children.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820045","SBIR Phase I:  VideoPoints: A Companion for Classroom Learning","IIP","SBIR Phase I","06/15/2018","01/23/2020","Tayfun Tuna","TX","Videopoints LLC","Standard Grant","Peter Atherton","03/31/2020","$225,000.00","","taytun@gmail.com","3615 Graustark St","Houston","TX","770064225","8328787217","ENG","5371","5371, 8031, 8032","$0.00","This SBIR Phase I project will develop text, speech and image analysis technologies to transform a set of educational lecture videos into an interactive learning companion. A lecture video will be presented as a series of topically cohesive sections, each represented by a textual summary, a visual summary, and a video segment. Students will be able to query a series of lecture videos with the answers presented as a combination of text, images, speech with transcript, and video, along with links to additional relevant resources. These technological enhancements will drive the development of the lecture video management system with capabilities well beyond commercial state of the art. The students will gain the ability to instantly access any information in a semester long course, with little overhead for the instructor. The business plan focuses on a freemium model designed for wide adoption with no cost to instructors and a very low cost to students. The ultimate potential societal benefit is a significantly better learning experience and learning outcomes for higher education students.<br/><br/><br/>The key innovations in this project are summarization of video segments and a Questions & Answers (QA) system customized for a series of lecture videos. These are formidable challenges despite existing substantial related research in text mining and image analysis. The information content of a lecture video spans multiple modalities, specifically, screen text, images, and speech. Further, each modality has unique characteristics. Screen text is typically unstructured and includes Optical Character Recognition errors. Transcripts from classroom lecture videos contain informal classroom interactions and suffer from speech recognition errors. The images in a lecture video can represent a variety of concepts including illustrative examples, graphs and charts, and camera images. Innovative approaches to topic modeling will be developed to handle the unique nature of the input compared to most documents. The project will employ representation learning approaches to map from each modality to a common semantic space that will drive the matching between student questions and the content of lecture videos. Since the best answer to a student question may not be fully contained in a lecture video, external resources including textbooks, message boards, and actual quizzes and exams will also be analyzed to drive the QA module.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819287","SBIR Phase I:  Natural Language Voice Controlled Science Equipment","IIP","SBIR Phase I","06/15/2018","06/14/2018","Clifton Roozeboom","CA","Myriad Sensors","Standard Grant","Rajesh Mehta","04/30/2019","$225,000.00","","clifton.roozeboom@gmail.com","505 Cypress Point Dr.","Mountain View","CA","940434849","7196513764","ENG","5371","5371, 8031, 8032","$0.00","This SBIR Phase I project will develop Natural Language Voice-Controlled Lab Assistant technology that enables students to control and interact with hands-on science lab equipment using natural language dialog. The lab assistant technology will connect with science equipment hardware, a mobile or computer app, and cloud-based software for speech recognition and data analysis. A student can perform hands-on experiments, ask questions like ""How high did my rocket go?"" or ""What was the force of the cart collision?"", and receive audible responses based on their own experimental measurements. The lab assistant is designed for students that encompass (1) students who are blind or low vision, (2) students with disabilities affecting physical skills, (3) students that would benefit from multi-sensory learning methods as required in Individualized Education Plans (IEPs), and (4) generally students that would benefit from increased engagement in Science, Technology, Engineering, and Mathematics (STEM). The broader impacts of the lab assistant technology will be to promote teaching and learning through professional development of K-12 educators in STEM, and enable broad participation of under-represented groups of people in authentic science inquiry.<br/><br/><br/>The proposed Lab Assistant will be the first application of state-of-the-art voice recognition technology for educational science experiments. The Lab Assistant will integrate with sensor hardware and mobile apps to enable hands-on experiments in physics, earth science, chemistry, and engineering. The intellectual merits of the Lab Assistant are (1) the development of software that can respond to a wide spectrum of natural language questions and (2) the systems integration of many cutting-edge technologies (wireless sensors, mobile apps, voice recognition software, cloud-based data analysis algorithms) into a simple user interface for science education. The voice interactions will help students overcome disabilities with traditional touch and visual technology, work more independently due to the presence of auditory help, work more effectively in groups, and gain confidence in their STEM abilities. For teachers, the Lab Assistant will provide an ""expert in the room"" to help guide the hands-on activities that they already do, and provide technical support. The Lab Assistant will be especially useful to teachers without formal science training, that nevertheless need to lead hands-on STEM activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749342","Doctoral Dissertation Research: Articulatory Dynamics of Sibilant Convergence & Change","BCS","DDRI Linguistics","02/15/2018","02/15/2018","Alan Yu","IL","University of Chicago","Standard Grant","Tyler Kendall","07/31/2020","$13,299.00","Jacob Phillips","aclyu@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","SBE","8374","1311, 9179","$0.00","The goal of this dissertation research is to contribute to the scientific understanding of sound change, focusing on the way in which some sound changes are asymmetrically distributed. That is, given that the phonetic motivations for a sound change may be present in multiple phonological environments (the adjacent sounds), why is the change observed in one environment and not the others? This project will address this question through an examination of /s/-retraction, an ongoing sound change in American English in which /s/ is pronounced as /sh/ in /str/ words (such that 'street' is pronounced as 'shtreet') but rarely in /spr/ and /skr/ words ('scream' is not pronounced as 'shcream' and 'spree' is not pronounced as 'shpree'). The results of this project will most directly influence the scientific understanding of /s/-retraction narrowly and sound change and its initiation and propagation more broadly, but can potentially contribute to developments in related fields and industries. By examining individual variation, these findings may offer insights into first and/or second language acquisition and may aid in the development of speech recognition systems that are faster and more accurate for a greater number of users, especially those with non-standard accents.<br/><br/>This project will examine participants' production and perception of /s/ in these environments in order to better understand the observed asymmetries in /s/-retraction. For this project, participants will listen to auditory prompts containing /s/ in different phonological environments corresponding to visually presented images, and in turn give oral instructions for the computer to select the correct image. This design has two important components: firstly, it is a lexical decision task, meaning that the participant has to select the best meaning (represented by the images) given the possibly ambiguous or unexpected acoustic signal (auditory instructions). Secondly, the task encourages phonetic accommodation, the process by which a speaker begins to sound more like their interlocutor (the model talker giving instructions), which can provide insight into how sound changes propagate across a speech community. Throughout the experiment, three simultaneous measurements will be collected: audio recording, ultrasound imaging of the tongue, and eye gaze tracking. Ultrasound imaging will elucidate the varying articulations of /s/ in different phonological environments and will also provide novel evidence for the articulatory reality of phonetic accommodation. Eye gaze measurements will provide information about listeners' use of /s/ variation in real time. These measurements will be collected simultaneously in order to examine the relationship between participants' production and perception of /s/ in these environments."
"1813695","Geometric Measure Theory, Image Processing, and Nonlinear Partial Differential Equations","DMS","APPLIED MATHEMATICS","08/15/2018","04/26/2018","Monica Torres","IN","Purdue University","Standard Grant","Victor Roytburd","07/31/2021","$171,691.00","","torres@math.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1266","","$0.00","This project will provide a mathematical foundation for the novel numerical methods used in image processing. Applications of image processing are ubiquitous. They range from analyzing medical images, such as CAT or MRI images, to forensic sciences, face recognition, satellite imagery, speech recognition (acoustic imaging), etc. All of these applications are underlied by fast mathematical algorithms that do the work of image enhancement and restoration.  To measure the difference between images, numerical methods under consideration in this project use a so-called ""earth mover's distance"" (in a more general context it is known as the Wasserstein metric). This is a novel application of the earth mover's distances that promises a substantial speed-up of image processing algorithms. Training, advising, and mentoring of graduate students toward their doctoral degree is intertwined with the research tasks of the project. The project will support at least 3 graduate students, two of them female students.<br/> <br/>This research is concerned with several diverse analytical topics; the mathematical analysis is unified through techniques of geometric measure theory. The numerical methods to compute the earth mover's distance are based on a regularization and a minimization over a class of vector fields satisfying a boundary condition. However, there is currently no theory confirming either the existence of minimizers or the convergence of the regularized problem to the original one. These questions will be addressed by using techniques of calculus of variations, gamma convergence, and the theory of traces of divergence-measure vector fields to deal with the boundary condition. Simultaneously this project will consider several unresolved problems concerning the theory of divergence-measure vector fields. Another important aspect of the field of image processing to be investigated is the advancement of methods of evaluating the image noise pioneered by Rudin, Osher, and Fatemi (the ROF model). The research of this project will also include analysis of models for segregation of populations using non-linear elliptic equations, with the goal to understand the free boundaries that separate the populations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826973","AAAI Student Outreach Workshop","IIS","Robust Intelligence","03/15/2018","03/15/2018","Sheila Tejada","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","James Donlon","04/30/2019","$15,000.00","","sheilatejada@gmail.com","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7556","$0.00","This grant supports the participation of undergraduate students in the AAAI Student Outreach Workshop, to be held in conjunction with the AAAI (Association for the Advancement of Artificial Intelligence) and EAAI (Educational Applications of Artificial Intelligence) conferences, February 2-7, 2018 in New Orleans.  This outreach program is aimed at undergraduate students with an interest in AI or Robotics.  Through this workshop, students will work with the Cozmo robot and the Calypso robot intelligence framework to gain hands-on experience in applying artificial intelligence to a commercial robot.  <br/><br/>This activity will expose undergraduate students to practical skills and research issues involved in developing systems with robust intelligence.  The activity promotes student interest in artificial intelligence and autonomous robots for future education and development. The workshop also contributes to the development of effective methods for teaching about artificial intelligence, which has the potential to impact the computer science and engineering education community's ability to recruit future researchers and workforce more broadly.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832717","Workshop on Artificial Intelligence and the ""Barrier of Meaning""","IIS","Robust Intelligence","05/01/2018","04/13/2018","Melanie Mitchell","NM","Santa Fe Institute","Standard Grant","James Donlon","04/30/2019","$20,088.00","","mm@pdx.edu","1399 HYDE PARK ROAD","SANTA FE","NM","875018943","5059462727","CSE","7495","7495, 7556, 9150","$0.00","This workshop brings together eminent scholars in the fields of computer science, psychology, biology, neuroscience, and others to address the topic of ""understanding"" in artificial intelligence.  In this activity, participants will consider what it would mean for advanced computer systems to possess human-like understanding, explore how necessary it is for intelligent systems to exhibit such understanding, and discuss approaches to imbuing these systems with such a capability.  Engagement of a multidisciplinary community will develop new and actionable insight into how we define, design, implement, and control complex systems that overcome this barrier of meaning.  The workshop will likely also lead to outcomes and follow-on activities to benefit AI education and public awareness regarding the state of current artificial intelligence, including its limitations and potential vulnerabilities.<br/><br/>The approach in this workshop is to explore how complex systems extract meaning from the information they encounter. Workshop participants will engage questions about the function and mechanisms of ""understanding"" or ""extracting meaning"" in complex systems across many disciplines, and focus specifically on the relevance of human-like understanding for creating artificial intelligence systems that are reliable, adaptable to novel situations, and robust against adversarial attacks.  Understanding the nature and necessity of understanding remains among the deepest intellectual challenges in AI research. Workshop discussions will be aimed at clarifying common questions and identifying possible novel pathways to answering these questions. Organizers will publish both technical and general-readership summaries communicating the results of the workshop discussions concerning the notions of understanding or meaning as phenomena in diverse disciplines, and how these phenomena relate to, or enable, the robustness that will be needed for safe and trustworthy AI in the real world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763199","CHS: Medium: Collaborative Research: Wearable Sound Sensing and Feedback Techniques for Persons who are Deaf or Hard of Hearing","IIS","HCC-Human-Centered Computing","08/01/2018","04/18/2019","Leah Findlater","WA","University of Washington","Standard Grant","Ephraim Glinert","07/31/2022","$915,994.00","Jon Froehlich","leahkf@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7367","7367, 7924, 9251","$0.00","The goal of this research is to advance wearable sound sensing and feedback technology for users who are deaf or hard of hearing, along with appropriate techniques for human-computer interaction.  A wearable device with multiple microphones and audio processing algorithms to automatically sense, localize, and identify sounds will be developed.  Means to discreetly provide this information to the user via emerging wearable technologies such as head-mounted displays and smartwatches will also be implemented.  Evaluations will include lab and field studies, everyday tasks such as noticing sounds and participating in oral conversations, and objective and subjective measures. Project outcomes will have broad impact by enabling new sound awareness options for users who are deaf or hard of hearing, thereby augmenting the wearer's existing strategies with additional, unobtrusive information. The new assistive technologies will have the potential to improve the lives of a large portion of the population, in particular the growing number of older adults with hearing loss in the United States.<br/><br/>To these ends, major subgoals will include: understanding user needs for wearable sound sensing and feedback, including prioritizing the importance of different sounds across a variety of contexts and based on an individual user's level of hearing loss; developing and evaluating a lightweight wearable sound sensing platform and accompanying algorithms, including both new sound scene analysis algorithms for a microphone array conformal on, or in proximity to, a complex-shaped baffle (the wearer's head) and that take into account how sound scatters off the wearer's body, along with adaptive state-of-the-art sound classification and speech recognition approaches to work with this processed audio; and developing and evaluating visual and haptic or vibrational feedback of the sensed sound via wearable prototypes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763219","CHS: Medium: Collaborative Research: Wearable Sound Sensing and Feedback Techniques for Persons who are Deaf or Hard of Hearing","IIS","HCC-Human-Centered Computing","08/01/2018","04/23/2020","Raja Kushalnagar","DC","Gallaudet University","Standard Grant","Ephraim Glinert","07/31/2022","$116,000.00","","raja.kushalnagar@gallaudet.edu","800 Florida Avenue, NE","Washington","DC","200023660","2026515497","CSE","7367","7367, 7924, 9251","$0.00","The goal of this research is to advance wearable sound sensing and feedback technology for users who are deaf or hard of hearing, along with appropriate techniques for human-computer interaction.  A wearable device with multiple microphones and audio processing algorithms to automatically sense, localize, and identify sounds will be developed.  Means to discreetly provide this information to the user via emerging wearable technologies such as head-mounted displays and smartwatches will also be implemented.  Evaluations will include lab and field studies, everyday tasks such as noticing sounds and participating in oral conversations, and objective and subjective measures. Project outcomes will have broad impact by enabling new sound awareness options for users who are deaf or hard of hearing, thereby augmenting the wearer's existing strategies with additional, unobtrusive information. The new assistive technologies will have the potential to improve the lives of a large portion of the population, in particular the growing number of older adults with hearing loss in the United States.<br/><br/>To these ends, major subgoals will include: understanding user needs for wearable sound sensing and feedback, including prioritizing the importance of different sounds across a variety of contexts and based on an individual user's level of hearing loss; developing and evaluating a lightweight wearable sound sensing platform and accompanying algorithms, including both new sound scene analysis algorithms for a microphone array conformal on, or in proximity to, a complex-shaped baffle (the wearer's head) and that take into account how sound scatters off the wearer's body, along with adaptive state-of-the-art sound classification and speech recognition approaches to work with this processed audio; and developing and evaluating visual and haptic or vibrational feedback of the sensed sound via wearable prototypes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763235","CHS: Medium: Collaborative Research: Wearable Sound Sensing and Feedback Techniques for Persons who are Deaf or Hard of Hearing","IIS","HCC-Human-Centered Computing","08/01/2018","07/21/2018","Ramani Duraiswami","MD","University of Maryland College Park","Standard Grant","Ephraim Glinert","07/31/2022","$200,000.00","","ramani@umiacs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7367","7367, 7924","$0.00","The goal of this research is to advance wearable sound sensing and feedback technology for users who are deaf or hard of hearing, along with appropriate techniques for human-computer interaction.  A wearable device with multiple microphones and audio processing algorithms to automatically sense, localize, and identify sounds will be developed.  Means to discreetly provide this information to the user via emerging wearable technologies such as head-mounted displays and smartwatches will also be implemented.  Evaluations will include lab and field studies, everyday tasks such as noticing sounds and participating in oral conversations, and objective and subjective measures. Project outcomes will have broad impact by enabling new sound awareness options for users who are deaf or hard of hearing, thereby augmenting the wearer's existing strategies with additional, unobtrusive information. The new assistive technologies will have the potential to improve the lives of a large portion of the population, in particular the growing number of older adults with hearing loss in the United States.<br/><br/>To these ends, major subgoals will include: understanding user needs for wearable sound sensing and feedback, including prioritizing the importance of different sounds across a variety of contexts and based on an individual user's level of hearing loss; developing and evaluating a lightweight wearable sound sensing platform and accompanying algorithms, including both new sound scene analysis algorithms for a microphone array conformal on, or in proximity to, a complex-shaped baffle (the wearer's head) and that take into account how sound scatters off the wearer's body, along with adaptive state-of-the-art sound classification and speech recognition approaches to work with this processed audio; and developing and evaluating visual and haptic or vibrational feedback of the sensed sound via wearable prototypes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750193","CAREER: Technology Assisted Conversations","IIS","HCC-Human-Centered Computing","08/01/2018","06/30/2020","Keith Vertanen","MI","Michigan Technological University","Continuing Grant","Ephraim Glinert","07/31/2023","$301,217.00","","vertanen@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","7367","1045, 7367","$0.00","Face-to-face conversation is an important way in which people communicate with each other, but unfortunately there are millions who suffer from disorders that impede normal conversation.  This project will explore new real-time communication solutions for people who face speaking challenges, including those with physical or cognitive disabilities, for example by exploiting implicit and explicit contextual input obtained from a person's conversation partner.  The goal is to develop technology that improves upon the Augmentative and Alternative Communication (AAC) devices currently available to help people speak faster and more fluidly.  The project will expand the resources for research into conversational interactive systems, the deliverables to include a probabilistic text entry toolkit, AAC user interfaces, and an augmented reality conversation assistant.  Project outcomes will include flexible, robust, and data-driven methods that extend to new use scenarios.  To enhance its broader impact, the project will educate the public about AAC via outreach events and by the online community the work will create.  The PI will assemble teams of undergraduates to develop the project's software, and he will host a summer youth program on the technology behind text messaging, offering scholarships for women, students with disabilities, and students from underrepresented groups.  Funded first-year research opportunities will further help retain undergraduates, particularly women, in computing.<br/><br/>This project will explore the design space of conversational interactive systems, by investigating both systems that improve communication for non-speaking individuals who use AAC devices and systems that enhance communication for speaking individuals who face other conversation-related challenges.  Context-sensitive prediction algorithms that use: 1) speech recognition on the conversation partner's turns; 2) the identity of the partner as determined by speaker identification; 3) dialogue state information; and 4) suggestions made by a partner on a mobile device will be considered.  User studies will investigate the effectiveness and user acceptance of partner-based predictions.  New methodologies will be created for evaluating context-sensitive AAC interfaces.  The impact of training AAC language models on data from existing corpora, from simulated AAC users, and from actual AAC users will be compared.  This research will expand our knowledge about how to leverage conversational context in augmented reality, and it will curate a public test set contributed by AAC users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810914","Order Determination for Hidden Markov and Related Models","DMS","STATISTICS","07/01/2018","05/10/2018","Samuel Kou","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2021","$250,000.00","","kou@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","1269","$0.00","Hidden Markov models (HMMs) are powerful tools for processing time series data and are widely used in scientific and engineering applications, including speech recognition, machine translation, computational biology, cryptanalysis, and finance. The fundamental components of an HMM include the noisy observations and the corresponding hidden states. In most applications, the number of hidden states (the order of the HMM) is not known beforehand but conveys important information about the underlying process. For example, in molecular biology, the total number of hidden states may be the number of distinct 3D conformations of a protein; in chemistry, the total number of hidden states may be the number of distinct chemical species in an organic reaction. This project plans to investigate order determination for HMMs, finite mixture models, and hierarchical HMMs; the latter two models are special cases and extensions of HMMs. The project aims to develop a consistent and competitive method for order selection. In addition to a thorough theoretical investigation, comprehensive numerical studies and applications in biology and chemistry will be conducted. The project will not only significantly advance the theoretical understanding of HMMs, but also provide powerful tools for researchers to analyze data. The data applications will help advance molecular biology and biochemistry. The project also aims to support and train undergraduate and graduate students, with special attention being given to recruiting students from under-represented groups into statistics and related fields. Education at the undergraduate and graduate levels will be integrated into the research activities. <br/><br/>The project will establish the marginal likelihood method as a consistent and competitive order selection method for HMMs, finite mixture models, and hierarchical HMMs. Five research studies will be carried out, enumerated as follows. (1) Investigate the order of HMMs, where the goal is to identify and develop consistent methods for HMM order determination. (2) Investigate order selection issues in finite mixture models. Finite mixture models can be reformulated as special types of HMMs. The goal is to develop a method for consistently estimating the number of mixture components. (3) Investigate order determination of hierarchical HMMs, where multiple HMMs are linked through a hierarchical structure. The aim here is to identify and develop consistent methods for determining the order of hierarchical HMMs, taking special effort to address the challenging issue that multiple HMMs often have quite diverse characteristics, such as lengths. (4) Study computational challenges and investigate and implement efficient computational methods for the order determination of HMM and related models, including the implementation and release of an open source, publicly available R package. (5) Apply the new method to ion channel data and single-molecule data on co-translational protein targeting. The PI also plans to develop courses that introduce and guide students in HMMs, mixture models, and hierarchical HMMs. The success of the proposed research will develop a theoretical basis and associated methodology for consistent order determination of HMMs and related models. The research achievements and the education components will broadly impact the analysis of HMMs, hierarchical HMMs, and model selection and also help train a new generation of scholars and researchers in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750333","CAREER: New Algorithms for Submodular Optimization","CCF","Algorithmic Foundations","02/01/2018","03/12/2020","Alina Ene","MA","Trustees of Boston University","Continuing Grant","A. Funda Ergun","01/31/2023","$297,368.00","","aene@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7796","1045, 7926","$0.00","Submodular optimization provides general solutions to a wide range of applications from monitoring water distribution networks to summarizing large corpora of documents and speech recognition. Most of the existing submodular optimization algorithms are not suitable for modern datasets, since they are designed for worst-case instances and they suffer from prohibitive running times and poor empirical performance. This project aims to develop scalable algorithmic approaches with improved empirical performance for submodular optimization and to transfer theoretical insights to applications. The proposed research brings together insights from computer science, mathematics and optimization, and strengthens connections among these fields. The project will involve training the next wave of students and equipping them with technical tools to work in all these fields.<br/><br/>The project focuses on three inter-related research directions in submodular function optimization: (a) Design faster algorithms for minimizing submodular functions with a decomposable or sum structure. The approach is to build on a rich set of tools from both discrete and continuous optimization. (b) Design algorithms for constrained submodular maximization problems with improved approximation guarantees and faster running times. The focus is on settling the approximability of constrained submodular maximization problems with a non-monotone objective and on designing faster algorithms for central families of constraints. (c) Design algorithms and frameworks for allocation or labeling problems with submodular costs. The main goal is to obtain more expressive algorithmic frameworks and efficient algorithms."
"1829786","Doctoral Mentoring Consortium at International Joint Conference on Artificial Intelligence (IJCAI) 2018","IIS","Robust Intelligence","03/15/2018","03/13/2018","Maria Gini","MN","University of Minnesota-Twin Cities","Standard Grant","James Donlon","02/29/2020","$20,000.00","","gini@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495","7495, 7556","$0.00","This grant supports student travel for select students to participate in the Doctoral Mentoring Consortium (DMC) at the International Joint Conference on Artificial Intelligence (IJCAI) that will be held in Stockholm, Sweden, July 13-19, 2018. This is the premier international conference for researchers in artificial intelligence research across a fully international research community. This activity will bring together a broad community of researchers in the field of AI, and support junior researchers at this critical early-career stage.  The DMC creates the opportunity to bring in participants who might not have attended an AI conference due to lack of resources. This is especially true for those at smaller institutions and those which have less developed AI programs. Engaging such participants has the potential to draw more talent into AI research, improve research ideas in their formative stage, and engender collaborations across the breadth of disciplines.  This event provides students with invaluable exposure to outside perspectives on their work at a critical time in their research and enables them to explore their career objectives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931227","FW-HTF: Collaborative Research: Pre-Skilling Workers, Understanding Labor Force Implications and Designing Future Factory Human-Robot Workflows Using a Physical Simulation Platform","DUE","FW-HTF-Adv Cogn & Phys Capblty","07/01/2018","06/03/2019","Kylie Peppler","CA","University of California-Irvine","Standard Grant","Alexandra Medina-Borja","09/30/2022","$300,002.00","","kpeppler@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","EHR","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim.<br/><br/>This collaborative project between Purdue University, Indiana University and the Massachusetts Institute of Technology is based on the rationale that today's manufacturers, especially small and medium enterprises, may struggle to keep pace with rapid changes in manufacturing. To help manufacturers thrive in a rapidly changing industry, this project aims to develop a Physical-Simulation Platform that will realistically simulate interactions between workers, robots, and machines in future factories, and at the same time, improve factory agility and productivity. This project will provide new insights into workers' spatial, multitasking, and predictive task abilities in manufacturing, and their performance in shared and smooth workflows. Those insights can then be used to shape the augmented manufacturing environment of the future by amplifying cognitive capacity and transferring some cognitive burden to artificial intelligence and smart automation.  Such changes can improve both productivity and worker experience. The project will explore the economic impact on different types of workers as well as the benefits of artificial intelligence-based augmentation technologies on human labor, factory productivity and agility. In addition, the project will contribute to workforce development by creating educational plans and outreach to prepare workers for the manufacturing workplace of the future. The researchers will directly engage underserved young people by introducing them to new toolkits and curriculum developed as part of this project.  These materials can then be adapted by educators across the country. Strong industry collaborations are present to facilitate testing and adoption of this approach. <br/><br/>The research team of mechanical and electrical engineers, psychologists, computer scientists, education researchers, and economists will work toward accomplishing five goals: (1) use Mixed Reality to capture interactions, shared workflows, and collaborative tasks as close as possible to a real manufacturing environment; (2) develop and demonstrate new types of authoring platform to program robots, internet-of-things-based machines, and humans interacting with them, with augmented reality, artificial intelligence to substantially reduce cognitive loads and enhance worker and factory overall capabilities and productivities; (3) discover, design and develop flexible representations of collaborative intelligence workflows and metrics to simulate and evaluate Humans-Robots-Machines shared work; (4) evaluate shared work economics and labor market implications of augmenting humans with robotics, augmented reality, and artificial intelligence; and (5) pre-skill the workforce and increase engagement towards the future of work at the human-technology interface.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839896","FW-HTF: Collaborative Research: Pre-Skilling Workers, Understanding Labor Force Implications and Designing Future Factory Human-Robot Workflows Using a Physical Simulation Platform","DUE","FW-HTF-Adv Cogn & Phys Capblty","10/01/2018","08/22/2018","Kylie Peppler","IN","Indiana University","Standard Grant","Alexandra Medina-Borja","07/31/2019","$300,002.00","","kpeppler@uci.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","EHR","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim.<br/><br/>This collaborative project between Purdue University, Indiana University and the Massachusetts Institute of Technology is based on the rationale that today's manufacturers, especially small and medium enterprises, may struggle to keep pace with rapid changes in manufacturing. To help manufacturers thrive in a rapidly changing industry, this project aims to develop a Physical-Simulation Platform that will realistically simulate interactions between workers, robots, and machines in future factories, and at the same time, improve factory agility and productivity. This project will provide new insights into workers' spatial, multitasking, and predictive task abilities in manufacturing, and their performance in shared and smooth workflows. Those insights can then be used to shape the augmented manufacturing environment of the future by amplifying cognitive capacity and transferring some cognitive burden to artificial intelligence and smart automation.  Such changes can improve both productivity and worker experience. The project will explore the economic impact on different types of workers as well as the benefits of artificial intelligence-based augmentation technologies on human labor, factory productivity and agility. In addition, the project will contribute to workforce development by creating educational plans and outreach to prepare workers for the manufacturing workplace of the future. The researchers will directly engage underserved young people by introducing them to new toolkits and curriculum developed as part of this project.  These materials can then be adapted by educators across the country. Strong industry collaborations are present to facilitate testing and adoption of this approach. <br/><br/>The research team of mechanical and electrical engineers, psychologists, computer scientists, education researchers, and economists will work toward accomplishing five goals: (1) use Mixed Reality to capture interactions, shared workflows, and collaborative tasks as close as possible to a real manufacturing environment; (2) develop and demonstrate new types of authoring platform to program robots, internet-of-things-based machines, and humans interacting with them, with augmented reality, artificial intelligence to substantially reduce cognitive loads and enhance worker and factory overall capabilities and productivities; (3) discover, design and develop flexible representations of collaborative intelligence workflows and metrics to simulate and evaluate Humans-Robots-Machines shared work; (4) evaluate shared work economics and labor market implications of augmenting humans with robotics, augmented reality, and artificial intelligence; and (5) pre-skill the workforce and increase engagement towards the future of work at the human-technology interface.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839971","FW-HTF: Collaborative Research: Pre-Skilling Workers, Understanding Labor Force Implications and Designing Future Factory Human-Robot Workflows Using a Physical Simulation Platform","DUE","FW-HTF-Adv Cogn & Phys Capblty","10/01/2018","08/22/2018","Karthik Ramani","IN","Purdue University","Standard Grant","Alexandra Medina-Borja","09/30/2022","$1,839,998.00","Shimon Nof, Alexander Quinn, Thomas Redick","ramani@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","EHR","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim.<br/><br/>This collaborative project between Purdue University, Indiana University and the Massachusetts Institute of Technology is based on the rationale that today's manufacturers, especially small and medium enterprises may struggle to keep pace with rapid changes in manufacturing. To help manufacturers thrive in a rapidly changing industry, this project aims to develop a Physical-Simulation Platform that will realistically simulate interactions between workers, robots, and machines in future factories, and at the same time, improve factory agility and productivity. This project will provide new insights into workers' spatial, multitasking, and predictive task abilities in manufacturing, and their performance in shared and smooth workflows. Those insights can then be used to shape the augmented manufacturing environment of the future by amplifying cognitive capacity and transferring some cognitive burden to artificial intelligence and smart automation.  Such changes can improve both productivity and worker experience. The project will explore the economic impact on different types of workers as well as the benefits of artificial intelligence-based augmentation technologies on human labor, factory productivity and agility. In addition, the project will contribute to workforce development by creating educational plans and outreach to prepare workers for the manufacturing workplace of the future. The researchers will directly engage underserved young people by introducing them to new toolkits and curriculum developed as part of this project.  These materials can then be adapted by educators across the country. Strong industry collaborations are present to facilitate testing and adoption of this approach. <br/><br/>The research team of mechanical and electrical engineers, psychologists, computer scientists, education researchers, and economists will work toward accomplishing five goals: (1) use Mixed Reality to capture interactions, shared workflows, and collaborative tasks as close as possible to a real manufacturing environment; (2) develop and demonstrate new types of authoring platform to program robots, internet-of-things-based machines, and humans interacting with them, with augmented reality, artificial intelligence to substantially reduce cognitive loads and enhance worker and factory overall capabilities and productivities; (3) discover, design and develop flexible representations of collaborative intelligence workflows and metrics to simulate and evaluate Humans-Robots-Machines shared work; (4) evaluate shared work economics and labor market implications of augmenting humans with robotics, augmented reality, and artificial intelligence; and (5) pre-skill the workforce and increase engagement towards the future of work at the human-technology interface.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839921","FW-HTF: Collaborative Research: Pre-Skilling Workers, Understanding Labor Force Implications and Designing Future Factory Human-Robot Workflows Using a Physical Simulation Platform","DUE","FW-HTF-Adv Cogn & Phys Capblty","10/01/2018","08/22/2018","Daron Acemoglu","MA","Massachusetts Institute of Technology","Standard Grant","Alexandra Medina-Borja","09/30/2022","$360,000.00","","daron@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","EHR","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim.<br/><br/>This collaborative project between Purdue University, Indiana University and the Massachusetts Institute of Technology is based on the rationale that today's manufacturers, especially small and medium enterprises may struggle to keep pace with rapid changes in manufacturing. To help manufacturers thrive in a rapidly changing industry, this project aims to develop a Physical-Simulation Platform that will realistically simulate interactions between workers, robots, and machines in future factories, and at the same time, improve factory agility and productivity. This project will provide new insights into workers' spatial, multitasking, and predictive task abilities in manufacturing, and their performance in shared and smooth workflows. Those insights can then be used to shape the augmented manufacturing environment of the future by amplifying cognitive capacity and transferring some cognitive burden to artificial intelligence and smart automation.  Such changes can improve both productivity and worker experience. The project will explore the economic impact on different types of workers as well as the benefits of artificial intelligence-based augmentation technologies on human labor, factory productivity and agility. In addition, the project will contribute to workforce development by creating educational plans and outreach to prepare workers for the manufacturing workplace of the future. The researchers will directly engage underserved young people by introducing them to new toolkits and curriculum developed as part of this project.  These materials can then be adapted by educators across the country. Strong industry collaborations are present to facilitate testing and adoption of this approach. <br/><br/>The research team of mechanical and electrical engineers, psychologists, computer scientists, education researchers, and economists will work toward accomplishing five goals: (1) use Mixed Reality to capture interactions, shared workflows, and collaborative tasks as close as possible to a real manufacturing environment; (2) develop and demonstrate new types of authoring platform to program robots, internet-of-things-based machines, and humans interacting with them, with augmented reality, artificial intelligence to substantially reduce cognitive loads and enhance worker and factory overall capabilities and productivities; (3) discover, design and develop flexible representations of collaborative intelligence workflows and metrics to simulate and evaluate Humans-Robots-Machines shared work; (4) evaluate shared work economics and labor market implications of augmenting humans with robotics, augmented reality, and artificial intelligence; and (5) pre-skill the workforce and increase engagement towards the future of work at the human-technology interface.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757269","CompCog: Helping people make more future-minded decisions using optimal gamification","SES","Decision, Risk & Mgmt Sci, CYBERINFRASTRUCTURE, Perception, Action & Cognition","08/15/2018","08/14/2018","Thomas Griffiths","CA","University of California-Berkeley","Standard Grant","Jeryl Mumpower","10/31/2019","$519,423.00","","tomg@princeton.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","SBE","1321, 7231, 7252","075Z, 7252, 9179","$0.00","Helping people, teams, and organizations achieve important goals may be one of the most effective ways to increase productivity and promote human progress. To achieve their goals, many organizations employ financial incentives or game elements, such as points, levels, and badges, to motivate employees to become more productive. This project develops a theoretical foundation and computational tools for designing better incentive structures to help people achieve their goals. The project connects the crucial challenges of goal achievement studied in psychology to the computational methods from artificial intelligence that can be used to solve them. By bridging this gap the project provides a new way for artificial intelligence to communicate with people and empowers them to overcome the motivational obstacles and cognitive limitations that might otherwise prevent them from making good decisions. <br/><br/>At the heart of this project is a mathematical theory for optimizing incentive structures to help people make better decisions in complex, partially unknown environments. This theory is used as the basis for two cognitive prostheses that leverage artificial intelligence and gamification to help people achieve their goals: an intelligent to-do list gamification system that helps people become more productive and procrastinate less and an app that reinforces good habits. Field experiments are used to evaluate whether these cognitive prostheses are effective in the real world, working towards the development of intelligent systems that can aid people in setting and achieving their goals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1855888","Collaborative Research: Hierarchical Intelligent and Adaptive Techniques to Enable Resilient DC Power Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2018","10/22/2018","Luis Herrera","NY","SUNY at Buffalo","Standard Grant","Anil Pahwa","08/31/2021","$242,338.00","","lcherrer@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","ENG","7607","155E, 7607, 9102","$0.00","As the adoption of energy sources and loads with inherent dc voltage continues to increase, an electric system based on dc power can offer tremendous advantages over ac, with higher efficiency, less power conversion stages, smaller footprint, and higher reliability.  For these reasons, dc power systems and microgrids are now used in electric vehicles, ships, aircraft, and in rural areas.  However, electrical faults in dc power networks can lead to extremely dangerous situations which are more difficult to interrupt than their ac counterparts, particularly due to the lack of zero voltage crossings.  Moreover, high impedance faults in the form of electrical arcs, such as those caused by loose connections or chafed wires, are very difficult to detect because of the low fault current.  The high penetration of electronics loads with advanced controllers make the fault detection and localization even more challenging.  To increase the safety and resiliency of dc based systems, the proposed project will address these technical challenges in detecting high impedance faults in dc power systems by developing intelligent and adaptive fault detection, localization, and isolation techniques that are built upon a comprehensive and systematic fault modeling and characterization study. These techniques can significantly improve the performance of existing and future dc systems to enable their wide adoption at larger scales, which can provide efficient and reliable interfaces to many renewable resources, energy storage units, and modern electronic loads and align with the nation's initiatives in using clean and green energy.  This project is intrinsically multidisciplinary by bringing advanced and exciting modern control theories, artificial intelligence, and signal processing techniques into electric power engineering.  The tasks in this project involve a wide range of expertise and experience from software simulation and control algorithms to hardware testing; from circuit level study to system level implementation, which provides a unique and high quality training opportunity for future engineers. The proposed educational activities will also broaden participation of women and other under-represented students.<br/><br/>The goal of the proposed research is to develop fault detection, localization, and isolation techniques for modern dc power systems through a hierarchical approach with intelligent and adaptive functionalities.  It addresses the most challenging issues in the protection of dc power systems with a systematic and transformative effort.  The fault modeling and characterization study of the proposed project will generate fundamental and critical knowledge of high impedance faults in modern application settings through comprehensive experimental and analytical approaches.  The proposed high impedance fault detection and localization techniques will take into account the effect of advanced controllers through dynamic parameter estimation.  The adaptive and integrated fault detection and localization schemes to be developed will significantly enhance the existing protection system design and online stability assessment methodologies by adopting modern nonlinear control theory and artificial intelligence tools. The proposed research is expected to produce significant results of both theoretical and practical values to the field of dc power systems.  When successfully completed, the project has the potential to revolutionize the control and protection aspects of dc power systems, minimizing the adverse impact of high impedance faults and constant power loads.  The proposed techniques can be applied to dc systems in different scales ranging from isolated dc distribution networks to interconnected dc microgrids, to improve the fault protection effectiveness and therefore their resiliency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809839","Collaborative Research: Hierarchical Intelligent and Adaptive Techniques to Enable Resilient DC Power Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2018","09/19/2018","Xiu Yao","NY","SUNY at Buffalo","Standard Grant","Anil Pahwa","08/31/2021","$282,023.00","","xiuyao@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","ENG","7607","155E, 9102","$0.00","As the adoption of energy sources and loads with inherent dc voltage continues to increase, an electric system based on dc power can offer tremendous advantages over ac, with higher efficiency, less power conversion stages, smaller footprint, and higher reliability.  For these reasons, dc power systems and microgrids are now used in electric vehicles, ships, aircraft, and in rural areas.  However, electrical faults in dc power networks can lead to extremely dangerous situations which are more difficult to interrupt than their ac counterparts, particularly due to the lack of zero voltage crossings.  Moreover, high impedance faults in the form of electrical arcs, such as those caused by loose connections or chafed wires, are very difficult to detect because of the low fault current.  The high penetration of electronics loads with advanced controllers make the fault detection and localization even more challenging.  To increase the safety and resiliency of dc based systems, the proposed project will address these technical challenges in detecting high impedance faults in dc power systems by developing intelligent and adaptive fault detection, localization, and isolation techniques that are built upon a comprehensive and systematic fault modeling and characterization study. These techniques can significantly improve the performance of existing and future dc systems to enable their wide adoption at larger scales, which can provide efficient and reliable interfaces to many renewable resources, energy storage units, and modern electronic loads and align with the nation's initiatives in using clean and green energy.  This project is intrinsically multidisciplinary by bringing advanced and exciting modern control theories, artificial intelligence, and signal processing techniques into electric power engineering.  The tasks in this project involve a wide range of expertise and experience from software simulation and control algorithms to hardware testing; from circuit level study to system level implementation, which provides a unique and high quality training opportunity for future engineers. The proposed educational activities will also broaden participation of women and other under-represented students.<br/><br/>The goal of the proposed research is to develop fault detection, localization, and isolation techniques for modern dc power systems through a hierarchical approach with intelligent and adaptive functionalities.  It addresses the most challenging issues in the protection of dc power systems with a systematic and transformative effort.  The fault modeling and characterization study of the proposed project will generate fundamental and critical knowledge of high impedance faults in modern application settings through comprehensive experimental and analytical approaches.  The proposed high impedance fault detection and localization techniques will take into account the effect of advanced controllers through dynamic parameter estimation.  The adaptive and integrated fault detection and localization schemes to be developed will significantly enhance the existing protection system design and online stability assessment methodologies by adopting modern nonlinear control theory and artificial intelligence tools. The proposed research is expected to produce significant results of both theoretical and practical values to the field of dc power systems.  When successfully completed, the project has the potential to revolutionize the control and protection aspects of dc power systems, minimizing the adverse impact of high impedance faults and constant power loads.  The proposed techniques can be applied to dc systems in different scales ranging from isolated dc distribution networks to interconnected dc microgrids, to improve the fault protection effectiveness and therefore their resiliency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809957","Collaborative Research: Hierarchical Intelligent and Adaptive Techniques to Enable Resilient DC Power Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2018","09/19/2018","Luis Herrera","NY","Rochester Institute of Tech","Standard Grant","Anil Pahwa","11/30/2018","$242,338.00","","lcherrer@buffalo.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","ENG","7607","155E, 9102","$0.00","As the adoption of energy sources and loads with inherent dc voltage continues to increase, an electric system based on dc power can offer tremendous advantages over ac, with higher efficiency, less power conversion stages, smaller footprint, and higher reliability.  For these reasons, dc power systems and microgrids are now used in electric vehicles, ships, aircraft, and in rural areas.  However, electrical faults in dc power networks can lead to extremely dangerous situations which are more difficult to interrupt than their ac counterparts, particularly due to the lack of zero voltage crossings.  Moreover, high impedance faults in the form of electrical arcs, such as those caused by loose connections or chafed wires, are very difficult to detect because of the low fault current.  The high penetration of electronics loads with advanced controllers make the fault detection and localization even more challenging.  To increase the safety and resiliency of dc based systems, the proposed project will address these technical challenges in detecting high impedance faults in dc power systems by developing intelligent and adaptive fault detection, localization, and isolation techniques that are built upon a comprehensive and systematic fault modeling and characterization study. These techniques can significantly improve the performance of existing and future dc systems to enable their wide adoption at larger scales, which can provide efficient and reliable interfaces to many renewable resources, energy storage units, and modern electronic loads and align with the nation's initiatives in using clean and green energy.  This project is intrinsically multidisciplinary by bringing advanced and exciting modern control theories, artificial intelligence, and signal processing techniques into electric power engineering.  The tasks in this project involve a wide range of expertise and experience from software simulation and control algorithms to hardware testing; from circuit level study to system level implementation, which provides a unique and high quality training opportunity for future engineers. The proposed educational activities will also broaden participation of women and other under-represented students.<br/><br/>The goal of the proposed research is to develop fault detection, localization, and isolation techniques for modern dc power systems through a hierarchical approach with intelligent and adaptive functionalities.  It addresses the most challenging issues in the protection of dc power systems with a systematic and transformative effort.  The fault modeling and characterization study of the proposed project will generate fundamental and critical knowledge of high impedance faults in modern application settings through comprehensive experimental and analytical approaches.  The proposed high impedance fault detection and localization techniques will take into account the effect of advanced controllers through dynamic parameter estimation.  The adaptive and integrated fault detection and localization schemes to be developed will significantly enhance the existing protection system design and online stability assessment methodologies by adopting modern nonlinear control theory and artificial intelligence tools. The proposed research is expected to produce significant results of both theoretical and practical values to the field of dc power systems.  When successfully completed, the project has the potential to revolutionize the control and protection aspects of dc power systems, minimizing the adverse impact of high impedance faults and constant power loads.  The proposed techniques can be applied to dc systems in different scales ranging from isolated dc distribution networks to interconnected dc microgrids, to improve the fault protection effectiveness and therefore their resiliency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814056","RI: Small: Designing Preferences, Beliefs, and Identities for Artificial Intelligence","IIS","Robust Intelligence","09/01/2018","07/23/2018","Vincent Conitzer","NC","Duke University","Standard Grant","Roger Mailler","08/31/2021","$400,000.00","","conitzer@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495","7495, 7923","$0.00","Historically, AI researchers have primarily focused on developing techniques that work well for pre-specified objectives that provide a useful measure of how well the techniques are working.  This approach is perfectly sensible in a situation where the techniques are not yet ready to make their way out of the lab and into the world.  However, as AI is now being broadly deployed in the world, more thought needs to be put into the methodologies for designing the objectives of AI systems.  This is because our aim is no longer just to evaluate whether our other techniques are able to pursue a given objective well, but rather to actually have them do good in the world.  Besides the AI system's objectives, we must also specify where one part of the system ends and another begins, as well as how it models the world.  Generally, it is not possible or desirable to simply hand off the system to a customer (in the broad sense of the word) who then must somehow fill in these blanks.  AI researchers need to be involved in this process because they understand how the system works and are able to provide algorithmic support for these decisions.  But rigorous computational frameworks for these processes are lacking, and they are what this research aims to provide.<br/><br/>Specifically, existing research in artificial intelligence, mirroring frameworks in economics and other related fields, is built on a conception of AI systems as agents.  It generally proceeds from the premise that each such agent has a well-defined identity over time, well-defined preferences over the different ways in which things may proceed, and well-defined beliefs about the world as it is and how it will develop over time.  Typical research then concerns the design of algorithms under the assumption that all these aspects have already been specified (with the common exception of still needing to do some learning about the environment).   However, as we design real AI systems, we in fact need to specify where the boundaries between one agent and another in the system lie, what objective functions these agents aim to maximize, and to some extent even what belief formation processes they use.  The premise of this research is that as AI is being broadly deployed in the world, we need well-founded theories of, and methodologies and algorithms for, how to design preferences, identities, and beliefs.  Doing so in a responsible fashion will require the development and rigorous evaluation of new techniques.  The project will address these questions from a rigorous foundation in decision theory, game theory, social choice theory, mechanism design theory, and the algorithmic and computational aspects of these fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842348","NSF Workshop on Reconfigurable Sensor Systems Integrated with Artificial Intelligence and Data Harnessing to Enable Personalized Medicine","ECCS","Plant Genome Research Project, ERC-Eng Research Centers, GOALI-Grnt Opp Acad Lia wIndus, EPMD-ElectrnPhoton&MagnDevices, CCSS-Comms Circuits & Sens Sys, EFRI Research Projects, Software & Hardware Foundation, BIOSENS-Biosensing, Computational Biology, Smart and Connected Health","09/15/2018","09/11/2018","Michael Daniele","NC","North Carolina State University","Standard Grant","Shubhra Gangopadhyay","08/31/2019","$59,472.00","Veena Misra, Edgar Lobaton, Nirmish Shah","mdaniel6@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","ENG","1329, 1480, 1504, 1517, 7564, 7633, 7798, 7909, 7931, 8018","7556, 7945","$0.00","This project is to convene a 2-day workshop, attended by a forum of experts in engineering, data science, computer science, biological and behavioral sciences, to explore the state-of-the art and the needs for the next-generation of sensors and systems hardware that integrate advanced data science and computing capabilities. The workshop will focus on the gaps and opportunities for new hardware development as it relates to applications in medicine. The products of the workshop will be a report which provides a technological roadmap that defines the critical needs to bridge the gaps at the interface of sensor hardware, data science, and computer science.<br/>  <br/>Intelligent, interactive, and networked sensor systems are a growing part of the biotechnological landscape, especially in the area of wearable, implantable, and point-of-use biosensors. The focus of this multi-phased workshop is to determine future strategies for advancing the fundamental understanding and engineering of reconfigurable sensor systems by integrating hardware with data harnessing, real-time learning, and artificial intelligence capabilities. Specifically, this workshop will define the state-of-the-art, necessary innovations, and future challenges facing the research and development of reconfigurable sensor systems for applications in understanding of human physiology, pathophysiology, metacognition, cognition, and behavioral psychology. To achieve this capability, this workshop aims to bring together the knowledge in hardware, theoretical models, methods and processes, and data from multiple disciplines to develop new platforms for addressing challenges at the human-device-data interface.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840433","Planning Grant: Engineering Research Center for Infrastructure Finance through Intelligent Design and Operations (InFinIDO)","EEC","ERC-Eng Research Centers","09/01/2018","08/31/2018","Peter Adriaens","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Eduardo Misawa","08/31/2019","$100,000.00","Debra Reinhart, Matthew Dixon, Liad Wagman, Jerome Lynch","adriaens@engin.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","1480","1480","$0.00","The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program.  Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>Public infrastructure investment in the US is woefully inadequate, not only to maintain existing demands, but more importantly to meet the needs of an increasingly data-driven economy. The transition to a data-driven economy depends on so-called ?smart cities? which monitor information on city utilities and operations and can respond to changing situations.  Smart infrastructure systems are in an early development stage with rapidly developing applications across water, energy, transportation and buildings. Increasingly, the data analysis skills needed for engineering smart cities, are the same skills needed for many financial models and transactions. Evidence suggests that the very data smart cities collect can lead to new financing models that help fund their development.  The planning grant team will bring together experts in these areas, as well as policy and law.  The goal is to integrate engineering design and financial models in a decision framework that seeks to overcome technical and legal barriers for deployment of intelligent infrastructures.  The societal benefits of this transition include identifying new financial instruments and resources that impacts GDP and jobs, and improves quality of life. Collaboration with public policy experts will explore how smart infrastructures can provide equitable access across rich and poor communities.  Engagement with law school colleagues will address privacy and security concerns, and the impact of data monetization on the financial system. New educational programs such as a Masters in Engineering in Smart Infrastructure Finance will be aimed at training new engineers in the era of artificial intelligence. Training will help engineers engage with business and key civic stakeholders. The described interactions between these disciplines will result in new technologies that stimulate a vibrant innovation ecosystem and facilitate access and use of these technologies. <br/><br/>The planning grant team will explore how sensor-enabled (smart) data networks provide intelligence for the design of efficient financing mechanisms to build and operationalize smart (adaptive, resilient) infrastructure systems. The planning grant activities, including two workshops, will allow the team to engage with potential academic, business and government partners. Despite the excitement, the scalability of smart city infrastructure is hampered by limited operational benchmarking, lack of robust economic models for valuation of information, and pricing mechanisms that may attract public or private investment. The research tests the hypothesis that intelligent infrastructures generate data of sufficient scope, scale, frequency and accuracy that can be aligned with, and tested against, financial models and specifications of emerging data markets.  Through these planning workshops, the proposed Center research activities and strategies to collaborate with industry, investors and the public will be further defined. The objective is to understand how physical or operational performance measurements of smart systems not only enable performance optimization or design iterations, but bring derivative value that can be used or traded in data exchanges.  How can intelligent infrastructures be safely designed - and the IoT data tested - against financial or auction models for value optimization?  Data-driven efficient capital such as insurance and derivatives (futures, options), as well as variable rate performance bonds and smart contracts, increasingly depend on real-time IoT (Internet of Things) information.  In cooperation with industry and public partners, the ERC planning grant team will develop, test and validate engineering models, econometric and financial theory principles using sensor and financial data models in a simulation environmental, and well as based on deployed pilot smart infrastructure systems.  These include smart city components such as: (storm)water utilities, intelligent transportation (roads and bridges), energy systems, and green buildings. The proposed ERC envisions structuring three trust areas reflecting complementary disciplines: (i) Risk quantification and dynamic characterization of infrastructure system data; (ii) Financial risk modeling, pricing, and information valuation in privacy- and cyber security-constrained data markets; (iii) Decision feedback models for infrastructure design, resilience management, and new investment paradigms.  The integration of engineering design with information valuation and pricing, and policy is an emerging field of inquiry and practice with implications for future designs of smart infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809458","From AlphaGo to Power System Artificial Intelligence","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2018","05/19/2020","Fangxing Li","TN","University of Tennessee Knoxville","Standard Grant","Anthony Kuh","07/31/2021","$362,000.00","","fli6@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","ENG","7607","155E, 1653, 9102, 9251","$0.00","The game of Go is an ancient board game which is considered by-far the most complex board game for computer software or artificial intelligence (AI) to solve. AlphaGo, developed by the Google DeepMind team, is the first AI application to defeat a human professional world champion. The three components in AlphaGo (Policy Network, Value Network, and Monte-Carlo Tree Search) have similarities to some classic complex power system problems. In the proposed project, several potential AI applications in electric power systems are categorized into two groups: game-based problems and search-based problems. Detailed analysis of AlphaGo-like algorithms will be investigated for game-based and search-based power system AI applications. This is particularly important under the ongoing paradigm change in power systems evidenced by increasing variable renewable generations and demand-side participations, which lead to a larger amount of data, more uncertain scenarios, and more players. Thus, the success of the proposed project can solve the emerging challenges and potentially change the operation and planning of the power grid. <br/><br/>The intellectual merit of the proposed work includes: (1) similarity comparison of the game of Go and power system problems; (2) detailed algorithm investigations for AlphaGo-like algorithms for game-based and search-based power system problems considering multistage, multiplayer, and multi-scenario studies to address emerging challenges such as high-penetration renewables and demand-side participations; and (3) an open-source software package to implement the proposed work.<br/><br/>The broader impacts lie in the opening of a new door in AI for the area of energy science and engineering. For instance, it may provide new technologies and insights for integrated multi-energy systems involving many players from different energy sectors like gas, thermal, and electricity. From the educational perspective, the results of this project will be utilized to develop new teaching materials, to promote interdisciplinary collaboration in STEM areas, to recruit underrepresented minority and female students, to continue excellence in teaching, advising, and mentoring undergraduate and graduate students, and to develop a Web-based information center with social media to disseminate research results and the open-source software package for power system AI.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840446","Planning Grant: Engineering Research Center for Augmentation Systems and Intelligent Support Technologies for Aging (ASISTa-ERC)","EEC","ERC-Eng Research Centers","09/01/2018","08/29/2018","Gregory Hager","MD","Johns Hopkins University","Standard Grant","Dana L. Denick","08/31/2020","$100,000.00","Elizabeth Mynatt, Wendy Rogers, Sarah Szanton","hager@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","ENG","1480","124E, 1480","$0.00","The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program.  Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>The objective of this planning grant is to further develop the goals and organization of the proposed ASISTa-ERC. The ASISTa-ERC will explore the use of technology to enhance the lives of the growing population of older adults and their caregivers. To do so, it will leverage advances in artificial intelligence, robotics, human-computer interaction, and mobile sensing and computing to create intelligent environments that augment cognitive ability, support physical activities, and amplify social and emotional support for aging individuals, their support network, and their caregivers. These environments will be designed to track individuals over time and adapt to changes in their daily life and their evolving healthcare needs. By enhancing and supporting the growing population of older adults and their caregivers, the work of the ASISTa-ERC will create significant economic and social benefits for older adults and their families. It will also create a new platform to support a rapidly growing workforce devoted to care for aging adults, and a unified framework for new commercial innovations.<br/><br/>The planning activities will refine the research, workforce development, culture of inclusion, and innovation programs of the ASISTa-ERC through a series of three meetings. These meetings will explore three cross-cutting themes: physical support, cognitive support, and social support. They will develop a strategy for creating systems addressing these opportunities that have three key properties: intelligence, interactivity, and individualization (I3). Intelligent systems employ models that can classify and anticipate patterns of activity, and respond to those patterns in ways that most enhance the quality of life of the individual. Interactive systems exploit advanced sensing and human-computer interaction to observe, engage and respond in an appropriate manner. Individualized systems adapt the blend of cognitive, social, and physical support to each unique life situation, and evolve that blend of support as the individual ages and life circumstances change.  All development efforts will be user-centered and user-informed to increase acceptance and adoption by the target populations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820520","ICLS 2018 Rethinking Learning in the Digital Age in ICLS Doctoral Consortium and Early Career Workshops","IIS","Cyberlearn & Future Learn Tech","04/01/2018","03/30/2018","Carolyn Rose","PA","Carnegie-Mellon University","Standard Grant","Amy Baylor","03/31/2019","$30,000.00","","cprose@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8020","7556, 8045","$0.00","This award will support travel and registration for a Doctoral Consortium and Early Career Workshop at the annual meeting of the International Society of the Learning Sciences, which in 2018 will be held in conjunction with the Artificial Intelligence in Education conference and the Association of Computing Machinery conference on Learning at Scale. The three conferences will come together as a Festival of Learning. These joint interdisciplinary conferences will offer a rare professional opportunity for doctoral students and early career professionals to converge and present cutting-edge research from the fields of artificial intelligence, learning science, computer science, cognitive and learning sciences, psychology, and educational technology. Results will contribute to the new generation of researchers in the forward-looking technical area of this interdisciplinary area, to include the learning sciences, data analytics, and artificial intelligence in education. <br/><br/>This project will provide a travel stipend for 5 advanced graduate students to attend the Doctoral Consortium and 5 early career faculty members to attend the Early Career Workshop at the Festival of Learning. The project will also fund a shared young researchers event where 250 young researchers from the three conferences will have a poster session and panel discussion. The project will improve the dissertations of the graduate students and the research agendas of the early career, post-PhD scholars and advance the conference theme ""Rethinking Learning in the Digital Age"". This project will build on the expertise of the learning sciences community and the outcomes of previous workshops held with the International Conference of the Learning Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838702","SCH: INT: Connected Smart Hospitals Enabled by Visible Light Communication","IIS","Smart and Connected Health","09/15/2018","07/11/2019","Albert Wang","CA","University of California-Riverside","Standard Grant","Sylvia Spengler","08/31/2022","$1,200,000.00","Ramdas Pai, Yehuda Kalay, Gang Chen","aw@ee.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","8018","8018, 8062","$0.00","Transformation in health and medicine calls for innovation and integration of information science and engineering approaches to revolutionize healthcare delivery systems, including high-performance wireless communication and sensing technologies. Radio-frequency (RF) wireless technologies can interfere with sensitive medical equipment, hence are not universally accepted in hospitals. This research will develop and apply a visible light communication platform technology, overlaying the existing lighting infrastructure using emerging greener solid-state light-emission diode bulbs, for live tracking and efficient networking to enable smart, connected and efficient hospitals. The scope of work and contributions include developing a novel radio-frequency-free harmless light-emission diode based visible light communication platform technology to enable two-way real-time light-based tracking and networking for smart and connected hospitals, a dynamic event-based behavioral model to simulate building-user interactions in hospital settings, and an artificial intelligence enhanced algorithm to facilitate people-data-system-connected intelligent healthcare ecosystems. Success of the proposed research will transform the field of medicine and health, and achieve affordable and efficient healthcare delivery. The research outcomes will have tremendous societal and economic impacts to the nation and humanity.<br/><br/>The goal of this four-year project is to develop and apply a transformative tracking, networking and operation technologies, using visible light communication (VLC) technology, to enable smart, connected and efficient hospitals. The technical approach will be to overlay the innovative light emission diode (LED) based VLC platform technology on the existing LED lighting infrastructure in a hospital to provide a wireless network for real-time visible light tracking and communications without using radio-frequency (RF) signals, which can interfere with sensitive medical equipment. The technical contributions include novel RF-free LED-based VLC platform technologies and system-on-a-chip solutions to enable two-way real-time visible light tracking and networking for smart and connected hospitals, new dynamic event-based behavioral models to simulate building-user interactions in hospital settings, and innovative artificial intelligence enhanced algorithms to facilitate people-data-system-connected intelligent healthcare delivery ecosystems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826962","WORKSHOP: Support for Travel to Participate in the ACM Intelligent User Interfaces (IUI) 2018 Conference and Student Consortium","IIS","HCC-Human-Centered Computing","03/15/2018","03/12/2018","Wai-Tat Fu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Ephraim Glinert","12/31/2019","$30,000.00","","wfu@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7367","7367, 7556","$0.00","This is funding to provide financial support for up to 15 graduate students (all from U. S. universities and working towards either their Master's degree or a Doctorate) to attend the 2018 International Conference on Intelligent User Interfaces (IUI 2018), to be held March 7-11 in Tokyo, Japan, and to participate in a full-day Student Consortium (workshop) that will be held on March 9.  Sponsored by ACM, the annual IUI conferences represent the growing interest in next-generation intelligent interactive user interfaces.  Attracting 200-300 attendees, they are the premier forum where researchers from academia and industry worldwide who work at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) come together to exchange insights and to present outstanding research and applications whose goal is to make the computerized world a more amenable place.  Unlike traditional AI the focus is not so much on making the computer smart all by itself, but rather on making the interaction between computers and people smarter; and unlike traditional HCI, there is a focus on solutions that involve large amounts of knowledge and emerging technologies such as natural language understanding, brain-computer interfaces, and gesture recognition. To this end, IUI encourages contributions not only from computer science but also from related fields such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, etc.  IUI 2018 will be the 23rd conference in the series; more information about the conference is available online at http://iui.acm.org/2018.  This funding will enable attendance at the IUI conference by students who might otherwise be unable to do so for financial reasons.  It will enhance the educational experience of funded participants, by bringing them into contact with leading researchers in the field and by exposing them to the lively discussion during the course of the conference that often leads to opportunities for career advancement.  The quality of the conference itself will be enhanced as well, thanks to a broadening of the base of institutions represented and increased diversity of participants.  The rich exchange of ideas at IUI has previously proven to be a valuable source of ideas for future research, as well as leading to collaborative efforts; this funding will extend the opportunities for collaboration and provide intellectual stimulus to programs that have previously sent few or no representatives to this conference.  The organizing committee has undertaken to proactively recruit student participants from schools that have not traditionally been well represented in the IUI community.  Women and students who are members of underrepresented groups will be particularly encouraged to participate.  To further assure diversity, no more than two students will be accepted from any given institution.<br/><br/>The IUI 2018 Student Consortium will build on the success of previous such events.  The heart of the Consortium will be a full-day workshop on March 9.  Student trainees will be afforded exposure to their new research community by giving a 20-30 presentation on their work and receiving feedback from peers and a panel of senior researchers.  A group lunch and dinner will encourage social interaction among the student cohort and informal personal interaction with the mentors.  The students' work will also be featured during the main conference in a poster session, where they will gain additional experience explaining their work to others in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840051","FW-HTF: The future of classroom work: Automated Teaching Assistants","DRL","FW-HTF-Adv Cogn & Phys Capblty","09/01/2018","08/24/2018","Kurt VanLehn","AZ","Arizona State University","Standard Grant","Amy Baylor","08/31/2021","$1,478,882.00","","Kurt.Vanlehn@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","EHR","082Y","063Z, 1340, 8045","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim.  Expert teachers effectively orchestrate the complex flow of ideas and products in the classroom through individual, small group, and whole class activities. In supporting the future work of teachers, this project will facilitate the classroom orchestration process through the development of an intelligent teaching assistant, implemented as a tablet-based dashboard. This will allow the teacher to delegate cognitively-demanding orchestration tasks related to classroom activities to the intelligent assistant, enabling him/her to focus on other tasks to support student learning.  Building on an existing shared-document system for middle school mathematics teachers, the proposed system will facilitate teachers in monitoring student work, assessing it, and making conclusions (e.g., indicating student progress, errors, and misconceptions) while allowing them to circulate among students who are working individually or in small groups.  Ultimately, the system will behave like an automated teaching assistant and allow teachers to be more effective and increase their job satisfaction by allowing them to concentrate on assisting learners who most need help.  <br/><br/>Building on the team's Formative Assessment with Computational Technologies (FACT) system, the development of an intelligent teaching assistant will enhance teachers' awareness of what is going on in the classroom and facilitate the cognitively-intensive tasks of orchestrating classroom activities. In the first six-month phase, the project will employ a knowledge engineering process to uncover the tacit knowledge teachers use for decision-making in managing the flow of classroom activities.  In the second phase, over two and one-half years, a series of ten comprehensive trials will be conducted to iteratively develop the cognitive policies of the system through data collection and optimizing the flow of work and ideas, carefully considering teacher input.  This knowledge will be encoded in a rule-based Artificial Intelligence where the intelligent teaching assistant can perform some of the decision-making.  Authorized by the teacher, it will send messages directly to students in specific situations - some will be feedback, some will be hints, some will be requests to visit other students and some will just be to redirect attention to get back on task.  The automated teaching assistant will also prioritize tasks for the teacher; e.g., which students to visit in the classroom.  As part of this process, new sensors will be added to FACT that will monitor the teacher's speech and location. The potential broader impact includes facilitating teachers in conducting more pedagogically complex and effective lessons and increasing teacher job satisfaction while improving student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1744118","EAGER: Integration of Heat Pipes in Gas Turbines using Artificial Intelligence and Additive Manufacturing","CBET","TTP-Thermal Transport Process","05/15/2018","06/05/2018","Amir Faghri","CT","University of Connecticut","Standard Grant","Ying Sun","04/30/2021","$149,013.00","","faghri@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","ENG","1406","7916","$0.00","Gas turbines power almost all modern aircraft and generate about 21% of the electricity used in the United States from natural gas. However, major losses of efficiency and reliability in turbines are caused by the tremendous amount of excess heat that turbines create. Current air-cooling technology has limited success in getting rid of this excess heat. This project uses an advanced cooling technology called heat pipes, which can dissipate significantly greater amounts of heat than air cooling. Using this technology will significantly improve fuel consumption, efficiency and reliability of gas turbines resulting in significant cost savings.<br/> <br/>The project involves constructing the vane or blade interior as a heat pipe and extending it into an adjacent heat sink, thus transferring incident heat through the heat pipe to the heat sink.  This design provides an extremely high heat transfer rate and a uniform temperature along the vane due to the internal changes of the phase of the heat pipe working fluid.  Furthermore, this technology eliminates hot spots at the vane leading and trailing edges and increases the vane life by preventing thermal fatigue cracking.  There is also the possibility of requiring no bleed air from the compressor, eliminating engine performance losses resulting from the diversion of compressor discharge air.  Combined air cooling with radially rotating heat pipes is also considered for a better cooling method. The proposed heat pipe is an innovative cooling system that includes non-conventional geometries, evaporation, condensation, vapor flow, and interaction of the liquid/vapor. These complex transport processes of heat transfer and two-phase flow of a liquid metal working fluid under high centrifugal forces and accelerations are all coupled. A detailed design analysis of integrated gas turbine heat pipe vanes and blades is made using artificial intelligence and additive manufacturing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822861","Human/AI Co-Orchestration of Dynamically-Differentiated Collaborative Classrooms","IIS","STEM + Computing (STEM+C) Part, Cyberlearn & Future Learn Tech","09/01/2018","04/03/2020","Vincent Aleven","PA","Carnegie-Mellon University","Standard Grant","Kurt Thoroughman","08/31/2021","$773,995.00","Nikol Rummel","vincent.aleven@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","005Y, 8020","063Z, 8045, 9251","$0.00","This project will create and demonstrate new technology that supports dynamically-differentiated instruction for the classroom of the future. This new vision centers on carefully-designed partnerships between teachers, students, and artificial intelligence (AI). AI-powered learning software will support students during problem-solving practice, providing either individual guidance (using standard intelligent tutoring technology) or guidance so students can effectively collaborate and tutor each other. These learning activities are constantly adjusted to fit each student's needs, including switching between individual or collaborative learning. The teacher ""orchestrates"" (instigates, oversees, and regulates) this dynamic process. New tools will enhance the teacher's awareness of students' classroom progress. The goal is to have highly effective and efficient learning processes for all students, and effective ""orchestration support"" for teachers. We will implement and test this vision in the context of AI-enhanced mathematics learning in middle school. The proposed work will greatly enhance current understanding of how to design effective AI-based ""co-orchestration"" tools that draw on complementary strengths of teachers, students/peers, and AI agents to make the vision of the dynamically-differentiated classroom feasible. It will provide insight into the new classroom dynamics that arise. The work may ultimately contribute to more individualized K-12 education. The work will create a testbed that could be used to explore and rigorously test a wide range of interesting hypotheses regarding co-orchestration and dynamic differentiation of individual and collaborative learning. <br/><br/>Effective orchestration of dynamically-differentiated instruction poses significant challenges in terms of design and technical implementation. Although existing AI-based learning technologies support forms of dynamic differentiation of instruction, they tend not to support dynamic combinations of individual and collaborative learning; in fact, most only support one of these two learning modes. In addition, existing teacher support tools tend to focus only on enhancing teacher awareness, not on supporting teachers' in-the-moment decision-making and action, and not on supporting dynamic interleaving of individual and collaborative learning. In the proposed work, we tackle this challenge by integrating and extending four strands of work: intelligent tutoring systems technology; a learning environment to support combinations of individual and collaborative learning; adaptive technology support for mutual peer tutoring; and a mixed reality tool (""smart glasses"") to support teacher/AI co-orchestration. Building on this foundation, we create and demonstrate technology support for dynamically-differentiated instruction by three strands of work. First, we create AI-based tutoring software capable of supporting both individual learners and students doing mutual peer tutoring. Second, we develop a tool to support the intelligent, real-time co-orchestration, between students, teachers, and AI agents, of dynamically differentiated combinations of collaborative learning and individual learning. We do so through design-based research, prototyping, and classroom piloting. Third, we evaluate the newly-created technology for dynamically-differentiated collaborative classroom in schools, to gain an initial understanding of its benefits and challenges, and the changes in classroom practices and learning outcomes that it brings about.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833225","Belmont Forum Collaborative Research Food-Water-Energy Nexus: Intelligent Urban Metabolic Systems for Green Cities of Tomorrow: an FWE Nexus-based Approach","ICER","INTERNATIONAL COORDINATION ACT","07/01/2018","07/03/2018","Luis Rodriguez","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Maria Uhle","06/30/2021","$247,793.00","Yanfeng Ouyang, Shaowen Wang","lfr@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","GEO","1679","1679, 7313, EGCH","$0.00","Many cities across the globe are facing difficult challenges managing their food, water and energy systems. The challenges stem from the fact that the issues of food, water and energy are often tightly connected with each other, not only locally but also globally. This is known as the Food-Water-Energy (FWE) nexus.  An effective solution to a local water problem may cause new local problems with food or energy, or cause new water problems at the global level. On a local scale, it is difficult to anticipate whether solutions to one issue in the nexus are sustainable across food, water and energy systems, both at the local and the global scale.  Innovative solutions that encompass the nexus are particularly important to enable cities to better manage their food, water and energy systems and understand the benefits and tradeoffs for different solutions.  <br/><br/>This award supports U.S. researchers participating in a project competitively selected by a 29-country initiative through the joint Belmont Forum- Joint Programming Initiative (JPI) Urban Europe.  The Sustainable Urbanization Global Initiative (SUGI)/Food-Water-Energy Nexus is a multilateral initiative designed to support research projects that bring together the fragmented research and expertise across the globe to find innovative solutions to the Food-Water-Energy Nexus challenge.  The call seeks to develop more resilient, applied urban solutions to benefit a much wider range of stakeholders. The rapid urbanization of the world's population underscores the importance of this focus. International partners were invited to develop solutions for this challenge.  The funds requested will be used to support U.S. participants to cooperate in consortia that consist of partners from at least three of the participating countries and that bring together natural scientists, social scientists and research users (e.g., civil society, NGOs, and industry).  Participants from other countries are funded through their national funding organizations. <br/><br/>This project aims to identify the key factors of urban metabolism and their complex interactions from FWE Nexus perspective and to improve the cost-benefit for FWE consumption by optimizing food-water-energy reallocations. This project will identify critical FWE factors and define critical pathways of food-water-energy delivery to urban centers using advanced tools such as Artificial Intelligence, data mining, system dynamics modelling, agro-logistics and scenario analysis.  The project seeks to understand the intertwined nature of food-water-energy in terms of their lifecycles, including production, processing, delivery, consumption, and disposal. The primary outcome will be the development of the intelligent urban metabolic systems for cities with challenges for green urban centers of tomorrow.  The comprehensive solutions will provide stakeholders and decision makers with information that can be used to develop strategies to help sustain the inevitable trends of urbanization by effectively managing the food-water-energy nexus.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810402","Spatially resolve three-dimensional tactile sensing using functionally graded piezoresistive pillar arrays","ECCS","EPMD-ElectrnPhoton&MagnDevices","08/01/2018","07/31/2018","Burak Aksak","TX","Texas Tech University","Standard Grant","Usha Varshney","07/31/2021","$287,659.00","Richard Gale","burak.aksak@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","ENG","1517","8028","$0.00","Despite much advancement in materials, computation power, actuators, sensors, and design, robots drastically lag in their ability to match humans in dexterous manipulation. Studies that show success in robotic manipulation suffer from complex sensing schemes and extensive post processing steps because they utilize tactile sensors that are not capable of human-like high spatial resolution or contact force magnitude and direction detection. Artificial skin-like flexible tactile sensors that can resemble the tactile sensing capabilities of their biological analogue are bound to revolutionize robotics, providing unprecedented control and dexterity, and support the recent advancements in artificial intelligence and humanoid robotics. This work addresses the pressing need for skin-like distributed tactile sensors and proposes a novel tactile sensor of flexible construction, which can resolve dynamic contact forces with fingertip-like high spatial resolution. The proposed work is transformative in its ability to equip robotic manipulators with tactile feedback comparable to that of humans, paving the way to human-like dexterity in robotics. This innovative technology has the potential to be a significant step toward the realization of corobots living and working with humans. This project complements the educational activities in biomimetic engineering and entrepreneurial awareness, giving undergraduate and graduate students the opportunity to be involved in cutting-edge research and gain skills in innovative thinking and entrepreneurship. Educational outreach activities to introduce and promote engineering to K-12 students and underrepresented groups will be an integral part of this project.   <br/><br/>The goal of this work is to enable spatially resolve three-dimensional contact force imaging and provide skin-like touch sensing capabilities (namely, local three-dimensional dynamic force sensing) to robotic platforms and facilitate human-like dexterous manipulation in robotic manipulators. The PI will achieve this goal by fabricating a novel array-type tactile sensor comprising a fibrillar polymeric contact layer which amplifies contact forces at the integrated piezoresistive base sensing layer and a flexible substrate with integrated electrodes. Preliminary experiments have demonstrated composite piezoresistors with pressure sensitivity close to that of a human fingertip. The objectives of the proposed work are to  (i) design a composite microfibrillar sensor array, based on piezoresistive sensing, which would be flexible, cheap, and durable; (ii) develop repeatable and scalable fabrication techniques; (iii) study the underlying physics for composite fiber sensing using micro-and-mesoscale characterization techniques; and (iv) study and demonstrate friction characterization, slip detection, and slip prevention using custom characterization tools.  The long-term scientific goal is to understand and quantify the relationship between three dimensional spatio-temporal contact force images and manipulation to advance robotics as well as its medical and biological applications. If successful, this project, in addition to providing unprecedented control and dexterity in robots, will support the recent advancements in other important areas of robotics, for example in artificial intelligence and humanoid robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832091","WORKSHOP: Doctoral Consortium at AAAI Conference on Human Computation & Crowdsourcing (HCOMP 2018)","IIS","HCC-Human-Centered Computing","04/01/2018","03/21/2018","Carsten Eickhoff","RI","Brown University","Standard Grant","Ephraim Glinert","03/31/2019","$24,009.00","","carsten_eickhoff@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7367","7367, 7556","$0.00","This grant supports participation of 8 promising doctoral students from U.S.-based educational institutions in a Doctoral Consortium (workshop), along with about 6 distinguished research faculty, to be held in conjunction with the 6th AAAI Conference on Human Computation & Crowdsourcing (HCOMP 2018), which will take place July 5-8 2018, in Zurich, Switzerland, and which is sponsored by the Association for the Advancement of Artificial Intelligence.  HCOMP is the premier venue for disseminating the latest research findings on crowdsourcing and human computation.  While artificial intelligence (AI) and human-computer interaction (HCI) represent traditional mainstays of this cross-disciplinary conference, HCOMP believes strongly in inviting, fostering, and promoting broad, interdisciplinary research.  More information about the conference is available online at http://www.humancomputation.com/2018/.  The Doctoral Consortium will be a research-focused full-day meeting immediately following the conference technical program on July 8.  It will enhance the scientific workforce in this emerging research area by nurturing a group of promising young investigators interested in human computation and crowdsourcing, allowing them: to attend the conference; to learn of potential career paths within academia and industry; to access an international network of researchers who can support their professional development; and to see firsthand the interdisciplinary nature, diversity and interrelationships of research in human computation.  The faculty mentors also will serve as the review committee for student applications. The organizers will promote diversity by selecting no more than one or two students from any one school, and by prioritizing the selection of women and underrepresented minorities.  <br/><br/>The full-day Doctoral Consortium will include activities to guide the research of these promising young researchers. The Consortium will allow participants to interact with established researchers and with other students, through presentations, question-answer sessions, panel discussions, and invited presentations. Each participant will give a short presentation on their research and will receive feedback from at least one faculty mentor and from fellow students. The feedback will be geared toward helping the student participants understand and articulate how their research is positioned relative to other work on human computation and crowdsourcing.  Activities led by the faculty will include a panel discussion to give students more information about the process and lessons of research and life in academia and industry. To further integrate the Doctoral Consortium participants into the conference itself, students will have a chance to present their work in an interactive poster session, and their papers will be posted online on the workshop webpage. These activities will benefit the participants by offering each fresh perspectives and comments on their work from researchers outside their own institution, both from faculty and other students; providing a supportive setting for mutual feedback on participants' current research and guidance on future research directions; and enabling participants to form a cohort of new researchers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813223","RI: Small: Concept Formation in Partially Observable Domains","IIS","Robust Intelligence","09/01/2018","08/30/2018","Cynthia Matuszek","MD","University of Maryland Baltimore County","Standard Grant","Roger Mailler","08/31/2021","$399,993.00","","cmat@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7495","7495, 7923","$0.00","This research focuses on providing artificial intelligence (AI) systems with ways to represent knowledge about a problem domain, by creating descriptions of its observations over time.  This work is important because the AI systems can transfer their knowledge from one problem domain to another, enabling them to learn complex behaviors in different environments over time.  In addition, the learned representation provides a basis for creating explanations of the agent's behavior, a capability that is becoming increasingly important as AI agents are being applied to more aspects of our daily lives.  The resulting learning transfer methods we will create are applicable to a wide variety of problems of interest to the broader AI community, including explainable systems, intelligent wearable computing, and robotic assistants in real world environments.<br/><br/>The agents enabled by this work will automatically extract concepts (high-level descriptors) from perceptions, construct a hierarchy of experiences, and record learned behaviors over this structure, by extending existing reinforcement learning methods with these novel representations. Concepts serve as simple, portable, efficient packets of hierarchical knowledge that can be learned in parallel.  Our novel contribution, concept-based memory, extends previous work on concept formation to identify useful properties of the domain that are not directly observable in all contexts, expanding the agent's world model and improving performance in partially observable domains.  Concept-based memory provides a process for creating multi-layered abstract representations of a domain and the tasks in the domain, enabling learning transfer across multiple tasks, and providing a basis for creating explanations of learned behaviors. Our method for concept formation in reinforcement learning domains, called concept-aware feature extraction (CAFE), produces concept-lattice representations that permit knowledge learned from one task to be applied to a new problem by identifying the appropriate level of generalization for common knowledge between the tasks.  We will enable scalability by integrating CAFE with abstract Markov decision processes (AMDPs) and by developing heuristic pruning methods that reduce the branching factor of the concept lattices<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837212","CPS: Medium: LEAR-CPS: Low-Energy computing for Autonomous mobile Robotic CPS via Co-Design of Algorithms and Integrated Circuits","CNS","CPS-Cyber-Physical Systems","10/01/2018","09/10/2018","Sertac Karaman","MA","Massachusetts Institute of Technology","Standard Grant","Sandip Roy","09/30/2021","$1,000,000.00","Vivienne Sze","sertac@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7918","7924","$0.00","The goal of this research is to enable a new era of low-energy mobile robotic Cyber-Physical Systems (CPS). The approach is the simultaneous design of the computing hardware with the computer algorithms, with input from the physics of the system. Applications include, but are not limited to, insect-size robotic bees for artificial pollination, robotic water striders for environmental monitoring, miniature underwater autonomous vehicles for inspection, orally-administered medical robotic vehicles that can intelligently navigate the digestive system, robotic gliders that can operate in the air or underwater for months at a time, and many more. The results will enable low-power computing for artificial intelligence and autonomy to complement the existing low-energy, miniature actuation and sensing systems that have already been developed. This will enable  low-energy, miniature mobile robotic CPSs that can still provide provable guarantees on completeness, optimality, robustness and safety. <br/><br/>This project will focus on the development of novel algorithms and novel computing hardware for miniature, energy-efficient mobile robotic CPS. The proposed research will enable low-energy computation for full autonomy by way of minimizing energy consumption during design time and run time, by simultaneously designing the algorithms and the computing hardware. Decision making algorithms will minimize computing energy during run time, for instance, by considering motions that may not require heavy computation for perception and planning. The project will demonstrate the new methods by constructing the smallest fully-autonomous aerial robotic vehicle ever built. We believe the proposed foundational research and the proposed demonstration will kickstart a new cyber-physical systems subfield at the intersection of the mobile robotics literature and the computing hardware (circuits) literature.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833317","Open Compass: Leveraging the Compass AI Engineering Testbed to Accelerate Open Research","OAC","XD-Extreme Digital","05/01/2018","04/25/2018","Paola Buitrago","PA","Carnegie-Mellon University","Standard Grant","Robert Chadduck","04/30/2021","$300,000.00","Nicholas Nystrom","paola@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7476","7916","$0.00","Artificial intelligence (AI) has immense potential to contribute to advances spanning progress in science, the national health, prosperity and welfare, education, benefit society, or secure the nation's defense. Research initiatives, conferences, investments, and products based on AI abound and are expanding rapidly, while the methods, performance, and understanding of AI are in their infancy. Researchers face vexing issues such as how to improve performance, transferability, reliability, comprehensibility, and how better to train AI models with only limited data. Future progress depends on advances in hardware accelerators, software frameworks, system architectures, and creating cross-cutting expertise between scientific and AI domains. One way to accelerate progress on these topics is to create an engineering testbed, which provides a controlled environment that allows investigators to explore solutions to these - and other - challenges.  Open Compass is an exploratory research project to conduct academic pilot projects on an advanced engineering testbed for artificial intelligence, culminating in the development and publication of best practices. <br/><br/>Open Compass will: 1) engage pilot projects and research groups, documenting approaches, experiences, and lessons learned; 2) conduct training events, in-person and using the Pittsburgh Supercomputing Center's wide area classroom; 3) organize and conduct a workshop focusing on advanced AI technologies; 4) integrate experiences gained through open research collaboration with those of industry experiences to identify a comprehensive set of best practices; and 5) publish results and best practices in peer-reviewed journals, conferences, and technical reports. The broad community will benefit from publication of research results, experiences, and a knowledge base of best practices. The research community will gain access to new technologies on which to develop algorithms and applications, along with insight across new fields of those technologies' applicability and important trends for AI. Open Compass will promote workforce development through student involvement in pilot projects and training and provide feedback to industry for to enable more efficient future AI technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821666","SaTC: EDU: Microlessons to Build Readiness in the Cybersecurity Workforce","DGE","Secure &Trustworthy Cyberspace","09/01/2018","08/17/2018","Louise Yarnall","CA","SRI International","Standard Grant","Li Yang","08/31/2021","$300,000.00","Grit Denker","louise.yarnall@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","EHR","8060","025Z, 7254, 7434, 9102, 9178, 9179, SMET","$0.00","Cyber threats to U.S. businesses and governments challenge both national security and industry competitiveness. Better solutions are needed for training the cybersecurity professionals who protect these networks, particularly the mid-career professionals who lack the time and resources to take courses. This research will develop and test the feasibility of using a smartphone application to deliver concise learning content, microlessons, in a user-tailored, coherent, and flexible way to support workplace learning about new cyber threats and ethical hacking procedures. This work advances learning scientists' understanding about the curriculum design processes required to convert long-form, technical lesson content into brief lessons and quizzes designed for self-directed adult learners. It also contributes to computer scientists' methods of using artificial intelligence to track learners' progress and make recommendations on the next steps in the learning process. <br/><br/>The research will have three main phases. First, researchers will select and transform existing online cybersecurity learning materials used in community college certification programs into a collection of 1- to 4-minute microlessons. The researchers will organize the content, ensure it is consistent with industry standards, and provide in-app badges. Researchers will also create set of intelligent lesson recommendations that guide learners through the reasoning that hackers use to stage attacks and that defenders use to detect, diagnose, monitor, and mitigate attacks. Lesson recommendations will adjust according to each learner's needs, existing knowledge, and past experience. Second, the research will deliver a test set of microlesson sequences on a smartphone platform called PERLS (Pervasive Learning System). This initial set of lessons will include between 200 and 350 different microlessons, quizzes, self-reflections on learning, and guided hands-on experiences with cybersecurity tools, as well as intelligent, personalized content recommendations. Third, the research will culminate with a pilot test of the prototype with up to 15 working professionals seeking to update their cybersecurity skills. This pilot will track tool usage frequency and learning progress as measured through quizzes and learners' self-reflections. The results will be shared through cybersecurity education and professional associations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849112","I-Corps:  Real-Time Monitoring with Predictive Intelligence for Efficient Warehouse and Outdoor Inventory Management","IIP","I-Corps","09/15/2018","08/16/2018","David Grau","AZ","Arizona State University","Standard Grant","Andre Marshall","02/28/2021","$50,000.00","","david.grau@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project supports improving the efficiency of inventory and warehouse management operations with a cost-affordable technology innovation capable of monitoring goods in real-time without the need for static communications infrastructure. The project promises to enhance such operations through the extraction of information from the collected data and the provision of time-critical insights at a fraction of the costs of current commercial tools. The monitoring innovation will potentially become the first solution in the market capable of continuously monitoring goods in outdoor environments. In addition, the monitoring innovation will potentially become the first cost-affordable monitoring solution for low-value goods. The innovation promises a primary impact in the sectors of defense and construction. In reality, though, it offers the potential to positively impact the cost and efficiency of complex inventory and warehouse management operations across US industry sectors. This I-Corps team will cross-pollinate discovery and innovation across the broad spectrum of implementation scenarios by addressing the existing latent inefficiencies in such operations.<br/><br/>This I-Corps project explores the market potential of the real-time monitoring, predictive, and inexpensive innovation in support of inventory and warehouse management operations. A realistic real-time monitoring solution without the need for static communications infrastructure is a technically complex endeavor beyond the capabilities of commercial tracking and monitoring tools. The proposed innovation optimizes operations and information flows, and the response to upstream (push) and downstream (pull) processes and overall system performance. As large datasets are fused the artificial intelligence aspect of the innovation automatically identifies inefficiencies and bottlenecks, and reassesses forecasts and system decisions that account for latest events, data uncertainties, and multiple local optima. This I-Corps team plans to investigate the market potential of the innovation, validate customer needs and how the innovation can alleviate such needs, and discern sustainable business models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815563","NeTS: Small:  Collaborative Research: Towards Adaptive and Efficient Wireless Computing Networks","CNS","Networking Technology and Syst","10/01/2018","08/21/2018","Bin Li","RI","University of Rhode Island","Standard Grant","Alexander Sprintson","09/30/2021","$250,000.00","","binli@uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","CSE","7363","7923, 9150","$0.00","Today's mobile devices are not merely smart, they are becoming intelligent as artificial intelligence applications such as Facebook Caffe2 and Google Tensor-Flow Lite are being pushed into mobile devices and as mobiles devices are being integrated into the cloud-fog-mobile architecture. This calls for efficient and adaptive computing/communication co-design of wireless networks to optimize application-level latency (including both communication latency and computing times) and to achieve energy efficiency (considering energy consumed by both communications and computing).  This project develops fundamental theories and novel architectures of low-latency, energy-efficient, and computing-centric wireless networks to support emerging mobile intelligence applications. Theories and algorithms developed by the PIs are constantly integrated into the undergraduate and graduate courses taught at the two universities. This project also provides hand-on experiences to undergraduate and high school students with state-of-the-art wireless technologies. <br/><br/>Computing/communication co-design, while new for wireless networks, is a central topic in data center networks. However, the proposed solutions, while inspiring, are not directly applicable to wireless computing networks because of the unique features of wireless networks such as wireless interference, channel fading and limited energy. This project focuses on provably optimal mechanisms that dynamically and adaptively schedule computing tasks and data transmissions to meet application-level performance requirements, and consists of three interdependent thrusts: (i) Optimal computing/communication co-design. This thrust develops mathematical models and theoretical limits of wireless computing networks, (ii) Robust computing/communication co-design. This thrust focuses on robust computing and communication co-design that achieves desired performance with imperfect state information and under unavoidable short-term system overload and (iii) Learning-aided adaptive computing/communication co-design. This thrust further improves the performance of wireless computing networks by leveraging both historical data and predictable user behaviors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827753","PFI-RP: Design and Fabrication of Hardware-based Security Platform using Fabrication Variability of Ultra low Power Memories","IIP","GOALI-Grnt Opp Acad Lia wIndus, PFI-Partnrships for Innovation","09/15/2018","08/22/2019","Fatemeh Afghah","AZ","Northern Arizona University","Standard Grant","Jesus Soriano Molla","08/31/2021","$803,347.00","Hugh Barnaby, Michael Kozicki, Abolfazl Razi, Bertrand Cambou","fatemeh.afghah@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","ENG","1504, 1662","019Z, 1662, 8032, 8033","$0.00","The broader impact and commercial potential of this PFI project is to pave the road for commercialization of a new generation of security solutions through leveraging recent advances in low power non-silicon based memories, advanced error recovery techniques and artificial intelligence. This new methodology offers a radical shift in the field of cybersecurity through departing from pure-software based cryptographic methods toward a hybrid solution of using fabrication variability of electronic devices and advanced error correction methods. This project is a collaboration between academia and industry to enhance current security solutions used in e-commerce, banking, smart cities, supply chains, smart health, machine to machine communications and the emerging Internet of Things. In addition to the scientific impact of opening up new research opportunities for scholars (faculty, graduate and undergraduate students), R&D, and production line workforce in semiconductor and security industry, this project has a great socioeconomic impact by restoring people's trust on using advanced technology and internet based service. Making the system robust to security attacks will save the country millions of dollars noting that more than $11 million dollars is spent directly to combat cybercrime in the US with an annual raise of 22%. <br/><br/>The proposed project offers a proof-of-concept security module that uses fabrication variability of embedded memories in electronic devices to secure web-based communication protocols against hacking attacks. In contrast to the current cryptographic methods, where the keys are store in conventional memories, no security keys will be stored in the proposed security module; therefore it will be highly protected against cloning attempts to provide full security in the case of physical hijacking. Further, recently developed ultra-low power memories will be used to make the developed module secure to side channel attacks, as an additional key feature of this device. Using a novel error correction mechanism by integrating ternary state logic, artificial intelligence and recent advances in modern coding theory, a long-lasting problem of key generation mismatch is resolved by making the key generation failure rate infinitesimal.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813392","NeTS: Small: Collaborative Research: Towards Adaptive and Efficient Wireless Computing Networks","CNS","Networking Technology and Syst","10/01/2018","08/21/2018","Lei Ying","AZ","Arizona State University","Standard Grant","Monisha Ghosh","02/29/2020","$250,000.00","","leiying@umich.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7363","7923, 9150","$0.00","Today's mobile devices are not merely smart, they are becoming intelligent as artificial intelligence applications such as Facebook Caffe2 and Google Tensor-Flow Lite are being pushed into mobile devices and as mobiles devices are being integrated into the cloud-fog-mobile architecture. This calls for efficient and adaptive computing/communication co-design of wireless networks to optimize application-level latency (including both communication latency and computing times) and to achieve energy efficiency (considering energy consumed by both communications and computing).  This project develops fundamental theories and novel architectures of low-latency, energy-efficient, and computing-centric wireless networks to support emerging mobile intelligence applications. Theories and algorithms developed by the PIs are constantly integrated into the undergraduate and graduate courses taught at the two universities. This project also provides hand-on experiences to undergraduate and high school students with state-of-the-art wireless technologies. <br/><br/>Computing/communication co-design, while new for wireless networks, is a central topic in data center networks. However, the proposed solutions, while inspiring, are not directly applicable to wireless computing networks because of the unique features of wireless networks such as wireless interference, channel fading and limited energy. This project focuses on provably optimal mechanisms that dynamically and adaptively schedule computing tasks and data transmissions to meet application-level performance requirements, and consists of three interdependent thrusts: (i) Optimal computing/communication co-design. This thrust develops mathematical models and theoretical limits of wireless computing networks, (ii) Robust computing/communication co-design. This thrust focuses on robust computing and communication co-design that achieves desired performance with imperfect state information and under unavoidable short-term system overload and (iii) Learning-aided adaptive computing/communication co-design. This thrust further improves the performance of wireless computing networks by leveraging both historical data and predictable user behaviors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831755","SCC: Smart and Connected Churches for Promoting Health in Disadvantaged Populations","CNS","S&CC: Smart & Connected Commun","10/01/2018","09/09/2018","Timothy Bickmore","MA","Northeastern University","Standard Grant","Wendy Nilsen","09/30/2022","$2,092,670.00","Michael Paasche-Orlow","bickmore@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","033Y","042Z","$0.00","This project brings together researchers from several disciplines with community partners to develop a range of novel sensing, monitoring, and messaging technologies to create a Virtual Safety Net (VSN) system for community members in an underserved urban community. In this effort, researchers will collaborate with members of the Black Ministerial Alliance and Health Ministry leaders of member churches, along with volunteers who provide health promotion outreach, the church leadership and a community liaison affiliated with a hospital to improve the overall health of this predominately African American community. The VSN is a research infrastructure for development of crowdsourced health interventions, in which community members author or culturally tailor intervention rules and messages. The VSN will empower the community to collectively solve health-relevant problems it identifies as important and provide preventive and health interventions that leverage support and messaging delivered individually via smartphone apps or during group meetings. The VSN will also be used to help the community identify and address social determinants of health, including food insecurity, homelessness, and health literacy. This research will have immediate impact for the 20,000 members of the 30 churches in the Black Ministerial Alliance of Boston. The technologies implemented can be rapidly disseminated to other church communities, businesses and social organizations in the US.<br/>    <br/>This SCC project aims to develop a research infrastructure that will be used to co-design and evaluate technologies in collaboration with community partners. The project will leverage the deep social support networks already existing in the Boston area. Key technologies developed in this effort include crowdsourced health interventions, in which community members author or culturally tailor intervention rules and messages. This will include developing methods to identify the range of modifications that laypersons can meaningfully make to expert-authored intervention dialogue and designing user interfaces to enable these modifications. It also includes advances in artificial intelligence-based indexing of community-authored narrative text for just-in-time messaging to motivate change in longitudinal, multi-behavior interventions. Advances in the automated generation of persuasive arguments for behavior change will also be developed. Dialogue-based interfaces to generate explainable artificial intelligence-learned models will be made to enable community members to understand and potentially modify how these models work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823068","Workshop on Inter-Disciplinary Research Challenges in Computer Systems","CNS","CSR-Computer Systems Research","03/01/2018","03/07/2018","Xipeng Shen","NC","North Carolina State University","Standard Grant","Samee Khan","02/28/2019","$49,731.00","James Tuck","xshen5@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","7354","7556","$0.00","This award funds a visioning workshop with the goal of bringing together leading researchers and practitioners in computing systems (architecture, operating systems, languages/compilers) to reflect on the research progress in the overlapped areas of the three fields and to discuss the grand challenges for the fields so that they can rise to meet the demands of modern computing in synergy.<br/><br/>The workshop will have focused discussions in the context of four important application domains: Internet of Things (IoT)/infrastructure, augmenting Human Abilities/Artificial Intelligence, security and privacy, and complexity management/environment. It will discuss how inter-disciplinary research in computing systems could help meet the grand challenges in these domains, such as the lack of principles and techniques that reliably and efficiently support large and heterogeneous ensembles of IoT devices and humans, the safety and efficiency barriers facing the deployment of Artificial Intelligence, and the vulnerabilities in both hardware and software that threaten to undermine security and privacy.<br/><br/>The principal investigators, along with workshop participants, will produce a detailed workshop report that will outline the grand challenges and promising directions for the further development of inter-disciplinary research in computing systems. The report will be made be publicly available through the internet as well as professional journals and magazines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815598","RI:Small:Tractable Decision-Theoretic Planning Driven by Data","IIS","Robust Intelligence","07/01/2018","06/25/2018","Prashant Doshi","GA","University of Georgia Research Foundation Inc","Standard Grant","Roger Mailler","06/30/2021","$466,514.00","","pdoshi@cs.uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","CSE","7495","7495, 7923","$0.00","Automated planning is about finding a sequence of actions that is expected to successfully complete the task at hand. Decision-theoretic planning approaches automated planning as a sequence of decisions, each of which optimizes the planner's combined immediate and longer-term preferences. This approach to automated planning allows for realistic actions whose outcomes are often uncertain and reasons with the planner's possibly inexact preferences in addition to precise goals. However, decision-theoretic planning relies on an accurate specification of the planning problem, which is often impractical and is computationally very costly. This research is addressing these challenges by investigating a new and meaningful planning problem representation that is learned directly from data, which alleviates the need for tedious specifications. The representation is designed to yield more efficient computation of solutions. Consequently, this research has the potential to transition automated planning to large pragmatic applications, such as in flow routing in high-density computer networks, which will be demonstrated in this project. The project will train graduate students for entering the workforce in an important area of artificial intelligence, and it will facilitate an international research collaboration between researchers in US and Canada. The PI will use the outcomes of this research to inform his classroom instruction, which will provide students with exposure to how automated planning can be useful to the society.      <br/><br/>The technical approach is merging two threads of previous progress toward developing a new graphical model called the dynamic sum-product-max network. In these previous threads, the PI generalized sum-product networks, which allow efficient probabilistic inference, in two directions. First, along the temporal dimension thereby allowing inference over a sequence of variables, and second, enabling efficient non-sequential decision making by including decision and utility variables. This research is reconciling the fundamental hardness of decision-theoretic planning with the efficiency of dynamic sum-product-max networks by studying which class of planning problems can be compactly represented by the new model. As these models can be directly learned from data, the research is also establishing the appropriate schema for the data and creating an evaluation testbed of datasets. A final thrust is developing a portfolio of methods for automatically learning both the structure and parameters of dynamic sum-product-max networks from appropriate data, with a focus on learning valid models. The research plan is expected to yield a new graphical representation and associated methods that allow efficient data-driven planning whose utility will be demonstrated by real-world applications in collaboration with industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822816","Collaborative Research: CSEdPad: Investigating and Scaffolding Students' Mental Models during Computer Programming Tasks to Improve Learning, Engagement, and Retention","IIS","S-STEM-Schlr Sci Tech Eng&Math, ECR-EHR Core Research, Cyberlearn & Future Learn Tech","09/01/2018","07/31/2018","Vasile Rus","TN","University of Memphis","Standard Grant","Kurt Thoroughman","08/31/2021","$499,136.00","Scott Fleming","vrus@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","005y, 1536, 7980, 8020","063Z, 8045","$0.00","Computing skills, such as computer programming, are an integral part of many disciplines, including the fields of science, technology, engineering, and math (STEM). Although such skills are in high-demand, and the number of aspiring Computer Science (CS) students is encouraging, a large gap between the supply of CS graduates and the demand persists because, for instance, college CS programs suffer from high attrition rates in introductory CS courses. One reason for the high attrition rates in introductory CS courses is the inherent complexity of CS concepts and tasks. To help students better cope with the high level of complexity, this project investigates a novel education technology, called CSEdPad (CS Education Pad), meant to ease students' introduction to programming during their early encounters with CS concepts and tasks. Moreover, the project forges new frontiers in CS education through a research program that advances our understanding of students' source code comprehension, learning, and motivational processes. The CSEdPad project has the potential to transform how students perceive computer science, increase their programming skills and self-efficacy, and lead to increased retention rates. The result will be a win-win-win situation for aspiring students, CS programs and their organizations, and the overall economy.<br/><br/>The CSEdPad system design brings to bear proven educational technologies and techniques to improve students' mental model construction, learning, engagement, and retention in CS education. In particular, the system targets source code comprehension, a critical skill for both learners and professionals. It monitors and scaffolds source code comprehension processes while students engage in a variety of code comprehension tasks. Key approaches being explored include Animated Pedagogical Agents, self-explanation, and the Open Social Learner Model. Outcome variables include comprehension measures, learning gains, engagement level, retention, and self-efficacy. Due to its interdisciplinary nature, the project will impact several fields including Computer Science education, cognitive psychology, intelligent tutoring systems, and artificial intelligence. Students participating in the experiments will be selected from a diverse student body with respect to gender, ethnicity, and socioeconomic status. An increase in recruitment and retention of students from these populations will have far-reaching implications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822181","2nd Summer School on Cognitive Robotics: Proposed Summer School","IIS","Robust Intelligence","07/01/2018","06/27/2018","Brian Williams","MA","Massachusetts Institute of Technology","Standard Grant","David Miller","06/30/2019","$25,000.00","","williams@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","7495, 7556","$0.00","This grant supports a week-long summer school on Cognitive Robotics to be held in Boston, Ma in July 2018. The summer school will be a combination of invited talks and tutorials, which are designed to introduce researchers to issues in planning and execution from perspectives of both Robotics and Cognitive Artificial Intelligence, and daily labs, which are designed to give students hands-on experience with robotic hardware and state-of-the-art software tools for developing robotic behaviors. The summer school will help expose graduate students to cutting-edge ideas at the intersection of Robotics and Cognitive Systems and will help to form a new community of researchers in this interdisciplinary area.  The hands-on experience with, and open-source release of, the Enterprise Executive Architecture will facilitate the formation of this collaborative community.  The summer school concludes with a grand challenge using robot hardware that combines what the students have learned during the week.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813490","RI: Small: Adaptive Metareasoning for Bounded Rational Agents","IIS","Robust Intelligence","08/01/2018","07/23/2018","Shlomo Zilberstein","MA","University of Massachusetts Amherst","Standard Grant","James Donlon","07/31/2021","$404,722.00","","shlomo@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7495","075Z, 7495, 7923","$0.00","Metareasoning is the process by which an intelligent agent monitors and controls its own thought processes so as to produce effective action in a timely manner.  Just as people must decide when to stop thinking and take action, AI systems also need to be able to interrupt their decision-making process and commit to an action or plan.  While people often use heuristic methods to determine the interruption time, this project offers metareasoning techniques that optimize the value of computation and stop planning when the urgency to take action outweighs the anticipated benefit of continued computation.  The project transforms the ability of researchers and practitioners to create responsive planning systems by offering easy-to-use, off-the-shelf adaptive metareasoning techniques to control them.  Additional areas of broader impact include mentoring of student researchers with special attention to underrepresented groups, a range of outreach activities to local schools, targeted activities to increase diversity in computer science, and industrial collaborations.<br/><br/>The approach uses planning algorithms that can be interrupted at any time, offering a tradeoff between runtime and quality of results.  To take advantage of this tradeoff, novel metareasoning techniques are developed that overcome the drawbacks of existing methods.  The key idea is to replace the reliance on extensive offline experiments by creating new ways to predict performance and adapt the prediction quickly to the specific problem instance at hand.  The project answers fundamental questions about the feasibility, efficiency, and scalability of optimizing meta-level control with minimal computational overhead.  The main contributions are: (1) online performance prediction methods for efficient meta-level control of anytime algorithms that outperform state-of-the-art methods; (2) a novel approach to create and adapt meta-level control policies online using reinforcement learning techniques; (3) extensions of the above methods to control a portfolio of anytime algorithms, allowing transitions from one algorithm to another using shared intermediate solution representations; and (4) extensions of the above methods to control the internal operation of adjustable anytime algorithms.  The team evaluates the new metareasoning techniques on complex computational tasks using a range of anytime algorithms based on different programming paradigms and demonstrates ease of use and significant performance gains relative to existing metareasoning techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816012","CHS: Small: Deep Integration of Crowds and AI for Robust, Scalable, and Privacy-Preserving Conversational Assistance","IIS","HCC-Human-Centered Computing","08/15/2018","03/25/2020","Jeffrey Bigham","PA","Carnegie-Mellon University","Standard Grant","William Bainbridge","07/31/2021","$532,000.00","","jbigham@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","075Z, 7367, 7923, 9251","$0.00","This research will use the recently deployed crowd-powered conversational assistant, Chorus, as a scaffold to develop technical components that allow it to automate itself over time. Chorus introduces a hybrid intelligence model in which humans and machines collaboratively power a single intelligent system. This is unlike both fully-automated approaches which are limited in terms of the domains they cover, and individual human-based conversational support which does not scale.  Conversation is interactive communication. When people converse with one another, they build and refine a shared context that makes finding and making sense of information efficient and more effective. Computers capable of engaging users in natural conversations about arbitrary topics would revolutionize how, when, and where people have access to information. Despite many successes, computers are still far from being able to converse naturally across general domains. Systems resulting from this research will be robust enough and scalable enough to be used in real world domains. These types of hybrid systems may lead to new, generally applicable models that are useful in real-time human computation and natural language understanding. This work will inform a better understanding of how automated agents can learn from crowd-powered systems in order to gradually assume more responsibility over time.<br/><br/>Creating a robust, general-purpose dialog system from the bottom up is difficult because it requires solving multiple hard problems at once. This project employs a complementary top-down approach that will (1) use the growing Chorus data set to train automatic responders, (2) facilitate integration of existing task-specific dialog systems, (3) develop learning systems to sample among integrated dialog systems and choose the best to respond, (4) develop learning systems to choose the best responses from among automated and human suggestions, (5) develop learning systems able to recommend relevant elements from the user's history based on context, (6) develop crowd-powered systems for allowing users to safely control their devices, and (7) develop crowd-powered systems that allow users to safely access private repositories such as their email. Integral to this work is the interplay between computers and people.  Central goals are to better understand how computers and people can complement the work of one another; learn how people can teach computers to be better in the difficult domain of robust dialog, and develop novel approaches for applying human computation when the crowd is handling confidential information or has control of a physical device such as a user's mobile phone. Lessons learned from exploring the top-down approach of introducing a crowd-powered conversational agent that is gradually replaced by automation may apply generally to other hard problems. This approach may allow research topics to be explored before successful computational approaches have been developed for foundational problems, such as learning how to properly curate persistent memory before having the ability to create reliable conversational assistants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755593","CRII: CHS: Predicting When, Why, and How Multiple People Will Disagree when Answering a Visual Question","IIS","HCC-Human-Centered Computing","05/01/2018","03/16/2018","Danna Gurari","TX","University of Texas at Austin","Standard Grant","Ephraim Glinert","04/30/2021","$174,947.00","","danna.gurari@ischool.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7367","7367, 8228","$0.00","The goal of a visual question answering (VQA) system is to empower people to find the answer to any question about any image.  For example, a VQA system could enable blind people to address daily visual challenges such as learning whether a pair of socks match or learning what type of food is in a can.  VQA services could also facilitate the creation of smarter environments, say to monitor how many defective products are on a factory assembly line at any given time.  A limitation of existing VQA systems is that they do not account for the fact that a visual question may elicit different answers from different people.  VQA systems could save time and reduce user frustration if they empowered users to anticipate and resolve any answer disagreements that may arise.  Blind and sighted people could more rapidly and accurately learn about the diversity of human perspectives on the visual world.  VQA services also could teach people how to ask visual questions that elicit the desired answer diversity.<br/><br/>This project will create artificial intelligence (AI) models that can account for the possible diversity of answers inherent in crowd intelligence.  Specifically, AI models will be designed to predict when, why, and how human answer disagreement occurs, which in turn will enable new designs for human-computer partnerships.  This is challenging because it necessitates designing frameworks that simultaneously model and synthesize different and potentially conflicting perceptions of images and language for the many possible causes of disagreement.  To ensure that the AI models generalize across a broad range of applications, an existing corpus of over one million visual questions asked by blind and sighted people will be used to create annotated datasets that indicate when, why, and how much answer disagreement arises.  Methods will then be developed for automatically predicting directly from a visual question how much answer diversity will arise from a crowd, and why disagreement arises when it does.  Finally, a system will be designed for guiding visually-impaired users to more quickly formulate visual questions so they can receive a single, unambiguous crowd response (e.g., guide the person to better frame the visual content of interest with a mobile phone camera).  User studies with blind users will be conducted to empirically test the efficacy of the new system, with a focus on uncovering human-based issues in real-world, real-time situations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822752","Collaborative Research: CSEdPad: Investigating and Scaffolding Students' Mental Models during Computer Programming Tasks to Improve Learning, Engagement, and Retention","IIS","Cyberlearn & Future Learn Tech","09/01/2018","07/31/2018","Peter Brusilovsky","PA","University of Pittsburgh","Standard Grant","Kurt Thoroughman","08/31/2021","$250,519.00","","peterb@mail.sis.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","8020","063Z, 8045","$0.00","Computing skills, such as computer programming, are an integral part of many disciplines, including the fields of science, technology, engineering, and math (STEM). Although such skills are in high-demand, and the number of aspiring Computer Science (CS) students is encouraging, a large gap between the supply of CS graduates and the demand persists because, for instance, college CS programs suffer from high attrition rates in introductory CS courses. One reason for the high attrition rates in introductory CS courses is the inherent complexity of CS concepts and tasks. To help students better cope with the high level of complexity, this project investigates a novel education technology, called CSEdPad (CS Education Pad), meant to ease students' introduction to programming during their early encounters with CS concepts and tasks. Moreover, the project forges new frontiers in CS education through a research program that advances our understanding of students' source code comprehension, learning, and motivational processes. The CSEdPad project has the potential to transform how students perceive computer science, increase their programming skills and self-efficacy, and lead to increased retention rates. The result will be a win-win-win situation for aspiring students, CS programs and their organizations, and the overall economy.<br/><br/>The CSEdPad system design brings to bear proven educational technologies and techniques to improve students' mental model construction, learning, engagement, and retention in CS education. In particular, the system targets source code comprehension, a critical skill for both learners and professionals. It monitors and scaffolds source code comprehension processes while students engage in a variety of code comprehension tasks. Key approaches being explored include Animated Pedagogical Agents, self-explanation, and the Open Social Learner Model. Outcome variables include comprehension measures, learning gains, engagement level, retention, and self-efficacy. Due to its interdisciplinary nature, the project will impact several fields including Computer Science education, cognitive psychology, intelligent tutoring systems, and artificial intelligence. Students participating in the experiments will be selected from a diverse student body with respect to gender, ethnicity, and socioeconomic status. An increase in recruitment and retention of students from these populations will have far-reaching implications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820415","SBIR Phase I:  Virtual Music helper, next-generation educational social platform for conservatory music practice and performance empowered by AI and AR","IIP","SBIR Phase I","07/01/2018","06/19/2018","Melody Fallah-Khair","CA","eBibelot Inc","Standard Grant","Rajesh Mehta","12/31/2018","$225,000.00","","melody@ebibelot.com","11955 Walbrook Dr.","saratoga","CA","950703450","6507049457","ENG","5371","5371, 8031, 8032","$0.00","This SBIR Phase I project focuses on development of a prototype for a Virtual Music Helper (VMH) for children. Research confirms that music and music learning enhances intelligence, learning and IQ specially in kids. Even children with attention deficit/ hyperactivity disorder benefit from listening to music beforehand in mathematics tests. Children who learn or practice music have shown better results in STEM subjects. However, studies also suggest children opt out of music class based on false belief that they lack musical ability and many kids often find the music practice non-engaging, and boring eventually losing interest and quitting. Parents find it challenging to entice the kids to sit down and practice or they cannot help their kids because either they don?t have time or are not music savvy themselves to help them. The virtual music helper can read the kids? music homework, guide them through the exercise and provide immediate correction. This helper is not to replace teachers, but help the parents at home and encourage kids to practice more. In broader terms this project may spur significant research in finding effective methods for tutoring kids in subjects other than music such as math or language, or for kids with special needs.<br/><br/><br/>This SBIR Phase I project proposes a Virtual Music Helper (VMH) that is empowered by Artificial Intelligence and Augmented Reality for kids. VMH is a 3D mobile-virtual-human (avatar) that can be personalized for each kid and conduct live dialog with kids both from visual and behavioral perspectives. VHM offers smart content for teachers about each student's progress and weak points. The technical challenges this project will address is the detection and correction of mistakes kids make including polyphonic pitch detection from acoustic instrument, conversion of note sheets to machine readable music format with high accuracy and speed, empowering the avatar with decision making algorithms to provide proper correction and feedback verbally and visually. This platform is also intended to support multiple popular instruments and multiple languages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844589","NCS-FO: Collaborative Research: Developing Underwater EEG Electrodes for Octopus Research","BCS","IntgStrat Undst Neurl&Cogn Sys","09/01/2018","08/08/2018","Peter Tse","NH","Dartmouth College","Standard Grant","Kurt Thoroughman","08/31/2020","$159,999.00","","Peter.Tse@dartmouth.edu","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551421","6036463007","SBE","8624","7916, 8089, 8091, 8551, 9150","$0.00","The octopus is a social animal, with high intelligence and problem-solving skills, that is very distant from humans in terms of its evolution.  This project aims to fabricate neuroelectric sensors and experimental protocols that would enable studying visual and higher level cognitive processes in the octopus while they are engaged in natural behaviors in an underwater environment. This will necessitate development of new engineering solutions for crafting electroencephalography (EEG) sensors that can record signal underwater, new solutions for removing noise artifacts from these highly complicated recordings, as well as careful design of experiments that could study such behaviors in a virtual-reality environment. While the brain of the octopus is very different from that of the human, it does support well-defined cognitive functions. Therefore, understanding whether and how octopuses' brains implement processes such as learning, attention, habituation, and surprise can produce new and important understandings of how neurobiological systems can support function.  This research might reveal that the neural substrates of cognitive function in the octopus are organized according to principles that differ drastically from those found in in humans.<br/><br/>This EAGER project has several aims. It will develop the first underwater EEG, first testing well-validated paradigms on humans performing task underwater and benchmarking against known waveforms. The electrodes will be constructed so that they do not corrode in salt water. It will also develop high-quality virtual reality stimulation that could impact octopuses' behavior in an underwater environment. It will utilize EEG frequency-tagging techniques to determine processing of environmental stimulus by the octopus. This will allow studying whether octopuses present characteristic responses that are analogous to surprise, adaptation, working memory and attention effects (in primates and other vertebrates). The study will also allow answering how and in what manner do octopuses sleep. All data, artifacts and modeling software will be made publicly available and constitute an important resource for the community. The results of this study could impact our general understanding of how brains support complex cognitive functions, with direct relevance to artificial intelligence efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816363","CHS: Small: Optimizing Human-Machine Performance via Neurofeedback and Adaptive Autonomy","IIS","HCC-Human-Centered Computing","09/01/2018","08/14/2018","Paul Sajda","NY","Columbia University","Standard Grant","Ephraim Glinert","08/31/2021","$498,785.00","","ps629@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7367","7367, 7923, 8089, 8091","$0.00","Our society is being fundamentally transformed by increased interaction between humans and autonomous artificial intelligence (AI) systems. However, the addition of autonomy to our lives will not be successful unless we understand how smart machines and humans should best interact and communicate. Human-machine communication today is almost entirely linguistic, using spoken language for systems such as Siri or Alexa, or typed text for chatbots.  However, humans communicate extremely efficiently with each other byu sing much more than just words; for example by being sensitive to facial expression, gestures, gait, and intonation. In fact, great teams, whether sports teams or military combat teams, are excellent at predicting teammates behavior and state of mind. In this project, the investigators consider both basic science and technology questions with respect to how to communicate that cognitive and physiological state of a human that is co-operating with an autonomous AI. The project has very broad implications since it addresses fundamental questions related to the interactions between humans and smart machines.<br/><br/>The project investigates the  hypothesis that adaptive autonomy together with coordinated neurofeedback can be employed in the same system to optimize human-machine performance. Investigators will develop a framework and investigate the hypothesis within the context of boundary avoidance tasks, or BAT, which  is a class of tasks in which  task critical boundaries surround the optimal operating point of the control system. These tasks are particularly interesting when considering human control because they typically result in a positive feedback loop that systematically increases the arousal state of the human subject, resulting in increasingly poor task performance and ultimate task failure, consistent with the Yerkes-Dodson law. Our framework uses a brain-computer interface (BCI) to both engage autonomy as well as being a source for neurofeedback that can shift human subjects to their performance 'sweet-spot'. This project will advance the science and technology development of how human-machine systems can be optimally integrated, specifically when both 1) the machine has access to ongoing changes in human cognitive and physiological state during performance of the task and 2) the human is made aware of their own state via appropriate neurofeedback.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845928","NCS-FO: Collaborative Research: Developing Underwater EEG Electrodes for Octopus Research","BCS","IntgStrat Undst Neurl&Cogn Sys","09/01/2018","08/08/2018","Walter Besio","RI","University of Rhode Island","Standard Grant","Kurt Thoroughman","08/31/2021","$100,000.00","","besio@uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","SBE","8624","7916, 8089, 8091, 8551, 9150","$0.00","The octopus is a social animal, with high intelligence and problem-solving skills, that is very distant from humans in terms of its evolution.  This project aims to fabricate neuroelectric sensors and experimental protocols that would enable studying visual and higher level cognitive processes in the octopus while they are engaged in natural behaviors in an underwater environment. This will necessitate development of new engineering solutions for crafting electroencephalography (EEG) sensors that can record signal underwater, new solutions for removing noise artifacts from these highly complicated recordings, as well as careful design of experiments that could study such behaviors in a virtual-reality environment. While the brain of the octopus is very different from that of the human, it does support well-defined cognitive functions. Therefore, understanding whether and how octopuses' brains implement processes such as learning, attention, habituation, and surprise can produce new and important understandings of how neurobiological systems can support function.  This research might reveal that the neural substrates of cognitive function in the octopus are organized according to principles that differ drastically from those found in in humans.<br/><br/>This EAGER project has several aims. It will develop the first underwater EEG, first testing well-validated paradigms on humans performing task underwater and benchmarking against known waveforms. The electrodes will be constructed so that they do not corrode in salt water. It will also develop high-quality virtual reality stimulation that could impact octopuses' behavior in an underwater environment. It will utilize EEG frequency-tagging techniques to determine processing of environmental stimulus by the octopus. This will allow studying whether octopuses present characteristic responses that are analogous to surprise, adaptation, working memory and attention effects (in primates and other vertebrates). The study will also allow answering how and in what manner do octopuses sleep. All data, artifacts and modeling software will be made publicly available and constitute an important resource for the community. The results of this study could impact our general understanding of how brains support complex cognitive functions, with direct relevance to artificial intelligence efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845123","NCS-FO: Collaborative Research: Developing Underwater EEG Electrodes for Octopus Research","BCS","IntgStrat Undst Neurl&Cogn Sys","09/01/2018","08/08/2018","Gideon Caplovitz","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Kurt Thoroughman","08/31/2020","$39,884.00","","gcaplovitz@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","SBE","8624","7916, 8089, 8091, 8551, 9150","$0.00","The octopus is a social animal, with high intelligence and problem-solving skills, that is very distant from humans in terms of its evolution.  This project aims to fabricate neuroelectric sensors and experimental protocols that would enable studying visual and higher level cognitive processes in the octopus while they are engaged in natural behaviors in an underwater environment. This will necessitate development of new engineering solutions for crafting electroencephalography (EEG) sensors that can record signal underwater, new solutions for removing noise artifacts from these highly complicated recordings, as well as careful design of experiments that could study such behaviors in a virtual-reality environment. While the brain of the octopus is very different from that of the human, it does support well-defined cognitive functions. Therefore, understanding whether and how octopuses' brains implement processes such as learning, attention, habituation, and surprise can produce new and important understandings of how neurobiological systems can support function.  This research might reveal that the neural substrates of cognitive function in the octopus are organized according to principles that differ drastically from those found in in humans.<br/><br/>This EAGER project has several aims. It will develop the first underwater EEG, first testing well-validated paradigms on humans performing task underwater and benchmarking against known waveforms. The electrodes will be constructed so that they do not corrode in salt water. It will also develop high-quality virtual reality stimulation that could impact octopuses' behavior in an underwater environment. It will utilize EEG frequency-tagging techniques to determine processing of environmental stimulus by the octopus. This will allow studying whether octopuses present characteristic responses that are analogous to surprise, adaptation, working memory and attention effects (in primates and other vertebrates). The study will also allow answering how and in what manner do octopuses sleep. All data, artifacts and modeling software will be made publicly available and constitute an important resource for the community. The results of this study could impact our general understanding of how brains support complex cognitive functions, with direct relevance to artificial intelligence efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835278","NCS-FO: Engineering Living Neural Networks for Learning","ECCS","CCSS-Comms Circuits & Sens Sys, IntgStrat Undst Neurl&Cogn Sys","09/01/2018","05/12/2020","Xiaochen Guo","PA","Lehigh University","Standard Grant","Shubhra Gangopadhyay","08/31/2022","$509,038.00","Zhiyuan Yan, Yevgeny Berdichevsky","xig515@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","ENG","7564, 8624","090E, 8089, 8091, 8551, 9251","$0.00","Recent developments in optogenetics, patterned optical stimulation, and high-speed optical detection enable simultaneous stimulation and recording of thousands of living neurons. Connected biological living neurons naturally exhibit the ability to perform computations and to learn. The proposed project will engineer living neural network to compute for learning task. Experimental testbed will be built to allow optical stimulation and detection. Algorithms will be developed to train the living neuron networks. The proposed testbed can be used by neuroscientists to verify network-level hypotheses. Insights learned from the proposed research can inspire other neuromorphic architectures based on solid state devices. Throughout this project, graduate students will be trained in computer engineering, bioengineering, and signal processing. Students will have the opportunity to work on interdisciplinary research in these fields. New courses based on the results from the proposed work will be introduced and new modules will be added to existing curriculum. The proposed outreach activities aim to attract interest to computer engineering and neural engineering.<br/><br/>The goal of this project is to use optogenetic in vitro neural network to run learning applications. Living neural networks have spontaneous activities, which can interfere with precise modification of synaptic strength. This research will study how to stabilize the living neural network such that a Spike Time Dependent Plasticity (STDP)-based programming protocol can imprint the desired synaptic strengths onto a living neural network. This research will also investigate how to strategically design and apply an STDP-based protocol to maximize programming throughput and optimize convergence rate of the network states. On the algorithm side, the proposed research will study data representation and training algorithms that consider various constraints of the proposed wetware system. Learning algorithms will be designed to work on random neural networks of unknown topology. Observable details of neuron activities will be used to improve accuracy of learning tasks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750051","CAREER: Theoretical foundations of neural networks - representation, optimization, and generalization","IIS","Robust Intelligence","03/15/2018","02/19/2020","Matus Telgarsky","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Rebecca Hwa","02/28/2023","$298,901.00","","mjt@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7495","1045, 7495","$0.00","Neural networks form the backbone of machine learning's recent advances and sudden ubiquity.  Despite this extensive empirical progress, however, a satisfactory understanding of their behavior is still missing.  As neural networks enter more and more into human-facing services (self-driving cars, medical diagnostics, etc.), this status quo and in particular its safety ramifications becomes worrisome.  This project aims for a theoretical understanding of the foundations of neural networks, divided into three pieces: (a) the representation question regarding which phenomena can be succinctly approximated by neural networks; (b) the optimization question of how to efficiently fit neural networks to data; and (c) the generalization question on why neural networks can fit not only the data they have seen but also the data they have not seen.  Developing this understanding will form the core of this project's three broader impacts: (1) the research component will aim to improve safety and reliability of user-facing deployments of neural networks; (2) as an educational component, the research will be simplified and incorporated into freely available course notes; (3) the award supports two outreach efforts co-founded by the PI: UIUC-ML, a university-wide ML seminar; and the midwest ML symposium, a yearly midwest ML gathering.<br/><br/>In more detail, the technical focus of this project, divided into the three learning theoretic topics above, is as follows. The core representation question is: what makes neural network representation special?  In more detail, the proposed representation questions are firstly to characterize the power gained by adding a single layer to a network, and secondly to characterize the representation properties of recurrent neural networks, namely neural networks which evolve their state along with a time series they consume.  Next comes the topic of optimization, where the key mystery is how neural networks manage to perfectly fit their data with simple iterative descent schemes, despite the apparent nonconvexity of the problem.  The plan here is to establish an even stronger property: these iterative schemes manage to output networks which not only fit their data, but do so confidently, in the classical sense of margin theory.  Finally, the proposal closes with the topic of generalization.  The first goal is to develop refined generalization bounds to the point that they can be algorithmically enforced via effective regularization schemes, and secondarily to apply these techniques to the fitting of neural networks to probability distributions, specifically the problem of training Generative Adversarial Networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755628","CRII: CHS: Automatically Praising Learning Process to Promote the Growth Mindset in Computer Science","IIS","HCC-Human-Centered Computing","03/15/2018","03/09/2018","Eleanor O'Rourke","IL","Northwestern University","Standard Grant","Balakrishnan Prabhakaran","02/28/2021","$174,738.00","","eorourke@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7367","7367, 8228","$0.00","There is a pressing need to train large numbers of computer scientists to meet the demands of our nation's economy, but many students struggle in introductory programming courses. Recent studies show that these courses often promote the fixed mindset, or the belief that programming aptitude is an inborn trait. Psychology research shows that students with the fixed mindset view mistakes as indications of low ability and perform poorly in the face of challenge. In contrast, students with the growth mindset believe that programming aptitude is malleable and excel when challenged. This project aims to design, build, and evaluate programming tools that help students develop the growth mindset by automatically detecting and praising good learning behaviors as students write code. This research will contribute scientific knowledge about the growth mindset in the domain of computer science and provide insights about the process of learning to program. The project team will deploy the tools to hundreds of students at their own university and release them for free online for any student or teacher to use. If successful, this intervention has the potential to improve the experiences, skills, and diversity of students who successfully complete programming courses and go on to participate in employment and research in STEM fields.<br/><br/>This project aims to develop a new growth mindset intervention that leverages the programming environment by using artificial intelligence techniques to automatically detect and praise good learning processes in real time. Programming environments provide a unique opportunity to track and understand student learning behaviors, and offer a scalable environment for praising good practices automatically. By exposing and praising the learning process, this intervention will teach students to attribute their successes and failures to malleable learning processes, rather than an innate aptitude for computer science. This research will be conducted in two phases. First, the project team will develop heuristics that detect good learning processes using behavioral log data, leveraging the computer science education literature and studying the behavior of fixed and growth mindset students to identify good processes. Second, the team will iteratively design and build a programming environment extension that uses the validated heuristics to automatically detect and praise good learning process, and evaluate this intervention through a controlled ten-week study with university students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830366","NRI: INT: Customizing Semi-Autonomous Nursing Robots Using Human Expertise","IIS","NRI-National Robotics Initiati","09/01/2018","08/18/2018","Kris Hauser","NC","Duke University","Standard Grant","Wendy Nilsen","07/31/2020","$962,572.00","Ryan Shaw","kkhauser@illinois.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8013","063Z, 8086","$0.00","Remote-controlled robots have the potential to allow humans to perform useful tasks without putting themselves in danger, or without travelling long distances. This project explores how humans can control nursing robots that can communicate with patients, collect vital signs, and perform routine cleaning tasks in quarantine environments. The use of these robots has the potential to protect nurses from infection during disease outbreaks, and to protect patients with weakened immune system. A significant challenge in this effort is to make the user interface to the robot easy enough for nurses to use without significant training. Because engineers are not experts in nursing, the research will let nurses customize the user interface by teaching the robot about objects, places, and tasks that are typically used in nursing. After training, artificial intelligence algorithms will then automatically estimate which actions the nurse wants to perform, and these will be presented in a simple user interface that allows the nurse to select those actions quickly. <br/><br/>This project will continue an interdisciplinary collaboration between Duke's School of Engineering and School of Nursing. Research will be conducted in three thrust areas: 1) smart human operator interfaces for supervised autonomy that learn mappings between multimodal sensor input streams to provide simple, interpretable task options and status feedback; 2) hierarchical task learning algorithms for helping human experts train novel composite tasks; and 3) real world evaluation of human-robot system speed, reliability, operator workload, and operator learning curve using registered nurses and nursing students performing simulated clinical tasks in training environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815275","RI: SMALL: Robust Reinforcement Learning Using Bayesian Models","IIS","Robust Intelligence","08/15/2018","03/15/2019","Marek Petrik","NH","University of New Hampshire","Standard Grant","Rebecca Hwa","07/31/2021","$437,753.00","Shady Atallah","mpetrik@cs.unh.edu","51 COLLEGE RD SERVICE BLDG 107","Durham","NH","038243585","6038622172","CSE","7495","075Z, 7495, 7923","$0.00","Basing decisions on data is preferable to relying on heuristics or rules of thumb. Using data effectively, however, can be challenging. In domains like agriculture or medicine, datasets are usually small, biased, and noisy. For instance, the full effects of reduced pesticide applications depend on the weather and the impacts on yield may not be known until the harvest. Reducing pesticide applications reduces costs and provides ecological and consumer benefits, but using too little of it can easily cause a crop failure and significant financial losses. These dual problems of limited data availability and a high cost of failure are also common in manufacturing, maintenance, and even robotics. Because most existing reinforcement learning methods assume large datasets, stakeholders often dismiss data-driven methods and rely on heuristics to make decisions that are apparently safe but quite sub-optimal. This research develops new robust methods for data-driven decision making that can recommend good actions that are also safe even when data is limited. The new reinforcement learning methods use prior domain knowledge to estimate the confidence in possible outcomes to prevent catastrophic failure when predictions are incorrect. The practical viability of these methods is tested on the problem of using historical data to recommending improved pesticide schedules for fruit orchards and is disseminated to practitioners.<br/><br/>This research targets reinforcement learning problems with 1) limited or expensive data and 2) a high cost of failure. When bad decisions cause large losses, injury, or death, then having confidence in a policy's quality is more important than its optimality gap. Computing high-confidence policies in reinforcement learning is difficult. Even small errors can quickly accumulate through positive feedback loops and covariate shift. Therefore, more robust methods are needed to convince practitioners to benefit from data instead of relying on heuristics. The project combines robust optimization with model-based reinforcement learning to compute good policies that are resistant to data errors. Robust optimization has achieved successes in many areas but can be difficult to use with reinforcement learning. It requires a model of plausible uncertainty levels, so-called ambiguity sets, to properly balance solution?s quality and confidence. Constructing good ambiguity sets manually in sequential decision problems is very difficult even for robust optimization experts. This research investigates a new data-driven Bayesian approach to robust reinforcement learning. It combines hierarchical Bayesian models with robust optimization to leverage powerful hierarchical modeling techniques while avoiding the computational complexity often associated with Bayesian reinforcement learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815886","RI: Small: Coordination in tightly coupled domains: Stepping stone rewards to induce the correct joint actions","IIS","Robust Intelligence","09/01/2018","07/02/2018","Kagan Tumer","OR","Oregon State University","Standard Grant","Roger Mailler","08/31/2021","$400,000.00","","kagan.tumer@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","075Z, 7495, 7923","$0.00","This project introduces a new multiagent learning approach that leads to coordinated behavior in tightly coupled domains, that is, in domains where all agents must do the right thing at the right time for the team to achieve its goals. For example, getting a team of agents to lift and move an object heavier than the payload capacity of any single agent requires a sufficient number of agents to perform the correct action at the correct time. Unfortunately, most current learning methods fail in such situations because they rely on reinforcing the correct agent behavior only after the agents stumble upon the right actions. But what if the agents never jointly find the right actions? This project addresses this issue by introducing ""stepping-stone rewards"" that incentivize agents to perform the right actions even if their teammates have not yet found the correct complementary actions. The impact of this project will be to create larger and more capable multiagent teams that can be deployed in industry (such as factory robots that are not limited to a single task), in the field (such as autonomous search and rescue systems), in education (such as interactive learning via online gameplay) and in the home (such as networks of smart appliances).<br/><br/>The main technical contribution of this project is to shift the learning problem faced by an agent from ""did I take the correct action?"" to ""would my action have been correct had other agents taken the complementary action?"" In tightly coupled multiagent domains, the first question results in very little positive feedback, creating a difficult to impossible learning problem. The new stepping stone rewards leverage hypothetical partners (partners that are surmised by an agent to explore the joint-action space) to overcome this difficulty by assessing the potential benefits of a particular action. Intuitively, stepping-stone rewards create a gradient for the agents to follow to enable fast and efficient learning in tightly coupled domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813651","CHS: Small: Watch One, Do One, Teach One: An Integrated Robot Architecture for Skill Transfer","IIS","HCC-Human-Centered Computing","08/15/2018","08/08/2018","Brian Scassellati","CT","Yale University","Standard Grant","Ephraim Glinert","07/31/2021","$500,000.00","","brian.scassellati@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7367","075Z, 7367, 7923","$0.00","In the last several years, robotics research has transitioned from being concerned exclusively with building fully autonomous and capable robots to include building partially-capable robots that collaborate with human partners, allowing the robot to do what robots do best and the human to do what humans do best.  This transition has been fueled by a renaissance of safe, interactive systems designed to enhance the efforts of small- and medium-scale manufacturing, and has been accompanied by a change in the way we think robots should be trained.   Learning mechanisms in which the robot operates in isolation, learning from passive observation of people performing tasks, are being replaced by mechanisms where the robot learns through collaboration with a human partner as they accomplish tasks together.  This project will seek to develop a robot architecture that allows for new skills to be taught to a robot by an expert human instructor, for the robot to then become a skilled collaborator that operates side-by-side with a human partner, and finally for the robot to teach that learned skill to a novice human student.  To achieve this goal, popular but opaque learning mechanisms will need to be abandoned in favor of novel representations that allow for rapid learning while remaining transparent to explanation during collaboration and teaching, in conjunction with a serious consideration of the mental state (the knowledge, goals, and intentions) of the human partner.  A fundamental outcome of this work will be a unified representation linking the existing literature in learning from demonstration to collaborative scenarios and scenarios involving the robot as an instructor. Thus, project outcomes will have broad impact in application domains such as collaborative manufacturing, while also enhancing our substantial investment in education and training (especially research offerings for graduate and undergraduate investigators), and will furthermore enrich the efforts to broaden participation in computing.<br/><br/>This effort will build upon research in three subfields and extend the state-of-the-art to address deficiencies in each:<br/><br/>1 - Robot as Student.  Building on work from Learning from Demonstration, the team will construct robots that learn task models from humans.   However, to be useful to the other thrust areas, these models must not be opaque as many current learning techniques are.   Instead, a transparent model will allow the robot to provide and ask feedback about its performance, explain what it has learned, and to proactively ask questions that speed up learning.<br/><br/>2 - Robot as Collaborator.  The relatively new field of Human-Robot Collaboration struggles with synchronizing task execution between human and robot partners.   By linking to models of learned task behavior and models of user intention and understanding, the team will construct systems that become proficient in negotiating task allocation, accommodating user preferences, and restoring/updating internal representations in case of errors or change of plans.<br/><br/>3 - Robot as Teacher.  Fields including Intelligent Tutoring Systems build models of user knowledge, typically modeled using Bayesian knowledge tracing.  These models, however, simply show knowledge as known, unknown, or forgotten, and only for factual knowledge.   By linking with concrete representations of task and intent, the team will create robots that can detect, extend, or repair the mental model of a student for real-world tasks.<br/><br/>A set of milestones across three years will culminate in a demonstration of a robot that can learn a new task, collaborate on that task, and then teach that task to others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841354","CHS: EAGER: Handling Online Risks and Creating Safe Spaces: Content Moderation in Live Streaming Micro Communities","IIS","HCC-Human-Centered Computing","08/01/2018","05/06/2019","Donghee Yvette Wohn","NJ","New Jersey Institute of Technology","Standard Grant","William Bainbridge","06/30/2021","$230,055.00","","wohn@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7367","7367, 7916, 9251","$0.00","This research will investigate how individuals and small groups handle content moderation real time in the context of live streaming, from both technical and social perspectives, distinguishing between professional content creators who create content for a living, and hobbyists.  Live streaming services such as Twitch are the latest form of social media that marries user-generated content with the traditional concept of live television broadcasting: as someone broadcasts, viewers can post comments in a chat interface that is displayed alongside the broadcast, creating an interactive synchronous media experience. This real-time interaction, however, makes the platform ripe for deviant behavior, as potential harassers can visually see the immediate impact of their harsh words on the person who is broadcasting. Most current forms of social media rely on crowdsourced methods of moderation, where users report bad content that is ultimately reviewed by a human moderator. This does not work well in the context of real-time moderation, posing greater social and technological challenges. This project will study approaches to improving understanding of the sociotechnical aspects of content moderation from the perspective of micro communities on live streaming platforms. By understanding how streamers currently moderate audiences through manual and automated labor, the research will identify opportunities for technology to assist and enhance the moderation process and provide guidelines for sustainable and scalable moderation. Exploration of different governance structures of moderation may also yield insights into alternative models of moderation for the future of social media and understanding of how different moderation practices may influence the evolution of positive and negative norms in micro communities. <br/><br/>Because live streaming is such a new phenomenon, presenting novel technical and social challenges, exploratory research is required before any serious attempt to solve its problems through technology design.  This research agenda will advance knowledge about how moderation influences the development of social norms in micro communities from a qualitative perspective, laying the groundwork for future large-scale empirical studies, experiments, and development of useful artificial intelligence tools. The research will be able to identify the breadth of methods that are employed in the practice of moderation that will yield a comprehensive framework of understanding the conceptual functions of moderation by building a taxonomy of moderation, and develop a common language for both academics and practitioners that enables mapping between problems and potential design solutions. Moreover, through ethnographic work, the research will provide descriptive knowledge of this new form of social media that results in novel research questions unique to this particular technology.  This research will inform design of moderation tools and practices that could impact millions of people who publish content online and yet even more people who view that content. By focusing on the individual producer, rather than the corporation running the system, as the center of their own system, the findings may be able to empower a new era of Internet activity in which individuals and small groups have more agency over what happens online.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832383","WORKSHOP: Doctoral Consortium at the 2018 ACM/IEEE Human Robot Interaction (HRI) Conference","IIS","HCC-Human-Centered Computing","04/15/2018","04/12/2018","William Smart","OR","Oregon State University","Standard Grant","Ephraim Glinert","03/31/2020","$20,000.00","","smartw@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7367","7367, 7556","$0.00","This award support a Pioneers Workshop (doctoral consortium) of approximately 24 students (20 graduate participants, one undergraduate participant, and three student organizers), along with distinguished research faculty.  The event takes place as part of the first day of activities at the 13th International Conference on Human Robot Interaction (HRI 2018), held March 5-8 in Chicago, and which was jointly sponsored by ACM and IEEE.  HRI is the premier conference for showcasing the very best interdisciplinary and multidisciplinary research on human-robot interaction, with roots in diverse fields including robotics, artificial intelligence, social psychology, cognitive science, human-computer interaction, human factors, engineering, and many more.  It is a single-track, highly selective annual international conference that invites broad participation.  The theme of HRI 2018 was ""Robots for Social Good.""  The conference sought contributions from a broad set of perspectives, including technical, design, methodological, behavioral, and theoretical, that advance fundamental and applied knowledge and methods in human-robot interaction, with the goal of enabling human-robot interaction through new technical advances, novel robot designs, new guidelines for design, and advanced methods for understanding and evaluating interaction.  More information about the conference is available online at http://humanrobotinteraction.org/2018.  The Pioneers Workshop was designed to afford a unique opportunity for the best of the next generation of researchers in human-robot interaction to be exposed to and discuss current and relevant topics as they are being studied in several different research communities.  This is important for the field, because it has been recognized that transformative advances in research in this fledgling area can only come through the melding of cross-disciplinary knowledge and multinational perspectives.  Participants were encouraged to create a social network both among themselves and with senior researchers at a critical stage in their professional development, to form collaborative relationships, and to generate new research questions to be addressed during the coming years.  Participants also gained leadership and service experience, as the workshop was largely student organized and student led.  The PI expressed his strong commitment to recruiting women and members from under-represented groups.  To further ensure diversity the event organizers considered each applicant's potential to offer a fresh perspective and point of view with respect to HRI, and worked to recruit students who are just beginning their graduate degree programs in addition to students who are further along in their degrees.  <br/><br/>The Pioneers Workshops are designed to complement the conference, by providing a forum for students and recent graduates in the field of HRI to share their current research with their peers and a panel of senior researchers in a setting that is less formal and more interactive than the main conference.  During the workshop, participants talk about the important upcoming research themes in the field.  The formation of collaborative relationships across disciplines and geographic boundaries is encouraged.  To these ends, the workshop format encompasses a variety of activities including keynotes, a distinguished panel session, and breakout sessions.  To start the day, all workshop attendees briefly introduce themselves and their interests.  Following the opening keynote, approximately half of the participants present 3-minute overviews of their work, leading into an interactive poster session.  This enables all participants to share their research and receive feedback from students and senior researchers in an informal setting.  The workshop organizers facilitate the post-presentation discussion and encourage participants to ask questions of their peers during the interactive break and poster session.  After lunch, the remaining workshop participants give their 3-minute overviews, followed by presentation of their posters during a second interactive poster session.  Senior researchers (in addition to those on the panel) are invited to attend the student presentations and poster sessions in order to provide feedback to participants, and workshop participants are invited to present their posters during the main poster session of the HRI conference as well.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1754770","Collaborative Research: Origin and Evolutionary Divergence of the Pancrustacean Brain","IOS","Organization","07/01/2018","06/12/2018","Todd Oakley","CA","University of California-Santa Barbara","Continuing Grant","Evan Balaban","06/30/2022","$250,000.00","","oakley@lifesci.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","BIO","7712","1096, 9178","$0.00","It is still unknown when brains first appeared during the early history of life. The ways in which major brain parts that are structurally distinctive have changed over evolutionary time are also poorly understood. These knowledge gaps are partly due to the fact that fossil brains are rare and have been difficult to study. This project features scientists from three collaborating laboratories who will pool their resources to identify a set of invertebrate brain centers that mediate learning and memory. Structural and functional similarities and differences among these areas will be established across modern insect and crustacean species. The major question this research is answering is whether these brain centers share common genetic and computational attributes due to the brain?s fundamental organization being inherited by the descendants from a common ancestor; or, because brains that have arisen independently in different invertebrate groups are not able to perform certain functions unless brain areas that give them these same abilities have also arisen independently. These questions will be answered by precisely measuring the brain structures in fossilized invertebrate animals and comparing their basic arrangements with modern counterparts. The broader impact of this research will be to identify invertebrate proxies of the learning-and-memory brain centers found in vertebrate animals alive today, including humans. Identification of such proxies will inform us about how brains have evolved, and will contribute to a broader understanding of how memory centers are organized. The results will impact theories of, and research on, neural networks and artificial intelligence, and at the same time the scientists carrying out this research will develop novel strategies for identifying genealogical correspondence of brain structures across a very broad range of species. Brains analyzed for this research will be digitally reconstructed in 3D and uploaded to an open-source database for education and research purposes. The research will also provide advanced neuroscience structural analysis and genomics training to students from diverse backgrounds.<br/><br/>The neuronal organization and circuit properties of insect mushroom bodies are well known, as are their functional properties for learning and memory. While the existence of mushroom-body-like centers exist across arthropods, it is not known whether these phenotypically or genotypically correspond to the centers in insects. The planned research will identify mushroom body-like centers across a broad range of species, analyze their discrete neural arrangements, circuit organization, and molecular attributes. These comparisons will identify the species within and outside Arthropoda that possess functional and morphological correspondences in these structures. Transcriptomics will address whether phenotypically-corresponding centers share common genomic attributes, and whether there are unique genetic networks that define arthropod mushroom bodies or whether these networks differentiate mushroom bodies in different groups of arthropods such as in insects and crustaceans. The identification of broad phenotypic and genotypic homology of these centers across a broad phyletic spectrum would suggest an ancient origin of these learning and memory centers. Equally intriguing would be results suggesting convergent evolution of learning and memory centers across taxa.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827546","PFI-TT: Smart Climate Control in Shared Workspaces for More Personalization and Efficiency","IIP","PFI-Partnrships for Innovation","08/15/2018","08/14/2018","Koushik Kar","NY","Rensselaer Polytechnic Institute","Standard Grant","Kaitlin Bratlie","01/31/2021","$200,000.00","Sandipan Mishra","koushik@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","ENG","1662","1662, 8034","$0.00","The broader impact/commercial potential of this PFI project is in attaining significant energy savings in building operations, and in creating more comfortable and personalized work environments for its occupants. Energy efficiency measures in the buildings not only provides a means to reduce energy related costs, it also provides tremendous opportunity to reduce greenhouse gas emissions. Further, the technology that this project aims to develop and evaluate can also create a more personalized workspace for human occupants in a shared space, leading to better wellness and increased productivity of employees in indoor shared workplaces. The education and outreach goals of the project will be enhanced through cross-disciplinary training of a graduate student researcher, and involvement of undergraduate students in the experimental evaluation of the BEES system in a Smart Conference Room facility at RPI. The project team will actively engage with industrial partners and potential clients towards maximizing its commercialization potential. We believe that the proposed technology has considerable future potential for creating new jobs in emerging areas such as Smart Buildings, Internet-of-Things, and Artificial Intelligence.<br/><br/>The proposed project seeks to develop a data-driven learning and integrated control solution for heterogeneous HVAC elements in shared office spaces, with the aim of providing more personalized thermal environments to occupants and minimizing overall energy usage. Even with the high cost of operating the current HVAC systems, occupant dissatisfaction with thermal environments in workplaces have been highlighted by several recent studies. This project seeks to address this issue through the integrated application of i) data-driven learning of the indoor environment, and ii) integrated control of heterogeneous HVAC elements that typically exist in such shared office spaces. Firstly, it will utilize data driven modeling of complex spatiotemporal dynamics of the physical environment, and the dependency between the controls available and the environmental variables. Secondly, it will utilize the data-driven model towards integrated control of the heterogeneous HVAC elements associated with the space to attain differentiated temperatures across the space as desired by the occupants. The product of this project will be a software prototype that will operate in conjunction of sensor and IoT devices, and networked HVAC elements. This integrated HVAC control technology will be evaluated for technical performance and commercial viability in a smart conference room.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1851362","EAGER: I-Corps: Smart Water Systems","CNS","I-Corps","11/01/2018","10/26/2018","Branko Kerkez","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Vipin Chaudhary","12/31/2019","$50,000.00","","bkerkez@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8023","8023","$0.00","Flooding is the leading cause of natural disaster fatalities and property damage across the United States. Mitigating water risks poses a serious challenge to water managers, companies, and home owners. This includes risk related to disaster response, stormwater infrastructure construction, and the management of water quality. Successful execution of this I-Corps project will result in new tools to address the challenges faced by the United States in the face of water disasters. This project will result in 'smart' water tools that address the management of water risk by investigating new methods based on data, analytics and sensing. This will reduce the risk associated with making decision in regard to flood forecasting, planning, and infrastructure construction. The project will also result in new jobs and workforce training as part of an emerging 'smart' water sector.<br/><br/>This I-Corps project will investigate the economic value of water information. The project will engage with decision makers, including water managers, energy producers, and agricultural managers. A series of interviews will be conducted to improve the understanding of water challenges and risks faced by these parties. A focus will be placed on the role of information technologies, such as sensors, autonomous systems, and artificial intelligence tools. By quantifying water risk and improving the understanding of uncertainty, the project will provide new insights into the barriers that stand in the way of technology adoption in the water sector. The outcome of the project will transform the fundamental understanding of the role of 'smart' technologies in managing water resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819546","Automated Feedback in Undergraduate Computing Theory Courses","DUE","IUSE","10/01/2018","08/30/2018","Ivona Bezakova","NY","Rochester Institute of Tech","Standard Grant","Michael Ferrara","09/30/2021","$299,417.00","Edith Hemaspaandra","ib@cs.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","EHR","1998","8209, 8244, 9178","$0.00","Computing theory poses and answers questions such as ""Which problems are efficiently computable and which are not?"" Answering such questions is important for any computer scientist and for any kind of software development.  For example, it is better to determine if a problem is computable before spending a lot of time trying to write a program to solve it. Unfortunately, many students struggle with computing theory, because it is more abstract and mathematical than other computer science topics. As in any other knowledge area, students need to practice to get better at computing theory. A problem is that feedback on their work is not immediate and, while students wait for feedback, they stop interacting with the material. They may have to wait for days, since grading an assignment often takes a lot of instructor time and the instructors may have many assignments to grade. This project will increase the speed and, potentially, the quality of feedback to computing theory students by developing an automated feedback tool.  The feedback will tell students whether a solution is correct or not, a convincing reason why an incorrect solution is incorrect, and provide information about the quality of a solution. Students will be able to use this immediate feedback to improve their solutions, get more practice, and increase their understanding of the material. In addition to building the feedback tool, this project aims to conduct research on the feedback tool's effectiveness.  This project has the potential to contribute to the education of a strong computing workforce and to support development of students' independent learning skills.  <br/><br/>Although understanding computing theory concepts is very important, it is challenging. Typically, as a first step, students in computing theory classes learn about various models of computation. To understand more complex computational issues, students need to fully comprehend the possibilities and limitations of these models. JFLAP (Java Formal Languages and Automata Package) is a widespread tool that provides a way for students to interact with these concepts. However, like other interactive tools in this area, it does not provide detailed feedback on student solutions. This project will build a feedback and grading tool on top of JFLAP, to increase the likelihood that the feedback tool will have broad applicability. To accomplish this goal, the project will develop and evaluate the tool in the context of three research areas: (1) Computer Science Education:  Do students who use the tool understand theoretical computer science concepts better than students who do not use the tool? (2) Theoretical Computer Science:  How can software generate a convincing reason for why a student solution is incorrect? and (3) Artificial Intelligence: How can feedback be given about the quality of a student's solution?  The project's research and software development activities will involve ten undergraduate students, who will be recruited with emphasis on including women and deaf/hard-of-hearing students.  Thus, the project will directly contribute to these students' scientific and professional development.  Project outcomes will be disseminated at scientific conferences and workshops, as well as at the University's innovation fair, which is attended by 35,000 visitors, including middle and high school students. Developing the feedback tool and completing research on its effectiveness has the potential to improve instruction and learning of computing theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832282","Building Capacity: Institute for Interdisciplinary Science:  Preparing Students for the 4th Industrial Revolution","DUE","HSI-Hispanic Serving Instituti","10/01/2018","10/29/2018","Andrea Holgado","TX","Saint Edward's University","Standard Grant","Ellen Carpenter","09/30/2023","$1,499,950.00","Charles Hauser, Laura J Baker, Paul Walter, Raychelle Burks, Bilal Shebaro","aholgado@stedwards.edu","3001 South Congress Ave","Austin","TX","787046489","5124442621","EHR","077Y","8209, 9178","$0.00","The Improving Undergraduate STEM Education: Hispanic-Serving Institutions Program (HSI Program) aims to enhance undergraduate STEM education and build capacity at HSIs. Projects supported by the HSI program will also generate new knowledge about how to achieve these aims. This project at St. Edward's University in Austin, Texas aims to create the Institute for Interdisciplinary Science.  The Institute will support student workforce training, cross-sector cooperation, and interdisciplinary activities that can prepare faculty and students for opportunities in the 4th Industrial Revolution.  This revolution is predicted to produce a virtually-connected world in which digital, physical, and biological domains are intertwined.  It is a world of smart homes, cloud computing, big data analytics, e-commerce, robotics, and artificial intelligence. As the 4th Industrial Revolution unfolds, collaborations between industry and higher education become critical to ensure that graduates have the knowledge and skills to be successful in the new workplace. The Institute will facilitate these collaborations by developing mutually beneficial industry-academia partnerships, providing professional development and research opportunities, and facilitating networking for exchange of new directions and ideas. The Institute's partnerships with companies will offer summer internships, which can provide economic support and encouragement to students pursuing STEM degrees, and support eventual entry into STEM careers.  In turn, students will contribute to research and development in the digital era, furthering the objectives of the companies where they work.  This project has the potential to serve as a model for other institutions, especially HSIs and other Minority-Serving Institutions (MSIs), to develop programs with similar goals, thus broadening participation and increasing diversity in the STEM workforce.<br/><br/>This project will support efforts of the School of Natural Sciences at St. Edward's University to build capacity in interdisciplinary sciences, informatics, and emerging technologies and to increase students' readiness for the 4th Industrial Revolution. The Institute aims to establish an educational and professional framework that will facilitate cross-sector partnerships and interdisciplinary collaborations. The Institute will: (i) coordinate on-campus interdisciplinary seminars and experiential learning events that will challenge faculty and students to explore complicated problems with cross-disciplinary approaches; (ii) organize cross-sector cooperative agreements with public and private entities around the Austin, Texas area and beyond; (iii) expose STEM majors to postgraduate opportunities by connecting them with employers and graduate programs through guaranteed internships; (iv) finance faculty and student professional development by offering awards to faculty and micro-credentialing scholarships to students; and (v) catalyze faculty advancement, interdisciplinary collaborations, and innovative research by offering research opportunity awards.   The project's research questions will focus on effects of the interventions on student persistence and on whether faculty professional development grants and research opportunity awards are a means to enhance student success.  Together, these activities are designed to improve student access and success in the emerging 4th Industrial Revolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835690","Elements: Software: Autonomous, Robust, and Optimal In-Silico Experimental Design Platform for Accelerating Innovations in Materials Discovery","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Byung-Jun Yoon","TX","Texas A&M Engineering Experiment Station","Standard Grant","Bogdan Mihaila","09/30/2021","$600,000.00","Xiaofeng Qian, Xiaoning Qian, Raymundo Arroyave","bjyoon@ece.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","1712, 8004","026Z, 054Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Accelerating the development of novel materials that have desirable properties is a critical challenge as it can facilitate advances in diverse fields across science, engineering, and medicine with significant contributions to economic growth. For example, the US Materials Genome Initiative calls for cutting the time for bringing new materials from discovery to deployment by half at a fraction of the cost, by integrating experiments, computer simulations, and data analytics. However, the current prevailing practice in materials discovery relies on trial-and-error experimental campaigns and/or high-throughput screening approaches, which cannot efficiently explore the huge design space to develop materials with the targeted properties. Furthermore, measurements of material composition, structure, and properties often contain considerable errors due to technical limitations in materials synthesis and characterization, making this exploration even more challenging. This project aims to develop a software platform for robust autonomous materials discovery that can shift the current trial-and-error practice to an informatics-driven one that can potentially expedite the discovery of novel materials at substantially reduced cost and time. Throughout the project, the PI and Co-PIs will mentor students and equip them with the skills necessary to tackle interdisciplinary problems that involve materials science, computing, optimization, and artificial intelligence. Research findings in the project will be incorporated into the courses taught by the PI and Co-PIs, thereby enriching the learning experience of students.<br/><br/>The objective of this project is to develop an effective in-silico experimental design platform to accelerate the discovery of novel materials. The platform will be built on optimal Bayesian learning and experimental design methodologies that can translate scientific principles in materials, physics, and chemistry into predictive models, in a way that takes model and data uncertainty into account. The optimal Bayesian experimental design framework will enable the collection of smart data that can help exploring the material design space efficiently, without relying on slow and costly trial-and-error and/or high-throughput screening approaches. The developed methodologies will be integrated into MSGalaxy, a modular scientific workflow management system, resulting in an accessible, reproducible, and transparent computational platform for accelerated materials discovery that allows easy and flexible customization as well as synergistic contributions from researchers across different disciplines.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815949","SHF: Small: Lazy Data Structures for Data-Intensive Applications","CCF","Software & Hardware Foundation","10/01/2018","08/15/2019","Yu David Liu","NY","SUNY at Binghamton","Standard Grant","Anindya Banerjee","09/30/2021","$511,402.00","","davidl@cs.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","7798","7923, 7943","$0.00","Developing and optimizing data-intensive applications is a crucial but challenging goal in the Big Data era. This project aims to research and design a novel programming system to improve the performance and assurance of data-intensive applications. The project's novelties are (i) laying a new foundation for programming, optimizing, and reasoning about Big Data systems, and (ii) building a practical software ecosystem to improve the quality of data-intensive applications. The project's impacts are (i) shedding fundamental insight in data-intensive programs, with a broad range of applications from social network analysis to artificial intelligence; (ii) enabling new curriculum development, and bringing underrepresented students to the exciting frontier of data-intensive computing. <br/><br/>The project centers around the idea of data-centric laziness: the operations to be performed over data structures --- such as topological changes or payload queries --- may be delayed and flexibly memoized within the data structure itself in a decentralized manner. The project is carried out in several directions. First, it conducts a foundational study on laziness in the presence of data processing, including a rigorous study on the subtleties in designing a lazy propagation system, a proof of observable equivalence between lazy and eager data processing, a cost-based semantics for capturing lazy behaviors, and a unification of eagerness vs. laziness and data vs. computation. Second, it investigates how parallelism and laziness interact to improve the performance of lazy data structures, through the support of asynchronous data processing, in-data propagation parallelism, and concurrent garbage collection of propagation labels. Third, it bridges the language foundation with practical algorithm design and system building, exploring algorithm-oriented programming abstractions, partition-based out-of-core data processing, just-in-time data structure re-organization, and propagation-aware performance monitoring.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839014","Challenges and Opportunities in Scientific Data Discovery and Reuse in the Data Revolution: Harnessing the Power of AI","OAC","NSF Public Access Initiative","10/01/2018","08/17/2018","Nicholas Nystrom","PA","Carnegie-Mellon University","Standard Grant","Beth Plale","09/30/2019","$50,000.00","Paola Buitrago, Huajin Wang, Keith Webster","nystrom@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7414","7556","$0.00","The volume and heterogeneity of scientific data goes beyond a researcher?s ability to find relevant data, make different formats interoperable, and deal with difficult ontological and language issues. Progress requires bringing together experts in data curation with experts in Artificial Intelligence (AI) to begin a dialog and collaboration on AI tools to span the gap between data and its reuse.  The PIs will organize the conference ""Challenges and Opportunities in Scientific Data Discovery and Reuse in the Data Revolution: Harnessing the Power of AI""; the objective is to bring together diverse stakeholders including research librarians and data managers, the AI community, and users and consumers of scientific data to dialog around critical places in the data lifecycle where AI could be groundbreaking.  <br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1754798","Collaborative Research: Origin and Evolutionary Divergence of the Pancrustacean Brain","IOS","Organization","07/01/2018","08/08/2019","Nicholas Strausfeld","AZ","University of Arizona","Continuing Grant","Evan Balaban","06/30/2022","$519,032.00","","flybrain@neurobio.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","BIO","7712","1096, 9178, 9179","$0.00","It is still unknown when brains first appeared during the early history of life. The ways in which major brain parts that are structurally distinctive have changed over evolutionary time are also poorly understood. These knowledge gaps are partly due to the fact that fossil brains are rare and have been difficult to study. This project features scientists from three collaborating laboratories who will pool their resources to identify a set of invertebrate brain centers that mediate learning and memory. Structural and functional similarities and differences among these areas will be established across modern insect and crustacean species. The major question this research is answering is whether these brain centers share common genetic and computational attributes due to the brain?s fundamental organization being inherited by the descendants from a common ancestor; or, because brains that have arisen independently in different invertebrate groups are not able to perform certain functions unless brain areas that give them these same abilities have also arisen independently. These questions will be answered by precisely measuring the brain structures in fossilized invertebrate animals and comparing their basic arrangements with modern counterparts. The broader impact of this research will be to identify invertebrate proxies of the learning-and-memory brain centers found in vertebrate animals alive today, including humans. Identification of such proxies will inform us about how brains have evolved, and will contribute to a broader understanding of how memory centers are organized. The results will impact theories of, and research on, neural networks and artificial intelligence, and at the same time the scientists carrying out this research will develop novel strategies for identifying genealogical correspondence of brain structures across a very broad range of species. Brains analyzed for this research will be digitally reconstructed in 3D and uploaded to an open-source database for education and research purposes. The research will also provide advanced neuroscience structural analysis and genomics training to students from diverse backgrounds.<br/><br/>The neuronal organization and circuit properties of insect mushroom bodies are well known, as are their functional properties for learning and memory. While the existence of mushroom-body-like centers exist across arthropods, it is not known whether these phenotypically or genotypically correspond to the centers in insects. The planned research will identify mushroom body-like centers across a broad range of species, analyze their discrete neural arrangements, circuit organization, and molecular attributes. These comparisons will identify the species within and outside Arthropoda that possess functional and morphological correspondences in these structures. Transcriptomics will address whether phenotypically-corresponding centers share common genomic attributes, and whether there are unique genetic networks that define arthropod mushroom bodies or whether these networks differentiate mushroom bodies in different groups of arthropods such as in insects and crustaceans. The identification of broad phenotypic and genotypic homology of these centers across a broad phyletic spectrum would suggest an ancient origin of these learning and memory centers. Equally intriguing would be results suggesting convergent evolution of learning and memory centers across taxa.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750731","CAREER: The Future of Work in Health Analytics and Automation: Investigating the Communication that Builds Human-Technology Partnerships","SES","Cross-Directorate  Activities, SoO-Science Of Organizations","09/01/2018","08/15/2018","Joshua Barbour","TX","University of Texas at Austin","Continuing Grant","Georgia Chao","08/31/2023","$204,471.00","","barbourjosh@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","SBE","1397, 8031","063Z, 1045","$0.00","Advances in technologies such as the internet of things, robotics, and artificial intelligence are transforming work through data-intensive automation, which may eliminate jobs without creating new ones or deskill and diminish existing work. This project will investigate how work is automated to encourage forms that benefit work and workers. The project will expand knowledge about the everyday conversations that shape the implementation of automation, and how leaders make choices about how to have those conversations in the first place. The project will focus on health and healthcare work, with is a context likely to be affected by datafication and automation and likely to provide STEM-related careers for individuals who have the right skills. In healthcare, automation may help providers prevent medical errors, lower the costs of caregiving, and augment or create new forms of work, but the success of such systems depends on how they are designed and implemented. By focusing on the actual communication involved in automation, the project will generate theoretical insights and practical recommendations for leaders in health and analytics organizations, regarding (a) what makes the communication involved in data-intensive automation effective or not, and (b) how to structure and facilitate that communication. The research will be used to create short films and a learning module for students making key career decisions. The films and module will be designed to reach groups underrepresented in STEM and to provide information about STEM careers affected by automation and STEM-related, communication competencies. The project will help students at community colleges and universities understand and prepare for the opportunities and challenges of automation.<br/><br/>Recent research has demonstrated that automation is determined not merely by the features of new technology or pressures to make work more efficient, but by a complex, communicatively-negotiated mix of workers' and managers' ideas about factors such as market forces, professional standards, regulation, industry knowledge, and human and technology workflows. Automation involves intertwined changes in the technologies and organization of work. These changes unfold in and through everyday communication about how work is and ought to be accomplished. Using a combination of interview and observational methods, the project will investigate two theoretically and practically important contexts: (1) Healthcare organizations that develop and implement technologies such as automated metrics dashboards and clinical decisions support systems, and (2) Quantified Self communities where practitioners of personal analytics are creating new human-technology partnerships, new forms of work and play, through automation. Insights from this project will advance research on automation, data-intensive work, communication design, and organizational and technological change.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822117","Planning IUCRC at Georgia Institute of Technology:  Center for Engineering and Manufacturing Technologies Advancing Food Safety and Security (CAFSS)","IIP","IUCRC-Indust-Univ Coop Res Ctr","09/01/2018","08/28/2018","Douglas Britton","GA","Georgia Tech Applied Research Corporation","Standard Grant","Prakash Balan","08/31/2019","$15,000.00","","doug.britton@gtri.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","5761","5761","$0.00","The US food and beverage industry employs 1.46 million individuals and is responsible for more than 10% of all manufacturing shipments. Even though sanitation is one of the food industry's biggest production expenses, inadequate sanitation remains one of the greatest threats to America's food safety and security due to the lack of modern technology and consistent application of sanitation policies.<br/><br/>CAFSS will strive to integrate and streamline the entire food supply chain in the US from 'Farm to Fork' and supply safe (pathogen and allergen free), secure (uninterrupted and fully traceable sources), wholesome and plentiful supply of food to the US and Global consumers.<br/><br/>CAFSS objectives will focus on addressing two grand challenges of national importance:<br/><br/>1) Ensuring a stable and sustainable supply of affordable, safe, nutritious food not only for the US but also for the rest of the world.<br/>2) Equipping and empowering US food manufacturers and their supporting industries, to establish highly competitive manufacturing plants in the US and the world.<br/><br/>The US food industry adds commercial value to agricultural products and provides employment opportunities in both rural and urban areas and CAFSS will enhance the overall sustainability and profitability through automation and new private-public partnerships.<br/><br/>The proposed CAFSS will bridge technological gaps, which fall under the general theme of food safety, security and traceability of all raw materials throughout the entire supply chain from 'Farm to Fork'.  Bridging these gaps will require multidisciplinary collaborations among public and private enterprises and groundbreaking research and development within the following fundamental science and engineering platforms:<br/><br/>1) Automation, control and robotics; IoT systems and data integration; <br/>2) New sensors; big data analytics and artificial intelligence; <br/>3) New functionalized surfaces, new materials / coatings  <br/>4) Novel food sterilization technologies. <br/><br/>Automation, control, and robotics minimize human contact with food and greatly enhance food safety. Newer sensors with data analytics enable the food companies to improve food safety, quality, and traceability. Novel functionalized surfaces enhance food sanitation efficiency thereby reducing production downtime, and energy and water requirements.  Novel food sterilization and pasteurization technologies improve food safety with minimal deterioration in food quality and they meet consumers' demand for minimally processed foods.<br/><br/>Research projects will be carried out at CAFSS hub facilities in Nebraska and Georgia and the resulting breakthroughs will allow the US food manufacturing industry to join other industries in terms of efficiency, automation, lower cost, predictability, safety and security of goods supplied.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800188","Collaboration Of Midwest Professionals for Logistics Engineering Technology Education Project","DUE","Advanced Tech Education Prog","10/01/2018","08/14/2018","Jeremy Banta","OH","Columbus State Community College","Standard Grant","John Jackman","09/30/2021","$268,000.00","","jbanta1@cscc.edu","550 East Spring Street","Columbus","OH","432151722","6142872639","EHR","7412","1032, 9178, SMET","$0.00","The supply chain, or logistics, industry continues to grow in the Midwest region of the United States. Jobs in this industry are also becoming more dependent on technologies such as predictive analytics, artificial intelligence, and robotics. Nearly 15,000 transportation and warehousing businesses are located across the cities of Columbus, Ohio; Dayton, Ohio; and Chicago, Illinois. The ongoing growth helps support and strengthen the logistics industry in this regional economy. As a result, there is an increasing need for logistics engineering technicians in the region. In this project, a grouping of community colleges who are leaders in the logistics field in this Midwest region will be formed. The project will be led by Columbus State Community College, Oakton Community College, and Sinclair Community College. It will seek to build career pathways in logistics engineering technology for students, encourage more students to complete degrees in this field, develop faculty experience on the latest technologies, and improve the technical skills of graduates. The goal will be to provide highly-skilled logistics engineering technicians to support the regional and national needs of the supply chain sector. <br/><br/>The project will aim to improve technician education to support the increasingly complex technology needs of the supply chain sector while connecting graduates to employment opportunities in a variety of logistics industries. This project will work with employers, industry experts, and colleges to establish an innovation network. The network of partners will identify and create educational resources for emerging skills and technologies within the sector. The existing logistics curriculum at Columbus State Community College combines technology applications with engineering systems and integrates them with supply chain operations. This curriculum will be enhanced to integrate new topics in data-driven analytics and networking systems. The adapted curriculum will also be implemented and tested at the partnering institutions. The network infrastructure will inform and regularly evaluate efforts in curriculum and career pathway development; in providing professional development material and trainings for high school and college faculty; and on the use of prior learning assessments for adult and returning learners, particularly veterans and recent graduates in need of new skills for the ever-changing logistics job market. Results will be widely disseminated through an interactive project website, through publications, and at regional and national conferences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800186","Collaboration Of Midwest Professionals for Logistics Engineering Technology Education Project","DUE","Advanced Tech Education Prog","10/01/2018","08/14/2018","Robert Sompolski","IL","Oakton Community College","Standard Grant","John Jackman","09/30/2021","$149,998.00","","somplski@oakton.edu","1600 East Golf Road","Des Plaines","IL","600161234","8473767099","EHR","7412","1032, 9178, SMET","$0.00","The supply chain, or logistics, industry continues to grow in the Midwest region of the United States. Jobs in this industry are also becoming more dependent on technologies such as predictive analytics, artificial intelligence, and robotics. Nearly 15,000 transportation and warehousing businesses are located across the cities of Columbus, Ohio; Dayton, Ohio; and Chicago, Illinois. The ongoing growth helps support and strengthen the logistics industry in this regional economy. As a result, there is an increasing need for logistics engineering technicians in the region. In this project, a grouping of community colleges who are leaders in the logistics field in this Midwest region will be formed. The project will be led by Columbus State Community College, Oakton Community College, and Sinclair Community College. It will seek to build career pathways in logistics engineering technology for students, encourage more students to complete degrees in this field, develop faculty experience on the latest technologies, and improve the technical skills of graduates. The goal will be to provide highly-skilled logistics engineering technicians to support the regional and national needs of the supply chain sector. <br/><br/>The project will aim to improve technician education to support the increasingly complex technology needs of the supply chain sector while connecting graduates to employment opportunities in a variety of logistics industries. This project will work with employers, industry experts, and colleges to establish an innovation network. The network of partners will identify and create educational resources for emerging skills and technologies within the sector. The existing logistics curriculum at Columbus State Community College combines technology applications with engineering systems and integrates them with supply chain operations. This curriculum will be enhanced to integrate new topics in data-driven analytics and networking systems. The adapted curriculum will also be implemented and tested at the partnering institutions. The network infrastructure will inform and regularly evaluate efforts in curriculum and career pathway development; in providing professional development material and trainings for high school and college faculty; and on the use of prior learning assessments for adult and returning learners, particularly veterans and recent graduates in need of new skills for the ever-changing logistics job market. Results will be widely disseminated through an interactive project website, through publications, and at regional and national conferences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800182","Collaboration Of Midwest Professionals for Logistics Engineering Technology Education Project","DUE","Advanced Tech Education Prog","10/01/2018","08/14/2018","Ned Young","OH","Sinclair Community College","Standard Grant","John Jackman","09/30/2021","$152,000.00","","ned.young@sinclair.edu","444 West Third Street","Dayton","OH","454021460","9375124573","EHR","7412","1032, 9178, SMET","$0.00","The supply chain, or logistics, industry continues to grow in the Midwest region of the United States. Jobs in this industry are also becoming more dependent on technologies such as predictive analytics, artificial intelligence, and robotics. Nearly 15,000 transportation and warehousing businesses are located across the cities of Columbus, Ohio; Dayton, Ohio; and Chicago, Illinois. The ongoing growth helps support and strengthen the logistics industry in this regional economy. As a result, there is an increasing need for logistics engineering technicians in the region. In this project, a grouping of community colleges who are leaders in the logistics field in this Midwest region will be formed. The project will be led by Columbus State Community College, Oakton Community College, and Sinclair Community College. It will seek to build career pathways in logistics engineering technology for students, encourage more students to complete degrees in this field, develop faculty experience on the latest technologies, and improve the technical skills of graduates. The goal will be to provide highly-skilled logistics engineering technicians to support the regional and national needs of the supply chain sector. <br/><br/>The project will aim to improve technician education to support the increasingly complex technology needs of the supply chain sector while connecting graduates to employment opportunities in a variety of logistics industries. This project will work with employers, industry experts, and colleges to establish an innovation network. The network of partners will identify and create educational resources for emerging skills and technologies within the sector. The existing logistics curriculum at Columbus State Community College combines technology applications with engineering systems and integrates them with supply chain operations. This curriculum will be enhanced to integrate new topics in data-driven analytics and networking systems. The adapted curriculum will also be implemented and tested at the partnering institutions. The network infrastructure will inform and regularly evaluate efforts in curriculum and career pathway development; in providing professional development material and trainings for high school and college faculty; and on the use of prior learning assessments for adult and returning learners, particularly veterans and recent graduates in need of new skills for the ever-changing logistics job market. Results will be widely disseminated through an interactive project website, through publications, and at regional and national conferences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760991","Seventh International Conference on Computational Harmonic Analysis","DMS","COMPUTATIONAL MATHEMATICS","01/01/2018","12/04/2017","Alexander Powell","TN","Vanderbilt University","Standard Grant","Leland Jameson","12/31/2018","$15,000.00","Emanuel Papadakis","alexander.m.powell@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","MPS","1271","7556, 9263","$0.00","The Seventh International Conference on Computational Harmonic Analysis (ICCHA7) will take place at Vanderbilt University in Nashville, TN during May 14-18, 2018, in conjunction with the 33rd Annual Shanks Conference and Lecture. The conference website https://my.vanderbilt.edu/iccha7 contains a list of the conference plenary speakers and other conference information.  Computational harmonic analysis is a fundamental tool for analyzing and representing information, and is especially motivated by modern applications where data has complex structure and massively high dimension.  Applications of computational harmonic analysis include many areas of national technological interest such as data science, neural networks and artificial intelligence, medical imaging, radar signal processing, coding theory, and quantum computing.  The conference will discuss the most recent theoretical breakthroughs, practical advances, and emerging directions in computational harmonic analysis.  The plenary speakers include a diverse assortment of experts, and female and early career scientists are well-represented.  An important broader impact of the conference is to support the participation of students, early career scientists, and underrepresented minorities; the conference will make a particular effort to invite and provide travel support to members of these groups.  Travel support will be strongly prioritized to those without other sources of travel funding, so as to make the conference accessible those who would not otherwise be able to attend.  This will contribute to expanding and diversifying the nation's talent pool and workforce in the mathematical sciences by contributing to the training of underrepresented groups in STEM fields.<br/><br/>Specific technical topics addressed at the conference will include, but are not limited to: compressed sensing, phase retrieval, convolutional neural networks, wavelets and multiscale transforms, frame theory, graph-based signal processing, time-frequency analysis, analog-to-digital conversion, signal and image processing, quantum computation, and mathematical learning theory.  A main intellectual merit of the conference is to provide a venue to disseminate recent advances in computational harmonic analysis.  The conference will consist of plenary talks, as well as shorter talks and minisymposia in parallel sessions.  There will be numerous opportunities for mathematical interaction and collaboration among the participants.  The conference will provide an interdisciplinary link between mathematicians, engineers, and scientists from other fields who are working on computational harmonic analysis."
"1829225","Individuating and comparing objects and events","BCS","Linguistics","03/01/2018","03/08/2018","Alexis Wellwood","CA","University of Southern California","Standard Grant","Tyler Kendall","12/31/2020","$462,064.00","","wellwood@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","SBE","1311","1311, 9178, 9179, 9251","$0.00","The primary mode by which we communicate our ideas about the world and about each other is through language. However, languages aren't merely passive vehicles for the transmission of information. Rather, our sentences carry along with them evidence of the fundamental concepts and categories that we use to understand the world and each other. At one level, this fact about language might seem obvious: a person on one side of an argument might choose to use words that a person on a different side might not, and attending to these different choices tells us something about the people speaking. Such observations about language and language users are studied in fields like sociolinguistics. Yet, our language also reveals more basic truths about us which are not as easily accessible to consciousness, and which are more tied to elements of our common experience. For example, people talk as if there are objects that can be counted (""four spoons"") and substances that cannot (e.g., ""four muds"" is odd), even if arranged in discrete piles. Investigating language at this deeper level can thus reveal basic structures of thought, informing theories of cognition and its development, as well as applications in artificial intelligence.<br/><br/>This project studies parallels in the conceptualization of the basic categories 'object' and 'event' as they are encoded in language and understood by both adults and 4 year olds. Previous research in linguistics and the philosophy of language has uncovered striking formal parallels in the encoding of these categories across nominal and verbal language. The project links this research to what is known about object representation in cognitive science, and uses this link to extend what is known about event representation. Specifically, the project (i) tests whether the observed linguistic parallels correspond to parallels in how adults and children conceptualize minimally-different static and dynamic scenes, (ii) investigates the extent to which representational biases for simple dynamic scenes predicts how adults and children understand quantificational language involving words like ""more"", and (iii) probes the hypothetical universality of the language-cognition linkages by teaching English-speaking adults and children attested, but non-English patterns of event encoding. The results of this project will demonstrate the fruitfulness of connecting formal semantics, philosophy of language, and cognitive science to illuminate the interface between linguistic and non-linguistic perception and cognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827633","PFI-TT: Scalable Silicon Photonic Switches for Data Centers","IIP","PFI-Partnrships for Innovation","08/15/2018","08/15/2018","Ming Wu","CA","University of California-Berkeley","Standard Grant","Kaitlin Bratlie","01/31/2021","$200,000.00","","wu@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","1662","1662, 8034","$0.00","The broader impact/commercial potential of this PFI project is to introduce scalable silicon photonic switches with fast response time and low power consumption to enhance the capacity and efficiency of the interconnect networks in data centers. The global internet traffic has continued to grow exponentially, fueled by the increase of mobile devices and cloud-based applications for artificial intelligence, video streaming, social networking as well as enterprise compute/collaborate needs. A majority of the data traffic remains within the datacenter. The number of hyperscale datacenters will double in five years. Each of these hyperscale datacenters houses hundreds of thousands of servers and consumes 100 MW of power. Such growth is not sustainable without fundamental changes in how datacenter is designed. Scalable photonic switches with fast response time and low power consumption can improve the efficiency and performance of datacenters. Optical switching is agnostic to data rate, unlike the electrical packet switches in current networks. It enables programmable datacenter networks, allowing the network to adjust its topology to match the traffic patterns of applications.<br/><br/>The proposed project will accelerate the development and commercialization of scalable silicon photonic switches and make them widely available to datacenters and telecommunication industry. Current optical switches in the market are bulky, slow, and expensive. In this PFI-TT, we propose to replace the bulk optical switches with fully integrated (single chip) silicon photonic switches. Silicon photonics use complementary-metal-oxide-semiconductor (CMOS) foundries to mass produce photonic integrated circuits at low cost, leveraging economies of scale of the multi-trillion-dollar microelectronics industry. We use micro-electro-mechanical-system (MEMS) actuation to dramatically reduce the optical insertion loss and power consumption. Silicon photonic switches with microsecond response time and low optical loss will revolutionize datacenter networks. The anticipated outputs of this program include a prototype switch with proven reliability and a strategy to scale up production. The ultimate goal is to spin out a new company building switching subsystems supplying to both datacenter and the telecommunications industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832993","RII Track-4: EASE - Functional Electrical Stimulation and Mechanical Actuation of Soft Exoskeletons","OIA","EPSCoR Research Infrastructure","09/01/2018","08/20/2018","Vishesh Vikas","AL","University of Alabama Tuscaloosa","Standard Grant","Chinonye Whitley","08/31/2021","$250,956.00","","vvikas@eng.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","O/D","7217","9150","$0.00","Non-Technical Description<br/>Soft material robotics is envisioned to be the future of robotics that combines the concepts of the Internet of Things (IoTs), wearable sensors, material science and artificial intelligence to fabricate robots that can assist and collaborate with humans. This field is of special interest to roboticists and engineers as it has multiple fundamental challenges and there are tremendous benefits for applications to fields such as agriculture, disaster robotics to assistive rehabilitation. This project will enable researchers from the University of Alabama to enhance their capabilities to develop next-generation soft material exoskeletons (exosuits) stimulated by mechano-neuromuscular actuators through a collaboration with researchers at the University of Pittsburgh. Mechanically and electrically actuated soft exosuits are envisioned to have an impact on the fields of assistive robotics, rehabilitation robotics, and elder care. The research will result in the development of design and control principles for mechano-neuromuscular actuated soft wearable exosuits, thus greatly enhancing life and reducing rehabilitation cost for individuals who suffer from paralysis, stroke, and spinal cord injuries. The applied nature of this research will play an instrumental role in attracting students to STEM fields that include computer science, electrical engineering, mechanical engineering and biomedical engineering.<br/><br/>Technical Description<br/>The proposed project will integrate learning with research to develop design methodologies and control principles for composite fiber-reinforced soft exosuits.  These exosuits will integrate electro-mechanical actuation (motor-tendons) with the functional electrical stimulation (FES) of muscles to provide ease of movement by assistance and rehabilitation. This research will advance the University of Alabama's (UA) rehabilitation research through the development of design methodologies for motor-tendon driven soft exosuits by adapting principles from fields of compliant mechanisms and composite materials for soft structures. The resulting multi-layer soft material composite exosuits, with reinforced fibers, will address stress concentration and distribution problems specific to electromechanical actuators. This will include addressing anchor-point stress concentration and efficient transfer of actuator forces between components. Nonlinear controllers will be developed to integrate electromechanical and neuromuscular actuation in soft exosuits to effect ease of movement. The proposed research will contribute towards the understanding of design principles for soft wearable materials, which are tough to model, and how their behavior and/or interaction varies with the environment of contact. Furthermore, the research will contribute towards the development of next-generation actuation technologies for mechano-neuromuscular actuators. The research will result in hybrid control principles for mechano-neuromuscular actuators that provide wearable soft exosuits with less stiffness. Given the medical resources and interdisciplinary faculty at UA, this proposal will help in building capacity for a wearable robotics and rehabilitation research program in the state of Alabama.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822092","Planning IUCRC at University of Nebraska-Lincoln:  Center for Engineering and Manufacturing Technologies Advancing Food Safety and Security (CAFSS)","IIP","IUCRC-Indust-Univ Coop Res Ctr","09/01/2018","07/09/2019","Theodore Lioutas","NE","University of Nebraska-Lincoln","Standard Grant","Prakash Balan","08/31/2020","$15,000.00","Jeyamkondan Subbiah, Terry Howell","Theo.lioutas@unl.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","ENG","5761","5761","$0.00","The US food and beverage industry employs 1.46 million individuals and is responsible for more than 10% of all manufacturing shipments. Even though sanitation is one of the food industry's biggest production expenses, inadequate sanitation remains one of the greatest threats to America's food safety and security due to the lack of modern technology and consistent application of sanitation policies.<br/><br/>CAFSS will strive to integrate and streamline the entire food supply chain in the US from 'Farm to Fork' and supply safe (pathogen and allergen free), secure (uninterrupted and fully traceable sources), wholesome and plentiful supply of food to the US and Global consumers.<br/><br/>CAFSS objectives will focus on addressing two grand challenges of national importance:<br/><br/>1) Ensuring a stable and sustainable supply of affordable, safe, nutritious food not only for the US but also for the rest of the world.<br/>2) Equipping and empowering US food manufacturers and their supporting industries, to establish highly competitive manufacturing plants in the US and the world.<br/><br/>The US food industry adds commercial value to agricultural products and provides employment opportunities in both rural and urban areas and CAFSS will enhance the overall sustainability and profitability through automation and new private-public partnerships.<br/><br/>The proposed CAFSS will bridge technological gaps, which fall under the general theme of food safety, security and traceability of all raw materials throughout the entire supply chain from 'Farm to Fork'.  Bridging these gaps will require multidisciplinary collaborations among public and private enterprises and groundbreaking research and development within the following fundamental science and engineering platforms:<br/><br/>1) Automation, control and robotics; IoT systems and data integration; <br/>2) New sensors; big data analytics and artificial intelligence; <br/>3) New functionalized surfaces, new materials / coatings  <br/>4) Novel food sterilization technologies. <br/><br/>Automation, control, and robotics minimize human contact with food and greatly enhance food safety. Newer sensors with data analytics enable the food companies to improve food safety, quality, and traceability. Novel functionalized surfaces enhance food sanitation efficiency thereby reducing production downtime, and energy and water requirements.  Novel food sterilization and pasteurization technologies improve food safety with minimal deterioration in food quality and they meet consumers' demand for minimally processed foods.<br/><br/>Research projects will be carried out at CAFSS hub facilities in Nebraska and Georgia and the resulting breakthroughs will allow the US food manufacturing industry to join other industries in terms of efficiency, automation, lower cost, predictability, safety and security of goods supplied.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830730","Research Initiation: Exploring Epistemologies where Engineering Meets Art","EEC","EngEd-Engineering Education","09/01/2018","08/01/2018","Suren Jayasuriya","AZ","Arizona State University","Standard Grant","Edward Berger","08/31/2021","$197,689.00","Nadia Kellam","sjayasur@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","1340","110E, 1340","$0.00","Recently, the combination of engineering and the arts has been championed as an effective educational model for training future engineers. The diverse ideas and skills required from combining engineering and art encourages creative student thinking and opens new, non-traditional pathways into engineering. New domains of engineering such as computer graphics, 3D printing, artificial intelligence, and human-computer interaction require engineers to work with designers, visual artists, and humanities scholars for their science and technologies to have societal impact. Engineers need not just technical skills, but social awareness and communication skills to effectively work across these domain boundaries of art and engineering. Through combining engineering and art curriculum, these needs are addressed in a meaningful way. These programs typically feature an extensive collaboration or design project for students in engineering and the arts to interact and work with one another. However, much research has pointed out the difficulty of these interactions between participants due to the diversity of viewpoint, background, and skills. This project aims to understand theories of knowledge that students utilize in a collaborative engineering and arts environment. This will help develop a better understanding of the nature of interactions at the boundary of art and engineering. Insights that emerge from this work promise to improve the educational experiences of engineers interacting in these programs and foster new, diverse and inclusive ways of working for the engineering workforce. <br/><br/>This project is a comparative study of two subsets of students at Arizona State University: (i) a group of engineering and arts students pursuing a year-long design/capstone project from the transdisciplinary School of Arts, Media and Engineering; and (ii) a contrasting group of engineering students pursuing a year-long senior design project in a traditional Electrical, Computer and Energy Engineering School. Through analysis of these student's experiences, researchers answer a series of research questions on the range and depth of epistemological beliefs held by participants in both populations, and any similarities and differences among the participants. In addition, through discourse analysis of language and terms used by participants and of observation notes of working groups, researchers reveal philosophical viewpoints and how these viewpoints affect communication, offering insight into effective collaboration between engineering and the arts.  The intellectual merits of this proposal include generation of fundamental knowledge into the epistemological viewpoints and interactions in art and engineering learning environments and conceptual frameworks of personal epistemology in education. The broader impacts of this project include pedagogical material for transdisciplinary engineering and media arts courses, public outreach and participation through the Center for Science and the Imagination, and participation in the Digital Culture Summer Institute to introduce a diverse set of middle and high school students to engineering and the arts projects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840510","Planning Grant: Engineering Research Center for Human, Machine, and Network Functional, Symbiotic Integration On Neural Systems (Human Fusions)","EEC","ERC-Eng Research Centers","09/15/2018","09/20/2018","Dustin Tyler","OH","Case Western Reserve University","Standard Grant","Dana L. Denick","05/31/2021","$100,000.00","Christian Zorman, Mark Griswold, Suzanne Rivera","dustin.tyler@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","ENG","1480","124E, 132E, 1480","$0.00","The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program.  Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>This project will develop the foundational plan of an Engineering Research Center (ERC) to form a unique, transformative, and transdisciplinary team that will create, define, understand, and teach the science, technology, ethics, regulatory framework, entrepreneurship methodologies, and societal impact of the rapidly evolving integration of humans and technology. Communication and technology revolutions, such as radio, television, and the internet, have resulted in profound societal changes. The human, however, has basically remained external to the system with technology serving only as a tool. We propose that a conceptual shift toward symbiotic integration of the human with technology will bring the next societal transformation leading to a more connected, global society with new operational models of work, anthropocentric technology, and human-human interaction. We envision a tech-plus, transdisciplinary team of scholars, entrepreneurs, ethicists, and members from partnering institutions (including companies) that will perform convergent research at a new frontier of human incorporation of technology into a sense of self, i.e. a symbiosis between humans and technology. The core is a shift from the current techno-centric approach, where human and technology are separate, to a human-centric technology development paradigm. We seek to shift the societal dialogue from that of a battle between humans and technology (such as artificial intelligence and robotics) to a more productive dialogue of merging the best of humans and technology for the mutual benefit of both.  Symbiotic incorporation of technology requires new interfaces to the human that add multiple sensory connections beyond current audio and visual inputs. This symbiotic relationship will augment human capability with those of technology and networked systems. New, symbiotic technology will radically change the future of work, human learning, human-human interaction, human networks, human health, human capability, and society overall for a safer, more prosperous future. The overall goal is to refine the model sufficiently to be compelling for a sustained research and development effort in an ERC for merging Humans, Machines, and Networks through Functional, Symbiotic, Integration On Neural Systems or an ERC for Human Fusions. Prior significant research shows that the core need of incorporation of technology into a human's sense of self, requires 1) a sense of agency over technology and 2) multi-sensory synchrony with technology. <br/><br/>Strictly, this project is a planning grant to develop the ERC structure and processes that will rationally evolve the relationship between humans and technology. Methods from the Science of Team Science (SciTS) will be employed to establish relationships between committed, energized stakeholders in this new, transdisciplinary effort in human-technology symbiosis and a strategic plan to grow and establish sustainable research capacity. The objectives of the project are to 1) assemble the expertise to define the transdisciplinary, tech-plus framework; 2) develop a process and the collaborative tools for the sustained, focused development and study of the new human-technology paradigm, and; 3) establish a central point of engagement for stakeholders and external communities. This project will foster a new dialogue regarding the evolution of the human-technology relationship. Tangible outcomes from the planning process will include social media networking platforms, a central collaboration and dissemination website, and surveys to gauge stakeholder commitment to and refinement of the symbiotic model of human-technology evolution. Successful realization of a symbiotic human-technology paradigm requires a transdisciplinary approach to address significant scientific, technical, ethical, and social challenges. The transdisciplinary model of the ERC for Human Fusions has a technical core addressing anthropocentric technology, multisensory human interfaces, and connection infrastructure. Expanding around this are disciplines to address ethical questions of symbiotic technology, regulatory frameworks to support the ethical principles, entrepreneurial models to introduce new technology, and sociology to understand how symbiotic technologies impact society. These are highly integrated such that each is integral to the development and understanding of the other. The potential ERC will provide leadership, intellectual resources, the establishment of world-class facilities for responsible and effective scientific discovery, technological innovation, and resources in the new symbiotic sciences for education, research and development and translation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842692","RAISE-EQuIP: Single-Chip, Wall-Plug Photon Pair Source and CMOS Quantum Systems on Chip","ECCS","SSA-Special Studies & Analysis, CCSS-Comms Circuits & Sens Sys","10/01/2018","05/24/2019","Milos Popovic","MA","Trustees of Boston University","Standard Grant","Dominique Dagenais","09/30/2021","$758,000.00","Vladimir Stojanovic, Prem Kumar","mpopovic@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","ENG","1385, 7564","049Z, 057Z, 093E, 094E, 095E, 096E, 100E, 106E, 9251","$0.00","The  amount of new data generated by humanity in the past year exceeds that created in all of human history before.  The processing demands of this data are driving the continued need for greater computational power, in domains including big data analytics, artificial intelligence, and augmented reality, serving technologies including personal, medical, research, engineering, finance, and weather prediction.  As ""Moore's Law"" of the semiconductor industry - which has guaranteed continued advance of computing power in the last 50 years - has ground to a halt in the past decade, new computational paradigms are being sought to remedy this dire situation.  Quantum information technology is the new and ultimate frontier for signal processing and computing and leverages the unintuitive laws of our universe that hold on small scales.  50-100 qubit processors have been developed by Intel, IBM and Google, but quantum optical networks, needed to network them into ""quantum data centers"" in a way similar to their conventional analogues, are missing.  This project aims to fill that gap by developing a new electronic-photonic chip technology and framework to allow creation of electronic-photonic quantum systems-on-chip (epQSoCs).  epQSoCs combine light, electronic circuits, and quantum functions on a single microchip that can provide a widely deployable technology platform for quantum networks.  The project will combine interdisciplinary expertise in photonics, electronic systems, and quantum communications to demonstrate the first epQSoC.  A single-chip, ""wall-plug"" source of quantum correlated photon pairs, this epQSoC is a fundamental building block for more complex epQSoCs and for quantum networks.  By integrating several components and novel capabilities never previously integrated in a single chip, this source will provide new levels of photon-pair source performance.  The interdisciplinary project team will also educate a new generation of engineers in this emerging new technology area to foster innovation, excellence and global leadership in the United States.<br/><br/>A ""wall plug"" single-chip source of photon pairs, a fundamental building block of most quantum photonic systems, will be demonstrated having a high efficiency, rate and reconfigurability to produce factorizable quantum states and allow heralding of pure single photons.  No such integrated device exists despite the fact that a rack-mounted fiber-nonlinearity-based source of this kind for lab use has been commercialized for almost a decade. The proposed project aims to change the quantum technology landscape with the demonstration of a fully integrated single-chip quantum pair source system. The chip photonic circuit will contain photonic elements for pre- and post-source linear pump filtering, a resonant nonlinear pair generator, pump pulse carver to allow active matching of the pump pulse length to the source's resonant bandwidth in order to control the produced photons joint spectral intensity (to yield a factorizable or other engineered biphoton states), and an ultra-low loss interface to fiber. The proposed approach addresses a number of challenges that arise in integration, on-chip filtering, and real-time control. In addition to standalone operation, the pair source will be the first implementation of an electronic-photonic quantum system-on-chip (epQSoC) and a key building block for more complex integrated quantum systems.  The proposed epQSoCs will be implemented in a commercial 45nm CMOS electronic-photonic platform (with potential for integrating single-photon detectors on chip as well).<br/> The project will create the technology framework (block libraries, tools, models and design methodologies) for low-cost, rapid innovation and design of sophisticated epQSoCs. This framework, along with associated educational materials and experiences will help create a new crop of engineers that are capable of tackling the complex, multidisciplinary nature of quantum information systems. Educational and outreach activities will provide exposure and training to a new generation of students and future leaders in this field, with special focus on underrepresented students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811317","Nonparametric Statistical Image Analysis: Theory and Applications","DMS","STATISTICS","05/01/2018","05/08/2020","Rabindra Bhattacharya","AZ","University of Arizona","Continuing Grant","Gabor Szekely","04/30/2021","$224,999.00","","rabi@math.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","MPS","1269","8091","$0.00","Non-Euclidean data are ubiquitous. They arise in many forms such  as digital images for (1) medical diagnostics (MRI, CT scans, DTI of the brain), (2) scene recognition from satellite images, (3) identifying defects in manufactured products, (4) artificial intelligence (robotic identification of objects), etc. Proper geometric descriptions of these require tools from modern differential geometry. Classical statistical methods are inadequate for their analysis; parametric models, which assume the form of the distribution of the underlying data modulo a finite number of unknown parameters, are often misspecified. A model-independent methodology developed by the PI and others has been shown to be very effective in analyzing such data. The present project aims at vastly broadening the scope of this methodology for applications. <br/><br/>A basic component of the methodology proposed is the notion of the Fre'chet mean of a probability Q on a metric space, which minimizes the expected squared distance from a point. The metric space is generally a differential manifold, often provided with a natural Riemannian metric. But it may also be a so called geodesic space of non-positive curvature, including many graphical spaces as well as stratified spaces made up of manifolds of different dimensions glued together. For the methodology to work one must establish (a) the uniqueness of the Fre'chet minimizer and (b) the asymptotic distribution of the sample Fre'chet mean. It is one of the goals of the present project to significantly extend the earlier theory in this regard, opening the way to many new applications. Another important objective is to extend to such spaces the nonparametric Bayes theory of density estimation, classification and regression. One special aim here is to explore an intriguing phenomenon:  in simulation studies with moderate sample sizes, the nonparametric Bayes estimator of the density of Q far outperforms not only the kernel density estimator, but also the MLE when the data are simulated from a parametric model! An understanding of this is expected to lead to a wider and more effective use of the nonparametric Bayes methodology. Finally, the PI proposes to develop a graphical method for robotic vision of objects, with much less computational complexity than that of other methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842567","NSF Nanoscale Science and Engineering (NSE) 2018 Grantees Conference, at  Westin Alexandria Hotel, Alexandria, VA, on December 6-7, 2018","ECCS","EPMD-ElectrnPhoton&MagnDevices, ENG NNI Special Studies","08/15/2018","06/07/2019","Robert Westervelt","MA","Harvard University","Standard Grant","Lawrence Goldberg","07/31/2019","$113,013.00","","westervelt@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","ENG","1517, 7681","7237, 7556","$0.00","The NSF Nanoscale Science and Engineering (NSE) 2018 Grantees Conference is a two-day annual event that brings together nanoscale educators, researchers, and experimentalists from academia, government, and industry to highlight information on the research and education activities funded by NSF NSE grants. The primary goals are to promote dissemination of innovative research progress, to facilitate research partnerships, and to identify future research directions. Panel discussions will be moderated by NSF-funded researchers as well as NSF Program Directors. The conference helps advance the goals of the NSF and the U.S. National Nanotechnology Initiative. Conference materials are made available to the global audience following the event at the website www.nseresearch.org, along with archival information from previous events. <br/><br/>The NSE grantees conference will foster interaction among academic, government, and industry researchers in nanotechnology fields. Keynote presentations and interactive panel discussions on the grand challenges in and convergence of nanotechnology generate opportunities for creative interdisciplinary collaboration. Identification and exploration of future trends in nanotechnology and cyberinfrastructure enable researchers and industry to prepare to fully capitalize on next-generation capabilities. The first day of the conference will focus on Progress in Foundational Nanotechnology and Infrastructure. Topics will include nanoscale modeling and simulation, the use of big data in nanotechnology research, nanotechnological devices and systems, two-dimensional nano-materials, and quantum phenomena in nanoscale systems. The second day of the conference will focus on Progress in Grand Challenges and Convergence. Topics will include the convergence of nanotechnology with a variety of other fields including biotechnology, cognitive science, cyber and artificial intelligence fields, and brain-like cognitive engineering systems. The second day will also include discussions of education challenges and societal impacts of nanotechnology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807368","NSF Workshop: New Vistas in Molecular Thermodynamics: Experimentation, Modeling and Inverse Design","CBET","Proc Sys, Reac Eng & Mol Therm","01/01/2018","05/04/2020","Jianzhong Wu","CA","University of California-Riverside","Standard Grant","Raymond Adomaitis","06/30/2021","$30,000.00","","jwu@engr.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","ENG","1403","7556","$0.00","The project will support a workshop on Molecular Thermodynamics that will be held on the campus of the University of California-Berkeley, from January 7 to 9, 2018. The workshop will bring together technology leaders and junior researchers to develop a roadmap for basic research in response to emerging opportunities and challenges in the Chemical Process Industries. The workshop will foster debate and discussion among scholars from different areas of the Molecular Thermodynamics community to uncover fundamental research questions whose answers will enable overcoming current technological challenges. It will also will also cover topics related to modernizing Engineering Education. Successful development and implementation of new tools for process design and optimization will ensure sustained competitiveness of the U.S. chemical industry while also addressing environmental challenges and promoting energy efficiency.<br/><br/>Recent trends emphasizing environmental sustainability and improved energy efficiency in the chemical process industries have created new research opportunities in the field of Molecular Thermodynamics. By connecting the physicochemical properties of matter at all length scales, Molecular Thermodynamics has been an enabling tool to accelerate scientific innovations, catalyze emerging technologies, and provide core knowledge for design and optimization of diverse industrial systems, ranging from energy production, materials synthesis, and chemical processing to pharmaceutical and environmental applications. The rapid emergence of new methods in computational algorithms, data science and artificial intelligence provide a unique opportunity for innovation and interdisciplinary collaboration by integrating experimental and computational methods into powerful tools for process design and optimization. In addition to defining new research frontiers in the field of Molecular Thermodynamics, the workshop will also address curriculum development and workforce training issues to ensure continued world-wide competitiveness of the U.S. chemical industry."
"1827523","PFI-TT: Acoustic Continuous Condition Monitoring of Manufacturing Machinery","IIP","PFI-Partnrships for Innovation","09/01/2018","07/23/2018","Juan Bello","NY","New York University","Standard Grant","Kaitlin Bratlie","02/28/2021","$200,000.00","","jpbello@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","ENG","1662","1662, 8029","$0.00","The broader impact/commercial potential of this PFI project is in providing the manufacturing sector with advanced solutions for the early detection of machine malfunctions via continuous acoustic monitoring. Machinery malfunctions have significant negative effects on the manufacturing industry, including: unscheduled downtime leading to the under-utilization of equipment and staff; the production of off-spec products leading to waste of finished product and raw materials; as well as costly-repairs and inefficient maintenance schedules. All these effects increase the cost of manufacturing and can result in loss of revenue, directly affecting the margin of profitability, and thus the competitiveness, for these companies. By improving machine condition monitoring and enabling the widespread adoption of predictive maintenance, we believe our solutions can contribute to the growth of the US manufacturing sector, with all the significant ancillary benefits that entails. Better prediction of machine failures could also potentially affect energy efficiency, environmental impact and workplace safety in manufacturing operations. <br/><br/><br/>The proposed project will develop an integrated, Industrial Internet-of-Things (IIoT) solution to continuous condition monitoring of manufacturing machinery. Our solution is centered around a network of low-cost, high quality, remote acoustic sensing devices with embedded artificial intelligence (AI) for sound recognition, that can automatically detect and diagnose the early signs of machine failure. Our novel focus on acoustic emissions, both in the audible and ultrasonic range, means that our sensors are non-contact and thus easy to install, capable of monitoring multiple parts per sensor, and able to produce earlier warnings than those possible with existing solutions. Furthermore, our use of AI for sound recognition results in fast and scalable analytics in real-time with minimal expertise required. We provide a unified cyber-infrastructure integrating edge computing, cloud data storage and an easy-to-use dashboard to facilitate navigation, retrieval and operation. This combination has the potential to result in a disruptive and transformative product that improves machine condition monitoring while significantly lowering the cost of deployment and operation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757885","REU Site: BME Community of Undergraduate Research Scholars for Cancer (BME CUReS Cancer)","EEC","EWFD-Eng Workforce Development","06/01/2018","04/09/2018","Mia Markey","TX","University of Texas at Austin","Standard Grant","Amelia Greer","05/31/2022","$397,808.00","Laura Suggs","mia.markey@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","ENG","1360","116E, 9178, 9250","$0.00","The BME Community of Undergraduate Research Scholars for Cancer (BME CUReS Cancer) REU Site at the University of Texas (UT) at Austin will provide opportunities for a diverse group of talented undergraduates traditionally underrepresented in engineering to make contributions towards the fundamental understanding of the physical principles of cancer development, prevention, and treatment during a ten-week summer research experience. An aim of this research experience is that the Scholars will be inspired to go on in their careers to function in multi-disciplinary teams of researchers from disparate fields in engineering, medicine, biological science, physical science, and healthcare fields to advance human health. The Site's partnership with the UT Medical School and Texas 4000, a non-profit organization that cultivates student leaders and engages communities in the fight against cancer, will help to increase public awareness of cancer research, in particular how physical science and engineering positively impact healthcare.<br/><br/>Cancer is a disease of complexity and engineering principles in particular are developed to study, model, and solve complex problems. The scientific theme of the BME CUReS Cancer Site is leveraging Biomedical Engineering to Open a New Frontier in Oncology. BME CUReS Cancer Scholars will be matched with a project of appropriate scope from 35 different laboratories representing a wide range of research topics pertinent to cancer research, such as biomaterials, drug delivery, optical imaging, medical imaging, and artificial intelligence in medicine. These topics were selected in response to the barriers to achieving progress in cancer research that have been identified by the National Cancer Institute. It is now broadly recognized that understanding how the range of physical laws and principles governing the behavior of all matter are operative in cancer at every scale will be critical to understanding and controlling cancer. Cancer is a disease of complexity and engineering principles in particular are developed to study, model, and solve complex problems. Biomedical Engineering is uniquely poised to make a significant intellectual contribution to addressing these key challenges in cancer<br/>research using an engineering approach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839072","I-Corps: Embedded Cooling","IIP","I-Corps","07/01/2018","06/18/2018","Yogendra Joshi","GA","Georgia Tech Research Corporation","Standard Grant","Andre Marshall","12/31/2019","$50,000.00","","yogendra.joshi@me.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project's technology has its basis in a range of microfluidic cooling products. The potential of the technology is its positive impacts on several computing technologies where it may expand the limits of processing frequency and therefore system speed/performance. Commercial applications that can benefit from the project's technology are broad, such as those requiring the acceleration of high-performance computing in data centers, graphics computing in the fields of artificial intelligence, self-driving transportation, videogame hardware, digital currency (blockchain) technology, augmented and virtual reality, among others. The I-Corps customer discovery activities will also provide useful insights into other potential markets for this project's cooling technology, confirming if there is a need/interest by other industries, such as lasers, concentrated solar photovoltaics, and thermoelectric products, among many potential markets that use liquid-cooling systems at some level. <br/><br/>This I-Corps project is motivated by a technology that consists of an embedded microfluidic cooling layer that offers significantly enhanced heat removal capabilities when compared with current thermal hardware -- this is as a result of eliminating the thermal interfaces and heat spreaders commonly used in microelectronic devices. The heat removal is directly managed by bonding the microfluidic cooling layer to the silicon chip surface through a metallic joint. The resulting micro-cooling layer replaces the need for the heat spreader used in current technologies since the microstructures embedded in the cooling layer have the appropriate feature sizes and layout with an appropriate flow distribution through engineered manifolds for effective heat dissipation. This technology provides the capability of unlocking the clock frequency on micro-processing units by allowing the input of higher voltages to the computing cores, while also keeping the device temperature below design limits. Laboratory experimentation with these microfluidic cooling layers under a wide variety of operating conditions and refrigerants has demonstrated capabilities for removing heat fluxes of up to 500 W/cm2; a 5x increase when compared with the current maximum heat fluxes of commercial, high-end central processing units (CPUs) and graphics processing units (GPUs).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755829","CRII: CIF: Fundamental Limits of Conditional Stochastic Optimization","CCF","Comm & Information Foundations","06/01/2018","12/18/2017","Niao He","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","05/31/2021","$175,000.00","","niaohe@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7935, 8228","$0.00","Decision-making in the presence of randomness has been a fundamental and longstanding challenge in many fields of science and engineering.  In the wake of recent breakthroughs in artificial intelligence, there has been a prominent transition of interests and demands from classical (single-stage) stochastic optimization to multi-stage stochastic programming. In contrast to classical stochastic optimization, multi-stage stochastic problems are known to suffer from the curse of dimensionality, for which efficient universal oracle-based algorithms are not readily available. The goal of this research is to build bridges from classical stochastic optimization to multi-stage stochastic problems by developing an understanding of the fundamental limits of an intermediate class of optimization problems - conditional stochastic optimization - in the hopes of closing the algorithmic and theoretical gaps. Because of its specificity (i.e., it involves nonlinear functions of conditional expectations and lacks unbiased stochastic oracles), this class of optimization problems falls beyond the theoretical and practical grasp of the vast majority of state-of-the-art optimization algorithms. <br/><br/>The investigator will undertake a systematic study of this subject by (i) establishing new techniques for the design of algorithms adapted to different observation schemes and exploitable structures and (ii) developing sample complexities and non-asymptotic convergence analysis for the proposed algorithms. This research will significantly extend the current scope of stochastic optimization in both theory and applicability.  It will also lay the foundation for achieving the long-term goal of bridging to multi-stage decision-making problems and enriching the computational toolbox and theoretical developments for optimization under uncertainty.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829622","CyberTraining: CIU: SJSU Data Science for All Seminar Series","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Leslie Albert","CA","San Jose State University Foundation","Standard Grant","Alan Sussman","08/31/2021","$410,060.00","Scott Jensen, Esperanza Huerta","leslie.albert@sjsu.edu","210 North Fourth Street","San Jose","CA","951125569","4089241400","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","The Nation's research enterprise faces a shortage of data scientists. Expanding the pipeline of data science students, particularly from underrepresented populations, requires educational institutions to increase awareness of data science and inspire a passion for data in students as they begin their academic careers. Currently, few community colleges or undergraduate programs provide training in cyberinfrastructure tools or data science techniques to a broad student population. This project takes a novel approach to augmenting the Nation's data science workforce by training community college and undergraduate students to provide data analytics support to data scientists through a series of ""Data Science for All"" extracurricular seminars. The seminars require no prior data science knowledge, emphasize transferable skills, and present a feasible path into data science-related research and other careers for students from a broad array of disciplines and from underrepresented groups without extending their time to graduation. By increasing the Nation's data science capabilities and the diversity of its data science research workforce, the project serves the national interest, as stated by NSF's mission: to promote progress of science and advance the prosperity and welfare of the Nation. <br/><br/>The goals of this project are to increase undergraduate student awareness of data-driven science and to grow and diversify the population of students trained to perform data wrangling - the data acquisition, transformation, cleaning, and profiling required to prepare data for analysis. According to industry experts, data wrangling is the ""heavy lifting"" of data science, constituting up to 80% of a data scientist's daily work. Shifting this time-consuming effort to trained data analysts free data scientists to focus more of their time on research. The project achieves its goals through the development and delivery of widely consumable, extracurricular seminars providing interactive training on data science concepts and industry-leading data wrangling tools to undergraduate and community college students. Initial seminar topics, selected in collaboration with the project's advisory board, include Python, Jupyter notebooks, Apache Spark, Tableau, and demystifying artificial intelligence (AI). The seminars' focus on data wrangling also introduces students to data preparation documentation - capturing the data provenance needed for reproducible science. This project's contribution to the Nation's data science workforce is broadened through the free and open distribution of its seminar materials and supplemental resources and its online instructor support community. To encourage adoption at Bay Area community colleges and universities, instructor training is provided through co-instruction and a teaching-the-teacher model. The project contributes to pedagogical research by identifying instructional approaches most effective in teaching data science to a diverse population of undergraduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812245","A Search for Neutrino-less Double Beta Decay with nEXO","PHY","NUCLEAR PRECISION MEASUREMENTS","09/01/2018","07/02/2019","Andrea Pocar","MA","University of Massachusetts Amherst","Continuing Grant","Allena K. Opper","08/31/2021","$500,000.00","","pocar@physics.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","MPS","1234","","$0.00","One fundamental question in science today is why the Universe and everything in it is made almost exclusively of matter and essentially no anti-matter when the equations describing Nature lead us to expect a nearly equal amount of matter and anti-matter.  This award supports investigations at the University of Massachusetts, Amherst into one possible origin of this imbalance through the study of whether the lightest fundamental particle in Nature, the neutrino, is made of both matter and anti-matter.   If this is the case, a very rare nuclear process known as neutrino-less double beta decay is possible. This decay, in which a nucleus transforms into another by emitting two electrons and nothing else, would unambiguously determine that neutrinos and anti-neutrinos are the same particle, i.e. that they are Majorana particles.  The work at UMass Amherst focuses on key R&D aspects for the design of the detector for the nEXO experiment, with research carried out in the lab with postdocs and students (graduate and undergraduate), in an academic, research-intensive environment.  The PI and his group are also involved in operating and analyzing the data of the EXO-200 experiment, a smaller scale nEXO predecessor running in New Mexico and holding one of the best sensitivities for neutrino-less double beta decay to date.  The activities supported by this award train human resources of diverse backgrounds and develop technologies that align with strategic sectors for the US, such as data science and artificial intelligence, nuclear medicine and non-proliferation, and national security programs at national laboratories. The UMass Amherst particle astrophysics group has a record of excellence and diversity for undergraduate involvement in research, and the Amherst Center for Fundamental Interactions (ACFI) actively contributes to promoting the physics related to this proposal to the broader physics community.<br/><br/><br/>The core of the nEXO detector is a Time Projection Chamber (TPC), a technology that can identify and suppress with high efficiency most background signals mimicking neutrinoless-double beta decays. The detector will measure the position and energy of each ionizing event occurring inside its volume, as well as finer information about the spatial distribution and sharing of event energy between two detection channels, ionization and scintillation light. Combined with a detector design that minimizes the residual radioactivity in its constituents, this information provides a powerful environment for a neutrino-less double beta decay signal to emerge once all other interactions are properly identified. The UMass Amherst group works on the development of key elements of nEXO. The PI coordinates the design of the nEXO TPC. With his group, he studies novel silicon-based detectors (specifically Silicon PhotoMultipliers, SiPMs) sensitive to the xenon scintillation light (178 nm) that combine good light collection efficiency, minimal radioactive contamination, uniformity of response, and are practically available to cover several square meters of surface in nEXO.   An integral part of the program is a continued participation in the EXO-200 experiment, and the analysis of its data. Of particular interest are refined searches for neutrino-less double beta decay of xenon-136 and xenon-134.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817212","AF: Small: A New Approach to Analysis and Design of Algorithms for Stochastic Control and Optimization","CCF","Algorithmic Foundations","10/01/2018","08/23/2018","Rahul Jain","CA","University of Southern California","Standard Grant","Joseph Maurice Rojas","09/30/2021","$399,999.00","","rahul.jain@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796","7923, 7926, 7933","$0.00","Randomized algorithms for stochastic optimization and control underpin many developing technologies such as Artificial Intelligence (AI), Autonomous Robotics, and Big Data Analytics. Their development is hampered by a lack of suitable mathematical tools. In many cases, current mathematical techniques such as those based on Stochastic Lyapunov theory are rather difficult to use, thus necessitating invention of customized techniques for algorithm design for each problem and its analysis. This project will develop a new class of mathematical techniques, called probabilistic contraction analysis, that are easier to use, and more broadly applicable. The project's aim is not just analysis of existing algorithms, but development of analysis tools with an eye on design. The project outcomes can accelerate development of new algorithms for stochastic control and optimization problems that arise in many important application fields such as AI, Autonomy, Big Data Analytics, etc. The project will train under-represented and/or female PhD students and postdocs, as well as high school students and teachers.<br/><br/>Given a randomized algorithm for stochastic optimization and control, this project views each iteration as applying a random operator, and develops new ""probabilistic contraction"" analysis techniques, created by the investigator, that use stochastic dominance arguments to show convergence to probabilistic fixed points. Specifically, the investigator will develop empirically-inspired algorithms for optimal control of continuous state and action space Markov decision processes, and unconstrained and constrained stochastic optimization problems. The techniques to be developed may be useful for a broader class of stochastic iterative algorithms, and lead to development of a probabilistic fixed point theory of random operators on Banach spaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809399","Topological Insulator Field Effect Transistors for Memory and Sensors","ECCS","EPMD-ElectrnPhoton&MagnDevices","08/01/2018","06/18/2018","Qiliang Li","VA","George Mason University","Standard Grant","Usha Varshney","07/31/2021","$360,000.00","Dimitrios Ioannou","qli6@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","ENG","1517","107E","$0.00","Relentless, Exponential progress on Complementary Metal-Oxide-Semiconductor (CMOS) technology over the last four decades has made possible the design and fabrication of the powerful silicon chips which are the engines of the microelectronics revolution which changed contemporary society: Computers, Smart Phones, Internet of things (IoT), Artificial Intelligence (AI), and the list goes on, there is no aspect of modern life that has not been touched by the silicon chip. Progress on conventional CMOS technology has however slowed down significantly, as miniaturization (according to Moore's Law) is reaching fundamental, physics imposed limits. To make progress possible ""beyond CMOS"", researchers around the world consider new approaches to use new materials (for example, topological insulators), and invent new types of transistors and new types of high-speed, high-density and low-power memory technology. Consequently, the goal of the research in this proposal is to further exploit our understanding of the properties of topological insulator nanowires and thin films to build new-concept field effect transistors with operational principles different than the conventional CMOS technology, while continuing to benefit from the existing vast experience semiconductor industry has accumulated over the years with this technology (CMOS). If successful, the outcomes of the proposed research will also include new memory devices and sensors, made possible by these topological insulator transistors. Graduate, undergraduates and high-school students will have the opportunity to interact with collaborators from Industry and Government Laboratories. <br/><br/>The goal of the proposed research is to design and fabricate Topological- Insulator Field-Effect transistors platform to explore and exploit the potential of gate-controlled topological surface state for applications in new-concept nonvolatile memory and sensor devices. The specific aims of this proposal are: (i) to design and fabricate topological insulator transistors with large on-state current and near-zero off-state current; (ii) to explore gate design and device geometry for achieving robust and efficient control of the spin-polarized electron current; (iii) to exploit the spin-polarized electron current for spin-based logic and nonvolatile memory devices with low-power operation; and (iv) to exploit the resulting devices for enhancing the topological photoelectronic effect for infrared sensors with high sensitivity and selectivity. The research involves preparation of novel topological insulator nanowires and thin films, nanoscale device integration, and characterization, with a focus on achieving in the first instance high-quality topological insulator transistors. The topological insulator nanowires and thin films will be grown at wafer scale for in-situ device integration to achieve clean device interfaces and metal contacts. The topological insulator transistors will be fabricated with engineered gate/source/drain contacts and ferromagnetic insulator/channel interface to achieve: high on/off current ratio, large on-state current and sharp switching, with surface states efficiently tuned by the gate-source electric field. This proposal presents a complete route from materials preparation, to device integration and measurement, to applications focusing on logic transistors, nonvolatile memory and sensors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812478","RUI: Compressive Sensing and Neuronal Network Structure-Function Relationships","DMS","MATHEMATICAL BIOLOGY","06/15/2018","05/16/2018","Victor Barranca","PA","Swarthmore College","Standard Grant","Junping Wang","05/31/2021","$140,301.00","","vbarran1@swarthmore.edu","500 COLLEGE AVE","Swarthmore","PA","190811390","6103288000","MPS","7334","8091, 9229","$0.00","The human brain is a complex network of billions of neurons whose intricate connectivity largely determines perception and behavior. To understand brain function, it is therefore paramount to efficiently measure and analyze neuronal network architecture. However, measuring the connectivity of large neuronal networks remains a challenge both experimentally and theoretically. An often more tractable approach to reconstructing connectivity in complex networks is to instead measure the dynamics of neurons of interest, and then use mathematical approaches to infer the network connectivity. This project will utilize the widespread sparsity found in brain networks to develop an efficient mathematical framework for reconstructing neuronal connectivity from limited measurements of neuronal dynamics. Upon accurately recovering the architecture of neuronal networks, this project will investigate how the sparse structure of natural stimuli impacts the early development of neuronal connectivity and what functional implication this has in the encoding of diverse classes of sensory signals. Analyzing the neuronal dynamics that optimally encode network connectivity and stimulus information, this project will provide new insights into sensory processing and abnormal brain function. In formulating novel methodologies for processing dynamic network data, this project will inform advances in artificial intelligence and prosthetics. This work will actively involve undergraduate students in all phases of research, promoting interdisciplinary scientific collaboration and deepening the scope of applied mathematics education for a diverse spectrum of students.<br/><br/>With the increasing prevalence of network models in the mathematical sciences, accurately measuring network structure and understanding its relationship with network function is of broad scientific importance. In neuroscience in particular, efficiently measuring large-scale brain connectivity and determining its impact on cognitive function is inherently challenging yet fundamental in characterizing the nature of computation in the brain. This project will formulate a novel framework for the reconstruction and characterization of neuronal connectivity by taking advantage of the widespread network sparsity found in the brain and utilizing recent advances in compressive-sensing (CS) theory.  Key facets of the project are to: (1) develop a novel CS-based mean-field approach for efficiently reconstructing sparse connections in physiological neuronal networks based on underlying input-output mappings embedded in the nonlinear network dynamics; (2) analyze the role of the balanced network operating regime in CS reconstruction of recurrent network connectivity; (3) investigate the basis for structural motifs in the visual system through supervised learning of neuronal connectivity aimed at optimized compressive encoding of sparse visual stimuli; and (4) characterize the functional role of  receptive field structure in the encoding of natural scenes through compressive network dynamics and the manifestation of related deficiencies in processing non-natural scenes, such as illusory images. This work will underline how the network dynamical regime impacts the inference of structural connectivity and network inputs from neuronal dynamics, improving the scale over which neuronal connectivity can be determined and providing novel insights into abnormal information processing in the brain.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829008","NRT: Technology-Human Integrated Knowledge Education and Research (THINKER)","DGE","NSF Research Traineeship (NRT), Project & Program Evaluation","09/01/2018","12/16/2019","Laine Mears","SC","Clemson University","Standard Grant","Vinod Lohani","08/31/2023","$2,993,421.00","Amy Apon, Deborah Switzer, Mary Kurz, Joshua Summers, Laura Stanley","mears@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","EHR","1997, 7261","062Z, 9150, 9179, SMET","$0.00","The pervasiveness of new digital technologies in manufacturing is changing the way that data are generated, interpreted and shared over networks of machines, robotics and software systems. This ""industrial internet of things"" holds great promise for improving the quality and productivity of manufacturing in the United States. However, the ability of human workers to effectively interface with such digital systems is limited, potentially leading to disruptions in cognition that may negatively affect output and job satisfaction. This National Science Foundation Research Traineeship (NRT) award prepares master's and doctoral degree students at Clemson University to advance discoveries at the nexus of humans, technology, work, and health, through the convergence of human factors, robotics, cognitive sciences, artificial intelligence, systems engineering, education, manufacturing and social behavioral sciences. This will be achieved through the design and integration of human digital technologies that enhance humans' physical and cognitive interaction and abilities in manufacturing environments. The project anticipates training fifty (50) M.S. and Ph.D. students, including twenty-two (22) funded trainees, from electrical engineering, industrial engineering, computer science, manufacturing, systems integration, psychology, and sociology. These students will interface with a parallel program of undergraduate and technical college students in a controlled manufacturing environment to test deployment and integration across multiple academic levels.<br/><br/>This project responds to the critical need to help shape and better prepare the STEM graduate student of tomorrow through an innovated curriculum that focuses on the new digital and smart manufacturing, automation, and associated data systems.  The training and research takes a human-centered design approach in the emerging digital manufacturing enterprise (i.e., Industrial Internet of Things), by quantifying physical and human cognition and developing augmented technologies (e.g. augmented reality aids for worker empowerment) to improve worker behaviors and attitudes in the manufacturing enterprise.  This project will focus on an automotive industry exemplar (i.e., vehicle assembly operation), employing a factory setting which includes parts manufacture, structural and subassembly operations, robotics, kitting, logistics, and a full-scale vehicle assembly line, together with parallel programs in undergraduate and technical college curricula. The multi-level educational approach is expected to drive improved team communication, generate knowledge on worker behaviors and attitudes, and prepare students for leading implementation of the technologies under study in manufacturing and other industries.  <br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training. The program is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas through comprehensive traineeship models that are innovative, evidence-based, and aligned with changing workforce and research needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828649","MRI: Acquisition of iMARC: High Performance Computing for STEM Research and Education in Southeast Wisconsin","CNS","Major Research Instrumentation, Special Projects - CNS","10/01/2018","09/18/2018","Rajendra Rathore","WI","Marquette University","Standard Grant","Rita Rodriguez","09/30/2021","$681,425.00","Scott Reid, Qadir Timerghazin","rajendra.rathore@marquette.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","1189, 1714","1189","$0.00","The regional resource, iMARC (infrastructure for MARquette Computing), in south east Wisconsin, provides faculty and students with appropriate infrastructure to engage efficiently in the challenges that require parallel computing and facilitate hands-on training in HPS techniques for large-scale compute- and data-intensive analyses.<br/><br/>iMARC is a replacement system that features a modern CPU, large RAM volumes, fast storage, and graphical processing unit (GPU) accelerators, aiming to increase 60+-folding computational power affording a new level of capabilities for research and education for more than 20 major users across Southeastern Wisconsin. It integrates MUGrid (Marquette University Computational Grid) and regional SeWHiP (Se Wisconsin High Performance cyberinfrastructure) for computational sciences, thus enabling the next generation of large-scale compute- and data-intensive research and education at Marquette and neighboring institutions. The high performance data storage of iMARC enables faculty across multiple departments and institutions to tackle new cutting-edge questions. This cluster, coupled with initiatives to support infrastructure renewals, addresses the growing demand for regional scientific computing resources and high-quality training in HPC technologies. iMARC creates a unique resource for parallel computing in data analytics for the challenges in large-scale computing and provides a development environment for large-scale parallel computing tasks that facilitate deployment to XSEDE (the national computing infrastructure). The majority of the users rely on and greatly benefit from the general computational capabilities of the cluster delivered by the CPU-only nodes. The new system will bring the capabilities related to the research in biomolecular modeling and artificial intelligence to a new level. The peak performance of the CPU-only nodes will be at 176 TFLOPS, which is a 30-fold increase over the peak performance of the old machine.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755419","Systematizing Connective Labor","SES","Sociology","02/15/2018","02/13/2018","Allison Pugh","VA","University of Virginia Main Campus","Standard Grant","Toby Parcel","01/31/2021","$205,000.00","","apugh@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","SBE","1331","1331","$0.00","This research investigates connective labor, a novel concept of service work, and examines the impact of contemporary trends in standardization and automation. Some jobs have relationships with people at the core of their work where these relationships require an emotional connection between workers and their charges.  Teachers, therapists, primary care physicians, even prison guards each depend on relationships in service to a larger goal: children learning, patients healing, prisons secure. Connective labor captures the relational work between practitioner and recipient, using their emotional connection to produce an outcome.  Existing research documents the importance of work involving relationships for valuable outcomes in arenas from schools to hospitals; these proven impacts of connective labor make its scarcity, uneven distribution or unreliable performance a social problem. Yet its emotional nature resists efforts to make it more systematic or automated. This research will provide new information to policymakers and the public about connective labor: its variation, its value, and the costs and benefits of making it more systematic, scaling it up, or delivering it by non-human agents. The project will also contribute to ongoing public debates and policy deliberations about automation and work involving relationships, by providing a new visibility for connective labor and the kind of standards and technology that support its excellence. <br/><br/>The goals of the project are: to distill the common practices and principles that comprise connective labor, for practitioners as well as the program administrators and artificial intelligence (AI) engineers who would systematize their work; investigate how workers experience different kinds of systematization, from checklists to robotics; and evaluate how such systematization affects connective labor. Research includes 95 in-depth interviews and ethnographic observations with connective laborers in fields focused on security and/or control, e.g. police; with low-wage workers in home health care; and with what might be called systematizers, e.g., administrators. The results of the study will document characteristics of different kinds of connective labor, outline the risks and rewards of the various ways these are systematized, and explain differing stances towards this work."
"1747381","SBIR Phase I:  Game-Based Psychometric Assessments for College and Career Match","IIP","SBIR Phase I","01/01/2018","01/28/2019","Allison Rosenberg","WA","posed2, Inc.","Standard Grant","Rajesh Mehta","06/30/2019","$224,885.00","","allisonrosenberg@live.com","1906 8th Ave. W","Seattle","WA","981192004","9196723442","ENG","5371","5371, 8031","$0.00","The broader social impact and commercial potential of this Small Business Innovation Research (SBIR) Phase I project reflects the increasing importance and proliferation of artificial intelligence and algorithmic systems in the educational arena and in other domains of human resources. This places the project strategically at the vanguard of scientific discovery, in building a technology tool that will serve society?s goals. In the post-secondary education and training domains, market inefficiencies require excess spending on both sides of the transaction, adversely affecting both supply and demand. As the nation continues to deliberate the value of higher education in the face of $1.3 billion in student loan debt, streamlining the college application process by matching students and institutions based on an applicant?s career goals can reduce costs for students, their families, and institutions alike. Outside the field of education, this project will impact workplaces where employer-employee matching algorithms will be used in recruiting, hiring, training, and team building, across the domains of health, innovation, national defense, and beyond. Ultimately, people analytics platforms have unlimited potential for national and international commercialization, thus generating tax revenue, creating jobs, and otherwise bolstering the economy. <br/> <br/>The proposed project focuses on the deployment of people analytics in the domain of college and career readiness and success. While talent evaluation through such innovative software is in its infancy, the small company's approach to college match will revolutionize how students identify their professional goals and then optimize their college experience for successful life launch. The small company's team of behavioral and data scientists, game designers, and software engineers, are creating video games that provide insights into a college applicant's career potential. Their platform captures massive amounts of micro-behavioral data -- how long players hesitate before making a move, persistence in the face of challenge or frustration, ability to parallel process -- to create a psychographic profile of the player's career-critical characteristics, including traits like competitiveness, extroversion, risk tolerance, and other enduring, personal traits difficult to discern through resumes and traditional, college application materials. Once a student's career objectives become clear, the company's proprietary recommendation engine suggests educational institutions and curricula best suited to attain the player?s career goals. In addition to expanding the game platform and collecting player data to develop and fine tune predictive algorithms, this project also will examine factors that influence a player's response to the insights and recommendations the platform provides. This work will investigate the impact of transparency, or ""scrutability"", of game-based assessments and implement visualization tools that promote the player's understanding of why specific recommendations are made."
"1804024","Inter-comparison of Direct Quantification and Areal Micrometeorological Methods to Investigate the Transport and Fate of Methane from Heterogeneous Sources in Natural Gas Fields","CBET","EnvE-Environmental Engineering","09/01/2018","07/27/2018","Derek Johnson","WV","West Virginia University Research Corporation","Standard Grant","Karl Rockne","08/31/2021","$321,788.00","Omar Abdul-Aziz","derek.johnson@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","ENG","1440","9150","$0.00","The use of natural gas is increasing worldwide. Methane, the primary component of natural gas, is a highly potent greenhouse gas (GHG) that contributes to climatic changes.  Recent research suggests that methane can be released from natural gas fields, but large uncertainties in these data do not allow accurate quantification of the amount. The proposed research will reduce this uncertainty and advance knowledge in measurement methods and modeling tools for the quantification of methane emissions in natural gas fields. The project will support graduate and undergraduate researcher from underrepresented engineering students in rural Appalachia, addressing societal needs for broadening participation in science and engineering. If successful, the results of this research will help protect the Nation's energy security by potentially influencing regulatory activities and improving the efficiency of natural gas production.<br/><br/>Due to maturation of technologies such as directional drilling and enhanced recovery, natural gas production continues to grow in the U.S. However, the magnitude and fate of leaking methane emissions from natural gas fields remain highly uncertain and unresolved. Recent studies show significant variations in methane losses, from less than 1% to 17% of the total natural gas produced. These large variations are generally attributed to uncertainty in top-down (indirect aerial fluxes or downwind measurements) and bottom-up (direct component level measurements and emissions factors) estimates, which include measurement uncertainty and a general lack of high fidelity data sets. The goal of this research is to reduce these uncertainties by collecting and analyzing temporal top-down measurements of methane fluxes and environmental variables based on eddy-covariance flux techniques and Gaussian plume measurements in concert with direct bottom-up measurements. This research will employ advanced data analytics using empirical models and artificial intelligence to improve the mechanistic understanding of methane emissions in natural gas fields across West Virginia and the greater Appalachian region. Accurate quantification with indirect measurement techniques would decrease the need for the labor-intensive leak detection schemes currently employed. The research findings will be broadly disseminated through peer-reviewed publications and conference presentations. The researchers will leverage industrial collaborations at the Marcellus Shale Energy and Environmental Laboratory (MSEEL) in Morgantown, WV, to communicate the research outcomes. The research group will also work with the West Virginia University Energy Institute to develop outreach materials and conduct seminars in the Marcellus region.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761178","Supporting Student Planning with Open Learner Models in Middle Grades Science","DUE","ECR-EHR Core Research","08/15/2018","11/19/2018","James Lester","NC","North Carolina State University","Standard Grant","Andrea Nixon","07/31/2021","$1,499,183.00","Roger Azevedo","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","EHR","7980","8244, 8817","$0.00","This project is supported by the Education and Human Resources Core Research (ECR) program, which supports fundamental research in STEM learning and learning environments.  Self-regulated learning requires learners to set learning goals, plan how to achieve them, monitor the success of their plans, and make needed changes.  Critical gaps exist in our understanding of how and to what extent middle school students can engage in self-regulated learning during science inquiry activities.  Analogous gaps exist in our understanding of how to help students become self-regulated learners.  Computer-based learning tools might help fill these gaps, but these tools are usually ""closed,"" meaning that how the software interprets students' knowledge and progress is hidden from the student.  In contrast, ""open"" learning tools provide students with understandable, visual representations of their knowledge and progress.  Such ""open learner models"" may be useful for helping students develop self-regulated learning skills.  For example, by providing easy-to-understand representations of student progress, open learner models may support the self-regulated learning processes of goal setting and planning. This project focuses on the design, development, and investigation of open learner models for student goal setting and planning in middle school science. The project will explore how students engage in goal-directed learning behaviors using data from eye tracking, log files, and think-aloud exercises.   It is anticipated that the project results can advance the goal of high quality STEM learning experiences for all students.<br/> <br/>A major project goal is improving students' problem-solving abilities and learning outcomes through a theoretically grounded, data-driven open learner model. The project is designed to integrate an open learner model into the Future Worlds science learning environment for middle school ecosystems education. A culminating between-subjects experiment will be conducted to compare a baseline version of Future Worlds that does not have an open learner model (control condition) with a version of Future Worlds that has an embedded open learner model (experimental condition). The project will use state-of-the-art artificial intelligence computational frameworks to recognize students' goals and plans from observations of their problem-solving activities in an online learning environment.  It is hypothesized that the open learner model will yield better student learning outcomes, with improved science problem-solving skills, increased science content knowledge, increased metacognitive awareness, and enhanced science self-efficacy.  The project is designed to make significant contributions to both theory and practice of self-regulated learning. The project's aim is to make contributions to foundational knowledge and theory by advancing our understanding of how to improve science learning with open learner models: (1) With a focus on student goal setting and planning in technology-rich learning environments, the project aims to formulate an empirically-based theoretical framework for open learner model-enhanced learning that addresses both cognitive and metacognitive components of middle grades science education. (2) By conceptualizing goal setting and planning to account for self-regulated learning in the context of science problem solving, the project is designed to create a rich framework that connects goal setting and planning to students' problem solving and metacognitive processes. (3) By expanding to fine-grained process data, the project is intended to make methodological contributions that will enable the field to go beyond self-report measures, which have long dominated research on self-regulated learning. (4) The project plans to produce learning analytic techniques that yield predictive models of goal setting and planning in science problem solving, as well as open learner models to effectively support student goal setting and problem solving.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1754610","Collaborative Research: Origin and Evolutionary Divergence of the Pancrustacean Brain","IOS","Organization","07/01/2018","07/25/2019","Gabriella Wolff","WA","University of Washington","Continuing grant","Evan Balaban","06/30/2022","$318,690.00","","gabwolff@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","BIO","7712","1096, 9178","$0.00","It is still unknown when brains first appeared during the early history of life. The ways in which major brain parts that are structurally distinctive have changed over evolutionary time are also poorly understood. These knowledge gaps are partly due to the fact that fossil brains are rare and have been difficult to study. This project features scientists from three collaborating laboratories who will pool their resources to identify a set of invertebrate brain centers that mediate learning and memory. Structural and functional similarities and differences among these areas will be established across modern insect and crustacean species. The major question this research is answering is whether these brain centers share common genetic and computational attributes due to the brain?s fundamental organization being inherited by the descendants from a common ancestor; or, because brains that have arisen independently in different invertebrate groups are not able to perform certain functions unless brain areas that give them these same abilities have also arisen independently. These questions will be answered by precisely measuring the brain structures in fossilized invertebrate animals and comparing their basic arrangements with modern counterparts. The broader impact of this research will be to identify invertebrate proxies of the learning-and-memory brain centers found in vertebrate animals alive today, including humans. Identification of such proxies will inform us about how brains have evolved, and will contribute to a broader understanding of how memory centers are organized. The results will impact theories of, and research on, neural networks and artificial intelligence, and at the same time the scientists carrying out this research will develop novel strategies for identifying genealogical correspondence of brain structures across a very broad range of species. Brains analyzed for this research will be digitally reconstructed in 3D and uploaded to an open-source database for education and research purposes. The research will also provide advanced neuroscience structural analysis and genomics training to students from diverse backgrounds.<br/><br/>The neuronal organization and circuit properties of insect mushroom bodies are well known, as are their functional properties for learning and memory. While the existence of mushroom-body-like centers exist across arthropods, it is not known whether these phenotypically or genotypically correspond to the centers in insects. The planned research will identify mushroom body-like centers across a broad range of species, analyze their discrete neural arrangements, circuit organization, and molecular attributes. These comparisons will identify the species within and outside Arthropoda that possess functional and morphological correspondences in these structures. Transcriptomics will address whether phenotypically-corresponding centers share common genomic attributes, and whether there are unique genetic networks that define arthropod mushroom bodies or whether these networks differentiate mushroom bodies in different groups of arthropods such as in insects and crustaceans. The identification of broad phenotypic and genotypic homology of these centers across a broad phyletic spectrum would suggest an ancient origin of these learning and memory centers. Equally intriguing would be results suggesting convergent evolution of learning and memory centers across taxa.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819850","SBIR Phase I:  Developing a Single-Visit System to Screen, Diagnose, and Treat Cervical Neoplasia","IIP","SBIR Phase I","07/15/2018","07/16/2018","Joseph Carson","CA","Pensievision, Inc.","Standard Grant","Henry Ahn","03/31/2019","$224,289.00","","joe@pensievision.com","5820 Oberlin Drive, #104","San Diego","CA","921213717","8582554529","ENG","5371","5371, 8038","$0.00","This SBIR Phase I project will develop an innovative 3D medical imaging technology for early-stage detection and analysis of cancers, initially focusing on detecting pre-cancer cervical lesions. This project will advance from creatively assembling existing active optics hardware to inventing a new microfiber-based active optics system to circumvent limits of space confinement and 3D resolution. The project's fundamental strategy for identifying pre-cancers of the cervix, which relies on a macroscopic 3D digital analysis combined with microscopic cell evaluation, is naturally amenable to artificial intelligence technologies. The versatility of this imaging platform enables the resolving of medical diagnostic challenges in wealthy settings and the resolving of cost-saving barriers in resource-limited settings. The imaging technology can be extended beyond medical practice to other scientific and industrial disciplines. Potentially, this project has an immediate impact on saving lives and costs via the early detection of fatal diseases. Additionally, the data and knowledge acquired developing and implementing this imaging system provide opportunities to meaningfully develop new computational strategies for educational, engineering and industrial interests. The innovative, commercially viable, platform technology offers opportunities for significant, tax-revenue-generating global profits, for future technology application spin-offs, and for producing high technology jobs for U.S. citizens.<br/><br/>The project innovation will voyage from state-of-the-art 3D software development to the creation of a new type of fiber bundle imager with an electronically controlled actively focusing lens. Combined, these tasks achieve the creation of a miniaturized 3D imaging system capable of navigating confined spaces within the body, such as inside the cervix opening. For all prototypes developed for this project, final 3D renderings are enabled by proprietary 3D rendering software, capable of quantifying tissue color, volume and shape at the macroscopic level, while also evaluating cell size and approximate shape at the microscopic level. In wealthy settings, this system would be desirable for implementing a single-phase cervical cancer screening strategy to replace the current two-phase approach, which requires Pap smear and/or human papilloma virus assay, followed by the more expensive colposcopy in the case of an abnormal result. In resource-limited regions and/or regions with difficult-to-reach populations, where it is challenging to get patients to return for a follow-up visit, the technology would offer a low-cost, pre-screening method that only requires a single visit. This 3D imaging system could be combined in the future with a therapeutic agent administered at the time of diagnosis, thus offering a single-visit, screen-diagnose-and-treat method.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806440","Rare Event Searches at MIT","PHY","NUCLEAR PRECISION MEASUREMENTS, Particle Astrophysics/Undergro","08/15/2018","07/27/2020","Lindley Winslow","MA","Massachusetts Institute of Technology","Continuing Grant","Jonathan Whitmore","07/31/2021","$711,641.00","","lwinslow@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1234, 7235","075Z, 7483","$0.00","Despite its successes, the Standard Model of particle physics cannot be the end of the story. There are too many features that the model does not explain, including the origin of dark matter, why neutrino masses are so light, how the matter-antimatter asymmetry arose, and why Charge-Parity-violation only appears when weak forces are concerned.  These are the questions that motivate the Rare Event Search Group at the Massachusetts Institute of Technology (MIT). This award provides support for two beyond-Standard-Model searches: the axion search ABRACADABRA, and the neutrinoless double-beta decay (NLDBD) search KamLAND-Zen and related R&D project NuDot.  ABRACADABRA will undertake a search for axions, which are one of the particles postulated to form dark matter.  A demonstration of neutrinoless double-beta decay would address the questions of why neutrino masses are so light and what produces the matter-anti-matter asymmetry in the universe.<br/><br/>The program will deliver several beneficial aspects to society through its integration of undergraduates into the program. The multidisciplinary aspects of the projects are appealing to students across a range of majors, exposing students to particle and nuclear physics when they might otherwise have missed the opportunity. The NuDOT and ABRACADABRA detectors are being developed in close association with scientists at local companies. This exposes the undergraduates in the group to alternative career paths as well as promoting strong ties between academia and industry<br/><br/>KamLAND-Zen will be the first ton-scale NLDBD experiment. NuDot, which is a potential upgrade path, can open the door to even large scales. On the other hand, ABRACADABRA is leading the way in a new wave of ""table top"" beyond-Standard-Model experiments. This is a largely unknown experiment, because it is about to bring out first results. ABRACADABRA brings discovery-level particle physics back to the university. The high intellectual merit of the proposed studies is demonstrated by the excellent alignment of the physics questions with the ""science drivers"" identified by the Particle Physics Program Prioritization Panel (P5). This proposal addresses: 1) the pursuit of the physics associated with neutrino mass; 2) the identification of the new physics of dark matter; and 3) the exploration of the unknown for new particles, interactions and physical principles. This work is similarly well-aligned with the Long Range Plan of the Nuclear Science Advisory Committee (NSAC), which made the call to move quickly on NLDBD experiments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1753228","CAREER: Enabling a Rich Astro-particle and Exotic Physics Program in DUNE","PHY","HEP-High Energy Physics","05/01/2018","05/28/2020","Georgia Karagiorgi","NY","Columbia University","Continuing Grant","Saul Gonzalez","04/30/2023","$400,000.00","","georgia@nevis.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1221","075Z, 1045, 7483","$0.00","The Standard Model of particle physics was a formative intellectual development of 20th century physics.  While the discovery of the Higgs mechanism in 2012 was a crowning achievement for the Standard Model, many mysteries remain, including the role of the elusive neutrino. Neutrinos are elementary particles that rarely interact with ordinary matter.  The Standard Model predicts three types of massless neutrinos.  However, experimentally, we know that neutrinos do have very small masses, and yet they permeate the universe.  Because they have mass, they can change from one type to another.  Measuring properties of these changes, and comparing them to theoretical predictions, provides a promising pathway to discover how neutrinos shape our universe. To that end, the neutrino community is embarking on a challenging quest to complete the picture of neutrino physics through the Deep Underground Neutrino Experiment (DUNE), which will be a massive, 40,000-ton instrument optimized to detect neutrino interactions about a mile underground at Sanford Lab in South Dakota.  This facility, being built during the next 10 years, will observe interactions of neutrinos produced at Fermilab and traveling 800 miles to DUNE and, due to its large volume, can also detect particles coming from astrophysical sources.  This work seeks to develop ways to look for and identify signals for new physics.<br/><br/>The DUNE experiment offers a unique opportunity for a rich astro-particle and exotic physics search program, including: observations of low-energy astrophysical neutrinos, e.g. from supernova core-collapse, thus lending itself to multi-messenger astrophysics, and searches for other rare processes, e.g. proton decay and neutron-antineutron oscillation. If observed, these signatures would have profound implications for particle physics, astrophysics, and cosmology. The rarity of these signals requires continuous, high-resolution readout and processing of Time Projection Chamber (TPC) data from the entire DUNE detector. The resulting data rates for such a data acquisition (DAQ) scheme are prohibitively large and create a challenge: to develop readout and DAQ systems that are capable of significant data reduction and efficient self-triggering with zero deadtime. This award will carry out a unique and ambitious R&D program to develop novel readout and triggering techniques involving Convolutional Neural Networks (CNNs) deployed on FPGA devices, including a demonstration of some of these techniques at the upcoming SBND experiment, beginning in 2019.<br/><br/>This CAREER award will strengthen the undergraduate research experience of Columbia University students, including underrepresented groups. The project will also provide unique opportunities of involvement in cutting edge research in readout electronics, detector R&D, and computer science applications for data handling and data analysis, and further promote interdisciplinary research opportunities for physics and non-physics majors through a Seminar Series developed and organized by the PI.  The award will sponsor an additional slot in Nevis Lab's Research Experience for Undergraduates (REU) Program for two years, specifically for non-physics majors interested in particle physics. Outreach to the public and local high-school students will also be carried out through lectures at local high schools and the Nevis Science-on-Hudson public lecture series. Recruited summer high-school students will develop a virtual reality visualization of a supernova burst, as ""seen"" in a Liquid Argon TPC. This visualization will be demonstrated annually at the World Science Festival.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834323","EAGER: Toward a General Framework for Optimal Experimentation in Computational Cognition","BCS","Methodology, Measuremt & Stats, Cross-Directorate  Activities, Perception, Action & Cognition, Animal Behavior","09/15/2018","08/09/2018","Woojae Kim","DC","Howard University","Standard Grant","Michael Hout","08/31/2020","$299,794.00","","woojae.kim@howard.edu","2400 Sixth Street N W","Washington","DC","200599000","2028064759","SBE","1333, 1397, 7252, 7659","041Z, 075Z, 1333, 7252, 7659","$0.00","Cognitive science aims to gain detailed insight into the underlying mechanisms of cognitive tasks. To achieve this, researchers change factors affecting a task in an experiment and observe responses. One way to learn from such an experiment is to build and test a computational model of stimulus-response relationships. However, the level of detail with which a model describes a task requires as much detail in supporting data. As observations from experiments are often expensive (e.g., child subject), this is a rather significant barrier. The method of optimal experiments can be a solution. It can optimize the selection of stimuli to maximize inference from responses. Nonetheless, the difficulty in applying the method to each new experiment has been a stumbling block. This project proposes to lay the foundation for a general framework for optimal experiments. The goal is to make it applicable to a wide range of modeling problems in cognitive science. This will help cognitive scientists to develop quantitative accounts of cognitive tasks effectively. Further, the method has the potential to accelerate scientific discovery broadly in social and behavioral research.<br/><br/>Conducting cognitive science experiments guided by optimal interaction with subjects toward a clear, quantified inference goal is a powerful idea. Such a method is particularly enticing for behavioral experiments in which the amount of noise in response is so great as to require many repeated measurements. Despite its groundbreaking potential for cognitive modeling research, the method of optimal experimentation is out of reach for most researchers in the field. The formidable task of implementing it for each unique experimental paradigm has been an obstacle to the realization of the methodology's promising power. The project focuses on establishing the technical feasibility of optimal experiments in arbitrary cognitive modeling contexts. The proposed research will define the need for the methodology in the field clearly, identify suitable computational strategies, and test alternative algorithms in simulation studies. The performance of algorithms under consideration will be evaluated on a testbed of modeling paradigms whose successful treatment would transfer to a wide range of similar problems. The project aims to create a tangible blueprint for a general-purpose methodology for optimal experimentation in computational cognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816874","AF: Small: Foundations for Data-driven Algorithmics","CCF","Special Projects - CCF","06/15/2018","06/04/2018","Yaron Singer","MA","Harvard University","Standard Grant","A. Funda Ergun","05/31/2021","$499,947.00","","yaron@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","2878","075Z, 7923, 7926","$0.00","The traditional approach in optimization assumes that the underlying objective is known, but in many real-life applications, the true objectives are not known and learned from data. This gap between theory and practice turns out to be quite dramatic, and leaves us without guarantees on the performance of optimization algorithms in such applications. The goal of this project is to develop a theory for algorithms whose input (i.e., the objective) is learned from data, and design algorithms that perform well in these settings. The technical challenges in this space are highly non-trivial, but their solution would dramatically impact our thinking in computer science and result in major advancements in AI. The project develops courses in optimization and data science that foster an interdisciplinary approach. The project will involve mentoring undergraduate and graduate students from underrepresented groups and promote an open access research culture. The investigator will develop new interdisciplinary connections through courses, seminars, and workshops with the goal of promoting a discipline of researchers working on algorithms for the information age.<br/><br/>In light of a recent line of impossibility results initiated by the investigator, the goal of this project is to investigate alternative notions of optimization that can facilitate desirable guarantees for data-driven optimization. The first direction in this project considers optimization from adaptive samples. The general notion of adaptivity is surprisingly under-explored, and advancement on this front can have a tremendous impact both on theory and applications. A complementary direction is to consider algorithms that are given samples on a training datasets, and seek to approximate the optimal solution of the testing dataset, drawn from the same distribution. Finally, the last direction considered is that of optimization from pairwise comparisons.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827993","Standard Grant: Productive Ambiguity in Classification","SES","Information Technology Researc, IIS Special Projects, STS-Sci, Tech & Society","08/01/2018","06/17/2019","Beckett Sterner","AZ","Arizona State University","Standard Grant","Frederick Kronz","07/31/2021","$158,162.00","Joeri Witteveen, Joeri Witteveen, Liz Lerman, Nico Franz, Manfred Laubichler","beckett.sterner@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","SBE","1640, 7484, 7603","075Z, 1353","$0.00","This is a project in the history and philosophy of biology that has very substantial ramifications for data-intensive science. The PI will investigate how the history of taxonomy can shed new light on the value of ambiguity for science in the domain of data-intensive science. The project focuses on detecting trade-offs in the value of ambiguity for scientific language as a function of changing social contexts. Accurate disambiguation relies on a shared background of knowledge and abilities, which may prove inadequate as concepts spread into new contexts or a community grows larger and more heterogeneous. The project will fund graduate and undergraduate research assistants to analyze a text corpus drawn from two centuries of history in biological taxonomy. It will also support public events and the creation of educational materials addressing the theme of productive ambiguity in naming and classification.<br/><br/>This project will implement an integrative conceptual framework enabling empirical investigation of ambiguity in linguistic settings. It will use an information-theoretic framework from cognitive pragmatics to quantify ambiguity in a way that is open-ended enough to accommodate a wide range of phenomena shaping human language and communication while also reflecting the specific constraints required by computers. It will provide novel tools for tracking changes in language at the level of populations rather than individuals while remaining sensitive to underlying social institutions and individual differences. Results from this project will open new perspectives on the history of types and subspecies in systematics, and it will inform contemporary debates about the virtues of maximal determinacy in the computational representation of human language and meaning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1805043","Collaborative Research: Engineering Architecture for Tissue Models","CBET","Engineering of Biomed Systems","08/01/2018","07/23/2018","Rebecca Carrier","MA","Northeastern University","Standard Grant","Aleksandr Simonian","07/31/2021","$162,225.00","","rebecca@coe.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","ENG","5345","9102","$0.00","This project focuses on developing a screen printing process to build models of both neural networks and the innervated colon (i.e., a colon with imbedded nerves to mimic the natural structure). The screen printing process has been around for thousands of years and is used to make art, to mass produce t-shirts, and to make microelectronics. It is simple, low cost, reproducible, and scalable, making it ideal for printing highly accessible tissue models. While there have been many exciting approaches to print tissue models, most involve expensive equipment and subject the cells to either ultraviolet rays or shearing through fine needles, both of which can impact cell survival and behavior. The objective of this project is to determine the resolution and reproducibility of the screen printing approach and then use the process developed to make the two new models. The neural network model (based on human derived stem cells) will lay the groundwork to study diseases and investigate therapies relevant to nerve tissue in a more physiology-based/meaningful way that is still suitable for high throughput systems. The colon model (consisting of epithelial cells, goblet cells producing mucous, immune cells, and neurons) will provide one of the first innervated gut models that is suitable for high throughput techniques. High throughput screening opens the possibilities of investigating the interplay between these cells in the development of food allergies and conditions such as colitis. The research will provide new tools for researchers interested in understanding the complex cross-talk between cells in these tissues and developing new therapies for insults to these systems. The screen printing technique will also be used to engage young researchers. Screen printing demonstrations will be built into outreach programs in schools and utilized to educate researchers in classrooms and labs. Screen printing is exceptionally familiar and ""user-friendly,"" thus a great platform for engaging and keeping people engaged in science. For example, Jello will be screen printed on edible paper to explain methods to make tissues as part of outreach program demonstrations in the Baltimore and Boston public schools. Educational and outreach efforts will be complemented by a website that highlights scientists from diverse backgrounds and the accessible, low tech parts of their work, like screen printing.<br/><br/>The objective of this project is to determine the resolution and reproducibility of a screen printing approach to making highly scalable hydrogel-cell tissue models and to then use the system developed to make new models of neural networks and the innervated colon.   The investigators hypothesize that screen-printing will provide a cost-effective approach to making tissue models that surpass those made via bioprinting approaches in terms of ease, cost and cell survival and thus enable more relevant models of tissues from cells that are highly sensitive to shear.  The Research Plan is organized under four aims.  THE FIRST AIM is to characterize the screen-printed materials (PLL(poly(L-lysine))/protein and hydroxy appetite-based gels functionalized with PEG-thiol groups printed on glass slides) with regards to the resolution, repeatability and lamination of layers.  Expected aim outcomes include: determining the gelation time and mechanics of the different materials, confirming the concentration and distribution of proteins on the gels, determining the strength of lamination between similar and different layers, and knowing the limits on how thin gels can be reproducibly screen printed.  THE SECOND AIM is to characterize the multilamellar, patterned gels with regards to architecture including resolution and reproducibility of features in the printing plane.  Expected aim outcomes include: determining the finest resolution that can be reproducibly achieved for a range of gels, confirming the concentration and distribution of proteins on and in the gels and determining if there are any types of features that are more or less easily printed at high resolution.  THE THIRD AIM is to determine the relationship between printing resolution, cell survival, and cellular behavior using human induced pluripotent stem (iPS) cells and human iPS-derived neurons to build neural circuits including excitatory and inhibitory neurons.  Expected aim outcomes include:  knowing which hydrogel augments iPS cell survival the most, knowing the impact of the matrix on the genetic and phenotypic cellular behavior of iPS and their progeny, knowing the impact screen printing has on the genetic and phenotypic cellular behavior of iPS cells and their progeny and developing a reproducible, scalable system for high throughput screening of 3D neural interactions that lead to neural circuits.  THE FOURTH AIM is to build a model of the innervated colon to investigate the application of screen printing in a multicellular, physiologically relevant, high throughput model.  Cell lines include enterocytes and goblet cells derived from Caco-2 cells, primary human intestinal epithelial cells, immune cells (dendritic cells) and neural cells (from Aim 3).  Matrix materials include PLL-PEG gel systems printed with laminin and collagen I.  The matrix optimized in Aim 3 will be used for the neural layer.   Expected aim outcomes include:  the development of a novel gut/colon model 1) that lays the foundation for study and enhanced understanding of the role of the enteric nervous system, 2) that, even without the neural component, will be one of the first colon models that can be prepared in a high throughput manner and interface with real time electrical recording and imaging modalities without a substantial investment in materials or equipment and 3) that lays for foundation for adding the next critical component--the microbiome.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810270","GOALI: Collaborative - Magnetoelectric Nanodevices for Wireless Repair of Neural Circuits Deep in the Brain","ECCS","SSA-Special Studies & Analysis, GOALI-Grnt Opp Acad Lia wIndus, EPMD-ElectrnPhoton&MagnDevices","10/15/2018","09/17/2018","Sakhrat Khizroev","FL","Florida International University","Standard Grant","Usha Varshney","07/31/2019","$450,000.00","Xiaoming Jin, Jeffrey Horstmyer","skhizroev@Miami.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","ENG","1385, 1504, 1517","107E, 108E, 1504, 8028","$0.00","The GOALI project at the intersection of engineering and medicine aims to address the following gap in the broad area of neurodegenerative diseases. Stimulation of the neural network by electric fields can repair the abnormal neural activity responsible for various neurodegenerative diseases such as Alzheimer's Disease (AD) and many others. Further, recently it has been shown that electric fields have fundamental effects on cell fate and neurogenesis. However, the existing approaches such as (1) direct deep-brain stimulation (DBS) by establishing direct electrical contact to the neural network and (2) less-invasive indirect transcranial magnetic stimulation (TMS) don't provide spatial and temporal resolutions required for adequate control of the neural network at the cellular level to effectively cure these diseases using electricity, without causing any devastating side effects. This project fills this gap by implementing a nanotechnology approach, according to which magnetoelectric nanoparticles (MENs) are used to combine the main advantages of electric and magnetic fields to enable wirelessly controlled high-efficacy, high-specificity and high-selectivity stimulation of selective regions in the brain to treat specific neurodegenerative diseases without any side effects.  The potential applications are far-reaching into engineering electromagnetic and multiferroic nanoparticle-driven systems which could impact the emerging field of personalized precision medicine, cognitive neuroscience, neuroimaging, clinical neurology, and psychiatry. The proposed system can help reverse engineer the brain and thus open a pathway to fundamental understanding of the brain. An important component of the project is to motivate underrepresented minorities to pursue cross-disciplinary degrees at the intersection of engineering and medicine.  A special emphasis will be made to attract local K-12 and undergraduate students to continue their research at FIU and Indiana University.<br/><br/>The GOALI proposal aims to conduct comprehensive studies to engineer magnetoelectric nanoparticles (MENs) based system for wireless stimulation of local regions deep in the brain to repair disease specific impediments. MENs can bridge local intrinsic electric fields deep in the brain with magnetic fields and thus enable an external control of local electric stimulation for repairing neural circuits locally. Like traditional magnetic nanoparticles, MENs can be used as image contrast agents in magnetic resonance imaging and navigated across the blood-brain barrier via application of magnetic field gradients.  In addition, unlike traditional nanoparticles, MENs display an entirely new property due to the presence of a non-zero magnetoelectric (ME) effect. The ME effect, which exists due to coupled magnetostrictive and piezoelectric components, allows to efficiently couple intrinsic electric fields deep in the brain to magnetic fields which in turn can be wirelessly controlled from outside the skull. Thus, MENs allow to use d.c. and a.c magnetic fields for separating the two functions, (i) application of a d.c. magnetic field gradient for image-guided navigation of MENs across BBB and into a disease-specific local region(s) and (ii) application of an a.c. magnetic field to stimulate this local region(s) locally via inducing local a.c. electric fields, respectively. Based on the physics of metastable systems, using a system of electromagnets, the nanoparticles can be effectively maintained in a quasi-diamagnetic state and thus moved to any point deep in the brain for further local electric stimulation via application of a.c. magnetic fields. Due to the ME effect, the image provided by MENs not only contains structural information but also reflects a local electric field due to the neuronal activity. All these effects will be studied in vitro and in vivo using animal models to understand field-controlled local effects of MENs on the underlying mechanisms of activation of neurons and synapses, the cortical neuronal activity, neuronal excitability, and synaptic transmission.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752355","CAREER:  Human motor inhibition: a neural race between motor emission and cognitive control","BCS","Cognitive Neuroscience, Perception, Action & Cognition","07/01/2018","09/18/2019","Jan Wessel","IA","University of Iowa","Continuing Grant","Kurt Thoroughman","06/30/2023","$438,847.00","","jan-wessel@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","SBE","1699, 7252","1045, 1699, 7252","$0.00","The ability to rapidly stop one's actions even after their initiation is paramount to survival in modern society. Be it in benign scenarios (checking the swing of a baseball bat) or in life-threatening scenarios, rapid action-stopping is a core human ability that is highly relevant in everyday life. In this project, Dr. Wessel will use neural recordings and brain stimulation in adult healthy humans to further our understanding of how brain activity underlying motor emission (initiating an action) and motor inhibition (stopping that action) interact during the rapid stopping of action. Elucidating the neural interactions underlying action-stopping will inform our basic understanding of flexible behavior in both health and disease. This is especially relevant since action-stopping is significantly impaired across many neuropsychiatric disorders. During this project, Dr. Wessel will implement an educational opportunity at his institution that aims to provide a first-hand research experience in human neuroscience to first-generation college students, a population that is traditionally underrepresented in basic academic science and the STEM workforce.<br/><br/>The ability to stop the execution of an already initiated action is a key cognitive control ability. In prominent theoretical models, stopping is conceptualized as a race between a ""go""-process and a separate ""stop""-process that races to intercept the go-process. While extensive past research has established that a fronto-basal ganglia brain network implements the ""stop"" side of this equation, human research has so far fallen short of demonstrating the existence of an actual race between separate stop- and a go-processes in the brain. By combining measurements of intra- / extracranial electroencephalography and cortico-spinal excitability with brain stimulation, the project aims to elucidate neural concomitants of the purported race on all levels of processing (motor, cortical, subcortical). First, a pure neural measurement of the stopping process will be identified by disentangling neural processes that reflect stop-signal detection from those that reflect the stop-process itself. Second, this signature of the stop-process and an according measurement of the go-process will be used to predict behavior based on the trial-by-trial outcome of their neural race. Third, cortical and subcortical nodes of the neural network underlying the stop-process will be disrupted using transcranial magnetic and deep-brain stimulation to test the hypothesis that such disruptions impair motor inhibition specifically by biasing the race towards the ""go"" process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807132","SemiSynBio: Collaborative Research: YeastOns: Neural Networks Implemented in Communicating Yeast Cells","MCB","SemiSynBio - Semicon Synth Bio, Genetic Mechanisms","08/01/2018","07/07/2020","Eric Klavins","WA","University of Washington","Continuing Grant","Arcady Mushegian","07/31/2021","$337,500.00","","klavins@ee.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","BIO","061Y, 1112","056Z, 7465","$0.00","Large, three-dimensional cell colonies grown inexpensively using simple raw materials could be made into cheap, energy-efficient computers. A fundamental challenge in using living cells for computing is that computation by cells is error prone, and cells divide, die and reorganize inside a cell culture, making it difficult to maintain a defined architecture. This research will explore the design of yeast cell-based computing systems inspired by how computing is performed by the animal brain cells. To develop new knowledge at the intersection of electronics, computing and biology will require a new generation of students familiar with each of these areas who can work in collaborative teams. Building on work with organizations including the Freshman Research Initiative at UT Austin and Women in Science and Engineering at JHU, the PIs will develop programs to allow groups of undergraduate researchers to engage in long term research programs in which students have the opportunity to perform independent investigations as part of collaborative, inter-university teams.<br/><br/>This project will combine ideas from computer architecture and systems neuroscience with new tools from synthetic biology to develop yeastons - Saccharomyces cerevisiae cells that can collectively emulate a feedforward neural network through engineered cell-cell communication processes and programmable transcriptional logic. Crucially, yeaston networks will be designed to tolerate the inherent noisiness of single-cell biomolecular information processing and require no specific higher order spatial organization or patterning. The project members will build new protein receptors for small molecule signals and genetic logic systems that will enable single yeastons to emulate nodes in a feedforward neural network. The input-output behavior of single yeastons and yeaston networks will be quantitatively characterized, making it possible to evaluate the potential for scalable computation in yeaston systems. High-level models from neuroscience will be used to develop design principles for assembling robust yeaston networks and to derive scaling laws for yeaston computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804743","Collaborative Proposal: Engineering Architecture for Tissue Models","CBET","Engineering of Biomed Systems","08/01/2018","05/20/2019","Erin Lavik","MD","University of Maryland Baltimore County","Standard Grant","Aleksandr Simonian","07/31/2021","$308,102.00","","elavik@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","ENG","5345","9102, 9251","$0.00","This project focuses on developing a screen printing process to build models of both neural networks and the innervated colon (i.e., a colon with imbedded nerves to mimic the natural structure). The screen printing process has been around for thousands of years and is used to make art, to mass produce t-shirts, and to make microelectronics. It is simple, low cost, reproducible, and scalable, making it ideal for printing highly accessible tissue models. While there have been many exciting approaches to print tissue models, most involve expensive equipment and subject the cells to either ultraviolet rays or shearing through fine needles, both of which can impact cell survival and behavior. The objective of this project is to determine the resolution and reproducibility of the screen printing approach and then use the process developed to make the two new models. The neural network model (based on human derived stem cells) will lay the groundwork to study diseases and investigate therapies relevant to nerve tissue in a more physiology-based/meaningful way that is still suitable for high throughput systems. The colon model (consisting of epithelial cells, goblet cells producing mucous, immune cells, and neurons) will provide one of the first innervated gut models that is suitable for high throughput techniques. High throughput screening opens the possibilities of investigating the interplay between these cells in the development of food allergies and conditions such as colitis. The research will provide new tools for researchers interested in understanding the complex cross-talk between cells in these tissues and developing new therapies for insults to these systems. The screen printing technique will also be used to engage young researchers. Screen printing demonstrations will be built into outreach programs in schools and utilized to educate researchers in classrooms and labs. Screen printing is exceptionally familiar and ""user-friendly,"" thus a great platform for engaging and keeping people engaged in science. For example, Jello will be screen printed on edible paper to explain methods to make tissues as part of outreach program demonstrations in the Baltimore and Boston public schools. Educational and outreach efforts will be complemented by a website that highlights scientists from diverse backgrounds and the accessible, low tech parts of their work, like screen printing.<br/><br/>The objective of this project is to determine the resolution and reproducibility of a screen printing approach to making highly scalable hydrogel-cell tissue models and to then use the system developed to make new models of neural networks and the innervated colon.   The investigators hypothesize that screen-printing will provide a cost-effective approach to making tissue models that surpass those made via bioprinting approaches in terms of ease, cost and cell survival and thus enable more relevant models of tissues from cells that are highly sensitive to shear.  The Research Plan is organized under four aims.  THE FIRST AIM is to characterize the screen-printed materials (PLL(poly(L-lysine))/protein and hydroxy appetite-based gels functionalized with PEG-thiol groups printed on glass slides) with regards to the resolution, repeatability and lamination of layers.  Expected aim outcomes include: determining the gelation time and mechanics of the different materials, confirming the concentration and distribution of proteins on the gels, determining the strength of lamination between similar and different layers, and knowing the limits on how thin gels can be reproducibly screen printed.  THE SECOND AIM is to characterize the multilamellar, patterned gels with regards to architecture including resolution and reproducibility of features in the printing plane.  Expected aim outcomes include: determining the finest resolution that can be reproducibly achieved for a range of gels, confirming the concentration and distribution of proteins on and in the gels and determining if there are any types of features that are more or less easily printed at high resolution.  THE THIRD AIM is to determine the relationship between printing resolution, cell survival, and cellular behavior using human induced pluripotent stem (iPS) cells and human iPS-derived neurons to build neural circuits including excitatory and inhibitory neurons.  Expected aim outcomes include:  knowing which hydrogel augments iPS cell survival the most, knowing the impact of the matrix on the genetic and phenotypic cellular behavior of iPS and their progeny, knowing the impact screen printing has on the genetic and phenotypic cellular behavior of iPS cells and their progeny and developing a reproducible, scalable system for high throughput screening of 3D neural interactions that lead to neural circuits.  THE FOURTH AIM is to build a model of the innervated colon to investigate the application of screen printing in a multicellular, physiologically relevant, high throughput model.  Cell lines include enterocytes and goblet cells derived from Caco-2 cells, primary human intestinal epithelial cells, immune cells (dendritic cells) and neural cells (from Aim 3).  Matrix materials include PLL-PEG gel systems printed with laminin and collagen I.  The matrix optimized in Aim 3 will be used for the neural layer.   Expected aim outcomes include:  the development of a novel gut/colon model 1) that lays the foundation for study and enhanced understanding of the role of the enteric nervous system, 2) that, even without the neural component, will be one of the first colon models that can be prepared in a high throughput manner and interface with real time electrical recording and imaging modalities without a substantial investment in materials or equipment and 3) that lays for foundation for adding the next critical component--the microbiome.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815528","III: Small: Reliable and Generalizable Neural Search Engine Architectures","IIS","Info Integration & Informatics","09/01/2018","08/02/2018","Jamie Callan","PA","Carnegie-Mellon University","Standard Grant","Maria Zemankova","08/31/2021","$499,659.00","","callan@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7364, 7923","$0.00","Scientists need to frequently search the scientific literature on the subject they are studying.  Despite the availability of papers and citation databases on the Web, the enormous growth of scientific publications in all disciplines makes this a daunting task.  Traditional commerical search engines, such as Google, often fail to include the most important documents in the first few pages of returned results - in other words, they do not do a good enough job of ranking scientific papers for a given query.  Recently, new algorithms for search based on artificial neural network techniques have emerged as an alternative to traditional search architectures. These new neural search architectures are more accurate, but must be first trained with millions of example queries and answers from user interactions; this limits their usefulness for many tasks.  This project will overcome this problem by developing new methods of training neural search engines that reduce the need for training examples by integrating explicit knowledge resources for a given discipline.  The new techniques will be disseminated in freely available open-source search software for both university and industry researchers, thus broadly benefiting scientific advancement. In addition, the project will broaden participation by under-represented groups by creating research opportunities for female and undergraduate students and technology transfer opportunities for industry.<br/><br/>This research develops new methods of training neural ranking architectures when a massive amount of training data is not available for the target application; integrates external knowledge resources to provide more information for making accurate ranking decisions; and applies the architecture to a domain-specific search task such as retrieving tabular data from scientific documents. This collection of problems is chosen to increase the practicality of neural ranking architectures outside of high-traffic commercial search environments, and to investigate and exploit the strengths of neural ranking architectures at using attention mechanisms to manage evidence, soft-matching across different types of evidence, and learning sophisticated nonlinear decision models. This research furthers the development of neural ranking architectures that are generally applicable and more reliable than current systems due to their ability to integrate a broader range of evidence in a predictable manner. Neural ranking architectures have generated much excitement and skepticism during the last several years. This research extends a recently-developed neural ranking system that is already able to beat strong learning-to-rank systems under specific conditions. It addresses one of the main obstacles to wider use of these models -- the availability of large amounts of training data. It integrates information from external semi-structured knowledge resources, because such information is effective in other ranking architectures and because it is likely to benefit from how neural ranking architectures manage and use diverse evidence of varying quality. Finally, it stress tests the architecture by applying it to a domain-specific task such as table retrieval from scientific documents, that requires the search engine to use several parts of the document selectively, rather than the entire document. These activities are designed to produce a neural ranking architecture capable of managing diverse evidence and document structure so as to provide greater knowledge about the particular strengths and weaknesses of neural ranking architectures. <br/><br/>This research develops new methods of training neural ranking architectures when a massive amount of training data is not available for the target application; integrates external knowledge resources to provide more information for making accurate ranking decisions; and applies the architecture to a domain-specific search task such as retrieving tabular data from scientific documents. This collection of problems is chosen to increase the practicality of neural ranking architectures outside of high-traffic commercial search environments, and to investigate and exploit the strengths of neural ranking architectures at using attention mechanisms to manage evidence, soft-matching across different types of evidence, and learning sophisticated nonlinear decision models. This research furthers the development of neural ranking architectures that are generally applicable and more reliable than current systems due to their ability to integrate a broader range of evidence in a predictable manner. Neural ranking architectures have generated much excitement and skepticism during the last several years. This research extends a recently-developed neural ranking system that is already able to beat strong learning-to-rank systems under specific conditions. It addresses one of the main obstacles to wider use of these models -- the availability of large amounts of training data. It integrates information from external semi-structured knowledge resources, because such information is effective in other ranking architectures and because it is likely to benefit from how neural ranking architectures manage and use diverse evidence of varying quality. Finally, it stress tests the architecture by applying it to a domain-specific task such as table retrieval from scientific documents, that requires the search engine to use several parts of the document selectively, rather than the entire document. These activities are designed to produce a neural ranking architecture capable of managing diverse evidence and document structure so as to provide greater knowledge about the particular strengths and weaknesses of neural ranking architectures. The project website (http://www.cs.cmu.edu/~callan/Projects/IIS-1815528/)  describes recent activities and provides access to research publications, experimental results, datasets, and open-sources software produced by the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807369","SemiSynBio: Collaborative Research: YeastOns: Neural Networks Implemented in Communicating Yeast Cells","MCB","SemiSynBio - Semicon Synth Bio, Genetic Mechanisms","08/01/2018","07/16/2020","Andrew Ellington","TX","University of Texas at Austin","Continuing Grant","Arcady Mushegian","07/31/2021","$337,683.00","","ellingtonlab@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","BIO","061Y, 1112","056Z, 7465","$0.00","Large, three-dimensional cell colonies grown inexpensively using simple raw materials could be made into cheap, energy-efficient computers. A fundamental challenge in using living cells for computing is that computation by cells is error prone, and cells divide, die and reorganize inside a cell culture, making it difficult to maintain a defined architecture. This research will explore the design of yeast cell-based computing systems inspired by how computing is performed by the animal brain cells. To develop new knowledge at the intersection of electronics, computing and biology will require a new generation of students familiar with each of these areas who can work in collaborative teams. Building on work with organizations including the Freshman Research Initiative at UT Austin and Women in Science and Engineering at JHU, the PIs will develop programs to allow groups of undergraduate researchers to engage in long term research programs in which students have the opportunity to perform independent investigations as part of collaborative, inter-university teams.<br/><br/>This project will combine ideas from computer architecture and systems neuroscience with new tools from synthetic biology to develop yeastons - Saccharomyces cerevisiae cells that can collectively emulate a feedforward neural network through engineered cell-cell communication processes and programmable transcriptional logic. Crucially, yeaston networks will be designed to tolerate the inherent noisiness of single-cell biomolecular information processing and require no specific higher order spatial organization or patterning. The project members will build new protein receptors for small molecule signals and genetic logic systems that will enable single yeastons to emulate nodes in a feedforward neural network. The input-output behavior of single yeastons and yeaston networks will be quantitatively characterized, making it possible to evaluate the potential for scalable computation in yeaston systems. High-level models from neuroscience will be used to develop design principles for assembling robust yeaston networks and to derive scaling laws for yeaston computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807546","SemiSynBio: Collaborative Research: YeastOns: Neural Networks Implemented in Communicating Yeast Cells","MCB","SemiSynBio - Semicon Synth Bio, Genetic Mechanisms","08/01/2018","07/13/2020","Rebecca Schulman","MD","Johns Hopkins University","Continuing Grant","Arcady Mushegian","07/31/2021","$457,818.00","Joshua Vogelstein","schulman.rk@gmail.com","1101 E 33rd St","Baltimore","MD","212182686","4439971898","BIO","061Y, 1112","056Z, 7465, 9251","$0.00","Large, three-dimensional cell colonies grown inexpensively using simple raw materials could be made into cheap, energy-efficient computers. A fundamental challenge in using living cells for computing is that computation by cells is error prone, and cells divide, die and reorganize inside a cell culture, making it difficult to maintain a defined architecture. This research will explore the design of yeast cell-based computing systems inspired by how computing is performed by the animal brain cells. To develop new knowledge at the intersection of electronics, computing and biology will require a new generation of students familiar with each of these areas who can work in collaborative teams. Building on work with organizations including the Freshman Research Initiative at UT Austin and Women in Science and Engineering at JHU, the PIs will develop programs to allow groups of undergraduate researchers to engage in long term research programs in which students have the opportunity to perform independent investigations as part of collaborative, inter-university teams.<br/><br/>This project will combine ideas from computer architecture and systems neuroscience with new tools from synthetic biology to develop yeastons - Saccharomyces cerevisiae cells that can collectively emulate a feedforward neural network through engineered cell-cell communication processes and programmable transcriptional logic. Crucially, yeaston networks will be designed to tolerate the inherent noisiness of single-cell biomolecular information processing and require no specific higher order spatial organization or patterning. The project members will build new protein receptors for small molecule signals and genetic logic systems that will enable single yeastons to emulate nodes in a feedforward neural network. The input-output behavior of single yeastons and yeaston networks will be quantitatively characterized, making it possible to evaluate the potential for scalable computation in yeaston systems. High-level models from neuroscience will be used to develop design principles for assembling robust yeaston networks and to derive scaling laws for yeaston computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755098","Extrinsic neuromodulation is a general mechanism to stabilize neural network function during temperature changes","IOS","Activation","08/01/2018","07/26/2018","Wolfgang Stein","IL","Illinois State University","Standard Grant","Sridhar Raghavachari","07/31/2022","$495,000.00","","wstein@IllinoisState.edu","100 S Fell Ave","Normal","IL","617901000","3094382528","BIO","7713","1096, 9178, 9179","$0.00","How nerve cells in the brain deal with temperature changes is not well understood. The electrical activity that nerve cells produce rely on a well-balanced flow of ions across the cell membrane. It is this balance that is critically altered by temperature, leading to failures in neural activity and accordingly severe consequences for vitality. Many animals have evolved compensatory mechanisms allowing their brains to function over a wider temperature range. This research elucidates such evolutionary conserved mechanisms with a novel approach by studying the same nerve cells in several different species of crustaceans. Crustaceans are ideal systems to address these issues because of their large and identified nerve cells and they live at a variety of temperatures. Recognizing evolutionarily conserved mechanisms for temperature compensation is not only crucial for understanding how animals survive in continuously changing environments, they are ultimately also a prerequisite for the investigation and treatment of hyper- and hypothermia. This project will provide comprehensive training in identifying temperature effects on nerve cell physiology and in cutting-edge electrophysiology for all levels of students. Results will be disseminated publicly through Youtube and a variety of public outreach programs. <br/><br/>Possessing compensatory mechanisms that maintain vital neuronal activity when temperature changes is critical for animal survival. Recent data from central pattern generators in the crustacean stomatogastric nervous system have shown that descending projection neurons that provide extrinsic neuromodulation to motor networks can counterbalance detrimental temperature effects in these networks. Compensation is achieved by counterbalancing temperature-dependent increases of ionic conductances, allowing for a quick and flexible response to temperature influences. Descending projection neurons are universal building blocks in the motor circuits of many taxa, and the goal of this project is to test the hypothesis that temperature compensation via neuromodulation is a widespread phenomenon and evolutionarily conserved. Experiments will be carried out on several closely and distantly related crustaceans with various evolutionary backgrounds and temperature tolerances. Specifically, extra- and intracellular recordings, including voltage- and dynamic clamp, from identified modulatory and pattern generating neurons in the stomatogastric ganglion will (1) determine temperature compensation in related species with similar temperature tolerance and (2) determine temperature compensation in distantly related species with different temperature tolerance. This study will show that neuromodulation is more than a means to increase flexibility in the nervous system in that it stabilizes neuronal activity in a functional context.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1850851","CRII: CSR: NeuroMC---Parallel Online Scheduling of Mixed-Criticality Real-Time Systems via Neural Networks","CNS","CRII CISE Research Initiation, Special Projects - CNS","08/01/2018","05/29/2020","Zhishan Guo","FL","The University of Central Florida Board of Trustees","Standard Grant","Erik Brunvand","06/30/2022","$190,946.00","","zhishan.guo@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","026Y, 1714","7354, 8228, 9150, 9251","$0.00","With progression of technology, as the transistors become smaller, more of them can be integrated into a semiconductor chip; this in turn enables integration of many processor ""cores"" into one chip. Emerging chips integrate different types of processor cores, which are specialized for various functions including graphics processing and pattern recognition.  Availability of many cores creates a task scheduling problem for a software program, namely which portions of the software should execute on what type of core. Given that the capabilities of the cores are not equal, a task may take different amounts of time to finish depending on which core it was assigned to.  In a real-time system, while mapping tasks to cores, deadlines should be met, which may not always be possible. Further, the consequences of missing a deadline are not the same for all tasks. Some are more forgiving than others. Consequently, tasks can be classified broadly based on their criticality. This research will investigate an efficient scheduler for mixed-criticality real-time systems.  <br/><br/>In a resource constrained system, consequences of failure to meet a deadline may range from catastrophic to Minor. Consequently, there are varying penalties associated with missing deadlines. The investigator plans to address the scheduling problem of mixed-criticality real-time systems. This scheduling problem is NP-hard; the proposed approach involves an artificial neural network (NN)-based scheduler.  The investigator will experiment with parallel NNs for faster convergence, and develop a prototype system to evaluate the efficiency and scalability of this solution. The project is expected to lead to the ability to make near-optimal decisions in real time.  This project serves as the initial step of a larger effort in bringing the parallel computation and neural networks together. <br/><br/>Broad economic and societal impacts of this project include integration of research and education, as it will involve development of a new course offering and redesign of an existing course on real-time and cyber-physical systems. The project will involve undergraduate students in programming graphics processing units and will seek to support multiple female students to broaden participation. Research results, educational material, software, and experimental data will be disseminated on the project website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755965","CRII: CSR: NeuroMC---Parallel Online Scheduling of Mixed-Criticality Real-Time Systems via Neural Networks","CNS","","07/01/2018","02/22/2018","Zhishan Guo","MO","Missouri University of Science and Technology","Standard Grant","Sandip Kundu","10/31/2018","$174,946.00","","zhishan.guo@ucf.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","026y","8228, 9150","$0.00","With progression of technology, as the transistors become smaller, more of them can be integrated into a semiconductor chip; this in turn enables integration of many processor ""cores"" into one chip. Emerging chips integrate different types of processor cores, which are specialized for various functions including graphics processing and pattern recognition.  Availability of many cores creates a task scheduling problem for a software program, namely which portions of the software should execute on what type of core. Given that the capabilities of the cores are not equal, a task may take different amounts of time to finish depending on which core it was assigned to.  In a real-time system, while mapping tasks to cores, deadlines should be met, which may not always be possible. Further, the consequences of missing a deadline are not the same for all tasks. Some are more forgiving than others. Consequently, tasks can be classified broadly based on their criticality. This research will investigate an efficient scheduler for mixed-criticality real-time systems.  <br/><br/>In a resource constrained system, consequences of failure to meet a deadline may range from catastrophic to Minor. Consequently, there are varying penalties associated with missing deadlines. The investigator plans to address the scheduling problem of mixed-criticality real-time systems. This scheduling problem is NP-hard; the proposed approach involves an artificial neural network (NN)-based scheduler.  The investigator will experiment with parallel NNs for faster convergence, and develop a prototype system to evaluate the efficiency and scalability of this solution. The project is expected to lead to the ability to make near-optimal decisions in real time.  This project serves as the initial step of a larger effort in bringing the parallel computation and neural networks together. <br/><br/>Broad economic and societal impacts of this project include integration of research and education, as it will involve development of a new course offering and redesign of an existing course on real-time and cyber-physical systems. The project will involve undergraduate students in programming graphics processing units and will seek to support multiple female students to broaden participation. Research results, educational material, software, and experimental data will be disseminated on the project website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1929607","US-German Collaboration: Toward a quantitative understanding of navigational deficits in aging humans","IIS","CRCNS-Computation Neuroscience","08/01/2018","06/07/2019","Ila Fiete","MA","Massachusetts Institute of Technology","Continuing grant","Kenneth Whang","06/30/2019","$16,321.00","","fiete@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7327","5345, 5936, 5980, 7298, 7327","$0.00","The goal of this project is to combine computational modeling with behavioral and neuroimaging studies to characterize the mechanisms of navigational abilities in humans and understand how they decline with age. The PIs will focus on an important navigational circuit in mammals, which consists of the hippocampus and associated areas, and includes grid cells of the entorhinal cortex as well as place cells. Place cells have highly location-specific responses, turning on at one location in an environment and firing little elsewhere; grid cells by contrast fire at multiple locations within an environment, with periodically separated activity blobs in a striking triangular lattice pattern. Studies in rodents have detailed the properties of grid and place cells, and led to neural network models whose additional predictions have often been borne out by single-unit neuron recordings. However, much less is known about grid cells and place cells in humans, and the nature of interactions between different parts of the navigation circuit remains unclear, in rodents and humans. <br/><br/>In this project, the PIs bring to bear virtual-reality-based behavioral experiments, ultra-high-resolution fMRI recordings during virtual navigation, and neural network modeling, to better understand the circuit for spatial navigation in humans. The PIs plan a three-pronged approach to these questions. The first is to characterize phenomenologically the characteristic errors made by humans, through navigation environments with and without accurate external landmark cues, and under other externally varying conditions, in aged and non-aged subjects. The second is to employ neural network models of grid cells, to model the network parameters that could give rise to the observed deficits, and in turn test the predictions of these models with the neuroimaging experiments. The experimental setup will permit systematic variation in the fidelity of external sensory cues, to probe the relative contributions of the complementary computations of dead-reckoning (path integration) versus landmark-based navigation, and uncover their potential neural substrates in humans. The results will help to develop models of how parallel streams of spatial information are combined and processed across brain areas to aid in navigation. The third component is to develop accurate algorithms for extracting spatial information from high-resolution fMRI data from regions and sub-regions of the entorhinal-hippocampal complex. The aim is to map the distribution of location information across areas and learn where it is most compromised in old age.<br/><br/>This award is being co-funded by NSF's Office of the Director, International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF)."
"1753684","CRII: CHS: Learning Procedural Modeling Programs for Computer Graphics from Examples","IIS","HCC-Human-Centered Computing","05/01/2018","04/24/2018","Daniel Ritchie","RI","Brown University","Standard Grant","Ephraim Glinert","04/30/2021","$175,000.00","","daniel_ritchie@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7367","7367, 8228, 9150","$0.00","Procedural modeling is used to programmatically generate visual content for instruction, simulation, animation, visual effects, architecture, graphic design, and other applications.  An effective procedural model can produce a variety of detailed, visually interesting, and even pleasantly surprising results.  Unfortunately, such models are difficult to author, requiring both visual creativity and programming expertise.  More people could be empowered to create and use procedural models were it possible to deduce them from examples.  The current project will tackle this long-standing open problem in computer graphics by building on the PI's prior work to develop a research program investigating new approaches to learning procedural models from examples by combining probabilistic programs with neural nets; programs are expressive enough to represent a variety of visual content, while neural networks provide flexible learning from data.  Project outcomes will help democratize procedural modeling by allowing users to create procedural models with examples rather than by writing code, so that a wider demographic of creative professionals and enthusiasts can participate.  All code and data produced will be released as open-source, to allow other researchers and developers to apply and extend the new techniques.<br/><br/>Because graphical content is often hierarchical, (probabilistic) grammars are typically used to procedurally model it.  However, such content is also characterized by continuous attributes: colors, affine transformations, and so on.  While grammars can be extended to support some of these attributes, there are no general-purpose methods for learning such models from examples.  Existing approaches either ignore continuous attributes or are specialized to one type of content (e.g., building facades).  This research presents a new general-purpose approach for example-based learning of procedural models which generate discrete hierarchical structures with continuous attributes.  The key insight is representing a procedural model as a probabilistic program whose control flow and data flow can be governed by neural networks.  Like a grammar, such a program can naturally represent (possibly recursive) hierarchical structure.  The neural network logic of the program can represent complex functions which generate continuous attributes such as transformations.  The model is efficiently learnable with stochastic-gradient-based methods and has the potential to scale from small numbers of examples to large datasets.  The initial focus will be on learning procedural models of 3D scene graphs, which are 3D objects composed of a hierarchy of parts.  The research will then expand into learning procedural models from large datasets of examples, applying the techniques to domains beyond 3D scene graphs, and leveraging unstructured inputs such as images as examples.  Project outcomes will include new mathematical frameworks for learning procedural models from examples, algorithms for efficiently solving the learning problem, and evaluations of the quality of content generated by learned models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840145","EAGER: Optofluidic Strategies for Therapeutic Delivery of Neurotransmitters to Restore Vision","CBET","EFRI Research Projects","07/15/2018","07/13/2018","Laxman Saggere","IL","University of Illinois at Chicago","Standard Grant","Leon Esterowitz","06/30/2021","$299,984.00","","saggere@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","ENG","7633","7916","$0.00","Vision is our most important and complex sense critical to daily living. Therefore, irreversible blinding diseases of the eye such as macular degeneration and retinitis pigmentosa that affect over 10 million people in the US alone have devastating consequences on the quality of life. Loss of vision in these diseases is due to progressive degeneration of cells called photoreceptors in the retina. The photoreceptors normally convert the incident light information into electrochemical signals that are relayed to the brain for the perception of sight through a complex neural network. Degeneration causes a break in the neural network and thereby the process of vision, even though the rest of the neural network is functionally intact. Retinal implants seeking to replace degenerated cells by stimulating surviving cells with electrical current are emerging as a promising option for treating such blindness. But, because current is an unnatural stimulus, they have difficulty restoring naturalistic vision and visual acuity below the legal blindness limit. Recent research from the PI's group has demonstrated that stimulating a live retinal tissue in a dish with the brain chemical glutamate mimics its natural activation following visual stimulation.<br/><br/>Artificially stimulating the retina with brain chemicals delivered through a tiny device implanted in the back of the eye could potentially restore more naturalistic vision and better visual acuity than current retinal prostheses. A suitable technology for the delivery of chemicals through an implantable device in the eye, is currently lacking. The proposed project aims to fill this technological gap by exploring strategies that could enable light-controlled delivery of therapeutic amounts of brain chemicals using only natural light passing through the eye. If successful, this project could accelerate the development of an implantable device to deliver brain chemicals to the retina and help restore high-acuity naturalistic vision to millions of people who are blind by neurodegenerative retinal diseases. It could also potentially prove effective in treating other neurodegenerative eye diseases such as glaucoma and optic neuritis that preclude the use of retinal prosthetics and brain disorders such as Parkinson's and Alzheimer's diseases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807551","SemiSynBio: Cardiac Muscle-Cell-Based Coupled Oscillator Networks for Collective Computing","ECCS","SemiSynBio - Semicon Synth Bio, Genetic Mechanisms","07/15/2018","07/28/2020","Pinar Zorlutuna","IN","University of Notre Dame","Continuing Grant","Usha Varshney","06/30/2021","$1,125,000.00","Hsueh-Chia Chang, Suman Datta, Nikhil Shukla","Pinar.Zorlutuna.1@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","ENG","061Y, 1112","108E, 7465","$0.00","Current rate of structured and unstructured data generation and the need for real-time data analytics requires radically different computational approaches that can operate in a massively parallel, scalable and energy efficient manner.  In addition, certain classes of computational problems, i.e. combinatorial optimization problems, which have extensive applications in many real-world situations such as fault diagnosis, scheduling, resource allocation and even neural network training are fundamentally difficult to solve using the Boolean framework, the backbone of our current computational framework. This proposal aims to explore the potential of coupled oscillator networks made of living heart muscle cells, or bio-oscillators, as a collective computing fabric for solving computationally hard problems such as optimization, learning and inferences. New computing paradigms that can solve computationally hard problems efficiently using cell-based collective computing fabrics will transform how synthetic biocomputing circuits are built. In addition, the proposed research will lead to a better understanding of the electrical communication in muscle cell networks impacting potential future applications ranging from biorobotics to understanding and treating muscle disorders. The broader impacts will further be achieved through interdisciplinary student mentoring and education by the PIs with complementary backgrounds in Biology, Engineering and Computer Science, timely dissemination of key research outcomes via published papers and presentations, as well as proposed outreach activities including a workshop for middle school girls providing hands-on activities in STEM, mentoring of high school students, and a weekend long workshop each year for undergraduate and graduate students, called 'Biology Inspired Computing'. These activities will contribute towards educating a new cadre of students that will meet the future need of this emerging field. Attention will be paid to student recruitment from under-represented groups by participating in such recruitment programs at the PIs' institution.<br/><br/>Current designs that explore biological components for biocomputing leverages the information processing units of the cells, such as DNA, gene or protein circuitries, which are inherently slow (hours to days speed). Using electrically active cells that could individually operate in the hundred Hertz regime, and can be connected as networks to perform massively parallel tasks, can transform biocomputing and lead to novel ways of energy efficient information processing. The goal of this project is to explore the potential of electrically coupled oscillator networks made of living heart muscle cells to form a collective computing fabric for solving computationally hard problems such as optimization, learning and inference tasks. Heart muscle cells are electrically active components that can initiate and relay electrical signals without loss. More interestingly, they spontaneously beat (i.e. oscillate) at a stable pace, and when coupled with each other through ion fluxes, they synchronize to a locked, steady frequency. In this study, it is hypothesized that reconfigurable circuits fabricated using coupled heart muscle cell oscillators, or bio-oscillators, can be configured into functional continuous-time dynamical systems to solve computationally hard problems. Towards this end, state-of-art nanobiofabrication methods will be used to create bio-oscillators and to bi-directionally couple them through nanoporous ionic membranes for programmable computing. The design space will be explored for the geometry and size of the individual bio-oscillators, nanoporous membrane design for bio-oscillator connectivity through ion fluxes, as well as the network topology and fabrication feasibility of large bio-oscillator arrays. First, a pair of coupled bio-oscillators will be studied and computationally efficient compact models will be developed that accurately reflect the continuous time dynamics of the coupled bio-oscillators. Then, the design will be scaled up to larger network of coupled bio-oscillators and a feasibility study would be conducted for solving a prototypical hard optimization task such as vertex coloring of graph and graph partitioning. In addition to their inherent connectivity, scalability and parallel processing ability, the muscle cell-based approach proposed in this study requires minimal energy mostly in the form of sugar, and as such will be a low-energy alternative to current energy demanding traditional computing approaches for solving hard optimization problems using heuristics based digital computing techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839232","RAISE-TAQS: Entanglement and information in complex networks of qubits","CCF","OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","09/13/2018","Zhexuan Gong","CO","Colorado School of Mines","Standard Grant","Pinaki Mazumder","09/30/2021","$985,926.00","Lincoln Carr, Cecilia Diniz Behn, Meenakshi Singh","gong@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","1253","049Z, 057Z, 7928","$0.00","Quantum computers are now approaching a size that will soon perform tasks surpassing the power of today's fastest classical computers. To attain the full power of a quantum computer, qubits inside the quantum computer should be well connected with each other, so that information can be transferred, and entanglement can be generated between any two qubits as fast as possible. The qubit interactions will form a complex and time-varying network and their dynamics will be too complicated for classical computers to predict. This project will provide a key step in understanding quantum systems of a rapidly increasing level of complexity and find out how such complexity can be employed to massively speed up quantum computing over systems with sparse and simple qubit connections. The project will expand the fields of quantum information science and condensed matter physics into the territory of complexity science, via concrete ways to quantify complexity of quantum states. As the quantum information frontier is fostering a new technological revolution around the world, the project will train a new generation of undergraduate and graduate students with expertise in quantum technologies and develop a new 12-credit graduate certificate program in quantum engineering to accommodate the pressing need from industry professionals in obtaining quantum expertise.<br/><br/>The first half of this project aims to find out how high qubit connectivity can be used to speed up quantum information processing, focusing on mathematical quantum speed limits akin to Lieb-Robinson bounds, optimal entangling protocols for very small or very large number of qubits, and experimental demonstrations of entangling speed limit using solid-state qubits. The second half of this project will focus on states created by Hamiltonians with dense and complex interactions, quantifying their complexity and understanding their entanglement structure. Various network measures borrowed from complexity science will be employed to study the experimentally measurable quantum mutual information network and the recently developed quantum neural network, in order to bring new insight to quantum critical phenomena, entanglement area laws, and nonequilibrium many-body dynamics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1753268","CAREER: Signal Crosstalk Within Complex Microbial Ecosystems","PHY","PHYSICS OF LIVING SYSTEMS","09/15/2018","07/08/2019","James Boedicker","CA","University of Southern California","Continuing Grant","Krastan Blagoev","08/31/2023","$200,000.00","","boedicke@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","7246","1045","$0.00","Microbial ecosystems in nature are typically composed of hundreds or thousands of microbial species, heterogeneously distributed in space and time. New experimental and theoretical tools are needed to develop a predictive understanding of the regulatory processes that control the outputs of these complex cellular networks. Current approaches focus on correlations between the components of the system (such as genetic composition, expression levels, metabolite concentrations), however such correlations alone are usually insufficient to develop strategies for robust control of the state and function of diverse microbial communities. In this project the PI will use multiscale approach that combines theoretical predictions with precision experimental tests to quantitatively understand how individual and groups of cells reach decisions to coordinate global behavior within microbial ecosystems. Tackling modern societal challenges related to health, agriculture, and the environment will benefit from a predictive understanding of how multiple components of a network work together, particularly for complex networks involving many densely connected components. To inspire the next generation of scientists to take up problems on the physics of systems and complexity, the PI has partnered with a local science non-profit to design and implement hands-on learning activities related to the biophysics of signal exchange. Students will gain an intuitive understanding of the physical principles that govern cellular networks, including insight into how network properties emerge from the behavior of individuals within a population. Themes of this research will also appear in an undergraduate general education course focused on training undergraduates from diverse background, including non-science majors, in essential quantitative skills intrinsic to the physical sciences.<br/><br/>The PI will examine the exchange of variant quorum sensing signals within diverse microbial networks to understand how signaling interactions between individual strains give rise to emergent properties of the network. An artificial neural network model predicts the impact of crosstalk between cell types on system-level signaling states. The stability of these signal-driven regulatory states will be examined in both models and experiments to gauge the potential to direct the outputs of multispecies networks through perturbations of species composition or signal exchange pathways. This combination of theory and experiment illuminates how systems-level behaviors such as robustness emerge from the collective action of multiple cell types working together.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815529","RI: Small: Computational analysis of eye movements in reading: reader characteristics, cognitive state, and natural language processing","IIS","Linguistics, Robust Intelligence","08/01/2018","07/31/2018","Roger Levy","MA","Massachusetts Institute of Technology","Standard Grant","D.  Langendoen","07/31/2021","$400,000.00","","rplevy@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1311, 7495","1311, 7495, 7923, 9178","$0.00","Reading is the most widely practiced skill in the world. It occupies many hours of our daily lives and is crucial to functioning successfully in modern society. When we read, our eyes move over the text in a way that reflects how we perceive, process, and understand it. This project uses eye tracking to develop a new approach to studying how our eyes move during reading and what eye movement patterns can reveal about readers' linguistic knowledge and how they interact with text. In particular, the researchers develop eye tracking-based methods that automatically determine readers linguistic proficiency, how well they understand the text, and how difficult they find it. The project improves automatic text processing by machines, by taking into account information about how humans read. This research program benefits society by advancing our scientific understanding of language processing during reading and enabling new technologies that support human readers from a wide range of backgrounds and skill levels. It lays the foundations for future digital platforms that make it easier for people to access textual information, improve literacy, learn new languages, and personalize text content and complexity according to readers' needs and goals.<br/><br/>The project introduces a novel conceptual and computational approach to studying human reading with both native and non-native speakers, by leveraging broad coverage eye movement patterns during reading of free-form text. It develops a computational framework that connects eye movement in reading to linguistic properties of the text, the readers' linguistic knowledge and their cognitive state during reading. To realize this framework, the project first focuses on using eye movement in reading to automatically predict readers' linguistic proficiency and estimate their comprehension of specific parts of a text. These and other related tasks help to characterize how language comprehension manifests in gaze and unfolds over time, and also lead to the development of a predictive computational framework for native and non-native reading. Its second focus is on integrating data and representations from eye tracking in natural language processing, with the aim of developing applications which support and enhance human reading and language learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755568","CRII: RI: Lyapunov-Certified Cognitive Control for Safe Autonomous Navigation in Unknown Environments","IIS","Robust Intelligence","05/01/2018","04/25/2018","Nikolay Atanasov","CA","University of California-San Diego","Standard Grant","Erion Plaku","09/30/2020","$173,130.00","","natanasov@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7495, 8228","$0.00","Applications for unmanned aerial and ground vehicles requiring autonomous navigation in unknown, cluttered, and dynamically changing environments are increasing in fields such as transportation, delivery, agriculture, environmental monitoring, and construction. To achieve safe, resilient, and self-improving autonomous navigation, this project focuses on the design of adaptive online environment understanding that guarantees stable and collision-free operation in challenging conditions. The proposed research is important because current practices rely on prior maps or hand-crafted online mapping that attempt to capture the whole environment, even if parts are irrelevant for specific navigation tasks. This increases memory and computation requirements, spreads the effects of noise, and makes current approaches brittle, particularly in conditions involving dynamic obstacles, unreliable localization, or illumination variation.<br/><br/>The proposal offers two technical innovations to achieve safe autonomous navigation. First, it develops a learnable neural map based on 3-D convolution over hierarchical (octree) partitioning of space to extract navigation-specific features and on differentiable memory to infer long-term dependence among the features. The neural map parameters are trained from navigation experience not to produce accurate maps but to quantify the collision probabilities of intended motion trajectories accurately. The second innovation is a Lyapunov-theoretic control approach that uses the total energy of an autonomous system with respect to a virtual kinematic system (that can stop immediately) to derive conditions that guarantee stable and collision-free tracking of the trajectories proposed by the neural network. The proposed learnable neural map significantly increases the robustness of collision prediction, while the Lyapunov-theoretic control guarantees stable and safe navigation in new, unpredictable, and cluttered environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816165","RI:Small: Nonlinear signal representations for speech applications","IIS","Robust Intelligence","08/15/2018","08/02/2018","Najim Dehak","MD","Johns Hopkins University","Standard Grant","Tatiana Korelsky","07/31/2021","$336,814.00","Jesus Villalba Lopez","ndehak3@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7495","7495, 7923","$0.00","Human speech is a very rich signal. In addition to words, it contains several kinds of important information about the speaker such as identity, gender, age, native language, dialect, and emotion. It also provides information about the transmission channel and environment; for example, whether the speech came from a phone call or a high-fidelity recording, and whether or not there was background noise.   This project aims to create a powerful uniform representation that reflects all the information carried by speech. Such representation would enable important speech applications in multiple sectors of society: commercial (security, healthcare, user interfaces), government (security, information filtering), and law enforcement (speaker identification, forensics).<br/><br/>In this project, Johns Hopkins University researchers, who invented the original i-vector framework, intend to progress beyond the linear i-vector approach by investigating non-linear models with the expectation to better explain the complex structure of speech. To achieve this goal, two different models are investigated. First, a non-linear i-vector version is explored. In this method, the speech signal distribution is modeled by a Gaussian mixture model (GMM). The super-vector formed by the GMM means is a non-linear function (neural network) of a latent variable (speech representation). The parameters of the neural network and the latent representation can be jointly estimated by stochastic gradient descent iterations. Secondly, the team intends to investigate different types of auto-encoder networks (AE, VAE, RBM, DBM) to obtain representations from their hidden layers. Preliminary research shows that it is feasible to obtain good representations by combining activations from several hidden layers. Visualization tools are used to understand how the speech data have been represented and structured. By understanding the non-linear relationships created via the auto-encoder network modeling and using the visualization tools, there is potential to produce valuable insights into speech modeling. These insights can help cognitive science and neuroscience researchers to understand how the brain represents speech signals. The proposed methods are developed as software that takes a speech segment as input and generates a single vector that may be used to characterize the segment for the important applications mentioned in the previous paragraph.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821525","EAGER: Statistical Modeling of Linguistic Change in Open Source Software","CCF","Software & Hardware Foundation","05/01/2018","04/25/2018","Anas Mahmoud","LA","Louisiana State University","Standard Grant","Sol Greenspan","04/30/2021","$63,067.00","","mahmoud@csc.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7798","7916, 7944, 9150","$0.00","The project explores a theory of open source software (OSS) evolution based on statistical natural language processing techniques. Based on the emerging recognition that software code is, in many ways, as ""natural"" as natural language (e.g., English), there is a trend to apply statistical models for software development tasks such as code analysis, comprehension, and programmer support. This grant extends the ""naturalness of code"" theory by studying how the code lexicon evolves in open source software as different developers work on a software project and features are added, modified, deleted.  The goal is to learn the extent to which the evolution of a developer's lexicon follows the laws of natural language evolution.<br/><br/>To create the needed demonstration, large datasets of code lexicons are being collected from a large number of OSS projects and their revisions (on GitHub and SourceForge). The main constructs of the frequency model of natural language evolution will be applied to track and identify the main patterns of language changes (e.g., birth, propagation, death of terms in the lexicon) throughout OSS projects life cycle. Part of the challenge is to better understand how events that instigate code evolution, such as maintenance activities and team formation, are fundamentally different from the events that instigate change in natural language, such as war and migration. The research should lead to new ways to predict software project outcomes and to improve software productivity and quality. The project will make available the data, tools, and algorithms that will be produced by the project, which will support future work to understand the dynamics of code evolution in open source software ecosystems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815291","RI: Small: Demographic-Aware Lexical Semantics","IIS","Robust Intelligence","09/01/2018","08/10/2018","Rada Mihalcea","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Tatiana Korelsky","08/31/2021","$450,000.00","Carmen Banea, Richard Gonzalez","mihalcea@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495","7495, 7923","$0.00","A central challenge in natural language processing is to develop methods for determining how meanings of words relate to one another.  This task is called ""lexical semantics"", because ""lexical"" means ""word"" and ""semantics"" means ""meaning"".  Traditional dictionaries do not solve the problem of lexical semantics, because definitions are often circular or incomplete, especially for the most common words.  Instead, models of lexical semantics are computed by processing large bodies of text, using the principle that pairs of words that often appear in the same contexts must have meanings that are similar along some dimensions.  For example, the words ""man"" and ""boy"" would be inferred to have similar meanings along the dimensions of ""human"" and ""gender"".  However, a limitation of current models is that they assume that the meaning of words is the same for all speakers of a language.  This is plainly false: we know, for example, that English speakers use words differently depending, among other factors, their age, gender, field of work, and geographic location; that is, on the basis of their demographics.  This project will overcome this limitation by developing methods for demographic-aware lexical semantics, where people-centric information complements language-based information.  This work will help improve systems for natural language communication between people and computers, such as Siri or Alexa, as well as improve systems for automatically translating between different languages.<br/><br/>Recent years have witnessed significant progress in research in lexical semantics using corpus-based approaches such as distributional vector-space models and word embeddings. At the same time, the growth of Web 2.0 has led to tremendous volumes of texts, most of which are rich in explicit or implicit demographic information, such as the age, gender, industry, or location of the writer. The goal of this project is to take the next natural step at the confluence of these two trends, and develop methods for demographic-aware lexical semantics, where people-centric information complements language-based information for enhanced linguistic representations that explicitly account for the demographics and traits of the people behind the language.  The project targets the following three main research objectives. First, it develops novel demographic-aware word representations models that account not only for contextual knowledge but also for people-centric information. Methods that are explored include distributional vector-space models that can be composed to create demographic-aware vector-space representations for various demographic profiles, and joint word embeddings that combine generic context-based embeddings with specialized embeddings that reflect the specifics of given demographic dimensions. Second, building upon extensive previous work in behavioral studies targeting the identification of systematic heterogeneity across groups, lab studies are devised to validate the findings from the computational models.  Third, the application of these novel people-centric word representations to three core tasks in natural language processing are explored, ranging from simple to complex, namely: word associations, text similarity, and diversified news.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751482","CAREER: Exploiting the Dynamic Dielectric Behavior of Water to Understand and Predict Polymer Composite Damage Progression","CMMI","Mechanics of Materials and Str, Special Initiatives","05/01/2018","08/28/2019","Landon Grace","NC","North Carolina State University","Standard Grant","Siddiq Qidwai","04/30/2023","$606,962.00","","lgrace2@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","ENG","1630, 1642","013E, 022E, 1045, 116E, 9161, 9178, 9231, 9251","$0.00","This Faculty Early Career Development Program (CAREER) project will focus on improving the safety and performance of polymer composites by discovering the fundamental mechanisms governing the evolution of damage in these next-generation materials. Polymer composite usage is growing rapidly; driven by increasing demand for high-strength, lightweight materials in the automotive, aerospace, and civil infrastructure industries. In these primarily outdoor applications, loss of load-bearing ability over time is driven by material property changes in response to the complex and combined effects of mechanical and environmental stresses. A fundamental understanding of these changes is crucial to safe operation throughout the life cycle of the structure. To derive this understanding, the experimental approach in this research project takes advantage of changes in dielectric behavior of absorbed atmospheric water molecules in response to damage initiation and progression. By tracking the ability of water molecules to rotate in response to an oscillating electromagnetic field, early and non-visible changes in the chemical and physical characteristics of the material can be measured. The new and valuable insight into the mechanisms responsible for the progression of damage derived from these measurements will improve our ability to design more robust materials and better predict impending failure, thus advancing national health, prosperity, welfare, and national defense. The research will be complemented by an effort to provide access to K-12 summer engineering camp activities at North Carolina State University for students from rural and isolated urban communities. A sustainable process for packaging and disseminating these high-value educational resources will be developed and implemented, expanding their impact beyond the students attending the on-site camps and increasing visibility and knowledge of engineering among students who may not otherwise consider or pursue STEM careers.<br/><br/>The overarching goal of the research is to derive the mechanistic underpinnings of polymer composite damage progression across spatial scales in response to coupled thermal, hygroscopic, and mechanical loading. The specific objectives in support of this goal are to: (i) describe the link between water-polymer interaction and topology, nanovoid content, polarity, and hygrothermal aging of fiber-reinforced epoxies; (ii) connect the response of absorbed molecular water to multiscale damage induced by dynamic, fatigue, and impact loading; (iii) use a neural-network technique to extract the salient variables that govern damage progression for use in deriving a mechanistic understanding of the molecular precursors to damage; and (iv) reconcile the experimental and neural-network derived insights with the physical basis of state-of-the-art multiscale, multiphysics simulation techniques. This project will allow the PI to expand the knowledge base in mechanics and materials science, enabling safer and more efficient use of polymer composites across multiple industries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752405","CAREER: Hybrid adaptive optics: a new paradigm for faster, deeper, volumetric microscopy in scattering media","CBET","BioP-Biophotonics","06/01/2018","12/15/2017","Steven Adie","NY","Cornell University","Standard Grant","Leon Esterowitz","05/31/2023","$500,000.00","","sga42@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","ENG","7236","1045","$0.00","Optical microscopy has played a key role in biomedical discoveries for clinical management of disease, but significant challenges remain for applications that require rapid, non-invasive imaging over large volumes, particularly when imaging deep into optically dense biological media. This proposal addresses these limitations by developing new ways of splitting up, and sharing the work of image formation between state-of-the-art computational and hardware approaches. These methods will be demonstrated by imaging biological phenomena that cannot be studied with existing methods. The accompanying education and outreach activities will foster a broader appreciation for biomedical optics and imaging, and train scientists and engineers to effectively interact with, and engage the public. Outreach aspects of this proposal will create experiential and interactive inquiry-based workshops for middle and high school students, develop interactive demonstrations for the Ithaca Sciencenter and train graduate students to effectively communicate their science with the public. <br/><br/><br/>High-throughput volumetric microscopy deep in biological media is important for the study of dynamic biological processes, such as the biophysical interactions associated with collective cell migration, or neural network activity in the mouse brain. Optical coherence microscopy (OCM) and three-photon microscopy (3PM) are currently the modalities that enable the deepest microscopic imaging in scattering biological samples. However, their volumetric imaging speed and penetration depth is currently limited by depth-dependent photon collection, or by wavefront distortions due to aberrations and multiple scattering. This proposal will synergistically combine hardware adaptive optics (AO) and computational adaptive optics (CAO), to dramatically improve the speed and imaging depth range of volumetric OCM and 3PM.This hybrid AO approach will be used to launch new avenues of investigation, including studies on inter-cell coordination of cell traction forces during 3D migration, and investigations on the connection between behavior and spatiotemporal patterns of neural network activity deep in the mouse brain.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849417","NRI: FND: Scalable, Customizable Sensory Solutions for Dexterous Robotic Hands","IIS","NRI-National Robotics Initiati","09/01/2018","08/15/2018","Nitish Thakor","MD","Johns Hopkins University","Standard Grant","David Miller","08/31/2021","$421,727.00","Rahul Kaliki","nthakor@bme.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8013","063Z, 8086","$0.00","This project aims to enhance the sense of touch for robotic hands. The main goal is to develop prosthetic hands with a sense of touch. The touch sensor's primary application is for upper limb amputees. The research team plans both fundamental research and its application. The types of applications of this kind of sensor include: humanoid robots for assistive work and elder-care, surgical robotics, underwater robotic manipulators and spacesuits. Additionally, educational initiatives including internships, training of under-represented minority, and research experiences and summer modules for high school students are planned through the Johns Hopkins Center for Talented Youth. <br/><br/>The technical goal of the project is to build a highly scalable sensor design mimicking different tactile receptors in human skins and encode information from the sensors in a manner analogous to the neural activity of the tactile receptors. The sensor will encode the tactile information at multiple scales, firstly based on different receptor properties, and secondly based on neuron-like encoding of the sensor signals. This technique of encoding the neural activity, known as neuromorphic encoding, converts the sensor activity as an event stream, and from that data obtains finer features. The receptor based sensing along with the various neural network algorithms, for the first time, will provide an approach to texture and shape recognition and, as such, can also be useful for intelligent palpation and tactile perception by dexterous robotic hands.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813153","NSF-BSF: RI: Small: Collaborative Research: Modeling Crosslinguistic Influences Between Language Varieties","IIS","Robust Intelligence","09/01/2018","06/25/2019","Noah Smith","WA","University of Washington","Continuing Grant","Tatiana Korelsky","08/31/2021","$167,464.00","","nasmith@cs.cmu.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7495, 7923","$0.00","Most people in the world today are multilingual. Though multilingualism is a gradual phenomenon, previous research has primarily examined text from second language learners who have not yet achieved fluency. This project focuses on text produced by nonnative but highly fluent speakers. Fluent but nonnative language differs subtly from native, monolingual language in the frequencies of certain concepts, constructions, and collocations. This raises the possibility that language technologies -- typically trained on ""standard"" native language -- are systematically biased in ways that render them less useful for the majority of users.  This project will develop methods to examine large datasets of fluent nonnative language to detect the subtle influences of the native language and deliver natural language processing (NLP) tools for these language varieties. Its methods will be applicable beyond the populations in this study, including NLP-based measurement for social science and research seeking to better understand cognition in the bilingual mind. Native language identification will enable potential applications in language learning, cybersecurity, geolocation, personalization, and more. The project will openly share implementations and data, and will include educational activities that bring research into education.<br/><br/>This project will advance natural language processing techniques to shed light on the differences in language use by fluent speakers with varying linguistic backgrounds:  native speakers, highly fluent nonnative speakers, and translators when translating from another language into English.  It is known that classifiers can be trained to discriminate with high accuracy among these populations, even though humans have difficulty telling them apart. This project will focus on semantic phenomena, which can confound even fluent nonnative speakers. If current NLP models are biased toward native language, then they may not support accurate measurement in nonnative text; the project will develop new techniques to mitigate this bias. This project will deliver a range of new models for native language identification, new measurement models and multi-variety models for language-variety-aware NLP tools, new semantic annotations in several Englishes, and a study on nonnative annotation.  These novel methods for studying variation within a language and building such variation into our NLP systems will lead to unprecedented flexibility in computational models of natural language semantics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812778","NSF-BSF: RI: Small: Collaborative Research: Modeling Crosslinguistic Influences Between Language Varieties","IIS","Robust Intelligence","09/01/2018","09/13/2018","Nathan Schneider","DC","Georgetown University","Continuing Grant","Tatiana Korelsky","08/31/2021","$166,347.00","","nathan.schneider@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7495","7495, 7923","$0.00","Most people in the world today are multilingual. Though multilingualism is a gradual phenomenon, previous research has primarily examined text from second language learners who have not yet achieved fluency. This project focuses on text produced by nonnative but highly fluent speakers. Fluent but nonnative language differs subtly from native, monolingual language in the frequencies of certain concepts, constructions, and collocations. This raises the possibility that language technologies -- typically trained on ""standard"" native language -- are systematically biased in ways that render them less useful for the majority of users.  This project will develop methods to examine large datasets of fluent nonnative language to detect the subtle influences of the native language and deliver natural language processing (NLP) tools for these language varieties. Its methods will be applicable beyond the populations in this study, including NLP-based measurement for social science and research seeking to better understand cognition in the bilingual mind. Native language identification will enable potential applications in language learning, cybersecurity, geolocation, personalization, and more. The project will openly share implementations and data, and will include educational activities that bring research into education.<br/><br/>This project will advance natural language processing techniques to shed light on the differences in language use by fluent speakers with varying linguistic backgrounds:  native speakers, highly fluent nonnative speakers, and translators when translating from another language into English.  It is known that classifiers can be trained to discriminate with high accuracy among these populations, even though humans have difficulty telling them apart. This project will focus on semantic phenomena, which can confound even fluent nonnative speakers. If current NLP models are biased toward native language, then they may not support accurate measurement in nonnative text; the project will develop new techniques to mitigate this bias. This project will deliver a range of new models for native language identification, new measurement models and multi-variety models for language-variety-aware NLP tools, new semantic annotations in several Englishes, and a study on nonnative annotation.  These novel methods for studying variation within a language and building such variation into our NLP systems will lead to unprecedented flexibility in computational models of natural language semantics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812327","NSF-BSF: RI: Small: Collaborative Research: Modeling Crosslinguistic Influences Between Language Varieties","IIS","Robust Intelligence","09/01/2018","09/13/2018","Yulia Tsvetkov","PA","Carnegie-Mellon University","Continuing Grant","Tatiana Korelsky","08/31/2020","$166,000.00","","ytsvetko@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923","$0.00","Most people in the world today are multilingual. Though multilingualism is a gradual phenomenon, previous research has primarily examined text from second language learners who have not yet achieved fluency. This project focuses on text produced by nonnative but highly fluent speakers. Fluent but nonnative language differs subtly from native, monolingual language in the frequencies of certain concepts, constructions, and collocations. This raises the possibility that language technologies -- typically trained on ""standard"" native language -- are systematically biased in ways that render them less useful for the majority of users.  This project will develop methods to examine large datasets of fluent nonnative language to detect the subtle influences of the native language and deliver natural language processing (NLP) tools for these language varieties. Its methods will be applicable beyond the populations in this study, including NLP-based measurement for social science and research seeking to better understand cognition in the bilingual mind. Native language identification will enable potential applications in language learning, cybersecurity, geolocation, personalization, and more. The project will openly share implementations and data, and will include educational activities that bring research into education.<br/><br/>This project will advance natural language processing techniques to shed light on the differences in language use by fluent speakers with varying linguistic backgrounds:  native speakers, highly fluent nonnative speakers, and translators when translating from another language into English.  It is known that classifiers can be trained to discriminate with high accuracy among these populations, even though humans have difficulty telling them apart. This project will focus on semantic phenomena, which can confound even fluent nonnative speakers. If current NLP models are biased toward native language, then they may not support accurate measurement in nonnative text; the project will develop new techniques to mitigate this bias. This project will deliver a range of new models for native language identification, new measurement models and multi-variety models for language-variety-aware NLP tools, new semantic annotations in several Englishes, and a study on nonnative annotation.  These novel methods for studying variation within a language and building such variation into our NLP systems will lead to unprecedented flexibility in computational models of natural language semantics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813949","CSR:Small: High Data Density Short Range Wireless Telemetry for Next Generation IoT Applications","ECCS","CCSS-Comms Circuits & Sens Sys","08/15/2018","11/25/2019","Mohammad Haider","AL","University of Alabama at Birmingham","Standard Grant","Lawrence Goldberg","07/31/2021","$466,416.00","Nasim Uddin","mrhaider@uab.edu","AB 1170","Birmingham","AL","352940001","2059345266","ENG","7564","153E, 9102, 9150, 9251","$0.00","High Data Density Short Range Wireless Telemetry for Next Generation Internet-of-Things (IoT) Applications (SWiT-IoT)<br/><br/>The long-term goal of this SWiT-IoT project is to establish a transformative and multidisciplinary research and innovation on high data-density short-range wireless telemetry and energy-efficient computational platform integrated with custom analog radio-frequency (analog-RF) circuit and system architectures for Internet-of-Things (IoT) applications. Proliferation of wireless sensors and sensor network has enabled widespread deployment of multi-sensors for IoT applications. With the increase of wireless sensors, the cumulative data volume from a large group of sensors is creating bottleneck for real-time data processing and data transmission through a limited wireless spectrum. To alleviate these problems, the SWiT-IoT research project is going to investigate a two-phase approach. On the first phase, the project will investigate a local processing or computational unit for in-situ low-level signal processing, data reduction, and enable high volume of data communication within the specified bandwidth. On the second phase, the project will investigate a novel pulse encoding scheme using orthogonal pulses to compress the data volume further. This two-fold data reduction (local processing and pulse encoding) enables high data-rate support within the specified bandwidth. The compressed data stream will then be fed to an injection-locked power oscillator to drive the antenna for wireless transmission. The proposed transformative large volume wireless data acquisition scheme will influence various sensor related applications and decision-making processes, such as transportation, public health, cortical mapping, smart homes, etc. The education goal is to integrate the artistic skills of underrepresented K-12 students to motivate, engage and help learning STEM materials through a fun loving environment and get them prepared for future STEM careers. The project will continue training undergraduate and graduate students, conduct summer camps, and promote outreach programs through ""It's Electric"", ""C3-STEM"" and UAB CORD program, to increase the number of underrepresented and minorities towards higher education.<br/><br/>In pursue of the research objectives in the SWiT-IoT project, mainly three research goals are targeted: (i) Energy-Efficient Local Processing for Data Reduction, (ii) An energy-efficient analog-RF chip level implementation of multi-order orthogonal pulse generator, and (iii) Spectral efficient orthogonal pulse based analog pulse-sequence encoding. First, a local processing unit will be investigated by employing oscillatory neural network (ONN) and improved spike detector algorithms. By utilizing ultra-low-power self-oscillating nodes, the ONN will perform class-associative pattern recognition task by synchrony or desynchrony among the nodes, and only relay the values of the recognition indicators frequency of synchronization and convergence time, instead of the raw data. A modified nonlinear energy operator based energy-efficient spike detector algorithm will be investigated for detecting signal events e.g. impact loading, action potentials, abrupt changes, etc. and relay the event stamp to reduce the data volume. Second, the SWiT-IoT project will investigate a Modified Hermite Polynomial based multi-order orthogonal pulse generation scheme. An innovative neuro-inspired architecture will be utilized with reduced system complexity, better energy-efficiency, and integrated circuit level implementation with smaller form factor. Third, a novel combinatorial pulse-sequence encoding will allow simultaneous multichannel wireless telemetry with superb spectrum-efficient data density and data security. In this scheme, n distinct multi-order orthogonal pulses will be used to create pulse-sequence for each channel. The use of (n-1) redundant orthogonal pulses for each channel will enable supporting a large number (= n!) of channels and (n-1)!-times of data rate improvement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749368","Syntax-Prosody in Optimality Theory","BCS","Linguistics","07/01/2018","06/04/2020","Junko Ito","CA","University of California-Santa Cruz","Standard Grant","Joan Maling","12/31/2021","$272,879.00","Armin Mester","ito@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","SBE","1311","1311, 9178, 9179, 9251, SMET","$0.00","Syntax is the set of linguistic rules, principles, and processes that govern the structure of sentences. Prosody deals with syllables, rhythmic groups of syllables known as ""feet"", and larger units of speech that contribute to intonation, tone, stress, and speech rhythm. The syntax-prosody interface is the study of how syntactic structures are realized in prosodic form in different languages. The fundamental problem in syntax-prosody research is to precisely predict, for any kind of syntactic structure in a given language, what kind of prosodic parse it will be assigned by the language's grammar. Languages are known to show wide variation in this respect. The tools and analyses developed in this project will enable researchers to explore and explain the extensive cross-linguistic variation observed in the relation between syntactic and prosodic structure, and will thereby extend the basic science of human language, cognition, and learning. <br/><br/>In order to approach the problem in a rigorous and precise way, it is necessary to consider the full set of imaginable prosodic structure candidates for any syntactic input, a task for which no formal tool has so far been developed. Automation is essential since the number of prosodic structure candidates increases exponentially, as length and syntactic complexity increase: Given a particular understanding of what constitutes a legitimate prosodic structure that reflects widely shared assumptions, 3 words receive 48 different parses, 4 words 352, 5 words 2880, etc. The aim of the project is to develop a software program SPOT (Syntax-Prosody in Optimality Theory), a JavaScript application that provides both exhaustive generation of the prosodic candidates, and automatic evaluation of each candidate in terms of a well-defined hierarchy of constraints. Beyond its immediate goals in linguistics, the program has clear application possibilities in natural language processing, speech technology (recognition and production, translation), as well as in second language research and teaching (e.g., to explicate the differences in syntactic and prosodic structure between the first and the second language).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1754211","Visual Perception as Retrospective Bayesian Decoding from High- to Low-level Features in Working Memory","BCS","Perception, Action & Cognition","04/01/2018","08/08/2019","Ning Qian","NY","Columbia University","Continuing Grant","Michael Hout","03/31/2021","$513,163.00","","nq6@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","SBE","7252","7252","$0.00","When looking at a scene, we typically have a quick and accurate perceptual understanding of its high-level category, for example, a home, office, street, or jungle. We rarely pay attention to the scene's low-level properties such as luminance values at various spots unless we are asked to report them, and even then, we are not very accurate about them. The fact that higher-level properties of a scene are more relevant to our behavior than low-level properties has informed global precedence theories of perception. However, experimental studies of the brain have established that lower-level features in a scene are detected before higher-level features; this result somehow led to the commonly used, but rarely tested, assumption that visual perception follows the same low-to-high-level hierarchy of feature detection. This project attempts to resolve this apparent contradiction by separating feature detection and perception, and by integrating visual perception and working memory, the brain's short-term storage of relevant visual information. The project will provide a new computational framework for understanding perception and memory which challenges traditional theories.<br/><br/>Technically, vision can be viewed as involving both encoding and decoding. Encoding refers to how visual stimuli evoke sensory responses in the brain whereas decoding concerns how these responses eventually lead to the subjective perception of the stimuli. A common assumption of many existing models is that decoding follows the same low-to-high-level hierarchy as encoding, but this was never rigorously tested. Additionally, under natural viewing conditions, the small fovea and frequent saccades introduce delays between sensory encoding of different parts of a scene and perceptual integration of the whole scene, suggesting that working memory must be involved in perceptual decoding; yet previous decoding models do not consider working memory. This project aims to address these issues using psychophysical and computational methods, with the specific goal of elucidating the nature of decoding hierarchy in light of working-memory properties. Specifically, compared with lower-level stimulus features, higher-level features are more invariant and categorical, thus requiring less information to specify and permitting more stable maintenance in noisy working memory. The brain should therefore prioritize decoding of reliable higher-level features and then use them to constrain and improve the decoding of unstable lower-level features in memory (when necessary). The project will test some surprising predictions of this retrospective Bayesian decoding theory and develop a neural network implementation of the theory.<br/> <br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800577","Understanding the restructuring of model metal catalysts in reactant gases","CHE","Chemical Catalysis","09/15/2018","06/28/2018","Franklin Tao","KS","University of Kansas Center for Research Inc","Standard Grant","Kenneth Moloy","08/31/2021","$305,084.00","","franklin.feng.tao@ku.edu","2385 IRVING HILL RD","Lawrence","KS","660457568","7858643441","MPS","6884","8037, 9150, 9263","$0.00","Transition metals play key roles as catalysts in applications ranging from chemical and energy production to environmental remediation. Ample evidence indicates that catalyst structures do not remain unchanged during use. Instead, substantial and important restructuring occurs due to rearrangement of the metal atoms to make new structures. This restructuring has major consequences on catalytic properties, sometimes beneficial and sometimes not. Currently, scientists do not understand how the restructuring occurs at the atomic scale or under conditions similar to those used during catalysis. With funding from the NSF Chemical Catalysis Program, this research is providing a fundamental understanding of this chemistry under relevant gas pressures and reaction temperatures. It is being performed with a combination of cutting edge experiments by Dr. Franklin Tao from University of Kansas and computational modeling by Dr. Philippe Sautet from University of California Los Angeles. Drs. Tao and Sautet are incorporating this theoretical modeling and experimental work into the education and training of high school, undergraduate, and graduate students. Summer research internships for public high school students in their laboratories are also being provided.<br/> <br/>With funding from the Chemical Catalysis Program of the Division of Chemistry, Dr. Tao of the University of Kansas and Dr. Sautet from UCLA are combining advanced in situ/operando characterization and first principle based modelling to develop a fundamental understanding of the metal catalyst restructuring process. The research compares several metals (Pt, Pd, Ni, Co, Cu) under a pressure of carbon monoxide and other reactants. The experimental methods include high pressure scanning tunneling microscopy (HP-STM) and operando Ambient Pressure X-ray Photoelectron Spectroscopy. These surface-sensitive surface techniques allow for uncovering surface chemistry and structure of metals under a pressure of gas and provide an initial global view of the mechanism and determine global kinetic rates. Theoretical modelling brings an understanding of the atomistic elementary steps and of their energies. It also provides kinetic results that are compared with experimental measurement, hence validating the approach. Since large systems are required for the calculation, energies are obtained by using an accurate high dimensional neural network potential, previously trained from a density functional theory database. The influence of restructuring on catalytic reaction is being explored with the example of the water-gas shift reaction on Pt and Cu surfaces. Drs. Tao and Sautet are developing an outreach program focused on introducing high-school students to model catalysis research and on undergraduate student training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832740","Group Travel Grant for the Doctoral Consortium of the IEEE Conference on Computer Vision and Pattern Recognition (2018)","IIS","Information Technology Researc, Robust Intelligence","06/01/2018","05/10/2018","Xiaoming Liu","MI","Michigan State University","Standard Grant","Jie Yang","11/30/2018","$20,620.00","","liuxm@cse.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1640, 7495","7495, 7556","$0.00","This grant partially supports the participation of students from US institutions in the Doctoral Consortium at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018 in Salt Lake City, UT. CVPR is the premier annual conference in computer vision with over 5000 senior, junior and student participants from all the world. The broader impacts of this project include supporting the career development of some of the brightest junior researchers in computer vision, contributing to the research community in general by drawing attention to an important aspect of graduate student development, potentially increasing the number of active researchers and educators in STEM, and ensuring that the computer vision community, through its recent graduates, makes fast advances in solving problems that will benefit society as a whole. The Doctoral Consortium aims to have representation from a diverse group of participants in terms of gender, ethnic background, academic institution, and geographic location. <br/><br/>NSF support covers some of the costs for 19 selected US-based graduate students to attend the Doctoral Consortium and CVPR conference. The Doctoral Consortium is to highlight the work of senior PhD students who are within six months of receiving their degrees (including recent graduates), and to give these students an opportunity to discuss their research and career options with faculty and researchers who have relevant expertise and experience. The opportunity to receive advice on their research work and career plans from experts from different institutions, and with potentially different perspectives, is often not available internally at one's own institution. Participants and recipients of travel support are selected by the 2018 CVPR Doctoral Consortium co-chairs. Travel awards partially cover any admissible conference-related costs such as registration, airfare, lodging, and board. This year's Doctoral Consortium features a poster session, one-on-one mentoring, and panel discussion. The last of these components was introduced in last two year's event and was well received.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814510","III: Small: Collaborative Research: A Multi-source Data Driven Optimization Framework for Inter-connected Express Delivery System Design and Inventory Rebalance","IIS","Info Integration & Informatics","08/15/2018","08/13/2018","Hui Xiong","NJ","Rutgers University Newark","Standard Grant","Wei Ding","07/31/2021","$249,977.00","","hxiong@rutgers.edu","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","CSE","7364","7364, 7923","$0.00","The inter-connected express delivery system is very needed for many emerging applications, such as public bike rental service, electric car sharing service, and fresh product delivery. The successful deployment of inter-connected express delivery systems can greatly improve transportation, energy saving, food supply, and urban sustainability. Compared with traditional delivery systems, the inter-connected express delivery system has the following unique characteristics: (1) each station covers a small service area; (2) all stations are internally connected because they can act as inventories or suppliers to each other. There are two fundamental research challenges for the development of the inter-connected express delivery system: how to decide the station locations for a given area and how to timely rebalance the inventories among stations. It is very important to address these fundamental challenges in order to make the inter-connected express delivery system more effective, efficient and sustainable. This project aims to develop a data driven solution for solving these challenges. This study will advance the field of inter-connected express delivery system, expand the curricular content of data mining and optimization, and train undergraduate and graduate students.<br/><br/>This project focuses on two basic research problems: station site selection and station inventory rebalancing optimization. To solve the first problem, this project collects and analyzes a variety of data from different sources, such as historical demand data and geographic data, and combines neural network-based prediction method and combinatorial optimization techniques. To solve the second problem, this project identifies two distinct cases of the inventory rebalancing problem: static rebalancing and dynamic rebalancing. The research objective of the static rebalancing is to minimize the overall travel distance. This project develops a clustering-based heuristic solution for solving the static rebalancing in order to make the solution scalable for practical use. The research objective of the dynamic rebalancing is to minimize the overall unsatisfied demand, which involves much more uncertainty than the static one. This project develops a hybrid approach that combines advanced data mining and stochastic optimization techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751064","CAREER: Functionality-Enhanced Devices for Extending Moore's Law","CCF","Software & Hardware Foundation","01/15/2018","02/11/2020","Pierre-Emmanuel Gaillardon","UT","University of Utah","Continuing Grant","Sankar Basu","12/31/2022","$305,406.00","","pierre-emmanuel.gaillardon@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798","1045, 7945","$0.00","This CAREER project intends to develop high-energy-efficiency computing systems by making a more ""useful"" elementary device rather than only focusing on its performances. This enables addressing the critical scientific question of ""How can we keep pushing computing performance limits""?. For more than four decades, the semi-conductor ecosystem answered the demand for higher levels of performance by manufacturing devices with increasingly small dimensions. Nevertheless, there is still the largely unexplored route of increasing the basic switching primitive of the elementary transistors, i.e., enhancing their functionality rather than focusing only on reducing their size and/or improving their performances. This project is also relevant from an industrial perspective, as it proposes novel solutions to push device and systems performance without overextending investments to reach an ever-larger degree of integration. More importantly, this CAREER project (i) will involve graduate/undergraduate students tackling research on problems that are directly relevant for industry, ultimately boosting their employability, and (ii) will develop a scientific popularization YouTube channel as a mechanism to increase interest and broaden the participation of K-12 students by capitalizing on their online curiosity.<br/><br/>The proposed research aims to developing novel computing systems exploiting Three-Independent-Gate Field Effect Transistors (TIGFETs), a novel device technology capable of device-level functionalities typically not achievable by standard CMOS and leading to major benefits at gate and circuit levels. A TIGFET can, depending on its usage, achieve three totally unique modes of operations: (i) the dynamic reconfiguration of the device polarity; (ii) the dynamic control of the threshold voltage; and (iii) the dynamic control of the subthreshold slope beyond the thermal limit. In order to fully assess the potential of this technology, this CAREER project will (1) fabricate, characterize and model TIGFETs using advanced semi-conductor materials, (2) develop a complete design framework for TIGFETs, including a design kit, novel circuit primitives and dedicated design tools, and (3) evaluate TIGFET-enabled low-energy high-precision neural network computing kernels."
"1814771","III: Small: Collaborative Research: A Multi-source Data Driven Optimization Framework for Inter-connected Express Delivery System Design and Inventory Rebalance","IIS","Info Integration & Informatics","08/15/2018","08/13/2018","Yong Ge","AZ","University of Arizona","Standard Grant","Wei Ding","07/31/2021","$249,888.00","","yongge@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7364","7364, 7923","$0.00","Inter-connected express delivery systems are a recent and rapidly growing category of delivery services.  Examples include public bike rental services, electric car sharing services, and fresh product delivery. The successful deployment of inter-connected express delivery systems can greatly improve transportation, energy saving, food supply, and urban sustainability. Compared with traditional delivery systems, an inter-connected express delivery system has the following unique characteristics: first, each station covers a small service area; second, all stations are internally connected because they can act as inventories or suppliers to each other. There are two fundamental research challenges for the development of the inter-connected express delivery system: how to decide the station locations for a given area and how to timely rebalance the inventories among stations. It is very important to address these fundamental challenges in order to make the inter-connected express delivery system more effective, efficient and sustainable. This project aims to develop a data driven solution for solving these challenges. This study will advance the field of inter-connected express delivery systems, expand the curricular content of data mining and optimization, and train undergraduate and graduate students.<br/><br/>This project focuses on two basic research problems: station site selection and station inventory rebalancing optimization. To solve the first problem, this project collects and analyzes a variety of data from different sources, such as historical demand data and geographic data, and combines neural network-based prediction method and combinatorial optimization techniques. To solve the second problem, this project identifies two distinct cases of the inventory rebalancing problem: static rebalancing and dynamic rebalancing. The research objective of the static rebalancing is to minimize the overall travel distance. This project develops a clustering-based heuristic solution for solving the static rebalancing in order to make the solution scalable for practical use. The research objective of the dynamic rebalancing is to minimize the overall unsatisfied demand, which involves much more uncertainty than the static one. This project develops a hybrid approach that combines advanced data mining and stochastic optimization techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830273","Doctoral Dissertation Research: Documenting Tense, Aspect, Mood and Polarity in a Language with a Complex Verbal System","BCS","DEL DDRIG Document Endangered","12/01/2018","12/12/2018","Jeffrey Good","NY","SUNY at Buffalo","Standard Grant","Joan Maling","05/31/2021","$18,472.00","Braden Brown","jcgood@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","SBE","036Y","1311, 7719, 9179, SMET","$0.00","Time is an important notion that pervades many aspects of daily life. Time is of such importance that all attested human languages have at least one grammatical strategy to encode temporal relationships. Most languages, however, have relatively complex systems of encoding time (which are generally referred to as tense/aspect systems) that interact with several features of the grammar. Individual tense/aspect systems, in addition to their internal complexity, also vary significantly from language to language. Because of the cross-linguistic variation that has been observed, any given language has the potential to offer new insights into the extent of possible temporal encoding strategies, as well as the extent of possible meanings that tense and aspect can carry. The language of study for this project, Iyasa (yko), is a Bantu language spoken in the Littoral Region of southwestern Cameroon by approximately 2,000 people. Tense/aspect studies in Bantu are especially critical as the systems in these languages are noted as being among the most complex in the world. This project will, thus, provide a thorough documentary and descriptive record of Iyasa's system of tense and aspect, utilizing methods in both linguistic fieldwork and linguistic analysis in order to uncover the nuances of the system. Broader impacts include the professional development of a doctoral student, the publicly accessible data, and the presence of an American linguist in Cameroon to help maintain and improve the academic relationship between the two nations. Additionally, the research project and its products can bring linguistic awareness and attention to a language, region, and community of speakers currently under-represented in scientific literature. <br/><br/>Records of tense/aspect systems of endangered language vary greatly in quality from mere labels of each tense and aspect (leaving it to the reader to assume what precise meanings should be associated with each label) to descriptions where nearly every detail related to tense and aspect is included. In order to achieve an adequately detailed description of the Iyasa tense/aspect system, researchers will make use of a wide variety of methodologies. These methodologies include (but are not limited to) administering tense/aspect questionnaires both already developed and newly developed by researchers on this project, analyzing of tense/aspect in natural narrative and conversational discourse, directed semantic elicitation (where a series of contexts that differ only slightly in meaningful ways are provided by the researcher), and directed narratives that are elicited by means of video clips or storyboards. Another important aspect of this project is that it will incorporate insights from a wide range of theories including cognitive theories of tense and aspect that move away from the notion of time as a single linear timeline. This project, in addition to providing important documentary materials and linguistic training opportunities to the Iyasa community, will provide important insights on how to improve computational models of tense and aspect for natural language processing and help strengthen our general understanding of how humans can and do encode time in language.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820462","SBIR Phase I:  Low-cost real-time perception system for self-driving consumer cars","IIP","SBIR Phase I","06/15/2018","06/15/2018","Koji Seto","CA","Apollo AI Inc.","Standard Grant","Muralidharan Nair","07/31/2019","$225,000.00","","kojiseto@apolloaisystems.com","1267 Willis Street, STE 200","Redding","CA","960010400","4087581593","ENG","5371","5371, 8034","$0.00","The broader impact/commercial potential of this project is the practical deployment of a low-cost and low-power real-time perception system in self-driving consumer cars. This edge computing functionality in sensors enables higher reliability and lower cost of overall sensing and computing needed for truly autonomous self-driving. Such innovation will contribute significantly to the early and widespread availability of safety and convenience benefits to consumers. Furthermore, the advanced perception system will have a potential long-term impact on robotics in general, which can lead to creation of new markets and new lifestyles.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project aims to develop efficient algorithms and software implementation of a real-time perception system to enable the use of low-cost computing systems for self-driving cars. The algorithms provide a novel way of using image features to perform simultaneous localization and mapping (SLAM) with 100 times less computational costs than the existing algorithms. They also include a truly novel neural network to fuse the image feature and light detection and ranging (LiDAR) features and perform object detection, which has 100 times less complexity compared to the state-of-the-art method. These reduced-complexity algorithms can be implemented on low-power and low-cost SoC processors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833638","Workshop: Student Travel to the 2018 Abusive Language Online Conference","IIS","Information Technology Researc, HCC-Human-Centered Computing, Robust Intelligence","08/01/2018","07/29/2018","Ruihong Huang","TX","Texas A&M Engineering Experiment Station","Standard Grant","Ephraim Glinert","07/31/2020","$20,000.00","","huangrh@cse.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","1640, 7367, 7495","7367, 7495, 7556","$0.00","This award will support student travel to attend the second edition of a workshop focused on abusive language in online environments. The workshop, the 2018 Abusive Language Online Conference, is co-located with the 2018 Conference on Empirical Methods in Natural Language Processing, October 31 - November 4, 2018, in Brussels, Belgium. The Workshop will develop students' skills and knowledge to use multidisciplinary approaches and computational methods for research on abusive language in contexts such as social media platforms. Students will learn about policies related to these behaviors and ways to develop tools and processes to allow relevant parties to respond quickly to abusive language online. Students will engage with researchers to develop ways to categorize and classify content and ways to evaluate guidelines and standards. Activities will include panel discussions and plenary talks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834899","Student Travel Grants for the Second International Workshop on The Bright and Dark Side of Computer Vision: Challenges and Opportunities for Privacy and Security","CNS","Secure &Trustworthy Cyberspace","06/01/2018","05/31/2018","David Crandall","IN","Indiana University","Standard Grant","Wei-Shinn Ku","05/31/2019","$10,000.00","Apu Kapadia","djcran@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8060","025Z, 7434, 7556","$0.00","This award funds student and speaker travel to a workshop that brings together researchers from the computer vision and the privacy and security communities: The Second International Workshop on Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security.  Computer vision is increasingly practical and useful in computational systems and has both strong potential benefits and risks for individuals' privacy and security.  The workshop brings together privacy, security, and computer vision experts to discuss how to better understand the potential threats of computer vision to people's security and privacy, as well as the potential opportunities and applications for enhancing them. <br/><br/>Providing these travel awards will support a high quality workshop in emerging research areas of computer security and computer vision by increasing and broadening the pool of participants who will be able to attend.  In particular, the workshop organizers will work to include students from historically underrepresented groups in computing.  The impact will not only be on the students who are supported directly by the grants, but also on other participants, who will benefit from the increased diversity of voices that the award will enable.  Increased attendance also helps improve networking, idea exchange, and partnerships for all attendees of the workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823652","Group Travel Grant for the PhD Forum to be Held in Conjunction with IEEE Winter Conference on Applications of Computer Vision (2018)","IIS","Robust Intelligence","03/01/2018","02/08/2018","Nathan Jacobs","KY","University of Kentucky Research Foundation","Standard Grant","Jie Yang","02/28/2019","$13,625.00","","jacobs@cs.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7495","7495, 7556, 9150","$0.00","This grant partially supports the participation of students from US institutions in the PhD Forum at the 2018 IEEE Winter Conference on Applications of Computer Vision (WACV) in Lake Tahoe, California/Nevada. WACV is the premier annual conference focused on the applications of computer vision. It is held in the United States and attended by members of the international research community. The goal of the PhD Forum is to highlight the work of PhD students working in computer vision and to give these students an opportunity to discuss their research and career options with senior researchers in the field. The broader impacts of this project include supporting the career development of some of the brightest junior researchers in computer vision, contributing to the research community in general by drawing attention to an important aspect of graduate student development, potentially increasing the number of active researchers and educators in STEM, and ensuring that the computer vision community, through its recent graduates, makes fast advances in solving problems that will benefit society as a whole. The PhD Forum aims to have representation from a diverse group of participants in terms of gender, ethnic background, academic institution, and geographic location.<br/><br/>Support from the National Science Foundation covers some of the costs for 10 selected US-based graduate students to attend the conference. Participants and recipients of travel support are selected by the 2018 WACV PhD Forum Chair. For the graduate student participants, this event is an opportunity to receive advice on their research work and career plans from experts from different institutions, with potentially different perspectives than their research supervisor. For the participating senior researchers, this is an opportunity to have a meaningful discussion with a promising young researcher and to give back to the community through their mentorship. This year's PhD Forum features a poster session and one-on-one mentoring.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807660","Collaborative Research:  Multimodal Sensing and Analytics at Scale:  Algorithms and Applications","ECCS","CCSS-Comms Circuits & Sens Sys","09/01/2018","08/21/2018","Nikolaos Sidiropoulos","VA","University of Virginia Main Campus","Standard Grant","Lawrence Goldberg","08/31/2021","$200,000.00","","nikos@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","ENG","7564","153E","$0.00","Finding highly correlated latent factors in multimodal signals and data: Scalable algorithms and applications in sensing, imaging, and language processing<br/><br/>Abstract: Multimodal signals and data arise naturally in many walks of science and engineering, and our digital society presents ever-increasing opportunities to collect and extract useful information from such data. For example, brain magnetic resonance imaging and electro-encephalography are two modes of sensing brain activity that can offer different ""views"" of the same set of patients (entities). Co-occurrence frequencies of a given set of words in different languages is another example. Crime, poverty, welfare, income, tax, school, unemployment, and other types of social data offer different views of a given set of municipalities. Integrating multiple views to extract meaningful common information is of great interest, and finds a vast amount of timely applications -- in brain imaging, machine translation, landscape change detection in remote sensing, and social science research, to name a few.  However, existing multiview analytics tools -- notably (generalized) canonical correlation analysis [(G)CCA] -- are struggling to keep pace with the size of today's datasets, and the problem is only getting worse. Furthermore, the complex structure and dynamic nature of some of the underlying phenomena are not accounted for in classical GCCA. This project will provide much needed scalable and flexible computational tools for GCCA-based multimodal sensing and analytics, thereby benefiting a large variety of scientific and engineering applications. It will produce a framework allowing for plug-and-play incorporation of application-specific prior information, and distributed implementation. Beyond linear and batch GCCA, nonlinear GCCA and streaming GCCA will be considered. These are appealing and timely for many applications, but associated computational tools are sorely missing.<br/><br/>In terms of theory and methods, many key aspects of GCCA (such as convergence properties, distributed implementation, and streaming variants) are still poorly understood. The research will provide a set of high-performance computational tools that are backed by advanced optimization theory and rigorous convergence guarantees. The research will evolve along the following synergistic thrusts: 1) scalable and stochastic GCCA algorithms; 2) distributed, streaming and nonlinear GCCA algorithms; and 3) validation, using a series of timely and important applications in remote sensing, brain imaging, natural language processing, and sensor array processing. Devising scalable, flexible, streaming, and nonlinear GCCA algorithms is very well-motivated for modern sensing and analytics problems which involve rapidly increasing amounts of data with unknown underlying dynamics. Using GCCA for large-scale dynamic and complex data also poses very challenging and exciting modeling and optimization problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808159","Collaborative Research: Multimodal Sensing and Analytics at Scale: Algorithms and Applications","ECCS","CCSS-Comms Circuits & Sens Sys","09/01/2018","04/27/2020","Xiao Fu","OR","Oregon State University","Standard Grant","Lawrence Goldberg","08/31/2022","$265,991.00","","xiao.fu@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","ENG","7564","153E, 9102, 9251","$0.00","Finding highly correlated latent factors in multimodal signals and data: Scalable algorithms and applications in sensing, imaging, and language processing<br/><br/>Abstract: Multimodal signals and data arise naturally in many walks of science and engineering, and our digital society presents ever-increasing opportunities to collect and extract useful information from such data. For example, brain magnetic resonance imaging and electro-encephalography are two modes of sensing brain activity that can offer different ""views"" of the same set of patients (entities). Co-occurrence frequencies of a given set of words in different languages is another example. Crime, poverty, welfare, income, tax, school, unemployment, and other types of social data offer different views of a given set of municipalities. Integrating multiple views to extract meaningful common information is of great interest, and finds a vast amount of timely applications -- in brain imaging, machine translation, landscape change detection in remote sensing, and social science research, to name a few.  However, existing multiview analytics tools -- notably (generalized) canonical correlation analysis [(G)CCA] -- are struggling to keep pace with the size of today's datasets, and the problem is only getting worse. Furthermore, the complex structure and dynamic nature of some of the underlying phenomena are not accounted for in classical GCCA. This project will provide much needed scalable and flexible computational tools for GCCA-based multimodal sensing and analytics, thereby benefiting a large variety of scientific and engineering applications. It will produce a framework allowing for plug-and-play incorporation of application-specific prior information, and distributed implementation. Beyond linear and batch GCCA, nonlinear GCCA and streaming GCCA will be considered. These are appealing and timely for many applications, but associated computational tools are sorely missing.<br/><br/>In terms of theory and methods, many key aspects of GCCA (such as convergence properties, distributed implementation, and streaming variants) are still poorly understood. The research will provide a set of high-performance computational tools that are backed by advanced optimization theory and rigorous convergence guarantees. The research will evolve along the following synergistic thrusts: 1) scalable and stochastic GCCA algorithms; 2) distributed, streaming and nonlinear GCCA algorithms; and 3) validation, using a series of timely and important applications in remote sensing, brain imaging, natural language processing, and sensor array processing. Devising scalable, flexible, streaming, and nonlinear GCCA algorithms is very well-motivated for modern sensing and analytics problems which involve rapidly increasing amounts of data with unknown underlying dynamics. Using GCCA for large-scale dynamic and complex data also poses very challenging and exciting modeling and optimization problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750695","CAREER: Semantic Divergences Across the Language Barrier","IIS","Robust Intelligence","02/01/2018","02/18/2020","Marine Carpuat","MD","University of Maryland College Park","Continuing Grant","D.  Langendoen","01/31/2023","$321,756.00","","marine@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","1045, 7495","$0.00","Despite the explosion of online content worldwide, much information is currently isolated by language barriers. While multilingual users and translators can help, the diversity and scale of online content make it impossible for humans alone to break the language barrier. Automated tools are needed to support and supplement their work. This project introduces computational representations and methods to compare and contrast the meaning of text in different languages. The resulting models will be useful to develop language technology that can support cross-lingual communication, and cross-cultural understanding, including and augmenting machine translation, by providing support for second language learners, volunteer translators, and security analysts. This CAREER project integrates research with education by using activities motivated by the practical problem of translating Wikipedia to illustrate the challenges of language technology developed on inevitably biased data. These activities target high-school and undergraduate students outside of computer science, as well as computer scientists of diverse backgrounds at the undergraduate and graduate level.<br/><br/>Cross-lingual work in natural language processing currently relies on the assumption that a source text and its translation are equivalent in meaning in the two languages, and that they can be decomposed into smaller equivalent units by aligning sentences, phrases and words. Yet, content conveyed in two languages is rarely exactly equivalent: the same topics or events can be discussed from widely different perspectives, and even faithful translations can be hard to understand without the appropriate linguistic and cultural background knowledge. Building on and connecting distinct bodies of work on machine translation and semantic analysis, this project provides techniques to detect and explain nuanced differences between words and sentences in different languages. We characterize semantic divergences, differences in meaning across languages, using an expressive set of semantic relations between words and sentences. We use the resulting models to improve machine translation quality, and to explain translations to readers of various backgrounds."
"1828199","Workshop on Natural Language Processing for Internet Freedom","SES","Secure &Trustworthy Cyberspace","04/01/2018","03/06/2018","Anna Feldman","NJ","Montclair State University","Standard Grant","Sara Kiesler","03/31/2021","$15,760.00","","feldmana@mail.montclair.edu","1 Normal Avenue","Montclair","NJ","070431624","9736556923","SBE","8060","025Z, 065Z, 7434, 7556","$0.00","This workshop brings together natural language processing (NLP) researchers whose work contributes to the free flow of information on the Internet. It should foster collaboration across disciplines, and between theoreticians and practitioners working in academic or industrial settings. This workshop could have a transformative impact on society by getting closer to achieving Internet freedom in countries where accessing and sharing of information are strictly controlled by censorship.<br/><br/>The topics of interest in this workshop include censorship detection, censorship circumvention<br/>techniques, linguistically inspired countermeasures for Internet censorship, and techniques to<br/>empirically measure Internet censorship across communication platforms. The workshop includes researchers working in the field of NLP, computer science and social sciences. Graduate students, faculty, and researchers from industry will participate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828268","MRI: Acquisition of a microfluidic-based 3D printer for additive manufacturing of biomaterials for fabrication of tissue-on-a-chip models.","DMR","Major Research Instrumentation","09/01/2018","08/22/2018","Binata Joddar","TX","University of Texas at El Paso","Standard Grant","Guebre Tessema","08/31/2021","$255,790.00","Thomas Boland, Katja Michael, Namsoo Kim, David Roberson","bjoddar@utep.edu","ADMIN BLDG RM 209","El Paso","TX","799680001","9157475680","MPS","1189","","$0.00","This award from the Major Research Instrumentation program supports the University of Texas at El Paso (UTEP) with the acquisition of a Lab-on-a-Printer (LOP) with RX1 Bioprinting Platform. The instrument will enable accelerated fundamental 3D-bioprinting research at UTEP. The LOP technology is uniquely capable of combining multiple cell types and biomaterial inputs on a single printhead cartridge, enabling the precise deposition of different cells and materials in 3D to recreate and mimic the complex structure of real tissue. The RX1 Bioprinting Platform uses the LOP technology to rapidly and precisely construct functional 3D living tissue, with an advanced material processing capability providing accurate control over patterning of the biological building blocks, rapidly.  At UTEP, the RX1 Bioprinting Platform will be used in several high impact projects including, engineering tissues with blood vessels, the human heart wall, tissues for nerve regeneration and developmental biology, and the application of novel 2D/advanced materials for in-vitro studies. Concurrently, the RX1 Bioprinting Platform will provide sustained and cross-disciplinary studies while contributing to and advancing STEM education and training by providing students access to cutting-edge teaching, research technologies and opportunities.<br/><br/> <br/>The main objective of the researchers is to employ the Lab-on-a-Printer (LOP)   to utilize hydrogels (synthetic and naturally derived) as scaffolds for bioprinting aimed at tissue-on-a-chip studies. In addition, non-hydrogel type materials such as polymer blends (including Polyurethane, PCL-PLLA), shape memory polymers and nanocomposites with nanowire inclusions will also be targeted. The 3D-printed constructs will include biological neural network development, dose optimization/ minimization of small molecule based therapeutics and enhanced dielectric energy storage and energy harvesting for energy related applications. The LOP technology can easily enable printing of soft structures (such as tissues) which provides needed exploration of biomaterial with advantageous properties than currently possible. The instrument will support multidisciplinary, team-based opportunities involving Engineering faculty at UTEP and faculty from other scientific disciplines and help establish new critical regional collaborations within the state of Texas and neighboring New Mexico. The technology and hands-on use of this printer will be incorporated into existing or new courses with laboratory component, exposing nearly 100 students annually to this technology. It will strengthen outreach to the community, local middle- and high-school students. It will lead to new industry-academic collaborations benefiting both the scientific community at UTEP and industry, nationwide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836518","EAGER Real-D: Real-time Data-Based Modeling and Control of Plasma-Enhanced Atomic Layer Deposition","CBET","Special Initiatives","09/01/2018","08/02/2018","Panagiotis Christofides","CA","University of California-Los Angeles","Standard Grant","Raymond Adomaitis","08/31/2021","$200,000.00","","pdc@seas.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","ENG","1642","7237, 7916, 8037","$0.00","CBET-1836518<br/>PI: Christofides, Panagiotis D. <br/> <br/>Next generation electronic devices require the use of improved materials and very precise material processing techniques. To reduce feature sizes and improve energy efficiency, the devices employ extremely thin layers, high aspect ratios, atomically-sharp interfaces, or any combination thereof. Due to inherent difficulties in applying real-time in situ monitoring and control of film properties, factory operators typically rely on batch thin film deposition and etching cycles, followed by scanning electron microscopy (SEM) or x-ray photoelectron spectroscopy (XPS) characterization of the deposited thin films to determine the effect of controllable reactor parameters on the resulting product. This empirical approach reduces productivity and fails to provide complete data on the behavior and operation of chambered reactors common to thin-film processing. Multiscale computational fluid dynamics (CFD) modeling provides a means for addressing these concerns by reducing empiricism and allowing the development of complete data sets that can be used to optimize and control reactor operating conditions in real time. <br/><br/>The proposed research is exploratory in nature and focuses on developing a multiscale CFD modeling and control framework that can enable control of thin film manufacturing via plasma-enhanced atomic layer deposition (PE-ALD) to optimize in real time the microstructure of the deposited thin films. CFD models have been shown to capture the complex reaction and transport phenomena present within plasma charged reactors, while microscopic models, typically based on kinetic Monte Carlo (kMC) algorithms, have successfully reproduced the surface features of deposited films. A multiscale CFD model encompassing both domains would represent a significant step forward in understanding of thin-film processing via PE-ALD and could allow for improved real-time online monitoring and control of chambered reactor operations. However, such a model will be unsuitable for the development of real-time optimizers and model-based controllers because CFD simulations are generally computationally demanding and cannot be linked to online model predictive control schemes. Nonetheless, the proposed multiscale CFD model can be used as a risk-free and effective tool to investigate previously unexplored operating conditions of the PE-ALD reactor and create a database which can be utilized to derive a computationally efficient data-driven model for PE-ALD real-time control. The multiscale CFD model which will be developed will allow for the application of a novel, computationally efficient data-based Bayesian artificial neural network (ANN). Furthermore, data-driven models developed using the reactor model will form the basis for real-time process optimization and control. The data-based model will be used to develop real-time operational decision strategies for PE-ALD that reduce thin film layer deposition times, which constitutes a necessary step for adoption of this technology. The proposed methodology can form a basis for real-time optimization and control of next generation deposition systems and may be adapted to a wide range of industrial processes. Dissemination of research results will include web-based access to a database and results repository. In addition to training a PhD student, the PI plans to integrate research results into the curriculum through the inclusion of CFD modeling and its integration with control in the Advanced Process Control course that the PI offers to both graduate and undergraduate UCLA students, as well as through the integration of CFD concepts and tools into the undergraduate numerical methods, process design and process control core courses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760928","Collaborative Research: Dynamics and Control of Long Range Micro Air Vehicles Inspired by Monarch Butterflies","CMMI","Dynamics, Control and System D","06/01/2018","05/17/2018","Taeyoung Lee","DC","George Washington University","Standard Grant","Robert Landers","05/31/2021","$260,895.00","","tylee@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","ENG","7569","030E, 034E, 8024","$0.00","This Collaborative research project will study the biomechanics of Monarch butterfly flight, with the goal of creating engineered flight vehicles with unprecedented capabilities. The seemingly fragile Monarch exhibits the longest flight range among insects. Individual Monarch butterflies may travel up to four thousand kilometers during the annual migration between North America and Central America. This project will examine the distinguishing characteristics of Monarch butterfly flight, including the slow tempo of the flapping wings, the effects of wing flexibility, and the mechanical coupling of wing and body movements. Furthermore, the Monarch flies at relatively high altitudes. Glider pilots have observed Monarch butterflies soaring on thermal currents at altitudes up to 1,250 km, and their overwintering grounds are at altitudes of up to 3,300 km. This project will examine whether the unmatched range of the Monarch among similarly sized animals lies in the combination of large, flexible wings, slow flapping speeds, and high-altitude flight. A multidisciplinary team will integrate expertise in computational mechanics, biological experiments, fluid dynamics, and nonlinear controls to uncover the physical mechanism underlying the highly efficient Monarch flight. This knowledge will be applied to the creation of transformative, bio-inspired micro-air vehicles with enhanced flight efficiency and superior flight range. Micro-air vehicles with extended flight range will improve the national quality of life by enabling long-term monitoring of environmental hazards. These vehicles will enhance national security by allowing long-term surveillance of large areas, and by providing long-range reconnoitering capacity for search and rescue. Engineering models of Monarch flight will also contribute to the understanding of their migration patterns, and thereby support the conservation of this endangered species.<br/><br/>The primary scientific objective of this project is to test the hypothesis that high-altitude flight is a critical component to the long-range flight characteristics of the Monarch butterfly. This will be achieved with a series of experimental, computational, and theoretical research efforts. The flight maneuvers of live Monarch will be measured by a motion capture system in a low-pressure chamber simulating the ambient environment at various altitudes. The measurements will be validated with computational fluid dynamics simulations for the unsteady viscous flows around flexible flapping wings integrated with a multibody dynamics model representing the thorax and the abdomen deformation. The resulting aerodynamic model will be approximated by an artificial neural network for real-time dynamic simulation, from which a nonlinear feedback control system will be constructed via Floquet-Lypuanov theory. The fidelity of the computational dynamic model and the feedback control system will be verified against experiments with Monarch butterfly inspired micro-air vehicle and live butterfly flight measurements. These will provide a comprehensive analysis of the low-frequency flapping dynamics of an articulated, flexible multibody system representing the remarkable flight characteristics of Monarch butterflies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761618","Collaborative Research: Dynamics and Control of Long Range Micro Air Vehicles Inspired by Monarch Butterflies","CMMI","Dynamics, Control and System D","06/01/2018","05/17/2018","Chang-kwon Kang","AL","University of Alabama in Huntsville","Standard Grant","Robert Landers","05/31/2021","$276,876.00","","ck0025@uah.edu","301 Sparkman Drive","Huntsville","AL","358051911","2568242657","ENG","7569","030E, 034E, 8024, 9150","$0.00","This Collaborative research project will study the biomechanics of Monarch butterfly flight, with the goal of creating engineered flight vehicles with unprecedented capabilities. The seemingly fragile Monarch exhibits the longest flight range among insects. Individual Monarch butterflies may travel up to four thousand kilometers during the annual migration between North America and Central America. This project will examine the distinguishing characteristics of Monarch butterfly flight, including the slow tempo of the flapping wings, the effects of wing flexibility, and the mechanical coupling of wing and body movements. Furthermore, the Monarch flies at relatively high altitudes. Glider pilots have observed Monarch butterflies soaring on thermal currents at altitudes up to 1,250 km, and their overwintering grounds are at altitudes of up to 3,300 km. This project will examine whether the unmatched range of the Monarch among similarly sized animals lies in the combination of large, flexible wings, slow flapping speeds, and high-altitude flight. A multidisciplinary team will integrate expertise in computational mechanics, biological experiments, fluid dynamics, and nonlinear controls to uncover the physical mechanism underlying the highly efficient Monarch flight. This knowledge will be applied to the creation of transformative, bio-inspired micro-air vehicles with enhanced flight efficiency and superior flight range. Micro-air vehicles with extended flight range will improve the national quality of life by enabling long-term monitoring of environmental hazards. These vehicles will enhance national security by allowing long-term surveillance of large areas, and by providing long-range reconnoitering capacity for search and rescue. Engineering models of Monarch flight will also contribute to the understanding of their migration patterns, and thereby support the conservation of this endangered species.<br/><br/>The primary scientific objective of this project is to test the hypothesis that high-altitude flight is a critical component to the long-range flight characteristics of the Monarch butterfly. This will be achieved with a series of experimental, computational, and theoretical research efforts. The flight maneuvers of live Monarch will be measured by a motion capture system in a low-pressure chamber simulating the ambient environment at various altitudes. The measurements will be validated with computational fluid dynamics simulations for the unsteady viscous flows around flexible flapping wings integrated with a multibody dynamics model representing the thorax and the abdomen deformation. The resulting aerodynamic model will be approximated by an artificial neural network for real-time dynamic simulation, from which a nonlinear feedback control system will be constructed via Floquet-Lypuanov theory. The fidelity of the computational dynamic model and the feedback control system will be verified against experiments with Monarch butterfly inspired micro-air vehicle and live butterfly flight measurements. These will provide a comprehensive analysis of the low-frequency flapping dynamics of an articulated, flexible multibody system representing the remarkable flight characteristics of Monarch butterflies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832817","Fragile-to-Strong Transitions in Phase-Change Materials for Next-Generation Memory Devices","DMR","CERAMICS","09/01/2018","07/08/2019","Pierre Lucas","AZ","University of Arizona","Continuing Grant","Lynnette Madsen","08/31/2022","$562,757.00","","pierre@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","MPS","1774","","$0.00","NON-TECHNICAL DESCRIPTION: With the exponential growth of information technologies, the ability to store huge amounts of data is becoming a major societal need. Phase-change materials are some of the most promising materials for future data-storage applications. They are the key component enabling high density optical data storage such as rewritable DVD (digital versatile disc) and nonvolatile computer memories (phase-change random access memory (PC-RAM)). However, they currently suffer from aging issues that lead to degradation over time and eventually to data loss. This research aims at developing a new generation of phase-change materials that are immune to this data-loss phenomenon. Moreover, such materials can enable emerging technologies (high density multilevel memories and ultrafast, artificial neuron-like processors) that rely on highly stable phase-change materials. Through the process of conducting this research, graduate students are being trained in the field of materials science and engineering. Materials expertise is in high demand in high technology sectors such as the microelectronic industry.<br/><br/>TECHNICAL DETAILS: The discovery of phase-change materials exhibiting a pronounced fragile-to-strong transition is key to the development of memory devices that are immune to the data-loss phenomenon currently plaguing memory technologies. The discovery of these new materials is achieved by characterizing the thermodynamic and structural behavior of families of phase change materials. The goal is to identify materials that are immune to structural relaxation. These materials are key in enabling emerging technologies that rely on fractional changes in physical properties and are particularly sensitive to drift. This research therefore supports the development of transformational new technologies such as artificial neural-network processor for ultra-fast computing. From a fundamental point of view, this research provides key insight into the origin of fragile-to-strong transitions and answers outstanding related questions in glass science such as the origin of beta-relaxation and polyamorphism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800601","Understanding the restructuring of model metal catalysts in reactant gases","CHE","Chemical Catalysis","09/15/2018","06/28/2018","Philippe Sautet","CA","University of California-Los Angeles","Standard Grant","Kenneth Moloy","08/31/2021","$320,190.00","","sautet@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","6884","8037, 9263","$0.00","Transition metals play key roles as catalysts in applications ranging from chemical and energy production to environmental remediation. Ample evidence indicates that catalyst structures do not remain unchanged during use. Instead, substantial and important restructuring occurs due to rearrangement of the metal atoms to make new structures. This restructuring has major consequences on catalytic properties, sometimes beneficial and sometimes not. Currently, scientists do not understand how the restructuring occurs at the atomic scale or under conditions similar to those used during catalysis. With funding from the NSF Chemical Catalysis Program, this research is providing a fundamental understanding of this chemistry under relevant gas pressures and reaction temperatures. It is being performed with a combination of cutting edge experiments by Dr. Franklin Tao from University of Kansas and computational modeling by Dr. Philippe Sautet from University of California Los Angeles. Drs. Tao and Sautet are incorporating this theoretical modeling and experimental work into the education and training of high school, undergraduate, and graduate students. Summer research internships for public high school students in their laboratories are also being provided.<br/> <br/>With funding from the Chemical Catalysis Program of the Division of Chemistry, Dr. Tao of the University of Kansas and Dr. Sautet from UCLA are combining advanced in situ/operando characterization and first principle based modelling to develop a fundamental understanding of the metal catalyst restructuring process. The research compares several metals (Pt, Pd, Ni, Co, Cu) under a pressure of carbon monoxide and other reactants. The experimental methods include high pressure scanning tunneling microscopy (HP-STM) and operando Ambient Pressure X-ray Photoelectron Spectroscopy. These surface-sensitive surface techniques allow for uncovering surface chemistry and structure of metals under a pressure of gas and provide an initial global view of the mechanism and determine global kinetic rates. Theoretical modelling brings an understanding of the atomistic elementary steps and of their energies. It also provides kinetic results that are compared with experimental measurement, hence validating the approach. Since large systems are required for the calculation, energies are obtained by using an accurate high dimensional neural network potential, previously trained from a density functional theory database. The influence of restructuring on catalytic reaction is being explored with the example of the water-gas shift reaction on Pt and Cu surfaces. Drs. Tao and Sautet are developing an outreach program focused on introducing high-school students to model catalysis research and on undergraduate student training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815619","CSR: Small: Towards Efficient Deep Inference for Mobile Applications","CNS","Special Projects - CNS, CSR-Computer Systems Research","07/01/2018","02/28/2019","Tian Guo","MA","Worcester Polytechnic Institute","Standard Grant","Erik Brunvand","06/30/2021","$515,723.00","Xiangnan Kong","tian@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","1714, 7354","7923, 9251","$0.00","An ever-increasing number of mobile applications are using deep learning models to provide novel and useful features, such as language translation and object recognition. These features are supported by passing input data, for example a photo or an audio clip, to complex models in order to generate meaningful output. However, mobile applications that use deep learning models currently need to choose between prediction accuracy and speed at development time. This can lead to poor user experience due to reasons such as running state-of-the-art models on older mobile devices. <br/><br/>The proposed MODI (MObile Deep Inference) project outlines new research in designing and implementing a mobile-aware deep inference platform that combines innovations in both algorithm and system optimizations. The proposed work will address mobile deep inference performance problems by enabling flexible, fine-grained model partition and layer-based inference execution, as well as mobile-specific model designs. In addition, MODI enables a scalable mobile deep inference paradigm with efficient model management both on-device and in the cloud. <br/><br/>The project will empower deep learning to provide useful features for mobile applications with significantly improved performance. Consequently, this project will open doors to allow running optimized deep learning models on much more resource-constrained devices such as embedded devices. The MODI project can be used as a standalone cloud system or integrated with existing general inference serving platforms by incorporating its mobile-specific optimizations, thereby increasing adoption. <br/><br/>The broader impacts of the project will include graduate and undergraduate courses that incorporate research results, outreach to expose undergraduates and K-12 students to research in both computer systems and deep learning. In addition, project related source code and other resources will be released to the research community through the project website at http://tianguo.info/projects/modi.html<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755695","CRII: SCH: Semi-Supervised Physics-Based Generative Model for Data Augmentation and Cross-Modality Data Reconstruction","IIS","Smart and Connected Health","06/01/2018","05/31/2018","Sarah Ostadabbas","MA","Northeastern University","Standard Grant","Wendy Nilsen","05/31/2021","$168,698.00","","ostadabbas@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8018","8018, 8228","$0.00","Deep learning approaches have been rapidly adopted across a wide range of fields because of their accuracy and flexibility, but require large labeled training sets. This presents a fundamental problem for applications with limited, expensive or private data, such as in healthcare. One example of these applications with the small data challenges is human in-bed pose and pressure estimation. In-bed pose estimation can be a critical part of prevention, prediction, and management of movement-related problems like pressure ulcers. These pressure ulcers often lead to costly and painful conditions such as bedsores. In this research, we propose a semi-supervised generative model based on novel data augmentation and cross-modality data reconstruction techniques to expand the use of powerful deep learning approaches to the in-bed pose and pressure estimation problems. This grant will directly fund the education and mentorship of graduate students involved in researching these problems. In addition, middle school and high school students will be engaged through summer school mentorship programs at Northeastern University.The educational outreach funded by this grant will be used to mentor at schools primarily serving minority student populations. This comprehensive mentorship from middle school to PhD creates a pipeline of experienced students in this important area. The PI actively maintains a diverse research group which includes 50% women and other members of under-represented groups. <br/><br/>This proposed research explores the use of semi-supervised physics-based generative models to bridge the gap between state-of-the-art deep learning techniques and the small data problem common in personalized healthcare and other data-limited domains. The use of a physics-based approach to generate image data from a low-dimensional parameter space is unique and transformative. This proposal organizes the research to two Thrusts: (I) data augmentation, which synthesizes the large training set required to train a deep learning model to recognize the in-bed pose from an image; and (II) cross-modality data reconstruction, which extracts pose parameters from one image modality to generate data in another image modality. The success of the data augmentation will be measured by using the synthesized image data to train a network, which will be tested against deep and non-deep models trained on publicly-available pose datasets. The accuracy of the pressure image reconstruction will be tested by comparing the results to pressure images taken from a high-resolution pressure sensing mat. The successful completion of this project enables (1) the use of high-accuracy deep learning techniques for robustly recognizing objects and object poses for which articulated 3D models are available or can be generated; and (2) generating highly realistic images of posable figures in one sensory domain using data from another, when one sensory domain is cheaper or easier to gather data in than others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832034","Excellence in Research: Collaborative Research: Strengthen the Foundation of Big Data Analytics via Interdisciplinary Research among HBCUs","CNS","HBCU-EiR - HBCU-Excellence in","10/01/2018","09/12/2018","Lei Huang","TX","Prairie View A & M University","Standard Grant","Marilyn McClure","09/30/2021","$499,998.00","Yonggao Yang","lhuang@pvamu.edu","P.O. Box 519","Prairie View","TX","774460519","9362611689","CSE","070Y","041Z, 1714","$0.00","This project will be implemented by an interdisciplinary research team from two neighboring historical Black Colleges and Universities (HBCUs), Prairie View A&M University (PVAMU) and Texas Southern University (TSU), with the goals of strengthening the theoretical foundation of big data analytics and developing a novel deep learning software package based on this enhanced foundation. Challenges around big data impacts many areas and encourages exciting further investigation to understand the complex requirements of real-world applications. <br/><br/>Specifically, this team aims to 1) improve the understanding and explainability of deep neural networks with the quantum theory of modern physics; 2) enhance the mathematical foundation of deep neural networks; 3) increase the computation efficiency of the deep learning training process with new algorithms that will scale; and 4) implement the deep learning research innovations into a new deep learning software package to deploy in cloud computing and High-Performance Computing platforms.<br/><br/>This project will aid the research in a wide range of areas of applications, from academic studies to the oil and gas industry and the military by offering deep learning models and improved computational efficiency and scalability. Students will benefit from the opportunity to contribute to the forefront of science research and technology. The project will broaden participation by opening stimulating research opportunities to a diverse group of underrepresented minority students. The two campuses and the members of the interdisciplinary team complement each other; by exchanging and sharing expertise and students the collaboration therefore has a beneficial and synergistic effect on multiple levels.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747769","Phase I I/UCRC Carnegie Mellon University: Center for Big Learning CBL","CNS","IUCRC-Indust-Univ Coop Res Ctr","02/15/2018","10/25/2018","Ruslan Salakhutdinov","PA","Carnegie-Mellon University","Continuing Grant","Dmitri Perkins","07/31/2018","$36,780.00","","rsalakhu@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","5761","5761","$0.00","This project establishes the Industry/University Cooperative Research Center for Big Learning (CBL). With substantial breakthroughs in deep learning, the renaissance of machine intelligence is unfolding. The vision is to create intelligence towards the intelligence-driven society. Through catalyzing the fusion of diverse expertise from the consortium of faculty members, students, industry partners, and federal agencies, CBL seeks to create state-of-the-art deep learning methodologies and technologies and enable intelligent applications, transforming broad domains, such as business, media, healthcare, Internet-of-Things, and cybersecurity. This timely initiative creates a unique platform for empowering our next-generation talents with real-world relevance and significance.<br/><br/>The mission is to pioneer novel deep learning algorithms, intelligent systems, and novel applications through unified and coordinated efforts in the CBL consortium via fusion of broad expertise from our large number of center faculty members, students, industry partners, and government agencies. The Carnegie-Mellon University site will focus on deep learning algorithms and platforms and work with other sites on novel applications.<br/><br/>The CBL is expected to have broad transformative impacts in technologies, education, and the society. CBL seeks to create pioneering research and applications to address a broad spectrum of real-world challenges, making significant contributions and impacts to the deep learning community, the industry and the society at large. The discoveries from CBL will make significant contributions to promote products and industrial services in general and CBL industry partners in particular. As the magnet of deep learning research and applications, CBL offers an ideal platform to nurture next-generation talents through world-class mentors from both academia and industry, disseminate the cutting-edge technologies, and facilitate industry/university collaborative research.<br/><br/>The center repository will be hosted at http://nsfcbl.org. The data, code, documents will be well organized and maintained on the CBL server for the duration of the center plus five years. The internal code repository will be managed by GitLab. After the software packages are well documented and tested, they will be released and managed by popular public servers, such as GitHub and Bitbucket."
"1831005","I-Corps:  An Interactive Query Interface","IIP","I-Corps","04/01/2018","04/04/2018","Isil Dillig","TX","University of Texas at Austin","Standard Grant","Pamela McCauley","09/30/2018","$50,000.00","","isil@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will be to enable companies and end-users to more easily extract relevant information from data sources. This system can be beneficial for a wide range of companies including e-commerce, business intelligence, and database and could systems. It is envisioned that this technology will be used in two ways: First, companies can use the system internally to enable easier and faster data access for their employees, thereby reducing dependence on database administrators and cutting costs. Second, companies could use the system externally to provide a better search interface to their customers. In addition, this technology can potentially also be useful to academic researchers, particularly within social and natural sciences, by allowing them to easily extract relevant data.<br/><br/>This I-Corps project provides an interface for extracting data from databases by writing English sentences. Although relational databases are the most common choice for the storage of information, retrieving a relevant subset of the data from them requires expertise in a formal query language. This technology allows end-users to query information stored in a relational database without having to learn a formal query language. The technology is based on a research which combines advanced natural language processing techniques with automated program synthesis and repair. A key advantage of this approach is that it is database-agnostic and does not require end-users to know the database schema.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830157","I-Corps: Analytic Tool Discovery System for Interdisciplinary Data Analysis","IIP","I-Corps","04/01/2018","04/04/2018","Etienne Gnimpieba","SD","University of South Dakota Main Campus","Standard Grant","Andre Marshall","09/30/2019","$50,000.00","","Etienne.Gnimpieba@usd.edu","414 E CLARK ST","vermillion","SD","570692307","6056775370","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to significantly improve end-users' data analysis productivity and result accuracy. The Resource Enhancement and Advanced Discovery System (READS) technology streamlines users approaches to locating the most relevant analytic tools for their specific problems through natural language capabilities.  While focusing on geoscientists, financial analysts, and biotechnologists as potential users at the inception of this I-Corps project, READS can be customized for any discipline and its specific data analytic tool requirements.  The three user examples listed above are only the beginning of the potential applications for which this analytic tool discovery system can be utilized.  The ability to query resources with discipline-specific language and increase result accuracy by annotating the queried resources with domain-specific terminology will drastically reduce the amount of time required to locate requisite tools, and ultimately attract users.  New users will not need to start from scratch in their analyses, but will be able to build upon the resources discovered by others, therefore maximizing productivity.  This broad range of appeal, combined with an accessible user interface not seen in other tools, underscores the potential for significant commercial impact of this technology.<br/><br/>This I-Corps project   aims to assist users in easily locating the ""best"" analytic tools for their specific task through an analytic tool discovery system for interdisciplinary data analysis. Data manipulation challenges are emerging and the integration of dataset analytic tools into a comprehensive framework is needed to further knowledge discovery.  Abundant tools and methods are available to the research and business communities to analyze datasets across disciplines, but it is often difficult to locate the most relevant tools for a specific task. Adding context to queries greatly enhances a users' ability to retrieve requisite analytic tools.  READS integrates analytic tool information into a one-stop-shop retrieval system capable of accessing valuable metadata from multiple repositories.  The system allows end-users to retrieve analytic tool information by submitting either keyword-based queries and/or free text-based questions.  The system leverages natural language processing, text mining, and an ontology-based metadata annotation system to allow collections of analytic tools and methods to be linked and discovered with high accuracy.  This allows end-users to discover, reuse, validate, share, and exchange knowledge related to their chosen data analytic tools. This novel approach facilitates the accessible analysis of cross-domain datasets, greatly enhancing end-users' ability to more efficiently answer challenging questions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763649","CHS: Medium: CAST: Child Adaptive Search Tool","IIS","HCC-Human-Centered Computing","07/01/2018","06/25/2018","Jerry Fails","ID","Boise State University","Standard Grant","Balakrishnan Prabhakaran","06/30/2022","$1,199,604.00","Maria Soledad Pera, Casey Kennington","jerryfails@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","7367","7367, 7924, 9102, 9150","$0.00","The goal of the project is to empower emergent searchers -- initially children ages 6-11 -- by researching, designing, and developing search tools that improve their information literacy and searching capabilities through modeling and adapting to their abilities as they search. Current search engines, even ones specifically designed for children, offer weak support for children's search needs due to their developing skills related to spelling, language use (including synonyms), understanding categories, refining queries, and evaluating relevance and quality of results. This makes it hard for children to create effective queries, use the results suggested by the search engine, and understand the relationship between the queries and the results returned. Bringing together expertise in human-computer interaction, information retrieval, natural language processing, and education, the project team will both (a) further scientific understanding of children's search abilities, and (b) design tools to support it through the iterative development of CAST (Child Adaptive Search Tool), designed for children aged 6-11.  CAST will be designed to model and respond to users' literacy and maturity levels as well as search intent missing from their formal queries. For example, when a child submits the query ""Tiger"", CAST will tend to prioritize tiger habitat or Winnie the Pooh's friend Tigger, which likely correlate better to a child's search intent than information on Tiger Woods. To reach this goal, the team will collaborate with children and teachers throughout the course of the project, working with partners in several local schools to increase the impact of the application itself and to improve the dissemination of the results. The results on supporting search in the special population of children in this research also have the potential to inform similar problems and methods aimed at other populations who might have systematic differences in their search ability, from older adults to second language speakers.<br/><br/>The team will use two main lenses to guide the project work. The first is an educational research lens that grounds design activities in the known needs of children, for example, using spelling development research to create models of spelling correction that are tailored to this audience rather than general dictionary-based spelling correction algorithms. The second is a version of participatory design called cooperative inquiry that closely involves both children and teachers along with the research team throughout the design process, allowing the team to benefit from children's direct perceptions of their own needs and ideas for addressing them as well as the teachers' knowledge of children's needs when learning to search for information to support their own education. The work will proceed in three main phases.  The first phase focuses on defining requirements and resources, through partnering with local public schools and collecting publicly available datasets of children's search behavior, vocabulary lists, popular websites for kids, and other resources that can inform models and algorithms tuned to children's search.  The second phase involves designing and developing an initial version of CAST that includes methods to support query formulation, spelling, and navigating ontologies in order to help children better express their search needs, along with tools such as enhanced result snippets for evaluating the quality and relevance of results. The third phase will focus on evaluating both individual components and overall system quality; this includes using quantitative analysis of observable behaviors in the system such as queries, session lengths, and returned result quality as well as direct testing with children and teachers to evaluate their perceptions of the system's usability and their willingness to use it going forward.  The team will use the evaluations to iteratively measure both individual components and the overall system, which will be deployed at a number of public libraries in the project team's home state for use by children outside of the research context.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831980","Excellence in Research: Collaborative Research: Strengthen the Foundation of Big Data Analytics via Interdisciplinary Research among HBCUs","CNS","HBCU-EiR - HBCU-Excellence in","10/01/2018","09/12/2018","Yunjiao Wang","TX","Texas Southern University","Standard Grant","Marilyn McClure","09/30/2021","$466,515.00","Daniel Vrinceanu","wangyx@tsu.edu","3100 Cleburne Street","Houston","TX","770044501","7133137457","CSE","070Y","041Z, 1714","$0.00","This project will be implemented by an interdisciplinary research team from two neighboring historical Black Colleges and Universities (HBCUs), Prairie View A&M University (PVAMU) and Texas Southern University (TSU), with the goals of strengthening the theoretical foundation of big data analytics and developing a novel deep learning software package based on this enhanced foundation. Challenges around big data impacts many areas and encourages exciting further investigation to understand the complex requirements of real-world applications. <br/><br/>Specifically, this team aims to 1) improve the understanding and explainability of deep neural networks with the quantum theory of modern physics; 2) enhance the mathematical foundation of deep neural networks; 3) increase the computation efficiency of the deep learning training process with new algorithms that will scale; and 4) implement the deep learning research innovations into a new deep learning software package to deploy in cloud computing and High-Performance Computing platforms.<br/><br/>This project will aid the research in a wide range of areas of applications, from academic studies to the oil and gas industry and the military by offering deep learning models and improved computational efficiency and scalability. Students will benefit from the opportunity to contribute to the forefront of science research and technology. The project will broaden participation by opening stimulating research opportunities to a diverse group of underrepresented minority students. The two campuses and the members of the interdisciplinary team complement each other; by exchanging and sharing expertise and students the collaboration therefore has a beneficial and synergistic effect on multiple levels.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840191","CICI: SSC: TrOnto - A Community-Based Ontology for a Trustworthy and ResiliCent Scientific Cyberspace","OAC","Cybersecurity Innovation","09/01/2018","08/27/2018","Raul Aranovich","CA","University of California-Davis","Standard Grant","Robert Beverly","08/31/2021","$640,000.00","Matt Bishop, Premkumar Devanbu, Vladimir Filkov, Kenji Sagae","raranovich@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8027","","$0.00","Scientific research relies on an infrastructure of networked computers (cyberinfrastructure), which need to be secured against malicious threats and intrusions. For instance, the privacy of clinical trial subjects, the integrity of medical data, and the safety of tissue samples and cultures can be compromised if the cyberinfrastructure of a university hospital is breached. Information Security Officers (ISOs) are in charge of maintaining a secure cyberinfrastructure for research, but their task is complicated by the speed at which new threats emerge, and by the proliferation of software (often written in different languages) and obsolete or specialized hardware among labs and research centers. ISOs look for answers in their quest to fight intrusions and vulnerabilities in documentation and on-line discussion forums, but this is a time-consuming process. To streamline the process, an interdisciplinary team of researchers are developing a computer system to automatically and continuously harvest all that knowledge from online sources, organizing it into a network of concepts and relations that can be queried and reasoned upon to keep ISOs a step ahead of malicious attacks. <br/><br/>The project develops a number of interconnected modules: 1) A sub-system that identifies concepts and relations in software documentation, advisory bulletins, and on-line technical forums, and then retrieves that information using state-of-the-art natural language processing techniques; 2) An ontology for cybersecurity, which is a knowledge representation system that organizes the retrieved concepts and relations into a logical network, allowing for implicit knowledge to be extracted by means of automatic reasoning algorithms, and;  3) A querying interface, which allows ISO staff to access the knowledge represented in the ontology to find answers to their questions about cybersecurity. This innovative approach to cybersecurity extends the use of ontologies in the biomedical field, leveraging the metaphor of vulnerabilities in information systems as viruses or infections. Even though the initial stages in the creation of the ontology will involve curation by human experts, the researchers expect that the system can itself automatically thanks to the use of information retrieval techniques, therefore overcoming one of the known bottlenecks in the usefulness of ontologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816330","Deep-Learning for Galaxy Morphology in the Big Data Era","AST","EXTRAGALACTIC ASTRON & COSMOLO","08/01/2018","07/16/2018","Mariangela Bernardi","PA","University of Pennsylvania","Standard Grant","Nigel Sharp","07/31/2021","$418,218.00","","bernardm@physics.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1217","1206, 7480","$0.00","Astronomy is entering the Big Data era.  The wealth of data which will soon be available from massive surveys will be invaluable for understanding galaxy evolution.  However, extracting and interpreting information from enormously rich datasets is a challenge with no sufficiently efficient, demonstrated solutions to date.  This is a project to tailor Deep Learning algorithms, which have been used successfully in other fields where pattern recognition matters, to measure galaxy morphologies quickly and accurately.  The long-term goal is to develop algorithms which transform Big Data into Big Discovery in astrophysics.  All algorithms and classifications will be made available for more general use.  The principal researcher is committed to supporting scientists from under-represented groups, through hiring practices, teaching in the US and in other countries, and by helping to build the capacity for developing countries to produce cutting-edge science.<br/><br/>Morphology is a key observable for constraining galaxy formation models, but quantifying morphology is currently a time-consuming process, severely compromised by the big-data transition.  Deep Learning algorithms may be the answer.  In the first phase, algorithms will learn from the large set of morphological classifications available from the Sloan Digital Sky Survey (SDSS).  The next phase studies how to transfer the knowledge gained from SDSS into analysis of images from the Dark Energy Survey (DES).  The use of simulated images to accelerate the learning process will also be studied.  This is the first step towards the automated classification of other aspects of galaxy structure.  The long-term goal is to develop algorithms which transform Big Data into Big Discovery in astrophysics.  Work with Deep Learning derived morphologies will illustrate the data-to-discovery process.  Teaching expertise in using the SDSS and DES databases to visiting astronomers, educators and masters-level students will help to ensure a wider global impact of the investment in building these databases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812660","Collaborative Research: Supporting Teachers in Responsive Instruction for Developing Expertise in Science","DRL","Discovery Research K-12","09/01/2018","08/29/2018","Brian Riordan","NJ","Educational Testing Service","Continuing Grant","Robert Ochsendorf","08/31/2022","$190,688.00","","briordan@ets.org","Center for External Research","Princeton","NJ","085402218","6096832734","EHR","7645","7645","$0.00","Many teachers want to adapt their instruction to meet student learning needs, yet lack the time to regularly assess and analyze students' developing understandings. The Supporting Teachers in Responsive Instruction for Developing Expertise in Science (STRIDES) project takes advantage of advanced technologies to support science teachers to rapidly respond to diverse student ideas in their classrooms. In this project students will use web-based curriculum units to engage with models, simulations, and virtual experiments to write multiple explanations for standards-based science topics. Advanced technologies (including natural language processing) will be used to assess students' written responses and summaries their science understanding in real-time.  The project will also design planning tools for teachers that will make suggestions relevant research-proven instructional strategies based on the real-time analysis of student responses. Research will examine how teachers make use of the feedback and suggestions to customize their instruction. Further we will study how these instructional changes help students develop coherent understanding of complex science topics and ability to make sense of models and graphs. The findings will be used to refine the tools that analyze the student essays and generate the summaries; improve the research-based instructional suggestions in the planning tool; and strengthen the online interface for teachers. The tools will be incorporated into open-source, freely available online curriculum units. STRIDES will directly benefit up to 30 teachers and 24,000 students from diverse school settings over four years. The Discovery Research K-12 program (DRK-12) seeks to significantly enhance the learning and teaching of science, technology, engineering and mathematics (STEM) by preK-12 students and teachers, through research and development of instructional innovations. Projects in the DRK-12 program build on fundamental research in STEM education and prior research and development efforts that provide theoretical and empirical justification for proposed projects. <br/><br/>Leveraging advances in natural language processing methods, the project will analyze student written explanations to provide fine-grained summaries to teachers about strengths and weaknesses in student work. Based on the linguistic analysis and logs of student navigation, the project will then provide instructional customizations based on learning science research, and study how teachers use them to improve student progress. Researchers will annually conduct at least 10 design or comparison studies, each involving up to 6 teachers and 300-600 students per year. Insights from this research will be captured in automated scoring algorithms, empirically tested and refined customization activities, and data logging techniques that can be used by other research and curriculum design programs to enable teacher customization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757858","REU Site: Research Experience for Undergraduates in Computer Vision","IIS","RSCH EXPER FOR UNDERGRAD SITES","05/01/2018","07/31/2019","Mubarak Shah","FL","The University of Central Florida Board of Trustees","Standard Grant","Wendy Nilsen","04/30/2021","$373,997.00","Niels da Vitoria Lobo","shah@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","1139","9250","$0.00","The young and talented minds passing through our colleges and universities today will determine our technological future. To maintain America's previously-held strategic position of command in Science and Engineering, American students need greater encouragement to pursue advanced studies in Science/Technology/Engineering/Mathematics (STEM) disciplines. This Research Experiences for Undergraduates (REU) site aims to encourage more American undergraduates in Computer Science to pursue graduate studies, and to assist them in realizing their full potential as researchers. The investigators believe that the best way to achieve these aims is to immerse capable undergraduates within a successful, active research group for a sufficient duration. This exposes them to the intellectual excitement that is involved in research activity, encourages them to think creatively and independently, and helps them to develop the skills necessary to work on research projects. This project represents renewal of the REU site in Computer Vision, which has operated successfully for the past thirty years. It is important to continue educating the next generation of computing researchers, including a special emphasis on engaging underrepresented members in computing fields and those students attending institutions that do not have substantial research opportunities. Dissemination of the best practices to other institutions remains our site's deep commitment and is achieved by publication in educational venues, and involvement in the REU community.<br/><br/>This project has ten undergraduate researchers per year, for three years. Each year, undergraduate researchers participate in a ten week, full-time Summer program, with optional follow-on part-time participation during the subsequent Fall and Spring. The key elements of our approach are (1) a full Summer experience for the undergraduate researchers, providing sufficient time to follow a substantial project through to completion, (2) to present each undergraduate researcher with several possible project topics, so that they can feel they have chosen a project which is most interesting to them, (3) to immerse the undergraduate researchers in the general research environment essentially as if they were graduate students, by having them meet with their faculty advisor regularly to discuss their project, participate in the weekly research group meetings, attend research presentations, PhD defenses and PhD proposal presentation and meet with visiting distinguished researchers, (4) to offer undergraduate researchers an optional follow-through over the year by working with the undergraduate  researchers to write a (publishable) technical report on their project, (5) to expose them to the benefits of graduate school through a half-day workshop on Why grad school, How to apply to grad school and fellowships, How to prepare for the standardized entrance exams, (6) to take them for field trips to local technology companies involved in real world applications of computer vision, and (7) to offer them mentoring at social events: group lunches, banquet, picnic, parties, etc. This site has been successful in contributing to educating generations of researchers. It is important to continue educating the next generation of computing researchers, including a special emphasis on engaging underrepresented members in computing fields. Another important emphasis is to provide intensive research experiences for those students attending institutions that do not have substantial research opportunities. Additionally, dissemination of the best practices to other interested institutions remains our site's deep commitment and is achieved by publication in educational venues, and involvement in the REU community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813713","Collaborative Research: Supporting Teachers in Responsive Instruction for Developing Expertise in Science","DRL","Discovery Research K-12","09/01/2018","06/10/2020","Marcia Linn","CA","University of California-Berkeley","Continuing Grant","Robert Ochsendorf","08/31/2022","$1,334,021.00","Elizabeth Gerard","mclinn@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","EHR","7645","7218, 7645","$0.00","Many teachers want to adapt their instruction to meet student learning needs, yet lack the time to regularly assess and analyze students' developing understandings. The Supporting Teachers in Responsive Instruction for Developing Expertise in Science (STRIDES) project takes advantage of advanced technologies to support science teachers to rapidly respond to diverse student ideas in their classrooms. In this project students will use web-based curriculum units to engage with models, simulations, and virtual experiments to write multiple explanations for standards-based science topics. Advanced technologies (including natural language processing) will be used to assess students' written responses and summaries their science understanding in real-time.  The project will also design planning tools for teachers that will make suggestions relevant research-proven instructional strategies based on the real-time analysis of student responses. Research will examine how teachers make use of the feedback and suggestions to customize their instruction. Further we will study how these instructional changes help students develop coherent understanding of complex science topics and ability to make sense of models and graphs. The findings will be used to refine the tools that analyze the student essays and generate the summaries; improve the research-based instructional suggestions in the planning tool; and strengthen the online interface for teachers. The tools will be incorporated into open-source, freely available online curriculum units. STRIDES will directly benefit up to 30 teachers and 24,000 students from diverse school settings over four years. The Discovery Research K-12 program (DRK-12) seeks to significantly enhance the learning and teaching of science, technology, engineering and mathematics (STEM) by preK-12 students and teachers, through research and development of instructional innovations. Projects in the DRK-12 program build on fundamental research in STEM education and prior research and development efforts that provide theoretical and empirical justification for proposed projects. <br/><br/>Leveraging advances in natural language processing methods, the project will analyze student written explanations to provide fine-grained summaries to teachers about strengths and weaknesses in student work. Based on the linguistic analysis and logs of student navigation, the project will then provide instructional customizations based on learning science research, and study how teachers use them to improve student progress. Researchers will annually conduct at least 10 design or comparison studies, each involving up to 6 teachers and 300-600 students per year. Insights from this research will be captured in automated scoring algorithms, empirically tested and refined customization activities, and data logging techniques that can be used by other research and curriculum design programs to enable teacher customization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831921","RIDIR: Collaborative Research: Data Science Tools for Policy Analyses","SMA","Data Infrastructure","09/01/2018","08/23/2018","Tyler Scott","CA","University of California-Davis","Standard Grant","Sara Kiesler","08/31/2021","$38,366.00","","tascott@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","SBE","8294","062Z, 065Z, 7433, 8083","$0.00","This research will design and develop a data science platform that provides access to policy documents. The platform will make available for the first-time analytical tools, using natural language computer processing and data science to enable systematic research and inquiry by practitioners, project proponents, scholars, and the public to answer a host of critical questions about public policy.<br/> <br/>Using natural language processing and data science, this research will design and develop a data science platform that provides access to tens of thousands of policy documents. The research team will produce a blueprint for the platform and validate its functionality with a community of scholars and practitioners, and establish the platform structure, ingest data, and refine analytical tools to integrate policy documents across many repositories, link text to metadata even when text comes from one source and the metadata from another, infer more detailed types of metadata that are not present in any of the existing repositories from analysis of the text, and allow researchers, contractors, and policy analysts to pose complex questions and answer them via analysis of policy documents. The team will also build a user community to catalyze scholarship and application and develop long-term mechanisms to ensure the sustainability and continued growth, management, and use of the platform and its resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831551","RIDIR: Collaborative Research: A Data Science Platform and Mechanisms for Its Sustainability","SMA","Data Infrastructure","09/01/2018","08/23/2018","Laura Lopez Hoffman","AZ","University of Arizona","Standard Grant","Sara Kiesler","08/31/2021","$1,500,000.00","Marc Miller, Sudha Ram, Steven Bethard, Elizabeth Baldwin","lauralh@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","SBE","8294","062Z, 065Z, 7433, 8083, 9178, 9179","$0.00","This research will design and develop a data science platform that provides access to documents. The platform will make available for the first-time analytical tools, using natural language computer processing and data science to enable systematic research and inquiry by practitioners, project proponents, scholars, and the public to answer a host of critical questions.<br/> <br/>Using natural language processing and data science, this research will design and develop a data science platform. The research team will produce a blueprint for the platform and validate its functionality with a community of scholars and practitioners. The team will establish the platform structure, ingest data, and refine analytical tools to integrate documents across many repositories, link text to metadata even when text comes from one source and the metadata from another, infer more detailed types of metadata that are not present in any of the existing repositories from analysis of the text, and allow researchers, contractors, and policy analysts to pose complex questions and answer them via analysis of documents. The team will also build a user community to catalyze scholarship and application and develop long-term mechanisms to ensure the sustainability and continued growth, management, and use of the platform and its resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822877","Technology Facilitated Training for Mental Health Counseling","IIS","Cyberlearn & Future Learn Tech","08/01/2018","07/21/2018","Vivek Srikumar","UT","University of Utah","Standard Grant","Amy Baylor","07/31/2021","$749,781.00","Zachary Imel, Michael Tanana, Eric Poitras","svivek@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8020","063Z, 8045","$0.00","Millions of Americans have been diagnosed with mental health or substance abuse problems. While conversational interventions like psychotherapy and other forms of counseling are among the most effective treatment options available, less than half of those in need receive care. One problem is that it is difficult to provide therapists in training with regular feedback based on direct observation of their work. Accordingly, training therapists is expensive and time consuming, leading to a shortage of expert counselors. Furthermore, due to difficulties in obtaining care, patients are turning to online sources of support, where quality may be difficult to ascertain. This project examines this timely question of how to use technology to improve the training of mental health counselors at all levels. This project will develop a novel intelligent tutoring system to capitalize on developments in natural language processing and also facilitate collaboration among trainees to enhance learning. Better trained therapists and improved interactions on mental health forums should improve the quality and timeliness of mental health care for everyone. Furthermore, this work will support lifelong and collaborative learning for licensed professional mental health counselors.<br/><br/>This project will develop and evaluate technological tools that facilitate new models for training tomorrow's mental health workforce. Specifically, this will involve creating and studying a novel text-based platform with the goal of training mental health counselors. Within this platform, two broad research questions include investigating the impact of (a) natural language processing driven helpers that provide feedback in real time, and, (b) crowd-sourced counseling using individuals with minimal training. To this end, several statistical models will be designed and trained to operate within the proposed text- based platform to interact with novice therapists. The efficacy of the two kinds of feedback (automatic and crowd-based) in terms of how well they can train different kinds of trainees (lay support providers in online forums, novice therapists in training) will be compared to models where the learner practices on their own and/or without specific feedback.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830158","RI/SES: Conference Proposal: Doctoral Consortium on Text as Data","IIS","Political Science, Robust Intelligence","09/01/2018","04/13/2018","Noah Smith","WA","University of Washington","Standard Grant","Tatiana Korelsky","08/31/2019","$24,987.00","","nasmith@cs.cmu.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1371, 7495","1371, 7495, 7556","$0.00","Computational methods from natural language processing enable new social science research that uses text as data; social science, in turn, offers insights that guide new applications of computation.  However, opportunities for conversation and building collaboration among researchers in the computational and social sciences, especially students, are relatlively rare.  The doctoral consortium that this award supports will bring together doctoral students working in computer science and those working in various areas of social science.  Moreover, participating doctoral students are paired with faculty mentors in the complementary discipline, thus providing for each student a novel perspective that is likely to be both challenging and inspirational.<br/><br/>The New Directions in Analyzing Text as Data (TADA) meeting has developed into a leading forum that brings these communities together, attracting top researchers from both sides.  Holding a doctoral consortium at TADA 2018 initiates a new outreach effort to broaden participation at the meeting.  By attending this doctoral consortium, computer science students benefit from the theoretical perspectives offered by social scientists, and social science students benefit by learning about new computational methods to support their research involving text data.  Funding is used primarily to support students to attend and fully participate in the meeting, where they are given an opportunity to present their current work and receive individual mentoring.  Mentors are recruited from the ranks of TADA veteran attendees.  These mentorship pairings are expected to significantly impact the direction of ongoing doctoral research, bringing natural language processing and social science closer together and creating a community with diverse young leaders who can fluently communicate across the disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838770","SCH: INT: The Virtual Assistant Health Coach: Learning to Autonomously Improve Health Behaviors","IIS","Smart and Connected Health","10/01/2018","05/02/2019","Brian Ziebart","IL","University of Illinois at Chicago","Standard Grant","Tatiana Korelsky","09/30/2022","$1,200,912.00","Ben Gerber, Lisa Sharp, Barbara DiEugenio, Bing Liu","bziebart@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","8018","8018, 8062, 9251","$0.00","Health coaching is an effective process for improving health behaviors by providing education on health-related topics, setting personalized and realizable health-related goals, monitoring and encouraging progress towards those goals, and sequencing or refining a progression of health goals over time. Though useful, its highly personalized and labor-intensive nature makes the cost of effective health coaching prohibitive for many underserved populations that could benefit significantly from it. This project will develop a virtual health coaching system that learns from human health coach demonstrations to interact with patients via the smart message system (SMS) to set appropriate health goals that are Specific, Measurable, Attainable, Relevant, and Timely (SMART). The virtual coach will become progressively more independent, but will not be fully autonomous for two reasons: first, at this stage of research, one cannot trust the system to always send appropriate messages when it encounters unique situations; second, the virtual coach would not substitute, but supplement/collaborate with a human coach. Successful development of these capabilities represents an important step towards more scalable health coaching systems that do not sacrifice the effectiveness realized by a human health coach.<br/><br/>This project develops the two key components of a virtual health coaching system: a dialogue management and sentiment analysis component that guides and analyzes communication with users; and an imitation learning component for learning high-level SMART goal-setting and coaching strategies from the human health coach's interactions with users.  Significant technical advances that address the underlying problems from these sub-areas are needed to achieve human-level health coaching efficacy and positive health outcome rates. These include: novel natural language processing methods to process informal, concise SMS including the emotion and mood they convey; expression of increasing autonomy via linguistic proxies such as initiative; and inverse optimal control methods for rationalizing observed behavior. Building on an initial feasibility study, this project will assess the benefits of a developed virtual health coach with varying degrees of autonomy for SMART goal setting and additional health coaching for a diverse range of users through SMS and fitness tracking devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757392","Workshop on Diversifying and Deepening Engagement and Learning in Science, Technology, Engineering, and Mathematics (STEM): Bringing Together Dutch and U.S. Scholars","DRL","ITEST-Inov Tech Exp Stu & Teac","02/01/2018","01/24/2018","Joseph Polman","CO","University of Colorado at Boulder","Standard Grant","Amy Wilson-Lopez","06/30/2019","$99,699.00","Melissa Braaten","joseph.polman@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","EHR","7227","7556, 8816","$0.00","This workshop on Diversifying and Deepening Engagement and Learning in STEM will bring together researchers from the United States and the Netherlands to exchange ideas and foster collaboration. Internationally, there is broad recognition that far too narrow a segment of the population is deeply engaged with and knowledgeable about STEM. This narrow engagement negatively affects the prospects of individuals across the globe. Diversifying the pool of children and young adults deeply engaged with STEM, and providing them with well-supported opportunities for deeper learning in STEM, will promote the progress of STEM by positively impacting research and development. Diversity of thought, perspective, experience and quality of preparation improve innovation and the quality of STEM research. The workshop and its products will advance STEM education by enhancing scholars' knowledge of relevant research on diversifying and deepening engagement and learning, and by creating stronger ties among researchers with common interests in and commitment to these issues. STEM education efforts sparked by this workshop will advance national health, prosperity, and welfare by improving individuals' quality of life and enabling broader public participation in democratic processes that are informed by STEM. <br/><br/>The workshop involves a select group of influential scholars from the U.S. and Netherlands whose work contributes to innovative research and development on STEM education both within and beyond schools. The workshop's theme refers to the key goal of diversifying the pool of young people deeply engaged with STEM, including groups historically underrepresented globally and in each nation, while also fostering deeper STEM learning. Throughout the workshop, participants will examine two questions: (1) What are the key social, cultural, and systemic contexts that facilitate and impede broadening STEM participation and learning? (2) How might researchers better understand them, and educators better address them? Comparative work across the U.S. and Netherlands national contexts will allow for bidirectional illumination of issues that might otherwise appear to be merely given or inevitable when examined from a single national standpoint. Gathered scholars will explore the challenges and opportunities for supporting and sustaining interest in, identification with, and deep learning of STEM among young people, through informal and formal learning opportunities. A key outcome of the workshop will be a white paper made freely and widely available to the STEM education community and the general public. The white paper will summarize existing literature and the discussion, framings, and suggestions for further research. The process of constructing and finalizing the white paper will focus the attention of participants in the workshop, and result in greater clarity about the body of theory and research that can inform educational action and future research. In addition, interest groups that form during the workshop will consider pursuing additional products that can inform the STEM education community, such as a journal special issue, an edited volume, research and practice briefs, or other collaborations."
"1762363","SHF: Medium: Collaborative Research: Computer-Aided Programming for Data Science","CCF","Software & Hardware Foundation","06/01/2018","07/16/2020","Ruben Goncalves Martins","PA","Carnegie-Mellon University","Continuing Grant","Nina Amla","05/31/2022","$111,639.00","","rubenm@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7924, 8206","$0.00","The goal of this project, named DataWizard, is to dramatically simplify the effort that is currently required for data analytics through the use of computer-aided programming. Specifically, this project aims to semi-automate data collection, querying, and wrangling tasks by automatically generating programs from informal specifications. As a result, the DataWizard project will allow domain scientists to focus on more interesting data analytics and visualization tasks, leaving the ""grunt work"" of data science to computer-aided programming tools. The project will also advance the state-of-the-art in automated program synthesis and natural language processing and apply these techniques to the burgeoning field of big data analytics. <br/><br/>From a technical perspective, the goals of the DataWizard project are three-fold. First, this project develops novel programming-by-example and information extraction techniques to address challenges that arise in data collection, including consolidation of different data sources, transformations between hierarchical and relational data, and extraction of information from unstructured data sources. Second, this project explores new techniques for querying data using natural language descriptions. In particular, this project considers data extraction from relational and noSQL databases as well as semi-structured data sources, such as XML and JSON. Third, this project develops novel program synthesis methods for automating data wrangling, cleaning, and imputation tasks that commonly arise in data analytics. Overall, these techniques  make it significantly easier for data scientists to gain insights from messy data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762299","SHF: Medium: Collaborative Research: Computer-Aided Programming for Data Science","CCF","Software & Hardware Foundation","06/01/2018","05/14/2020","Isil Dillig","TX","University of Texas at Austin","Continuing Grant","Nina Amla","05/31/2022","$744,053.00","Gregory Durrett","isil@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7924, 8206, 9251","$0.00","The goal of this project, named DataWizard, is to dramatically simplify the effort that is currently required for data analytics through the use of computer-aided programming. Specifically, this project aims to semi-automate data collection, querying, and wrangling tasks by automatically generating programs from informal specifications. As a result, the DataWizard project will allow domain scientists to focus on more interesting data analytics and visualization tasks, leaving the ""grunt work"" of data science to computer-aided programming tools. The project will also advance the state-of-the-art in automated program synthesis and natural language processing and apply these techniques to the burgeoning field of big data analytics. <br/><br/>From a technical perspective, the goals of the DataWizard project are three-fold. First, this project develops novel programming-by-example and information extraction techniques to address challenges that arise in data collection, including consolidation of different data sources, transformations between hierarchical and relational data, and extraction of information from unstructured data sources. Second, this project explores new techniques for querying data using natural language descriptions. In particular, this project considers data extraction from relational and noSQL databases as well as semi-structured data sources, such as XML and JSON. Third, this project develops novel program synthesis methods for automating data wrangling, cleaning, and imputation tasks that commonly arise in data analytics. Overall, these techniques  make it significantly easier for data scientists to gain insights from messy data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822747","Collaborative Research: Automatic Text-Simplification and Reading-Assistance to Support Self-Directed Learning by Deaf and Hard-of-Hearing Computing Workers","IIS","","08/01/2018","07/26/2018","Matt Huenerfauth","NY","Rochester Institute of Tech","Standard Grant","Tatiana Korelsky","07/31/2021","$391,868.00","Lisa Elliot","matt.huenerfauth@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","005y","063Z, 8045","$0.00","While there is a shortage of computing and IT professionals in the U.S., there is underrepresentation of people who are Deaf and Hard of Hearing (DHH) in such careers. There is great diversity in the English reading skills among DHH Americans, who face challenges in the IT industry where regular upskilling is required throughout the working career.  This interdisciplinary research will focus on studying automatic text-simplification technologies as reading assistance to help DHH individuals keep up with rapidly changing technologies through self-directed learning, outside of a formal classroom setting.  The research team consists of experts in natural language processing (NLP), human-computer interaction (HCI), accessibility, and Deaf STEM education research. The resources and technologies developed for this project will be adaptable to other languages and text domains (e.g. medical information for lay readers), benefiting a wide range of populations (e.g. children, non-native speakers, people with reading disabilities).<br/><br/>Prior text-simplification research for specific user groups focused on preliminary data collection or classical NLP methods, and little HCI research has explored design options for reading-assistance systems. This project will fill this critical gap in research and will investigate the learning and reading-assistance needs of upskilling DHH computing workers, design parameters influencing usability of reading-assistance tools, new training data and methods for text simplification tailored to specific reader groups and technical genres, automatic and human-based evaluation methods, and the impact of such tools on heutagogical learning. The methods will include interview and survey research with DHH computing workers, prototyping and testing of design variations, creation of parallel simplification corpora, readability annotation of lexicon and texts by DHH individuals, NLP research on domain adaptation and syntax-based neural machine translation for text simplification, and observation of real-world deployment of a prototype with DHH students and recent graduates from computing-related programs at the National Technical Institute for the Deaf (NTID).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833400","Student Support for the 17th International Semantic Web Conference 2018 (ISWC 2018)","IIS","Information Technology Researc, Info Integration & Informatics","05/01/2018","04/13/2018","Bo Fu","CA","California State University-Long Beach Foundation","Standard Grant","Maria Zemankova","04/30/2019","$18,000.00","","Bo.Fu@csulb.edu","6300 State Univ. Dr.","Long Beach","CA","908154670","5629858051","CSE","1640, 7364","1640, 7364, 7556","$0.00","This award provides support 13 U.S.-based students to participate in the 17th International Semantic Web Conference (ISWC 2018), held in Monterey, California from October 8 to 12, 2018. ISWC is the premier international forum for state-of-the-art research on all aspects of the Semantic Web and Linked Data Community. The supported students will attend the conference, discuss and disseminate their work. They will also have an opportunity to interact with future national and international scientific collaborators and senior researchers and practitioners. Students will benefit from the Doctoral Consortium - a full day event where students can get critical, but encouraging, feedback on their work from senior members of the community. The program also includes a career mentoring meeting, where experienced members of the community from both academia and industry answer questions in an informal setting. The selected students will form a diverse group in order to broaden participation in computer science.<br/><br/>The International Semantic Web Conference, now in its 17th year, is an interdisciplinary conference that includes work on: Data Management, Natural Language Processing, Knowledge Representation and Reasoning, Ontologies and Ontology Languages, Semantic Web Engineering, Linked Data, User Interfaces and Applications. The conference regularly includes several hundred attendees. In addition to the main technical tracks, the conference includes a variety of events that provide opportunities for deeper interaction amongst researchers at different institutions, at different stages of their research careers, and researchers who are interested in many different aspects of Semantic Web Research. The ISWC 2018 web site (http://iswc2018.semanticweb.org) provides additional information on the conference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758600","SBIR Phase II:  SunDIAL: Slot DIscovery And Linking","IIP","SBIR Phase II","04/01/2018","07/23/2020","James Kukla","MD","RedShred","Standard Grant","Peter Atherton","03/31/2021","$893,511.00","","jmk@redshred.com","5520 Research Park Drive","Baltimore","MD","212284851","4438046865","ENG","5373","169E, 5373, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is to search and understand documents across domains. It is a powerful tool for consumers and businesses to route, categorize and ensure key points of a document are seen. This makes it easy for overwhelmed users to search, link topic information to knowledge bases and extract customized data, even if they can't think of the right keywords.  Online search can infer the popularity or relevance of a page based on data from millions of users, including a user's location and what they've shown interest in; however, business users don't benefit from these features. Users view concepts in a summary linked to a knowledge base, allowing them to see summaries, triage documents on mobile devices, write rules to sort or filter them automatically, and see key data before ever opening the document.  This Phase II project improves information extraction systems by generating answerboxes from complex documents. Consumers will be able to quickly get the gist of formal documents such as consumer credit contracts, insurance policies, industry solicitations, engineering and other difficult-to-read documents. This innovation advances knowledge discovery, information retrieval methods, information extraction, slot-filling, and knowledge-base population.<br/><br/>This Small Business Innovation Research Phase II Project discovers unsupervised and unrestricted slots and fillers (attributes and values) to construct structured summaries based on keywords and underlying patterns in document collections. It extends the state of the art by not requiring a manually crafted catalogue of slots and complements supervised approaches by discovering new slots. While conventional Natural Language Processing (NLP) approaches are effective on well-formed sentences, the techniques described here are effective for semi-structured content such as section headers, lists and tables. Additionally, NLP based approaches for fact extraction and text summarization are primarily lexical, requiring further processing for disambiguating and linking to unique entities and concepts in a knowledge base. This approach further advances the state of the art by eliminating these steps as it identifies keywords and links to knowledge base concepts as a first step in the discovery process. This concept-linking enables terms to explicitly map to semantic concepts in other ontologies such that they are available to enable reasoning and understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1912898","SBE: Small: Behavioral Control of Deceivers in Online Attacks","SES","Secure &Trustworthy Cyberspace","08/15/2018","02/04/2019","Lina Zhou","NC","University of North Carolina at Charlotte","Standard Grant","Sara Kiesler","08/31/2021","$112,337.00","","lzhou8@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","SBE","8060","7434, 7923, 9102, 9179","$0.00","Online attacks can cause not only temporary asset loss, but long-term psychological or emotional harm to victims as well. The richness and large scale of online communication data open up new opportunities for detecting online attacks. However, attackers are motivated to constantly adapt their behaviors to changes in security operations to evade detection. Deception underlies most attacks in online communication, and people are poor at detecting deception. Against this backdrop, this project aims to improve the resilience of solutions to online attacks and enable predictive methods for their detection. Although a complete set of deception behaviors of online attackers is assumed to be unknown, there is a reason to expect that some behaviors are more difficult for attackers to control than others. By identifying such behaviors and their relations in online communication, the project lays the groundwork for the development of resilient and predictive approaches to the detection of online attacks, and advances the state of knowledge on online deception behavior and its identification. At the educational front, the project provides new educational material for enriching the curriculum in cyber security and related disciplines. The interdisciplinary nature of this work contributes to graduate student training toward a new generation of scientists who are capable of conducting multi-disciplinary cutting-edge research using a variety of research methods. The PIs actively engage students at both graduate and undergraduate levels in their research activities, particularly making a strong effort to engage women and underrepresented minorities.<br/><br/>Online attackers' evolving behaviors can make the existing solutions to online attacks become ineffective quickly. This project not only discovers new deception behaviors and their relations from the discourse and structure of online communication, but also determines attackers' behavioral control during online attacks by comparing different types of online deception behavior. Further, this project develops techniques for automatic extraction of deception behaviors from online communication by building upon natural language processing and network analysis techniques. Some anticipated advances include: (1) deception theory extension by investigating deception behavior in online attacks via a new lens of behavior control, (2) guidelines on how to improve the resilience of online attack detection methods by identifying deception behaviors that likely escape the attackers' control attempt, (3) a predictive approach to attack detection in online communication by exploring the temporal relationships among deception behaviors, and (4) techniques for extracting deception behaviors from online discourse and structure. This project can lead to integrative and effective methods for online attack detection."
"1822754","Collaborative Research: Automatic Text-Simplification and Reading-Assistance to Support Self-Directed Learning by Deaf and Hard-of-Hearing Computing Workers","IIS","Cyberlearn & Future Learn Tech","08/01/2018","11/02/2018","Wei Xu","OH","Ohio State University","Standard Grant","Tatiana Korelsky","07/31/2021","$375,732.00","","xu.1265@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8020","063Z, 8045, 9251","$0.00","While there is a shortage of computing and IT professionals in the U.S., there is underrepresentation of people who are Deaf and Hard of Hearing (DHH) in such careers. There is great diversity in the English reading skills among DHH Americans, who face challenges in the IT industry where regular upskilling is required throughout the working career.  This interdisciplinary research will focus on studying automatic text-simplification technologies as reading assistance to help DHH individuals keep up with rapidly changing technologies through self-directed learning, outside of a formal classroom setting.  The research team consists of experts in natural language processing (NLP), human-computer interaction (HCI), accessibility, and Deaf STEM education research. The resources and technologies developed for this project will be adaptable to other languages and text domains (e.g. medical information for lay readers), benefiting a wide range of populations (e.g. children, non-native speakers, people with reading disabilities).<br/><br/>Prior text-simplification research for specific user groups focused on preliminary data collection or classical NLP methods, and little HCI research has explored design options for reading-assistance systems. This project will fill this critical gap in research and will investigate the learning and reading-assistance needs of upskilling DHH computing workers, design parameters influencing usability of reading-assistance tools, new training data and methods for text simplification tailored to specific reader groups and technical genres, automatic and human-based evaluation methods, and the impact of such tools on heutagogical learning. The methods will include interview and survey research with DHH computing workers, prototyping and testing of design variations, creation of parallel simplification corpora, readability annotation of lexicon and texts by DHH individuals, NLP research on domain adaptation and syntax-based neural machine translation for text simplification, and observation of real-world deployment of a prototype with DHH students and recent graduates from computing-related programs at the National Technical Institute for the Deaf (NTID).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1936523","CHS: Small: Translating Compilers for Visual Computing in Dynamic Languages","CCF","Software & Hardware Foundation","07/01/2018","06/24/2019","Baishakhi Ray","NY","Columbia University","Standard Grant","Anindya Banerjee","08/31/2019","$20,300.00","","rayb@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7798","7923, 7943","$0.00","This collaborative project is developing technologies to enable students, scientists, and other non-expert developers to use computer languages that facilitate rapid prototyping, and yet still automatically convert such programs to have high performance. In this research, the PI and co-PIs focus on programs that operate over visual data, such as programs in computer graphics, computer vision, and visualization. Visual data is important because visual datasets are rapidly growing in size, due to the use of cell-phone cameras, photo and video sharing online, and in scientific and medical imaging. The intellectual merits are that specialized program optimizations are being developed specifically for visual computing and for languages that enable rapid prototyping, alongside techniques that allow the computer to automatically search through different candidate optimizations and choose the fastest one. The project's broader significance and importance are that it will make the writing of computer programs that operate over visual datasets more accessible to novice programmers, make visual computing more accessible to a broader audience, permit faster research and development over visual programs, and make such programs themselves be more efficient.<br/><br/>More specifically, this research program is producing translating compilers that are specialized to handle programs that compute over visual data. The group led by the PI is researching new compilers that translate code from dynamic languages into highly efficient code in a target language. Dynamic languages are defined as those with a very dynamic run-time model, for example, MATLAB, Python, and Javascript. The target language is a language such as C that permits implementation of highly efficient programs. This research framework incorporates ideas from compilers, graphics, computer vision, visual perception, and formal and natural languages. The research will make a number of key intellectual contributions. First, new domain-specific translations and optimizations for visual computing will be formalized into manual rules that can be applied to any input program. Second, the team will research a novel approach of automatically learning translations, instead of using manually-coded rules. This can take the form of learning translation ""suggestions"" from humans, who can interactively suggest better output code. Third, a new search process based on offline auto-tuning will be used to select the translations that result in the fastest program. The success of the project will be verified against a comprehensive test suite of programs from computer vision and graphics."
"1757644","REU Site: Program for Access to Training in Health Informatics (PATHI)","IIS","RSCH EXPER FOR UNDERGRAD SITES","04/01/2018","07/31/2019","Kim Unertl","TN","Vanderbilt University Medical Center","Standard Grant","Wendy Nilsen","03/31/2021","$369,223.00","Bradley Malin","kim.unertl@vanderbilt.edu","1161 21st Ave. South","Nashville","TN","372325545","6153222450","CSE","1139","9250","$0.00","The Research Experiences for Undergraduates - Program for Access to Training in Health Informatics (REU-PATHI) will provide 10 undergraduate students with the opportunity to conduct meaningful health informatics research for a period of 10 weeks in the summer. As a field, health informatics connects science, engineering, social sciences, and multiple other fields towards addressing critical health-related research topics. The REU-PATHI site seeks to broaden participation in health informatics and open new opportunities in this rapidly expanding field. At least 50% of REU-PATHI students will come from groups that are underrepresented in Science, Technology, Engineering and Mathematics (STEM) and Computing fields, including women, members of minority groups, first generation college students, and students attending institutions with limited access to research experiences. Students will be matched with faculty mentors in the Vanderbilt University Medical Center Department of Biomedical Informatics, who will work with students to develop knowledge, skills, and experience in health informatics research. The undergraduate students will have the opportunity to work in one of three health informatics focus areas: 1) Computing, focusing on computationally-intensive topics such as natural language processing, data science, predictive analytics, and data privacy; 2) Precision Health, focusing on topics such as genome-wide and phenome-wide association studies, genomics, and proteomics, and 3) Human-Technology Interaction, applying concepts from human-computer interaction, human factors engineering, and sociotechnical systems to topics such as studying workflow in complex clinical environments and patient safety. Regardless of specific research area, all REU-PATHI students will participate in twice-weekly seminars presented by faculty, computer programming workshops, and other professional development activities. <br/><br/>The objectives of the REU-PATHI Site are to engage undergraduate students from diverse backgrounds in meaningful scientific research projects at the nexus of computer science, basic sciences, engineering, social sciences, and health, and to prepare them to become the next generation of researchers identifying, designing, developing, deploying, and studying innovative technology-based solutions for important health-related problems. Health Informatics is one of the fastest growing fields today. However, there are significant challenges in orienting and training diverse undergraduate students to the many career pathways available in this high-impact scientific research field. REU-PATHI addresses this urgent problem by 1) training undergraduate students to conduct cutting-edge health informatics research; 2) introducing students to pathways in health informatics graduate education and research; 3) involving graduate students, postdoctoral scholars, and faculty in mentoring undergraduate students; 4) providing opportunities for REU students to disseminate their research to peers, graduate students, faculty, and the field; and 5) linking high school students in our larger internship program to undergraduate peers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811402","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","05/08/2018","James Pustejovsky","MA","Brandeis University","Standard Grant","Stefan Robila","12/31/2019","$99,344.00","","pustejovsky@gmail.com","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","8004","026Z, 7916, 8004","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811101","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","05/08/2018","Brent Cochran","MA","Tufts University","Standard Grant","Stefan Robila","12/31/2019","$29,816.00","","Brent.cochran@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","8004","026Z, 7916, 8004","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848966","EAGER: New Algorithms for Feature-Efficient Learning","CCF","Algorithmic Foundations","10/01/2018","09/11/2018","Lev Reyzin","IL","University of Illinois at Chicago","Standard Grant","A. Funda Ergun","09/30/2021","$100,000.00","","lreyzin@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","7796","7916, 7926","$0.00","The main goal of this exploratory research project is to invent theoretically sound and practical machine-learning algorithms designed to perform well under various limitations involving access to data features during deployment.  This tackles a major difficulty encountered in many machine-learning applications: in running an algorithm, accessing features of the data can be time consuming or costly.  For example, in medical diagnosis, features of patients may correspond to results of medical tests, which can take significant time to run, carry enormous cost, and even impose heath risks.  Current machine-learning techniques are ill-equipped to tackle such impediments.  This project involves approaches that incorporate feature-efficient optimization into the training phase of machine-learning algorithms and also the creation of new frameworks for reducing both error rates and costs associated with acquiring features. Successful developments in feature-efficient algorithms create an important advance for application areas ranging from medical diagnosis to query-answering on the World Wide Web.  Additional facets of this project include incorporating its research findings into graduate courses and broadening participation in research.<br/><br/>This project investigates new models for jointly optimizing feature costs, prediction time, and classification error rates, to create feature-efficient predictors.  Techniques for this exploratory project include solving original optimization problems, creating novel machine-learning reductions, and analyzing the problem via statistical query oracles.  Another aspect of this work is to tackle a budgeted learning formalization by moving the feature-cost optimization into the training phase of budgeted boosting classifiers and support vector machines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812747","Harnessing Machine Learning to Study the Life Cycle of Stars","AST","GALACTIC ASTRONOMY PROGRAM","09/01/2018","06/22/2018","Stella Offner","TX","University of Texas at Austin","Standard Grant","Glen Langston","08/31/2021","$370,333.00","","offner@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","MPS","1216","1206, 8084","$0.00","Stars like our Sun are born in large clouds of gas that usually produce thousands of stars at once. As the stars form, they add energy back into their environment ('feedback') and influence the birth gas cloud. This feedback appears as fast-moving gas, which sometimes looks like large bubbles. The data are complex, so identifying feedback is difficult. Typically, astronomers have found the feedback ""by eye"", which is subjective and time-consuming. A new field of computer science, machine learning, provides an alternative approach to find feedback. In machine learning, computer algorithms are trained to identify features in the same way the human brain recognizes objects - like cats, dogs and cars.  The investigator's group will use state-of-the-art models of forming stars to train machine learning algorithms to find feedback. They will apply the algorithms to telescope observations and compare with the feedback sources previously found by humans. <br/><br/>The investigator will share the models with the public through the Milky Way project, which is an online astronomy program that trains people to identify feedback in telescope images of gas clouds. The program will also train students in research techniques, including undergraduates from underrepresented groups. <br/><br/>The proposal addresses a fundamental star formation question:  How much mass and energy is associated with stellar feedback in the interstellar medium?  To answer this question, the PI and collaborators will use magnetohydrodynamic simulations of forming stars, for which full feedback information is known, to train machine learning algorithms to identify and quantify feedback. Dust and molecular line 'synthetic observations' will be produced and used together with observational data as a training set.  The investigators will compare the machine learning identifications to prior visually identified feedback catalogs, including those from the citizen-science Milky Way Project, create an updated census, and publicly release the algorithm and data to the community. The broader impact objectives are to increase public participation in the Milky Way Project, develop a WorldWide Telescope tour on feedback, and train students, including undergraduates in the Texas Astronomy Undergraduate Research experience for Under-Represented Students (TAURUS) summer program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816403","CHS: Small: Collaborative Research: Tools for Mental Health Reflection: Integrating Social Media with Human-Centered Machine Learning","IIS","HCC-Human-Centered Computing","10/01/2018","06/17/2019","Munmun De Choudhury","GA","Georgia Tech Research Corporation","Continuing Grant","Andruid Kerne","09/30/2021","$292,808.00","","munmund@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","7367, 7923, 9251","$0.00","The widespread adoption of wearable devices and social media is generating population-scale data about people's behavior as situated in their everyday lives.  Prior research has shown how machine learning techniques can use such data in modeling attributes of individuals' health and wellbeing; these techniques are also promising additions to tools that support self monitoring practices and health interventions outside of clinical contexts.  However, most such tools enable only simple mechanisms to review one's data, and require high compliance from individuals actively volunteering relevant information. These limitations prevent such tools from effectively supporting reflection, that is, conscious re-examination of prior experiences to form new understanding. Reflection is a key to improved health and health maintenance, and recent work has shown the promise of data-driven health reflection. Effectively supporting reflection, however, requires more sophistication than simply showing a patient their data. This project will develop tools to support reflection for eating disorders (ED) by combining voluntarily shared and unobtrusively gathered social media data with strategic presentation of machine learning analyses. The interface designs will meet the needs of multiple stakeholders: patients, family members, and clinical partners.  By doing so, the research will result in novel mechanisms to support the treatment of ED, going beyond existing personal health informatics tools by being sensitive to the complex psychological struggles of ED patients.<br/><br/>The proposed research will follow a multi-phase process, interleaving the use of machine learning and human-centered approaches. The first phase will seek to understand the current practices of three stakeholders, patients, clinicians, and support network members, involved in ED reflection. In the second phase, informed by those current practices, we will develop theoretically-motivated, and psychometrically and clinically validated, machine learning techniques to support ED inference and reflection based on analysis of both textual and visual social media data. The third phase will use participatory design methods to develop interactive tools that encapsulate these machine learning techniques in order to support ED reflection among the three stakeholders. This final phase will include evaluating these tools through a field deployment to understand how they become embedded in and affect current practices of ED reflection.  These activities will lead to complementary contributions in the areas of machine learning and of designing for reflection, offering novel approaches to outstanding challenges surrounding mental health and facilitating novel collaborations between computational and clinical researchers. Broader implications of the research will include conducting mental health outreach activities in the researchers' respective campuses and facilitating the training of the next generation of cyber-human researchers and professionals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814909","CHS: Small: Collaborative Research: Tools for Mental Health Reflection: Integrating Social Media with Human-Centered Machine Learning","IIS","HCC-Human-Centered Computing","10/01/2018","08/30/2019","Eric Baumer","PA","Lehigh University","Continuing Grant","Andruid Kerne","09/30/2021","$213,875.00","","ericpsb@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","7367","7367, 7923, 9251","$0.00","The widespread adoption of wearable devices and social media is generating population-scale data about people's behavior as situated in their everyday lives.  Prior research has shown how machine learning techniques can use such data in modeling attributes of individuals' health and wellbeing; these techniques are also promising additions to tools that support self monitoring practices and health interventions outside of clinical contexts.  However, most such tools enable only simple mechanisms to review one's data, and require high compliance from individuals actively volunteering relevant information. These limitations prevent such tools from effectively supporting reflection, that is, conscious re-examination of prior experiences to form new understanding. Reflection is a key to improved health and health maintenance, and recent work has shown the promise of data-driven health reflection. Effectively supporting reflection, however, requires more sophistication than simply showing a patient their data. This project will develop tools to support reflection for eating disorders (ED) by combining voluntarily shared and unobtrusively gathered social media data with strategic presentation of machine learning analyses. The interface designs will meet the needs of multiple stakeholders: patients, family members, and clinical partners.  By doing so, the research will result in novel mechanisms to support the treatment of ED, going beyond existing personal health informatics tools by being sensitive to the complex psychological struggles of ED patients.<br/><br/>The proposed research will follow a multi-phase process, interleaving the use of machine learning and human-centered approaches. The first phase will seek to understand the current practices of three stakeholders, patients, clinicians, and support network members, involved in ED reflection. In the second phase, informed by those current practices, we will develop theoretically-motivated, and psychometrically and clinically validated, machine learning techniques to support ED inference and reflection based on analysis of both textual and visual social media data. The third phase will use participatory design methods to develop interactive tools that encapsulate these machine learning techniques in order to support ED reflection among the three stakeholders. This final phase will include evaluating these tools through a field deployment to understand how they become embedded in and affect current practices of ED reflection.  These activities will lead to complementary contributions in the areas of machine learning and of designing for reflection, offering novel approaches to outstanding challenges surrounding mental health and facilitating novel collaborations between computational and clinical researchers. Broader implications of the research will include conducting mental health outreach activities in the researchers' respective campuses and facilitating the training of the next generation of cyber-human researchers and professionals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755611","CRII: CHS: Concept-Driven Visual Analysis","IIS","HCC-Human-Centered Computing","04/01/2018","05/08/2018","Khairi Reda","IN","Indiana University","Standard Grant","Balakrishnan Prabhakaran","03/31/2021","$190,977.00","","redak@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7367","7367, 8228, 9178, 9251","$0.00","Tools that create visualizations, or visual representations of large datasets, are increasingly important for making use of data in a number of domains from commerce to science.  To date, most visualization tools have been designed to support open-ended exploration of patterns in data; though useful for some tasks, this exploratory model does not fit well when analysts have existing models or hypotheses.  This project aims to support a ""concept-driven"" analysis style in which analysts can share their existing conceptual models with the system, which uses those models to generate visualizations that allow the analyst to explore places where the models and data disagree and develop revised models that reconcile those discrepancies.  To do this, the research team will design a number of prototype techniques for communicating conceptual models, algorithms for selecting visualizations and data features that best match those models, and interfaces that highlight discrepancies and provide tools for analysts to dig into the data around them.  If successful, these concept-driven analyses will provide better ways for scientists and other analysts with existing models to leverage data while reducing the risk of confirmation biases in which people choose analyses that don't show where their existing models are wrong. The project will also enable the research team to learn more about the ways people come to form and express expectations about data.  Lastly, project will provide opportunities for graduate research training as well as tools to support K-12 outreach workshops that introduce younger students to data science.<br/><br/>The project has two main activities.  The first involves prototyping three elicitation techniques that prompt users to externalize their mental models and expectations about a dataset: free text expressions combined with natural language processing techniques that extract both variables of interest and implied relationships between them; concept mapping tools that allow users to graphically express relationships between entities, ideas, and concepts as node-link diagrams in which the nodes represent key aspects of the data and links represent suspected relationships between them; and tools for sketching expected relationships between variables using existing visualizations such as line charts and heatmaps.  The team will also develop interfaces that encourage analysts to develop several alternative models to reduce the chance of confirmation bias.  The second main activity is using the captured models to generate relevant visualizations that support discrepancy exploration.  To do this, the team will first use a taxonomy of best practices for choosing visualizations that best fit the concepts and relationships represented in the models.  They will then design interfaces that highlight discrepancies in both the visualizations (for instance, by highlighting data that badly fits a model) and the models (for instance, by highlighting links in a concept map that are not supported by the data) to call attention to inconsistencies.  Both the elicitation and feedback interfaces will be refined through a series of semi-structured visual analysis studies in which participants use them to analyze data in domains of general interest such as socioeconomic indices, crime statistics, and health risks.  The refined versions will then be used to compare the effectiveness of the concept-driven approach with more traditional exploratory approaches, as well as against both structured and unstructured workflows that interleave exploratory and concept-driven elements, in a series of lab studies using participants drawn from a number of scientific disciplines and a case study with scientific partners at Argonne National Laboratory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811123","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","03/26/2019","Nancy Ide","NY","Vassar College","Standard Grant","Stefan Robila","12/31/2019","$177,639.00","Anton Nekrutenko","ide@vassar.edu","124 Raymond Avenue","Poughkeepsie","NY","126040657","8454377092","CSE","8004","026Z, 7916, 8004, 9251","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756023","CRII: RI: Explaining Decisions of Black-box Models via Input Perturbations","IIS","Robust Intelligence","07/01/2018","03/30/2018","Sameer Singh","CA","University of California-Irvine","Standard Grant","Rebecca Hwa","06/30/2021","$174,942.00","","sameer@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","7495, 8228","$0.00","Machine learning is at the forefront of many recent advances in science and technology, enabled in part by complex models and algorithms. However, as a consequence of this complexity, machine learning systems essentially act as ""black-boxes"" as far as users are concerned. Thus, it is incredibly difficult to predict what they will do when deployed, understand why they are making the decisions, guarantee their robustness, or broadly speaking, trust their behavior. As these algorithms become an increasing part of our society, our financial systems, our healthcare providers, our scientific advances, and our defense systems, it is crucial to address this challenge. In this work, the PI and his team will develop algorithms that explain why any classifier is making its decisions, without any access to its underlying implementation, in order to make the inner workings understandable to the users. Such explanations make machine learning more transparent, leading to a more robust evaluation pipeline, reduced debugging efforts, and increased ease of use (and of trust) of these complex, black-box systems.<br/><br/>For a decision made by a machine learning classifier, the team will develop methods that accurately characterize the relationship between the input instance and the algorithm's prediction, and present it in an intuitive manner. The primary intuition is to estimate the instance-specific behavior of the predictor by observing the output of the classifier as the input instance is perturbed. The first proposed thrust of this work extends this basic framework by considering rules that define counter-examples, and summarize the behavior over multiple instances, providing detailed and accurate insights into the behavior with minimal effort on the users' part. The second thrust identifies automated ways to learn domain-specific perturbation functions that generate realistic instances to compute the explanations. The team proposes a comprehensive evaluation of these explainers consisting of user experiments in comparing, trusting, and modifying machine learning algorithms, with applications to diverse tasks such as sentiment analysis, machine translation, time series, visual question answering, and object detection.<br/><br/>Due to the many potential applications of this work, both for machine learning practitioners and end-users, dissemination of the results is a key focus, and the team will augment standard channels (such as publications) with novel ones that include open-source software, jargon-free documentation, and interactive tutorials/demonstrations to encourage application of machine learning to novel domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750286","CAREER: Robust, scalable, reliable machine learning","IIS","Robust Intelligence","03/15/2018","06/22/2020","Tamara Broderick","MA","Massachusetts Institute of Technology","Continuing Grant","Rebecca Hwa","02/28/2023","$319,819.00","","tbroderick@CSAIL.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","1045, 7495","$0.00","Machine learning is increasingly deployed in large-scale, mission critical problems for the purpose of making decisions that affect a vast number of individuals' employment, savings, health, and safety. The potential for machine learning to dramatically impact and change people's lives necessitates that machine learning methods be robust, explainable, and understandable---rather than black-box. This research develops new techniques that are both computationally motivated and theoretically sound for robust machine learning at scale. The work is situated in the context of three modern classes of applications. (1) Economists are interested in analyzing the efficacy of microcredit, small loans to individuals in impoverished areas with the goal of eliminating poverty. (2) Biologists are interested in using single-cell RNA sequencing data to understand cells' relationships and development trajectories. (3) The Internet of Things (IoT) is poised to generate a wealth of complex data across energy readings in buildings, within transportation infrastructure, from vehicles on the road, and from many other sensor sources. The PI is working directly with area experts so as to have immediate, broad impact across application domains. In an educational component of the project, the PI is a core part of developing a new graduate curriculum and degree in statistics, data science, and statistical machine learning at MIT. The methods and applications in this proposal feature in a new course on modern machine learning methods. The PI is also developing a high-school level introduction to machine learning as part of the established Women's Technology Program (WTP).<br/><br/>The issues of robustness and explainability particularly arise in domains with nontrivial spatial and temporal dependencies, where the amount of data is often massive, and where practitioners typically have some expert knowledge about the domain before engaging with a particular dataset. These are precisely the domains where existing machine learning methodologies are less well-developed. The need to bring structural knowledge to bear on the problem suggests the use of Bayesian methods, which can incorporate this knowledge via prior and modeling assumptions. To live up to the promise of these methods, though, practical approaches need to be robust to assumptions as well as to noisy or adversarial data, lest this data change important decisions in ways not understood by the practitioner. This research incorporates advances in statistical physics to assess the sensitivity of a data analysis to assumptions and data values. And to realize the advantages of the proposed robust and understandable machine learning framework, practitioners must face extreme scalability issues---both from a computational perspective as well as a modeling perspective. On the computational side, this research builds on recent advances from computational geometry to scale to data sets at modern sizes. On the modeling side, note that while small-scale problems exhibit dense spatio-temporal dependencies, large-scale problems tend to be sparser, and practical approaches must reflect this sparsity to be reliable at scale. This work incorporates advances in probability theory to model sparse IoT networks. This proposal is highly interdisciplinary---bringing together ideas from machine learning, statistics, physics, theoretical computer science, probability theory, and systems and applying these ideas to microcredit, single-cell RNA sequencing, sensor networks, international trade, and industrial applications including customer service at scale.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746602","SBIR Phase I:  Driving Timely Point-of-Care Treatment in Hospitals with a High Precision Bayesian Machine Learning Platform","IIP","SBIR Phase I","01/01/2018","02/27/2020","Suchi Saria","DE","Bayesian Health LLC","Standard Grant","Peter Atherton","12/31/2018","$225,000.00","","suchi.saria@gmail.com","901 N. Market St, Suite 705","Wilmington","DE","198013098","4082058035","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to provide clinical decision support software that assists inpatient providers in improving care for preventable acute inpatient harms, and thereby reduce mortality and morbidity. This grant develops a cloud-based platform that applies machine learning (ML) algorithms in real-time on data extracted from Electronic Health Records (EHRs) and physiologic monitoring devices attached to a patient. The ML tools employed estimate the degree of reliability for each of the data elements as they are collected and integrates these signals to provide an accurate, individualized risk estimate of patient health over time in order to best guide patient treatment and allocation of hospital resources. Our initial target condition is sepsis, one of the most costly and most deadly diseases in hospitals. This grant develops an end-to-end system to provide risk assessment and implementation of timely treatment. For commercial potential, the underlying core technology can be extended to other clinical scenarios. <br/><br/>The proposed project enables scaling of high-precision state-of-the-art Bayesian machine learning techniques that forecast the chance of acute deterioration. This includes tackling the challenges in scaling this machine learning system to function across many care providers, patients, and hospitals. To achieve these goals, this project will develop new methods for running machine learning algorithms in a distributed fashion in cloud computing settings, especially in distinguishing where multiple machines need to coordinate, and arguably more importantly, where they can avoid coordinating in training on data. Further, the project develops software to provide information back to providers so as to enable interventions that can alter patient trajectory. Here the software will encompass how to best use the resulting inferences in guiding care."
"1906694","III: Small: Algorithms and Theoretical Foundations for Approximate Bayesian Inference in Machine Learning","IIS","Info Integration & Informatics","09/01/2018","12/14/2018","Roni Khardon","IN","Indiana University","Continuing Grant","Wei Ding","07/31/2021","$376,314.00","","rkhardon@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7364","7364, 7923","$0.00","Over the last two decades Bayesian models have become central in machine learning. Bayesian models often hypothesize latent (non-observed) variables with explanatory or predictive power toward observed phenomena. The challenge is to infer the state of these variables or a belief over that state from observed data. For example, one might try to infer a user's preferences from observations about their own behavior and the behavior of other users. The goal of this project is to develop general approximate inference algorithms that work across large families of Bayesian models so that solutions can be widely reused. The algorithmic work will be complemented by developing a learning theory for approximate Bayesian inference in machine learning. The theoretical framework will aim to prove performance guarantees for Bayesian prediction algorithms and inform the design of algorithms with desirable properties. The project will contribute to basic scientific research, advancing core goals in machine learning. The project will support training and research of PhD students and therefore will directly support human development. Through classroom teaching and outreach the project will expose a larger population of students to machine learning and its potential in applications.<br/><br/>More concretely, the project will investigate non-conjugate Bayesian latent variable models, i.e., it will avoid the often used but limiting simplifying assumption of conjugacy. On the algorithmic side the project will aim to generalize the paradigm of variational message passing for non-conjugate graphical models, and to develop stochastic variational inference algorithms using optimal structured approximations for large sub-families of such models. The proposed sub-families will capture the properties of many important problems in the literature. Exploratory research in several specific applications further motivates the work and will be used to test the algorithms. The project will develop a new angle for theoretical analysis of Bayesian algorithms, deriving performance guarantees on their expected error. A core idea is to view variational inference algorithms through the so-called agnostic learning framework where guarantees sought are relative to the best that can be done within a specific limited class of approximations. This will provide a fresh outlook that informs the design of algorithms with desired performance guarantees. The expected scientific impact of the project is having better algorithms with well understood performance characteristics and applicable for a larger class of machine learning problems."
"1810125","Enabling Adaptive Voltage Regulation: Control, Machine Learning, and Circuit Design","ECCS","CCSS-Comms Circuits & Sens Sys","08/15/2018","08/13/2018","Peng Li","TX","Texas A&M Engineering Experiment Station","Standard Grant","Jenshan Lin","11/30/2019","$360,000.00","Edgar Sanchez-Sinencio","lip@ece.ucsb.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","7564","106E","$0.00","Supply voltage regulation serves the critical role of delivering power to on-chip devices at well-regulated voltage levels. Voltage regulation presents key design challenges of electronic systems ranging from high-performance microprocessors to mobile system-on-a-chips. In such systems, the ever-growing need for processing capability must be fulfilled while staying within specified power, thermal, and battery-life limits.  Power must be managed and delivered while maximizing system power efficiency in every possible way. The proposed research aims to address the above voltage regulation challenges by taking an interdisciplinary approach.  Innovations in control, machine learning, and circuit design will be developed to enable adaptive supply voltage regulation systems involving a variety of on-chip/off-chip voltage regulators. The expected outcomes of this project will help build new generations of highly efficient circuits and systems that can self-adapt to varying operating conditions. The synergies between circuit/system design, control-theoretical exploration, and machine learning as pursued in this project will promote a new interdisciplinary direction for advancing electronic system design.  The depth and breadth of this research will expose students to outstanding educational and training opportunities. Participation from undergraduate and underrepresented students is an important education mission of this project and will be promoted through recruiting and outreaching. The anticipated results from this project are expected to be broad and will be widely disseminated as well as brought to classroom to benefit undergraduate and graduate curriculum.  Collaboration and interaction with industry constitutes an important channel for this project to impact the real world, which will be actively pursued.  <br/><br/>This project is based on the vision that the ultimate quality and efficiency in supply voltage regulation may be best achieved via a heterogeneous chain of voltage processing starting from on-board switching voltage regulators (VRs), to in-package/on-chip switching VRs, and finally to networks of distributed on-chip linear VRs.  Heterogeneous voltage regulation (HVR) systems are promising as they encompass regulators with complimentary tradeoffs in response time, size, efficiency, and cost. The ultimate aim of this project is to enable HVR systems that will guarantee power integrity, incur minimal power loss, and autonomously adapt to workload changes and system/environmental uncertainties at multiple temporal scales.  The above goal will be achieved by pursuing an integrated solution of novel control theory, circuits, and machine-learning enabled autonomous adaptation. Rigorous design techniques for decentralized and centralized control will be developed for distributed on-chip linear regulator networks and the HVR system with guaranteed stability and regulation performance.  Efficient machine-learning algorithms and their on-chip integration will be employed to provide accurate real-time prediction of time-varying load currents. Autonomous adaptation of the HVR system will be supported by power-efficient control policies that preemptively adapt on-chip linear regulator networks and on-chip/off-chip VRs based on machine-learning predicted future current loads.  Coping with system uncertainties is another key objective and will be achieved via deployment of control policies that are self-tuned by machine learning to attain the optimal power efficiency.  The project will explore system-level design optimization to jointly optimize regulation performance, power efficiency, and design overhead across all voltage processing stages in a HVR system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914575","CAREER: From Data to Knowledge: Extracting and Utilizing Concept Graphs in Online Environments","IIS","Info Integration & Informatics","08/26/2018","05/12/2020","Cornelia Caragea","IL","University of Illinois at Chicago","Continuing Grant","Sylvia Spengler","05/31/2022","$395,379.00","","cornelia@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","7364","1045, 7364","$0.00","Knowledge bases today are central to the successful utilization of information available in the large and growing amounts of digital data on the Web. Such technologies have started to unleash a transformation of Web search from a keyword match to discovery, learning, and creativity, which are crucial to promoting the goal of knowledge discovery. Unfortunately, the search for information remains inherently difficult for significant portions of the Web such as the Scholarly Web, which contains many millions of scientific documents. For example, PubMed has over 20 million documents, whereas Google Scholar is estimated to have more than 100 million. Open-access digital libraries such as CiteSeerX, which acquire freely-available research articles from the Web, witness an increase in their document collections as well. Despite attractive advancements by scholarly search portals, semantic search technologies that ""understand"" complex concepts and their relations and can systematically satisfy users' intricate information needs have yet to be investigated on the Scholarly Web. The goal of this project is to design solutions to make information more accessible and comprehensible to Scholarly Web users in particular, and Web users in general, and to help them discover knowledge more effectively and efficiently. The approach taken will be to develop an integrated framework, focusing on the extraction and utilization of scholarly knowledge graphs in online scholarly environments. Educationally, this work will involve: training of graduate, undergraduate, and high-school students, particularly encouraging the participation of women and underrepresented groups in the research efforts; curriculum development and integration of research into courses taught by the PI; exposure of students to industry and international experiences; and education for the general public.<br/> <br/>The project will target the following research objectives: (1) explore the construction of scholarly knowledge graphs that combine evidence from multiple resources in an open information extraction framework; (2) design and develop novel algorithms for the detection and analysis of interesting and previously unknown connections between concepts, in order to enforce knowledge discovery on the Scholarly Web; and (3) investigate the utility of scholarly knowledge graphs in a question answering system. The results of this research will be integrated into the CiteSeerX digital library (http://citeseerx.ist.psu.edu). The software, tools, and benchmark datasets, which will be developed during the course of this project will be made publicly available. All findings will be shared with the research community through publications in academic journals and presented in Information Retrieval, Text Mining and Natural Language Processing conferences. For further information, see the project web page:  http://www.cse.unt.edu/~ccaragea/skg.html."
"1842952","EAGER: Collaborative Research: MATDAT18 Type-I: Development of a machine learning framework to optimize ReaxFF force field parameters","DMR","TRIPODS Transdisciplinary Rese, DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY","10/01/2018","08/22/2018","Tirthankar Dasgupta","NJ","Rutgers University New Brunswick","Standard Grant","Daryl Hess","09/30/2020","$140,002.00","Ying Hung","tirthankar.dasgupta@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","MPS","041Y, 1712, 1765","054Z, 062Z, 7916, 7926, 9216","$0.00","NONTECHNICAL SUMMARIES.<br/>This award supports continued collaboration of materials researchers with data scientists kindled at the MATDAT18 Datathon event. Recent advancements in technological devices, such as smart phones, batteries, and solar cells, are consequences of the discovery and application of novel materials. Computer simulations of systems of atoms could be insightful in predicting and discovering new materials. Simulations based on quantum mechanics are computationally expensive and prohibitive for all but for systems of a few atoms. Simulations involving a much larger number of atoms can be done using molecular dynamics which utilizes models for the interactions between atoms. ReaxFF is one such interaction model which can also describe chemical bonding. Currently, more than a thousand academic groups and companies are using ReaxFF to model systems of atoms. It takes many parameters to fully specify a ReaxFF model. These parameters control the interactions between atoms and must be individually optimized for different types of materials. Due to the prohibitively large number of possible combinations of parameters, this optimization process is time consuming and complex, and consequently limits the applicability of ReaxFF. A procedure that can produce optimum parameter sets within a reasonable time will facilitate novel material research by accelerating the investigation of underlying physics and chemistry on the scale of atoms. Recent developments in machine learning are promising in terms of solving such high dimensional global optimization problems. The goal of this study is to develop a procedure that will enable fast and high-quality force field development using machine learning models and make this procedure accessible to all current and future ReaxFF users.<br/><br/><br/>The results of this project can also be applied to other large-scale multi-objective optimization problems and can have impacts on many scientific disciplines that involve large and complex data. The developed machine learning code and optimization procedure will be shared with researchers through the Materials Computation Center at Penn State University and GitHub. Some outreach programs will be conducted for educating the next generation of materials scientists, data scientists and statisticians. The research teams will create diverse environments in their laboratories in terms of race, gender and national origin. The research will also provide an excellent opportunity to recruit students from underrepresented groups to participate in projects at the interface between materials science, data science, and statistics and is highly relevant to societal needs.<br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports continued collaboration between a materials researcher and a data scientist kindled at the MATDAT18 Datathon event. ReaxFF is a commonly used reactive force field method, capable of simulating bond formation and dissociation in large atomistic systems. In order to reveal the physics behind these systems accurately by using the ReaxFF simulations, the force field parameters must be optimized for each different materials system, and the high-dimensional force field parameter landscape should be explored thoroughly during optimization. However, the large number of existing parameters limit the optimization stage of the force field development, as the conventional optimization approaches become time-consuming. This challenge can be resolved by the development of an efficient optimization framework. In this project, an efficient sequential optimization framework will be developed, including a ""minimum energy"" sequential search and a novel ""divide-and-conquer"" strategy for efficient Gaussian process modeling. This study will make ReaxFF force field development more practical, which will enable fast access to physics and chemistry in a wide range of material systems to enhance novel material design.<br/>    <br/>This project can serve is an example of how rigorous statistical/machine learning methods can be used to tackle important problems in materials science and engineering. The project may be transformative, as it can empower the atomistic-scale understanding of materials systems by using novel techniques in data science and machine learning. The developed iterative optimization procedure will be combined under Python programming language to facilitate implementation to commercial molecular dynamics packages. From a statistical point of view, the idea of divide-and-conquer and design-based subsample aggregation to reduce computational complexity of Gaussian process modeling is innovative. It can open a new path in statistics/data science with big data settings and can lead to advances in machine learning and optimization. The sequential optimization framework constructed for high-dimensional problems may open new avenues for studying problems with massive and complex input structure and energize both theoretical and applied research in statistics and machine learning.<br/><br/>The award is jointly funded through the Division of Materials Research and the Division of Mathematical Sciences in the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822279","Conference: Machine Learning in Science and Engineering","CCF","STATISTICS, PROJECTS, Algorithmic Foundations, Big Data Science &Engineering, Materials Eng. & Processing","06/01/2018","05/18/2018","Dana Randall","GA","Georgia Tech Research Corporation","Standard Grant","A. Funda Ergun","10/31/2019","$29,999.00","Newell Washburn","randall@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1269, 1978, 7796, 8083, 8092","047Z, 062Z, 7556","$0.00","This award supports the first annual Symposium on Machine Learning in Science and Engineering (MLSE), held in Pittsburg, Pennsylvania, June 6-8, 2018.  The meeting, initially organized by Carnegie Mellon University and Georgia Tech, is the first comprehensive and open annual conference bringing together leading researchers in science and engineering whose work benefits from advances in machine learning and data science.  While machine learning has revolutionized many areas of biological and biomedical research, its impact across the sciences and engineering is at an early stage. This symposium will bring together researchers in a diversity of Science, Technology, Engineering, and Math (STEM) areas focused on applying machine learning to problems of fundamental or applied nature. Presentations will focus on adapting existing machine learning methods to current research areas, developing new machine learning algorithms specific to science and engineering, and identifying new frontiers of research that may only be pursued using a data-driven approach. The symposium will offer attendees focused short courses taught by experts in machine learning on a variety of cutting-edge tools that are critical in advancing these fields. The MLSE symposium will help catalyze machine learning methodologies and collaborations across the sciences and engineering, bringing together researchers in a diversity of STEM areas focused on applying machine learning to fundamental and applied problems. Presentations will focus on adapting existing machine learning methods to current research areas, developing new machine learning algorithms specific to science and engineering, and identifying new frontiers of research that may only be pursued using a data-driven approach.<br/><br/>The symposium is anticipated to reach, in its first year, at least 400 direct participants and attendees, including under-represented minorities, those attending Minority Serving Institutions (MSIs), and low income students local to the conference venues, or selected to travel to the event based on merit and need. Several groups within the research community are involved, including a diverse group of students, early career researchers and faculty. Information will be widely disseminated on a continuing basis through news items published via community-specific and broad news release venues. Partial support is being provided primarily to enable participation by students and young researchers, in addition to a limited number of tutorial and plenary speakers.  The organizers are committed to promoting participation among underrepresented groups, junior researchers and students, and including tutorials to widen accessibility to as large a group of attendees, as possible. The organizers will have an open competition for these travel awards, selected by a diverse committee, and the opportunity to apply will be widely disseminated across the relevant disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819388","SBIR Phase I:  Determination of complex outcome measures using narrative clinical data to enable observational trials","IIP","SBIR Phase I","07/01/2018","06/25/2018","Daniel Riskin","DE","VMT, Inc.","Standard Grant","Henry Ahn","06/30/2019","$224,793.00","","grants@verantos.com","113 BARKSDALE PROFESSIONAL CTR","Newark","DE","197113258","6507777978","ENG","5371","5371, 8038","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) project is to enable accurate electronic health record (EHR)-based studies to support precision medicine. The country is embarking on a journey of using real world evidence (RWE) to adjust the existing standard of care. EHR-based subgroup analytics and comparative effectiveness studies will increasingly be used to augment regulatory and reimbursement approval. These efforts require accurately recognizing clinical outcomes in real world scenarios. Studies attempting to identify real world outcomes, such as pain and disease free survival, have shown low accuracy rates in claims and EHR discrete data. This proposal aims to accurately detect challenging outcomes from EHR data using advanced semantic technologies. The goal is to enable accurate RWE studies to achieve safer and more effective use of RWE in clinical practice.<br/><br/>This SBIR Phase I project proposes to create an application to extract clinical outcomes from real world data to enable EHR-based pragmatic clinical trials (PCTs). The approach uses natural language processing (NLP) to dive deep into the health record for exposure, intervention, and outcome data that do not exist or are inaccurate in claims and EHR discrete data. Project objectives include extracting features from clinical data using NLP and ontologic mapping, developing a knowledge database that reflects common outcomes in observational and clinical trials, and inferring outcome from extracted features using clinical data. The project will validate the outcome detection engine using de-identified longitudinal clinical data to assess accuracy of feature extraction and inferred outcome.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755895","CRII: RI: Towards Learning Skills from First Person Demonstrations","IIS","Robust Intelligence","03/15/2018","03/30/2018","Hyun Soo Park","MN","University of Minnesota-Twin Cities","Standard Grant","Jie Yang","01/31/2021","$175,000.00","","hspark@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495","7495, 8228","$0.00","Humans learn a skill from an expert's demonstrations such as playing tennis, which requires understanding subtle details of sequential actions, e.g., eye-hand coordination across swing motion. This project develops technologies to learn such skills by observing demonstrations from first-person videos. The first-person videos are highly dynamic, local, and person-biased due to severe head movements, which generates a larger variation of visual data. Analyzing the videos produced by the head-mounted camera system is challenging because state-of-the-art computer vision systems built upon third-person videos cannot be directly applied. The research team addresses these challenges by developing both hardware and computational models. The principal investigator of the project will integrate the research results into a sequence of newly designed computer vision courses in the University of Minnesota. The research team will publicly share the dataset, representation, and trained models, and organize workshops and tutorials to broader audiences in computer vision and robotics.<br/><br/>This research investigates problems in learning skills from first person demonstrations. This project designs a head-mounted camera system composed of a first-person camera and multiple proxemic cameras that can fully cover the space of interactions. The project also develops a new representation specific to the head-mounted camera system, called proxemic affordance map, to efficiently represent visual scene and action in 3D. The proxemic affordance map encodes 3D visual semantics in a form of 3D depth map, visual attention, and body pose, which enables measuring the correlation between action and its surroundings. This allows learning the dynamics of proxemic affordance map to model diverse physical activities, e.g., how an action will change the state of its surrounding contexts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801751","SaTC: CORE: Medium: Collaborative: Towards Robust Machine Learning Systems","CNS","Secure &Trustworthy Cyberspace","08/01/2018","07/18/2018","Hao Chen","CA","University of California-Davis","Standard Grant","Wei-Shinn Ku","07/31/2022","$400,000.00","","chen@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8060","025Z, 7434, 7924","$0.00","Machine learning techniques, particularly deep neural networks, are increasingly integrated into safety and security-critical applications such as autonomous driving, precision health care, intrusion detection, malware detection, and spam filtering.  A number of studies have shown that these models can be vulnerable to adversarial evasion attacks where the attacker makes small, carefully crafted changes to normal examples in order to trick the model into making incorrect decisions.  This project's goal is to develop formal understandings of and defenses against these vulnerabilities through characterizing the relationship between adversarial and non-adversarial examples, developing mechanisms that exploit this relationship to support better detection of adversarial examples, and metrics and methods to demonstrate the robustness of machine learning models against them.  Together, the  theories, algorithms, and metrics developed will improve the robustness of machine learning systems, allowing them to be deployed more securely in mission-critical applications.  The team will also make their datasets and source code publicly available and use them in their own courses and research with both graduate and undergraduate students, with particular efforts to include students from underrepresented groups in Science, Technology, Engineering and Math.  The work will also support high school outreach programs and summer camps to attract younger students to study machine learning, security, and computer science.<br/><br/>The project is organized around three main thrusts that combine to provide a holistic approach to modeling and defending against evasion attacks.  The first thrust aims to characterize both normal and adversarial examples via systematic measurement studies.  This includes considering different types of regions around specific examples (e.g., metric ball, manifold, and transformation-induced regions) and then characterizing the examples' vulnerability based on a number of algorithms for combining classifications of other examples in the nearby regions.  The second thrust focuses on designing robust defenses against adversarial examples by using representative data points in a region, aggregating multiple data points, and using a diverse set of classifiers to reduce the vulnerability induced by using single data points or algorithms.  The third thrust involves defining metrics for modeling robustness along with theories and algorithms that leverage those metrics to analyze model robustness.  These include lower bounds of adversarial perturbation in metric balls, robustness metrics based on computational costs, analyses of the representativeness of new datasets relative to training data, and methods for leveraging human estimation of adversarialness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800237","D3SC: In Silico Design of Molecular Catalysts for C-H Functionalization via Machine Learning Algorithms","CHE","Chemical Catalysis","08/01/2018","07/26/2018","Konstantinos Vogiatzis","TN","University of Tennessee Knoxville","Standard Grant","Kenneth Moloy","07/31/2021","$390,000.00","","kvogiatz@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","6884","062Z, 8037, 9263","$0.00","Can we teach a machine chemical intuition so that the machine can help us discover new molecules and materials with useful properties? Can doing so accelerate these discoveries? These are questions that Dr. Vogiatzis of the University of Tennessee is addressing. To achieve these goals, he is applying statistical models that can learn new complex mathematical functions. These functions have the power to connect meaningful chemical information with molecular structure by looking for patterns that contribute to important and desirable properties. This chemically-driven machine learning (CDML) computational approach is a promising technique for a large variety of environmental, biological, and energy-related problems. To demonstrate the strength and applicability of CDML, Dr. Vogiatzis and his research group are examining the chemical properties of iron species that mimic the action of iron-containing enzymes. Dr. Vogiatzis is providing interdisciplinary research opportunities to students, including those from underrepresented groups. In turn, they are learning about data science and machine learning methodologies, an important tool for the development of tomorrow's technologies. The computational tools developed in Dr. Vogiatizis' research group are being provided free to other researchers interested in machine learning and chemistry.<br/><br/>With funding from the Chemical Catalysis Program and the D3SC (Data Driven Discovery Science in Chemistry) initiative of the Chemistry Division, Dr. Vogiatzis of the University of Tennessee is developing computational tools for efficient high-throughput computational screening of large libraries of molecular complexes. The long-term target is the design of the next generation of catalysts for efficient C-H functionalization via quantum chemistry and machine learning. His research group is currently working on an integrated computational protocol that examines one class of reactive sites for C-H activation, but the proposed methodology is transferable to other chemical procedures as well. The biomimetic catalytic site that is currently examined is the Fe(IV)-oxo intermediate, active site of heme and non-heme enzymes, and is chosen due to the vast literature that can guide the development of the computational model. Dr. Vogiatzis is engaging undergraduate and graduate students from underrepresented groups in his research. He is also engaging students from East and Central Tennessee about his scientific interests. In addition, the development of free, open-source software is of high importance for the scientific community and the advance of the science. The computational tools that are developed with funding from the Chemical Catalysis Program are being provided free-of-charge to other researchers interested in machine learning and chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1903202","III: Small: Collaborative Research: High-Dimensional Machine Learning Methods for Personalized Cancer Genomics","IIS","Info Integration & Informatics","07/01/2018","10/25/2018","Quanquan Gu","CA","University of California-Los Angeles","Continuing Grant","Sylvia Spengler","07/31/2021","$300,000.00","","qgu@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364","7364, 7923","$0.00","The key to success in personalized and precision cancer genomics lies in: (1) discovering and understanding the molecular-level mechanisms of how genetic alterations influence various cellular processes relevant to cancer, and (2) utilizing molecular signatures to tailor more personalized treatment strategies for patients. In order to achieve these goals, various high-throughput experimental methods have been developed in recent years to obtain information about a patient's cancer genome sequence, mRNA expression, protein expression, epigenetic readout, and other detailed information about a patient's tumor. However, algorithms that fully harness such a massive amount of high dimensional data to yield biomedical insights are often lacking. This project will advance the field of data-driven complex modeling of cancer genomic data for personalized cancer treatment by developing novel algorithms that use emerging and new techniques in high-dimensional machine learning. The results of this research have the potential to impact both the machine learning field and the computational genomics field. The educational components integrated with the research program will develop new curriculum materials, involve undergraduate students and underrepresented groups in research, and train a new generation of interdisciplinary graduate researchers. <br/><br/>This project consists of two synergistic research thrusts to develop novel high-dimensional machine learning algorithms for analyzing high-throughput cancer genomic data. First,  the project will develop high-dimensional graphical models for multi-view data modeling to integrate data from heterogeneous genome-wide data sources. Second, it will devise novel high-dimensional collaborative learning methods for personalized drug recommendation. The high-dimensional graphical models will be used to estimate networks for different cancer subtypes.  These networks will then  be integrated into the recommendation algorithms, which in turn will help improve the multi-view graphical model estimation. This project will enhance the ability to interpret large-scale cancer genomics data by pinpointing the roles of complex molecular interactions in cancer onset and progression, which will enable  novel ways to more effectively discover personalized molecular signatures and more targeted potential treatments of cancer. Such technical innovation and conceptual advancement have the potential to reshape the way that one approaches graphical model estimation and its role in biological contexts. The project will potentially open up new possibilities for both theoreticians and practitioners in machine learning and computational biology as well as other disciplines."
"1750555","CAREER: Stable Foundations for Reliable Machine Learning","CCF","Algorithmic Foundations","02/01/2018","06/04/2020","Moritz Hardt","CA","University of California-Berkeley","Continuing Grant","A. Funda Ergun","01/31/2023","$285,898.00","","hardt@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7796","1045, 7926","$0.00","Across all sciences, researchers hope to use algorithms and machine learning to derive reliable insights from data, but often research findings turn out to be false or hard to replicate. Indeed, assessing the validity of insights suggested by data is presently a difficult and error-prone task. Even in industry, where machine learning has fueled dramatic advances, more principled ways of benchmarking and improving the performance of a machine learning system would make a major difference. What often work best in practice are poorly understood heuristics, leading to much guesswork with varying results. This inscrutable behavior of machine learning also has repercussions on society at large as more and more people struggle with the implications of algorithmic decisions in their daily lives. Fairness, interpretability, and transparency have become major talking points as algorithms increasingly aid or replace human judgment.<br/><br/>The PI aims to build guiding theory alongside scalable algorithms that make the practice of machine learning more reliable, transparent, and aligned with societal values. Focusing on algorithmic stability as a unifying technical framework, this proposal targets several foundational challenges including the design of a robust methodology to address the reliability crisis in data science, a working theory for why and when large artificial neural networks train and generalize well, and a universal framework to reason about generalization in unsupervised learning as is presently lacking. A particular emphasis is on application domains of societal impact. The PI has long been invested in topics such as privacy, fairness, accountability and transparency in machine learning not only through academic publications, but also through workshops, mentorship, teaching, and interdisciplinary engagements."
"1901527","RI: SMALL: Fast Prediction and Model Compression for Large-Scale Machine Learning","IIS","Robust Intelligence","08/13/2018","02/12/2019","Cho-Jui Hsieh","CA","University of California-Los Angeles","Standard Grant","Rebecca Hwa","07/31/2020","$362,846.00","","chohsieh@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7495, 7923","$0.00","In order to handle large-scale problems, many algorithms have been proposed for improving the training speed of machine learning models. However, in many real world applications the bottleneck is at the prediction phase instead of the training phase due to the time and space complexity of prediction. Unlike the training phase that can run for several hours on multiple machines, the prediction phase usually runs on real-time systems; as a result, each prediction has to be done in a few seconds in order to provide immediate feedback to users. Furthermore, applications that run on mobile devices have even more strict constraints on memory capacity and computational resources. To address these issues, this research develops a new family of machine learning algorithms with faster prediction time and smaller model size. The outcome of this project creates a fundamental shift in the applicability of machine learning models to real-time online systems and on-device applications. Software packages and experimental platforms are made available to the public after being tested on applications. Besides the research objectives, the PI also pursues educational objectives including promoting undergraduate research, involving under-represented minorities in science and engineering, and developing undergraduate and graduate data science curriculums.<br/><br/>The goal of this project is to develop novel approaches for reducing prediction time and model size of machine learning algorithms. In particular, the project focuses on machine learning applications with large output space (matrix factorization, extreme multi-class/multi-label classification), and highly nonlinear models (kernel methods and deep neural networks). A series of approximation algorithms are studied, including tree-based algorithms, clustering approaches, and sub-linear time search algorithms. A unified framework is developed for these algorithms and the trade-off between accuracy and prediction time/model size is studied both in theory and in practice. The proposed algorithms are evaluated on a broad range of real world applications, including online web services and on-device applications."
"1839340","TRIPODS+X: VIS: Creating an Annual Data Science Forum","DMS","TRIPODS Transdisciplinary Rese, Special Initiatives, DMR SHORT TERM SUPPORT","10/01/2018","09/10/2018","Dana Randall","GA","Georgia Tech Research Corporation","Standard Grant","Tracy Kimbrel","09/30/2020","$200,000.00","Srinivas Aluru, Jeannette Wing, Newell Washburn","randall@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","041Y, 1642, 1712","047Z, 062Z","$0.00","This project aims to create an annual week-long Data Science Forum (DSF), co-locating several events intended to advance knowledge, create networks of researchers, and train students and faculty.  The focus of this forum will be on developing the next generation of AI technologies and researchers with the underlying hypothesis that applications across the sciences and engineering disciplines will be the context of the next major advances in data-driven approaches.  DSF brings together the Second Symposium on Machine Learning in Science and Engineering (MLSE), a Women in Data Science Workshop (WDSW), and a Foundations of Data Driven Discovery workshop (FDDD).  A forum including multiple events will bring more women to MLSE and FDDD, while juxtaposing MLSE with FDDD will encourage cross-fertilization among the domain scientists and engineers and the core TRIPODS (Transdisciplinary Research in Principles of Data Science) community.  The 2nd MLSE will conclude with a Visioning Working Group where a select group of researchers will produce a white paper on the future of machine learning.<br/><br/>The Data Science Forum will help catalyze machine learning methodologies and collaborations across the sciences and engineering, bringing together a diverse set of STEM fields applying machine learning to fundamental and applied problems.  Presentations will focus on adapting existing machine learning methods to current research areas, developing new machine learning algorithms specific to science and engineering, and identifying new frontiers of research that may only be pursued using a data-driven approach.  Disseminating machine learning methods across science and engineering could have lasting implications for US research.  The forum will supplement the technical research program with short courses taught by experts in machine learning on a variety of cutting-edge tools that are critical in advancing these fields.  Each track will have a theme centered in a traditional domain, but each is aiming to itself be interdisciplinary.  Running these tracks in parallel will help foster tight knit communities of researchers in each of the applied science or engineering tracks, as well as the TRIPODS community, while fostering cross-fertilization of ideas across fields through joint events and co-location.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750358","CAREER: Generative Models for Targeted Domain Interpretability with Applications to Healthcare","IIS","Info Integration & Informatics","02/15/2018","01/29/2019","Finale Doshi-Velez","MA","Harvard University","Continuing Grant","Sylvia Spengler","01/31/2023","$436,764.00","","finale@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7364","1045, 7364, 9102","$0.00","The imminent deployment of AI and machine learning in poorly-characterized settings such as autonomous driving, personalized news feeds, and treatment recommendation systems has created an urgent need for machine learning systems that explain their decisions. Interpretability helps human experts ascertain whether machine learning systems, trained on technical objective functions, have sensible outputs despite unmodeled unknowns. For example, a clinical decision support system will never know all of a patient's history, nor may it know which of many side effects a specific patient is willing to tolerate. An important challenge, then, is how to design machine learning systems that both predict well and provide explanation.  Within this broad challenge, this work develops techniques for domain-targeted interpretability, finding summaries of high-dimensional data that are relevant for making decisions.  The proposed work focuses on healthcare applications, where interpretable models are essential to safety.  However, the project aims to produce foundational learning algorithms applicable to a range of scientific and social domains.  The developed methods will be tested on real problems in personalizing treatment recommendations and prognoses for sepsis, depression, and autism spectrum disorder. Thus, the successful completion of the work will impact both interpretable machine learning and clinical science.  All software developed in the course of the project will be freely shared.  The educational component of the proposed work will educate early elementary students about the impact of statistics in medicine and educate policy-makers and legal scholars on how a right to explanation might be regulated in the context of machine learning, such as clinical decision support systems. PI Doshi-Velez also engages high school students, undergraduates, women, and researchers from underserved areas in her lab.<br/><br/><br/>The proposed work addresses a specific challenge common in scientific settings: domain-targeted interpretability. In many scientific domains, unsupervised generative models are used by domain experts to understand patterns in the data, but as the dimensionality of data grow, the most salient patterns in the data may not be relevant for the specific investigation. For example, a psychiatrist may find the strongest signals in the data from his patient cohort come from diabetes and heart disease, which may not be relevant for choosing therapies for depression.  The proposed work leverages synergies in explaining domain-relevant patterns in the data and performing well on domain-relevant tasks to achieve domain-targeted interpretability. It defines a task-constrained approach to domain-targeted interpretability and develops essential inference techniques, develops extensions to sequential decision making, and defines extensions to improve downstream task performance while retaining interpretabilty.  While there is a large body of work on making unsupervised learning models also useful for downstream tasks, none of these approaches truly manage the trade-offs between providing an interpretation of data and task performance. The proposed work addresses these shortcomings to make domain-targeted interpretability and task performance synergistic goals, and proposes a number of innovations to solve the proposed objective. Innovations include combining an existing rich literature on inference traditional unsupervised models with modern inference techniques and directly searching for dimensions or patterns relevant to the downstream task."
"1801584","SaTC: CORE: Medium: Collaborative: Towards Robust Machine Learning Systems","CNS","Secure &Trustworthy Cyberspace","08/01/2018","07/18/2018","Neil Gong","IA","Iowa State University","Standard Grant","Wei-Shinn Ku","08/31/2019","$400,000.00","","neil.gong@duke.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","8060","025Z, 7434, 7924","$0.00","Machine learning techniques, particularly deep neural networks, are increasingly integrated into safety and security-critical applications such as autonomous driving, precision health care, intrusion detection, malware detection, and spam filtering.  A number of studies have shown that these models can be vulnerable to adversarial evasion attacks where the attacker makes small, carefully crafted changes to normal examples in order to trick the model into making incorrect decisions.  This project's goal is to develop formal understandings of and defenses against these vulnerabilities through characterizing the relationship between adversarial and non-adversarial examples, developing mechanisms that exploit this relationship to support better detection of adversarial examples, and metrics and methods to demonstrate the robustness of machine learning models against them.  Together, the  theories, algorithms, and metrics developed will improve the robustness of machine learning systems, allowing them to be deployed more securely in mission-critical applications.  The team will also make their datasets and source code publicly available and use them in their own courses and research with both graduate and undergraduate students, with particular efforts to include students from underrepresented groups in Science, Technology, Engineering and Math.  The work will also support high school outreach programs and summer camps to attract younger students to study machine learning, security, and computer science.<br/><br/>The project is organized around three main thrusts that combine to provide a holistic approach to modeling and defending against evasion attacks.  The first thrust aims to characterize both normal and adversarial examples via systematic measurement studies.  This includes considering different types of regions around specific examples (e.g., metric ball, manifold, and transformation-induced regions) and then characterizing the examples' vulnerability based on a number of algorithms for combining classifications of other examples in the nearby regions.  The second thrust focuses on designing robust defenses against adversarial examples by using representative data points in a region, aggregating multiple data points, and using a diverse set of classifiers to reduce the vulnerability induced by using single data points or algorithms.  The third thrust involves defining metrics for modeling robustness along with theories and algorithms that leverage those metrics to analyze model robustness.  These include lower bounds of adversarial perturbation in metric balls, robustness metrics based on computational costs, analyses of the representativeness of new datasets relative to training data, and methods for leveraging human estimation of adversarialness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746469","SBIR Phase I:  Processor Architecture for Radically Improved Performance and Energy Efficiency on Sparse Machine Learning","IIP","SMALL BUSINESS PHASE I","01/01/2018","12/21/2017","Mitchell Hayenga","TX","Revolution Computing Incorporated","Standard Grant","Rick Schwerdtfeger","09/30/2018","$224,643.00","","mitch.hayenga@gmail.com","9308 Springwood Drive","Austin","TX","787502940","9794504469","ENG","5371","5371, 8035","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to expand the capability of modern computer systems to execute machine learning applications and enable new uses of machine learning in the everyday lives of people.  Due to widespread success, machine learning is being applied to automate tasks across most modern businesses.  However, the increasingly computationally complex and data intensive nature of these new problems is rapidly increasing. The scale of current problems stress the computational abilities and memory requirements of modern systems.  The technology to be developed under this Phase I project, will enable computer systems with radically higher performance and energy-efficiency while performing these machine learning tasks.  Enabling efficient machine learning will enable complex tasks to fit within modern mobile devices while simultaneously enabling computers within datacenters to solve increasingly large problems.  Finally, as businesses rush to deploy hardware for machine learning, the underlying algorithms and techniques are rapidly evolving.  The technology to be developed is highly adaptable, enabling high efficiency on current machine learning techniques while mitigating risks for businesses likely to adapt new machine learning algorithms.<br/><br/>The proposed project introduces a new hardware architecture for the execution of data and control intensive machine learning workloads.  As machine learning has expanded in use, increasing data sizes have brought about the use of compressed data representations.  However, modern computational devices like microprocessors or graphics processors are highly inefficient when working on problems using these compressed representations due to irregular control and data access patterns.  This Phase I project introduces an adaptable architecture that excels at irregular computing and can dynamically re-allocate resources to hasten execution.  To demonstrate the capabilities of the new architecture, key execution kernels from modern machine learning applications will be adapted and developed to operate on compressed representations.  An existing simulation infrastructure will be extended to model key hardware requirements and gather performance estimations of the newly proposed hardware architectures.  Preliminary estimates demonstrate that multiple factors of improvement in energy efficiency and performance are expected across the key operations of machine learning applications. "
"1745640","RTG: Advancing Machine Learning - Causality and Interpretability","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","08/01/2018","07/14/2020","Deborah Nolan","CA","University of California-Berkeley","Continuing Grant","Victor Roytburd","07/31/2023","$1,195,875.00","Bin Yu, Jasjeet Sekhon, Peng Ding, Avi Feller","nolan@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 7335","7301","$0.00","Faculty in the Statistics Department at the University of California, Berkeley develop an integrated program of research and education to support undergraduate research experiences, graduate research traineeships, and postdoctoral fellowships. The common research theme of the training activities is how to leverage the predictive power of statistical machine learning to address questions of causality and interpretability. The project aims to prepare the next generation of statisticians and data scientists to tackle new, important problems that arise from the analysis of massive data. Intuitively it seems that more reliable and precise inferences can be drawn from larger data sets. However, decisions and interventions must be interpretable and justified by statistical measures of uncertainty, which are challenging in this setting.  This program will infuse ideas, energy, and resources in an integrated way at all levels of the educational program, from the undergraduate major to the postdoctoral experience, recruiting students and preparing them to participate in the extraordinary range of opportunities in this exciting new field.<br/><br/>The research in this project will pursue theory to bridge the gap between causal inference and machine learning research, including high-dimensional inference, multiple testing, causal inference with interference, and causality and gene expression. The project is at the frontiers of statistics and data science, bridging the divide between machine learning and causal inference with potential impact far beyond the discipline of statistics. Plans are to redesign and expand the engagement of undergraduates in research through a graduate student mentorship program; to design new courses at the graduate and undergraduate levels, including an introductory course that builds on connections between data science, social sciences, and ethics; and to enhance graduate research training via a research symposium. The program will include a graduate professional development training series that addresses topics in technology, presentation and writing skills, and building an inclusive science community.  The project will also provide significant training in teaching for graduate students and postdoctoral associates. Through a combination of channels, the innovations in training will spread to other institutions and disciplines, e.g., demonstrating the power of machine learning in policy and education settings where causal inference is central. The program also includes the development of educational materials with plans to disseminate them widely throughout the broader community. The project will emphasize recruitment and retention efforts targeted to increase the diversity of domestic students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833154","Workshop for Women in Machine Learning","IIS","Information Technology Researc, Robust Intelligence","11/01/2018","05/31/2018","Tamara Broderick","MA","Massachusetts Institute of Technology","Standard Grant","Rebecca Hwa","10/31/2020","$50,000.00","","tbroderick@CSAIL.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1640, 7495","7495, 7556","$0.00","Since 2006, the annual workshop for Women in Machine Learning (WiML) has brought together women researchers of all levels, from industry and academia, to exchange research ideas and build mentoring and networking relationships.  The one-day workshop has been especially beneficial for junior graduate students, giving them a supportive environment in which to present their research (in many cases, for the first time) and enabling them to meet peers and more senior researchers. Networking opportunities have helped senior graduate students and postdoctoral fellows find jobs following graduation.  Indirectly, by retaining women in the field, WiML assists in the development of research products by women. Women continue to be underrepresented in the machine learning community. The ability to meet and interact with other women (role models and colleagues) in a technical environment allows women to build confidence in sharing their work; it is the most often cited benefit of the workshop.  <br/><br/>The workshop achieves these goals via events including the following. (1) Invited talks by established researchers. Technical talks by researchers from academia and industry share core machine learning research as well as serve as role models. (2) Contributed talks and posters by students provide opportunities for graduate students to showcase their own research and raise the presenters' visibility among their peers. (3) Research and career advice roundtable discussions. WiML matches senior researchers with junior researchers in small discussion and mentoring groups. The support from senior women, many of whom were previous attendees, speaks to how WiML is creating a lasting community where women help retain women in the field. WiML is primarily a technical event. Attendees have substantive technical discussions with other attendees, and many attendees go on to stay in contact with colleagues they meet at WiML as research collaborators. WiML has an established pipeline of senior women who continue to return to the workshop each year to encourage and mentor younger researchers.  WiML has also helped with publicity and coordination when other organizations wanted to host events for women, such as lunches at conferences or more local events.  The WiML Board has released a public directory of women in machine learning (with over 1,300 entries) to assist event organizers in finding female speakers and panelists.  Thus, in addition to the workshop, WiML is becoming the de facto source for connecting and finding women in machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750679","CAREER: Autonomous Wearable Computing for Personalized Healthcare","CNS","CSR-Computer Systems Research","05/15/2018","04/03/2020","Hassan Zadeh","WA","Washington State University","Continuing Grant","Marilyn McClure","04/30/2023","$313,821.00","","hassan.ghasemzadeh@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7354","1045, 9251","$0.00","Wearables are poised to transform health and wellness through automation of cost-effective, objective, continuous, real-time, and remote health monitoring and interventions. These technologies utilize machine learning algorithms to detect important health events and to predict impending medical complications. Currently, however, machine learning algorithms for these systems are designed based on sensor data that are collected and labeled/annotated in controlled environments such as laboratory settings and clinics. This process of data collection, data annotation, and algorithm training has created great impediments to scalability of wearable systems because (1) collecting and labeling sufficiently large amounts of sensor data is a time consuming, labor-intensive, expensive, and often infeasible task; and (2) wearables are deployed in highly dynamic environments of the end-users whose physical, behavioral, social, and environmental context undergo consistent changes. Such changes result in drastic decline in the accuracy of the machine learning algorithms trained in controlled environments. Therefore, it is important to develop autonomously reconfigurable machine learning algorithms as wearable sensors, settings in which they are utilized, and their configuration changes. This project introduces computational autonomy as an overarching solution for training accurate machine learning algorithms, without human supervision, in highly dynamic, unpredictable, and uncontrolled settings. The successful conclusion of this work will enable future wearables to learn in-situ autonomously, operate in-the-wild reliably, and adapt to the changing context of their users automatically. <br/><br/>This project will develop foundations of computational autonomy for wearable-based health monitoring and interventions through the following research objectives: (1) investigating methods of automatic and autonomous labeling of sensor data in a new setting based on labeled sensor data collected in a different setting by designing combinational optimization methodologies for cross-subject, cross-context, cross-platform, and cross-modality sensor data mapping; (2) developing non-parametric label refinement algorithms to reliably infer labels in a new setting based on uncertain and sporadic knowledge obtained from another, potentially unreliable and heterogeneous, sensor; (3) exploring methodologies for training machine learning algorithms that are robust to unknown parameters of a source sensor and adaptive to dynamically changing signal attributes of the new setting; and (4) validating the developed algorithms and tools through both in-lab experiments and in-the-wild user studies.<br/><br/>This interdisciplinary project will not only address the technical challenges in developing highly performant wearable systems but will also enable actual monitoring of a variety of populations. The work has major broader impacts on conducting high-precision chronic disease management and on the availability of wearable-based consumer applications. This has the potential to lead to the development of products around the concept of computational autonomy and its use in automation of health management, as well as, applications yet to be envisioned. The interdisciplinary nature of this work will provide unique opportunities for integrated research and education. To  this end, the educational objectives will focus on developing a new ambassador program to increase interest of underrepresented minority community college students in Science, Technology, Engineering and Math (STEM) careers in general and in computer science and engineering careers in particular, developing a novel patron program to improve retention of transferred underrepresented minority students through student and parental exposure to wearable-based health monitoring research, engaging undergraduate students in research, and establishment of an interdisciplinary research-based curriculum on computational autonomy. All the data produced over the course of this project, including design methodologies, software algorithms and tools, experimental data, publications, and curriculum will be made publicly available at http://epsl.eecs.wsu.edu/. The data will be stored and hosted on local servers and replicated on external public web servers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854000","TWC: Medium: Collaborative: Efficient Repair of Learning Systems via Machine Unlearning","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/05/2018","09/19/2018","Yinzhi Cao","MD","Johns Hopkins University","Standard Grant","Wei-Shinn Ku","08/31/2021","$465,004.00","","ycao43@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1714, 8060","025Z, 7434, 7924, 9178, 9251","$0.00","Today individuals and organizations leverage machine learning systems to adjust room temperature, provide recommendations, detect malware, predict earthquakes, forecast weather, maneuver vehicles, and turn Big Data into insights. Unfortunately, these systems are prone to a variety of malicious attacks with potentially disastrous consequences. For example, an attacker might trick an Intrusion Detection System into ignoring the warning signs of a future attack by injecting carefully crafted samples into the training set for the machine learning model (i.e., ""polluting"" the model). This project is creating an approach to machine unlearning and the necessary algorithms, techniques, and systems to efficiently and effectively repair a learning system after it has been compromised. Machine unlearning provides a last resort against various attacks on learning systems, and is complementary to other existing defenses.  <br/><br/>The key insight in machine unlearning is that most learning systems can be converted into a form that can be updated incrementally without costly retraining from scratch. For instance, several common learning techniques (e.g., naive Bayesian classifier) can be converted to the non-adaptive statistical query learning form, which depends only on a constant number of summations, each of which is a sum of some efficiently computable transformation of the training data samples. To repair a compromised learning system in this form, operators add or remove the affected training sample and re-compute the trained model by updating a constant number of summations. This approach yields huge speedup -- the asymptotic speedup over retraining is equal to the size of the training set. With unlearning, operators can efficiently correct a polluted learning system by removing the injected sample from the training set, strengthen an evaded learning system by adding evasive samples to the training set, and prevent system inference attacks by forgetting samples stolen by the attacker so that no future attacks can infer anything about the samples."
"1841979","EAGER: Exploring Machine Learning and Atmospheric Simulation to Understand the Role of Geomorphic Complexity in Enhancing Civil Infrastructure Damage during Extreme Wind Events","CMMI","ECI-Engineering for Civil Infr","08/15/2018","03/07/2019","Forrest Masters","FL","University of Florida","Standard Grant","Joy Pauschke","07/31/2021","$315,952.00","Luis Aponte","masters@ce.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","ENG","073Y","036E, 039E, 040E, 1057, 116E, 7231, 7916, 9178, 9231, 9251, CVIS","$0.00","Motivated by the extensive damage to Puerto Rico caused by Hurricane Maria's landfall in September 2017, this EArly-concept Grant for Exploratory Research (EAGER) will study how complex topography can accelerate wind and, ultimately, exacerbate damage to buildings and other constructed civil infrastructure.  This research will utilize recent advancements in machine learning and weather forecasting to predict wind speed-up in mountainous terrain and other complex terrestrial environments. The project will leverage the NSF-supported Natural Hazards Engineering Research Infrastructure (NHERI) Terraformer Boundary Layer Wind Tunnel (BLWT) at the University of Florida to characterize the surface wind field over geometrically scaled models of Puerto Rico and the municipal Islands of Vieques and Culebra. This EAGER is a collaboration between the University of Florida (which serves the most hurricane prone state in the U.S.) and the University of Puerto Rico at Mayaguez (a Hispanic-serving institution still recovering from Hurricane Maria), and graduate and undergraduate students from both institutions will be actively involved in the experimental and computational work.  Anticipated project outcomes will include important new insights about the influence of topography on the behavior of damaging winds, new scientific tools that fuse experimentation with advanced computing methods to study extreme wind effects on constructed civil infrastructure, and benchmark datasets that will be made available to other researchers in the NHERI Data Depot (https://www.DesignSafe-ci.org).  Knowledge created by this project can inform future research studies and wind load provisions to improve the resilience of the U.S. to hurricane impacts, and thus better secure the nation's welfare and prosperity after windstorm events.   <br/><br/>This research will make knowledge advancements on multiple fronts.  It will investigate topographic wind effects (i.e., speed-up) on Puerto Rico, with the goal of advancing understanding of how geomorphic complexity (topography) enhances surface winds and makes civil infrastructure more vulnerable to damage.  Specifically, the research will explore and assess the predictive capability of machine learning and multi-scale atmospheric simulation, i.e., computational fluid dynamics nested within a numerical weather prediction (NWP) framework. To support this effort, high-resolution stereoscopic velocity fields over geometrically scaled models of Puerto Rico and the municipal Islands of Vieques and Culebra will be collected from a precision-guided particle image velocimetry system in the Terraformer BLWT. Experiments will be designed to yield critical insights for improving BLWT modeling, while producing foundational datasets to assess the efficacy of (a) supervised regression-based machine learning at predicting how changes in the upwind elevation modify flows and (b) supervised classification-based machine learning methods for determining where ""special"" wind regions should apply in structural wind load provisioning.  Concurrently, NWP enhanced with large eddy simulation (LES) will be applied to demonstrate that NWP-LES can improve the hindcasting of a hurricane's wind field in the built environment. If successful, this effort can critically aid the engineering and atmospheric science fields in reaching consensus on standardizing approaches to predict the behavior of surface winds during an extreme wind event.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815780","CSR: Small: ARTEMIS: Algorithm-Hardware Co-Design for Efficient Machine Learning Systems","CNS","CSR-Computer Systems Research","10/01/2018","07/21/2018","Diana Marculescu","PA","Carnegie-Mellon University","Standard Grant","Matt Mutka","09/30/2021","$500,000.00","","dianam@utexas.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7354","7923","$0.00","With the increased popularity of machine learning algorithms deployed on a variety of hardware systems, the problem of identifying the best model among numerous possible configurations has drawn significant attention. The problem is compounded by the need to select the right platform to run these applications, under given power or latency constraints. This ""hardware wall"" forces machine learning service providers to constantly redesign the underlying hardware fabric to satisfy certain constraints. This project develops tools for automatic and efficient co-design of machine learning algorithms and hardware platforms that will result in significant cost and time-to-market reduction for machine learning systems.<br/><br/>The project introduces efficient meta-learning for machine learning systems and algorithm-hardware platform co-design. Specifically, the project will develop meta-learning algorithms for the optimization of machine learning models under system hardware constraints and formulate the hardware design of efficient machine learning systems as a machine learning problem itself, that can be effectively solved by meta-learning optimization algorithms. Finally, the project will develop multi-objective algorithms for the co-design of machine learning applications and hardware platforms they need to run on, and exploit domain knowledge from hardware engineering and design schemes to substantially accelerate hardware-aware model optimization.<br/><br/>The results of the project seek to change the landscape of modeling, optimization, and design methodologies for efficient machine learning systems. Furthermore, the work aims to have an important educational and mentoring component by potentially changing how engineers are trained in a multidisciplinary fashion for dealing with next generation technological advances in general, and the problem of efficiently and intelligently co-designing machine learning algorithms and the hardware platforms they are running on, in particular. The project will involve a diverse graduate and undergraduate trainee population, while expanding the project's outreach to high-school and middle-school students.<br/><br/>The data, code, results, and simulators developed in this project will be made available publicly throughout the duration of the project and for at least four years after the end of the project. The location of the repository is on the website of Carnegie Mellon University's Energy Aware Computing group (www.ece.cmu.edu/~enyac).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757140","Demand Analysis with Many Prices: Methods and Application","SES","Economics, Methodology, Measuremt & Stats","04/01/2018","03/02/2018","Whitney Newey","MA","National Bureau of Economic Research Inc","Standard Grant","Nancy Lutz","03/31/2021","$183,662.00","","wnewey@mit.edu","1050 Massachusetts Avenue","Cambridge","MA","021385398","6178683900","SBE","1320, 1333","1320, 9179","$0.00","This research develops machine learning methods to estimate economic welfare from big data. Scanner data, as collected in grocery and other retail stores, provides big data that can be used to estimate economic welfare. The investigator develops new double machine learning estimators of economic welfare based on big data. These estimators combine novel machine learning of certain economic weights with machine learning estimators of demand functions to do double machine learning estimation of welfare. These estimators are also generalized and applied to many other problems. In addition, this research uses the fact that scanner data follows individuals over time. Hence, individual demand functions are estimated and averaged to construct improved welfare measures.<br/><br/>The objective of this research is to develop and apply economic demand analysis for large data sets that include many prices, such as scanner data. A common feature of scanner data is that cross price effects tend to be small, often an order of magnitude smaller than own price effects. This feature suggests that machine learning methods that allow for approximate sparsity, where most cross price effects are small, might be useful in practice. This research develops double machine learning estimators of exact consumer surplus and other welfare effects. The investigator uses novel machine learning of objects in Riesz representations that are not conditional expectations. This research produces a general method of double machine learning for generalized method of moments with first step series estimators. Scanner data is often panel data, where individuals or households are followed over time. The investigator further derives identification results for demand in panel data with general heterogeneity, and analyzes regularized fixed effect panel data estimators of average effects that can be applied to demand estimation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842922","EAGER: Collaborative Research: MATDAT18 Type-I: Development of a machine learning framework to optimize ReaxFF force field parameters.","DMR","TRIPODS Transdisciplinary Rese, DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY","10/01/2018","08/22/2018","Adri van Duin","PA","Pennsylvania State Univ University Park","Standard Grant","Daryl Hess","09/30/2020","$140,000.00","","acv13@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","MPS","041Y, 1712, 1765","054Z, 062Z, 7916, 7926, 9216","$0.00","NONTECHNICAL SUMMARIES.<br/>This award supports continued collaboration of materials researchers with data scientists kindled at the MATDAT18 Datathon event. Recent advancements in technological devices, such as smart phones, batteries, and solar cells, are consequences of the discovery and application of novel materials. Computer simulations of systems of atoms could be insightful in predicting and discovering new materials. Simulations based on quantum mechanics are computationally expensive and prohibitive for all but for systems of a few atoms. Simulations involving a much larger number of atoms can be done using molecular dynamics which utilizes models for the interactions between atoms. ReaxFF is one such interaction model which can also describe chemical bonding. Currently, more than a thousand academic groups and companies are using ReaxFF to model systems of atoms. It takes many parameters to fully specify a ReaxFF model. These parameters control the interactions between atoms and must be individually optimized for different types of materials. Due to the prohibitively large number of possible combinations of parameters, this optimization process is time consuming and complex, and consequently limits the applicability of ReaxFF. A procedure that can produce optimum parameter sets within a reasonable time will facilitate novel material research by accelerating the investigation of underlying physics and chemistry on the scale of atoms. Recent developments in machine learning are promising in terms of solving such high dimensional global optimization problems. The goal of this study is to develop a procedure that will enable fast and high-quality force field development using machine learning models and make this procedure accessible to all current and future ReaxFF users.<br/><br/><br/>The results of this project can also be applied to other large-scale multi-objective optimization problems and can have impacts on many scientific disciplines that involve large and complex data. The developed machine learning code and optimization procedure will be shared with researchers through the Materials Computation Center at Penn State University and GitHub. Some outreach programs will be conducted for educating the next generation of materials scientists, data scientists and statisticians. The research teams will create diverse environments in their laboratories in terms of race, gender and national origin. The research will also provide an excellent opportunity to recruit students from underrepresented groups to participate in projects at the interface between materials science, data science, and statistics and is highly relevant to societal needs.<br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports continued collaboration between a materials researcher and a data scientist kindled at the MATDAT18 Datathon event. ReaxFF is a commonly used reactive force field method, capable of simulating bond formation and dissociation in large atomistic systems. In order to reveal the physics behind these systems accurately by using the ReaxFF simulations, the force field parameters must be optimized for each different materials system, and the high-dimensional force field parameter landscape should be explored thoroughly during optimization. However, the large number of existing parameters limit the optimization stage of the force field development, as the conventional optimization approaches become time-consuming. This challenge can be resolved by the development of an efficient optimization framework. In this project, an efficient sequential optimization framework will be developed, including a ""minimum energy"" sequential search and a novel ""divide-and-conquer"" strategy for efficient Gaussian process modeling. This study will make ReaxFF force field development more practical, which will enable fast access to physics and chemistry in a wide range of material systems to enhance novel material design.<br/>    <br/>This project can serve is an example of how rigorous statistical/machine learning methods can be used to tackle important problems in materials science and engineering. The project may be transformative, as it can empower the atomistic-scale understanding of materials systems by using novel techniques in data science and machine learning. The developed iterative optimization procedure will be combined under Python programming language to facilitate implementation to commercial molecular dynamics packages. From a statistical point of view, the idea of divide-and-conquer and design-based subsample aggregation to reduce computational complexity of Gaussian process modeling is innovative. It can open a new path in statistics/data science with big data settings and can lead to advances in machine learning and optimization. The sequential optimization framework constructed for high-dimensional problems may open new avenues for studying problems with massive and complex input structure and energize both theoretical and applied research in statistics and machine learning.<br/><br/>The award is jointly funded through the Division of Materials Research and the Division of Mathematical Sciences in the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815412","CSR: Small: Data Parallel Frameworks for Large-scale Machine Learning through Sync-on-the-Fly","CNS","CSR-Computer Systems Research","10/01/2018","07/29/2018","Lixin Gao","MA","University of Massachusetts Amherst","Standard Grant","Matt Mutka","09/30/2021","$489,999.00","","lgao@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7354","7923","$0.00","The advances in sensing, storage, and networking technologies have led to the collections of high-volume, high-dimensional data. Making sense of these data is critical for companies and organizations to make better business decisions, to bring convenience to our daily life, and even enable better health through biological information and drug discovery. Recent advances in machine learning have led to a flurry of data analytic techniques that typically require an iterative refinement process. However, the massive amount of data involved and potentially numerous iterations required make performing data analytics in a timely manner challenging.<br/><br/>This project aims to design and implement a data parallel programming framework called Sync-on-the-fly.  The framework enables machine learning computations within cloud computing environments to establish synchronization barriers during the execution of the computations. Since synchronization barriers are established for building consistent model parameters, they do not need to be predefined.  The barriers can be established during the computation based on the progress of the computation. This data parallel programming model preserves the semantics of machine learning algorithms. The goals are to build theoretical foundations and create efficient distributed frameworks for a series of well-known machine learning algorithms, and establish the programming models for these computations.<br/><br/>The technologies developed from this project will have immediate important applications on road traffic prediction, biological information discovery, online marketing, and computer forensic analysis. This project will bring fast, accurate, and cost-effective processing of massive data to users. This project will also train new graduate engineers in distributed framework design, machine learning algorithms, and big data analytics. All of these skillsets are in broad demand in US industry.<br/><br/>The data and software codes produced for the distributed framework and distributed machine learning algorithms will be made publicly available at the research website http://rio.ecs.umass.edu/html/research/index.html. The website will be retained for at least three years after the conclusion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824253","Machine Learning in Macroeconomic Modeling","SES","Economics","07/15/2018","07/26/2018","In-Koo Cho","IL","University of Illinois at Urbana-Champaign","Standard Grant","Nancy Lutz","10/31/2019","$310,999.00","","in-koo.cho@emory.edu","1901 South First Street","Champaign","IL","618207406","2173332187","SBE","1320","9179","$0.00","This award funds research that will examine whether incorporating machine learning algorithms into macroeconomic models can result in better ways to understand modern economies.  The project will begin by assuming that individual consumers, workers, and firms can be modeled as making their decisions according to a particular type of machine learning algorithm (a boosting algorithm). The project seeks to understand the conditions under which a machine learning algorithm can emulate the decision-making process of a rational individual. The team will also analyze likely long run economic outcomes when these algorithms are used under various institutional and informational assumptions. The project, therefore, may develop a valuable new technique for modeling individual decisions in the context of an entire economic system. It could also help us understand how future economic outcomes may be affected by the increased use of machine learning methods to aid or even substitute for human decision making. The project could therefore help guide efforts to improve the competitiveness of the US economy.<br/><br/>The research team will exploit one of the central components of the machine learning algorithm, called the boosting algorithm, to build a highly accurate forecasting rule from a collection of rudimentary and possibly inaccurate forecasting rules. The usual approach in economic models is to assume that the agents (individuals, firms, etc.) are typically endowed with misspecified models. When this is the case, an individual or firm's decision-making process typically relies on simple, yet well fit, forecasting rules, which can differ from the true data generating process. The team aim to understand whether an agent endowed with flawed but well fit models can behave as if she knows the true data generating process.  The team plans to pursue this objective in two steps. In the first part of the project, the team will examine learning dynamics under misspecified models.  As an example, it examines a new class of learning models in which the agent has to learn the growth rate instead of the level of a variable of interest. In many macroeconomic models, the growth rate (e.g., inflation rate) rather than the level of a variable (e.g., price) is the main focus of the investigation.  Assuming that the agent learns the growth rate through a recursive learning process rather than rational expectations may result in better explanations of important macroeconomic dynamics, such as recurrent hyperinflation and stock price volatility. The second step in the research plan investigates the dynamics of a specific machine learning algorithm with two research objectives:  [1] if an agent is endowed with misspecified models, how the decision maker can test and build a new model to improve the forecast, and [2] what are the asymptotic properties of the processes of constructing new models, in particular whether the agent can emulate the rational agent in the long run.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764071","CHS: Medium: Collaborative Research: Inverse Anatomical Modeling of the Face for Orthognathic Surgery","IIS","HCC-Human-Centered Computing, Smart and Connected Health","08/01/2018","07/26/2018","Ladislav Kavan","UT","University of Utah","Standard Grant","Ephraim Glinert","07/31/2022","$889,173.00","Srikumar Ramalingam, Jesse Goldstein","ladislav.kavan@gmail.com","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7367, 8018","7367, 7924, 8018","$0.00","The face is the center of an individual's sense of identity and self-esteem, and plays a crucial role in interpersonal relationships. The current state of the art approaches computational modeling of human faces from two distinct angles. Computer graphics models feature high visual realism, as seen in the movies.  Whereas biomechanics focuses on physical realism, modeling the face as a sophisticated mechanical system that obeys the laws of physics. This project will bridge the gap between these two viewpoints and construct models of the face that offer both visual and physical realism. This is of the utmost importance in applications such as surgical prediction. Close to five percent of the population of the United States has a dentofacial anomaly that may require jaw surgery, which can have a profound effect on the appearance of the face. Virtually every patient asks, ""How will I look after the treatment?"" Even though this is an important and well-studied problem, there are currently no methods capable of predicting post-operative changes in facial expressions. By combining both visual and physical realism, this research will create the first system that can provide a natural, 3D visual answer to the patient's question by displaying a photorealistic facial animation after a simulated surgical procedure. Additional broad impact will derive from project outcomes because the new numerical techniques for the efficient simulation of biomaterials will provide a reusable foundation that can be leveraged for computational modeling of a variety of engineering materials that exhibit pronounced heterogeneity and anisotropy. The anatomical modeling framework developed in this work will also serve as a launchpad for future inquiry of interest to medical science (modeling of soft-tissue surgery, exploration of aging or pathology in the mechanics of facial expression, etc.).<br/><br/>To these ends, the project will create algorithms for the automated development of accurate patient-specific models of facial anatomy capable of representing realistic behavior of soft tissues, including the formation of facial expressions. The research aims at challenges which require coordinated efforts across various disciplines, including computer graphics, computer vision, biomechanics and craniofacial surgery. Novel computer vision methods will leverage information from 3D imaging (MRI/CT) to capture details of in-vivo human face deformations. The acquired data will serve as input to inverse finite element solvers, which will compute the unknown mechanical parameters of person-specific soft tissues, accounting for pre-strain and muscle activation units. This data-centric approach is a departure from established model-building methodologies, and has the potential to make a transformative impact on the anatomical modeling field. Furthermore, although the clinical application of orthognathic surgery is used as the motivation and key benchmark for the work, the algorithmic innovations produced in this activity transcend the specific scope of this task and deliver broader utility in the fields of visual computing and computational dynamics. Physics-based models of shape and deformation of elastic objects will be incorporated into visual acquisition systems as structural priors, enhancing the robustness and accuracy of the data collection.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763638","CHS: Medium: Collaborative Research: Inverse Anatomical Modeling of the Face for Orthognathic Surgery","IIS","HCC-Human-Centered Computing, Smart and Connected Health","08/01/2018","08/15/2019","Eftychios Sifakis","WI","University of Wisconsin-Madison","Standard Grant","Ephraim Glinert","07/31/2022","$319,957.00","","sifakis@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7367, 8018","7367, 7924, 8018","$0.00","The face is the center of an individual's sense of identity and self-esteem, and plays a crucial role in interpersonal relationships. The current state of the art approaches computational modeling of human faces from two distinct angles. Computer graphics models feature high visual realism, as seen in the movies.  Whereas biomechanics focuses on physical realism, modeling the face as a sophisticated mechanical system that obeys the laws of physics. This project will bridge the gap between these two viewpoints and construct models of the face that offer both visual and physical realism. This is of the utmost importance in applications such as surgical prediction. Close to five percent of the population of the United States has a dentofacial anomaly that may require jaw surgery, which can have a profound effect on the appearance of the face. Virtually every patient asks, ""How will I look after the treatment?"" Even though this is an important and well-studied problem, there are currently no methods capable of predicting post-operative changes in facial expressions. By combining both visual and physical realism, this research will create the first system that can provide a natural, 3D visual answer to the patient's question by displaying a photorealistic facial animation after a simulated surgical procedure. Additional broad impact will derive from project outcomes because the new numerical techniques for the efficient simulation of biomaterials will provide a reusable foundation that can be leveraged for computational modeling of a variety of engineering materials that exhibit pronounced heterogeneity and anisotropy. The anatomical modeling framework developed in this work will also serve as a launchpad for future inquiry of interest to medical science (modeling of soft-tissue surgery, exploration of aging or pathology in the mechanics of facial expression, etc.).<br/><br/>To these ends, the project will create algorithms for the automated development of accurate patient-specific models of facial anatomy capable of representing realistic behavior of soft tissues, including the formation of facial expressions. The research aims at challenges which require coordinated efforts across various disciplines, including computer graphics, computer vision, biomechanics and craniofacial surgery. Novel computer vision methods will leverage information from 3D imaging (MRI/CT) to capture details of in-vivo human face deformations. The acquired data will serve as input to inverse finite element solvers, which will compute the unknown mechanical parameters of person-specific soft tissues, accounting for pre-strain and muscle activation units. This data-centric approach is a departure from established model-building methodologies, and has the potential to make a transformative impact on the anatomical modeling field. Furthermore, although the clinical application of orthognathic surgery is used as the motivation and key benchmark for the work, the algorithmic innovations produced in this activity transcend the specific scope of this task and deliver broader utility in the fields of visual computing and computational dynamics. Physics-based models of shape and deformation of elastic objects will be incorporated into visual acquisition systems as structural priors, enhancing the robustness and accuracy of the data collection.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840813","EAGER: Invisible Shield: Can Compression Harden Deep Neural Networks Universally Against Adversarial Attacks?","CNS","Secure &Trustworthy Cyberspace","09/01/2018","07/20/2018","Wujie Wen","FL","Florida International University","Standard Grant","Sandip Kundu","01/31/2020","$250,000.00","","wuw219@lehigh.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","8060","025Z, 7434, 7916","$0.00","Deep neural networks (DNNs) are finding applications in wide-ranging applications such as image recognition, medical diagnosis and self-driving cars. However, DNNs suffer from a security threat: decisions can be misled by adversarial inputs crafted by adding human-imperceptible perturbations into normal inputs during training of DNN model. Defending against adversarial attacks is challenging due to multiple attack vectors, unknown adversary's strategies and cost. This project investigates a compression/decompression-based defense strategy to protect DNNs against any attack, with low cost and high accuracy. <br/><br/>The project aims to create a new paradigm of safeguarding DNNs from a radically different perspective by using signal compression with a focus on integrating defenses into compression of the inputs and DNN models. The research tasks include: (i) developing defensive compression for visual/audio inputs to maximize defense efficiency without compromising testing accuracy; (ii) developing defensive model compression, and novel gradient masking/obfuscating methods without involving retraining, to universally harden DNN models; and (iii) conducting attack-defense evaluations through algorithm-level simulation and live platform experimentation.<br/><br/>Any success from this EAGER project will be useful to research community interested in deep learning, hardware- and cyber- security, and multimedia. This project enhances economic opportunities by promoting wider applications of deep learning into realistic systems, and gives special attention to educating women and students from traditionally under-represented/under-served groups in Florida International University (FIU).<br/><br/>The project repository will be stored on a publicly accessible server at FIU (http://web.eng.fiu.edu/wwen/).  Data will be maintained for at least 5 years after the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840352","Planning Grant: Engineering Research Center for Edge Intelligence","EEC","ERC-Eng Research Centers","09/01/2018","08/29/2018","Marilyn Wolf","GA","Georgia Tech Research Corporation","Standard Grant","Sarit Bhaduri","08/31/2019","$100,000.00","Shuvra Shikhar Bhattacharyya, Nagi Gebraeel, Cathy Bodine","mwolf@unl.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1480","112E, 132E","$0.00","The Planning Grants for Engineering Research Centers (ERC) competition was run as a pilot solicitation within the ERC program.  Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>This grant will provide planning resources for a proposed Center for Edge Intelligence. The proposed center will develop new techniques for data science driven by Internet-of-Things (IoT) systems. IoT systems interact with the real world: industry, health care, agriculture, etc. IoT systems promise substantial benefits for these and other application areas by gathering and processing data from the real world. However, new approaches are required to harness the potential of IoT systems. IoT devices must operate with limits on both electrical power and communication. These factors mean that key aspects of these intelligent systems must be embodied outside of large data centers, at the edge of the Internet, close to the points at which data are created and used. This planning grant has three goals.  The planning grant effort will identify a full center team including member institution researchers, diversity and inclusion coordinators, workforce development coordinators, and entrepreneurship coordinators. The proposing team will identify a broad set of stakeholders among researchers, industrial partners, entrepreneurs, and educators. The full center team and stakeholders will create a strategic plan for the Center for Edge Intelligence.<br/><br/>Edge Intelligence is the convergence of three fields: data science, low-power computing, and distributed systems. Data science and IoT systems are typically studied, developed, and deployed by very different groups. The Center for Edge Intelligence's mission will be to build a community of stakeholders across industry and academia to develop and harness the capabilities of this emerging technology. The intellectual approach of the Center will be based on several foundations: machine learning, statistics, low-power computing, distributed algorithms. Internet-of-Things systems are widely used in manufacturing, health care, logistics, agriculture, and many other areas. Machine learning applied to IoT systems can provide new levels of customization and improved system operation. The Center for Edge Intelligence will pursue a convergent, holistic research approach in foundations, engineering, and applications. Foundational goals include machine learning algorithms designed for distributed computing systems with limited bandwidth and power. Incremental learning algorithms to customize and update systems for their particular environment. Engineering goals include distributed systems that perform machine learning on distributed platforms, including edge nodes, fog hubs, and the cloud. Low-power machine learning systems. Low-power approaches to edge- and hub-based incremental training. Applications include manufacturing, health care and wellness, agriculture, transportation and logistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826099","Research: A Mixed-Methods Approach to Characterizing Engineering Students' Computational Habits of Mind","EEC","EngEd-Engineering Education","09/15/2018","08/30/2018","Mireille Boutin","IN","Purdue University","Standard Grant","Edward Berger","08/31/2021","$350,000.00","Alejandra Magana-de-Leon, Kerrie Douglas","mboutin@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","1340","110E, 1340","$0.00","Finding solutions to complex problems in our society requires engineering graduates who not only possess the technical knowledge and skill set of a discipline, but also a professional mindset.  ""Habits of mind"" are modes of thinking required to become effective problem solvers capable of transferring such skills to new contexts. Such skills form an essential part of engineer's professional mindset. This project studies undergraduate engineering students' habits of mind related to computing. The novelty of the proposed approach is that the data collection and analysis methods integrate machine learning with methods traditionally used in education to analyze data.<br/> <br/>The guiding education research question of the project is: What computational habits of mind do engineering students exhibit at different points of their academic development? The goal of the effort is to provide a better understanding of how these patterns relate to academic achievement, which in turn informs learning pathways toward computational proficiency in engineering. The project follows an Explanatory Sequential Mixed Method Design to study engineering students' computational habits of mind by integrating machine learning in a mixed method research. High-dimensional quantitative data is collected for each learner and machine learning is used to find patterns corresponding to groups of similar learners. By dividing the learners into similarity groups, these patterns provide a higher detail of understanding than global analysis methods. The reasons behind the patterns will be identified by collecting and analyzing qualitative data. Building on the machine learning results, an automatic subject selection strategy is used to reduce the number of learners considered to a manageable size for the qualitative part of the research. The goal of this learner reduction strategy is to obtain a small but statistically meaningful sampling for each group of subjects in order to focus resources for the most costly and time consuming part of the research. By doing so, mixed method research is shown to be scalable to large and complex datasets. <br/><br/>The rationale for this project is that its successful completion: a) addresses challenges in cultivating a culture of lifelong learning among professional and future engineers; b) provides an understanding of how patterns of habits of mind relate to academic achievement, which in turn informs learning pathways toward computational proficiency in engineering; and (c) demonstrates how mixed method research can be scaled up to study very large and complex multidimensional datasets through the use of machine learning. This effort supports the need for the foundation in engineering students' education to incorporate necessary skills such as creativity, ingenuity, professionalism, persistence, and willingness to take calculated risks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760922","Collaborative research: Fostering conceptual understanding and skill with an intelligent tutoring system for equation solving","DRL","ECR-EHR Core Research","06/01/2018","03/16/2020","Vincent Aleven","PA","Carnegie-Mellon University","Standard Grant","Amy Baylor","05/31/2021","$1,023,223.00","","vincent.aleven@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","EHR","7980","9251","$0.00","This Education and Human Resources Core Research project addresses a persistent challenge in designing effective mathematics instruction: how to foster deep learning of concepts and skills, with strong connections among them. This challenge arises especially in middle school and high school algebra; far too often, students in algebra courses come away with only a moderate level of skill at procedures (e.g., equation solving) with almost no conceptual understanding of why these procedures work. This lack of deep understanding is unfortunate, because algebra is an important stepping stone to advanced mathematics as well as to future educational and employment opportunities. Thus, there is a great need for algebra instruction that helps students acquire well-integrated knowledge of key concepts and problem-solving procedures. This project tackles this issue using, as a platform, a software-based intelligent tutoring system that will extend an existing tutoring system for algebra. The new, enhanced tutoring system will support both practice in solving equations and new conceptually-oriented algebra activities. It will provide detailed and targeted guidance that adapts to individual students' errors, strategies, and developing algebra knowledge. Lab-based and classroom studies will investigate three key questions: (1) What mix of conceptual and procedural activities best helps students acquire deep skill and conceptual knowledge of algebra? (2) How frequently should students shift back and forth between the two types of activities? (3) How effective is the new tutoring system compared to commonly-used software, namely, Khan Academy, or a standard intelligent tutoring system that provides procedural practice only? The project will lead to new knowledge about how to create effective instruction for middle school and high school algebra, together with new intelligent tutoring software that embodies this knowledge.<br/><br/>Based on past research, it is commonly accepted that in many domains, gains in conceptual knowledge can lead to gains in procedural knowledge, and vice versa. Further, there is some evidence that instruction that shifts back and forth between conceptually-oriented activities and procedurally-oriented activities is especially effective. However, it is not yet known what sorts of activities best support the development of conceptual knowledge, or how they should be integrated with activities that target learning of procedures so that students might make connections between the two. To address these issues, the project will extend existing tutoring system for equation solving (called Lynnette) with conceptual activities, based on past research that suggests that worked examples, self-explanation, visual representations (namely, bar diagrams), and activation of prior knowledge can be effective for promoting gains in conceptual knowledge. In doing so, this will leverage an established technical infrastructure for intelligent tutoring systems research and development, called CTAT/Tutorshop. The project will conduct two lab studies and two classroom studies to investigate what mix of conceptually-oriented activities is the most effective complement to the system's current set of procedurally-oriented activities, how frequently students should shift back and forth between procedurally- and conceptually-oriented activities, and how effective the resulting tutoring system is, compared to two control conditions in which students work with commonly used types of software for algebra learning (Khan Academy and Lynnette). The project will generate new knowledge about how to design effective instruction that helps students acquire well-integrated conceptual and procedural knowledge that is effective in real educational settings. This knowledge could lead to better, more conceptually-oriented instruction in algebra, and it has the potential to improve instruction in other STEM learning environments. The project will also create a new intelligent tutoring system for algebra that is much more effective than current systems. The system will be made available to teachers and schools free of charge.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760947","Collaborative Research: Fostering conceptual understanding and skill with an intelligent tutoring system for equation solving","DRL","ECR-EHR Core Research","06/01/2018","05/25/2018","Martha Alibali","WI","University of Wisconsin-Madison","Standard Grant","Amy Baylor","05/31/2021","$524,049.00","","mwalibali@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","EHR","7980","","$0.00","This Education and Human Resources Core Research project addresses a persistent challenge in designing effective mathematics instruction: how to foster deep learning of concepts and skills, with strong connections among them. This challenge arises especially in middle school and high school algebra; far too often, students in algebra courses come away with only a moderate level of skill at procedures (e.g., equation solving) with almost no conceptual understanding of why these procedures work. This lack of deep understanding is unfortunate, because algebra is an important stepping stone to advanced mathematics as well as to future educational and employment opportunities. Thus, there is a great need for algebra instruction that helps students acquire well-integrated knowledge of key concepts and problem-solving procedures. This project tackles this issue using, as a platform, a software-based intelligent tutoring system that will extend an existing tutoring system for algebra. The new, enhanced tutoring system will support both practice in solving equations and new conceptually-oriented algebra activities. It will provide detailed and targeted guidance that adapts to individual students' errors, strategies, and developing algebra knowledge. Lab-based and classroom studies will investigate three key questions: (1) What mix of conceptual and procedural activities best helps students acquire deep skill and conceptual knowledge of algebra? (2) How frequently should students shift back and forth between the two types of activities? (3) How effective is the new tutoring system compared to commonly-used software, namely, Khan Academy, or a standard intelligent tutoring system that provides procedural practice only? The project will lead to new knowledge about how to create effective instruction for middle school and high school algebra, together with new intelligent tutoring software that embodies this knowledge.<br/><br/>Based on past research, it is commonly accepted that in many domains, gains in conceptual knowledge can lead to gains in procedural knowledge, and vice versa. Further, there is some evidence that instruction that shifts back and forth between conceptually-oriented activities and procedurally-oriented activities is especially effective. However, it is not yet known what sorts of activities best support the development of conceptual knowledge, or how they should be integrated with activities that target learning of procedures so that students might make connections between the two. To address these issues, the project will extend existing tutoring system for equation solving (called Lynnette) with conceptual activities, based on past research that suggests that worked examples, self-explanation, visual representations (namely, bar diagrams), and activation of prior knowledge can be effective for promoting gains in conceptual knowledge. In doing so, this will leverage an established technical infrastructure for intelligent tutoring systems research and development, called CTAT/Tutorshop. The project will conduct two lab studies and two classroom studies to investigate what mix of conceptually-oriented activities is the most effective complement to the system's current set of procedurally-oriented activities, how frequently students should shift back and forth between procedurally- and conceptually-oriented activities, and how effective the resulting tutoring system is, compared to two control conditions in which students work with commonly used types of software for algebra learning (Khan Academy and Lynnette). The project will generate new knowledge about how to design effective instruction that helps students acquire well-integrated conceptual and procedural knowledge that is effective in real educational settings. This knowledge could lead to better, more conceptually-oriented instruction in algebra, and it has the potential to improve instruction in other STEM learning environments. The project will also create a new intelligent tutoring system for algebra that is much more effective than current systems. The system will be made available to teachers and schools free of charge.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818977","Uncertainty Quantification for Machine Learning","DMS","COMPUTATIONAL MATHEMATICS","06/15/2018","06/07/2018","Andrew Stuart","CA","California Institute of Technology","Standard Grant","Leland Jameson","05/31/2021","$250,000.00","","astuart@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","MPS","1271","9263","$0.00","Computers are increasingly making decisions that impact on each and every one of us, in scenarios as wide-ranging as deciding credit card limits, flying aircraft, predicting weather and recommending products via the internet. The algorithms which perform these tasks are complex and their intricacies cannot be fully appreciated by all the people who rely on them. The goal of this project is to equip these decision-making algorithms with measures of uncertainty so that, where appropriate, human or further computer intervention can be used to ensure fair and safe outcomes. The junior researchers involved in the program will also engage in outreach programs, organized through Cal Tech. These outreach programs are aimed at high school students, and designed in particular to impact on a diverse range of high school students; this outreach work will be enhanced by experience with the research about equipping everyday algorithms with measures of uncertainty.<br/><br/>The purpose of this project is to obtain a deeper understanding of machine learning algorithms. This will be achieved by formulating and solving the problems in a statistical fashion in which uncertainty in both the mathematical models used for learning, and the data used to train them, is tracked and quantified. The objectives are twofold: (i) to improve existing algorithms by allowing them to be cognizant of their own uncertainties, or by allowing humans to interact with them in an informed fashion; (ii) to use knowledge of uncertainties to study the predictive power of the algorithms and identify laws or rules implicitly encoded within them. A Bayesian formulation of a number of machine learning tasks will be adopted, with focus on neural networks, and related issues arising in graph-based semi-supervised learning. Recent advances in the development of Monte Carlo Markov chain (MCMC) samplers in high dimensions will be deployed to make empirical studies of uncertainty. Various parameter limits (including large data volume, data in high dimensional spaces, and small data noise) will be used to develop mathematical theories which quantify uncertainty in the predictions made by machine learning algorithms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839332","TRIPODS+X:RES:Collaborative Research: Learning with Expert-In-The-Loop for Multimodal Weakly Labeled Data and an Application to Massive Scale Medical Imaging","DMS","TRIPODS Transdisciplinary Rese, IIS Special Projects","10/01/2018","09/10/2018","Nematollah batmanghelich","PA","University of Pittsburgh","Standard Grant","A. Funda Ergun","09/30/2021","$300,000.00","","kayhan@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","MPS","041Y, 7484","047Z, 062Z","$0.00","The methods developed in this project address pressing problems at the interface of machine learning and a life-sciences application. This work hopes to spur a rich variety of followup research that not only builds on the mathematical tools developed but also on the insights gained from the project's focus on interpretable and interactive machine learning. As a key application, implementation of the technology obtained from this research on medical images will enable automatic report generation. This technology could reduce the workload of radiologists significantly, and hence reduce human error and optimize resource utilization. Ultimately, reduction in human error or missing cases in radiology can greatly benefit patient well-being and care. More broadly, the work will enhance the ongoing adoption of data-driven thinking in healthcare and thereby help accelerate new discoveries. This project also impacts education, and involves intellectual and professional development of students at a variety of academic stages. <br/><br/>Most applications of machine learning to medical imaging (and other human-centric tasks) focus on supervised learning, which demands a large amount of expensive labeled-data. This limitation recurs throughout applications, while real-world use of machine learning demands robustness as well as an ability to work with limited supervision. This project focuses on developing new machine learning tools for working with weak-supervision, while advancing state-of-the-art in interpretable and interactive learning. Moreover, attention to large-scale data and incorporation of domain knowledge is paid, whenever feasible. The project shall also apply the theoretical advances that it will make to a real-world medical imaging application. Theoretical advances of the proposed work will rely on tools from geometry, especially metric learning (including over infinite dimensional spaces), mathematical models motivated by optimal transport theory, as well as nonlinear representations based on neural networks as well as kernel methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841351","EAGER: A Framework for Learning Graph Algorithms with Applications to Social and Gene Networks","IIS","Info Integration & Informatics","09/01/2018","07/30/2018","Le Song","GA","Georgia Tech Research Corporation","Standard Grant","Amarda Shehu","08/31/2021","$300,000.00","Srinivas Aluru","lsong@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364","7364, 7916","$0.00","Many real world applications, such as discovering gene interaction networks, detecting fraud in financial networks and personalizing recommendations in social networks, involve NP-hard graph problems.  Typically, approximation or heuristic algorithms designed for these problems rely heavily on manually specified structural information of graphs. Furthermore, previous  graph algorithms  seldom systematically  exploit a  common  trait of  industrial graph  problems:  instances of the same type of problem need to be solved repeatedly on a regular basis, and algorithms which are effective on average are more preferable than those with only a worst case guarantee.  This project explores a novel deep learning framework for automating the design of algorithms for challenging graph problems.  The framework delegates difficult choices during the design to deep learning models, and uses a distribution of problem instances to train effective graph algorithms. The project presents a paradigm shift in graph algorithm design,  and results in a software package to disseminate the research.   The project also involves a broader swath of students including undergraduates and underrepresented minorities through multiple existing summer research internship programs that target students nationwide.<br/><br/>More specifically, the framework casts a graph algorithm as a composition of many small learnable operators either because it works on graph inputs, produces structured outputs, or the computation graph of the algorithm itself contains structures such as branches and recursions. Instead of specifying each operator manually as in traditional algorithm design, the framework parameterizes these operators using nonlinear embeddings, and learns them jointly from graph input and output pairs using supervised learning or reinforcement learning. Though demonstrated in specific gene and social networks, the framework is generic and broadly applicable to a large class of graph analysis problems appearing in a diverse range of real world applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839684","EAGER: Real-Time: Intelligent Mitigation of Low-Frequency Oscillations in Smart Grid Using Real-time Learning","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2018","09/19/2018","Yilu Liu","TN","University of Tennessee Knoxville","Standard Grant","Anil Pahwa","08/31/2021","$275,762.00","Lin Zhu","liu@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","ENG","7607","155E, 1653, 7916","$0.00","As a critical underpinning of modern society, the electric power grid is one of the most complex and man-made dynamic systems in the world. Numerous real-time data of different types, different components, and various locations are generated to monitor and control power grids. Currently, the control of large-scale power grids is still mainly based on the physical system model, while the hidden knowledge in the abovementioned large-volume data has not been fully exploited. This project selects one typical control function in smart grids, low-frequency oscillation control, to explore the potential to enhance smart grid controls using the hidden knowledge. <br/>Low-frequency oscillation is a common phenomenon in operation of large-scale power systems. If not controlled properly, these oscillations may degrade power system security and make a large number of customers lose their power. This project aims at developing an intelligent controller to mitigate these low-frequency oscillations using data and machine learning technologies. If successful, it will advance the technology in smart grid, and remove obstacles for application of machine learning technologies in smart grid control. The proposed approach will contribute to more secure, reliable and economic operations of U.S. power grids. For example, the risk of blackout can be significantly mitigated; and thus outage cost could be saved, e.g., more than $1 billion for U.S. western grid collapse in 1996. The proposed project is also coupled with a broad dissemination of research findings and a strong educational component to engage students from underrepresented groups. <br/><br/>The proposed research effort focuses on a completely new design methodology of intelligent oscillation damping control using the data-driven models. These data-driven models of power grids derive from synchronized measurement data using machine learning technologies, in conjunction with power grid domain knowledge. Specifically, this project will: (1) build a self-evolving dynamic knowledge base based on historical measurement data under different oscillation scenarios; (2) extract the critical features from historical and real-time data, and select the optimal features to improve data-driven model prediction accuracy; (3) develop machine learning algorithms to predict data-driven models for oscillation damping control design; and (4) validate and demonstrate the proposed methodology via computer simulations and hardware testbed experiments. This advanced approach will contribute to the energy security and efficiency of the U.S. electric power grids. This project will expose both undergraduate and graduate students to the state-of-the-art machine learning education and workforce training program. By coordinating with an established outreach program in an existing NSF/DOE engineering research center, the research results will be integrated into weekly seminars and short courses that are accessible to four partner universities, nine affiliate universities and more than 35 industry partners. Moreover, this project will encourage students to get involved with STEM (Science, Technology, Engineering and Mathematics) courses early in their pre-college years to prepare for STEM careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839258","TRIPODS+X:RES:Collaborative Research: Learning with Expert-In-The-Loop for Multimodal Weakly Labeled Data and an Application to Massive Scale Medical Imaging","DMS","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, IIS Special Projects","10/01/2018","09/10/2018","Suvrit Sra","MA","Massachusetts Institute of Technology","Standard Grant","A. Funda Ergun","09/30/2021","$300,000.00","","suvrit@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","041Y, 1253, 7484","047Z, 062Z","$0.00","The methods developed in this project address pressing problems at the interface of machine learning and a life-sciences application. This work hopes to spur a rich variety of followup research that not only builds on the mathematical tools developed but also on the insights gained from the project's focus on interpretable and interactive machine learning. As a key application, implementation of the technology obtained from this research on medical images will enable automatic report generation. This technology could reduce the workload of radiologists significantly, and hence reduce human error and optimize resource utilization. Ultimately, reduction in human error or missing cases in radiology can greatly benefit patient well-being and care. More broadly, the work will enhance the ongoing adoption of data-driven thinking in healthcare and thereby help accelerate new discoveries. This project also impacts education, and involves intellectual and professional development of students at a variety of academic stages. <br/><br/>Most applications of machine learning to medical imaging (and other human-centric tasks) focus on supervised learning, which demands a large amount of expensive labeled-data. This limitation recurs throughout applications, while real-world use of machine learning demands robustness as well as an ability to work with limited supervision. This project focuses on developing new machine learning tools for working with weak-supervision, while advancing state-of-the-art in interpretable and interactive learning. Moreover, attention to large-scale data and incorporation of domain knowledge is paid, whenever feasible. The project shall also apply the theoretical advances that it will make to a real-world medical imaging application. Theoretical advances of the proposed work will rely on tools from geometry, especially metric learning (including over infinite dimensional spaces), mathematical models motivated by optimal transport theory, as well as nonlinear representations based on neural networks as well as kernel methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755785","CRII: RI: Representation Learning and Adaptation using Unlabeled Videos","IIS","Robust Intelligence","06/01/2018","04/13/2018","Jia-Bin Huang","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jie Yang","05/31/2021","$172,903.00","","jbhuang@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7495","7495, 8228","$0.00","Recent success in visual recognition relies on training deep neural networks (DNNs) on a large-scale annotated image classification dataset in a fully supervised fashion. The learned representation encoded in the parameters of DNNs have shown remarkable transferability to a wide range of tasks. However, the dependency on supervised learning substantially limits the scalability to new problem domains because manual labeling is often expensive and in some cases requires expertise. In contrast, a massive amount of free unlabeled images and videos are readily available on the Internet. This project develops algorithms to capitalize on large amounts of unlabeled videos for representation learning and adaptation. The developed methods significantly alleviate the high cost and scarcity of manual annotations for constructing large-scale datasets. The project involves both graduate and undergraduate students in the research. The research materials are also integrated to curriculum development in courses on deep learning for machine perception. Results will be disseminated through scientific publications, open-source software, and dataset releases.<br/><br/>This research tackles two key problems in representation learning. In the first research aim, the project simultaneously leverages spatial and temporal contexts in videos to learn generalizable representation. The research takes advantages of rich supervisory signals for representation learning from appearance variations and temporal coherence in videos. Compared to the supervised counterpart (which requires millions of manually labeled images), learning from unlabeled videos is inexpensive and is not limited in scope. The project also seeks to adapt the learned representation to handle appearance variations in new domains with minimal manual supervision. The effectiveness of representation adaptation is validated in the context of instance-level video object segmentation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1904007","Cortical Motion Coding and Gaze Control in Natural Vision","IOS","PHYSICS OF LIVING SYSTEMS, Activation","01/01/2018","08/22/2019","Leslie Osborne","NC","Duke University","Continuing Grant","Sridhar Raghavachari","08/31/2020","$652,023.00","","leslie.osborne@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","BIO","7246, 7713","8091, 9178, 9179","$0.00","The human eye sends information to the brain at an estimated rate of approximately 10 megabits per second, roughly the speed of an ethernet connection. Processing such a large bandwidth stream of visual information on behaviorally relevant time scales requires the brain to extract and represent information from visual signals efficiently, i.e. represent the most information for the least cost in time, hardware and energy. In essence, the brain needs to compress the visual stream in much the same way that software compresses the digital representation of a movie. This coding enhancement might arise because the brain has evolved coding strategies that specifically account for the fact that because of both object and eye movements, the visual input to the eye may be correlated in space and time. As a result, the visual signals to the brain from the eye and retina may be quite predictable. One of the primary questions in current sensory-motor systems research is to what extent the brain utilizes prediction to compensate for the fact that it takes a finite amount of time to process information even though the visual scene might change in the interim. This proposal focuses on neural representation of visual motion and gaze behavior for natural motion videos and uses a novel video game environment to simplify the analysis of gaze. The project will also create a publicly available database of natural gaze recordings, analyze the statistics of natural retinal image motion, characterize the representation of naturally correlated motion stimuli in cortical neurons, and to articulate the strategy underlying gaze control. This database will benefit neuroscience, computer vision, media design, and other fields.<br/><br/>The experimental approach combines cortical physiology in non-human primates with high-resolution eye movement recording in both humans and monkeys. The PI proposes to use high-resolution videos of natural moving scenes as visual stimuli while recording neural activity in motion-sensitive visual cortex.  By carefully degrading the movies to make them increasingly less natural and measuring the impact on neural responses, the experiments will determine what features of the moving visual scene are represented most precisely.  A second set of experiments will study the interactions between the visual scene and eye movements. The PI will develop an innovative Pong-like video game that actively engages the viewers and creates a common viewing purpose (scoring points) while simplifying the identification of the target of interest to aid analysis, thereby controlling the cognitive state of the viewer. The interdisciplinary nature of the work will provide training opportunities for undergraduate and graduate students crossing over from mathematics and physics to neurobiology, and for students with a biology background to gain skills in computational analysis."
"1823148","CRI:CI:SUSTAIN: Next-Generation, Sustainable Infrastructure for the RF-Powered Computing Community","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2018","07/17/2018","Joshua Smith","WA","University of Washington","Standard Grant","Alexander Sprintson","09/30/2021","$979,997.00","Shyamnath Gollakota","jrs@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7359","7359","$0.00","The energy efficiency of microelectronics has been improving exponentially for decades. It is becoming possible to operate low power sensing, computing and communication platforms in a perpetual, battery-free fashion, with all power provided by Radio Frequency (RF) signals or other energy harvesting.  The Wireless Identification and Sensing Platform (WISP) is an open source battery-free platform that the present investigators originally introduced in 2006.  Hundreds of WISPs have been manufactured and distributed to researchers around the world.  This infrastructure has enabled research in diverse areas of computer science, including networking, Human-Computer Interaction, Ubiquitous Computing, Robotics, and other areas.  The present proposal will allow the researchers to integrate the latest research results, such as Ambient Backscatter Communication, into the WISP family, and also to reap the benefits of the most recent improvements in low power microelectronics.  The proposal will allow us to produce a new generation of the infrastructure and mature it to the point that it becomes self sustaining, via sales of hardware or other means. We expect that the sustained infrastructure will support research in backscatter communication, low power systems and networking, and applications of ultra-low-power platforms.  Battery-free sensing systems are expected to enable a wide array of new capabilities, which will generate substantial commercial impact in a wide variety of markets.<br/><br/>Computing is becoming connected more and more deeply to the physical world, a transformation that can enable smart environments, better medical care, more efficient manufacturing, and more. However, the need to power physically embedded microelectronic systems is a key challenge. This project will allow us to sustain the WISP infrastructure for battery-free, RF-powered computing and communication.  The infrastructure will enable research in several areas. In recent years, the PI and co-PI introduced Ambient Backscatter Communication, and backscatter-based WISP cameras, which have been widely recognized in the research community. Making these tools widely available will enable research on topics such as (ambient) backscatter networking, applications of battery-free cameras, and algorithms for interactive compression and computer vision in battery-free camera systems. This research would likely remain inaccessible for a long time to many computer and information science and engineering researchers, since there are no widely accessible platforms that support research on these topics. The sustained infrastructure will also enable novel application research, in areas such as improved human activity detection systems, battery-free input devices, and also research on body-implanted electronics, and long term structural health monitoring.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804222","SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems","CNS","Secure &Trustworthy Cyberspace","10/01/2018","07/27/2020","Dan Boneh","CA","Stanford University","Continuing Grant","Phillip Regalia","09/30/2023","$1,163,624.00","Percy Liang","dabo@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","8060","025Z, 7434, 8087","$0.00","This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.<br/><br/>The center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804603","SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","10/01/2018","07/24/2020","David Evans","VA","University of Virginia Main Campus","Continuing Grant","Phillip Regalia","09/30/2023","$561,127.00","","evans@cs.virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","1714, 8060","025Z, 7434, 8087, 9178, 9251","$0.00","This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.<br/><br/>The center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808051","Combining Optimization, Machine Learning, and Model Structure to Improve the Robustness and Agility of Modern Bipedal Machines","ECCS","EPCN-Energy-Power-Ctrl-Netwrks, NRI-National Robotics Initiati","08/15/2018","08/15/2018","Jessy Grizzle","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Radhakisan Baheti","07/31/2021","$400,000.00","","grizzle@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","7607, 8013","092E, 8013","$0.00","Bipedal robots are being built to aid in search and rescue, provide last mile delivery of packages, and to assist people in their homes. Lower-limb exoskeletons are being designed to help patients recover the ability to walk after strokes or even severe injuries resulting in paralysis. While the feedback control algorithms required to allow a bipedal robot to walk and a patient to safely operate a lower-limb exoskeleton are not identical, they share enough common elements that pursing their investigation in tandem is insightful and important. This project combines recent advances in the ability to quickly compute energy optimal solutions of bipedal dynamical systems with the mathematics of machine learning and geometric control theory to achieve unprecedented performance and safety in bipedal walking. The proposed research will greatly expand the class of robots for which feedback controllers can be designed with provable stability and it will significantly enhance the safety than can be achieved with exoskeletons that allow a paraplegic to walk without the use of crutches. One of the many technical challenges to be overcome in this research is the complexity of the mathematical models that describe the movement these legged machines. For example, printing out the symbolic model for the exoskeleton studied here would take thousands of pages. If a human ever opened the files to examine them, they would be incomprehensible. Yet, the PI and his students provide concrete means for designing feedback controllers for these machines and say deep things about how the closed-loop system will behave. This is the beauty of feedback control theory when it is married with modern computational tools. In addition, each year, the PI and his students share the excitement of engineering by giving tours of his robotics lab to hundreds of students, from grade school through high school, sharing the excitement and personal fulfillment of careers in STEM fields. Presidents of major universities and management teams of corporations visit his lab for the pure pleasure of seeing a robot doing something amazing and yet at the same time, almost ordinary: walking roughly like a human. The PI works with the media to share with the general public the excitement of cutting-edge engineering research and how it benefits society. <br/><br/>The project seeks major advances in the theoretical conception and practical synthesis of feedback controllers for bipedal robots and lower-limb exoskeletons. The theory will be carefully tested on a Cassie-series bipedal robot and an exoskeleton. The theoretical thrust of the proposal aims to mitigate obstructions imposed by high-dimensional bipedal models (dimension 30 or more), without resorting to simplified pendulum models that are all too common in the robotics literature. The research seeks to work directly with the full model of the robot, making it possible to generate motions that exploit its full capabilities while respecting actuator limitations, ground contact forces, and terrain variability.  The process begins with trajectory optimization to design an open-loop periodic walking motion of the high-dimensional model, and then adding to this solution, a carefully selected set of additional open-loop trajectories of the model that steer toward the nominal motion. Supervised Machine Learning is used to extract from the open-loop behavior (i.e., the collection of input and state trajectories) a low-dimensional state-variable realization (i.e., a low-dimensional manifold and associated vector field). The special structure of mechanical models of bipedal robots is used to embed the low-dimensional model in the original model in such a manner that it is both invariant and locally exponentially attractive, and show that this locally exponentially stabilizes the desired walking motion in the full state space of the robot. Transitions among periodic orbits will also be addressed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760353","FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/2018","08/02/2018","Petros Drineas","IN","Purdue University","Standard Grant","Leland Jameson","07/31/2021","$343,223.00","","pdrineas@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1269, 1271","1616, 9263","$0.00","A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.<br/><br/>The proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839804","EAGER: Real-Time: Collaborative Research: Unified Theory of Model-based and Data-driven Real-time Optimization and Control for Uncertain Networked Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks, EFRI Research Projects","09/15/2018","09/20/2018","Frank Lewis","TX","University of Texas at Arlington","Standard Grant","Anthony Kuh","08/31/2021","$220,010.00","Yan Wan, Ali Davoudi","lewis@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","ENG","7607, 7633","155E, 1653, 7916","$0.00","The project seeks to find a common decision-making framework that seamlessly integrates offline data and computing, real-time data and computing, learning, and probabilistic predictive decision. It provides a unified theory of model-based and data-driven real-time optimization and control for uncertain networked systems. Integral Reinforcement Learning holds the key to integrating real-time data-driven methods, model-based methods, and physical constraints. The structure of Integral Reinforcement Learning will be explored to investigate exactly how and where to use Deep Learning neural networks in architectures that have multiple nested learning loops. A probabilistic spatiotemporal scenario data-driven framework will then be developed for multi-scale sequential control of networked engineering systems under uncertainty. The algorithms and tools developed will be used to sculpt optimal power profiles for power electronics converters in a DC distribution network and help mitigate the adverse effects of intermittent sources, uncertain load demand, or faults.<br/> <br/>The project represents a radical departure from the exiting big data and decision-making research, toward developing autonomous decision-making under uncertainty constructs for systems of growing scales and time critical mission requirements.  Algorithms and tools developed can be extended to other smart and connected domains, e.g., air traffic management, networked traffic platoons, and sensor networks. US microgrid capacity is expected to reach 4.3 GW by 2020. DC distribution networks are emerging alternatives to AC distribution ones, and are critical to the scalable integration of renewable energy resources and electrified transportation fleets. Research results will be ported into topics in reinforcement learning, optimal control, networked control systems, data-driven analysis and decision-making, and power electronics systems. This project synergizes research activities between University of Texas at Arlington (UTA) and Texas A&M-Corpus Christi (TAMUCC), both HBCU/MI Hispanic Serving Institutions, and involves students from Electrical Engineering and Computer Science backgrounds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839707","EAGER: Real-Time: Collaborative Research: Unified Theory of Model-based and Data-driven Real-time Optimization and Control for Uncertain Networked Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2018","09/20/2018","Junfei Xie","TX","Texas A&M University Corpus Christi","Standard Grant","Anthony Kuh","10/31/2019","$79,963.00","","jxie4@sdsu.edu","6300 Ocean Drive, Unit 5844","Corpus Christi","TX","784125844","3618253882","ENG","7607","155E, 1653, 7916","$0.00","The project seeks to find a common decision-making framework that seamlessly integrates offline data and computing, real-time data and computing, learning, and probabilistic predictive decision. It provides a unified theory of model-based and data-driven real-time optimization and control for uncertain networked systems. Integral Reinforcement Learning holds the key to integrating real-time data-driven methods, model-based methods, and physical constraints. The structure of Integral Reinforcement Learning will be explored to investigate exactly how and where to use Deep Learning neural networks in architectures that have multiple nested learning loops. A probabilistic spatiotemporal scenario data-driven framework will then be developed for multi-scale sequential control of networked engineering systems under uncertainty. The algorithms and tools developed will be used to sculpt optimal power profiles for power electronics converters in a DC distribution network and help mitigate the adverse effects of intermittent sources, uncertain load demand, or faults. <br/><br/>The project represents a radical departure from the exiting big data and decision-making research, toward developing autonomous decision-making under uncertainty constructs for systems of growing scales and time critical mission requirements.  Algorithms and tools developed can be extended to other smart and connected domains, e.g., air traffic management, networked traffic platoons, and sensor networks. US microgrid capacity is expected to reach 4.3 GW by 2020. DC distribution networks are emerging alternatives to AC distribution ones, and are critical to the scalable integration of renewable energy resources and electrified transportation fleets. Research results will be ported into topics in reinforcement learning, optimal control, networked control systems, data-driven analysis and decision-making, and power electronics systems. This project synergizes research activities between University of Texas at Arlington (UTA) and Texas A&M-Corpus Christi (TAMUCC), both HBCU/MI Hispanic Serving Institutions, and involves students from Electrical Engineering and Computer Science backgrounds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844584","The Deep Learning for Multi-phase Organic Chemistry Conference; Irvine, California; September 27-28, 2018","AGS","Atmospheric Chemistry","09/01/2018","08/29/2018","Ann Marie Carlton","CA","University of California-Irvine","Standard Grant","Sylvia Edgerton","08/31/2019","$5,000.00","","agcarlto@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","GEO","1524","7556","$0.00","This workshop is bringing together experts from different disciplines, including computer scientists, computational chemists and atmospheric chemists to discuss protocols that may lead to development of an artificial organic chemist capable of accurately predicting complex environmental chemistry in non-ideal multiphase systems. The goal of the workshop is to identify strategies that capitalize on computational approaches to solve complex chemistry. <br/><br/>The participants of the workshop include 20-25 researchers, representing a cross-section across multiphase chemistry and computation, to identify critical open questions in computational chemistry for multiphase systems, as well as to formulate protocols for answering those questions. Topics to be included include discussions on the best combination of experiments involving theory, numerical simulations (e.g. modeling of statistical mechanics and density functional theory), and laboratory and field experimentation to test deep learning techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757533","REU Site: Research Experiences in Pervasive Computing for Smart Health, Safety, and Well-being","CNS","RSCH EXPER FOR UNDERGRAD SITES, Special Projects - CNS","03/01/2018","07/12/2019","Jamie Payton","PA","Temple University","Standard Grant","Harriet Taylor","02/28/2021","$348,575.00","Jie Wu","jamie.payton@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","1139, 1714","9250","$0.00","This funding renews a Research Experiences for Undergraduates (REU) Site at Temple University.  This REU Site will host a diverse group of students from universities across the nation to spend their summer working on research projects related to pervasive computing solutions for improving health, public safety, and well-being. Pervasive computing typically involves embedding computational capability, usually on microprocessors, into everyday objects to make them effectively communicate and perform useful tasks often with little interaction directly from the end user of the object or device. This project involves research into pervasive computing applications that have the potential to benefit the broader community at large.  The research is led by an experienced faculty team that plans to offer a balance of theory, applications, and practical skills as well as mentoring and professional development opportunities for the students.  The students will be part of a community of practice that designs, implements, and assesses research projects that can impact the lives of all citizens. The site will focus on recruiting under-represented minorities and women, particularly drawing on undergraduate institutions where students have limited research opportunities.  <br/><br/>The project has the potential to develop new algorithms and technologies that lead to an advanced understanding of the field of pervasive computing and its component areas of networking, computer vision, and data analytics.  The projects are led by faculty mentors with strong research credentials as well as significant experience in undergraduate mentoring. The projects will have access to state-of-the art facilities in the new Science Education and Research Center as well as the resources of the Center for Networked Computing. The focus of the projects revolves around designing pervasive computing systems that employ sensing, actuation, and computation embedded in everyday objects, landscapes, and wearable items to improve the health, safety, and well-being of the general public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1903420","Multiscale computational methods in kinetic theory and optimal transport","DMS","COMPUTATIONAL MATHEMATICS","10/12/2018","10/25/2018","Li Wang","MN","University of Minnesota-Twin Cities","Continuing Grant","Leland Jameson","06/30/2020","$83,789.00","","wangli1985@gmail.com","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1271","9263","$0.00","Kinetic equations with multiple scales arise in diverse applications such as rarefied gas dynamics, plasma physics, semiconductors, and biology; they often introduce severe numerical challenges due to the stiffness that comes from the small scales. Optimal transport plays a fundamental role in image registration, video restoration, urban transport, kinetic theory and many others. However, numerical methods for it have not reached their full capacity to meet the most demanding practical applications. This project aims at both advancing the multiscale computational methods - particularly the asymptotic preserving (AP) schemes - in new prospects for kinetic equations including multi-stage and fractional asymptotic limit, and developing fast parallelizable algorithms for optimal transport via advanced optimization technique. <br/><br/>Specifically, the following topics will be investigated in this project: (1) theoretically study the AP schemes for semiconductor Boltzmann equation with two-scale collisions at a deeper depth and generalize them to implicit/high order schemes and to capture the hierarchy of macroscopic models; (2) extend the AP scheme for kinetic equation with fractional diffusion limit to a broader scope including anisotropic scattering, degenerate collision, and Levy-Fokker-Planck interaction (applications to nonclassical photon transport in clouds will be addressed); (3) develop efficient algorithms for optimal transport problems and conduct convergence analysis and apply it to practical problems especially for human crowd dynamics in panic situations. With increasing interest in multiscale kinetic equations and optimal transport, the computational methods developed here will impact beyond the particular applications in this proposal. The dynamics of electron transport in semiconductor devices are one of the main concerns in physics and engineering; the developed methods from this proposal will be equally applicable in a broader context such as gas discharges and multi-group radiative transfer. Nonclassical transport that leads to a fractional diffusion has attracted much attention in plasma physics and economy; it has now been applied in climate science to model the photon transport in clouds as well as in criminology to model the hotspots in residential burglaries. Optimal transport has become a useful tool in image processing, urban transport, computer vision and etc; the development of fast parallelizable algorithms will substantially advance these areas and the application in modeling human crowds is crucial for better preparation of safe mass events."
"1907316","Fast and Robust Gaussian Process Inference for Bayesian Nonparametric Learning","DMS","STATISTICS","05/17/2018","04/17/2019","Yun Yang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","05/31/2021","$120,000.00","","yy84@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","1269","8083, 9263","$0.00","Advances in modern technology have empowered researchers to collect massive data to conduct inference and making predictions. With the abundance of available observations, traditional statistical methods under the parametric assumption that a model can be characterized by a pre-specified number of parameters become inadequate and less attractive. Bayesian nonparametric models are attractive in this context which allow the resolution level of the analysis to be determined in a data-driven manner, and provide automatic characterization of uncertainty. The goal of this project is to develop new theory, methodology and computational tools for Bayesian nonparametric inference via Gaussian process priors. <br/><br/>Given the availability of massive data, nonparametric inference offers an attractive framework for flexibly modeling the underlying structure and extracting useful information. For instance, such challenges occur in chemical physics, computational biology, computer vision, engineering, and meteorology. This project aims to lay down a solid methodological, algorithmic, and theoretical foundation for nonparametric inference based on Gaussian processes. In particular, Gaussian process-based approaches tend to be vulnerable to data contamination and have heavy computational costs. To alleviate the high-computational cost of Gaussian process inference procedures, the investigator puts forward two novel computational frameworks which differ at their respective approximating targets as being either the prior or the posterior. To enhance the robustness of Gaussian process inference against data contamination, the investigator proposes a novel class of Bayesian hierarchical models for incorporating this extra measurement error structure, leading to a class of robust Gaussian process inference procedures. The new theoretical development offers valuable insight to experiment-design practitioners into the impact of measurement errors upon prediction and estimation, and provides evidence on the deep connection between computational complexity and statistical learnability. These computational and theoretical frameworks also benefit other disciplines such as applied mathematics, computer science and finance where stochastic processes such as Gaussian processes are routinely used.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810831","Fast and Robust Gaussian Process Inference for Bayesian Nonparametric Learning","DMS","STATISTICS","06/01/2018","05/11/2018","Yun Yang","FL","Florida State University","Standard Grant","Nandini Kannan","05/31/2019","$120,000.00","","yy84@illinois.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","MPS","1269","8083, 9263","$0.00","Advances in modern technology have empowered researchers to collect massive data to conduct inference and making predictions. With the abundance of available observations, traditional statistical methods under the parametric assumption that a model can be characterized by a pre-specified number of parameters become inadequate and less attractive. Bayesian nonparametric models are attractive in this context which allow the resolution level of the analysis to be determined in a data-driven manner, and provide automatic characterization of uncertainty. The goal of this project is to develop new theory, methodology and computational tools for Bayesian nonparametric inference via Gaussian process priors. <br/><br/>Given the availability of massive data, nonparametric inference offers an attractive framework for flexibly modeling the underlying structure and extracting useful information. For instance, such challenges occur in chemical physics, computational biology, computer vision, engineering, and meteorology. This project aims to lay down a solid methodological, algorithmic, and theoretical foundation for nonparametric inference based on Gaussian processes. In particular, Gaussian process-based approaches tend to be vulnerable to data contamination and have heavy computational costs. To alleviate the high-computational cost of Gaussian process inference procedures, the investigator puts forward two novel computational frameworks which differ at their respective approximating targets as being either the prior or the posterior. To enhance the robustness of Gaussian process inference against data contamination, the investigator proposes a novel class of Bayesian hierarchical models for incorporating this extra measurement error structure, leading to a class of robust Gaussian process inference procedures. The new theoretical development offers valuable insight to experiment-design practitioners into the impact of measurement errors upon prediction and estimation, and provides evidence on the deep connection between computational complexity and statistical learnability. These computational and theoretical frameworks also benefit other disciplines such as applied mathematics, computer science and finance where stochastic processes such as Gaussian processes are routinely used.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1805310","SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","10/01/2018","07/23/2020","Patrick McDaniel","PA","Pennsylvania State Univ University Park","Continuing Grant","Phillip Regalia","09/30/2023","$2,922,421.00","","mcdaniel@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1714, 8060","025Z, 7434, 8087, 9178, 9251","$0.00","This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.<br/><br/>The center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760316","FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/2018","08/02/2018","Michael Mahoney","CA","University of California-Berkeley","Standard Grant","Leland Jameson","07/31/2021","$790,668.00","Ming Gu","mmahoney@icsi.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 1271","1616, 9263","$0.00","A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.<br/><br/>The proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821183","CDS&E: Collaborative Research: Scalable Nonparametric Learning for Massive Data with Statistical Guarantees","DMS","OFFICE OF MULTIDISCIPLINARY AC, CDS&E-MSS, CDS&E","08/01/2018","08/15/2018","Guang Cheng","IN","Purdue University","Standard Grant","Christopher Stark","07/31/2021","$190,000.00","","chengg@stat.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1253, 8069, 8084","026Z, 8083, 9263","$0.00","We now live in the era of data deluge. The sheer volume of the data to be processed, together with the growing complexity of statistical models and the increasingly distributed nature of the data sources, creates new challenges to modern statistics theory. Standard machine learning methods are no longer able to accommodate the computational requirements. They need to be re-designed or adapted, which calls for a new generation of design and theory of scalable learning algorithms for massive data. This project aims to provide a collection of state-of-the-art nonparametric learning tools for big data analysis, which can be directly used by scientists and practitioners and have beneficial impacts on various fields such as biomedicine, health-care, defense and security, and information technology. The deliverables of this project include easy-to-use software packages that will be thoroughly evaluated using a range of application examples. They will directly help scientists to explore and analyze complex data sets.<br/> <br/>Due to storage and computational bottlenecks, traditional statistical inferential procedures originally designed for a single machine are no longer applicable to modern large datasets. This project aims to design new scalable learning algorithms of wide-ranging nonparametric models for data that are distributed across a large number of multi-core computational nodes, or in a fashion of random sketching if only a single machine is available. The computational limits of these new algorithms will be examined from a statistical perspective. For example, in the divide-and-conquer setup, the number of deployed machines can be viewed as a simple proxy for computing cost. The project aims to establish a sharp upper bound for this number: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. Related questions will also be addressed in the randomized sketching method in terms of the minimal number of random projections.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804794","SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems","CNS","Secure &Trustworthy Cyberspace","10/01/2018","07/25/2020","Jacob Steinhardt","CA","University of California-Berkeley","Continuing Grant","Phillip Regalia","09/30/2023","$447,907.00","","jsteinhardt@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8060","025Z, 7434, 8087","$0.00","This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.<br/><br/>The center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804829","SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems","CNS","Secure &Trustworthy Cyberspace","10/01/2018","07/23/2020","Kamalika Chaudhuri","CA","University of California-San Diego","Continuing Grant","Phillip Regalia","09/30/2023","$405,173.00","","kamalika@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8060","025Z, 7434, 8087, 9102","$0.00","This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.<br/><br/>The center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804648","SaTC: CORE: Frontier: Collaborative: End-to-End Trustworthiness of Machine-Learning Systems","CNS","Secure &Trustworthy Cyberspace","10/01/2018","07/24/2020","Somesh Jha","WI","University of Wisconsin-Madison","Continuing Grant","Phillip Regalia","09/30/2023","$413,313.00","","jha@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8060","025Z, 7434, 8087","$0.00","This frontier project establishes the Center for Trustworthy Machine Learning (CTML), a large-scale, multi-institution, multi-disciplinary effort whose goal is to develop scientific understanding of the risks inherent to machine learning, and to develop the tools, metrics, and methods to manage and mitigate them.  The center is led by a cross-disciplinary team developing unified theory, algorithms and empirical methods within complex and ever-evolving ML approaches, application domains, and environments. The science and arsenal of defensive techniques emerging within the center will provide the basis for building future systems in a more trustworthy and secure manner, as well as fostering a long term community of research within this essential domain of technology. The center has a number of outreach efforts, including a massive open online course (MOOC) on this topic, an annual conference, and broad-based educational initiatives. The investigators continue their ongoing efforts at broadening participation in computing via a joint summer school on trustworthy ML aimed at underrepresented groups, and by engaging in activities for high school students across the country via a sequence of webinars advertised through the She++ network and other organizations.<br/><br/>The center focuses on three interconnected and parallel investigative directions that represent the different classes of attacks attacking ML systems: inference attacks, training attacks, and abuses of ML.   The first direction explores inference time security, namely methods to defend a trained model from adversarial inputs. This effort emphasizes developing formally grounded measurements of robustness against adversarial examples (defenses), as well as understanding the limits and costs of attacks. The second research direction aims to develop rigorously grounded measures of robustness to attacks that corrupt the training data and new training methods that are robust to adversarial manipulation.   The final direction tackles the general security implications of sophisticated ML algorithms including the potential abuses of generative ML models, such as models that generate (fake) content, as well as data mechanisms to prevent the theft of a machine learning model by an adversary who interacts with the model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839591","EAGER: Real-D: Integrating Data-Driven Methods and Engineering Models in Manufacturing Systems","CMMI","Special Initiatives","09/01/2018","08/22/2018","Kamran Paynabar","GA","Georgia Tech Research Corporation","Standard Grant","Georgia-Ann Klutke","08/31/2021","$299,951.00","Edmond Chow","kamran.paynabar@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1642","071E, 7916, 8029","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) will contribute to national prosperity and economic welfare by studying new methods for combining machine learning with engineering knowledge to improve performance of manufacturing systems. Machine learning has attracted much attention as it offers the potential for analysis of massive data in various application domains, including engineering and manufacturing. However, the reliance on data-driven methods alone, outside of the context of engineering knowledge and physical principles, can led to misspecified models with low accuracy and/or black-box models with poor interpretability. On the other hand, engineering models generally rely on assumptions that may not hold in practice, leading to bias and poor predictive capability. This award supports fundamental research that bridges the gap between pure data-driven methods and those based purely on engineering models by introducing a novel framework that integrates statistical machine learning methods with physics, engineering and first principles to create more accurate analytical models for manufacturing systems. This research creates an analytical framework enabling a better understanding of manufacturing system performance through the fusion of data with engineering principles.  This approaches is expected to improve product quality, increase machine availability and reduce manufacturing costs by identifying and controlling critical process factors. New methodologies developed in this research will be incorporated into the STEM educations curriculum and teaching activities. <br/><br/>Going beyond existing machine learning techniques, this research will integrate data analysis and engineering modeling to provide more accurate methods for data analysis and prediction.  These new methods are expected to outperform both data-driven and first principles models when they are used separately.  The project will create a new sampling strategy for conducting experiments and collecting data that, unlike current design of experiment and optimal design methods, uses engineering models to guide the sampling direction and select the sampling points that most improve accuracy. The project will also build a new real-time dimension reduction and feature extraction method from streaming data that can extract both low-dimensional spatial and temporal features embedded in data streams leading to effective dimension reduction. A set of computationally efficient estimation algorithms will be developed that enable the real-time feature learning and analysis for high-velocity streams. From a quality improvement viewpoint, the project will enable researchers in the quality engineering community to reexamine quality monitoring and improvement methods with a new perspective based on the fusion of data and engineering models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760374","FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/2018","08/02/2018","Ilse C.F. Ipsen","NC","North Carolina State University","Standard Grant","Leland Jameson","07/31/2021","$366,109.00","","ipsen@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","MPS","1269, 1271","1616, 9263","$0.00","A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.<br/><br/>The proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833373","CSR: Medium: Collaborative Research: Scale-Out Near-Data Acceleration of Machine Learning","CNS","CSR-Computer Systems Research","02/07/2018","08/30/2019","Hadi Esmaeilzadeh","CA","University of California-San Diego","Continuing Grant","Matt Mutka","09/30/2021","$365,683.00","","hadi@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7354","","$0.00","A growing number of commercial and enterprise systems increasingly rely on machine learning algorithms. This shift is, on the one hand, due to the breakthroughs in machine learning algorithms that extract insights from massive amounts of data. Therefore, such systems need to process ever-increasing amounts of data, demanding higher memory bandwidth and capacity. However, the bandwidth between processors and off-chip memory has not increased due to various stringent physical constraints. Besides, data transfers between the processors and the off-chip memory consume orders of magnitude more energy than on-chip computation due to the disparity between interconnection and transistor scaling.<br/><br/>Exploiting recent 3D-stacking technology, the researcher community has explored near-data processing architectures that place processors and memory on the same chip. However, it is unclear whether or not such processing-in-memory (PIM) attempts will be successful for commodity computing systems due to the high cost of 3D-stacking technology and demanded change in existing processor, memory and/or applications. Faced with these challenges, the PIs are to investigate near-data processing platforms that do not require any change in processor, memory and applications, exploiting deep insights on commodity memory subsystems and network software stack. The success of this project will produce inexpensive but powerful near-data processing platforms that can directly run existing machine learning applications without any modification.<br/>"
"1746170","SBIR Phase I:  A software product that empowers healthcare teams with community resource information and facilitates post-treatment care coordination.","IIP","SBIR Phase I","01/01/2018","03/25/2019","Tom Lee","TX","Medical Innovators Company, LLC","Standard Grant","Henry Ahn","07/31/2019","$224,935.00","","tomlee6750@hotmail.com","10707 Holly Springs","Houston","TX","770421411","7132480311","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to develop and test feasibility of a web-based software and machine learning technology that is able to empower healthcare teams with community resource information as well as facilitate coordination of post-treatment care. Lack of access to community resources such as housing, food and transportation (also known as social determinants of health) has been associated with negative health outcomes such as unplanned hospital readmissions and emergency room visits. This leads to high healthcare costs. Therefore, healthcare teams (i.e. social workers, case managers and discharge planners) spend a significant amount of time to locate appropriate community resources for their patients. Our innovation will leverage a community of healthcare professionals and the resource providers to ultimately find community resources for patients in a faster and less costly manner. The machine learning technology supported by this award will connect healthcare professionals with relevant community resources that ultimately reduces the cost and time associated with this process. A successful implementation of this technology will lead to improved post-treatment care outcomes for the patients and reduced cost of care.  <br/><br/>The proposed project will develop and test the feasibility of a web-based software platform to empower healthcare professionals to share community resources. Novel machine learning technology will be developed to facilitate appropriate and efficient exchange of community resources. Healthcare professionals using the platform will be able to get onto the platform as well as search and share resources. The machine learning algorithm leverages the relationships of healthcare professionals and resource providers to coordinate community resource sharing. A successful implementation of this algorithm will provide substantial improvements on the ability to acquire timely community resources as compared current methods. The goal of this research is to validate whether the machine learning technology is able to help healthcare professionals identify appropriate community resources."
"1815561","RI: Small: A Unified Compositional Model for Explainable Video-based Human Activity Parsing","IIS","Robust Intelligence","09/01/2018","08/15/2018","Ying Wu","IL","Northwestern University","Standard Grant","Jie Yang","08/31/2021","$449,000.00","","yingwu@eecs.northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7495","7495, 7923","$0.00","An ultimate goal of computer vision is understanding scene and activities from images and video. This task involves many perceptual and cognitive processes at various semantic levels. A next step beyond visual classification is visual interpretation, that is, to explain the relations among visual entities through visual inference and reasoning. Due to the enormous variability across instances of this problem, semantic parsing for explaining a visual scene and activities is highly challenging. This project studies how the structural composition of visual entities can be used to overcome the diversity in the visual scene and activities. It advances and enrich the basic research of computer vision, and brings significant impact on many merging applications, including autonomous or assisted driving, intelligent robots, and intelligent video surveillance. This research also contributes to education through curriculum development, student training, and knowledge dissemination. It includes interactions with K-12 students for participation and research opportunities.  <br/><br/>This research is to develop a unified visual compositional model that can effectively learn complex semantic concepts in a scalable end-to-end fashion, while achieving good generalizability and providing explainable parsing of the visual data. The project is focused on: (1) a principled model and its theoretical foundation, by designing a stochastic grammar based on the probabilistic And/Or-Graph to model the structural composition; (2) an effective computational approach for learning and parsing, by exploiting data-driven pattern mining to discover structural components and by exploring how the patterns may be self-formed; (3) a solid case study on video human activity parsing and interpretation, by inferring the complex compositions of human actions, body movements, and interaction with the environment; and (4) tools and prototype systems for human articulated body pose estimation, contextual object discovery, and video-based human activity analysis and interpretation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821157","CDS&E: Collaborative Research: Scalable Nonparametric Learning for Massive Data with Statistical Guarantees","DMS","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","08/01/2018","08/15/2018","Zuofeng Shang","IN","Indiana University","Standard Grant","Christopher Stark","12/31/2019","$155,000.00","","zshang@njit.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","MPS","1253, 8004, 8069","026Z, 8004, 8083, 9263","$0.00","We now live in the era of data deluge. The sheer volume of the data to be processed, together with the growing complexity of statistical models and the increasingly distributed nature of the data sources, creates new challenges to modern statistics theory. Standard machine learning methods are no longer able to accommodate the computational requirements. They need to be re-designed or adapted, which calls for a new generation of design and theory of scalable learning algorithms for massive data. This project aims to provide a collection of state-of-the-art nonparametric learning tools for big data analysis, which can be directly used by scientists and practitioners and have beneficial impacts on various fields such as biomedicine, health-care, defense and security, and information technology. The deliverables of this project include easy-to-use software packages that will be thoroughly evaluated using a range of application examples. They will directly help scientists to explore and analyze complex data sets.<br/> <br/>Due to storage and computational bottlenecks, traditional statistical inferential procedures originally designed for a single machine are no longer applicable to modern large datasets. This project aims to design new scalable learning algorithms of wide-ranging nonparametric models for data that are distributed across a large number of multi-core computational nodes, or in a fashion of random sketching if only a single machine is available. The computational limits of these new algorithms will be examined from a statistical perspective. For example, in the divide-and-conquer setup, the number of deployed machines can be viewed as a simple proxy for computing cost. The project aims to establish a sharp upper bound for this number: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. Related questions will also be addressed in the randomized sketching method in terms of the minimal number of random projections.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830964","EAGER: Development of 3D Second Harmonic Generation Tomography and Deep Learning Algorithms for Classification of Human Ovarian Cancer","CBET","BioP-Biophotonics","06/01/2018","05/31/2018","Paul Campagnola","WI","University of Wisconsin-Madison","Standard Grant","Leon Esterowitz","05/31/2021","$299,831.00","Vikas Singh","pcampagnola@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","7236","7916","$0.00","This proposal will address the problem of distinguishing between normal tissue and precancer tissue changes, critical for successful treatment of ovarian cancer. Ovarian cancer has a very poor prognosis unless detected at the earliest stage, which is extremely difficult. Current clinical imaging techniques lack sufficient resolution, specificity, and sensitivity for effective differentiation of normal and diseased cancerous tissue. <br/><br/><br/>There is a pressing need for new insight into disease etiology and progression. The PI will develop new Second Harmonic Generation (SHG) imaging tools for this purpose. They will extend current SHG microscopy to tomography for better characterization of 3D collagen structure. They will a) develop SHG excitation tomography; b) develope new reconstruction algorithms; and c) incorporate the 3D data into a respective classification scheme, and explore the use of deep learning algorithms to refine the classification analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750575","CAREER: New Paradigms for Online Machine Learning","IIS","Robust Intelligence","03/15/2018","02/07/2019","Karthik Sridharan","NY","Cornell University","Continuing Grant","Rebecca Hwa","02/28/2023","$264,517.00","","sridharan@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7495","1045, 7495","$0.00","We live in a technology-driven world, where users interact with large-scale, automated systems on a daily basis. Online recommendation systems, search engines and personalized medicine are just a few examples of  systems that use Machine Learning (ML) algorithms at their core. The long term success of ML as a field relies on transforming it into an easily usable, seamless technology with rigorous, provable guarantees on performance. Further, to have a positive impact on society, ML technologies need to be equipped to handle the social challenges that accompany any large multi-user systems. The overarching goal of this CAREER project is to make socially-responsible ML a readily accessible black-box technology that is applicable in large multi-user interactive systems.  In particular, the project focuses on three concrete challenges. The first challenge is to make ML a plug-and-play technology by automating the process of designing task specific ML algorithms. The second challenge is to develop ML methods for modern applications such as predicting user preferences in social networks, where data is evolving and complexly interconnected. The third challenge is to develop theory and algorithms for recommendation systems that are socially responsible and do not polarize its users. <br/><br/>In recent years, exploring inherent connections between probability theory and sequential prediction problems have lead to a unifying theory and algorithm design principles for online learning. This CAREER project will build on these developments. Using the so called Burkholder method from probability theory and advances in the field of mathematical programming, the project will aim at automating the process of designing new and effective online learning algorithms. Building on the recently developed idea of online relaxations, the project will introduce novel methodology for designing computationally efficient algorithms for learning from interconnected data points. Finally, using and extending ideas from classical statistics to deal with control and nuisance variables, the project will develop new methods for recommender systems that can avoid polarizing users. The CAREER program will advance STEM education by developing new educational components related to ML. ``Machine Learning for the Masses'' workshops will be co-organized with Women In Computing at Cornell aimed at involving women and underrepresented minorities and exposing undergraduates to research and job opportunities in the field of ML during their formative years.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817226","CHS: Small: Improving Usability and Reliability for Motor Imagery Brain Computer Interfaces","IIS","HCC-Human-Centered Computing","09/01/2018","09/13/2018","Virginia de Sa","CA","University of California-San Diego","Continuing Grant","Ephraim Glinert","08/31/2021","$500,000.00","","vdesa@cogsci.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7367","7367, 7923, 8089, 8091","$0.00","Brain-computer interfaces (BCIs) allow a user to interact with the world directly through brain activity.  These systems are being developed to provide a communication method for users with severe motor impairments who are not able to control the movements of their arms, tongue, and even eyes well enough to communicate in the usual ways.  While the cognitive abilities of these individuals are thought to be largely preserved, they are often described as being ""locked in"" to their bodies, unable to interact with the outside world through the usual means of typing, talking, etc.  Electroencephalogram (EEG) based motor imagery BCIs attempt to distinguish brain activity by measuring electrical activity on the scalp caused by the user imagining moving different body parts.  Commonly, such systems try to distinguish when the user is imagining moving their right or their left hand.  Imagining different body parts can then be mapped to different tasks to allow a user to interact with the world (e.g., to turn a light on or off, or to move a robot arm to one object or another).  The goal of this research is to make these types of systems easier for users to learn and more reliable, by improving the feedback that is given to the user and improving the classification of the brain signals.  The work has the potential to open up this method of communication for more people, and project outcomes may have even broader impact by enabling us to learn more about brain signals that can be used for communication in BCIs.  In addition, diverse graduate students will be trained in interdisciplinary research, and undergraduate students in the BCI class will work on small related projects, some of which will be presented to high school students to encourage and stimulate their interest in science.<br/><br/>The ability of users to generate discriminable control signals is very variable. Moreover, environmental effects such as other brain processes, emotion and fatigue affect current BCI systems.  The goal of this project is to improve the usability of EEG-based motor-imagery brain-computer interfaces. To this end, a multi-pronged approach will be used.  First, richer feedback will give users a better visualization of the effects of their imagery and provide them with a better chance to learn how to discriminate the motor imagery of different body parts.  Second, the machine classification of the EEG signal during motor imagery will be improved.  This will include looking for other signals that may provide additional insight into the top-level state and goals of the user as well as developing new deep learning algorithms that can benefit from multi-task learning and transfer learning between individuals.  Third, different closed-loop control methods will be explored to improve the total information transfer rate of the BCI as well as to reduce the number of training trials needed.  The team's prior work has shown that interactive signals that respond to the feedback provided by the system are more robust to system estimation errors and non-stationarities.  These signals can arise passively but also can be actively used by exploiting interactive commands that vary with the received feedback.  Whether active control of interactive commands, or active control of standard commands with passive interactive recognition, performs better will be tested.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816504","III: Small: Collaborative Research: Social Media Based Analysis of Adverse Drug Events: User Modeling, Signal Reliability, and Signal Validation","IIS","Info Integration & Informatics","08/01/2018","08/04/2018","Ahmed Abbasi","VA","University of Virginia Main Campus","Standard Grant","Sylvia Spengler","07/31/2021","$229,974.00","Richard Netemeyer, Jingjing Li","abbasi@comm.virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7364","7364, 7923","$0.00","Adverse drug reactions (ADRs) have been associated with significant morbidity and mortality, and have been a significant cause of hospital admissions, accounting for as much as 5% of all admissions. About 2,000,000 serious ADRs are reported yearly in the US; 100,000 annual deaths are related to adverse drug events; serious ADRs rank 4th to 6th as causes of death. The problem stems from the fact that the ADR profile of a given drug is rarely complete at the time of official approval. The typically limited preapproval evaluation often results in the possibility that when the drug is finally approved for use in the general population (with significant diversity in race, gender, age, lifestyle), some previously unidentified ADRs are often observed. This problem is acute for psychotropic medications, given the fact that most people with psychiatric diseases tend to have other health issues, with the individual taking multiple drugs at the same time (both psychotropic and non-psychotropic), with often unknown interactions between them. Initial results have shown the promise of using social-media data for ADR signal detection. However, these methods are still faced with two critical challenges, namely, signal reliability and biological validation. Thus, this project proposes a detailed study on key determinants of signal reliability: credibility of social media sources, model of the users that generate source content, signal generation from such sources, and validation of the generated signals. This work will be relevant to government agencies charged with drug approval, drug monitoring, and disease monitoring, drug companies, hospitals, and the general public. The impact of the proposed work will go beyond drug surveillance, since the approaches proposed can be adapted for other healthcare problems, and for other scenarios, such as financial markets, and national security. Planned educational activities include outreach to high-school students, and involvement of undergraduate and graduate students. Research results will be disseminated via technical publications in professional journals and conference presentations.<br/> <br/><br/>The project has three specific aims: (1) Enrich signal reliability in social media analysis of adverse drug events, using credibility analysis, user modeling and signal fusion via deep learning; (2) Signal validation via molecular level analysis; (3) Prototype development and evaluation. The ubiquity, veracity and diversity of data from various social media channels and other sources of user-generated content necessitate a serious consideration of their credibility, recency, uniqueness and salience. To enrich signal reliability, the team will propose novel methods for ADR signal detection using credibility analysis, and for user modeling and signal fusion based on deep leaning techniques.  For signal validation, biological support for hypothesized ADRs, essentially connecting high-level observations from social media interactions to potential associations at molecular level networks and pathways, will be used. The results will change the current largely passive approach to post-marketing drug surveillance that relies heavily on voluntary reports, by ensuring reliability in social-media based approaches, thus making the public an integral part of a proactive drug surveillance system. The idea of signal fusion and deep learning for user modeling and signal generation can be extended for other uses beyond drug surveillance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746113","SBIR Phase I:  Using Deep Learning and Action Recognition to Automatically Digitize Human Actions at Scale: Putting Workers at the Center of the Next Industrial Revolution","IIP","SBIR Phase I","01/01/2018","12/23/2017","Prasad Akella","CA","Drishti Technologies, Inc.","Standard Grant","Muralidharan Nair","06/30/2018","$225,000.00","","p.akella@drishtilabs.com","951 El Cajon Way","Palo Alto","CA","943033409","6503844521","ENG","5371","5371, 8034","$0.00","The broader impact/commercial potential of this project is to enable the automatic analysis of human motion using visual information gathered at high frequency. Within the manufacturing context?which represents 11% of US GDP [Bureau of Economic Analysis]?the collection and interpretation of this new set of large-scale time-and-motion data enables dramatic improvements in the understanding of assembly processes and optimization of human productivity. In addition, manufacturers can use this system to flag process errors or deviations?ideally in real time?allowing for mitigation before the deviations propagate further down the value chain. Just these two capabilities enable manufacturers to avoid costly rework and recalls, improve worker accuracy, discover new opportunities to optimize processes and generate revenue generation, and flag worker safety issues. As a result, workers and management become aligned around shared goals of efficiency and competitiveness?reducing fears of automation while dramatically improving productivity and possibly protecting jobs. <br/> <br/>This Small Business Innovation Research Phase I project will improve the robustness of a prototype deep learning back-end for automatic action recognition from a stream of video data. While object recognition is now commonplace, action detection?e.g., inferring the behavior and intentions of actors and objects over time?has not yet been solved or commercialized. In fact, it is still an active research area. Solving this problem requires overcoming technical hurdles in industrial settings that include changing actors, lighting conditions and camera perspectives; manual labelling of large volumes of video data; transmitting large volumes of data to and from the cloud; accurately inferencing with high levels of confidence; and developing intuitive human/system interfaces that may, in the future, include unconventional channels such as AR/VR, text-to-speech, haptic feedback, amongst others."
"1763134","GOALI: Coadaptation of Intelligent Office Desks and Human Users to Promote Worker Productivity, Health and Wellness","CMMI","M3X - Mind, Machine, and Motor, GOALI-Grnt Opp Acad Lia wIndus","08/01/2018","07/20/2018","Burcin Becerik-Gerber","CA","University of Southern California","Standard Grant","Robert Scheidt","07/31/2021","$667,716.00","Gale Lucas, Francesco Anselmo, Shawn Roll","becerik@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","ENG","058Y, 1504","060Z, 063Z, 070E, 1504, 7632, 9102","$0.00","The main objective of this Grant Opportunity for Academic Liaison with Industry (GOALI) project is to perform fundamental research that will ultimately allow the GOALI team to develop and test an intelligent office workstation (a smart desk) that will optimize the user's wellbeing and productivity through adjustment of postural, thermal, and lighting settings.  The smart desk uses wearable and workstation-mounted sensors to infer human intent, physiological condition and current task. The project advances fundamental research addressing how best to combine sensor data, machine learning approaches and structured communication between the user and the workstation to bring thermal, visual and postural conditions closer to proven best practices over time, while simultaneously improving user satisfaction and willingness to use the system. The project will generate the experimental data, task-specific models of human intent and preferences, and adaptive control algorithms needed to develop a robotic device that will interact physically and intuitively with workers to enhance their physical comfort and workplace productivity. The project is significant because the addition of intelligent workstations in offices has potential to change the way health and wellbeing are promoted and achieved in the workplace.  This project directly serves the NSF mission by promoting science that explores modes of interaction between human workers and intelligent robotic systems that advance the health, prosperity and welfare of individual workers, their employers, and the nation. The project supports education and promotes diversity through outreach activities aimed at recruiting and retaining under-represented students in research, as well as by promoting entrepreneurship and innovation.<br/><br/><br/>This intelligent workstation will learn worker preferences and shape worker behavior through an ongoing, bi-directional, adaptive process of sensing, feedback and manipulation of environmental parameters that have the potential to directly impact postural, thermal and visual comfort and to increase worker productivity.  Four tasks are researched. Task 1 evaluates sensing and learning methods for inferring the worker's existing state of thermal, visual and postural comfort. Sensing modalities will include: wearable sensors for skin temperature, galvanic skin response, and heartrate; environmental sensors for temperature, humidity, air pressure, light intensity and color temperature; a structured light depth sensor (for postural assessment); and ""passive"" sensors to record user changes to desk/chair height, fan/heater speed/set-points, and light intensity and color. Learning approaches include supervised learning (driven by user adjustments to the workstation as well as ground truth assessments of user comfort using Likert-like scales), unsupervised learning, and semi-supervised learning (using limited user feedback to label data clusters). The system will consider task context when inferring user preferences. Task 2 examines how the user and the autonomous workstation might best negotiate control of the local workstation environment to optimize worker productivity, health and well-being.  Two sub-tasks are researched.  The first explores shared control under the extreme conditions (full user control with machine cueing or full workstation control with manual overrides). The second sub-task explores approaches for adaptive, negotiated control of environmental state between the worker and the workstation when the human and workstation have full access to the sensor data. Task 3 will use focus group and user experience studies to identify the kinds of communication and feedback prompts best suited to promote shared-control and user/workstation interactions that drive environmental conditions toward the ideal. Task 4 integrates the results from the first three tasks to synthesize a workstation controller that will facilitate user/workstation co-adaptations promoting productivity, health and wellness.  The team will then test the shared-autonomy workstation in a 6-month efficacy study examining the extent to which the co-adaptive human/machine system can bring thermal, visual and postural conditions closer to proven best practices over time, while also improving user satisfaction and willingness to use the system. The project outcomes may have long-term impact by improving individual and societal workplace productivity, health and well-being.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835860","Collaborative Research: HDR: Data-Driven Earth System Modeling","AGS","Climate & Large-Scale Dynamics, Software Institutes, EarthCube","11/01/2018","08/06/2019","Tapio Schneider","CA","California Institute of Technology","Continuing Grant","Eric DeWeaver","10/31/2023","$2,499,842.00","Andrew Stuart","tapio@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","GEO","5740, 8004, 8074","026Z, 062Z, 4444, 7925, 8004","$0.00","Global weather and climate models represent the atmosphere on computational grids with horizontal spacing of perhaps 100km, stacked in layers which can be over a kilometer thick.  Such grids suffice to capture the dynamics of cyclones, fronts, and other large-scale atmospheric phenomena, but these phenomena depend critically on processes with spatial scales much smaller than the grid spacing.  The small-scale processes must be represented indirectly, through parameterization schemes which estimate their net impact on the resolved atmospheric state.  For example clouds are typically too small for the grid spacing yet they are critical for moving moisture from the ocean surface to the mid-troposphere, thus cloud parameterizations play a key role in determining atmospheric humidity even on the largest spatial scales.  Parameterization schemes are inherently approximate, and the development of schemes which produce realistic simulations is a central challenge of model development.  Shortcomings in parameterization limit the usefulness of weather and climate models both for scientific research and for societal applications.<br/><br/>Most parameterization schemes depend critically on various parameters whose values cannot be determined a priori but must instead be found through trial and error.  This task, referred to as ""tuning"", is laborious as it is performed separately for each parameterization scheme and involves multiple integrations of the model in multiple configurations. It is also inefficient in its use of observations, which is unfortunate given the large amount of observational data available from satellites and other sources. The resulting parameter sets may not be optimal and may produce unexpected results when all the schemes interact with each other in global simulations.  Finally, manual tuning is not conducive to uncertainty quantification, which would be valuable for estimating the uncertainty in future climate change projections. <br/><br/>The goal of this project is to replace ad hoc manual tuning with a combination of data assimilation, machine learning, and fine-scale process modeling using large eddy simulation (LES) models.  LES models have grid spacings of a few tens of meters and can explicitly simulate the clouds and turbulence represented by parameterization schemes.  These ingredients are combined to create a global Machine Learning Atmospheric Model (MLAM), in which LES models embedded in selected grid columns of a global model explicitly simulate subgrid-scale processes which are represented by parameterization schemes in the other columns. Machine learning is used to tune the schemes to emulate the behavior of the LES simulations, so that explicit simulations become an online benchmark for parameterization.  In this way all the schemes can be tuned together and interactively within a running global simulation.  Observational data from a variety of sources is assimilated during the model integration to provide a further constraint on parameter values, and estimates of parameter uncertainty are generated as part of the automated tuning.  A similar tuning process is implemented in an ocean general circulation model, and the two are combined to produce a machine learning climate model.  Model tuning is generally viewed as a necessary but mundane activity which is not in itself a research topic.  But a model capable of learning its parameters from observations and process models offers a new path forward, toward both better models and better ways of using models.<br/><br/>The work has broader impacts due to the societal value of better forecasts and projections from weather and climate models.  The work directly addresses uncertainty in forecasts and projections used by decision makers to plan for weather and climate impacts.  In addition, the modeling strategy developed here is applicable to a broad class of research areas which face the problem of relating large-scale behaviors to small-scale unresolved processes (the problem of relating genotypes to phenotypes in evolutionary biology, for example).  In addition, the PIs will establish a cross-disciplinary graduate program on data-driven Earth system modeling. The program bridges the gap between environmental and computational sciences which currently hinders progress in environmental modeling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804945","Learning to Automatically Evaluate Pathological Gait: A Data-Driven System for Characterizing Disability and Informing Therapeutic Interventions","CBET","Disability & Rehab Engineering","09/01/2018","06/27/2018","Kathleen Sienko","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Aleksandr Simonian","08/31/2021","$223,962.00","Jenna Wiens","sienko@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","5342","9102","$0.00","Being able to balance is something most of us take for granted. However, approximately 35% of U.S. citizens 40 years and older are affected by vestibular-related balance issues. The vestibular system, located in the inner ear, is one of several sensory systems that provides our central nervous system with balance and spatial orientation information. When a person's vestibular system is impaired by disease or injury, he/she can experience balance and gait deficits in addition to dizziness and vertigo. There are physical, emotional, and monetary costs associated with sensory-based balance disabilities, such as vestibular disabilities, and the falls that typically follow bouts of balance instability. Most fall-related injuries occur during walking (gait), but treating imbalance during gait is challenging. Current clinical tools for assessing gait pathologies (gait abnormalities due to injury or disease) in people with vestibular disabilities do not fully capture body motion, neglecting potentially critical features of sensory-related disabilities during gait-based activities. The goal of this project is to develop and test data-driven algorithms (problem solving instructions) for characterizing pathological motion. This work will lead to new methods for assessing sensory-related gait disorders and support the development of novel rehabilitation strategies. As part of this research, large motion sensing networks will be combined with machine learning algorithms to identify and measure gait abnormalities in people with vestibular disabilities. Though the focus of this project is on vestibular disabilities, the methods developed can be generalized to a wide range of balance impairments stemming from sensory disabilities, injuries, neural disabilities, motor disabilities, and aging. This research will also contribute to the training of both undergraduate and graduate students through capstone design projects, clinical immersion experiences to identify unmet rehabilitation needs, and the development and implementation of an open access, online educational module focused on applications of machine learning for societal impact.<br/><br/><br/>This project's primary purpose is to develop and assess data-driven machine learning (ML) algorithms that identify and quantify pathological gait in people with vestibular disabilities for the purposes of informing the creation of new assessment techniques and supporting the development of novel rehabilitation strategies.  The Research Plan is organized under three objectives.   The first objective is to create a shareable database of gait measurements from subjects with vestibular disabilities.  Activities include:  a) recruiting participants with vestibular deficits and age-matched healthy controls, b) collecting kinematic data during an experimental session in which subjects are instrumented with a full set of passive markers and up to 17 IMUs (Inertial Measurement Units), c) collecting clinical vestibular testing diagnostic data, e.g., electronystagmography test battery, and d) collecting Physical Therapist (PT) labels based on videotaped gait rehabilitation exercises that are viewed and rated on a 1-5 visual analog scale by a small cohort of PTs and d) sharing data by organizing data into tables that can be downloaded in a local database format.  The second objective is to develop robust data-driven ML algorithms for automatically evaluating and characterizing pathological gait patterns in people with vestibular disabilities.  Activities are organized under sub-objectives designed to learn data-driven models to a) automatically differentiate subjects with vestibular disabilities from healthy controls, b) characterize subpopulations by developing a notion of prototypical gait pathologies for each clinical subgroup and c) quantify the extent of the disability and generate hypotheses regarding the root sensorimotor or biomechanical problem.  The third objective is to develop and prospectively evaluate a portable system for real-time assessment.  Activities include: a) developing a portable smartphone gait assessment tool that will generate real-time ratings during gait-based rehabilitation exercises using data obtained from no more than 7 IMUs and b) prospectively testing the system in a proof-of-concept study involving 10 adults.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835576","Collaborative Research: HDR: Data-Driven Earth System Modeling","AGS","PHYSICAL OCEANOGRAPHY, Climate & Large-Scale Dynamics, EarthCube","11/01/2018","08/07/2019","Raffaele Ferrari","MA","Massachusetts Institute of Technology","Continuing Grant","Eric DeWeaver","10/31/2023","$1,250,000.00","","rferrari@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","GEO","1610, 5740, 8074","026Z, 062Z, 4444, 7925, 8004","$0.00","Global weather and climate models represent the atmosphere on computational grids with horizontal spacing of perhaps 100km, stacked in layers which can be over a kilometer thick.  Such grids suffice to capture the dynamics of cyclones, fronts, and other large-scale atmospheric phenomena, but these phenomena depend critically on processes with spatial scales much smaller than the grid spacing.  The small-scale processes must be represented indirectly, through parameterization schemes which estimate their net impact on the resolved atmospheric state.  For example clouds are typically too small for the grid spacing yet they are critical for moving moisture from the ocean surface to the mid-troposphere, thus cloud parameterizations play a key role in determining atmospheric humidity even on the largest spatial scales.  Parameterization schemes are inherently approximate, and the development of schemes which produce realistic simulations is a central challenge of model development.  Shortcomings in parameterization limit the usefulness of weather and climate models both for scientific research and for societal applications.<br/><br/>Most parameterization schemes depend critically on various parameters whose values cannot be determined a priori but must instead be found through trial and error.  This task, referred to as ""tuning"", is laborious as it is performed separately for each parameterization scheme and involves multiple integrations of the model in multiple configurations. It is also inefficient in its use of observations, which is unfortunate given the large amount of observational data available from satellites and other sources. The resulting parameter sets may not be optimal and may produce unexpected results when all the schemes interact with each other in global simulations.  Finally, manual tuning is not conducive to uncertainty quantification, which would be valuable for estimating the uncertainty in future climate change projections. <br/><br/>The goal of this project is to replace ad hoc manual tuning with a combination of data assimilation, machine learning, and fine-scale process modeling using large eddy simulation (LES) models.  LES models have grid spacings of a few tens of meters and can explicitly simulate the clouds and turbulence represented by parameterization schemes.  These ingredients are combined to create a global Machine Learning Atmospheric Model (MLAM), in which LES models embedded in selected grid columns of a global model explicitly simulate subgrid-scale processes which are represented by parameterization schemes in the other columns. Machine learning is used to tune the schemes to emulate the behavior of the LES simulations, so that explicit simulations become an online benchmark for parameterization.  In this way all the schemes can be tuned together and interactively within a running global simulation.  Observational data from a variety of sources is assimilated during the model integration to provide a further constraint on parameter values, and estimates of parameter uncertainty are generated as part of the automated tuning.  A similar tuning process is implemented in an ocean general circulation model, and the two are combined to produce a machine learning climate model.  Model tuning is generally viewed as a necessary but mundane activity which is not in itself a research topic.  But a model capable of learning its parameters from observations and process models offers a new path forward, toward both better models and better ways of using models.<br/><br/>The work has broader impacts due to the societal value of better forecasts and projections from weather and climate models.  The work directly addresses uncertainty in forecasts and projections used by decision makers to plan for weather and climate impacts.  In addition, the modeling strategy developed here is applicable to a broad class of research areas which face the problem of relating large-scale behaviors to small-scale unresolved processes (the problem of relating genotypes to phenotypes in evolutionary biology, for example).  In addition, the PIs will establish a cross-disciplinary graduate program on data-driven Earth system modeling. The program bridges the gap between environmental and computational sciences which currently hinders progress in environmental modeling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1906169","CAREER:  Scaling Up Knowledge Discovery in High-Dimensional Data Via Nonconvex Statistical Optimization","IIS","Info Integration & Informatics","07/01/2018","06/29/2020","Quanquan Gu","CA","University of California-Los Angeles","Continuing Grant","Wei Ding","07/31/2022","$396,664.00","","qgu@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364","1045, 7364","$0.00","The past decade has witnessed a surge of research activities on knowledge discovery in high-dimensional data, among which convex optimization-based methods are widely used. While convex optimization algorithms enjoy global convergence guarantees, they are not always scalable to high-dimensional massive data. Motivated by the empirical success of nonconvex methods such as matrix factorization, the objective of this project is to develop a new generation of principled nonconvex statistical optimization algorithms to scale up high-dimensional machine learning methods. This project amplifies the utility of high-dimensional knowledge discovery methods in various fields such as computational genomics and recommendation systems. It incorporates the resulting research outcomes into curriculum development and online courses, to train a new generation of machine learning and data mining practitioners. In addition, special training is provided to K-12 students and community college students for a broader education of modern data analysis techniques.<br/><br/><br/>This project consists of three synergistic research thrusts. First, it develops a family of nonconvex algorithms for structured sparse learning, including extensions to both parallel computing and distributed computing. Second, it devises a unified nonconvex optimization framework for low-rank matrix estimation, which covers a wide range of low-rank matrix learning problems such as matrix completion and preference learning. Several acceleration techniques are also explored. Third, it develops a family of alternating optimization algorithms, to solve the bi-convex optimization problem for estimating various complex statistical models. This project integrates modern optimization techniques with model-based statistical thinking, and provides a systematic way to design nonconvex high-dimensional machine learning methods with strong theoretical guarantees. The targeted applications include but not limited to computational genomics, neuroscience, and recommendation systems."
"1816005","III: Small: Collaborative Research: Social Media Based Analysis of Adverse Drug Events: User Modeling, Signal Reliability, and Signal Validation","IIS","Info Integration & Informatics","08/01/2018","09/13/2018","Donald Adjeroh","WV","West Virginia University Research Corporation","Continuing Grant","Sylvia Spengler","07/31/2021","$270,000.00","Wanhong Zheng, Marie Abate, Gianfranco Doretto","donald.adjeroh@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","7364","7364, 7923, 9150","$0.00","Adverse drug reactions (ADRs) have been associated with significant morbidity and mortality, and have been a significant cause of hospital admissions, accounting for as much as 5% of all admissions. About 2,000,000 serious ADRs are reported yearly in the US; 100,000 annual deaths are related to adverse drug events; serious ADRs rank 4th to 6th as causes of death. The problem stems from the fact that the ADR profile of a given drug is rarely complete at the time of official approval. The typically limited preapproval evaluation often results in the possibility that when the drug is finally approved for use in the general population (with significant diversity in race, gender, age, lifestyle), some previously unidentified ADRs are often observed. This problem is acute for psychotropic medications, given the fact that most people with psychiatric diseases tend to have other health issues, with the individual taking multiple drugs at the same time (both psychotropic and non-psychotropic), with often unknown interactions between them. Initial results have shown the promise of using social-media data for ADR signal detection. However, these methods are still faced with two critical challenges, namely, signal reliability and biological validation. Thus, this project proposes a detailed study on key determinants of signal reliability: credibility of social media sources, model of the users that generate source content, signal generation from such sources, and validation of the generated signals. This work will be relevant to government agencies charged with drug approval, drug monitoring, and disease monitoring, drug companies, hospitals, and the general public. The impact of the proposed work will go beyond drug surveillance, since the approaches proposed can be adapted for other healthcare problems, and for other scenarios, such as financial markets, and national security. Planned educational activities include outreach to high-school students, and involvement of undergraduate and graduate students. Research results will be disseminated via technical publications in professional journals and conference presentations.<br/> <br/><br/>The project has three specific aims: (1) Enrich signal reliability in social media analysis of adverse drug events, using credibility analysis, user modeling and signal fusion via deep learning; (2) Signal validation via molecular level analysis; (3) Prototype development and evaluation. The ubiquity, veracity and diversity of data from various social media channels and other sources of user-generated content necessitate a serious consideration of their credibility, recency, uniqueness and salience. To enrich signal reliability, the team will propose novel methods for ADR signal detection using credibility analysis, and for user modeling and signal fusion based on deep leaning techniques.  For signal validation, biological support for hypothesized ADRs, essentially connecting high-level observations from social media interactions to potential associations at molecular level networks and pathways, will be used. The results will change the current largely passive approach to post-marketing drug surveillance that relies heavily on voluntary reports, by ensuring reliability in social-media based approaches, thus making the public an integral part of a proactive drug surveillance system. The idea of signal fusion and deep learning for user modeling and signal generation can be extended for other uses beyond drug surveillance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816986","RI: Small: Actively Learning From The Crowd","IIS","Robust Intelligence","09/01/2018","08/14/2018","Reinhard Heckel","TX","William Marsh Rice University","Standard Grant","Rebecca Hwa","08/31/2021","$474,322.00","","rh43@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7495","7495, 7923","$0.00","How can a large number of questions be answered from few and only partially correct responses? This problem lies at the heart of labeling large collections of unlabeled data, which are common in machine learning and data science. It also lies at the heart of learning about preferences of people and quality of items by carrying out surveys. A popular approach to address this problem is to crowdsource the labeling or learning task by paying a large number of people small amounts of money to answer questions on the internet through a crowdsourcing platform. However, the quality of the workers responses varies significantly due to different abilities of the people and difficulties of the questions. To account for the uncertainty of the responses, each question is assigned to multiple people and their responses are aggregated. However, the assignment process is often agnostic to the peoples abilities and questions difficulties, since both are unknown a priori. This project will develop algorithms that adapt to the people and questions and thereby significantly reduces the number of responses required to enable machine learning algorithms to perform well and surveys to be informative. Besides the research objectives, the researchers will pursue educational objectives by integrating parts of this project into a graduate class, promoting undergraduate research, and fostering exchange across disciplines by running an interdisciplinary machine learning seminar. <br/><br/>The core optimization problem in crowdsourcing is to achieve confidence in the final answers at minimal cost, by assigning only few tasks to the people (or workers). Intuitively, that can be accomplished by only posing a question to the workers best qualified to answer that question. This project will develop efficient and practical active schemes for crowdlabeling and crowdsourcing that adaptively choose which question to pose to which worker. For each algorithm, the project will prove corresponding rigorous computational and statistical problem-instance dependent performance guarantees, as opposed to worst-case performance guarantees. The theoretical results will be complemented with practical open-source implementations and experiments on real-world data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815697","RI: Small: Learning discrete structure from continuous spaces","IIS","Robust Intelligence","08/15/2018","08/08/2018","Mikhail Belkin","OH","Ohio State University","Standard Grant","Rebecca Hwa","07/31/2021","$450,000.00","Yusu Wang","mbelkin@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 7923","$0.00","Science, medicine, business, and engineering are increasingly data-driven. Hypotheses, diagnoses, decisions, and designs are made by gathering and analyzing a wealth of data in search of meaningful patterns. It is the goal of the field of machine learning to develop methods for inferring and reasoning about patterns in the data. The research supported by this award addresses the question of learning from unlabeled data, one of the more vexing problems of data science.  The PI's analyze and develop algorithms for problems such as clustering, i.e., finding groups of similar objects, as well as understanding continuously changing attributes in data. By integrating machine learning, modeling and geometric data analysis, this work injects new ideas and methodologies to modern data analysis, helps build practical algorithms for unsupervised and unsupervised learning and analyze their properties and domains of applicability. Students working on this project have a unique opportunity to be exposed to a broad spectrum of topics including machine learning, statistics, geometry  and applied mathematics.<br/><br/>On a more technical level, the unifying  perspective  for the proposed research is that many of these unsupervised learning problems can be viewed as recovering structure or invariants of the underlying continuous space through the lens of the discrete data. This work takes that point of view to consider  a number of important aspects of unsupervised learning including hierarchical clustering in the density model, data quantization, graphon clustering and estimation, as well as learning metric structure from data. The project also considers applications of these ideas to supervised learning, particularly in helping to scale algorithms to large data.   While the work on this project concentrates on theoretical analyses, these  are developed with a view toward practical algorithms, implementations and applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816227","RI:Small: Online Maximization Algorithms for Streaming Data","IIS","Robust Intelligence","08/15/2018","07/27/2020","Yiming Ying","NY","SUNY at Albany","Standard Grant","Rebecca Hwa","07/31/2021","$498,333.00","Siwei Lyu","yying@albany.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","7495","7495, 7923","$0.00","Although existing machine learning (ML) methods are effective for analyzing data that are static in nature, many of today's applications from information retrieval to web searching require real-time processing of massive amounts of streaming data. Often, the data is available in an online fashion, meaning the entire dataset is not available at once, but rather, individual data instances arrive sequentially. Area under the Receiver Operating Characteristic Curve (AUC) has been proven to be an effective evaluation metric and learning objective in many application domains, but it is difficult to optimize directly. To date, there are no satisfactory approaches to incorporate AUC into online ML algorithms for classification and ranking problems. This project will address the fundamental theoretical and algorithmic challenges in ML algorithms based on AUC maximization for processing streaming, high-dimensional data with efficient algorithms and theoretical analysis. The results of this project are expected to have broader impact in intrusion detection for cyber-security, fault detection in safety critical systems, information retrieval and cancer diagnosis. The planned research will also integrate with educational activities, including developing new undergraduate/graduate courses on optimization and machine learning, organizing a workshop and making software tools freely available to the public. The principal investigators also plan to undertake outreach activities to improve STEM learning of high school students.<br/><br/>A central topic of this project is to develop efficient online learning algorithms for AUC maximization and bipartite ranking, making them amenable for online processing of high dimensional and large volume of streaming data. This project also establishes a rigorous statistical foundation and thorough theoretical analysis of the online AUC maximization algorithms developed. The primary technical challenge in developing online AUC maximization algorithms and theory is that the objective functions involve statistically dependent pairs of instances while individual instances continuously arrive in a sequential order. This is in contrast to standard classification based on accuracy, where the loss functions only depend on individual instances and the related algorithms and theory are well developed. The project will address this gap by sufficiently exploiting the structures of the population objective involved in the problem of AUC maximization, rather than an empirical objective on finite data, through novel interaction of machine learning, statistical theory and applied mathematics. This will help to remove the pairwise structure in the original objective function and facilitate the development of efficient online learning algorithms for AUC maximization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755656","CRII: CIF: Crowdsourcing-aware Learning","CCF","Comm & Information Foundations","04/01/2018","12/18/2017","Nihar Shah","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","03/31/2020","$174,934.00","","nihars@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7935, 8228","$0.00","Machine learning has significantly advanced the state of the art in a variety of applications. These successes have required massive labeled datasets for training machine learning algorithms. The collection of these labeled datasets usually involves human annotation. For instance, the training labels for supervised learning algorithms are often obtained through ""crowdsourcing"" where people label the data over the Internet in exchange for monetary incentives. Most learning algorithms, however, are agnostic of this human-labeling process. This project designs improved learning algorithms by incorporating the ""human"" aspect of the data collection process in the machine learning objective.<br/><br/>In more detail, this project considers supervised binary classification tasks where the labels for the training data are obtained from people. The research involves design of learning algorithms that jointly consider the human collection process -- including the interfaces and incentives available to the human labelers -- and the overall learning objective. Theoretical guarantees of optimality are derived and compared with guarantees for algorithms which are agnostic of the human component. The algorithms and guarantees are based on models of human behavior from psychology, such as permutation-based models, that allow for maximal accuracy while making minimal assumptions on how the human labelers behave. The theoretical results are corroborated with practical implementations (open sourced) and real-world experiments (data freely available online).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818546","Conference on Statistical Learning and Data Science","DMS","STATISTICS","05/01/2018","04/26/2018","Annie Qu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","04/30/2019","$15,000.00","","aqu2@uci.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","1269","7556","$0.00","Columbia University will host a three-day conference on Statistical Learning and Data Science/Nonparametric Statistics, June 4-6, 2018 in New York City. The objective of the conference is to bring together researchers in statistical learning, data science and nonparametric statistics from academia, industry, and government. Statistical machine learning is widely recognized as a very active area of interdisciplinary research, closely related to statistics, optimization, and computer science. In addition, it also plays an essential role in the new important areas of data science and big data. Due to advances in technology, massive and complex data in the ""big data era"" are prevalent in almost every aspect of modern scientific research. It is critical to manage such huge amounts of data, and make reliable prediction and inference. Statistical machine learning techniques have developed substantial flexibility in handling such data, with a wide range of applications in diverse scientific disciplines. <br/><br/>This conference is expected to (1) bring together researchers from different disciplines, including statistics, computer science, machine learning, engineering, and biomedical and other related research fields, to address recent development and emerging issues in statistical learning, data science and nonparametric statistics, (2) promote interactions and collaborations among researchers, (3) discuss new ideas and future research directions for statistical learning and data science, with a focus towards knowledge discovery in sciences and engineering, (4) provide an excellent opportunity for junior researchers to interact and learn from leading scientists in the field. The conference will consist of three plenary sessions, 55 invited sessions and poster sessions. Conference topics include unsupervised, semi-supervised and supervised learning, with applications in rankings, text and web mining, network analysis, bioinformatics, high-dimensional data, functional data, genomics, drug discovery, intrusion and fraud detection. NSF funding will provide travel support to students, post-doctoral scholars, and early-career researchers to encourage their participation in this event. The conference website is https://publish.illinois.edu/sldsc2018/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764415","CDS&E: D3SC: Applying Video Segmentation to Coarse-grain Mapping Operators in Molecular Simulations","CHE","Chem Thry, Mdls & Cmptnl Mthds","08/01/2018","07/23/2018","Andrew White","NY","University of Rochester","Standard Grant","Evelyn Goldfield","07/31/2021","$488,605.00","Chenliang Xu","andrew.white@rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","MPS","6881","062Z, 8084, 9216, 9263","$0.00","Andrew White and Chenliang Xu of the University of Rochester is supported by an award from the Chemical Theory, Models and Computational Methods program in the Division of Chemistry to apply advances in computer vision to improve models of multiscale systems in chemistry.  Multiscale systems describe chemical and physical processes that occur on many different time and spatial scales, for example, both very fast and very slow motions may contribute to the overall process.  In both the computer processing of videos and the modeling of multiscale chemical systems, reducing complexity via removing extraneous details is essential. Without removing some model details, simulating multiscale processes like DNA transcription or the peptide aggregation which leads to plaque formation in Alzheimer's disease is impossible. Current approaches to reduce the number of atoms in a model rely on intuition and tradition due to the near infinite ways in which atoms can be removed or combined. White, Xu and their research groups are developing a novel approach built upon advances in video segmentation. Video segmentation is the process of identifying foreground, background, and objects in a video. Surprisingly, the same mathematical structure can be applied to chemical systems and that is the goal of this research. White, Xu and their collaborators will introduce the research to a broader audience via an augmented-reality laboratory for students. Students will be able to decide how to simplify molecular models and see the results by combining the visual experience of augmented-reality with the interactivity of molecular simulations. <br/><br/>Coarse-graining (CG) is the dimension reduction technique used to simulate multiscale systems more efficiently. There is not a rigorous theory for generating mappings from all-atom (fine-grain) system to the CG system. This missing component is essential because past CG work shows that many mappings lead to homogeneous, weakly interacting, gas-like CG models but the number of possible mappings is combinatorial with respect to the number of atoms. Andrew White and his collaborators are working to solve this mapping problem by (i) developing a theory to represent mapping algorithms based on video segmentation algorithms; (ii) creating a database of mappings and their performance on benchmark simulations to foster community involvement; (iii) studying and testing these methods on multi-protein surface interactions, where current mapping approaches struggle. Achieving success here, along with recent advances in calculating CG potentials, will better advance the community's ability to model complex multiscale phenomena.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821266","Theory-Driven Solutions to Robust and Non-Convex Data Science Problems","DMS","CDS&E-MSS","07/01/2018","06/14/2018","Gilad Lerman","MN","University of Minnesota-Twin Cities","Standard Grant","Christopher Stark","06/30/2021","$200,000.00","","lerman@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","8069","8083, 9263","$0.00","Many solutions to data science problems are based on non-convex optimization. Recent surprising theoretical and empirical results have shown that directly solving non-convex problems can yield computationally efficient, high-accuracy algorithms.  This has spurred the expanding body of work on the analysis of non-convex algorithms for a variety of structured data problems. One class of these is robust recovery problems, which aim to recover a hidden structure in corrupted data. While robustness is a classical theme in mathematical statistics, computationally efficient methods with guaranteed accuracy for high levels of corruption have not been widely studied. Non-convex minimization methods indicate the potential to obtain competitive speed and accuracy for such problems, as was already demonstrated by the PI and his collaborators on the problem of robust subspace recovery. The research work aims to further establish and extend such guarantees to many other robust recovery problems whose solutions are crucial for modern applied problems. Applications include the problem of three-dimensional reconstruction from a set of two-dimensional images.<br/> <br/>The PI and his collaborators aim to develop robust theory and algorithms for truly challenging non-convex recovery problems. These recovery problems are typically NP-hard with highly complex energy landscapes. However, these landscapes often exhibit special structure that seems to allow for recovery under certain adversarial settings and recovery with high probability under certain generative models. For some of the problems of outlier-robust optimization over special, continuous non-convex sets, the PI and his collaborators aim to establish that the corresponding non-convex energy landscapes are ""well-tempered"" under some generic conditions. This implies that, under these conditions, one may apply an iterative scheme initialized at a pre-specified point and guarantee its fast convergence to the global minimum of the energy landscape. In other challenging discrete settings, ""multi-valley"" landscapes are observed. In this case, special structures and phase transitions will be quantified. The PI and his collaborators plan to apply their  theory-driven, non-convex optimization solutions in order to solve several applied scientific problems, in particular, problems that arise in the current pipeline of structure from motion in computer vision.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817122","SaTC: CORE: Small: Machine Learning for Effective Fuzz Testing","CNS","Secure &Trustworthy Cyberspace","10/01/2018","08/27/2018","Koushik Sen","CA","University of California-Berkeley","Standard Grant","Sol Greenspan","09/30/2021","$500,000.00","Dawn Song","ksen@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8060","025Z, 7434, 7923","$0.00","In recent years, fuzz testing has evolved as one of the most effective testing techniques for finding security vulnerabilities and correctness bugs in real-world software systems.  It has been used successfully by major software companies for security testing and quality assurance.   State-of-the-art fuzz testing tools have found numerous security vulnerabilities and bugs in widely used software such as Web browsers, network tools, image processors, popular system libraries, C compilers, and interpreters.<br/><br/>Fuzz testing works by generating random input data for a program under test.  A key reason behind its huge popularity is that it has low computation overhead compared to other sophisticated techniques such as dynamic symbolic execution.  While fuzz testing has been highly successful in practice, it has been mostly implemented in ad-hoc ways by incorporating a collection of hacks and best practices. As such, fuzz testing techniques usually generate many redundant test inputs and take several days to weeks to find bugs. For complex input formats, such as for random C program inputs for a C compiler, a huge amount of manual tuning is required to make fuzz testing generate valid test inputs.  This project proposes to make fuzz testing smarter and more effective by applying machine learning with customizable testing objectives. The proposed techniques will use probabilistic machine learning models, such as n-grams, recurrent neural networks (RNN), recursive neural networks, or multi-armed bandits, to generate inputs from scratch or to mutate a set of seed inputs.  The model will be trained in such a way that the inputs generated by it will maximize the custom testing objective. We expect that such a model will generate fewer redundant inputs and can be customized to user-provided testing objectives. This project aims to contribute to the development of reliable, secure, and trustworthy software.  The tools and techniques developed in this project will make it easier for programmers to write correct and secure programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750729","CAREER: Recovering complex associations from high-dimensional genomic data","IIS","Info Integration & Informatics","02/15/2018","07/15/2020","Barbara Engelhardt","NJ","Princeton University","Continuing Grant","Sylvia Spengler","01/31/2023","$468,393.00","","bee@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7364","1045, 7364, 9102","$0.00","In this proposal, the PI will analyze current genomic data using powerful modern machine learning methods to help make personalized medicine for each patient a reality. Imagine using genomic data and clinical traits to accurately predict  risk for preterm birth early in gestation, or a college student's risk for future heart attacks, and tailoring preventative measures to the specific risk mechanisms. This research addresses a core theme of machine learning methods applied to scientific data: how to robustly and efficiently build predictive models using complex hidden structure in high dimensional data with limited numbers of samples, as is common in genomic and biomedical data. This proposal addresses a number of fundamental questions: How to use correlation among features to share strength across limited numbers of samples? How to test for causality of an observation in a cell on disease? How to encode biological structure in nonlinear functions? These fundamental questions in applied machine learning and statistical genetics will be addressed through the creation of hierarchical models and methods for computationally tractable analyses. These projects will enable recovery of genomic signals with predictive ability essential for personalized medicine.  The PI also plans active engagement with underrepresented minorities in computer science and making publicly available software.<br/><br/>This research aims to develop computationally tractable structured hierarchical models to find complex signals in genomic data that are hidden to current methods that will be used to build predictive models using existing genomic study data, and to use these predictive variants to precisely quantify disease risk for each patient. Success of these goals impacts personalized medicine, enabling a complete understanding of the genetic regulators of disease and making individual-specific disease risk prediction and treatment a reality. Although linear models have been used to analyze scientific data for 125 years, these methods assume unlimited availability of samples and simple linear structure, and fail to recover variants with more complex associations. In genomic data, predictive signal is often compositional, including linear, sparse, low-rank, or nonlinear structure. This proposed research will drastically shift current scientific data analysis by developing efficient methods that recover predictive genetic variants with complex effects. This research is organized around three integrated projects. 1) High-dimensional correlations. Current methods for correlation do not exploit multiple, correlated traits to improve power to find relationships between two high-dimensional sets of observations. The PI will develop computationally tractable models and robust inference methods for structured latent variable models in the presence of substantial observation noise. 2) Sparse, nonlinear regression for prediction by exploiting nonaddictive effects. Standard predictive models for genomics assume that associations are sparse and additive across predictors; nonlinear terms are not regularized appropriately. The PI will develop a predictive model that robustly recovers variants with additive and nonadditive effects. 3) Causal inference to study the mechanism of genetic regulation of disease. Current models of causal inference in genomics make unrealistic assumptions and fail to exploit modern machine learning approaches to nonlinearity, regularization, and approximate inference. The PI will develop a hierarchical model for causal analysis to pinpoint the cellular mechanisms of disease."
"1814993","An Astronomical Time Machine: Light Echoes from Historic Supernovae","AST","GALACTIC ASTRONOMY PROGRAM","07/01/2018","05/24/2018","Armin Rest","MD","Johns Hopkins University","Standard Grant","Glen Langston","06/30/2021","$324,176.00","Charles Kilpatrick","arest@stsci.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1216","1206, 1207","$0.00","The investigator has discovered a way to study the 3-dimensional (3D) properties of a Supernova (SN), by observing SN light that only now reaches Earth, after bouncing off dust filaments. These light echoes provide a unique opportunity in astronomy: direct observation of the cause (the explosion) as well as the effect (the expanded remnant) of the same astronomical event.  Because the light echoes come from many directions around the supernovae, a 3D model of the event can be constructed.<br/><br/>The investigators plan public outreach talks, video summaries of their research and citizen science opportunities for the public.  The light echoes are novel events and are naturally great introductions to the history of astronomy, light travel time, SN explosion physics, and the advantage of spectroscopy compared to historical brightness measurements.  They are developing interactive learning materials available for introductory undergraduates. They have an exciting Citizen Science plan, which enables the public to learn the scientific process by identifying new light echoes in their observations.<br/><br/>The 3D model is valuable because modern theoretical work suggests that asymmetry may be a critical ingredient in the SN explosion mechanism. The investigators will expand their program to regularly observe 6 historical SN events.  They will observe with the NSF?s Blanco telescope, a large telescope, with a 4-meter aperture size. The 3-degree square field of view, combined with the fast 20 second readout of DECam, this the ideal telescope for this search. Since their targets are generally older than already detected supernovae, they expect the targets to be slightly fainter.  They will produce many 5 minute duration images a night.  They will use computers programmed for machine learning to identify echoes.  They will compare their machine learning results with Citizen Science analysis of the same images.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755873","CRII: CHS: Early Detection of Collective Misconceptions with Network-aware Machine Learning Tools","IIS","HCC-Human-Centered Computing","03/15/2018","03/09/2018","Emoke-Agnes Horvat","IL","Northwestern University","Standard Grant","William Bainbridge","02/28/2021","$174,788.00","","a-horvat@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7367","7367, 8228","$0.00","This project builds theory, algorithms, and frameworks that can be used to design network-aware machine learning tools aimed at eliciting useful diversity and improving the accuracy of collective forecasting. Researchers in the social and economic sciences know that there is great capacity for collective intelligence to emerge from Web-based systems. Yet herding and homophily effects often restrain the wisdom of crowds, vastly limiting this potential. The research furthers the study of complex systems by introducing a new framework that improves our understanding of the mechanisms that govern decision-making under social influence. Advancing complex systems theory in this way greatly enhances the ability to predict when crowds will provide accurate decision-making support for complex problems and when they will fail miserably. Further, the research aids the development of opinion aggregation mechanisms that efficiently capitalize on diversity. The planned work will result in developments that make collective intelligence detection tools practical by providing early warning signs of shared misconceptions. <br/><br/>To attain these goals, the research will apply a general framework that incorporates (1) network models that help understand the social processes that lead to observed decision patterns; (2) machine learning tools that draw from uncovered processes to identify signals that optimize the accuracy of collective judgment; and (3) evaluation testbeds that use simulation tools in addition to rich high-dimensional, real-world data about the various stages and performance of group decisions.  This research will contribute to societally-relevant outcomes, including: (a) understanding decision-making in online investment and lending settings to enhance the economic growth of underserved market segments; (b) generating novel knowledge about the performance benefits of collective judgment, and (c) quantifying the link between limited opinion diversity and crowd misconceptions. The project will connect undergraduate students, including women and under-represented minorities, to authentic practice in science and engineering research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814041","CCF-BSF: AF: Small: Collaborative Research: Practice-Friendly Theory and Algorithms for Linear Regression Problems","CCF","Algorithmic Foundations","10/01/2018","06/02/2020","Petros Drineas","IN","Purdue University","Standard Grant","A. Funda Ergun","09/30/2021","$257,892.00","","pdrineas@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7796","014Z, 7923, 7926, 7933, 9251","$0.00","The project focuses on one of the most fundamental problems in the intersection of applied mathematics and computer science: solving systems of multiple linear equations in multiple variables. Such systems, also known as linear regression problems, have applications in various fields, from classical engineering to data science and machine learning. These applications yield systems with millions of equations and variables. The design of very efficient solver algorithms is thus a problem of paramount importance. Over the last twenty years there has been a tremendous focus and progress in the theory of algorithms for solving certain types of linear systems that are ubiquitous in applications, despite the fact that they are somewhat restricted (e.g. each equation has only two variables). Along with these algorithms, a wealth of new notions, techniques and tools has been acquired. The project will develop extensions of these techniques, targeting concrete applications in related fields. Towards this end, the project includes research problems that are appropriate for advanced undergraduate and graduate students with complementary interests and skills, ranging from applied to theoretical. Research will be disseminated through all standard channels, importantly including free software.<br/><br/>The project will pursue three main directions: (i) Bring the recent progress from the theoretical to the practical realm. Linear system solvers are useful in a variety of contexts, implying a need for implementations in disparate computational environments, including basic consumer computers, graphical processing units, or big parallel and distributed systems. This necessitates the development of new theory and algorithms that are practice-friendly, i.e. designed with the practical performance end-goal in mind. (ii) The impact of linear system solvers in the downstream applications in Data Science and Machine Learning can be accelerated and strengthened by pursuing their tighter integration with the target applications. A second major goal of the project is thus to pursue an exportation of techniques and notions from the theory of linear regression to specific problems in Machine Learning. This will require the development of adaptations and enhancements of these techniques. (iii) The study of specific algorithmic applications in Machine Learning also serves the third major goal of the project: the design of solvers for regression problems that go beyond the restricted types for which efficient solvers are currently known.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813374","CCF-BSF: AF: Small: Collaborative Research: Practice-Friendly Theory and Algorithms for Linear Regression Problems","CCF","Algorithmic Foundations","10/01/2018","08/27/2018","Ioannis Koutis","NJ","New Jersey Institute of Technology","Standard Grant","Tracy Kimbrel","09/30/2021","$249,866.00","","ikoutis@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7796","7923, 7926, 7933","$0.00","The project focuses on one of the most fundamental problems in the intersection of applied mathematics and computer science: solving systems of multiple linear equations in multiple variables. Such systems, also known as linear regression problems, have applications in various fields, from classical engineering to data science and machine learning. These applications yield systems with millions of equations and variables. The design of very efficient solver algorithms is thus a problem of paramount importance. Over the last twenty years there has been a tremendous focus and progress in the theory of algorithms for solving certain types of linear systems that are ubiquitous in applications, despite the fact that they are somewhat restricted (e.g. each equation has only two variables). Along with these algorithms, a wealth of new notions, techniques and tools has been acquired. The project will develop extensions of these techniques, targeting concrete applications in related fields. Towards this end, the project includes research problems that are appropriate for advanced undergraduate and graduate students with complementary interests and skills, ranging from applied to theoretical. Research will be disseminated through all standard channels, importantly including free software.<br/><br/>The project will pursue three main directions: (i) Bring the recent progress from the theoretical to the practical realm. Linear system solvers are useful in a variety of contexts, implying a need for implementations in disparate computational environments, including basic consumer computers, graphical processing units, or big parallel and distributed systems. This necessitates the development of new theory and algorithms that are practice-friendly, i.e. designed with the practical performance end-goal in mind. (ii) The impact of linear system solvers in the downstream applications in Data Science and Machine Learning can be accelerated and strengthened by pursuing their tighter integration with the target applications. A second major goal of the project is thus to pursue an exportation of techniques and notions from the theory of linear regression to specific problems in Machine Learning. This will require the development of adaptations and enhancements of these techniques. (iii) The study of specific algorithmic applications in Machine Learning also serves the third major goal of the project: the design of solvers for regression problems that go beyond the restricted types for which efficient solvers are currently known.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763000","Collaborative Research: Operations-Driven Machine Learning","CMMI","OE Operations Engineering","08/15/2018","08/03/2018","Adam Elmachtoub","NY","Columbia University","Standard Grant","Georgia-Ann Klutke","07/31/2021","$314,206.00","","adam@ieor.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","ENG","006Y","073E, 078E, 5514","$0.00","This award will contribute to the Nation's prosperity and welfare by capitalizing on the increased availability and accessibility of data to improve operational decision making.  Operational decisions are ubiquitous in all aspects of the commercial economy, and even incremental improvements in operations can have major impacts in the competitiveness of such sectors as transportation, logistics, healthcare delivery, supply chain management.  Similarly, public sector service operations involve decision making to wisely invest limited public resources.  The ongoing data revolution has created great opportunities for leveraging large scale data to improve operational decision making.  This award will support research in new techniques to make effective use of these data in the management of operations.  This project provides a broadly applicable framework for addressing operational decisions and will result in improved performance and efficiency in practice.  The project will involve outreach engagements with diverse organizations, including a nonprofit foster care agency. <br/><br/>Current operational decision-making often involve two significant challenges:  prediction and optimization.  These tasks are usually addressed sequentially: key parameters are first predicted using modern statistical machine learning tools, and then planning decisions are made using these predictions within a complex optimization model.  This project advances a new, broadly applicable framework, called Smart ""Predict, then Optimize"" (SPO), that effectively addresses the prediction and optimization challenges in tandem.  In this new framework, operational performance is measured by the true objective value of the solutions generated from the predicted parameters.  This project investigates the statistical and computational properties of novel loss functions in the SPO framework, including convex surrogates as well as non-convex formulations.  The project will also develop new algorithms for training machine learning models, such as linear models, logistic models, and decision trees, using the new loss functions, and will extend the SPO framework to handle regularization, robustness, different data primitives, and dynamic data collection with exploration-exploitation tradeoffs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821342","Learning Geometry for Inverse Problems in Imaging","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","07/01/2018","06/05/2020","Stacey Levine","PA","Duquesne University","Standard Grant","Christopher Stark","06/30/2021","$116,803.00","","sel@mathcs.duq.edu","Room 310 Administration Building","Pittsburgh","PA","152193016","4123961537","MPS","1271, 8069","9251, 9263","$0.00","The quantity of digital image data our society relies on for medical, military, and a wide range of engineering and scientific applications continues to increase, yet digital images still typically exhibit some level of degradation during formation, transmission, and storage, often obscuring vital information. Despite significant advances in the fields of image processing and computer vision, this problem still persists due to the difficulty in accurately modeling random degradations such as noise, pixel loss, and blur. The main objectives for this project are to learn critical geometric and higher order image features for accurately solving a variety of inverse problems in imaging, including image denoising, image deblurring, image inpainting (filling in of missing data), and super-resolution.  The new algorithms developed in this project are expected to yield improvements over existing algorithms in the form of standard image quality metrics as well as in the preservation of accurate, fine details, a feature missing from many current state of the art image processing algorithms, yet vital for automatically interpreting this image data in practice.<br/> <br/>In recent work the PI and collaborators have developed several frameworks for image denoising that attempt to recover an image from a denoised geometric feature of the image. These approaches have successfully improved upon existing state of the art denoising algorithms, providing information in the reconstruction that has been elusive using alternate approaches. The challenge in working with this geometric data is that while it is very robust in practice, mathematically sound mechanisms developed for handling natural image data do not necessarily apply to their geometric features. This project involves learning geometric descriptors from image data that have suffered from some combination of the aforementioned random and/or linear degradations for the purpose of aiding in image reconstruction, analysis, and interpretation. Preliminary analyses and experiments indicate that the benefits of this approach could be significant, yet computationally intensive experiments are required to explore how best to exploit these benefits in practice. Theoretical analyses of these models will be an important part of this project as well, in order to better to understand when these models are guaranteed to be reliable in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812850","RI:Small:Collaborative Research: Understanding Human-Object Interactions from First-person and Third-person Videos","IIS","Robust Intelligence","08/15/2018","08/09/2018","Yong Jae Lee","CA","University of California-Davis","Standard Grant","Jie Yang","07/31/2021","$150,000.00","","yongjaelee@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7495","7495, 7923","$0.00","Ubiquitous cameras, together with ever increasing computing resources, are dramatically changing the nature of visual data and their analysis. Cities are adopting networked camera systems for policing and intelligent resource allocation, and individuals are recording their lives using wearable devices. For these camera systems to become truly smart and useful for people, it is crucial that they understand interesting objects in the scene and detect ongoing activities/events, while jointly considering continuous 24/7 videos from multiple sources. Such object-level and activity-level awareness in hospitals, elderly homes, and public places would provide assistive and quality-of-life technology for disabled and elderly people, provide intelligent surveillance systems to prevent crimes, and allow smart usage of environmental resources.  This project will investigate novel computer vision algorithms that combine 1st-person videos (from wearable cameras) and 3rd-person videos (from static environmental cameras) for joint recognition of humans, objects, and their interactions. The key idea is to combine the two views' complementary and unique advantages for joint visual scene understanding. To this end, it will create a new dataset, and develop new algorithms that learn to recognize objects jointly across the views, learn human-object and human-human relationships through the two views, and anonymize the videos to preserve users' privacies. The project will provide new algorithms that have the potential to benefit applications in smart environments, security, and quality-of-life assistive technologies. The project will also perform complementary educational and outreach activities that engage students in research and STEM.<br/><br/>This project will develop novel algorithms that learn from joint 1st-person videos (from wearable cameras) and 3rd-person videos (from static environmental cameras) for joint recognition of humans, objects, and their interactions. The 1st-person view is ideal for object recognition, while the 3rd-person view is ideal for human activity recognition. Thus, this project will investigate unique solutions to challenging problems that would otherwise be difficult to overcome when analyzing each viewpoint in isolation. The main research directions will be: (1) creating a benchmark 1st-person and 3rd-person video dataset to investigate this new problem; and developing algorithms that (2) learn to establish object and human correspondences between the two views; (3) learn object-action relationships across the views; and (4) anonymize the visual data for privacy-preserving visual recognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808859","Low-Complexity Algorithms for Sparse Conic Optimization with Applications to Energy Systems and Machine Learning","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2018","09/25/2019","Somayeh Sojoudi","CA","University of California-Berkeley","Standard Grant","Anil Pahwa","07/31/2021","$360,000.00","Richard Zhang","sojoudi@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","7607","155E","$0.00","The development of fast numerical algorithms is crucial for large-scale optimization problems arising in a wide range of areas, such as power systems, machine learning, control theory, transportations and operations research. The main challenge is the inability of the existing methods in handling the nonlinearity (non-convexity) of many real-world problems. Conic optimization is able to solve these nonconvex problems to global optimality in a rigorous and principled manner through the notion of convexification. Despite a mature theory on convexification, the practical use of conic optimization remains limited since this technique greatly increases the dimension of a problem. It is common amongst researchers to view conic optimization as a powerful theoretical tool that is inaccessible for real-world applications, due to the lack of efficient numerical algorithms for conic optimization. The objective of this proposal is to design low-complexity algorithms for conic optimization that directly exploit the structure of a give problem to reduce the complexity. The outcomes of this project will lead to wide-ranging societal impact in all areas of design, analysis, operation, and control in real-world systems. This project has several outreach and educational activities, such as participation in multiple programs for students from underrepresented groups, fostering undergraduate research, and organizing tutorial sessions and workshops. <br/><br/>This project develops numerical algorithms for sparse conic optimization by exploiting problem structure, with a particular emphasis on sparse semidefinite programs. The proposed approach uses the notion of tree decomposition to solve sparse problems in near-linear time and linear memory. The main objectives of this proposal are as follows: 1) to identify graph-theoretic structures that control the computational complexity of sparse conic optimization; 2) to design numerical algorithms based on this graphical analysis to achieve best complexities; 3) to develop parallel and distributed versions of these algorithms for real-time computing. This is an interdisciplinary project theoretically underpinned by graph theory, numerical algorithms, matrix completion, conic optimization, low-rank matrix optimization, and algebraic geometry, and finding applications in power systems and machine learning. The proposed project will apply the designed numerical algorithms to nonlinear power optimization problems with tens of thousands of parameters to demonstrate its impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823546","FoMR: Using Machine Learning to Design Next Generation Caches and Data Prefetchers","CCF","Special Projects - CCF, Software & Hardware Foundation","10/01/2018","04/07/2020","Calvin Lin","TX","University of Texas at Austin","Standard Grant","Yuanyuan Yang","09/30/2021","$256,956.00","","lin@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","2878, 7798","2878, 7798, 7941, 9251","$0.00","While the general public typically focuses on processor clock speed as a measure of microprocessor performance, it is often the memory system that limits overall system performance, particularly for today's data-hungry uses of computing.  This project explores a transformative approach to designing computer hardware, particularly the memory system:  Rather than rely solely on human insight and intuition, this approach adapts and leverages machine learning techniques to explore larger and richer design spaces in a more thorough and systematic manner.  This approach enables hardware designers to consider more complex design factors than are currently possible.  <br/><br/>This project consists of two phases.  The first phase seeks to dramatically improve traditional memory system components, such as cache replacement policies and data prefetchers, which have been heavily studied but are now ripe for innovation with the use of machine learning. The second phase considers complicating factors such as criticality, the interaction among these components, and the use of new memory technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762744","Collaborative Research: Operations-Driven Machine Learning","CMMI","OE Operations Engineering","08/15/2018","08/03/2018","Paul Grigas","CA","University of California-Berkeley","Standard Grant","Georgia-Ann Klutke","07/31/2021","$290,060.00","","pgrigas@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","006Y","073E, 078E, 5514","$0.00","This award will contribute to the Nation's prosperity and welfare by capitalizing on the increased availability and accessibility of data to improve operational decision making.  Operational decisions are ubiquitous in all aspects of the commercial economy, and even incremental improvements in operations can have major impacts in the competitiveness of such sectors as transportation, logistics, healthcare delivery, supply chain management.  Similarly, public sector service operations involve decision making to wisely invest limited public resources.  The ongoing data revolution has created great opportunities for leveraging large scale data to improve operational decision making.  This award will support research in new techniques to make effective use of these data in the management of operations.  This project provides a broadly applicable framework for addressing operational decisions and will result in improved performance and efficiency in practice.  The project will involve outreach engagements with diverse organizations, including a nonprofit foster care agency. <br/><br/>Current operational decision-making often involve two significant challenges:  prediction and optimization.  These tasks are usually addressed sequentially: key parameters are first predicted using modern statistical machine learning tools, and then planning decisions are made using these predictions within a complex optimization model.  This project advances a new, broadly applicable framework, called Smart ""Predict, then Optimize"" (SPO), that effectively addresses the prediction and optimization challenges in tandem.  In this new framework, operational performance is measured by the true objective value of the solutions generated from the predicted parameters.  This project investigates the statistical and computational properties of novel loss functions in the SPO framework, including convex surrogates as well as non-convex formulations.  The project will also develop new algorithms for training machine learning models, such as linear models, logistic models, and decision trees, using the new loss functions, and will extend the SPO framework to handle regularization, robustness, different data primitives, and dynamic data collection with exploration-exploitation tradeoffs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730082","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","Expeditions in Computing","03/01/2018","12/23/2019","Margaret Martonosi","NJ","Princeton University","Continuing Grant","Almadena Chtchelkanova","02/28/2023","$600,000.00","","martonosi@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756056","I-Corps: Automating reforestation using machine learning and unmanned aerial vehicles","IIP","I-Corps","01/01/2018","01/08/2018","Nancy Jackson","NJ","New Jersey Institute of Technology","Standard Grant","Nancy Kamei","06/30/2019","$50,000.00","","jacksonn@njit.edu","University Heights","Newark","NJ","071021982","9735965275","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to provide enhanced capabilities for improved management of forest resources via aerial reforestation. Sustainable forest management practices are important to decrease soil erosion, increase biodiversity, sequester carbon and meet future demand for timber and biomass supply to the pulp, paper, wood, and energy sectors. Reforestation (natural or assisted) of cleared or degraded land is an important management objective to achieve these goals. Management costs of private forest lands have increased over time particularly for small-scale private landowners. Unmanned aerial vehicles, equipped with customized hardware and software, have the ability to replace or supplement conventional assisted-reforestation practices (hand planting or mechanical) and provide a cost-effective alternative. This project's approach to reforestation can increase accessibility to physically remote or topographically constrained locations that are difficult to access with large heavy machinery and vehicles.<br/><br/>This I-Corps project will explore the commercial viability of a data driven management system for assisted reforestation efforts. The system is designed to plant seeds using unmanned aerial vehicles outfitted with customized hardware. The system hardware is capable of projecting customize seed pellets into a range of soil conditions and at penetration depths necessary for successful germination. Coupled to the system is a machine learning algorithm that provides data metrics on forest land holdings using remotely sensed images. The machine learning algorithm can be trained, based on data type and quantity, for more accurate and reliable analysis. These data enable private landowners to assess overall conditions of their holdings, facilitate development of reforestation plans, and determine flight patterns. In addition, the algortihm is capable of communicating and guiding the unmanned aerial vehicle's autonomous guidance system to direct field planting."
"1730104","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","Expeditions in Computing","03/01/2018","02/27/2018","Kenneth Brown","GA","Georgia Tech Research Corporation","Continuing Grant","Almadena Chtchelkanova","04/30/2018","$200,000.00","","kenneth.r.brown@duke.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822813","Cyberlearning: Sensei: High-Fidelity, Non-Invasive Classroom Sensing for Professional Development","IIS","S-STEM-Schlr Sci Tech Eng&Math","09/01/2018","08/18/2018","Amy Ogan","PA","Carnegie-Mellon University","Standard Grant","Amy Baylor","08/31/2021","$750,000.00","Yuvraj Agarwal, Christopher Harrison","aeo@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1536","063Z, 8045","$0.00","For years, research has shown that moving away from large lectures and increasing student engagement and participation in classrooms significantly improves learning. Unfortunately, professors lack quality professional development opportunities to improve their instruction, and typically receive no training on how to teach. This project is addressing the issue of college professors' professional development through a cyberlearning innovation called Sensei. Sensei has novel capabilities using sensors to capture, isolate, and analyze voice and video that will provide near real time data on classroom interactions such as the percent time students talk vs professors, the percent time students talk to students, student engagement through facial expression analysis, turn taking between students and professors, etc all of which involve multimodal analysis of voice and video. The second component of this research is the development of suggested actions to improve the professor's performance as a teacher.<br/><br/>More precisely, Sensei draws on technical and socio-technical advances in sensing arrays, computer vision, intelligent environments, and personal informatics, as well as frameworks of professional development in higher education. In this project the researchers will 1) develop the technologies needed to automatically sense and display feedback to instructors, 2) deploy this system in-vivo to college instructors over semesters of use in a series of design-based research studies, and interpret the results to 3) iterate on our framework for the routine incorporation of classroom data into professional development. This research is enabled by a cyber innovation in which computing is expanded by the capabilities of state of the art multimodal sensing approaches to achieve non-invasive sensing at classroom-scale. This cyber innovation drives a learning innovation of delivering near-real-time data on teaching practices in a combined reflection and training system by delivering rapid and frequent feedback and instruction on good strategies in manageable instructional units, that support a focus on student-centered beliefs. In turn, the learning innovation advances understanding of how instructors learn in technology-rich learning environments by exploring mechanisms in a framework of professional development that would not be possible without this new cyberlearning genre. In particular, through a series of design-based research studies with instructors teaching STEM college courses, the researchers explore ways in which Sensei  a) can trigger critical self reflection, b) how this self-reflection changes based on the features of the data viewed, c) how datadriven goal-setting can foster self-efficacy in teaching, and d) how these effects vary over time. All of the code will be developed as open source and, if successful, Sensei could be generalized to include K-12 teachers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757787","REU Site: Intelligent and Scalable Systems","CNS","RSCH EXPER FOR UNDERGRAD SITES, Special Projects - CNS","05/01/2018","07/19/2019","Brian Davison","PA","Lehigh University","Standard Grant","Harriet Taylor","04/30/2021","$371,959.00","Michael Spear","bdd3@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","1139, 1714","9250","$0.00","The Intelligent and Scalable Systems Research Experiences for Undergraduates (REU) site provides an inspirational and successful research experience for 10 undergraduate students over a 10 week period at Lehigh University. Over the summer, participants learn how to harness the incredible potential of machine learning, and the unprecedented power of modern parallel compute resources, to invent new systems and algorithms that are infused with intelligence and hence capable of solving pressing societal problems. The proposed REU Site employs a multifaceted recruiting strategy, with a focus on encouraging women, underrepresented, and minority students to participate; a special focus will be placed on recruiting from institutions that lack undergraduate research opportunities. Students will also benefit from a program of seminars and tutorials designed to ensure that they are ready to collaborate, knowledgeable about the key concepts and technologies in modern intelligent and scalable systems, and inspired to pursue both further education and life-long careers in Computer Science.<br/><br/>The objective of this ten-week summer research experience is to increase the number and diversity of students pursuing graduate degrees in computer science, around the timely theme of intelligent and scalable systems.   This REU Site will facilitate research into fundamental topics in both machine learning and scalable computer systems, such as new algorithms for machine learning, new approaches to privacy preservation, and new techniques for increasing the performance of parallel programs. It will also support application-focused research, with an emphasis on creating solutions to hard problems in both research and society. Through seminars and faculty-supervised collaborative projects, participants will learn about the research process, hone their software development skills, improve their comfort with technical communication, become knowledgeable about graduate school and its application process, and ultimately develop greater confidence in their ability to succeed in a Science/Technology/Engineering/Math career.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845606","CAREER:  Reinforcement Learning of the Free Energy Landscapes of Proteins","MCB","Molecular Biophysics","12/01/2018","08/14/2019","Diwakar Shukla","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Wilson Francisco","11/30/2023","$409,155.00","","diwakar@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","BIO","1144","1045, 7465","$0.00","Proteins are molecular machines that perform a variety of tasks in living organisms, such as transporting nutrients and communicating signals within and between cells. Some of these proteins undergo a change in their conformation or shape to carry out these functions. It is challenging to visualize the different shapes of these molecular machines using traditional experimental techniques, such as X-ray crystallography. Therefore, the molecular level understanding of these processes and its implications for protein function remains elusive. This lack of molecular information makes it difficult to develop engineering approaches to regulate protein function. Molecular Dynamics simulations provide a way to observe these movements at the atomic scale. However, significant amount of computer time is required to observe long timescale conformational change processes in proteins. The objectives of this project are (1) to develop efficient algorithms based on machine learning techniques to understand how proteins undergo conformational change and (2) to apply these algorithms to understand the role of protein dynamics in transport of molecules across the cell membrane via membrane transporters. The specific transporters investigated in this project play a critical role in determining crop productivity and neurological disorders in humans. In concert with these research objectives, the PI will develop outreach activities teaching high school girls about computational methods used to investigate protein function via Girl's Adventure in Mathematics, Engineering and Sciences summer camp at the University of Illinois. PI also plans to engage African-American boys at local Urbana-Champaign schools via a three-day after-school program to teach them about protein structure and function.<br/><br/>This project will develop computational methods that can efficiently explore the free energy landscapes associated with protein conformational changes. This work is guided by the hypothesis that leveraging ideas from reinforcement learning technique and using evolutionary coupled residue pair distances as order parameters for protein functional dynamics will allow efficient sampling of free energy landscapes. The development of the algorithm called ""Reinforcement Learning-Based Adaptive Sampling"" (REAP) has been initiated. Preliminary results have shown promising application of these ideas to several proteins. The fully developed algorithm would be particularly useful for systems with limited structural information, as order parameters can be identified using evolutionary coupled residues based on sequence information alone. The specific goals of this project include: (1) further develop the REAP methodology to efficiently explore the free energy landscapes associated with protein function, (2) test the new methodology to understand molecular processes of high biological importance but with limited availability of structural information. In particular, molecular mechanisms of substrate transport and its regulation for the following transport processes will be investigated: (1) Nitrate transport process via root-associated transporters in plants with applications in increasing the crop yields. (2) Serotonin transport in brain via human Serotonin transporter for elucidating molecular origin of neurological disorders. (3) Sugar transport via SWEET family transporter in Rice with applications in enhancing plant growth. Despite differences in their function and structures, these systems share similarities in terms of their modes of regulation that could allow for a comprehensive understanding of the regulatory mechanisms in membrane transporters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812943","RI:Small:Collaborative Research: Understanding Human-Object Interactions from First-person and Third-person Videos","IIS","Robust Intelligence","08/15/2018","08/09/2018","Michael Ryoo","IN","Indiana University","Standard Grant","Jie Yang","07/31/2021","$250,000.00","","mryoo@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7495","7495, 7923","$0.00","Ubiquitous cameras, together with ever increasing computing resources, are dramatically changing the nature of visual data and their analysis. Cities are adopting networked camera systems for policing and intelligent resource allocation, and individuals are recording their lives using wearable devices. For these camera systems to become truly smart and useful for people, it is crucial that they understand interesting objects in the scene and detect ongoing activities/events, while jointly considering continuous 24/7 videos from multiple sources. Such object-level and activity-level awareness in hospitals, elderly homes, and public places would provide assistive and quality-of-life technology for disabled and elderly people, provide intelligent surveillance systems to prevent crimes, and allow smart usage of environmental resources.  This project will investigate novel computer vision algorithms that combine 1st-person videos (from wearable cameras) and 3rd-person videos (from static environmental cameras) for joint recognition of humans, objects, and their interactions. The key idea is to combine the two views' complementary and unique advantages for joint visual scene understanding. To this end, it will create a new dataset, and develop new algorithms that learn to recognize objects jointly across the views, learn human-object and human-human relationships through the two views, and anonymize the videos to preserve users' privacies. The project will provide new algorithms that have the potential to benefit applications in smart environments, security, and quality-of-life assistive technologies. The project will also perform complementary educational and outreach activities that engage students in research and STEM.<br/><br/>This project will develop novel algorithms that learn from joint 1st-person videos (from wearable cameras) and 3rd-person videos (from static environmental cameras) for joint recognition of humans, objects, and their interactions. The 1st-person view is ideal for object recognition, while the 3rd-person view is ideal for human activity recognition. Thus, this project will investigate unique solutions to challenging problems that would otherwise be difficult to overcome when analyzing each viewpoint in isolation. The main research directions will be: (1) creating a benchmark 1st-person and 3rd-person video dataset to investigate this new problem; and developing algorithms that (2) learn to establish object and human correspondences between the two views; (3) learn object-action relationships across the views; and (4) anonymize the visual data for privacy-preserving visual recognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811845","Analysis of Singularities of the Ricci Flow","DMS","GEOMETRIC ANALYSIS","07/01/2018","05/11/2018","Ovidiu Munteanu","CT","University of Connecticut","Standard Grant","Christopher Stark","06/30/2021","$158,151.00","","ovidiu.munteanu@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","MPS","1265","","$0.00","Differential geometry is the key mathematics in Einstein's theory of relativity. Indeed, Einstein wrote his famous field equations in the language of tensors on manifolds, and these equations have since been well studied by mathematicians and physicists alike.  Certainly, besides being fundamental in general relativity, differential geometry is  also very useful in other fields of science, such as in control theory,  in computer vision, data analysis, and many others. For this reason, understanding the structure of manifolds is a fundamental problem in science. This project will focus on the behavior of geometric flows on manifolds. A typical example of a geometric flow is the heat equation, which describes the distribution of heat in a region over time.  This proposal studies a more advanced form of the heat equation, called the Ricci flow. A Riemannian metric on a manifold tells us about the shape of that object, how to measure angles and distances. The Ricci flow is a heat-type equation for Riemannian metrics. It is hoped, and confirmed in some cases, that the Ricci flow will evolve a given metric on a manifold to an improved one, such as an Einstein metric. However, a major difference between this theory and that of the standard heat equation is that the Ricci flow is a non-linear equation, and as such it usually develops singularities after some time. When such singularities are understood, the process may be continued. This has played a central role in the proof of the long-standing Poincare conjecture about the topology of three dimensional manifolds.  The main goal of this project is to understand such singularities in dimension four, and to investigate the implications of our findings to the structure of four dimensional manifolds.  Because Ricci flow can be seen as the renormalization group flow in string theory, there are other possible applications of this study to theoretical physics. Other related flows, like the mean curvature flow, have further remarkable applications to other fields, such as in computer visualization, for eliminating noise, or in metallurgy, for heat treatment of metals. The outreach components of this project disseminate the results to general public and contribute to the development of young talent.<br/><br/>Ricci flow was introduced by Richard Hamilton in the early eighties, in a fundamental work devoted to understanding positively curved three dimensional manifolds. It became clear later that if one flows an arbitrary metric on a given manifold, the flow will generally develop singularities. One needs to understand these singularities in order to continue the flow, and to not lose any significant topological information about the space. The singularities of Ricci flow are modeled by Ricci solitons, which are fixed points of the flow, modulo diffeomorphisms and scaling. Three-dimensional shrinking Ricci solitons have been classified through the work of Hamilton, Ivey and Perelman. This has important consequences to understanding the behavior of Ricci flow with surgeries on three-dimensional manifolds, and indeed, for the resolution of the Poincare conjecture. The main goal of this project is to classify four-dimensional complete noncompact Ricci solitons. This will be achieved through a complete understanding of the asymptotic geometry of these spaces and through studying corresponding rigidity questions. It is expected that this project will advance our insight on the behavior of Ricci flow in dimension four, which will enable a Ricci flow approach to some important questions about the topology of four dimensional manifolds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833322","I-Corps: Market research for unmanned aircraft system imaging of agricultural fields","IIP","I-Corps","04/01/2018","04/04/2018","Katherine Rainey","IN","Purdue University","Standard Grant","Steven Konsek","03/31/2019","$50,000.00","","krainey@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","8023","","$0.00","The broader impact of this I-Corps project is to promote the use of drones in agriculture and natural resource commercial endeavors and research. Due to scale, logistics, and lack of rural broadband internet access, there is a surprising lack of data in agriculture and agronomic research. At the same time, farmers and agronomists are in one way or another adapting to precision agriculture crop management and need high-resolution data at the landscape scale. This project will drastically reduce the time needed to generate accurate crop growth metrics from drone imagery, and then act on the information. Development of this project will improve data-driven decision-making in agriculture and agronomic research, including plant breeding and sustainable crop management. Data from our technology can be used for drone-based yield prediction for economic and value-chain applications, with information provided sooner than is available from current methods.<br/><br/>This I-Corps project will reduce the cost and time to analyze unmanned aircraft system imagery of crop fields for quantification of metrics describing crop health, growth, and development, proactively during the season. The technology is software and workflows that use techniques from remote sensing, photogrammetry, and computer vision to automatically extract replicate images of research plots and management zones from raw drone imagery of crop fields, instead of relying on expensive and massive image ortho-mosaics and high-grade GPS measurements for plot extraction and analysis. The innovation can be applied in the field by eliminating the need for internet access or high-performance computers. The innovation also provides custom zoning of metrics, and quality control of the data generated. The innovation is an inexpensive method of high-throughput field phenotyping in the plant sciences. The innovation can contribute significantly to early-season yield prediction for a range of decision-making and forecasting applications. Implementation of regular consideration of growth analysis using inexpensive aerial imagery in the seed industry will improve genetic gain and product placement in precision agriculture management zones. Precision agriculture management will be facilitated by real-time, responsive, high-resolution assessments of crop health. Improved crop modeling and yield predictions are economically valuable to many sectors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763795","SHF: Medium: Collaborative Research: Predictive Modeling for Next-generation Heterogeneous System Design","CCF","Special Projects - CCF, Software & Hardware Foundation","10/01/2018","04/17/2020","Philip Brisk","CA","University of California-Riverside","Standard Grant","Sankar Basu","09/30/2021","$338,350.00","","philip@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","2878, 7798","7924, 7945, 9251","$0.00","With semiconductor scaling reaching physical limits, performance and power consumption are ever more critical aspects in the design of emerging computer systems. Fast and accurate design models and tools are critical to support future computer system designers in evaluating design options before they can be built. Traditional simulation-based or analytical models are often too slow or inaccurate to effectively support design processes. This project instead develops novel machine learning-based, predictive methodologies to rapidly estimate the performance and power consumption of future generation products at early design stages using observations obtained on commercially available silicon today. Such techniques will allow efficient design cycles ensuring that the next-generation computing infrastructure meets the needs and expectations of consumers and continues to meet them over the product lifecycle. Along with research activities, course material on predictive modeling will be integrated into the university courses taught by the investigators, technology will be transferred to industrial partners through training and tutorials, and tools and models developed in this project will be released as open source software. In addition to training of graduate students, emphasis will be paid to undergraduate student training, towards including federally recognized under-represented groups, training of STEM teachers, and to run summer code camps to increase access for middle school and high school students. <br/><br/>This project specifically investigates use of advanced machine learning techniques for prediction of power and performance of any machine based on hardware-dependent and independent application characteristics obtained by running on any existing other machine, focusing on large-scale data center and accelerator technologies, namely multi-core CPUs, GPUs and FPGAs. Specific research tasks include the investigation of: (1) fast and accurate models for system designers and system programmers to perform rapid, early hardware and software design space exploration; (2) fast online prediction models that can be integrated into modern operating systems and virtual machine; and (3) fast yet accurate model training procedures that can create new predictive models while applications run. This research is expected to also allow semiconductor companies to better understand the scenarios under which predictive modeling is sufficiently accurate to be deployed during an industrial design process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841810","Nonlinear Manifold Learning of Protein Folding Funnels from Delay-Embedded Experimental Measurements","DMS","MATHEMATICAL BIOLOGY","07/01/2018","08/08/2018","Andrew Ferguson","IL","University of Chicago","Standard Grant","Junping Wang","07/31/2021","$162,000.00","","andrewferguson@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","7334","","$0.00","Proteins are the molecular engines that perform biological functions essential to life. A major milestone in the understanding of protein behavior emerged with the advent of the ""new view"" of protein folding. This perspective conceives of protein structure, stability, and dynamics as governed by a molecular-level landscape not unlike a relief map describing the surface of the Earth. Each point on the landscape - analogous to latitude and longitude - corresponds to a particular spatial arrangement of protein atoms. The altitude of each point on the map defines protein stability - unstable conformations lie on mountaintops and stable conformations within valley floors. Determining these landscapes is a key goal of protein biology since they are useful in the understanding of natural proteins and design of synthetic proteins as drugs, enzymes, and molecular machines. It is relatively straightforward to calculate these landscapes for small proteins using computer simulations, but it has not been possible to do so from experimental measurements. It is the primary objective of this research to combine mathematical tools from the modeling of dynamical systems with machine learning approaches to analyze high-dimensional datasets to determine approximate protein folding landscapes directly from experimental data. The approach will first be validated in computer simulations of small proteins where the folding landscape is known. Theoretical analyses will place bounds on how close the approximate landscapes are to the true landscapes, and place conditions on the experimental data required for their determination. Ultimately the approach will be applied to experimental measurements of a tuberculosis protein. The computational analysis tool will be released as user-friendly software for free public download. Positive research experiences have great benefits for undergraduate success and retention, and this award will support summer and academic year research opportunities. New educational outreach materials will be developed for the University of Illinois ""Engineering Open House"" to promote awareness of materials science and engineering among middle- and high-school students.<br/><br/>The aim of this work is to integrate nonlinear manifold learning with dynamical systems theory to reconstruct protein folding landscapes from experimental time series measuring a single system observable. The ""new view"" of protein folding revolutionized understanding of folding as a conformational search over rugged and funneled free energy landscapes parameterized by a small number of emergent collective variables, with transformative implications for the understanding and design of proteins as drugs, enzymes, and molecular machines. It is now relatively routine to determine multidimensional folding landscapes from computer simulations in which all atomic coordinates are known, but it has not been possible to do so from experimental measurements of protein dynamics that are restricted to small numbers of coarse-grained observables. This research project integrates Takens' delay embeddings with nonlinear manifold learning using diffusion maps to first project univariate time series in an experimentally measurable observable into a high-dimensional space in which the dynamics are C1-equivalent to those in real space, and then extract from this space a topologically and geometrically equivalent reconstruction of the folding funnel to that which would have been determined from knowledge of all atomic coordinates. The reconstructed landscape preserves the topology of the true funnel - the metastable configurations and folding pathways - but the topography may be perturbed, i.e., the heights and depths of the free energy peaks and valleys. The three primary objectives of this work are to (i) validate the approach in molecular dynamics simulations of small proteins for which the true landscape is known, (ii) place conditions on the sampling resolution and signal-to-noise ratio in experimental measurements for robust landscape recovery, and theoretical bounds on the induced topographical perturbations, and (iii) apply the approach to experimental single-molecule Forster resonance energy transfer (smFRET) measurements on the lid-opening and closing dynamics of Mycobacterium tuberculosis protein tyrosine phosphatase (Mtb-PtpB)."
"1730449","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","FET-Fndtns of Emerging Tech, Special Projects - CCF, Expeditions in Computing, Quantum Computing","03/01/2018","06/24/2020","Frederic Chong","IL","University of Chicago","Continuing Grant","Almadena Chtchelkanova","02/28/2023","$3,082,009.00","John Reppy, Diana Franklin, David Schuster","chong@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","089Y, 2878, 7723, 7928","7723, 7928, 9251","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1729369","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","Expeditions in Computing","03/01/2018","09/09/2019","Peter Shor","MA","Massachusetts Institute of Technology","Continuing Grant","Almadena Chtchelkanova","02/28/2023","$1,543,431.00","Edward Farhi, Isaac Chuang, Aram Harrow","shor@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832377","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","Expeditions in Computing","03/01/2018","09/09/2019","Kenneth Brown","NC","Duke University","Continuing Grant","Almadena Chtchelkanova","02/28/2023","$600,000.00","","kenneth.r.brown@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839308","TRIPODS+X:RES: Investigations at the Interface of Data Science and Neuroscience","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, Cognitive Neuroscience, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","09/10/2018","Nicholas Turk-Browne","CT","Yale University","Standard Grant","Christopher Stark","09/30/2021","$599,992.00","Jeffrey Brock, Damon Clark, John Lafferty","nicholas.turk-browne@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","041Y, 1253, 1699, 8624","047Z, 062Z, 8089, 8091","$0.00","This project will build a transformative bridge between data science and neuroscience. These two young fields are driving cutting-edge progress in the technology, education, and healthcare sectors, but their shared foundations and deep synergies have yet to be exploited in an integrated way - a new discipline of ""data neuroscience."" This integration will benefit both fields: Neuroscience is producing massive amounts of data at all levels, from synapses and cells to networks and behavior. Data science is needed to make sense of these data, both in terms of developing sophisticated analysis techniques and devising formal, mathematically rigorous theories. At the same time, models in data science involving AI and machine learning can draw insights from neuroscience, as the brain is a prodigious learner and the ultimate benchmark for intelligent behavior. Beyond fundamental scientific gains in both fields, the project will produce additional outcomes, including: new collaborations between universities, accessible workshops, graduate training, integration of undergraduate curricula in data science and neuroscience, research opportunities for undergraduates that help prepare them for the STEM workforce, academic-industry partnerships, and enhanced high-performance computing infrastructure.<br/><br/>The overarching theme of this project is to develop a two-way channel between data science and neuroscience. In one direction, the project will investigate how computational principles from data science can be leveraged to advance theory and make sense of empirical findings at different levels of neuroscience, from cellular measurements in fruit flies to whole-brain functional imaging in humans. In the reverse direction, the project will view the processes and mechanisms of vision and cognition underlying these findings as a source for new statistical and mathematical frameworks for data analysis. Research will focus on four related objectives: (1) Distributed processing: reconciling work on communication constraints and parallelization in machine learning with the cellular neuroscience of motion perception to develop models of distributed estimation; (2) Data representation: examining how our understanding of the different ways that the brain stores information can inform statistically and computationally efficient learning algorithms in the framework of exponential family embeddings and variational inference; (3) Attentional filtering: incorporating the cognitive concept of selective attention into machine learning as a low-dimensional trace through a high-dimensional input space, with the resulting models used to reconstruct human subjective experience from brain imaging data; (4) Memory capacity: leveraging cognitive studies and natural memory architectures to inform approaches for reducing/sharing memory in artificial learning algorithms. The inherently cross-disciplinary nature of the project will provide novel theoretical and methodological perspectives on both data science and neuroscience, with the goal of enabling rapid, foundational discoveries that will accelerate future research in these fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730088","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","Expeditions in Computing","03/01/2018","09/09/2019","Danielle Harlow","CA","University of California-Santa Barbara","Continuing Grant","Almadena Chtchelkanova","02/28/2023","$210,888.00","","dharlow@education.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827668","PFI-TT: Creation of an augmented-reality platform for treatment of phobias/anxiety in children with Autism Spectrum Disorder","IIP","PFI-Partnrships for Innovation","08/01/2018","08/15/2018","Zhandong Liu","TX","Baylor College of Medicine","Standard Grant","Kaitlin Bratlie","01/31/2021","$199,791.00","Jack Dempsey","zhandonl@bcm.edu","ONE BAYLOR PLAZA","HOUSTON","TX","770303411","7137981297","ENG","1662","1662, 8042","$0.00","The broader impact/commercial potential of this PFI project will be in enabling future technology-based therapies for Specific Phobia (SP) in both the general population and individuals with Autism Spectrum Disorder (ASD). Nearly 9% of US population meets criteria for SP and very few seek treatment. Use of augmented reality (AR) could, in the future, reduce barriers to seeking treatment and also improve the health provider's ability to effectively and efficiently deliver evidence-based therapy. Costs of delivering 'minimally adequate treatment' for SP can range from $600-$900 per patient and can be much higher for payers, patients, and health providers. Fortunately, augmented reality would allow the advantages of providing therapy in a virtual environment with lower costs and greater flexibility of options for the healthcare provider.<br/><br/><br/>The proposed project is an AR platform that allows individuals with ASD as well as those within the general population to overcome debilitating fears characteristic of SP. Despite the availability of highly-effective treatments, the majority of individuals experiencing debilitating fears characterizing SP will never receive treatment, particularly those with ASD. The AR platform created through this grant will, in the future, reduce the high rejection rate of traditional exposure therapy as well as encourage individuals to seek treatment who would not have done so before. Adaptive learning and computer vision technologies will be used to enhance the efficacy and usability of the AR platform.  The  project will involve development of a prototype platform allowing individuals to observe a computer generated phobic stimuli through an AR device.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848835","I-Corps Team Proposal ""Mini Signal""","CNS","I-Corps","09/15/2018","08/23/2018","Min Wu","MD","University of Maryland College Park","Standard Grant","Pamela McCauley","09/30/2019","$50,000.00","Chau-Wai Wong","minwu@eng.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","8023","","$0.00","The broader impact/commercial potential of this I-Corps project lies in three aspects: through a systematic customer discovery process, it enhances the R&D community's understanding of the potentially broad needs and application spaces of the contact-less physiological monitoring. For the robust heart-rate tracking technologies that this I-Corps project is based on, the potential commercial impact includes monitoring drivers' conditions to enhance driving safety, supporting safe and effective athletic/fitness training, offering additional analytics from surveillance video for public safety, and more. The outcome of the I-Corps customer discovery will provide real-world requirements and data to a new round of R&D and enhance the R&D community's scientific and technological understanding.  The I-Corps training and experience will prepare the Team to identify and pave a way to the most effective customer-technology path for commercialization, which will lead to an economic and societal impact.  The training would also provide valuable insights to the Technical Lead and Co-Lead, who as faculty members can potentially incorporate the Lean Startup and entrepreneurial thinking into their future education and mentoring activities. This would extend the benefits to many students in the years to come.<br/><br/>This I-Corps project builds on the research advances of contact-free sensings, such as through video, to capture the very small color changes of a person's face as his/her heart pumps blood to reach the whole body.  The team has developed a strong expertise to harvest such micro signals under a number of scenarios and motivating applications. From the patent-pending research on which this I-Corps project is based, accurate heart rate detection and tracking have been achieved even under significant motion as users move and lighting conditions change, which is common in real-world use.  The synergy of multiple traditionally separate fields, including image/video processing, computer vision, statistical signal processing and estimation, paved a foundation to address the R&D challenges.  The methodologies learned from the I-Corps program serve as a bridge between the theory and practice of micro signal analytics in the important emerging area of health analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752116","CAREER: Sparse Model Selection for Nonlinear Evolution Equations","DMS","COMPUTATIONAL MATHEMATICS","06/01/2018","06/22/2020","Hayden Schaeffer","PA","Carnegie-Mellon University","Continuing Grant","Leland Jameson","05/31/2023","$211,260.00","","hschaeff@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1271","1045, 9263","$0.00","Extracting information from stationary and/or dynamic data is an important task in many scientific and industrial problems; including but not limited to, machine learning, data mining, image processing, and automated analysis of scientific data. This project focuses on learning the underlying process that generates observational data, in a sense, ""reverse-engineering"" models from data. These models are often used to gain insights on the data (for example, determining mathematical principles from experimental observations) or to make data-enabled decisions (for example, trend prediction). This is a challenging mathematical and computational problem, since one often has limited information on the process beforehand and real data is often noisy and/or incomplete. The research objective is to construct efficient computational methods for learning generating functions. This will involve a variety of mathematical techniques centered around optimization and sampling theory. The educational objective is to provide advanced training to undergraduate and graduate students in order to prepare them for the U.S. STEM workforce. In particular, students will be mentored and trained through mathematical and computational research projects, collaborative summer programs, working groups, and advanced courses that integrate education and research.<br/><br/>The goal is to develop computational methods for model learning, data analysis, and other machine learning tasks.  The overall objectives include: (i) the construction of optimization models that use sparsity, smoothness, and randomness to supplement the learning, (ii) the design of efficient and provably convergent numerical methods, (iii) the development of methods that are robust to sample size and outliers, and (iv) the creation and implementation of activities for undergraduate and graduate students that integrate education and research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839816","EAGER: Real-Time: Learning-Mediated Control for Traffic Shaping","ECCS","EPCN-Energy-Power-Ctrl-Netwrks, EFRI Research Projects","10/01/2018","09/17/2018","Nicholas Duffield","TX","Texas A&M Engineering Experiment Station","Standard Grant","Lawrence Goldberg","09/30/2021","$300,000.00","Krishna Narayanan, Srinivas Shakkottai, Alireza Talebpour","duffieldng@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","7607, 7633","152E, 1653, 7916","$0.00","Efficient Management of Vehicular Traffic via Real-time Machine-Learning-Mediated Control and Traffic Shaping<br/><br/>While connectivity and automation promise orders of magnitude gains in the safety and efficiency of vehicular transportation networks, these gains cannot be realized without monitoring, learning the behavior, and control of vehicles at different aggregation levels. Indeed, current congestion mitigation methods, such as speed harmonization that uses a sequence of variable speed limits along a highway do not reliably control congestion, and may exacerbate it (e.g., via shocks propagated through speed limit changes) due to the inconsistency between congestion prediction and real-time control. The objective of this project is to develop a holistic approach using machine-learning methods to identify and predict macroscopic congestion behavior of traffic based on both vehicle-borne and transportation infrastructure measurements, while designing fine-grain control systems for individual vehicles that can help to mitigate congestion effects.  In doing so, the project recognizes that these designs must account for the possibility of low take-up rates of connected, automated vehicles (CAVs) over the next decade, and the consequent dominance of human-mediated vehicle operation for some time to come.  The project also includes the development of educational materials on data analytics and vehicular control systems. Intrinsic to the program are efforts at outreach to involve high-school students via demonstrations and lectures based on the technology developed.<br/><br/>The goal of this project is to develop the theory of and evaluate a novel approach to traffic management entitled ""real-time learning-mediated control"".  The key idea is to meld large-scale real-time learning about macroscopic phenomena in a physically interpretable manner, with distributed dynamic control of individual vehicles in a provably safe and efficient manner. The work comprises two thrusts, namely (i) Traffic State Prediction, which offers a Graph Signal Processing (GSP)-based congestion prediction approach for planned and unplanned congestion-causing events, and (ii) Traffic Shaping and Control, which offers novel vehicular control methods that shape traffic in a stable manner over the multiple dimensions of target time headway and velocities over space and time, and candidate time-gap and velocity profiles in a mixed environment of both connected, automated vehicles and human driven ones.  Thus, the overall aim is to combine the ability of learning methods to provide predictions about complex interconnected systems, with control laws that are safe and consistent with the laws of physics.   The value of this research to broader society is in combining traffic prediction, control and learning, which can result in accurate congestion mitigation and increased throughput.   Incorporating analytical concepts into senior design projects and courses enhances the project via educational impact. The project also contributes to development of systems-design expertise for students, as well as to diversity enhancement through minority student engagement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1912474","NCS-FO: Integrating Non-Invasive Neuroimaging and Educational Data Mining to Improve Understanding of Robust Learning Processes","DGE","ECR-EHR Core Research","09/01/2018","07/30/2019","Erin Walker","PA","University of Pittsburgh","Standard Grant","Gregg Solomon","08/31/2021","$351,509.00","","eawalker@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","EHR","7980","8089, 8091, 8212, 8244, 8551, 8817","$0.00","From elementary school math games to workplace training, computer-based learning applications are becoming more widespread. With these programs, it becomes increasingly possible to use the data generated, such as correct and incorrect problem-solving responses, to develop ways to test for student knowledge and to personalize instruction to student needs. The logs of student responses can capture answers, but they fail to capture critical information about what is happening during pauses between student interactions with the software. This project, led by a team of researchers at Arizona State University and Worcester Polytechnic Institute, will explore the use of measurements of brain activity from lightweight brain sensors alongside student log data to understand important mental activities during learning. The study will examine developmental math learning in college and community college students using the ASSISTments intelligent tutoring system. Using brain imaging, the project team will examine whether students are thinking deeply about the problem or mind-wandering during pauses in the learning tasks and use the combined log and brain data to make predictions about learning outcomes. This work will build a foundation for new methods of combining neuroimaging, machine learning, and personalized learning environments. With a better understanding of when and how learning occurs during pauses in tutoring system use, learning technology researchers and developers will be able to create adaptive interventions within tutoring systems that are better personalized to the needs of the individual. This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE).<br/><br/>This project has of three goals: 1) Integrating multiple data streams for the creation of an interdisciplinary corpus; 2) Detecting real-time changes in cognitive states during pauses in log data; and 3) Predicting learning outcomes from brain-based and log-based inferences of cognitive states. In addressing these goals, the team will collect brain data, using functional near-infrared spectroscopy neuroimaging, and behavioral data from controlled, well-understood tasks related to rule learning and mind wandering and from authentic learning tasks. Cognitive neuroscience research involving recordings of brain activity traditionally requires paradigms with highly constrained stimuli, timing, and task requirements, whereas research in complex real-world environments such as tutoring systems rarely align with these paradigms. Features of the brain activity during the cognitive tasks will be used to make inferences about student cognition during authentic learning tasks. In addition, brain features will be combined with log data features to create machine learning models that make accurate predictions of student robust learning outcomes, to be assessed using a posttest given after students use the interactive learning environment. Contributions of this project to STEM learning will include improved understanding of how students build knowledge in response to instructional events within digital learning environments, the construction of better predictive models of when students learn from the use of personalized learning environments, and a mapping between learning processes and the length and context of pauses. This project will also contribute to understandings of how to combine analyses of neuroimaging data and log data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1908166","III: Small: Collaborative Research: Structured Methods for Multi-Task Learning","IIS","Info Integration & Informatics","10/16/2018","12/18/2018","Shuiwang Ji","TX","Texas A&M Engineering Experiment Station","Standard Grant","Wei Ding","07/31/2020","$178,564.00","","sji@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7364","7364, 7923","$0.00","The ability of human to learn from and transfer knowledge across related learning tasks enables us to grasp complex concepts from only a few examples. For instance, a three-year old child is able to discriminate chairs from tables without having been exposed to hundreds of different examples. In contrast, computer learning programs typically require training on a large number of examples in order to achieve similar levels of recognition. This prompts the study of multi-task learning in which multiple related tasks are learned simultaneously, thereby facilitating inter-task knowledge transfer. However, most multi-task learning studies are restricted to problems with well-defined tasks and structures. This project aims at developing algorithms and tools (including open source software) to attack problems that are not traditionally treated, but can potentially be reformulated and solved more effectively by multi-task learning. This allows a broad class of challenging machine learning problems to benefit from multi-task learning techniques. This project also develops a new curriculum that incorporates the proposed research into classroom. In addition, this project will allow the PIs to continue the ongoing efforts of actively recruiting and advising students from under-represented groups. <br/><br/>To achieve these goals, this project focuses on an innovative, integrated research and education plan that includes the following components: (1) providing principled guidelines for reformulating problems into the multi-task learning formalism; (2) developing robust and clustered multi-task learning models to identify and prevent false interactions among unrelated tasks; (3) developing sparsity-inducing multi-task learning models to capture richly structured task interactions; (4) developing high-order multi-task learning models to capture task relatedness from interactions between features; and (5) investigating computational algorithms and theoretical properties of multi-task learning. The outcome of this project includes the capabilities of reformulating diverse machine learning problems into the multi-task learning framework and providing radically new ways to attack challenging problems that cannot be solved effectively by traditional methods. The systematic study of multi-task learning in this project is expected to generate novel reformulations, structured mathematical models, efficient optimization algorithms, and principled theoretical analyses, which will lead to significant practical and theoretical advances in multi-task learning."
"1755884","CRII: SaTC: Image Publication with Differential Privacy","CNS","CRII CISE Research Initiation","06/01/2018","03/07/2018","Liyue Fan","NY","SUNY at Albany","Standard Grant","Nina Amla","09/30/2019","$174,967.00","","liyue.fan@uncc.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","026Y","025Z, 8228, 9102","$0.00","The publication of image data captured by ubiquitous surveillance devices, such as traffic cameras and security surveillance cameras, would greatly benefit various communities and enable many applications. However, sharing image data with untrusted parties would raise privacy concern due to potential sensitive content, like identities and activities that may be in the images.  Standard image obfuscation techniques, such as pixelation and blurring, do not provide effective privacy preservation for people or objects represented in the data.   The goal of this project is to quantitatively define the notion of privacy in image data and develop image publication solutions to achieve rigorous privacy guarantees.  By formally modeling image privacy, this project promises significant impact in enabling image data sharing with a wide range of recipients while ensuring individual privacy. <br/><br/>This project develops rigorous privacy notions based on differential privacy for image data, while accounting for the representation of sensitive content in images and effective image obfuscation algorithms to guarantee privacy, and incorporates widely adopted feature extraction techniques in computer vision. The investigator will evaluate the utility and efficiency of the obfuscation algorithms, including the feasibility for popular image processing applications such as crowd counting and object recognition. The project also includes educational activities for K-12 students and involvement of women and minorities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755772","CRII: SaTC: Enhancing Mobile App Security by Detecting Icon-Behavior Contradiction","CNS","CRII CISE Research Initiation, Special Projects - CNS","08/01/2018","04/22/2019","Xusheng Xiao","OH","Case Western Reserve University","Standard Grant","Sol Greenspan","07/31/2021","$190,923.00","","xusheng.xiao@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","026Y, 1714","025Z, 8228, 9178, 9251","$0.00","Mobile applications (i.e., apps) are becoming critical parts in our daily life. While these apps provide better customized services using users' personal data, certain behavior of the apps is less than desirable or harmful. For example, if an app's user interface (UI) has no texts or images to indicate that it will access users' personal data (e.g., GPS data), but the app discloses users' personal data when an action is performed (e.g., pressing a button), then red flags should be raised. Thus, it is crucial to understand the intents of the app to determine whether the app will perform within the user's expectation. Various research efforts have been dedicated to understand apps' intents via analyzing the semantics of texts in UI. However, images, especially icons, remain unexplored. In apps' UIs, icons are often used in interactive widgets (e.g., buttons) to express the intents to use sensitive data. It is often difficult to analyze the semantics of icons due to the varieties in image styles and the lack of descriptive texts.<br/><br/>The proposed research will build a knowledge base of icons' semantics via collecting icons from apps in major smartphone markets, and develop a framework to infer the semantics of icons based on the collected icons. More specifically, the PI proposes to adapt computer vision techniques to develop icon recognition techniques that identify similar icons based on the collected icons, and leverage program analysis techniques to check the compatibility between the icons and the program behaviors. Furthermore, this research will combine the semantics of both texts and icons to better detect undesired behavior in apps. The proposed research in understanding apps' intents improves mobile app security, which will have tremendous economical impact on society due to our increasing reliance on mobile apps. The proposed techniques will also benefit the security analysis of other event-driven GUI software applications, such as desktop applications, wearable apps, and web apps.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831347","SCC: Data-Informed Scenario Planning for Mobility Decision Making in Resource Constrained Communities","SES","S&CC: Smart & Connected Commun, Secure &Trustworthy Cyberspace","09/01/2018","07/08/2020","Jerome Lynch","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sara Kiesler","08/31/2022","$1,399,861.00","Pascal Van Hentenryck, Tierra Bills, Robert Goodspeed","jerlynch@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","SBE","033Y, 8060","042Z, 062Z, 7434","$0.00","The rapid emergence of new information and sensing technologies is empowering the formation of smart and connected communities (S&CC). This project aims to advance the use of smart and connected technologies to empower new modes of community-based decision making to identify and implement transformative solutions to community challenges. The project focuses on resource-constrained communities. The team will offer the community of Benton Harbor, Michigan, tools needed to explore new mobility solutions that provide greater access to employment, education, and healthcare. The project deploys sensing technologies to collect data needed to create analytical models of resident mobility preferences and mobility service performance. A community-based decision making framework will be created using scenario planning methods; in this framework, stakeholders are provided tools to explore mobility solutions with predicted outcomes visualized. Included in the team is the Twin Cities Area Transportation Authority (TCATA), which will iteratively implement mobility solutions originating from the scenario planning process with solutions quantitatively assessed. A partnership with Lake Michigan College further enhances the project's broader impacts by engaging community college students in the research and offering them experiences in the smart city field of study.<br/><br/>To explore the fundamental question of how resourced-constrained communities can utilize smart and connected technologies to implement novel but lean solutions to mobility challenges, the project will define a cost-effective data collection strategy that can assess the performance of existing solutions, track the mobility patterns of residents, and acquire resident perceptions of their mobility. GPS tracking using cell phones apps and computer vision on city buses will be used to generate the data needed to model the performance of current mobility configurations. Surveys of residents will augment these data sources. The project will map mobility data to an analytical framework that can predict both resident demand for mobility services and the performance of these services given changes in user demand. Activity-based models will be created with special emphasis on fine-grain estimation of travel demand in small communities. Predictive models will be developed to predict the quality of transit services provided by configurations of the mobility network. A key advancement will be the creation of scalable computational methods that optimize the mix of fixed route service with on-demand shuttling. This project will enable community-based decision making by visualizing mobility data and predictive outputs during a participatory planning process. The team will also provide TCATA with the ability to track and iteratively shape public transportation in order to enhance access to employment, healthcare, and education outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838200","BigData:IA:Collaborative Research: TIMES: A tensor factorization platform for spatio-temporal data","IIS","Big Data Science &Engineering","10/01/2018","08/05/2019","Joyce Ho","GA","Emory University","Continuing Grant","Wei-Shinn Ku","09/30/2022","$950,337.00","Li Xiong","joyce.c.ho@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","8083","062Z, 8083, 9102","$0.00","Spatio-temporal analyses can enable many discoveries including reducing traffic congestion, identifying hotspot areas to deploy mobile clinics, and urban planning. Unfortunately, the data poses many computational challenges.  Standard assumptions in machine learning and data mining algorithms are violated by the complex nature of spatio-temporal data.  These include spatial and temporal correlation of observations, dynamic and abrupt changes in observations, variability in measurements with respect to length and frequency, and multi-sourced data that spans multiple sources of information. In recognition of these challenges, various efforts have been undertaken to develop specialized spatiotemporal models. Yet, to date, these algorithms are predominately designed to analyze small- to medium-sized datasets. The goal of this project is to develop a comprehensive computational tensor platform to perform automated, data-driven discovery from spatio-temporal data across a broad range of applications. The project also includes a set of integrated educational activities such as a Massive Open Online Course that covers cross-disciplinary topics at the confluence of computer science and geospatial applications, annual spatio-temporal data challenges and hackathons, and an annual event at the Atlanta Science Festival to create public awareness and encourage participation by women and minorities.<br/><br/>The project will contain algorithmic innovations that reflect appropriate assumptions of spatio-temporal data without sacrificing real-time performance, computational scalability, and cross-site learning even under privacy constraints. The proposed platform will generalize tensor modeling to encompass the complex nature of spatio-temporal data including time irregularity, spatiotemporal correlations, and evolving distributions. It will enable the integration of multi-sourced data from heterogeneous sources to yield robust and cohesive learned patterns. The novel algorithms will also facilitate learning in decentralized settings while preserving privacy. The computational platform will contain interchangeable modules that can adapt to new spatio-temporal settings and incorporate additional contextual information.  The accompanying suite of algorithms will enable predictive learning, pattern mining, and change detection from large-sized spatio-temporal data.  The broad applicability of the project will be demonstrated on a diverse range of data including urban transportation services, real estate market transactions, and population health. The algorithmic innovations introduced can be used to scale other machine learning models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838042","BigData:IA:Collaborative Research: TIMES: A tensor factorization platform for spatio-temporal data","IIS","Big Data Science &Engineering","10/01/2018","09/10/2018","Jimeng Sun","GA","Georgia Tech Research Corporation","Standard Grant","Wei-Shinn Ku","07/31/2020","$773,528.00","","jimeng@illinois.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8083","062Z, 8083","$0.00","Spatio-temporal analyses can enable many discoveries including reducing traffic congestion, identifying hotspot areas to deploy mobile clinics, and urban planning. Unfortunately, the data poses many computational challenges.  Standard assumptions in machine learning and data mining algorithms are violated by the complex nature of spatio-temporal data.  These include spatial and temporal correlation of observations, dynamic and abrupt changes in observations, variability in measurements with respect to length and frequency, and multi-sourced data that spans multiple sources of information. In recognition of these challenges, various efforts have been undertaken to develop specialized spatiotemporal models. Yet, to date, these algorithms are predominately designed to analyze small- to medium-sized datasets. The goal of this project is to develop a comprehensive computational tensor platform to perform automated, data-driven discovery from spatio-temporal data across a broad range of applications. The project also includes a set of integrated educational activities such as a Massive Open Online Course that covers cross-disciplinary topics at the confluence of computer science and geospatial applications, annual spatio-temporal data challenges and hackathons, and an annual event at the Atlanta Science Festival to create public awareness and encourage participation by women and minorities.<br/><br/>The project will contain algorithmic innovations that reflect appropriate assumptions of spatio-temporal data without sacrificing real-time performance, computational scalability, and cross-site learning even under privacy constraints. The proposed platform will generalize tensor modeling to encompass the complex nature of spatio-temporal data including time irregularity, spatiotemporal correlations, and evolving distributions. It will enable the integration of multi-sourced data from heterogeneous sources to yield robust and cohesive learned patterns. The novel algorithms will also facilitate learning in decentralized settings while preserving privacy. The computational platform will contain interchangeable modules that can adapt to new spatio-temporal settings and incorporate additional contextual information.  The accompanying suite of algorithms will enable predictive learning, pattern mining, and change detection from large-sized spatio-temporal data.  The broad applicability of the project will be demonstrated on a diverse range of data including urban transportation services, real estate market transactions, and population health. The algorithmic innovations introduced can be used to scale other machine learning models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749940","CAREER: Harness the Big Data via Large-Scale Lifelong Learning","IIS","Info Integration & Informatics","08/01/2018","06/11/2020","Jiayu Zhou","MI","Michigan State University","Continuing Grant","Wei Ding","07/31/2023","$433,151.00","","dearjiayu@gmail.com","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7364","1045, 7364","$0.00","Recent advances in big data infrastructures and algorithm foundations have unleashed a torrent of data being collected and stored in distributed data centers all over the world. The ever-increasing availability of these massive datasets leads to many machine learning tasks that are inherently related. Therefore transfer learning paradigms have been developed in the past decade to perform knowledge transfer among tasks to improve their generalization performance. This project will develop a suite of large-scale lifelong learning methods to address significant challenges from knowledge transfer on big data. The algorithms and tools developed in this project will directly impact biomedical informatics and intelligent transportation systems, as they will be used to build personalized predictive models from electronic medical records and traffic state models from big traffic data. The success of this project will be used to develop a new curriculum that incorporates research into the classroom and provides students from under-represented groups with opportunities to participate in machine learning research. <br/><br/>The properties of velocity, volume, variability, and variety that characterize big data have imposed significant challenges in the traditional lifelong learning approaches. This project will advance lifelong learning by (1) developing a distributed life-long learning framework to enable online knowledge transfer on large-scale distributed datasets; (2) designing effective methods to track temporal drifting in the task relationship, and leverage human knowledge via interactive transfer; and (3) investigating strategies that enable the distributed life-long learning to handle heterogeneities from both feature spaces and learning tasks. The results of this project will have an immediate and strong impact on Big Data theoretical and algorithmic foundations, by enabling a large-scale lifelong learning framework readily available for many Big Data analytics. All findings, publications, software, and data will be made publicly available at the project website: http://jiayuzhou.github.io/projects/career.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763848","SHF: Medium: Collaborative Research: Predictive Modeling for Next-generation Heterogeneous System Design","CCF","Software & Hardware Foundation","10/01/2018","08/23/2018","Andreas Gerstlauer","TX","University of Texas at Austin","Standard Grant","Sankar Basu","09/30/2021","$679,411.00","Lizy John","gerstl@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7924, 7945","$0.00","With semiconductor scaling reaching physical limits, performance and power consumption are ever more critical aspects in the design of emerging computer systems. Fast and accurate design models and tools are critical to support future computer system designers in evaluating design options before they can be built. Traditional simulation-based or analytical models are often too slow or inaccurate to effectively support design processes. This project instead develops novel machine learning-based, predictive methodologies to rapidly estimate the performance and power consumption of future generation products at early design stages using observations obtained on commercially available silicon today. Such techniques will allow efficient design cycles ensuring that the next-generation computing infrastructure meets the needs and expectations of consumers and continues to meet them over the product lifecycle. Along with research activities, course material on predictive modeling will be integrated into the university courses taught by the investigators, technology will be transferred to industrial partners through training and tutorials, and tools and models developed in this project will be released as open source software. In addition to training of graduate students, emphasis will be paid to undergraduate student training, towards including federally recognized under-represented groups, training of STEM teachers, and to run summer code camps to increase access for middle school and high school students. <br/><br/>This project specifically investigates use of advanced machine learning techniques for prediction of power and performance of any machine based on hardware-dependent and independent application characteristics obtained by running on any existing other machine, focusing on large-scale data center and accelerator technologies, namely multi-core CPUs, GPUs and FPGAs. Specific research tasks include the investigation of: (1) fast and accurate models for system designers and system programmers to perform rapid, early hardware and software design space exploration; (2) fast online prediction models that can be integrated into modern operating systems and virtual machine; and (3) fast yet accurate model training procedures that can create new predictive models while applications run. This research is expected to also allow semiconductor companies to better understand the scenarios under which predictive modeling is sufficiently accurate to be deployed during an industrial design process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713545","Multimodal Visitor Analytics: Investigating Naturalistic Engagement with Interactive Tabletop Science Exhibits","DRL","AISL","03/01/2018","04/27/2020","James Lester","NC","North Carolina State University","Continuing Grant","Julie Johnson","02/28/2021","$1,951,956.00","James Minogue, Jonathan Rowe","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","EHR","7259","8244","$0.00","Engagement is the cornerstone of learning in informal science education. During free-choice learning in museums and science centers, visitor engagement shapes how learners interact with exhibits, navigate through exhibit spaces, and form attitudes, interests, and understanding of science. Recent advances in multimodal learning analytics are creating novel opportunities for expanding the range and richness of measures of visitor engagement in free-choice settings. In particular, multimodal learning analytics offer significant potential for integrating multiple data sources to devise a composite picture of visitors' cognitive, affective, and behavioral engagement. The project will center on providing a rich empirical account of meaningful visitor engagement with interactive tabletop science exhibits among individual visitors and small groups, as well as uncovering broader tidal patterns in visitor engagement that unfold across exhibit spaces. A key objective of the project is creating models and practitioner-focused learning analytic tools that will inform the best practices of exhibit designers and museum educators. This project is funded by the Advancing Informal STEM Learning (AISL) program.  As part of its overall strategy to enhance learning in informal environments, AISL funds research and innovative approaches and resources for use in a variety of settings.<br/><br/>The research team will conduct data-rich investigations of visitors' learning experiences with multimodal learning analytics that fuse the rich multichannel data streams produced by fully-instrumented exhibit spaces with the data-driven modeling functionalities afforded by recent advances in machine learning and educational data mining. The research team will conduct a series of visitor studies of naturalistic engagement in solo, dyad, and group interactions as visitors explore interactive tabletop science exhibits. The studies will utilize eye trackers to capture visitors' moment-to-moment attention, facial expression analysis and quantitative field observations to track visitors' emotional states, trace logs generated by exhibit software, as well as motion-tracking sensors and coded video recordings to capture visitors' behavioral interactions. The studies will also use conversation recordings and pre-post assessment measures to capture visitors' science understanding and inquiry processes. With these multimodal data streams as training data, the research team will use probabilistic and neural machine learning techniques to devise learning analytic models of visitor engagement. <br/><br/>The project will be conducted by a partnership between North Carolina State University and the North Carolina Museum of Natural Sciences. The research team will 1) design a data-rich multimodal visitor study methodology, 2) create the Visitor Informatics Platform, a suite of open source software tools for multimodal visitor analytics, and 3) launch the Multimodal Visitor Data Warehouse, a curated visitor experience data archive. Together, the multimodal visitor study methodology, the Visitor Informatics Platform, and the Multimodal Visitor Data Warehouse will enable researchers and practitioners in the informal science education community to utilize multimodal learning analytics in their own informal learning environments. It is anticipated that the project will advance the field of informal STEM learning by extending and enriching measures of meaningful visitor engagement, expanding the evidence base for visitor experience design principles, and providing learning analytic tools to support museum educators. By enhancing understanding of the cognitive, affective, and behavioral dynamics underlying visitor experiences in science museums, informal science educators will be well-positioned to design learning experiences that are more effective and engaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839371","TRIPODS+X:RES: Safe Imitation Learning for Robotics","DMS","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, IIS Special Projects","10/01/2018","09/10/2018","Zaid Harchaoui","WA","University of Washington","Standard Grant","Huixia Wang","09/30/2021","$600,000.00","Maryam Fazel, Sham Kakade, Siddhartha Srinivasa","zaid@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","041Y, 1253, 7484","047Z, 062Z","$0.00","Learning algorithms are now pervasively deployed in robotic systems. However, safe learning procedures with high-probability theoretical guarantees on the acceptability of the predictions have been far less studied, especially for robotic systems trained with data collected from experts and making decisions sequentially. The PIs shall bring together ideas and techniques from statistics, machine learning, and mathematical optimization, to design the next generation of imitation learning approaches with provable safety guarantees for several classes of modern robots that interact with humans.   <br/><br/>The project aims to: (1) develop new formulations of safe imitation learning; (2) design fast learning algorithms with theoretical guarantees on safety; (3) explore trust-building processes for beneficial human-machine interaction. The research approaches will be evaluated in several robotic problem domains, including robotic manipulation and wheeled mobile robot navigation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835307","NCS-FO: Integrating Non-Invasive Neuroimaging and Educational Data Mining to Improve Understanding of Robust Learning Processes","DGE","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","09/01/2018","07/25/2019","Erin Solovey","MA","Worcester Polytechnic Institute","Standard Grant","Gregg Solomon","08/31/2021","$680,167.00","","esolovey@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","EHR","7980, 8624","8089, 8091, 8212, 8244, 8551, 8817","$0.00","From elementary school math games to workplace training, computer-based learning applications are becoming more widespread. With these programs, it becomes increasingly possible to use the data generated, such as correct and incorrect problem-solving responses, to develop ways to test for student knowledge and to personalize instruction to student needs. The logs of student responses can capture answers, but they fail to capture critical information about what is happening during pauses between student interactions with the software. This project, led by a team of researchers at Arizona State University and Worcester Polytechnic Institute, will explore the use of measurements of brain activity from lightweight brain sensors alongside student log data to understand important mental activities during learning. The study will examine developmental math learning in college and community college students using the ASSISTments intelligent tutoring system. Using brain imaging, the project team will examine whether students are thinking deeply about the problem or mind-wandering during pauses in the learning tasks and use the combined log and brain data to make predictions about learning outcomes. This work will build a foundation for new methods of combining neuroimaging, machine learning, and personalized learning environments. With a better understanding of when and how learning occurs during pauses in tutoring system use, learning technology researchers and developers will be able to create adaptive interventions within tutoring systems that are better personalized to the needs of the individual. This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE).<br/><br/>This project has of three goals: 1) Integrating multiple data streams for the creation of an interdisciplinary corpus; 2) Detecting real-time changes in cognitive states during pauses in log data; and 3) Predicting learning outcomes from brain-based and log-based inferences of cognitive states. In addressing these goals, the team will collect brain data, using functional near-infrared spectroscopy neuroimaging, and behavioral data from controlled, well-understood tasks related to rule learning and mind wandering and from authentic learning tasks. Cognitive neuroscience research involving recordings of brain activity traditionally requires paradigms with highly constrained stimuli, timing, and task requirements, whereas research in complex real-world environments such as tutoring systems rarely align with these paradigms. Features of the brain activity during the cognitive tasks will be used to make inferences about student cognition during authentic learning tasks. In addition, brain features will be combined with log data features to create machine learning models that make accurate predictions of student robust learning outcomes, to be assessed using a posttest given after students use the interactive learning environment. Contributions of this project to STEM learning will include improved understanding of how students build knowledge in response to instructional events within digital learning environments, the construction of better predictive models of when students learn from the use of personalized learning environments, and a mapping between learning processes and the length and context of pauses. This project will also contribute to understandings of how to combine analyses of neuroimaging data and log data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819367","SBIR Phase I:  Project Trident: A Three-Pronged Automated Solution to Satellite Derived Littoral Bathymetry Mapping","IIP","SBIR Phase I","06/15/2018","06/15/2018","Kyle Goodrich","CO","TCarta Marine, LLC","Standard Grant","Rick Schwerdtfeger","05/31/2019","$223,655.00","","kg@tcarta.com","3015 W 9th Ave","Denver","CO","802043109","3032846144","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be the establishment of the analytical framework and software for a global bathymetry mapping system based upon freely available satellite imagery, three independent extraction methods, and ongoing data collection to create geospatial products for a wide range of applications.? Shallow water zones are crucial to the world?s ecology, economy, and culture.  An estimated 70% of the world?s littoral zone is not mapped to modern standards, and traditional methods of mapping these areas are prohibitively expensive and dangerous. Through remote sensing techniques, this innovation will produce detailed bathymetric data for shallow water areas around the globe at far lower costs than traditional methods. This is particularly significant for impoverished remote, low-lying islands which have scarce resources for mapping the natural environment on which their economies heavily rely. ?Additionally, shallow water bathymetry data is vital to geospatial intelligence and to accurately assessing the feasibility and environmental impact of projects relating to global trade/port development, aquaculture, tourism, and resource development.? The results of this innovation will provide a modern baseline of seabed topography for future scientific analysis, change detection, and greater understanding of the marine environment.<br/><br/>The proposed project addresses the budgetary and health and safety challenges of nearshore bathymetry mapping by advancing three distinct methods of calculating satellite derived bathymetry. These methods rely on different parameters of measurement for water depth retrieval, enabling the advantages of one method to overcome the shortcomings of another. This will create a self-validating method for leveraging advanced remote sensing algorithms in an integrated software platform with ongoing data collection to produce 10-30m resolution bathymetry elevation models.? Extraction methods utilize underwater stereophotogrammetry from multiple overlapping images, wave kinematic detection from multispectral imagery, and measurement of variable attenuation of multispectral signals through the water column.? Research will be conducted on computer vision applications for assessing and allocating images based upon metocean and atmospheric parameters; seafloor spectral segmentation techniques and effects on subsequent multispectral depth retrieval; automation of stereophotogrammetric seafloor target identification and correlation, and integration and automation of wave kinematic derived depth values with multispectral depth retrieval values.? Statistical analysis of resulting data from this combined processing method, evaluated against in situ data for trial locations, will provide direct comparisons for accuracy reporting, algorithm refinements, computational optimization, and determine overall technical and commercial feasibility of future global implementation.?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835251","NCS-FO: Integrating Non-Invasive Neuroimaging and Educational Data Mining to Improve Understanding of Robust Learning Processes","DGE","ECR-EHR Core Research","09/01/2018","08/21/2018","Erin Walker","AZ","Arizona State University","Standard Grant","Gregg Solomon","05/31/2019","$335,509.00","","eawalker@pitt.edu","ORSPA","TEMPE","AZ","852816011","4809655479","EHR","7980","8089, 8091, 8551, 8817","$0.00","From elementary school math games to workplace training, computer-based learning applications are becoming more widespread. With these programs, it becomes increasingly possible to use the data generated, such as correct and incorrect problem-solving responses, to develop ways to test for student knowledge and to personalize instruction to student needs. The logs of student responses can capture answers, but they fail to capture critical information about what is happening during pauses between student interactions with the software. This project, led by a team of researchers at Arizona State University and Worcester Polytechnic Institute, will explore the use of measurements of brain activity from lightweight brain sensors alongside student log data to understand important mental activities during learning. The study will examine developmental math learning in college and community college students using the ASSISTments intelligent tutoring system. Using brain imaging, the project team will examine whether students are thinking deeply about the problem or mind-wandering during pauses in the learning tasks and use the combined log and brain data to make predictions about learning outcomes. This work will build a foundation for new methods of combining neuroimaging, machine learning, and personalized learning environments. With a better understanding of when and how learning occurs during pauses in tutoring system use, learning technology researchers and developers will be able to create adaptive interventions within tutoring systems that are better personalized to the needs of the individual. This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE).<br/><br/>This project has of three goals: 1) Integrating multiple data streams for the creation of an interdisciplinary corpus; 2) Detecting real-time changes in cognitive states during pauses in log data; and 3) Predicting learning outcomes from brain-based and log-based inferences of cognitive states. In addressing these goals, the team will collect brain data, using functional near-infrared spectroscopy neuroimaging, and behavioral data from controlled, well-understood tasks related to rule learning and mind wandering and from authentic learning tasks. Cognitive neuroscience research involving recordings of brain activity traditionally requires paradigms with highly constrained stimuli, timing, and task requirements, whereas research in complex real-world environments such as tutoring systems rarely align with these paradigms. Features of the brain activity during the cognitive tasks will be used to make inferences about student cognition during authentic learning tasks. In addition, brain features will be combined with log data features to create machine learning models that make accurate predictions of student robust learning outcomes, to be assessed using a posttest given after students use the interactive learning environment. Contributions of this project to STEM learning will include improved understanding of how students build knowledge in response to instructional events within digital learning environments, the construction of better predictive models of when students learn from the use of personalized learning environments, and a mapping between learning processes and the length and context of pauses. This project will also contribute to understandings of how to combine analyses of neuroimaging data and log data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755844","CRII: RI: Towards Human-Level Assessment of Speech Quality and Intelligibility in Real-World Environments","IIS","Robust Intelligence","06/01/2018","04/13/2018","Donald Williamson","IN","Indiana University","Standard Grant","Tatiana Korelsky","05/31/2021","$174,995.00","","williads@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7495","7495, 8228","$0.00","Separating speech from background noise is crucial for many speech-based applications, including hearing prostheses, robotics, and multimedia communication. Many speech separation algorithms perform reasonably well when they are tested in simulated environments, but this level of performance does not always carry over to real environments that are more nuanced. For example, a common complaint of many hearing aid users is that their hearing aid is not effective in noisy environments such as restaurants. Current computational measures do not enable practical or convenient speech assessment in everyday environments, and this is a major hurdle for improving real-world separation performance. In addition, the end-user has largely been left out of the development and evaluation process, which is not ideal since an approach's usefulness is ultimately determined by people. The objective of this project is to develop computational evaluation algorithms to better assess speech quality and intelligibility in real environments. <br/><br/>A key area of research focuses on developing novel, data-driven assessment algorithms that use deep learning to predict human assessment scores, which enables testing in real environments.  Considering the recent success that deep learning has had in speech processing, this new assessment approach is promising and offers substantial differences from prior approaches. The relationship between spectral-temporal speech attributes and human assessment scores are determined as a result of this project. Quantifying this relationship ensures that assessment algorithms are accurate and have strong agreement with human evaluations. An effective integration of human assessment in speech separation algorithm development should result in improved separation algorithms, which ultimately benefits users and applications. This is expected since accurate assessment enables researchers to more easily identify and correct weaknesses based on real-world environmental factors. The research activities lay the foundation for the emerging research area of improving realism in speech processing applications and offer key insights on human perception to the larger scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841752","I-Corps:  Scalable Storage of Whole Slide Images and Fast Retrieval of Tiles for Next-Generation Image Analytics","IIP","I-Corps","09/15/2018","08/16/2018","Praveen Rao","MO","University of Missouri-Kansas City","Standard Grant","Andre Marshall","04/30/2020","$50,000.00","","praveen.rao@missouri.edu","5100 Rockhill Road","Kansas City","MO","641102499","8162355839","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is pivoted around the emerging whole slide imaging market segment in digital pathology. The digital pathology market is predicted to reach over $800 million by 2025. The motivation of this project stems from the gigabyte size of whole slide images (WSIs), which are digital images of glass slides produced at near optical resolution. With rapid increase in the number of WSIs produced by hospitals and pathology labs, the storage and management of WSIs has become an urgent problem to tackle for next-generation image analytics. The commercial viability of the project can significantly impact researchers, medical professionals, software developers, and IT staff who wish to manage large number of WSIs using modern cluster computing and big data techniques. Thus, next-generation image analytics (e.g., using deep learning) for automatic detection and analysis of cellular and morphological features in human tissues can be performed faster on large numbers of WSIs. The potential societal benefit of the project includes enabling improved diagnosis of diseases by pathologists using next-generation image analytics and the creation of a tech startup leading to new jobs. This project will provide training to two Ph.D. students from underrepresented groups in STEM.<br/><br/>This I-Corps project is based on a software technology that aims to solve the fundamental problem of scalable storage of WSIs and fast retrieval of tiles using a commodity cluster and big data techniques. The value proposition of the technology is efficient and cost-effective storage of large-scale WSIs and fast retrieval of tiles to enable next-generation image analytics for human disease diagnosis. The technology encompasses intelligent data partitioning using space-filling curves, in-memory data structures, and effective organization of tiles to enable fast retrieval of tiles during image analysis. It employs space-efficient storage formats to maximize storage efficiency. On an average, it required a few seconds to retrieve a single tile on 80 WSIs using a 16-node cluster. Therefore, we believe next-generation image analytics on WSIs (e.g., using deep learning) can run faster on large number of WSIs, which can consume terabytes of storage, through faster access of image tiles. As the technology relies on commodity hardware and open source software, it is cost-effective and can be easily deployed as a product or a service. The technology has the potential to advance the state-of-the-art in WSI storage and management using a big data approach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816917","Geometric Analysis for Classification and Reassembly of Broken Bones","DMS","APPLIED MATHEMATICS","09/01/2018","08/16/2018","Peter Olver","MN","University of Minnesota-Twin Cities","Standard Grant","Victor Roytburd","08/31/2021","$418,069.00","Jeffrey Calder","olver@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1266","9251","$0.00","The reassembly of broken objects is a fundamental issue in many real-world applications.  The goal of this project is to apply modern machine learning and geometric tools to the problems of classification and automated reassembly of broken bone fragments from an archaeological context, to determine agents of breakage (e.g., carnivore, hominin, etc.,), improve taxonomic identifications, and better understand site formation processes through spatial analysis of refits.  The anthropological implications are expected to impact the study of early human origins and dispersal, prehistory, culture, predator avoidance, and social organization -- hunting, scavenging, food provisioning, etc.  The project relies on samples and data generated both locally by known agents and through ongoing field work in Dmanisi, Georgia; if successful, these methods can be applied to archaeological sites from all times around the world.  The potential impact of success is underscored by the April, 2017 announcement that, based on the geometry of broken mastodon bones, humans settled the Americas 100,000 years earlier than the standard estimates of 30-40,000 years ago, although this claim remains highly controversial in the field.  Accurate and precise methods of determining the agent of breakage have not yet been completely worked out by archaeologists, and hence a reassessment of their bone analysis would be of supreme interest.  Besides zooarchaeological applications, potential areas of significant impact include computer-aided and virtual reassembly of other archaeological objects (pottery, statuary, lithics and tools, etc.), paleontology (dinosaurs and other fossils), art restoration, and computer-assisted surgery, where the mathematical techniques can aid the surgeon to both plan and undertake an operation while minimizing the invasiveness and impact on the patient.  Other areas where these techniques have already had some impact include the reassembly of jigsaw puzzles, shredded documents, and whole histological sections from digitized tissue fragments, as well as the diagnosis of cancer in breast tumors and the distinguishing of moles from melanomas.  Graduate and undergraduate students participate in the research.<br/><br/>The project seeks to adapt and extend known geometric methods, data analysis, and numerical schemes, particularly those based on continuous and discrete invariant signatures, to the problem of analyzing and reconstructing broken solid objects, with a particular emphasis on bone fragments.  Notable success in the automatic reassembly of non-pictorial jigsaw puzzles and broken surfaces, e.g., egg shells, using differential invariant signatures, suggests that one of the key goals of the project, the three-dimensional solid object reconstruction problem, is attainable.  In addition, new (to anthropological field work) geometric tools of surface geometry -- principal curvatures, torsion and curvature of three-dimensional break curves, histogram and other discrete integral invariants -- are applied to analyze breakage geometry, starting with controlled samples of ungulate (elk, cow, and goat) bones that have been broken by humans using stone tools and by animals (hyenas in the Milwaukee County Zoo and the Irvine Zoo in Chippewa Falls), in preparation for the eventual analysis of field samples from Dmanisi and possibly other sites around the world, such as Olduvai Gorge in Tanzania and The Cradle of Humankind in South Africa.  This analysis is used to develop machine learning algorithms, both fully supervised and semi-supervised, for classifying bone fragments based on agent of breakage.  We map the bone fragments into feature spaces via computation of histograms of geometric invariants, and train a machine learning algorithm, such as k-nearest neighbors or support vector machine classifiers, to distinguish between different agents of breakage.  The controlled samples of ungulate bones broken by humans are used as training data, and field data from the Dmanisi site are used as testing data.  Graduate and undergraduate students participate in the research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829096","Workshop on Data Storage Research Vision","CNS","CSR-Computer Systems Research","04/01/2018","03/28/2018","Ali Butt","VA","Virginia Polytechnic Institute and State University","Standard Grant","Samee Khan","09/30/2018","$49,999.00","","butta@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7354","7556","$0.00","This award funds a focused workshop for bringing together a broad range of scientists, systems researchers, and information technology (IT) practitioners to discuss and establish a vision for the future of storage systems. The output of the workshop will be a public report documenting the discussions and a set of recommendations on future storage research directions. The workshop and the report should also stimulate effective research collaborations between the disparate research communities represented at the workshop. <br/><br/>The big data revolution along with the dawn of the age of Internet of Things (IoT) is driving the need for novel and innovative storage systems to store, manage, retrieve, and efficiently utilize unprecedented volumes of data at increasingly faster speeds. There are a number of open challenges and research issues that need to be addressed both in the short and long term to ensure sustained storage systems efficacy and performance. In particular, the wide variety of applications of modern and emerging storage systems entail that the fundamental design of storage systems should be revisited. Existing standards and abstractions need to be reevaluated, and new sustainable data representations need to be designed to support emerging applications. New storage software designs are also necessary to take advantage of the hardware advancements such as persistent memories in order to maximize efficiency and performance.<br/><br/>The workshop will identify the major issues of designing future storage systems and define a vision for storage research. Some of the broad set of directions to consider include, but are not limited to: 1) How to evolve storage systems to meet the challenges of scale, throughput, and sustainability arising from emerging applications such as deep learning and IoT? 2) How to address allocation, management, privacy, performance, and multi-tenancy to meet the demands of the intense migration of data from on-premise to cloud deployments? 3) How to design file system programming interfaces and higher-level yet simple-to-use interfaces for new storage systems? 4) How to design programming models to efficiently support innovative storage and deep storage hierarchies? 5) How to address pipeline issues and train the next generation of storage systems researchers? 6) How to design and make it easy for systems researchers to realize new applications and storage hardware? Workshop participants will be charged with assessing the current state and direction of research related to storage systems while making recommendations for research related to the current, short-term, and long-term needs of designing sustainable and scalable storage architectures.<br/><br/><br/>The workshop report, along with the list and biographies of the participants and the organizers, will be made available to the community via the workshop website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841119","EAGER: Constraint Aware Generative Adversarial Networks","IIS","Info Integration & Informatics","09/01/2018","08/17/2018","Xintao Wu","AR","University of Arkansas","Standard Grant","Wei Ding","08/31/2020","$100,000.00","","xintaowu@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","7364","7364, 7916","$0.00","Generative adversarial networks (GAN), which estimate the real data distribution through an adversarial game, have achieved great success in image generation and text generation. However, many decision models from real world applications are trained on relational data that contains both numerical and categorical attributes. Furthermore, real world applications often have legal or ethical requirements on the generated data, e.g., discrimination free or privacy preservation, or expect to generate complementary samples to their existing data that may only contain normal samples. The research significantly improves the applicability of generative adversarial networks from image/text data to relational data with application requirements and contributes to the limited base of knowledge in the area of using generative adversarial networks for constraint aware relational data generation. The findings, tools, software code, and curricular materials documents are disseminated to the research community, IT industry, and users, which expects to help domain users generate constraint aware data to meet their business needs. <br/><br/>This EAGER research develops novel techniques that enable the current generative adversarial networks to generate realistic relational data with constraints. The developed framework adds a decoder to the generator to generate both numerical and categorical data and incorporates constraint terms into the objective functions of generator and discriminator or introduces multiple discriminators to enforce requirement constraints. The research adopts f-divergences to analyze the convergence of the constraint aware GAN framework when complex constraints are introduced. The research then focuses on developing under the unifying framework two specific models, fair GAN for generating discrimination-free data, and complementary GAN for generating negative samples when only positive samples are available in the training data. The research conducts empirical evaluations of the framework and two specific models in terms of accuracy and convergence, implements and integrates the developed algorithms into TensorFlow, an open source deep learning software system. The developed framework expects to advance theoretical understanding of generative adversarial networks and the two specific GAN models expect to improve the current research on fairness aware learning and fraud detection. In particular, the fair GAN introduces the new approach of fair data generation based on GAN as current fairness aware learning research mainly adopts data modification. The complimentary GAN outperforms existing one-class classification models for fraud detection by generating complementary samples and enabling the trained discriminator to accurately separate abnormal samples from normal ones.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809739","Cyber-Physical Systems Security through Robust Adaptive Possibilitistic Algorithms: a Cross Layered Framework","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2018","08/12/2018","Arturo Bretas","FL","University of Florida","Standard Grant","Anil Pahwa","07/31/2021","$360,000.00","Janise McNair, Alina Zare","arturo@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","ENG","7607","155E","$0.00","The goal of this project is to develop a cross-layer cyber-physical security framework for the smart grid. The proposed research will improve the quality of real-time monitoring of the smart grid through anomaly analysis. This will lead to more reliable data for control, situation awareness to first responders and other improved applications to smart grids. The proposed research will improve the resilience of smart grids to cyber-attacks in meters, parameters, topology and communication infrastructure and large physical disturbances by developing new techniques for distributed control of large complex systems that guarantees secure and reliable performance. The project will foster education through enhancement to curriculum by building bridges among communications, machine learning, power and control systems. The PIs plan to teach short courses on smart grid security at conferences.  In addition, they plan to engage under-represented minority students in their project. <br/><br/>The project aims at developing a distributed nonlinear controller for transient stability enhancement. The new control layer will actuate on distributed energy storage systems, be robust to uncertainties in modelling and capable of compensating input time-delay while independent of operating conditions. Furthermore, the robust controller will not require exact knowledge of the system dynamics. Second, bad data analytics based on the innovation approach and cross-layered information provided by distributed software-defined network will be developed. The bad data analytics will consider the inherent interdependencies of the physical processes while providing a countermeasure. Third, an adaptive distributed robust machine learning approach will be developed. The overwhelming majority of supervised machine learning methods require large amounts of carefully labeled training data that is representative of the data distribution to be seen under test. However, in security applications, novel threats and malicious attacks are continuously being developed and attempted. Thus, approaches that rely on prior training data are unlikely to be robust in the case of behaviors never seen before, as would be the case in a rapidly changing threat environment. The novel distributed machine intelligence method that will be developed will be focused on being rapidly adaptive to identifying and distinguishing novel threats given even only one example of an anomalous novel threat.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836978","FMitF: Collaborative Research: Formal Methods for Machine Learning System Design","CCF","FMitF: Formal Methods in the F, Software & Hardware Foundation","10/01/2018","09/10/2018","Somesh Jha","WI","University of Wisconsin-Madison","Standard Grant","Nina Amla","09/30/2022","$406,000.00","Xiaojin Zhu","jha@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","094Y, 7798","062Z, 8206","$0.00","Machine learning (ML) algorithms, fueled by massive amounts of data, are increasingly being utilized in several critical domains, including health care, finance, and transportation. Models produced by ML algorithms, for example deep neural networks, are being deployed in these domains where trustworthiness is a big concern. It has become clear that, for such domains, a high degree of assurance is required regarding the safe and correct operation of ML-based systems. This project seeks to provide a systematic framework for the design of ML systems based on formal methods. The project seeks to review and improve almost every aspect of the design flow of ML systems, including data-set design, learning algorithm selection, training of ML models, analysis and verification, and deployment.  The theory and ideas generated during the project will be implemented in a new software toolkit for the design of ML systems in the context of cyber-physical systems.<br/><br/>The project focuses on cyber-physical systems (CPS), which is a rich domain to apply formal methods principles. Moreover, the research ideas from this project can be readily applied to other contexts. A key aspect of this research is the use of a semantic approach to the design and analysis of ML systems, where the semantics of the target application and a formal specification for the full system, comprising the ML component and other components, are cornerstones of the design methodology. The project employs a range of formal methods, including satisfiability solvers, simulation-based verification, model checking, specification analysis, and synthesis to improve all stages of the ML design flow. Formal techniques are also used for the tuning of hyper-parameters and other aspects of the training process, to aid in debugging misclassifications produced by ML models, and to monitor ML systems at run time and ensure that outputs from ML models are used in a manner that ensures safe operation at all times.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821992","Accelerated Dynamics of Surface Chemical Reactions","CBET","Proc Sys, Reac Eng & Mol Therm","01/06/2018","01/04/2018","Ramamurthy Ramprasad","GA","Georgia Tech Research Corporation","Standard Grant","Raymond Adomaitis","08/31/2020","$241,246.00","","ramprasad@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1403","7752","$0.00","1600218  PI: Ramprasad    <br/>Institution: University of Connecticut<br/>Title: Accelerated Dynamics of Surface Chemical Reactions<br/><br/>Surface chemical reactions are ubiquitous in natural phenomena and play a key role in several scientific disciplines, such as heterogeneous catalysis, crystal growth, and electrochemistry.  Achieving a molecular-level picture of the mechanisms and dynamical detail of surface chemical reactions though remains a daunting task. Current methods of inquiry, be it empirical or computational, provide us with only a limited view of this complex world. One way of achieving the requisite level of understanding is by the use of molecular dynamics (MD) simulations. These simulations can directly monitor the progression of reactions at surfaces at the molecular level as a function of a variety of relevant conditions. Nevertheless, significant gaps remain in present-day MD capabilities: they are either fast (but not versatile or accurate), e.g., those based on empirical force-fields, or they are versatile and accurate (but not efficient), e.g., those based on quantum mechanical (or ab initio) methods. The proposed work will exploit an adaptive machine learning scheme that can both accelerate ab initio MD simulations as well as create accurate force-fields (at no extra cost) on-the-fly. Timescales previously unreachable using quantum mechanical simulations may be accessed using this new paradigm (conceivably,milliseconds to seconds), while still preserving the fidelity of quantum mechanics.<br/><br/>The primary reason ab-initio MD simulations are slow is largely because of enormous redundancies that permeate present-day paradigms. Energies and forces, the ingredients necessary to perform MD simulations, are evaluated for every configuration that is visited, regardless of whether a new configuration is similar to a previously visited configuration. The basic premise underlying this proposal is that a methodology based on machine learning can be used to eliminate the significant effort involved in predicting atomic forces and energies of revisited or similar states within a local minimum, and at equivalent multiple local minima, thus eliminating an enormous amount of redundancies. Only when a truly new configuration is encountered is an ab initio scheme necessary; otherwise, the inexpensive machine learning algorithm is used to predict atomic forces and energies. In the former instance, the learning algorithm is retrained to include the new information, thus making the prediction scheme adaptive on-the-fly. The specific goals of the proposed research are: (1) The development of stand-alone force-fields just using the data accumulated during ab initio MD simulations; and (2) Application of this development to important model surface science problems, including surface adatom diffusion, surface oxidation, and surface catalytic reactions. Several educational initiatives are also planned, including new course offerings, workshops, online lectures, short courses, and symposia."
"1839338","TRIPODS+X:RES: Collaborative Research: Data Science Frontiers in Climate Science","DMS","TRIPODS Transdisciplinary Rese, EarthCube","10/01/2018","09/10/2018","Rebecca Willett","WI","University of Wisconsin-Madison","Standard Grant","Nandini Kannan","06/30/2019","$300,000.00","Stephen Wright, Garvesh Raskutti","willett@uchicago.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","041Y, 8074","047Z, 062Z","$0.00","Understanding the factors that determine regional climate variability and change is a challenge with important implications for the economy, security, and environmental sustainability of many regions around the globe. Our understanding and modeling of the large-scale dynamics of the Earth climate system and associated regional-scale climate variability significantly affects our ability to predict and mitigate climatic extremes and hazards. Earth observations and climate model outputs are witnessing an unprecedented increase in data volume, creating new opportunities to advance climate science but also leading to new data science challenges that must be addressed using tools from mathematics, statistics, and computer science. This project focuses on two central challenges at the heart of modern data-enabled climate science: (1) Increasing the predictive capacity of subseasonal forecasts by discovering and quantifying the sources of (un)predictability, including known and emergent climate modes and their interactions and non-stationarities; and (2) Understanding and quantifying the intricate space-time dynamics of the climate system to provide guidance for climate model assessment and regional forecasting.  This project brings together an interdisciplinary team that combines expertise in both hydroclimate science and statistical machine learning to create new platforms for climate diagnostics and prognostics.  The broader impacts of an enhanced knowledge of the climate system and robust and accurate seasonal forecasts have wide-ranging implications for society as a whole. For example, better seasonal forecasts will allow water resource managers to make sustainable decisions for water allocation.<br/><br/>This TRIPODS+CLIMATE project will develop novel machine learning and network estimation methodologies for analyzing the climate system over a range of space and time scales, to understand climate modes of variability and change and to explore their predictive ability for regional hydroclimatology. The two main objectives of this project are the following.  Objective 1: Develop novel classification and regression tools that account for highly-correlated features or covariates, nonlinear interaction terms in high-dimensional settings, and nonstationarity in climate observations. These tools will be used to improve seasonal-to-subseasonal forecasts of regional precipitation using multidimensional climate modes and feature vectors in the presence of evolving dynamics and nonstationarities. Objective 2: Develop network identification methods that leverage recent advances in machine learning and statistics and that can account for the nonstationarity and limited timeframe of climate data. The network representation will be used to analyze the structure and dynamics of the learned dependencies to contextualize and interpret them physically, and to quantify changing patterns in climate modes and their regional predictive capacity.  Emphasis will be placed on the western Pacific dynamics where an interhemispheric bi-directional connection has recently been discovered, promising earlier and more accurate seasonal-to-subseasonal forecasts in the southwestern US and other parts of the world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751206","CAREER: Weakly-Supervised Visual Scene Understanding: Combining Images and Videos, and Going Beyond Semantic Tags","IIS","Robust Intelligence","04/01/2018","03/25/2020","Yong Jae Lee","CA","University of California-Davis","Continuing Grant","Jie Yang","03/31/2023","$302,596.00","","yongjaelee@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7495","1045, 7495, 9251","$0.00","The internet provides an endless supply of images and videos, replete with weakly-annotated meta-data such as text tags, GPS coordinates, timestamps, or social media sentiments. This huge resource of visual data provides an opportunity to create scalable and powerful recognition algorithms that do not depend on expensive human annotations. The research component of this project develops novel visual scene understanding algorithms that can effectively learn from such weakly-annotated visual data. The main novelty is to combine both images and videos together. The developed algorithms could have broad impact in numerous fields including AI, security, and agricultural sciences. In addition to scientific impact, the project performs complementary educational and outreach activities. Specifically, it provides mentorship to high school, undergraduate, and graduate students, teaches new undergraduate and graduate computer vision courses that have been lacking at UC Davis, and organizes an international workshop on weakly-supervised visual scene understanding.<br/><br/>This project develops novel algorithms to advance weakly-supervised visual scene understanding in two complementary ways: (1) learning jointly with both images and videos to take advantage of their complementarity, and (2) learning from weak supervisory signals that go beyond standard semantic tags such as timestamps, captions, and relative comparisons. Specifically, it investigates novel approaches to advance tasks like fully-automatic video object segmentation, weakly-supervised object detection, unsupervised learning of object categories, and mining of localized patterns in the image/video data that are correlated with the weak supervisory signal. Throughout, the project explores ways to understand and mitigate noise in the weak labels and to overcome the domain differences between images and videos.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820821","Variational Decomposition Models in Imaging Sciences and High Dimensional Multi-Time Hamilton-Jacobi Equations","DMS","CDS&E-MSS","09/01/2018","09/03/2019","Jerome Darbon","RI","Brown University","Continuing Grant","Christopher Stark","08/31/2021","$140,000.00","","jerome_darbon@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","8069","9263","$0.00","Many imaging science problems can be formulated as a variational inverse problem that can be solved using optimization techniques. Imaging models used to process data in order to extract the relevant information for applications are becoming more and more complex and involve many terms. The main objectives of the project consist of establishing new and unique connections between complex variational imaging models and partial differential equations in high dimensions. This connection will be used to better understand, create and learn robust models for processing data to extract the relevant information for applications. The investigator will also use this connection to create new numerical algorithms for solving these optimization problems.<br/><br/>The fundamental research in this project will exhibit new and unique connections between image processing and computer vision techniques based on convex optimization and multi-time Hamilton-Jacobi partial differential equations in very high dimensions. The research will generate new mathematical results about the robustness of solutions of variational inverse problems in imaging sciences and new formulas that describe the impact of the estimation under perturbations of models. New representation formulas, existence and uniqueness results for a large class of multi-time Hamilton-Jacobi equations will be rigorously obtained.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839974","FW-HTF: Collaborative Research: Enhancing Human Capabilities through Virtual Personal Embodied Assistants in Self-Contained Eyeglasses-Based Augmented Reality (AR) Systems","CMMI","FW-HTF-Adv Cogn & Phys Capblty","09/15/2018","09/13/2018","Gordon Wetzstein","CA","Stanford University","Standard Grant","Robert Scheidt","08/31/2021","$809,999.00","Jeremy Bailenson","gordon.wetzstein@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","ENG","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>This award supports basic research underpinning development of an eyeglass-based 3D mobile telepresence system with integrated virtual personal assistant. This technology will increase worker productivity and improve skills. The system automatically adjusts visual focus and places virtual elements in the image without eye strain.  The user will be able to communicate to the system by speech.  The system also uses sensors to keep track of the user's surroundings and provide the relevant information to the user automatically.  The project will explore two of the many possible uses of the system: amplifying a workers capabilities (such as a physical therapist interacting with a remote patient), and accelerating post-injury return to work through telepresence (such as a burn victim reintegrating into his/her workplace). The project will advance the national interest by allowing the right person to be virtually in the right place at the right time. The project also includes an education and outreach component wherein undergraduate and graduate students shall receive training in engineering and research methods. Course curriculum at Stanford University and the University of North Carolina at Chapel Hill shall be updated to include project-related content and examples. <br/><br/>This project comprises fundamental research activities needed to develop an embodied Intelligent Cognitive Assistant (GLASS-X) that will amplify the capabilities of workers in a way that will increase productivity and improve quality of life. GLASS-X is conceived of as an eyeglass-based 3D mobile telepresence system with integrated virtual personal assistant. Methods include: body and environment reconstruction (situation awareness) from a fusion of images provided by an eyeglass frame-based camera array and limb motion data provided by inertial measurement units; fundamental research on adaptive focus displays capable to reduce eye strain when using augmented reality displays; dialog-based communication with a virtual personal assistant, including transformations from visual input to dialog and vice versa; human subject evaluations of GLASS-X technology in two workplace domains (remote interactions between a physical therapist and his/her patient; burn survivor remote return-to-work). This research promises to push the state of the art in core areas including: computer vision; augmented reality; accommodating displays; and natural language and dialogue models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818978","SBIR Phase I:  Cloud-Based Voice Analysis and Machine Learning to Improve Teaching with Data","IIP","SMALL BUSINESS PHASE I","07/01/2018","06/15/2018","Amy Spinelli","WA","Earshot LLC","Standard Grant","Rajesh Mehta","02/28/2019","$225,000.00","","amy@theearshotapp.com","1100 NE Campus Pkwy","Seattle","WA","981056605","9178226074","ENG","5371","5371, 8031, 8032","$0.00","This SBIR Phase I project will study how teachers use data to improve their instruction. In response to widespread understanding that feedback is a necessary component to improvement, school districts are increasingly investing in teacher coaches to provide feedback critical for teachers. However, this is not scalable, and typically only new or underperforming teachers have access to coaching. Earshot seeks to scale and democratize some aspects of coaching so every teacher can have access to the personalized data and feedback necessary for improvement. The goal is for all teachers to maximize their potential, better engaging students in learning.<br/><br/>Earshot has developed a cloud-based system using voice analysis to provide data about instruction. The core innovation is a machine learning process to listen to dialogue between teachers and students and analyze the quality of the educational exchange. Funding for this project would enable study of two critical questions: How does providing data about teacher behavior affect what teachers do in their classrooms? And, what kinds of resources, content, and support are necessary to help teachers improve? This research has the potential to transform the way teachers use data about their own performance and receive the necessary help to improve.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750362","CAREER: Theoretical Foundations for Probabilistic Models with Dense Random Matrices","CCF","Special Projects - CCF, Comm & Information Foundations","03/01/2018","09/08/2019","Galen Reeves","NC","Duke University","Continuing Grant","Phillip Regalia","02/28/2023","$386,994.00","","galen.reeves@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","2878, 7797","1045, 7935","$0.00","Many real-world scientific and engineering applications require sophisticated processing of large and complex data sets. Examples include wireless communications,  computational photography, and the training of multilayer networks for classification tasks. In some cases, performance is fundamentally limited by the amount of data. In other cases, the main limitation is the computational complexity of processing algorithms. A major challenge for researchers is to understand these fundamental limits. This project explores these limits by studying probabilistic models that describe the statistical relationship between the data and the unknown quantities of interest (e.g., transmitted message or correct label). The research involves combining ideas from information theory and statistical physics to compute fundamental limits and using these results to design efficient methods with improved performance. The interdisciplinary nature of the research is mirrored in the education activities of this project, which focuses on making connections between engineering, statistical physics, and the information sciences, as well as improving undergraduate education through exploratory data analysis.<br/><br/>The key conceptual idea behind this research is that statistical dependencies induced through multiplication by dense random matrices can be understood through connections with simpler models involving additive Gaussian noise. In a recent breakthrough, the investigator showed how ideas from information theory could provide rigorous proofs for behaviors that had been conjectured using the heuristic replica method from statistical physics. Building upon this insight, the research is organized around three thrusts: i) Developing new theoretical methods to provide rigorous and interpretable characterization of fundamental limits; ii) Designing new algorithms for inference, learning, and compression; and iii) Analyzing bi-linear and multi-layer inference problems with applications to deep learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750399","CAREER: Staging Compilers for Heterogeneous Platforms","CCF","Software & Hardware Foundation","02/01/2018","05/18/2020","Louis-Noel Pouchet","CO","Colorado State University","Continuing Grant","Almadena Chtchelkanova","01/31/2023","$275,322.00","","pouchet@cs.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7798","1045, 7923, 7942, 7943","$0.00","Power density and energy considerations have become the primary constraints driving technology directions for embedded, mainstream, as well as peta/exascale computing at the high end. Non-homogeneous CPU cores and increasingly complex System-on-Chips are on the roadmap of most manufacturers. In a word, computing platforms are now heterogeneous, after decades of mass marketing homogeneous single-core x86 processors. Optimizing compilers are a cornerstone of the software stack: they are in charge of producing high-quality machine-specific code from the input program. The current development model where either an application is manually tuned by expert engineers to the specifics of the new target platform, or simply left untuned and heavily under-utilizing the hardware resources is not sustainable. This project targets the design of a complete system to efficiently compile several key computation patterns to heterogeneous targets, from a single input source. The PI investigates how to automatically characterize the quality and performance of software transformation systems, so as to better exploit their strengths; and create new customized compilation techniques to produce optimized binaries for heterogeneous processors. <br/>In particular, the PI develops a novel system that automatically learns what types of programs an optimization tool (e.g., a vendor compiler) can optimize well, focusing on performance-critical loop-based program regions amenable to polyhedral compilation. By combining automatic benchmark generation and deep learning techniques, this system automatically builds a performance contract for the compiler: a program that meets specific syntactic and semantics restriction (the contract) is guaranteed to be well optimized by that compiler. Then, in order to best exploit such compilers, programs are automatically restructured to expose program sub-regions that meet the contract requirements. With the assistance of target-specific performance models, the best restructuring is chosen at compile-time for each hardware target. This system can then be applied at compile-time for various execution contexts (e.g., for different CPU frequencies, core counts, etc.), to deliver an adaptive binary where the best implementation is selected at run-time as a function of the execution context. The project aims to demonstrate how to best stage various compilers to exploit their strengths, in turn significantly reducing the time currently spent by developers to tune their implementation for better performance. Education material to be produced includes a lecture series for educators and students on how to write programs that compilers can optimize well, and a MOOC on polyhedral compilation, the mathematical framework to reason about programs that is central to this project."
"1837132","FMitF: Collaborative Research: Formal Methods for Machine Learning System Design","CCF","FMitF: Formal Methods in the F, CPS-Cyber-Physical Systems","10/01/2018","09/10/2018","Sanjit Seshia","CA","University of California-Berkeley","Standard Grant","Nina Amla","09/30/2022","$294,000.00","","sseshia@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","094Y, 7918","062Z, 8206, 8234, 9102","$0.00","Machine learning (ML) algorithms, fueled by massive amounts of data, are increasingly being utilized in several critical domains, including health care, finance, and transportation. Models produced by ML algorithms, for example deep neural networks, are being deployed in these domains where trustworthiness is a big concern. It has become clear that, for such domains, a high degree of assurance is required regarding the safe and correct operation of ML-based systems. This project seeks to provide a systematic framework for the design of ML systems based on formal methods. The project seeks to review and improve almost every aspect of the design flow of ML systems, including data-set design, learning algorithm selection, training of ML models, analysis and verification, and deployment.  The theory and ideas generated during the project will be implemented in a new software toolkit for the design of ML systems in the context of cyber-physical systems.<br/><br/>The project focuses on cyber-physical systems (CPS), which is a rich domain to apply formal methods principles. Moreover, the research ideas from this project can be readily applied to other contexts. A key aspect of this research is the use of a semantic approach to the design and analysis of ML systems, where the semantics of the target application and a formal specification for the full system, comprising the ML component and other components, are cornerstones of the design methodology. The project employs a range of formal methods, including satisfiability solvers, simulation-based verification, model checking, specification analysis, and synthesis to improve all stages of the ML design flow. Formal techniques are also used for the tuning of hyper-parameters and other aspects of the training process, to aid in debugging misclassifications produced by ML models, and to monitor ML systems at run time and ensure that outputs from ML models are used in a manner that ensures safe operation at all times.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823559","FoMR: Improving Microprocessor IPC for Data Center Workloads","CCF","Special Projects - CCF","10/01/2018","08/22/2018","Heiner Litz","CA","University of California-Santa Cruz","Standard Grant","Yuanyuan Yang","09/30/2021","$173,000.00","","hlitz@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","2878","021Z, 2878, 7798, 7941","$0.00","More and more companies and institutions are moving their products and services into the cloud, increasing the number of compute cycles spent in data centers significantly. The explosive growth of such cloud computation has delivered great value to society at the cost of increasing the environmental footprint on our world. This project can improve the energy efficiency of data center systems through the design and optimization of cloud application-specific microprocessors. The research will be infused into the undergraduate curriculum at University of California Santa Cruz, educating the next generation of students on designing energy efficient, data center specific architecture and systems.<br/><br/>This project will develop machine learning techniques to predict datacenter application behavior and apply them to processor microarchitecture. In particular, a new cross-layer approach will be developed to improve the instructions-per-cycle (IPC) performance of data center workload-optimized processors. It will explore and develop solutions for a number of challenges including offline learning of data center application behavior, leveraging machine learning models for instruction prefetching and branch prediction, and building resource and power efficient hardware to leverage the machine learning models at execution time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811920","Efficient Monte Carlo Algorithms for Bayesian Inference","DMS","STATISTICS","08/01/2018","05/05/2020","Wing Hung Wong","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2021","$200,000.00","","whwong@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","1269","9251","$0.00","Data sets arising from current applications of statistics and machine learning are of very large size and require large models for their analysis. Bayesian inference and global optimization are two powerful methods for learning from such data, but the large size of the data sets and the resulting computational difficulties greatly limit the applicability of these methods.  The research in this project aims to increase computational efficiency of these methods, thereby substantially expanding their usefulness for the analysis of large data sets. The methods and algorithms from this research will be implemented on modern distributed computing platforms and made freely available for the scientific community. The results will have wide applications in statistics and machine learning.<br/><br/>Specifically, the use of mini-batches in Markov Chain Monte Carlo (MCMC) will be investigated. MCMC is perhaps the most widely used computational approach for Bayesian statistical inference. Since each step in the simulation of the Markov chain requires the scanning of all the observations, for a large data set this computation is prohibitive. On the other hand, in the area of machine learning researchers have found that stochastic optimization techniques, which examine only a mini-batch of data points at a time, can deliver excellent performance. In this project, a framework for unifying mini-batch based MCMC and global optimization will be developed. It is showed that simulation from of a tempered version of the posterior distribution can be approximated by a MCMC process with Metropolis-Hasting updates that depend only on mini-batches. This approach will be combined with eqi-energy sampling to achieve a unified simulation and global optimization methodology. This framework will allow us to improve the performance of both MCMC methods and non-convex global optimization methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1930049","TRIPODS+X:RES: Collaborative Research: Data Science Frontiers in Climate Science","DMS","TRIPODS Transdisciplinary Rese, EarthCube","09/15/2018","05/28/2019","Rebecca Willett","IL","University of Chicago","Standard Grant","Huixia Wang","09/30/2021","$300,000.00","","willett@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","041Y, 8074","047Z, 062Z","$0.00","Understanding the factors that determine regional climate variability and change is a challenge with important implications for the economy, security, and environmental sustainability of many regions around the globe. Our understanding and modeling of the large-scale dynamics of the Earth climate system and associated regional-scale climate variability significantly affects our ability to predict and mitigate climatic extremes and hazards. Earth observations and climate model outputs are witnessing an unprecedented increase in data volume, creating new opportunities to advance climate science but also leading to new data science challenges that must be addressed using tools from mathematics, statistics, and computer science. This project focuses on two central challenges at the heart of modern data-enabled climate science: (1) Increasing the predictive capacity of subseasonal forecasts by discovering and quantifying the sources of (un)predictability, including known and emergent climate modes and their interactions and non-stationarities; and (2) Understanding and quantifying the intricate space-time dynamics of the climate system to provide guidance for climate model assessment and regional forecasting.  This project brings together an interdisciplinary team that combines expertise in both hydroclimate science and statistical machine learning to create new platforms for climate diagnostics and prognostics.  The broader impacts of an enhanced knowledge of the climate system and robust and accurate seasonal forecasts have wide-ranging implications for society as a whole. For example, better seasonal forecasts will allow water resource managers to make sustainable decisions for water allocation.<br/><br/>This TRIPODS+CLIMATE project will develop novel machine learning and network estimation methodologies for analyzing the climate system over a range of space and time scales, to understand climate modes of variability and change and to explore their predictive ability for regional hydroclimatology. The two main objectives of this project are the following.  Objective 1: Develop novel classification and regression tools that account for highly-correlated features or covariates, nonlinear interaction terms in high-dimensional settings, and nonstationarity in climate observations. These tools will be used to improve seasonal-to-subseasonal forecasts of regional precipitation using multidimensional climate modes and feature vectors in the presence of evolving dynamics and nonstationarities. Objective 2: Develop network identification methods that leverage recent advances in machine learning and statistics and that can account for the nonstationarity and limited timeframe of climate data. The network representation will be used to analyze the structure and dynamics of the learned dependencies to contextualize and interpret them physically, and to quantify changing patterns in climate modes and their regional predictive capacity.  Emphasis will be placed on the western Pacific dynamics where an interhemispheric bi-directional connection has recently been discovered, promising earlier and more accurate seasonal-to-subseasonal forecasts in the southwestern US and other parts of the world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839336","TRIPODS+X:RES: Collaborative Research: Data Science Frontiers in Climate Science","DMS","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, EarthCube","10/01/2018","09/10/2018","Efi Foufoula-Georgiou","CA","University of California-Irvine","Standard Grant","Huixia Wang","09/30/2021","$300,000.00","James Randerson, Padhraic Smyth","efi@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","MPS","041Y, 1253, 8074","047Z, 062Z","$0.00","Understanding the factors that determine regional climate variability and change is a challenge with important implications for the economy, security, and environmental sustainability of many regions around the globe. Our understanding and modeling of the large-scale dynamics of the Earth climate system and associated regional-scale climate variability significantly affects our ability to predict and mitigate climatic extremes and hazards. Earth observations and climate model outputs are witnessing an unprecedented increase in data volume, creating new opportunities to advance climate science but also leading to new data science challenges that must be addressed using tools from mathematics, statistics, and computer science. This project focuses on two central challenges at the heart of modern data-enabled climate science: (1) Increasing the predictive capacity of subseasonal forecasts by discovering and quantifying the sources of (un)predictability, including known and emergent climate modes and their interactions and non-stationarities; and (2) Understanding and quantifying the intricate space-time dynamics of the climate system to provide guidance for climate model assessment and regional forecasting.  This project brings together an interdisciplinary team that combines expertise in both hydroclimate science and statistical machine learning to create new platforms for climate diagnostics and prognostics.  The broader impacts of an enhanced knowledge of the climate system and robust and accurate seasonal forecasts have wide-ranging implications for society as a whole. For example, better seasonal forecasts will allow water resource managers to make sustainable decisions for water allocation.<br/><br/>This TRIPODS+CLIMATE project will develop novel machine learning and network estimation methodologies for analyzing the climate system over a range of space and time scales, to understand climate modes of variability and change and to explore their predictive ability for regional hydroclimatology. The two main objectives of this project are the following.  Objective 1: Develop novel classification and regression tools that account for highly-correlated features or covariates, nonlinear interaction terms in high-dimensional settings, and nonstationarity in climate observations. These tools will be used to improve seasonal-to-subseasonal forecasts of regional precipitation using multidimensional climate modes and feature vectors in the presence of evolving dynamics and nonstationarities. Objective 2: Develop network identification methods that leverage recent advances in machine learning and statistics and that can account for the nonstationarity and limited timeframe of climate data. The network representation will be used to analyze the structure and dynamics of the learned dependencies to contextualize and interpret them physically, and to quantify changing patterns in climate modes and their regional predictive capacity.  Emphasis will be placed on the western Pacific dynamics where an interhemispheric bi-directional connection has recently been discovered, promising earlier and more accurate seasonal-to-subseasonal forecasts in the southwestern US and other parts of the world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812292","NSF Postdoctoral Fellowship in Biology FY 2018","DBI","Broadening Participation of Gr","08/01/2018","07/16/2018","Julie Miller","CA","Miller                  Julie          S","Fellowship","Amanda Simcox","10/31/2020","$138,000.00","","","","Los Angeles","CA","900957246","","BIO","1157","","$0.00","This action funds an NSF Postdoctoral Research Fellowship in Biology for FY 2018, Broadening Participation of Groups Under-represented in Biology. The fellowship supports a research and training plan for the Fellow that will increase the participation of groups underrepresented in biology. The goal of this project is to understand the role of group size in the evolution of self-organization in ant colonies. Across social insects, larger colony sizes are associated with greater organizational complexity, but we still don't know how enlarging group size causes this pattern. Colony organization may result from natural selection to reduce potential gridlock of large groups, targeting individual behaviors or the shape of the colony's nest, both of which may govern interactions. Alternatively, complex organization may instead be a beneficial by-product of increasing group size. The fellow will test these hypotheses by comparing species of ants with different colony sizes. The findings from this research will help elucidate the evolutionary forces structuring groups more generally, from cell aggregations to animal societies. The fellow plans to broaden participation of under-represented groups by partnering with teachers at her former schools to create classroom workshops in which students collect data for this project, thereby creating opportunities for underrepresented K-12 students to participate directly in research, and will extend the same outreach opportunities to other minority-serving LA public schools partnered with UCLA's MESA School Program. <br/><br/>The project has three, interrelated objectives that examine how colony organization has evolved in response to increasing group size. First, the fellow will distinguish whether increasing colony size leads to costly versus beneficial changes by enlarging the groups of small-colony ant species (Temnothorax) and comparing them to naturally large colony species of equivalent size (Tapinoma, Linepithema). Second, the fellow will determine the contribution of nest architecture in structuring colony organization by housing colonies in subdivided or single-chambered nests. To facilitate comparison between distantly related ant species in the above objectives, the fellow will generate standardized metrics of (1) colony organization from social networks of nutrient flow and (2) colony functionality by assessing how well foragers respond to the nutritional needs of the entire colony. The fellow will learn to fluorescently label these nutrients and track them within the colony using computer vision. Third, the fellow will investigate which aspects of nest architecture have been adapted for large colony size. The fellow will lead k-12 students in to complete this final objective by allowing classroom ant colonies of various sizes to excavate nests in observable structures. The fellow will use the classroom ant colonies to facilitate educational workshops that will be targeted to help satisfy national teaching standards in STEM fields (NGSS) at the appropriate grade level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838314","RoL:  FELS:  EAGER Rules of lifespan determination and buffering from lifelong spatiotemporal activity of key aging pathways.","IOS","Cross-BIO Activities","08/15/2018","07/16/2018","Adriana San Miguel","NC","North Carolina State University","Standard Grant","Kathryn Dickson","07/31/2021","$299,999.00","Kevin Flores","asanmiguel@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","BIO","7275","068Z, 7916, 9179","$0.00","This project will develop tools and data necessary to accurately predict lifespan from the activity of genes that respond to environmental conditions such as food intake or stress exposure. It is not fully understood how genes and the environment determine an individual's specific longevity outcome. For example, it is known that calorie reduction or reduction in oxidative stress can lead to increased longevity, but precisely predicting longevity from this information alone is not possible. Given the difficulties of studying aging in humans, the roundworm, Caenorhabditis elegans, will be used as a model to link the activity of specific genes known to affect aging. A mathematical model of the relationship between environment, genes, and longevity will be generated to reveal which genes and environments are most important in determining aging and longevity and to enable the prediction of lifespan. This information will be used in future work that focuses on whether these predictions can be extended to other organisms, and ultimately to humans. Through this work, two graduate students and one postdoctoral researcher will receive interdisciplinary training. In addition, this project will support a Latinx-directed STEM initiative for high-school students.<br/><br/>The project's main goal is to develop an experimental and analytical pipeline that will enable the establishment of rules of lifespan determination in the model organism Caenorhabditis elegans. Mathematical and statistical models that can predict lifespan of an individual based on lifelong activity of key aging pathways will be developed. The research tests whether variability in activity of key genes (due to biological noise or environmental or genetic perturbations) is propagated to variability in lifespan. To address this question, a microfluidic platform that enables monitoring of multiple individuals throughout their lifespan, while quantifying the activity of key aging pathways through endogenous fluorescent reporters, will be developed. The proposed platform will allow data pairing, where gene activity at the single individual level can be paired with its lifespan. Experimental tools that enable simultaneous quantitative analysis of in vivo gene activity and combinatorial environmental perturbations will be developed. These tools hinge on microfluidics, computer vision, automation, and new reporter strains engineered by CRISPR to track gene activity under endogenous regulatory control. The acquired data sets will be used to derive data-driven mathematical models that describe the stochastic dynamics of gene activity and its downstream phenotypic outcome: lifespan. Novel statistical techniques will be combined with mathematical models to infer phenotypic heterogeneity and robustness within populations. Interdisciplinary training will be provided to two graduate students and one postdoctoral researcher, and the award will support an educational program directed at broaden participation of Latinx high-school students in science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829369","I-Corps:  The Augmented Reality WEarable Device (ARWED) for Upper-Limb Rehabilitation","IIP","I-Corps","04/01/2018","04/04/2018","Nina Robson","CA","California State University-Fullerton Foundation","Standard Grant","Andre Marshall","09/30/2020","$50,000.00","","nrobson@fullerton.edu","1121 North State College Blvd.","Fullerton","CA","928313014","6572782106","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is based on the characteristics of the Augmented Reality WEarable Device (ARWED) for training of post-stroke patients, allowing, for example, the user to perceive an impaired arm as healthy. Preliminary training protocols, incorporated within the ARWED, make the system a reliable solution that is expected to be broadly utilized by medical professionals working in rehabilitation, sports therapy ,and convalescence. Although the ARWED has been developed mainly for the physical training of post-stroke patients, the project's explorations are expected to provide effective tools for the physical rehabilitation of patients with limb limitations at any scale -- ranging from individuals suffering partial loss of motor ability to those with severe limitations in mobility due to strokes, birth defects or accidents. Since observational learning may offer greater benefits regarding transfer to Activities of Daily Living (ADLs), in comparison to robotic-based stroke training, the medical community would also benefit from the development of the ARWED. Insights into how training using technology may enhance the recovery of motor control in diverse populations while providing a novel intervention that may prove more effective than what is currently available. <br/><br/>This I-Corps project centers on exploiting the link between action-observation and action execution in order to develop training protocols, based on observational learning techniques, to facilitate rehabilitation following a neurological disorder. Such a disorder may range from such catastrophic events as stroke or a spinal cord injury. Currently, there are limited systems that utilize virtual reality in the relearning of biological movements. The development of the ARWED required solving several challenges in computer vision, modeling, and robot motion. The current testing of the effectiveness of the device requires combined expertise from the areas of biomechanics, signal analysis, virtual reality, robotic fault recovery theory and rehabilitation. Therefore, the theoretical contributions emerging from this collaborative research is expected to advance knowledge and understanding not only within the medical field, but also across the above-mentioned research areas. The multidisciplinary nature of the research will lead to the education of undergraduate and graduate students in the areas of Engineering, Medicine and Kinesiology, among others. The outcomes of the project will facilitate a dialogue between Physical/Occupational Therapy and Engineering students and professionals that can aid in the human recovery following neurological disorders efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1804321","Non-Intrusive Interpretation and Improvement of Multi-Occupancy Human Thermal Comfort through Analysis of Facial Infrared Thermography","CBET","EnvS-Environmtl Sustainability","09/01/2018","08/01/2018","Carol Menassa","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Bruce Hamilton","08/31/2021","$365,000.00","Eunshin Byon, Vineet Kamat","menassa@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","7643","","$0.00","In the U.S. and worldwide, HVAC systems represent one of the largest energy end uses, accounting for approximately 50 percent of the total energy required to operate residential and commercial buildings. Despite the significant energy footprint of HVAC systems, occupants in the built environment are often dissatisfied with their thermal comfort. Current ""human-in-the-loop"" approaches provide opportunities for occupants to vote on thermal comfort and preferences (for example, by adjusting the thermostat), thus allowing for HVAC system adjustment based on human feedback. However, relying on intermittent human feedback prevents robust evaluation of the comfort level and determination of a comfortable setpoint. This project will explore the feasibility of using infrared thermography as a non-intrusive method for predicting human thermal comfort preferences in single and multi-occupancy building spaces. It will also design and validate a robust HVAC control framework for buildings that will synchronously use the analyzed thermography data to adjust its setpoint to improve thermal comfort in indoor spaces and reduce overall dissatisfaction among occupants in multi-occupancy spaces.<br/><br/>The project aims to explore the premise that thermal comfort can be measured non-intrusively and reliably in real, operational built environments. The resulting new knowledge has the potential to transition building HVAC control from a passive and user-empirical process to an automated, user-centric and data-driven mechanism that can simultaneously improve occupant satisfaction in indoor environments while reducing energy consumption. The research extends theory from human thermal comfort evaluation, computer vision and optimization under uncertainties with the objective of creating a robust and scalable non-intrusive HVAC control framework for thermal comfort optimization in various indoor contexts. Although the focus is on thermal comfort in this project, the developed framework and methodology can be extended to evaluate other indoor environmental quality factors such as lighting and airflow. This project will also build a publicly available thermal comfort dataset consisting of facial thermal images, subjective thermal sensations and preferences of occupants, and ambient room conditions, that will enable consistent evaluation and benchmarking of new methods in the future.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1802207","Homological Aspects of Exterior and Other Power Operations","DMS","ALGEBRA,NUMBER THEORY,AND COM","06/01/2018","04/19/2018","Claudia Miller","NY","Syracuse University","Standard Grant","Janet Striuli","05/31/2021","$168,000.00","","clamille@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","MPS","1264","","$0.00","This award supports research into one of the fundamental questions of algebraic geometry: How does one find measures of a singularity? A singularity is a place on a curve, surface, or higher dimensional space where it is not smooth, that is, it has a cusp (a sharp point, a place where it folds or turns abruptly) or it crosses itself. The understanding of singularities has applications to computer vision and medical imaging. In general, one wants to find and study measures of how extreme a singularity is and how these vary under operations such as deformations. This is done using abstract algebraic techniques as usually the dimension, or the number of equations, or variable and unknown coefficients make the spaces completely impractical to visualize. The goal is to attach invariants to singularities that are measures of the nature of the singularity. This project studies such invariants in three different ways. <br/><br/>More precisely, this project involves the study of the exterior algebra, modules of higher differentials, exterior powers of complexes, and related power operations such as Adams operations, sometimes with differential graded algebra structures playing a role. The homological behavior of exterior power operations is known to be complicated, yet these algebras and operations play a central role throughout many parts of algebra and mathematics. This project focuses on the following research directions (1) Adams operations and applications to Hilbert-Kunz multiplicities, (2) invariants of singularities for p-differentials, and (3) resolutions of graded Artinian algebras. For the first, the main goal is to develop a characteristic zero version of Hilbert-Kunz multiplicity. For the second, the focus is on studying singularities via their modules of higher differentials; the goal is to understand symmetries and the vanishing of invariants obtained from them, such as generalized Tjurina numbers, by showing how their resolutions are interrelated in the general non-isolated Gorenstein singularity setting. The third involves applying the resolution of Artinian algebras to various problems, such as conjectures on Betti numbers and finding dg-structures on resolutions in order to further understand Koszul homology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840131","FW-HTF: Collaborative Research: Enhancing Human Capabilities through Virtual Personal Embodied Assistants in Self-Contained Eyeglasses-Based Augmented Reality (AR) Systems","CMMI","FW-HTF-Adv Cogn & Phys Capblty","09/15/2018","09/13/2018","Henry Fuchs","NC","University of North Carolina at Chapel Hill","Standard Grant","Robert Scheidt","08/31/2021","$2,190,000.00","Jan-Michael Frahm, Mohit Bansal, Felicia Williams, Prudence Plummer","fuchs@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","ENG","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>This award supports basic research underpinning development of an eyeglass-based 3D mobile telepresence system with integrated virtual personal assistant. This technology will increase worker productivity and improve skills. The system automatically adjusts visual focus and places virtual elements in the image without eye strain.  The user will be able to communicate to the system by speech.  The system also uses sensors to keep track of the user's surroundings and provide the relevant information to the user automatically.  The project will explore two of the many possible uses of the system: amplifying a workers capabilities (such as a physical therapist interacting with a remote patient), and accelerating post-injury return to work through telepresence (such as a burn victim reintegrating into his/her workplace). The project will advance the national interest by allowing the right person to be virtually in the right place at the right time. The project also includes an education and outreach component wherein undergraduate and graduate students shall receive training in engineering and research methods. Course curriculum at Stanford University and the University of North Carolina at Chapel Hill shall be updated to include project-related content and examples. <br/><br/>This project comprises fundamental research activities needed to develop an embodied Intelligent Cognitive Assistant (GLASS-X) that will amplify the capabilities of workers in a way that will increase productivity and improve quality of life. GLASS-X is conceived of as an eyeglass-based 3D mobile telepresence system with integrated virtual personal assistant. Methods include: body and environment reconstruction (situation awareness) from a fusion of images provided by an eyeglass frame-based camera array and limb motion data provided by inertial measurement units; fundamental research on adaptive focus displays capable to reduce eye strain when using augmented reality displays; dialog-based communication with a virtual personal assistant, including transformations from visual input to dialog and vice versa; human subject evaluations of GLASS-X technology in two workplace domains (remote interactions between a physical therapist and his/her patient; burn survivor remote return-to-work). This research promises to push the state of the art in core areas including: computer vision; augmented reality; accommodating displays; and natural language and dialogue models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1825646","QRM: Understanding the Mechanical Design of Natural Cellular Materials via a Multiscale Quantitative Structural Representation","CMMI","GOALI-Grnt Opp Acad Lia wIndus, Materials Eng. & Processing, ","09/01/2018","08/21/2019","Ling Li","VA","Virginia Polytechnic Institute and State University","Standard Grant","Alexis Lewis","02/28/2022","$590,919.00","Yunhui Zhu","lingl@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","ENG","1504, 8092, R210","019Z, 024E, 077E, 8021, 9102","$0.00","Many biological materials systems, such as wood, bones, sponges, and sea urchins, utilize cellular structures for simultaneous mechanical function and weight saving. Similarly,  engineering cellular solids with lightweight design and mechanical efficiency, have become increasingly favored for applications in energy, infrastructure, aerospace, biomedical, and the automotive vehicle industries. While many engineering cellular materials are manufactured to have either completely stochastic or periodic lattices, natural cellular materials often exhibit complex three-dimensional cellular designs with controlled structural morphologies at multiple length scales. This is believed to contribute to their remarkable mechanical robustness, especially considering their mechanically weak or soft constituents. This award will support a fundamental study to elucidate the multi-scale structural basis of the high-performance natural cellular materials, through an integrated multi-disciplinary effort involving materials science and engineering, mechanical engineering, computer vision, and data science. The insights gained from natural cellular materials will provide important guidance in the design and manufacturing of engineering cellular materials, deepen our understanding of structural cellular solids, and provide insights for the design and fabrication of advanced lightweight materials and structures, with potential benefits to US Manufacturing and Infrastructure sectors.<br/><br/>This work will establish a quantitative, comprehensive, and efficient structural representation scheme for the multi-scale cellular network of highly porous structures found in natural systems. The researchers will study the bioceramic cellular structure of sea urchin spines as a testbed system, and multiscale structural representations will elucidate fundamental understanding of their remarkable mechanical robustness and damage tolerance. The work will address the challenges of representing complex, hierarchical cellular structures via three major innovations, including high-resolution tomography imaging and adaptive compressive data processing, multi-scale representation of the cellular structure (individual strut and node level, local unit-cell level, and global cellular network level), and in-silico ""regrowth"" of bio-inspired cellular structures with morphological control at multiple length scales. The research team will further validate the multi-level cellular structural representation through systematic finite element simulations coupled with synchrotron imaging-based in-situ mechanical testing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806501","Multiscale Studies of Collective Behavior in a Model Social Bacterium","PHY","PHYSICS OF LIVING SYSTEMS","09/15/2018","07/01/2019","Joshua Shaevitz","NJ","Princeton University","Continuing Grant","Krastan Blagoev","08/31/2022","$312,432.00","","Shaevitz@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","MPS","7246","7237, 9183","$0.00","The ability of groups of individuals to form complex and dynamic spatial patterns is a key aspect of biological phenomena ranging from collective behavior to multi-cellularity to development. In a cellular context, this often involves complicated chemical signaling and chemotaxis strategies. However, the PI has recently discovered that some bacterial species have evolved to take advantage of an active-matter phase separation that generates patterns without the need for chemical signaling.  This project strives to understand this process from a physicist's perspective, but is hindered by a lack of tools to physically probe the mechanical properties and interactions of groups of motile bacteria. The PI's work with the single-celled bacterium Myxococcus xanthus focuses on the molecular details of force generation and the interactions of cells at the start of collective starvation-induced fruiting body formation. In this project, the PI seeks to explain the process of fruiting body development as an active dewetting process, linking new theoretical models with cutting-edge experimental data. The PI's research goals are complemented by an outreach plan that aims to involve more undergraduate students in biological physics and to engage non-scientists through public lectures. The PI's goals over the next few years include (i) expanding the Integrated Science program for first year undergraduates, (ii) starting a summer school aimed at advanced undergraduates, and (iii) putting on a series of public lectures in New York City meant to convey the excitement and innovation of biophysics using examples relevant to everyday life.<br/><br/>The three aims below seek to determine the role of motility and adhesion in driving starvation-based dewetting. The PI's current models of Myxococcus xanthus aggregation rely on particle jamming in 2D. While these incredibly simple models capture some of the observed phenomena, the actual dynamics occur in 3D as the population dewets off the surface without jamming to form round droplets. This more complicated reality requires more sophisticated experimental data.  The laboratory combines expertise in Myxococcus xanthus motility, cutting-edge imaging techniques, force microscopy, and computer vision analyses, making the group uniquely qualified to carry out the proposed research. Aim 1: To understand the forces that cells generate on each other and on a substrate, the PI will measure cell-cell and cell-substrate mechanical interactions using a custom-built optical trapping microscope and mutant strains that lack specific motor proteins and adhesion molecules. Aim 2: To probe the motility of cells inside a fruiting body, the group will track cells in 3D using confocal microscopy to (i) compare the motion over time and between different locations in the aggregate, and (ii) investigate the formation of layered structures and flows within the fruiting body. Aim 3: To probe the macroscopic mechanics involved in fruiting body formation, the group will (i) measure the development of droplet shape and rheology using confocal imaging and atomic force microscopy, and (ii) probe the forces generated on the substrate using traction force microscopy. At each step, data from the three aims will be used to test, and be tested by, the active-dewetting theories being developed by the group's collaborators. This project lies firmly within the goals of the Physics of Living Systems program at the NSF by using physical measurements and analyses to understand the dynamics of living cells across spatial scales.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819248","SBIR Phase I:  Analysis of Progress Photos for Indoor Construction Progress Monitoring","IIP","SMALL BUSINESS PHASE I","06/15/2018","06/15/2018","Derek Hoiem","IL","RECONSTRUCT INC","Standard Grant","Peter Atherton","11/30/2018","$224,996.00","","dhoiem@illinois.edu","60 Hazelwood Dr","Champaign","IL","618207460","4129526964","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is that it will save billions of dollars by anticipating construction delays and streamlining coordination to prevent them. Lower costs will help upgrade our nation's infrastructure, which is a critical national priority. Many construction companies strongly want a solution to indoor progress monitoring. Laser scanning is too expensive and slow, and photography services can cost hundreds of thousands of dollars for large projects and introduce logistical challenges. The proposed research would simplify the workflow for progress monitoring of interiors by automatically registering 360 degree photos and video and aligning them to building information models (BIM), providing a cheap, quick, and effective solution for daily progress monitoring.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project aims to localize images, reconstruct 3D models, align them to floorplans or 3D BIM, and use the aligned models to provide actionable data to construction managers. A main challenge is to robustly solve for camera pose using structure-from-motion in indoor scenes that are unfinished and contain textureless and reflective surfaces. A second challenge is to automatically or semi-automatically register the models to 2D or 3D plans, made more difficult by the fact that the site is incomplete and constantly changing. A third challenge is to create interfaces that project personnel can use to perform visual inspection, progress monitoring, requests for information, and other supervision and coordination tasks. The project will investigate the use of tags (i.e., markers) to improve robustness of 3D reconstruction and to perform registration without requiring the 3D positions of tags to be known in advance. Thus, the project addresses major unsolved problems in computer vision, robotics, and their application to construction management.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821258","Collaborative research: Gaussian Process Frameworks for Modeling and Control of Stochastic Systems","DMS","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","08/01/2018","08/07/2018","Robert Gramacy","VA","Virginia Polytechnic Institute and State University","Standard Grant","Branislav Vidakovic","07/31/2021","$150,000.00","","rbg@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","MPS","1253, 8004, 8069","026Z, 8004, 9263","$0.00","Quantitative models for decision making under uncertainty continue to attract intense effort across natural sciences and engineering. With the advent of ever more sophisticated models in applications, computational demands continue to outpace what is feasible and the premium on efficient numerical approaches remains high. The investigators will explore synergies between the latest machine learning techniques and control paradigms, arising in applications as diverse as finance, energy storage and security, and the epidemiological modeling of infectious diseases. The developed ""smart"" algorithms will deliver performance upgrades essential for using simulations in tackling large-scale/complex settings. The project will also contribute to inter-disciplinary training in mathematical sciences across undergraduate, graduate and post-doctoral levels. <br/><br/>The investigators will investigate statistical learning techniques for modeling, analysis and control of nonlinear dynamic stochastic systems. Through developing algorithms and statistical models for complex stochastic simulators, and active learning strategies for autonomous data acquisition, the project will achieve enhanced capabilities and efficiency in mathematical analysis of dynamic random phenomena. The approach hinges on the use of high fidelity approximate Gaussian Process surrogates to adaptively allocate computing resources in order to maximize the learning rate of the input-output relationship for modeling objectives or of the input-control map for dynamic programming. By connecting stochastic simulation with machine learning and non-parametric statistics, and integrating with the computational implementation, the project will enhance knowledge discovery in large-scale simulation and optimization settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822831","Collaborative Research: Human-Technology Partnership Supporting Career Path Exploration and Navigation","IIS","Cyberlearn & Future Learn Tech","09/01/2018","07/25/2018","Carolyn Rose","PA","Carnegie-Mellon University","Standard Grant","Amy Baylor","08/31/2021","$550,000.00","Geoffrey Gordon, Norman Bier","cprose@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8020","063Z, 8045","$0.00","The employment landscape is changing and expanding, creating uncertainty as individuals navigate decisions regarding career preparation and continuing education.  This project develops technology to provide guidance for college students to access high quality information and guidance within a large and complex decision space. The approach includes integrating social support from human career coaches or peers to increase confidence and motivation in the problem solving and decision making process. The approach is fundamentally data driven, built upon computational modeling while informed by theories of human behavior, embodied within a novel machine learning paradigm termed Socially-Sensitive Reinforcement Learning (SSRL). Based on this new paradigm, the system will generate guidance to support students while being sensitive to student needs and preferences so that there is a high probability that students will accept and benefit from the guidance. The project targets nontraditional students who embark upon or advance STEM career paths and aims to broaden participation in STEM.<br/><br/>The project contributes to both human-computer interaction and machine learning and takes an interdisciplinary approach in its modeling of human interaction and sociotechnical support for learning. It comprises a three pronged solution: (1) a computational modeling strand proposes new computational paradigms that continually map best practices and inform data-driven recommendations in support of effective decision making; (2) a behavioral research strand provides insight through qualitative and quantitative investigations, uncovering properties of recommendations as well as recommenders that are associated with whether guidance is accepted and how it influences success in career advancement; and (3) an intelligent coach development strand embodies findings from the other two strands in the CareerScope Intelligent Coach Agent (ICA), designed as a sociotechnical solution leading to impact student decision making. The partnership with Western Governor's University (WGU) facilitates transition of research into practice at large scale through research with WGU students and deployment on the WGU platform. While the research will be housed within a single platform, the technical innovations will ultimately be integrated into a wide range of platforms. The results of this project will broaden understanding of the limitations and opportunities for ICAs to guide learners, particularly non-traditional learners, through their career paths. These results can inform both online and brick-and-mortar universities how to best coach/train/mentor students or integrate peer/professional mentorship and peer interaction into current advising practices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821240","Collaborative Research: Gaussian Process Frameworks for Modeling and Control of Stochastic Systems","DMS","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","08/01/2018","08/07/2018","Michael Ludkovski","CA","University of California-Santa Barbara","Standard Grant","Branislav Vidakovic","07/31/2021","$150,000.00","","ludkovski@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1253, 8004, 8069","026Z, 8004, 9263","$0.00","Quantitative models for decision making under uncertainty continue to attract intense effort across natural sciences and engineering. With the advent of ever more sophisticated models in applications, computational demands continue to outpace what is feasible and the premium on efficient numerical approaches remains high. The investigators will explore synergies between the latest machine learning techniques and control paradigms, arising in applications as diverse as finance, energy storage and security, and the epidemiological modeling of infectious diseases. The developed ""smart"" algorithms will deliver performance upgrades essential for using simulations in tackling large-scale/complex settings. The project will also contribute to inter-disciplinary training in mathematical sciences across undergraduate, graduate and post-doctoral levels. <br/><br/>The investigators will investigate statistical learning techniques for modeling, analysis and control of nonlinear dynamic stochastic systems. Through developing algorithms and statistical models for complex stochastic simulators, and active learning strategies for autonomous data acquisition, the project will achieve enhanced capabilities and efficiency in mathematical analysis of dynamic random phenomena. The approach hinges on the use of high fidelity approximate Gaussian Process surrogates to adaptively allocate computing resources in order to maximize the learning rate of the input-output relationship for modeling objectives or of the input-control map for dynamic programming. By connecting stochastic simulation with machine learning and non-parametric statistics, and integrating with the computational implementation, the project will enhance knowledge discovery in large-scale simulation and optimization settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750649","CAREER: Robotic Manipulation Using Deep Deictic Reinforcement Learning","IIS","Robust Intelligence","09/01/2018","07/23/2020","Robert Platt","MA","Northeastern University","Continuing Grant","Erion Plaku","08/31/2023","$499,904.00","","r.platt@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7495","1045, 7495","$0.00","As robot tasks and environments become more complex, it is getting too challenging to program every detail of the robot's behavior explicitly, by hand.  An alternate approach is to learn behaviors through experience, a type of machine learning known as ``reinforcement learning'', where the robot learns through trial and error. Pure trial and error, however, is inefficient, which means it takes the robot a long time to learn.  The goal of this project is to enable robots to focus attention on the parts of the environment that lead to effective learning and good generalization to new tasks.  A result of this research is the ability of assistive robots in the home, such as an assistive wheelchair equipped with a robotic arm, to learn how to better help the infirm and people with disabilities.<br/><br/>This project will develop a new approach to applying deep reinforcement learning (deep RL) to robotic manipulation problems by incorporating deictic representations. A deictic representation encodes state/action relative to a marker that the agent places in the environment. In this project, the marker is a 6-DOF reference frame, placed in a 3-D point cloud, or truncated signed distance function. The robot decides where to place the marker and how it should move relative to that marker by solving a Markov decision process using deep reinforcement learning. Preliminary results suggest that this new method can enable robots to learn control policies that solve complex manipulation problems without the need for precise geometric models of the objects being manipulated. While the method still estimates some elements of object pose implicitly, it does so in a way that generalizes well to novel objects and does not necessarily estimate full object pose unless required by the task.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818945","Collaborative Research: Geometric Analysis and Computation for Generative Models","DMS","COMPUTATIONAL MATHEMATICS","07/01/2018","06/21/2018","Xiuyuan Cheng","NC","Duke University","Standard Grant","Leland Jameson","06/30/2021","$100,000.00","","xiuyuan.cheng@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","MPS","1271","062Z, 9263","$0.00","Research in unsupervised learning and generative models is concerned with uncovering structure and relationships in data with the intent of being able to generate new, as yet unseen, examples of the data set. Generative models learn the distribution of a data set from finite samples and provide an efficient sampler of the approximated density, rather than relying on labels for supervision. These models are a powerful tool for analyzing large volume, high-dimensional data in an unsupervised way. While generative models are an active research topic in machine learning, many theoretical and computational questions for such models remain unclear. This collaborative research project will study generative models from a geometric perspective, focusing on both performance guarantees and efficient implementations. The ability to efficiently create new data points that are guaranteed to be similar to the existing data has important implications in a variety of applications, including medical data analysis and privacy, bioinformatics, modeling of image and audio signals, and general high-dimensional data analysis in which it is difficult to collect labeled data for supervised algorithms.<br/><br/>The ideas and approaches in this research project center around the techniques that have evolved in the manifold learning field over the past decade. These mathematical tools, in particular local neighborhood preserving maps, approximation analysis in terms of intrinsic dimensionality, and construction of global coordinate systems based upon local affinity, have natural applications in the study of generative models. The project is comprised of four fundamental questions that arise in the field: (a) What are the types of distributions that generative networks are capable of learning efficiently, and how does the intrinsic dimensionality of the distribution affect convergence? (b) How can non-parametric generative models be created for dimension-reduced representations that arise in manifold learning, and which only depend on the intrinsic geometry of the data? (c) How can efficiently-computed metrics be defined between high-dimensional distributions for use in assessing the validity of various generative models? (d) How can these metrics be used to examine the various paths generative models take through the parameter space while being trained, and what clusters of starting points give optimal generators? The project will focus on both mathematical and computational aspects of these problems, aiming at resolving fundamental questions about these tools that are widely used in various data analysis and signal processing applications in science and industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755769","CRII: III: Theory and Practice of Learning on Graphs","IIS","Info Integration & Informatics","09/01/2018","03/28/2018","Zhenming Liu","VA","College of William and Mary","Standard Grant","Wei Ding","08/31/2021","$174,983.00","","zliu20@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7364","7364, 8228","$0.00","Learning on graphs plays a key role in digital communications. Tech companies (social media, etc.) rely on social network analysis in their core services. Financial services (proprietary trading, funds, etc.) use interaction networks of on-list companies to ""beat the market"". Intelligence agencies analyze the interactions between terrorists to understand the structure of terrorist cells. However, there is a gap between the theory and practice of designing graph-learning algorithms. On the theory side, substantial progress has been made to design learning algorithms for stylized models, but the practical impacts are limited. On the practice side, efficient heuristics have been developed to analyze real-world graphs, but they often lack theoretical guarantees in terms of running time or accuracy. To bridge the theory-practice gap, this project will draw upon techniques from theoretical computer science, machine learning, and high-dimensional statistics to design theoretically sound, graph-learning algorithms that can (i) automatically choose the most suitable statistical network, (ii) leverage node-level information (e.g., users' profile in a social network), and (iii) properly handle the serial correlations in a graph. The algorithms will help practitioners to build more accurate network models at reduced cost, and have beneficial economic and social impacts on downstream applications.<br/><br/>From a technical standpoint, this research program will study the design of statistically sound, computationally tractable, and practically relevant algorithms for inferring latent structures in graphs. It consists of two thrusts: (i) Realistic interaction models. The project will use random graph theory and kernel learning to develop data-driven procedures to automatically select the most suitable assortative graph models, and introduces and analyzes a semi-adversarial model that can analyze more realistic interactions than those captured by real-valued weighted graphs; (ii) Time-evolving statistical models. The project will study how tools from property testing results in theoretical computer science can detect change-points in a network evolution process, and then merges the spectral methods and tools from high-dimensional statistics to build new inference algorithms for serially correlated graphs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815145","AF: Small: Approximation Algorithms for Learning Metric Spaces","CCF","Algorithmic Foundations","06/01/2018","05/24/2018","Anastasios Sidiropoulos","IL","University of Illinois at Chicago","Standard Grant","A. Funda Ergun","05/31/2021","$399,899.00","","Sidiropo@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","7796","7923, 7926, 7929","$0.00","The analysis of large and complicated data sets is a task of increasing importance in several brunches of science and engineering. In many application scenarios, the data consists of a set of objects endowed with some information that quantifies similarity or dissimilarity of certain pairs. Examples of such inputs include statistical distributions, user preferences in social networks, DNA sequences, and so on. A natural way to interpret such data sets is as metric spaces. That is, each object is treated as a point, and the dissimilarity between two objects is encoded by their distance. The successful application of this data-analytic framework requires finding a distance function that faithfully encodes the ground truth. The project seeks to develop new algorithms for computing such faithful metrical representations of data sets. The underlying research problems lead to new connections between the areas of computational geometry, algorithm design, and machine learning. The project aims at transferring ideas between these scientific communities, and developing new methods for data analysis in practice. The project will be part of the investigator's curriculum development, teaching, and educational and outreach activities. The main research problems that the project seeks to study will form the basis for the training of both undergraduate and graduate students. The investigator is committed to working with minorities and underrepresented groups.<br/><br/><br/>This metrical interpretation of data sets has been successful in practice and has lead to a plethora of algorithmic methods and ideas, such as clustering, dimensionality reduction, nearest-neighbor search, and so on. The area of metric learning is concerned mainly with methods for discovering an underlying metric space that agrees with a given set of observations. Specifically, the problem of learning the distance function is cast as an optimization problem, where the objective function quantifies the extend to which the solution satisfies the input constraints. A common thread in many works on metric learning is the use of methods and ideas from computational geometry and the theory of metric embeddings. Over the past few decades, these ideas have also been the subject of study within the theoretical computer science community. However, there are several well-studied problems in metric learning that have not received much attention in the algorithms and computational geometry communities. Moreover, there are many metric embedding tools and techniques, that where developed within the algorithms community, that have not yet been used in the context of metric learning. The project aims at bridging this gap by a systematic study of algorithmic metric learning problems from the point of view of computational geometry and approximation algorithms. The major goals of this effort are transferring algorithmic tools to the metric learning framework, as well as providing theoretical justification for metric learning methods that are successful in practice but currently lack provable guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1904183","III: Small: Collaborative Learning with Incomplete and Noisy Knowledge","IIS","Info Integration & Informatics","07/01/2018","11/18/2019","Quanquan Gu","CA","University of California-Los Angeles","Standard Grant","Wei Ding","07/31/2021","$350,919.00","","qgu@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364","7364, 7923","$0.00","The accelerated growth of Big Data has created enormous amount of information at the macro level for knowledge discovery. But at the micro level, one can only expect a handful of observations in most individual users. This hinders the exploration of subtle patterns and heterogeneities among distinct users for improving the utility of Big Data analytics at a per-user basis. The objective of this project is to develop a set of algorithmic solutions to perform online learning in a collaborative fashion, where personalized learning solutions actively interact with users for feedback acquisition and collaborate with each other to learn from incomplete and noisy input.  This project amplifies the utility of statistical learning in many important fields, such as healthcare, business intelligence, crowdsourcing, and cyber physical systems, where automated decision models are built on diverse, noisy and heterogeneous supervision. The research activities will be incorporated into teaching materials for student training and education in the areas of information retrieval, machine learning and data mining. <br/><br/>This project consists of three synergistic research thrusts. First, it develops a family of contextual bandit algorithms to perform collaborative online learning over networked users. Dependency among users is estimated and exploited to collaboratively update the individualized bandit parameters. Second, it develops principled solutions to optimize task-specific and general loss functions for online learning, which enables the collaborative learning solutions reach more important real-world applications, such as information retrieval and user behavior modeling. Third, it models and differentiates the reliability of the sources of feedback to optimize the overall online learning effectiveness, which is especially important in the applications such as health informatics, crowdsourcing and cyber physical systems. Expected outcomes of the project include: 1) open source implementations for the developed online learning solutions; and 2) evaluation corpora that will enable researchers to conduct follow-up research in related domains."
"1838147","BIGDATA: F: Collaborative Research: Collective Mining of Vertical Social Communities","IIS","Big Data Science &Engineering","09/15/2018","09/06/2018","Arjun Mukherjee","TX","University of Houston","Standard Grant","Sara Kiesler","08/31/2022","$305,963.00","","amukher6@Central.UH.EDU","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","8083","062Z, 7433, 8083","$0.00","A large fraction of internet social media content is found in thousands of specialized communities that are hosted by news outlets, typically in the form of reader forums or comments on news articles. The users of the such a site are said to form a vertical social community (VSC), because they deeply engage with a single media source.  While each VSC is tiny compared to broad communities such as Facebook, they are important because they expose how different segments of society feel about various world events. This can be a very useful resource for downstream intelligence and predictive analytics.  However, current web crawlers cannot effectively access VSCs. Thus their data is invisible to search engines, and remains hidden from analytics tools.  The goals of this project are to enable effective access to vertical social communities coalesced at news reports online, and to mine their comments and debates. This project will provide researchers with tools to collect data from these communities and analyze them.  The educational component of the project includes the involvement of graduate and undergraduate student training and research and the incorporation of research projects and results in courses.<br/><br/>The researchers will develop algorithms to unearth the content generated at thousands of vertical social communities and make their content transparently accessible to data management and analytics tools. The researchers will develop novel deep learning techniques for content detection, and build a novel scalable end-to-end system for real-time access and collective mining of these communities, capable of handling large parallel data streams based on shifting ideas. The specific algorithms will include user population estimation, bootstrap communication patterns for automatic crawling of content, and fine-grained sentiment analysis for intelligence and predictive analytics. Software tools will be made available to researchers in academe and industry. Distribution of free, open-source software for implementing the techniques developed will enhance existing research infrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838145","BIGDATA: F: Collaborative Research: Collective Mining of Vertical Social Communities","IIS","Big Data Science &Engineering","09/15/2018","09/06/2018","Eduard Dragut","PA","Temple University","Standard Grant","Sara Kiesler","08/31/2022","$427,912.00","","edragut@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","8083","062Z, 7433, 8083","$0.00","A large fraction of internet social media content is found in thousands of specialized communities that are hosted by news outlets, typically in the form of reader forums or comments on news articles. The users of the such a site are said to form a vertical social community (VSC), because they deeply engage with a single media source.  While each VSC is tiny compared to broad communities such as Facebook, they are important because they expose how different segments of society feel about various world events. This can be a very useful resource for downstream intelligence and predictive analytics.  However, current web crawlers cannot effectively access VSCs. Thus their data is invisible to search engines, and remains hidden from analytics tools.  The goals of this project are to enable effective access to vertical social communities coalesced at news reports online, and to mine their comments and debates. This project will provide researchers with tools to collect data from these communities and analyze them.  The educational component of the project includes the involvement of graduate and undergraduate student training and research and the incorporation of research projects and results in courses.<br/><br/>The researchers will develop algorithms to unearth the content generated at thousands of vertical social communities and make their content transparently accessible to data management and analytics tools. The researchers will develop novel deep learning techniques for content detection, and build a novel scalable end-to-end system for real-time access and collective mining of these communities, capable of handling large parallel data streams based on shifting ideas. The specific algorithms will include user population estimation, bootstrap communication patterns for automatic crawling of content, and fine-grained sentiment analysis for intelligence and predictive analytics. Software tools will be made available to researchers in academe and industry. Distribution of free, open-source software for implementing the techniques developed will enhance existing research infrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821231","Collaborative Research: New Statistical Learning for Complex Heterogeneous Data","DMS","CDS&E-MSS","09/01/2018","06/26/2018","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Standard Grant","Christopher Stark","08/31/2021","$110,000.00","","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","MPS","8069","8083, 9263","$0.00","This project focuses on several important and challenging issues concerning complex heterogeneous data that arise from medical imaging and social networks. The major goals are to develop powerful and innovative statistical machine learning methods and tools that are able to flexibly model signal heterogeneity across images, integrate imaging data with multimodal and spatially distributed data, and tackle heterogeneity of network data. The integrated program of research and education will have significant impacts in many different fields such as biomedical studies, genomic research, environmental studies, public health research, and social and political sciences, among others. Te project will also stimulate interdisciplinary research and collaboration with scientists from disparate fields.<br/><br/>This project will lead to substantial advancement in heterogeneity learning and modeling through exploiting individual variation from the general population, and integration of multiple sources of imaging information to enhance prediction accuracy for disease diagnoses and treatment outcomes. In addition, this project develops innovative unsupervised learning methods through utilizing node covariate information for analyzing heterogeneous network data. Each component of the research plan contains a broad range of topics, from methodological and computational development to applications in real world problems. Specifically, the PIs study subject-variant scalar-on-image regression models to incorporate the heterogeneity variation for brain imaging data, multi-dimensional tensor learning methods for breast cancer imaging data, flexible Gaussian graphical models for network data, and a novel clustering framework for heterogeneous data that are linked by networks. Furthermore, the development of advanced optimization techniques, algorithms and computational technologies will be applicable to many practical problems arising from large-scale heterogeneous data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824998","Synthesis and design workshop: Research Priorities in Learning Analytics","DRL","STEM + Computing (STEM+C) Part","09/01/2018","07/03/2018","Stephanie Teasley","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Amy Baylor","08/31/2019","$100,000.00","Rada Mihalcea","steasley@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","EHR","005Y","7556, 8045","$0.00","This workshop is funded through the ""Dear Colleague Letter: Principles for the Design of Digital Science, Technology, Engineering, and Mathematics (STEM) Learning Environments (NSF 18-017)."" Post-secondary educational institutions are being buffeted by three major transitions: changes in the nature of the competencies students need and want, changes in the demographic mix of people that they serve, and changes in the technologies and strategies available to build, measure, and communicate competence and expertise. Powerful new information technologies, unprecedented opportunities to gather and analyze large volumes of data, and new insights into how people learn, make it possible to imagine designing learning environments that make learning more productive, more affordable, and more gracefully adaptable to individuals throughout their lifetimes.  The new field of learning analytics has made major advances in understanding how educational Big Data can produce insights to improve classroom practices. Yet, a number of critical questions remain unanswered in important areas to ensure all Americans receive the education they need to prosper in a modern economy. The aim of this workshop is to investigate the role of learning analytics in contributing to advances in technological learning environments. <br/><br/>The workshop will bring together interdisciplinary experts to articulate the state-of-the-art and propose research priorities for learning analytics in the coming decade.  A central theme will be to explore new ways to use powerful tools in data science (machine learning, social network analysis, analytics and visualization of complex data, temporal, multi-scale and statistical models, integration of heterogeneous data, data scrubbing, wrangling and provenance tracking, data privacy and cybersecurity) to define competence, measure it, and build it using a rich array of new approaches to study learning. The workshop will also explore how the full power of these tools can be applied to the most critical challenges faced in learning analytics. The outcome of the workshop will be a clear definition of the highest priority research needs in learning analytics and a practical roadmap to guide public and private research support in these areas. The resulting white paper will provide insights to improve a wide range of learning environment contexts in in post-secondary education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821198","Collaborative Research: New Statistical Learning for Complex Heterogeneous Data","DMS","CDS&E-MSS","09/01/2018","06/26/2018","Annie Qu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Christopher Stark","03/31/2020","$124,999.00","","aqu2@uci.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","8069","8083, 9263","$0.00","This project focuses on several important and challenging issues concerning complex heterogeneous data that arise from medical imaging and social networks. The major goals are to develop powerful and innovative statistical machine learning methods and tools that are able to flexibly model signal heterogeneity across images, integrate imaging data with multimodal and spatially distributed data, and tackle heterogeneity of network data. The integrated program of research and education will have significant impacts in many different fields such as biomedical studies, genomic research, environmental studies, public health research, and social and political sciences, among others. The project will also stimulate interdisciplinary research and collaboration with scientists from disparate fields.<br/><br/>This project will lead to substantial advancement in heterogeneity learning and modeling through exploiting individual variation from the general population, and integration of multiple sources of imaging information to enhance prediction accuracy for disease diagnoses and treatment outcomes. In addition, this project develops innovative unsupervised learning methods through utilizing node covariate information for analyzing heterogeneous network data. Each component of the research plan contains a broad range of topics, from methodological and computational development to applications in real world problems. Specifically, the PIs study subject-variant scalar-on-image regression models to incorporate the heterogeneity variation for brain imaging data, multi-dimensional tensor learning methods for breast cancer imaging data, flexible Gaussian graphical models for network data, and a novel clustering framework for heterogeneous data that are linked by networks. Furthermore, the development of advanced optimization techniques, algorithms and computational technologies will be applicable to many practical problems arising from large-scale heterogeneous data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833553","EAGER-DynamicData: Subspace Learning From Binary Sensing","ECCS","Big Data Science &Engineering","01/01/2018","08/14/2018","Yuejie Chi","PA","Carnegie-Mellon University","Standard Grant","Lawrence Goldberg","08/31/2019","$81,760.00","","yuejiechi@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","ENG","8083","153E, 7916","$0.00","Decentralized sensing systems play an increasingly critical role in everyday life, including wireless sensor networks, mobile crowd-sensing with internet-of-things, and crowdsourcing with human workers, with applications in network analysis, distributed wideband spectrum sensing, target tracking, environmental monitoring, and advertisement prediction. Despite the promise, however, efficient inference is extremely challenging due to processing large amounts of data at the typically resource-starved sensor nodes. This project develops efficient feature extraction and dimensionality reduction tools for decentralized sensing systems with minimal computation, storage and communication requirements of each sensor node to make sense of the surrounding dynamic environments. Students on this program will develop multi-disciplinary expertise in signal processing, machine learning, optimization, and statistics. New graduate-level courses on high-dimensional data analysis will be developed by the PI at Ohio State University. <br/><br/>More specifically, this project offers an integrated approach for subspace learning from bits, where the sampling strategy explicitly accounts for the communication burden by only requesting a single bit from each sensor node. This project opens up opportunities to develop a theory of principal component analysis (or subspace learning) based on binary sensing, where noisy data samples are synthesized into coarse yet high-fidelity binary measurements that are more amenable for communication and inference. The consideration of binary measurements is well-motivated, as in practice, measurements are either mapped to bits from a finite alphabet before computation, or available naturally in the quantized form, such as comparison outcomes from human as sensors; constraints in storage and communication are often expressed in terms of the number of bits rather than the number of real measurements; finally, binary measurements are also more robust against unknown, nonlinear and heterogeneous distortions from different sensors compared with real measurements. Unfortunately, none of the existing subspace learning frameworks is tailored to acquire and process quantized measurements, and will yield highly sub-optimal results if naive quantization is applied. This project addresses the above challenge and highlights a novel interplay between the quantity, precision, and fidelity of measurements in sensing for estimating and tracking a low-dimensional subspace in a dynamic environment. Decentralized and online inference algorithms for subspace learning are developed together with adaptive sensing schemes to speed up convergence."
"1838427","Student Travel Support for the 26th ACM International Conference on Multimedia 2018 (ACM MM 2018)","IIS","Info Integration & Informatics","06/15/2018","05/24/2018","Vivek Singh","NJ","Rutgers University New Brunswick","Standard Grant","Maria Zemankova","05/31/2020","$15,543.00","","vivek.k.singh@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7364","7364, 7556","$0.00","This student travel grant supports about 15 graduate/undergraduate students enrolled in U.S. institutions to attend the 26th annual ACM Multimedia Conference 2018 (ACM MM 2018) to be held in Seoul, Korea October 22-26, 2018. This is the worldwide premier conference and a key world event to present and discuss scientific achievements and innovative industrial products in the field of multimedia computing. Participation in such an event allows students to learn/experience innovative research, develop professional networks, and interact with industry and academia leaders involved in practical applications and technology transfer. The opportunity will be broadcast widely to the technical community; students from underrepresented groups will be particularly encouraged to apply. The active participation in ACM Multimedia 2018 will be valuable for students to prepare for the success of their research career. Ultimately, the participation by U.S.-based students will support development of the next generation multimedia researchers and foster US competitiveness. <br/><br/>ACM Multimedia covers a broad spectrum of technical areas, including: Understanding (Multimedia and Vision, Multimodal Analysis and Description, Deep Learning for Multimedia); Engagement (Emotional and Social Signals in Multimedia, Multimedia Search and Recommendation, Social Multimedia, Multimedia Art, Entertainment and Culture); and Experience (Multimedia human-computer interaction and Quality of Experience, Multimedia Art, Entertainment and Culture, Multimedia for Collaboration in Education and Distributed Environments, Music, Speech and Audio Processing in Multimedia). The supported students will greatly benefit from participating in the conference, where they will interact with research leaders from a variety of disciplines and will be exposed to the most recent developments in multimedia research as showcased in keynote addresses, latest technical papers, technical demos, grand challenge competition, open source software competition, doctoral symposium, and also an interactive art program stimulating artists and computer scientists to meet and discover together the frontiers of effective communications. Further, each supported student will be paired with a senior mentor who would spend one-on-one time with the student and advise them on strengthening their current research work as well as their broader future research agenda. This will ensure that students will be actively engaged and will receive valuable advice. More details on the conference are available at ACM MM 2018 website (http://www.acmmm.org/2018/) and the proceedings of the conference will be available via the ACM Digital Library (https://dl.acm.org/).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746819","SBIR Phase I:  Building Extensible and Customizable Binary Code Analytics Engine for Malware Intelligence as a Service","IIP","SBIR Phase I","01/01/2018","12/27/2017","Xunchao Hu","CA","DEEPBITS TECHNOLOGY LLC","Standard Grant","Peter Atherton","08/31/2019","$224,987.00","","xchu@deepbitstech.com","20871 Westbury Rd.","Riverside","CA","925082974","9518276437","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to spark more cybersecurity innovations, by reducing the R&D expenditures via providing fundamental security analytics tools as a service. Global cybersecurity spending is increasing significantly year over year. Enormous R&D resources have been invested in the development of a range of security products to meet this market. However, different security product providers repeatedly build the fundamental security analytics tools and use them to further develop different innovative security solutions. That is a huge waste of R&D resources.  The proposed solution reduces the R&D expenditure of customers and lowers the entry bar for the growing cybersecurity market. With the lowered entry bar, the company anticipates that more innovations will be put into practice. As a result, with the increased competition and reduced R&D expenditure, the company expects a reduction in cybersecurity spending by companies and the government.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project focuses on malware intelligence, which has been a long-standing as well as increasingly complex cybersecurity problem. Traditional signature based detection and manual reverse engineering approaches can no longer keep up with the pace of increasingly sophisticated obfuscation and attack techniques. The objective of this project is to develop a security analysis tool for malware intelligence by combining the following two unique techniques: ""whole-system emulation based dynamic binary analysis"" and ""deep-learning based binary code similarity detection"". The first technique provides a fine-grained monitor capability to observe the behaviors of malware. The second technique provides the capability of learning and characterizing complex features. By combining these two techniques, the proposed technology will be able to better understand malware and generate actionable intelligence."
"1816495","SaTC: CORE: Small: Towards Robust Moving Target Defense: A Game Theoretic and Learning Approach","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace, EPSCoR Co-Funding","08/15/2018","02/24/2020","Zizhan Zheng","LA","Tulane University","Standard Grant","Sara Kiesler","07/31/2021","$260,105.00","","zzheng3@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","1714, 8060, 9150","025Z, 065Z, 7434, 7923, 9150, 9178, 9251","$0.00","Malicious attacks are constantly evolving to inflict even more damage on the nation's infrastructure systems, corporate information technology (IT) systems, and our digital lives. A fundamental obstacle to achieving effective defense is information asymmetry, through which, under current static and passive defense schemes, the attacker has essentially limitless time to observe and learn about the defender, while the defender knows very little about the attacker. A promising approach to reverse information asymmetry is Moving Target Defense (MTD), whereby the defender dynamically updates system configurations to impede the attacker's learning process. Although MTD has been successfully applied in various domains, existing solutions typically assume an attacker with fixed capabilities and behavioral patterns that are known to the defender. The overarching goal of this project is to develop the foundations for the design and analysis of robust MTD mechanisms that can provide a guaranteed level of protection in the face of unknown and adaptive attacks. The proposed research contributes to the emerging field of the science of security via a cross-disciplinary approach that combines techniques from cybersecurity, game theory, and machine learning. The investigator will disseminate the research findings to industry to help impact real systems. Elements from this research are to be incorporated into new courses on cybersecurity at Tulane University. The project engages underrepresented students and K-12 students and provides rich research experience to undergraduate students.<br/><br/>Developing robust MTD faces three major challenges induced by (1) the coupling of system dynamics and incentives; (2) the hidden behavior of stealthy attacks; (3) the necessity of coordinating multiple defenders in large systems. To tackle these challenges, the investigator will focus on three interrelated thrust areas. In the first thrust, a dynamic two-timescale MTD game that captures a variety of attack patterns and feedback structures is designed and techniques for handling games with large state spaces are investigated. In the second thrust, reinforcement learning-based MTD policies for thwarting unknown attacks are studied. The focus is on developing approximately optimal solutions with low complexity that can effectively exploit the delayed and noisy feedback during the game. In the third thrust, the MTD game and learning framework are extended to incorporate multiple attackers and defenders, and information sharing and mediation schemes for enabling coordinated MTD are investigated. The developed game models and defense strategies are validated via testbed implementations and trace-driven simulations. The research outcomes are expected to provide new insights and novel mechanisms that will significantly advance our understanding of how strategic thinking and learning can help achieve more adaptive cyber defense against advanced attacks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755910","CRII: III: Modeling Student Knowledge and Improving Performance when Learning from Multiple Types of Materials","IIS","Info Integration & Informatics","09/01/2018","06/17/2019","Shaghayegh Sahebi","NY","SUNY at Albany","Standard Grant","Maria Zemankova","08/31/2021","$190,029.00","","ssahebi@albany.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","7364","7364, 8228, 9251","$0.00","As the national interest in higher and professional education has been increasing, interest in online learning systems has also grown rapidly. Online learning systems, such as Massive Open Online Courses and Intelligent Tutoring Systems, aim to contribute to the society by providing high quality, affordable, and accessible education, at scale. They highly impact advancement of the national prosperity by preparing skillful professionals for high-demand jobs. Delivering such high-impact goals requires automatic tools that can help us understand students' learning process and answer questions such as what knowledge is gained by watching a video lecture (domain knowledge modeling), what is a student's state of knowledge (student knowledge modeling), and how a specific student would perform on a test (predicting student performance). Ideally, these tools should model student's learning from various learning material types (such as problems, readings, and video lectures) and capture the knowledge span offered by combinations of gradable and non-gradable learning resources. However, the current tools are limited to a single type of learning material (typically, ""problems""), ignoring the heterogeneity of learning materials from which students may learn. This project aims to achieve a better understanding of students' learning process in online educational systems by presenting an integrated research and education plan (1) to model student interactions with both gradable and non-gradable learning material types, (2) to integrate the proposed models with learning material content, and (3) to evaluate the proposed models by experimenting with real-world online educational datasets. The project will provide learning and research opportunities to graduate and undergraduate students.<br/><br/>To achieve the goal of improving students' learning process in online educational systems, the researchers develop multi-view machine learning algorithms that minimize the error of student performance prediction while maximizing the correlations among multiple views of the learning data. In the first year of this project, using activity sequences of students a model will be built that can capture a shared latent knowledge space among sets of gradable learning material and non-gradable ones. During the second year of this project, content information of learning materials, including expert labels, will be included in the learning model in order to improve it. This model aims to discover the relationship between content information and the shared latent knowledge space. The project results are evaluated using the task of predicting student performance. This project is at the intersection of domain adaptation, sequence modeling, and educational data mining. The model is inspired by Canonical Correlation Analysis as an approach for transferring information and adapting various views to student activity data, while modeling student learning process as a sequence of knowledge acquisitions. This is a novel treatment of the student modeling problem, with a sequential domain-adaptation view, that facilitates future research directions, such as personalized education and improved student retention in online learning environments. This work contributes novel sequential and content-aware domain adaptation and multi-variate analysis models that combine information from multiple sequential data sources and time-invariant content resources at the same time. While motivated by the task of student knowledge modeling, the models are general and can be applied to a broad spectrum of research including domain adaptation problems and recommender systems. The developed solutions will be presented in journals and conference venues, and the project website will provide access to the results, with references to code for the developed and evaluated models that will be available at GitHub.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763562","AF: RI: Medium: Collaborative Research: Understanding and Improving Optimization in Deep and Recurrent Networks","IIS","Robust Intelligence","08/01/2018","06/18/2018","Ruslan Salakhutdinov","PA","Carnegie-Mellon University","Standard Grant","Rebecca Hwa","07/31/2021","$329,074.00","","rsalakhu@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7924","$0.00","Machine learning using deep neural networks has recently demonstrated broad empirical success. Despite this success, the optimization procedures that fit deep neural networks to data are still poorly understood. Besides playing a crucial role in fitting deep neural networks to data, optimization also strongly affects the model's ability to generalize from training examples to unseen data. This project will establish a working theory for why and when large artificial neural networks train and generalize well, and use this theory to develop new optimization methods. The utility of the new methods will be demonstrated in applications involving language, speech, biological sequences and other sequence data. The project will involve training of graduate and undergraduate students, and the project leaders will offer tutorials aimed at both the machine learning community, and other researchers and engineers using machine learning tools. <br/><br/>In order to establish a theory of why and when non-convex optimization works well when training deep networks, both empirical top-down and analytic bottom-up approaches will be pursued. The top-down approach will involve phenomenological analysis of large scale deep models used in practice, both when presented with real data, and when presented with data specifically crafted to test the behavior of the network. The bottom-up approach will involve precise analytic investigation from increasingly more complex models, starting with linear models, and non-convex matrix factorization, progressing through linear neural networks, models with a small number of hidden layers, and eventually reaching deeper and more complex networks. The theory developed aims to be both explanatory and actionable, and will be used to derive new optimization methods and modifications to architectures that aid in optimization and generalization. A particularly important testbed is the case of recurrent neural networks. Recurrent neural networks are powerful sequence models that maintain state as they process an input sequence and are used for sequence data. Particularly challenging to optimize, recurrent neural networks still leave much room for a stronger principled understanding, which the project aims to provide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819222","Collaborative Research: Geometric Analysis and Computation for Generative Models","DMS","COMPUTATIONAL MATHEMATICS","07/01/2018","06/21/2018","Alexander Cloninger","CA","University of California-San Diego","Standard Grant","Leland Jameson","06/30/2021","$195,869.00","","acloninger@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","MPS","1271","062Z, 9263","$0.00","Research in unsupervised learning and generative models is concerned with uncovering structure and relationships in data with the intent of being able to generate new, as yet unseen, examples of the data set. Generative models learn the distribution of a data set from finite samples and provide an efficient sampler of the approximated density, rather than relying on labels for supervision. These models are a powerful tool for analyzing large volume, high-dimensional data in an unsupervised way. While generative models are an active research topic in machine learning, many theoretical and computational questions for such models remain unclear. This collaborative research project will study generative models from a geometric perspective, focusing on both performance guarantees and efficient implementations. The ability to efficiently create new data points that are guaranteed to be similar to the existing data has important implications in a variety of applications, including medical data analysis and privacy, bioinformatics, modeling of image and audio signals, and general high-dimensional data analysis in which it is difficult to collect labeled data for supervised algorithms.<br/><br/>The ideas and approaches in this research project center around the techniques that have evolved in the manifold learning field over the past decade. These mathematical tools, in particular local neighborhood preserving maps, approximation analysis in terms of intrinsic dimensionality, and construction of global coordinate systems based upon local affinity, have natural applications in the study of generative models. The project is comprised of four fundamental questions that arise in the field: (a) What are the types of distributions that generative networks are capable of learning efficiently, and how does the intrinsic dimensionality of the distribution affect convergence? (b) How can non-parametric generative models be created for dimension-reduced representations that arise in manifold learning, and which only depend on the intrinsic geometry of the data? (c) How can efficiently-computed metrics be defined between high-dimensional distributions for use in assessing the validity of various generative models? (d) How can these metrics be used to examine the various paths generative models take through the parameter space while being trained, and what clusters of starting points give optimal generators? The project will focus on both mathematical and computational aspects of these problems, aiming at resolving fundamental questions about these tools that are widely used in various data analysis and signal processing applications in science and industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832882","RII TRACK-4: Multiple Global Change Factors Control Forest Nitrogen Cycling - Remote Sensing and Machine Learning Identify Forest Function Across Developed Landscapes","OIA","EPSCoR Research Infrastructure","10/01/2018","08/20/2018","Tara Trammell","DE","University of Delaware","Standard Grant","Jeanne Small","09/30/2021","$203,346.00","","ttram@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","O/D","7217","9150","$0.00","Nontechnical Description<br/><br/>Nitrogen (N), an essential element required by all life, moves through our environment in a very tight cycle. Human activity has more than doubled the amount of nitrogen that cycles through our environment, and yet our understanding of the global nitrogen cycle is about 50 years behind our understanding of the global carbon cycle. While large intact forests have the capacity to store excess N in substantial quantities, the majority of temperate deciduous forests in the U.S. are small forest patches due to expanding urban and suburban development. Small forests are more susceptible to consequences from human activities, such as excess N inputs and non-native invasive plant spread. This research seeks to understand the N sink potential of small forests embedded across developed landscapes that receive excess N from human activities and that experience altered N availability from non-native plant invasion. The fellowship makes new collaborations possible between the early-career PI at the University of Delaware and a senior scientist at the University of Wisconsin-Madison to utilize novel remote sensing techniques that enable large-scale study of forest N dynamics in the face of multiple global change factors, such as excess N inputs and non-native plant invasion. The outcome of this work will provide novel understanding of the source/sink potential of an important air/water pollutant (nitrogen). <br/><br/>Technical Description<br/><br/>The global nitrogen (N) cycle has changed dramatically over the last century through increases in reactive N worldwide due to anthropogenic activities. Temperate deciduous forests are an important sink for reactive N deposition unless N inputs exceed N demand and forests become an N source. Multiple facets of global change, such as invasive species spread and altered nutrient cycling can interact and feedback to alter forest structure and function, ultimately determining the ability of forests to act as an N sink. This project seeks to ascertain how multiple, co-occurring global changes alter N cycling in forests by utilizing innovative remote sensing and machine learning techniques to integrate forest canopy chemistry and hyperspectral imaging to determine forest health and nutrient status across heterogeneous landscapes. The research will leverage remote sensing techniques to assess how urbanization and invasion impact forest canopy N content and resorption, which are indicators of forest N availability. Forest canopy N dynamics will be evaluated across an urbanization gradient to determine N available for tree uptake in high N-deposition environments. Additionally, forest canopy N will be compared in invaded and uninvaded forests to determine whether plant invaders outcompete native trees for soil N. We will extend beyond the results of this work to forests in different regions across the US using a macroecology approach. The research from this fellowship will enhance our capability to estimate the N source vs sink potential of temperate deciduous forests, and will improve our predictive ability on the fate of global reactive N in forests.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751009","CAREER: Data-Driven Network Resource Management Systems","CNS","Networking Technology and Syst","05/15/2018","07/13/2020","Mohammad Alizadeh","MA","Massachusetts Institute of Technology","Continuing Grant","Ann Von Lehmen","04/30/2023","$377,219.00","","alizadeh@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7363","1045","$0.00","Modern networks require sophisticated systems and algorithms to manage resources efficiently and deliver high quality of experience to users. These systems are critical to services society has come to rely on, from video streaming to social networks to AI applications. Video streaming, for example, involves numerous systems that control everything from the resolution of the video to the network path and the video download speed based on dynamic network conditions. As networks and applications have become more complex, existing approaches have become inadequate and designing algorithms that deliver high performance in all conditions has become exceedingly difficult. The goal of this research is to address this challenge by developing network systems that learn to manage resources automatically through experience by applying new machine learning techniques. This new paradigm, if successful, will make networks simpler to design, more efficient and cost effective, and able to deliver better services to businesses and consumers. <br/><br/>This project's goal is to develop the algorithmic and systems foundations for designing resource management systems that use modern reinforcement learning and other predictive control techniques to achieve strong performance across heterogeneous networks and applications. To this end, the researchers plan to build a series of practical systems for important applications, including schedulers for cluster computing systems (e.g., for data-parallel analytics workloads), and context-aware network control protocols (e.g., for adaptive streaming of 360 virtual reality video). In building these systems, the researchers will tackle fundamental challenges that confront data-driven network resource management, including (i) techniques to represent workloads (e.g., graph-structured jobs) and networks (e.g., topologies, queues, flows) to facilitate learning using neural networks; (ii) techniques to handle challenging resource management problems with large and deep action spaces; (iii) techniques to efficiently collect data across a myriad of devices for learning control models; and (iv) techniques to bootstrap learning models from data collected offline and continually train models safely after deployment<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840440","Planning Grant: Engineering Research Center for Rapid Innovations in SystEms Engineering and Agricultural Sustainability (RiseEnAg)","EEC","ERC-Eng Research Centers","09/01/2018","08/31/2018","Cranos Williams","NC","North Carolina State University","Standard Grant","Sandra Cruz-Pol","07/31/2021","$99,940.00","Kemafor Anyanwu-Ogan, Michael Kudenov, Omer Oralkan, Rosangela Sozzani","cmwilli5@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","ENG","1480","128E, 1480","$0.00","The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program. Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>The projected global increase to 9 billion people by the year 2050 will impose significant challenges on our ability to produce enough food for the world's population. Specific challenges that hinder our ability to meet the world's food security needs include: 1) our inability to sense and quantify, in a strategic way, physical and chemical variables in the environment and in-planta across scales; 2) the lack of scalable data analytics and multi-scale models that transform concurrent datasets across multiple scales into decision making strategies for improving crop yield and minimizing crop loss; and 3) the lack of intelligent data cyber-infrastructures that efficiently integrate heterogeneous data, supporting the development of complex multi-scale biological models. This planning grant will be used to support the development of an Engineering Research Center Proposal on Rapid Innovations in SystEms Engineering and Agricultural Sustainability (RiseEnAg).  The proposed Engineering Research Center will use convergent research to accelerate engineering innovation in plant sensor development, data analytics, and data integration via three synergistic research thrusts: 1) Sensor Development, Calibration, and Integration; 2) Data Mining, Machine Learning, and Multi-scale Modeling; and 3) Data Management and Integrative Cyberinfrastructures.  Phenotyping testbeds will be engineered to induce complex growth conditions and monitor biological and chemical responses. These ""tools of discovery"" will address the challenges of today's growers by translating real-time knowledge of crop and soil health and an increased understanding of relationships across the biological scale, into actionable strategies that advance crop field performance, increase post-harvesting longevity, and minimize post- harvest biomass loss.  The RiseEnAg Engineering Research Center will aim to target a complex grand challenge (food security) and defines the convergent space of research that merges the ideas, approaches, and technologies from engineering and the biological sciences needed to address it <br/><br/>The goal of the RiseEnAg convergent engineering research center is to establish engineered systems platforms that integrate advanced sensors, data analytics, and phenotyping testbeds that address stakeholder (e.g. growers, researchers, and industry) challenges and accelerate research towards the goal of increased food security in the 21st century.  RiseEnAg will develop the engineering tools needed to achieve this goal via three integrated research thrusts.  Research Thrust 1 (Sensor Development, Calibration, and Integration) will develop and employ sensors for monitoring physical, chemical, biochemical, and molecular parameters, both in-planta and in the environment, to include rhizosphere microbiota-chemical interactomes that are key to plant performance.  Research Thrust 2 (Data Mining, Machine Learning, and Multiscale Modeling) will develop scalable modeling and machine learning algorithms for formulating integrative plant analytics and building decision support systems for growers.  Research Thrust 3 (Data Management and Integrative Cyberinfrastructures) will develop a robust, adaptable, and scalable framework for storing, accessing, and analyzing heterogeneous biological datasets.  High-throughput phenotyping plant growth testbeds will be engineered to induce complex growth conditions and monitor biological and chemical responses in-planta and in the soil.  These testbeds will accelerate the identification of novel biological and environmental targets that, when manipulated individually or in combination, can produce the desired performance outcomes under diverse growth conditions. Funds allocated via this planning grant will provide the resources needed to 1) explore and identify unique and common challenges across all stakeholders that, if addressed, accelerate us toward the common goal of agricultural sustainability, 2) identify novel areas of engineering that can contribute to addressing these problems and target potential roadblocks where innovation is needed, 3) outline specific goals for the remaining three fundamental components of the CERC (Engineering Workforce Development, Innovation Ecosystem, and Culture of Inclusion), and 4) formulate working draft of the final CERC proposal.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756031","CRII: CHS: Enabling Safe and Adaptive Robot-aided Gait Training through Biomechanical Characterization and Learning from Demonstration","IIS","HCC-Human-Centered Computing","09/01/2018","05/12/2020","Wenlong Zhang","AZ","Arizona State University","Standard Grant","Ephraim Glinert","08/31/2021","$191,000.00","","Wenlong.Zhang@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7367","7367, 8228, 9251","$0.00","The unprecedented growth in the elderly population is generating a high demand for gait rehabilitation due to age-related neurological diseases.  To address this urgent need various assistive robots have been developed to improve gait training outcomes, and impedance control (controlling the force of resistance to external motions that are produced by the environment) has been widely employed in these robots to ensure safe human-robot interaction.  However, it is difficult to personalize the virtual impedance for such robots due to the complex nature of human neurological and musculoskeletal dynamics.  On the other hand, a physical therapist can provide adaptive assistance to a patient at the correct moment in a gait cycle based on real-time sensory feedback and clinical experience.  Inspired by this observation, one could imagine designing an assistive robot control system by learning from therapists' demonstrations, but such a purely data-driven approach could lead to significantly degraded performance with new gait patterns, which creates safety risks for users. This research will develop a hybrid assistive robot control approach, which integrates model-based impedance control with machine learning from therapists' behaviors so that the resultant robot assistance is safe yet adaptive.  Project outcomes will include a novel algorithm framework for physical human-robot collaboration that exhibits both performance guarantees due to the model-based control and intelligent adaptation resulting from robot learning.  The new technology will have a wide range of applications in many other safety-critical human-robot collaboration scenarios, including collaborative manufacturing, (semi) autonomous driving, and service robots.  The broader impacts of the work will be further enhanced by tight integration of the research with educational activities including new modules in existing robotics classes, research opportunities for undergraduate students from underrepresented groups, and internships for local high-school students.<br/>  <br/>The scientific contribution of the work will include: 1) integration of heterogeneous wearable sensor data to build the robot learning model from therapists' demonstrations, and human knee impedance characterization to build the robot impedance control model; 2) a robot planning approach based on a fusion of learning from demonstration and impedance control, with the weights determined by the degree of confidence in the robot learning model; and 3) automatic requests for new demonstration data and incorporation of subject feedback to refine both the robot learning and impedance control models.  Performance of the approach will be assessed in biomechanical simulations, in lab tests with healthy subjects, and in a pilot study with stroke and Parkinson's disease patients.  It is envisioned that project outcomes will make assistive robots highly intelligent so that a therapist could work with multiple patients simultaneously and even remotely, which could significantly reduce both the therapists' labor intensity and cost of rehabilitation training for patients.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762211","Collaborative Research: Enhancing Gait Dynamics via Physical Human-human and Human-Robot Interactions at the Hands","CMMI","M3X - Mind, Machine, and Motor","06/01/2018","05/30/2018","Lena Ting","GA","Emory University","Standard Grant","Robert Scheidt","05/31/2021","$372,946.00","Madeleine Hackney","lting@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","ENG","058Y","063Z, 070E, 7632, 9102","$0.00","This project contributes to the understanding of how intelligent robotic systems can use a model of human intent, perception, and behavior to enhance human-robot cooperation, as may occur during rehabilitative gait training in physical therapy.  The project seeks to understand how small forces applied to the hand can be used to alter clinically-relevant human gait patterns.  The project is significant because it generates the experimental data and models of human gait and sensorimotor cognition needed to develop a robotic device that will interact physically and intuitively with individuals with mobility impairments to enhance their quality of life.  By developing a robotic test-bed system and the models of human sensorimotor behavior that will guide its actions, this project will serve the national interest and advance the NSF mission to promote the progress of science and to advance the national health.  <br/><br/>The project takes a three-stage approach to advancing the objectives of the NSF's Mind, Machine and Motor Nexus (M3X) program, which are to understand how the human mind controls body movements during the manipulation of machines, and how machine response can shape and influence both the mindset and movements of the human user. First, the project team will use novel instrumented devices and motion capture technology to measure interactive human-to-human hand forces and the resulting gait motions that occur during the proposed therapeutic intervention. In Stage 2 they will use machine learning techniques to develop a model of the motor and cognitive transformations that occurred during the human-to-human experiments of Stage 1. Finally, the team will embed the models within a novel robotic test-bed that will implement physical human-robot interactions at the hands. Human-robot experiments will evaluate the ability of the test-bed to promote desired changes in clinically-relevant gait characteristics such as gait speed, step length and step cadence.  Fundamental issues addressed by the project include: 1) identifying the relationships between hand forces, gait dynamics and perceived intents of the instructor (therapist) and student (patient) that arise during a simplified version of rehabilitative partner dance; 2) modeling those relationships so as to enable prediction of how small hand forces indirectly change human gait dynamics; and 3) developing a robotic gait coach capable of promoting desired changes in human gait dynamics.  In addition, the project supports education and promotes diversity through innovative outreach activities that will engage teams of students and older adults from underrepresented communities in the conception, design and prototyping of mobility-related assistive technologies. The project outcomes may have long-term impact on the quality of life of millions of Americans with deficits of gait by developing a robotic system that can assist therapists in their efforts to improve patient fitness, mobility, and independence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814888","SHF: Small: Communication-Efficient Distributed Algorithms for Machine Learning","CCF","Software & Hardware Foundation","07/15/2018","07/17/2018","MERT GURBUZBALABAN","NJ","Rutgers University New Brunswick","Standard Grant","Almadena Chtchelkanova","06/30/2021","$464,412.00","Maryam Mehri Dehnavi, MERT GURBUZBALABAN","mg1366@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798","7923, 7942, 9102","$0.00","Advances in sensing and processing technologies, communication capabilities and smart devices have enabled deployment of systems where a massive amount of data is collected and then processed in order to make decisions. The platforms that process this vast amount of data differ depending on the application. Among these, data centers are powerful platforms with vast computational resources where the collected data can be distributed over multiple processors that are all connected through a high-bandwidth network. Data can also be generated and processed in multi-agent systems which are made up of multiple interacting computational units (such as smart devices connected through wireless internet) with limited resources in terms of storage, power, computation, and communication capabilities. Data communication costs, which include the bandwidth and latency, often dominate floating point operation costs thus the performance of optimization algorithms when operating on large data sets is bounded by data communication for both multi-agent systems and data centers. This project proposes novel communication-efficient methods for a class of distributed optimization problems arising in large-scale data analysis and machine learning. The methods and techniques developed under the scope of this project contribute to the efficiency, practical performance and to the mathematical foundations of distributed optimization algorithms. The project is also developing a high-performance software framework that allows the dissemination of efficient domain-specific software and benchmarks.<br/><br/>The project has three goals: the first goal is to improve the communication efficiency of existing algorithms for solving distributed optimization problems in the context of multi-agent systems, through a distributed algorithm for improving the total number of communications required in consensus iterations. The approach is based on leveraging the notion of the effective resistance of a link to identify bottleneck edges for communication purposes, and modifying the classical consensus averaging by taking effective resistances into account. The second goal is to develop communication-avoiding algorithms for data centers, through a framework that allows for reduction in communication by a tunable amount while keeping the arithmetic costs and bandwidth costs the same for a number of applications and existing algorithms. The third goal is to improve communication for hybrid systems which interpolate between multi-agents systems and data centers in terms of communication structure, using a framework that generates algorithm- and architecture-aware codes for reducing communication over these hybrid platforms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821243","Collaborative Research: New Statistical Learning for Complex Heterogeneous Data","DMS","CDS&E-MSS","09/01/2018","06/26/2018","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Christopher Stark","08/31/2021","$110,000.00","","jizhu@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","8069","8083, 9263","$0.00","This project focuses on several important and challenging issues concerning complex heterogeneous data that arise from medical imaging and social networks. The major goals are to develop powerful and innovative statistical machine learning methods and tools that are able to flexibly model signal heterogeneity across images, integrate imaging data with multimodal and spatially distributed data, and tackle heterogeneity of network data. The integrated program of research and education will have significant impacts in many different fields such as biomedical studies, genomic research, environmental studies, public health research, and social and political sciences, among others. The project will also stimulate interdisciplinary research and collaboration with scientists from disparate fields.<br/><br/>This project will lead to substantial advancement in heterogeneity learning and modeling through exploiting individual variation from the general population, and integration of multiple sources of imaging information to enhance prediction accuracy for disease diagnoses and treatment outcomes. In addition, this project develops innovative unsupervised learning methods through utilizing node covariate information for analyzing heterogeneous network data. Each component of the research plan contains a broad range of topics, from methodological and computational development to applications in real world problems. Specifically, the PIs study subject-variant scalar-on-image regression models to incorporate the heterogeneity variation for brain imaging data, multi-dimensional tensor learning methods for breast cancer imaging data, flexible Gaussian graphical models for network data, and a novel clustering framework for heterogeneous data that are linked by networks. Furthermore, the development of advanced optimization techniques, algorithms and computational technologies will be applicable to many practical problems arising from large-scale heterogeneous data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848613","Convergence: RAISE: Auto-regulatory Scaffolds for Directed Evolution of  Non-living Functional Materials","CMMI","GCR-Growing Convergence Resear, SSA-Special Studies & Analysis, DMR SHORT TERM SUPPORT","09/15/2018","09/19/2018","Teri Odom","IL","Northwestern University","Standard Grant","Y. Kevin Chou","08/31/2021","$999,999.00","George Schatz, Bozhi Tian","todom@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","ENG","062Y, 1385, 1712","049Z, 9102","$0.00","This award supports a new approach to identify and realize new functional materials under realistic conditions. The method is directed evolution of non-living materials based on auto-regulatory scaffold hosts. The method is bio-inspired. It mimics the approach observed in living things. The research involves developing a platform which incorporates synthesis, screening and feedback, by design, and offers a practical pathway to materials discovery. This strategy overcomes the limitations of screening materials via computer simulations only. The project builds on the convergence of different research areas such as tissue engineering, systems biology, scalable nanomanufacturing and machine learning. The discovery of new materials leads to new functionalities, which leads to new devices and systems, which leads to new products, which benefits society and economy and enhances the nation's prosperity and security. The project demonstrates a paradigm shift in materials discovery and invention. Education plans involve training graduate students and postdocs in convergence approaches to materials screening and discovery, design and realization of auto-regulatory scaffolds and machine learning. Outreach plans are to develop programs such as dissemination through public lectures, integration of research results into new undergraduate courses, and publication of perspectives that combine convergence of research from different fields.<br/><br/>The project's approach is to screen materials through the auto-regulatory interaction of sensors, regulators and known and unknown materials.  These components are located on scaffolds, which are tissue engineering-inspired constructs, whose dimensions are convenient for the developed fabrication and synthesis tools and which may be adapted to a wide range of node materials that include hydrogels, polymers, nanomaterials, and biomaterials. The auto-regulatory aspects of the research involve humidity and photothermal energy sources, among others. The auto-regulatory processes are based on interference effects, thermal expansion, chemical reactions, and cellular response and motion for which the project develops theory and modeling. One benefit of the evolutionary platform is that it interfaces seamlessly with seemingly incompatible combinations of materials such as hydrogels and semiconducting nanomaterials or biological cells and inorganic materials. Materials screening is coupled with machine learning methods for prediction of new materials, which can be extended to other user-defined outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829771","CyberTraining:CIC: DeapSECURE: A Data-Enabled Advanced Training Program for Cyber Security Research and Education","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Hongyi Wu","VA","Old Dominion University Research Foundation","Standard Grant","Alan Sussman","08/31/2021","$500,000.00","Wirawan Purwanto, Masha Sosonkina","h1wu@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","As the volume and sophistication of cyber-attacks grow, cybersecurity researchers, engineers and practitioners heavily rely on advanced cyberinfrastructure (CI) techniques such as big data, machine learning, and parallel programming, as well as advanced CI platforms, e.g., cloud and high-performance computing to assess cyber risks, identify and mitigate threats, and achieve defense in depth. However, advanced CI techniques have not been widely introduced in undergraduate and graduate cybersecurity curricula. This lack creates a hurdle for many senior undergraduates and early-stage graduate cybersecurity students who are keen to conduct cutting-edge cybersecurity research and/or participate in advanced industrial cybersecurity projects. This project introduces a unique Data-Enabled Advanced Training Program for Cyber Security Research and Education (DeapSECURE), aimed to prepare undergraduate and graduate students with advanced CI techniques and teach them to use CI resources, tools, and services to succeed in cutting-edge cybersecurity research and industrial cybersecurity projects. The project responds to the urgent need for well-prepared cybersecurity workforce in the Hampton Roads metropolitan region, the Commonwealth of Virginia, and the Nation. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/>This project develops six new CI training modules which emphasize the practical use of the advanced CI techniques, especially the tools that implement them, in the context of cybersecurity research. Each training module includes three sections: (1) an overview presented by an invited cybersecurity faculty about his/her research, concluding with a research problem that heavily depends on CI techniques; (2) an introduction of corresponding CI skills, tools and platforms; (3) a hands-on lab session where students will apply the CI techniques to solve the research problem formerly introduced by the cybersecurity faculty. The modules will be delivered via two distinct means: monthly workshops and summer institutes. Six monthly workshops are conducted during academic year, primarily targeting students enrolled at Old Dominion University (ODU). The summer institutes present these six modules to students from local community colleges, Research Experiences for Undergraduates program at ODU, and other Virginia universities; they also include special activities such as field trips, open house for K-12 students, Cyber Night events, cybersecurity career panels, and student competitions. Complementing the workshops and summer institutes, an online continuous learning community is created, which includes a virtual computer lab and a student forum, as a place for students to continue their learning engagement after the face-to-face sessions. Archived workshop materials, as well as additional learning materials are also posted on this online platform as open educational resources, to be made available to the cybersecurity research and education communities. The open-source style development of the learning modules facilitates a wide-range of adoption, adaptations, and contributions in an efficient manner. The project leverages existing and new partnerships to ensure broad participation, and accordingly broaden the adoption of advanced CI techniques in the cybersecurity community. The project employs a rigorous assessment and evaluation plan rooted in diverse metrics of success to improve the curricula and demonstrate its effectiveness. The metrics, which are based on the students' outcomes and exit surveys, are assessed by an independent evaluator. The adoption of the learning modules outside of the training program is also considered as a metric of success.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838796","SCH: INT: Collaborative Research: Novel Computational Methods for Continuous Objective Multimodal Pain Assessment Sensing System (COMPASS)","IIS","Smart and Connected Health","09/15/2018","09/06/2018","Yingzi (Lynn) Lin","MA","Northeastern University","Standard Grant","Wendy Nilsen","08/31/2022","$613,946.00","Sagar Kamarthi","yi.lin@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8018","8018, 8062","$0.00","Few objective pain assessment techniques are currently available for use in clinical settings. Clinicians typically use subjective pain scales for pain assessment and management, which has resulted in suboptimal treatment plans, delayed responses to patient needs, over-prescription of opioids, and drug-seeking behavior among patients. This project will investigate science-based methods to build a robust Continuous Objective Multimodal Pain Assessment Sensing System (COMPASS) and a clinical interface capable of generating objective measurements of pain from multimodal physiological signals and facial expressions. COMPASS will allow objective measurements that can be used to significantly improve pain assessment, pain management strategies, reduce opioid dependency, and advance the field of pain-related research. The educational plan will include activities to engage patient training, K-12 students, minorities and underrepresented groups, as well as general public. These outcomes will also lead to development of a diverse work force needed to support advanced medical technologies and services.<br/><br/>Using advanced biosensing systems, data fusion algorithms and machine learning models, this project will develop a robust, reliable, and accurate pain intensity classification system, COMPASS, for estimating pain intensity experienced by patients in real-time on a 0-10 scale, which is the standard scale used by physicians in clinical settings. In the initial phase of the project, the team will conduct a pilot at Brigham and Women's Hospital to experiment with the different elements for developing the sensing systems and collect data to develop data fusion algorithms and machine learning models. In the later phase of the project, the team will collect an extensive set of data to train and validate the fully implemented COMPASS. Physiological sensor data from electroencephalograph, facial-expression, patient self-reported pain scales, and physician/nurse assessed pain scales will be collected from the subjects as they experience pain modulated by medical therapies that cause patients pain. The project will investigate evidence-based machine learning and feature extraction methods for physiological signals and facial-expression images. This highly interdisciplinary research will make significant contributions to the areas of pain assessment and management, human factors and patient safety.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838621","SCH: INT: Collaborative Research: Novel Computational Methods for Continuous Objective Multimodal Pain Assessment Sensing System (COMPASS)","IIS","Smart and Connected Health","09/15/2018","09/06/2018","Yan Xiao","TX","University of Texas at Arlington","Standard Grant","Wendy Nilsen","08/31/2022","$171,560.00","","yan.xiao@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","8018","8018, 8062","$0.00","Few objective pain assessment techniques are currently available for use in clinical settings. Clinicians typically use subjective pain scales for pain assessment and management, which has resulted in suboptimal treatment plans, delayed responses to patient needs, over-prescription of opioids, and drug-seeking behavior among patients. This project will investigate science-based methods to build a robust Continuous Objective Multimodal Pain Assessment Sensing System (COMPASS) and a clinical interface capable of generating objective measurements of pain from multimodal physiological signals and facial expressions. COMPASS will allow objective measurements that can be used to significantly improve pain assessment, pain management strategies, reduce opioid dependency, and advance the field of pain-related research. The educational plan will include activities to engage patient training, K-12 students, minorities and underrepresented groups, as well as general public. These outcomes will also lead to development of a diverse work force needed to support advanced medical technologies and services.<br/><br/>Using advanced biosensing systems, data fusion algorithms and machine learning models, this project will develop a robust, reliable, and accurate pain intensity classification system, COMPASS, for estimating pain intensity experienced by patients in real-time on a 0-10 scale, which is the standard scale used by physicians in clinical settings. In the initial phase of the project, the team will conduct a pilot at Brigham and Women's Hospital to experiment with the different elements for developing the sensing systems and collect data to develop data fusion algorithms and machine learning models. In the later phase of the project, the team will collect an extensive set of data to train and validate the fully implemented COMPASS. Physiological sensor data from electroencephalograph, facial-expression, patient self-reported pain scales, and physician/nurse assessed pain scales will be collected from the subjects as they experience pain modulated by medical therapies that cause patients pain. The project will investigate evidence-based machine learning and feature extraction methods for physiological signals and facial-expression images. This highly interdisciplinary research will make significant contributions to the areas of pain assessment and management, human factors and patient safety.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833005","RII Track-4: A Reflective Learning and Association Control Framework based on Adaptive Dynamic Programming: Architecture and Applications in Robotics","OIA","EPSCoR Research Infrastructure","10/01/2018","01/31/2020","Zhen Ni","SD","South Dakota State University","Standard Grant","Jose Colom","09/30/2019","$50,899.00","","zhenni@fau.edu","1015 Campanile Ave","Brookings","SD","570070001","6056886696","O/D","7217","9150","$0.00","Nontechnical description: <br/>Data efficiency and learning speed are two of the major bottlenecks for applying biologically-inspired control methods in many domains. The project's goal is to address these fundamental challenges by introducing a new adaptive dynamic programming-based learning control framework and integrate it into space robot navigation and scouting applications such as the Mars Rover. The scientific contribution of this project will promote interdisciplinary research in computational intelligence, machine learning, control and robotics. In addition to space applications, the proposed structure can also be applied to robot-assisted pedestrian evacuation application and cyber-physical power systems and is expected to impact general systems beyond this project period. Due to geographic isolation, South Dakota doesn't have a National Aeronautics and Space Administration (NASA) research center, and research collaboration opportunities on space technology is very limited. This project will expand the principle investigator (PI)'s research capacity through an extended visit and collaboration with NASA Ames Research Center located in San Jose, CA, and transform the PI's career path from theoretical algorithm/architecture development towards a new direction in complex space applications. Meanwhile, the outcomes of this project align well with the South Dakota's and South Dakota State University's strategic plans. The collaboration fits well with NASA's mission to Mars and technology roadmaps.<br/><br/><br/>Technical description: <br/>The proposed project will fundamentally advance the learning and association of biologically-inspired control methods. Three major contributions to the scientific field are expected. First, a new experience network is proposed and systematically integrated into a model-free adaptive dynamic programming-based learning control framework. The PI will design an experience replay tuple (i.e., state-action-reward pair) based on backward temporal difference information from historical data. This design can avoid the model network/prediction noted in existing literature and significantly save computation resources. Second, instead of a uniform sampling method, the PI proposes a prioritized sampling method based on the Bellman's estimation error. This new method is expected to enhance the controller's reflective learning performance with useful long-short term memory. The stability and convergence properties will also be analyzed. Third, this project is closely tied with NASA on robot and optimal control for space program. This new learning control structure will be integrated for robot navigation, exploration and scouting in unknown spaces. The PI and the collaborator will use both a virtual reality platform and a real Rover facility to analyze the control performance of the proposed algorithm at NASA Ames. The PI's outreach and dissemination plans will cultivate the scientific curiosity of K-12 students and motivate their interest in STEM programs. Moreover, the integration of the project's cutting-edge research results into the PI's new courses will aid retention of current STEM students. Specific plans include a workshop for a local middle school, a distance course for demographically diverse institutions, and development of new courses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839730","Collaborative Research: FW-HTF: Integrating Cognitive Science and Intelligent Systems to Enhance Geoscience Practice","DUE","FW-HTF-Adv Cogn & Phys Capblty","10/01/2018","08/22/2018","Basil Tikoff","WI","University of Wisconsin-Madison","Standard Grant","Stephanie August","09/30/2021","$499,992.00","","basil@geology.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","EHR","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>This project will make a significant contribution toward the support of future workers in geology. Understanding how geologists reason, plan to collect new data, consider three-dimensional spatial relations, and evaluate uncertainty are critically important for supporting scientists working on applied problems, such as natural resource exploration. This project will enhance existing efforts in geology to collect data using robot drones.  Drones allow access to important areas of the world too dangerous to access in person and not visible from satellite or plane.  The project will use machine learning to incorporate expert knowledge into drone flights to support effective autonomous data collection.  The data will yield improved geological understanding of an important fault system. Findings from the project will improve understanding of uncertainty in volumes and thus improve our understanding of earthquakes and the analyses of petroleum workers.  Understanding how expert geologists reason will support new exploration and mapping strategies for human-robot teams working in natural environments. The collaborative efforts of the interdisciplinary team will advance the fields of cognitive science, geology, and machine learning. The integration of cognitive science, robotics, and geology will develop new approaches to field work with human-autonomous systems teams that are faster and more effective than any either human or autonomous system would be acting alone. <br/><br/>The project will characterize expert spatial reasoning about 3D relations and uncertainty as geologists collect data to develop a 3D understanding of a new field area, make predictions about future observations, and construct geological models.  Errors in reasoning about 3D structures will be used to develop quantitative models of expert uncertainty. These models will be used to help explicitly visualize uncertainty for the experts and to construct cost functions for the robot navigation. The cost functions will include metrics that capture scientific value. The project will develop new approaches to drone exploration and mapping, including machine learning of features of interest to geologists. Drones will autonomously explore and map natural rock formations in canyon environments to support and speed up the data collection and interpretation efforts of field geologists.  The project will study the structural geology of the Mecca Hills area of California, a well exposed portion of the San Andreas fault system.  Robot drones will collect data about surface features to develop maps of subsurface structures. The cognitive science-infused robot design will employ successful expert strategies and focus on areas where experts are likely to make errors to prioritize exploration of those areas in navigation plans. The proposed strategies will enable 3D surface reconstruction of canyon surfaces. They will also enable better understanding of how to enhance planning and on-the-fly decision making of experts for collecting scientifically important data.   The project's foundational work aims to develop an interdisciplinary understanding of how geologists build a scientific understanding of a region over time. It also aims to design autonomous exploration strategies for human-robot teams, and test new ways to support the sequential decisions about where to collect data to maximize scientific impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835378","Seeking Synergy: K-12 Professional Development as a Model for College Science Faculty","DUE","IUSE","10/01/2018","07/30/2018","Paula Lemons","GA","University of Georgia Research Foundation Inc","Standard Grant","Andrea Nixon","09/30/2020","$49,005.00","Jennifer Knight, Kevin Haudek, Christopher Wilson, Luanna Prevost","plemons@uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","EHR","1998","7556, 8209, 9178","$0.00","It is a national priority to produce a well-prepared workforce in science, technology, engineering, and mathematics (STEM).  This workforce is needed to make progress in science, to advance national health initiatives, and support overall national prosperity in an increasingly technical economy. One contemporary challenge colleges and universities face in meeting this challenge is the need to provide effective professional development opportunities for faculty members.  For example, professional development could help faculty learn about emerging research related to student learning, and this knowledge could inform teaching practices.  This project aims to serve the national interest by convening professional development leaders from both K-12 and higher education to determine the design principles of a new model of professional development for college science teaching.  This project is designed to draw upon evidence-based, effective professional development practices from the K-12 sector, and to apply these strategies to programs for faculty members teaching undergraduate biology students.  Multiple K-12 models for professional development have been rigorously tested and shown to improve teacher knowledge, skill, and student learning.  This gathering is designed to focus specifically on teaching and learning methods designed to reveal and respond to student thinking.<br/><br/>This project will support a three-day conference to define the design principles of a new professional development program for college biology faculty. The model will derive components from K-12 professional development models and focus on helping faculty learn the pedagogy of revealing and responding to student thinking. The conference will extend the work of two well-established research and professional development groups in STEM education, the Automated Analysis of Constructed Response project (AACR) and the Science Teachers Learning from Lesson Analysis project (STeLLA). AACR focuses on undergraduate science education and addresses: the creation of constructed-response assessments that reveal students' mental models of scientific concepts; the creation of machine learning models to predict experts' scoring of student responses; the production of reports for faculty that describe the computerized analysis of student writing; and long-term, local faculty learning communities that support faculty users of AACR. STeLLA supports K-12 teachers to view their teaching through the lens of student thinking. The efficacy of STeLLA's approach to professional development has been demonstrated through randomized and quasi-experimental studies that link teacher learning with increased student learning. The conference will include members of AACR and STeLLA as well as experts from other higher education and K-12 professional development arenas. The conference will produce a framework for developing a new college biology professional development program, which can be implemented and researched. It also will create a network of science education professional development specialists and researchers from both K-12 and higher education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747338","SBIR Phase I:  Teachpad - A Teach-to-Learn Paradigm for Difficult Math Concepts","IIP","SBIR Phase I","01/01/2018","02/11/2019","Srividya Raman","CA","RoundEd Learning","Standard Grant","Rajesh Mehta","03/31/2019","$224,170.00","","vidyar1@gmail.com","218 Del Valle Court","Pleasanton","CA","945669456","4088871334","ENG","5371","5371, 8031","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to use current research in the learning sciences to develop games that make middle school math engaging and relevant.  Middle school is a crucial time in a student's educational journey as it is the period at which their self-concept of ability becomes stable. Studies show that it is also the period at which students start getting disengaged with math and there is a marked increase in negativity towards math from sixth to eighth grade. According to the National Report Card, only about a third of 8th graders in US public schools, of over 3.6 million students, score at the proficient or above level in Math every year. The goal of this proposal is to implement and test the effectiveness of a scaffolded learning and teaching environment, called the 'Teachpad', that helps middle school students learn difficult math concepts. Multiple studies have indicated that students put more effort into learning when they have to teach someone else. This project will extend these studies and evaluate whether the teachpad increases student engagement and improves learning and long term retention of difficult math concepts.<br/><br/>The proposed project uses a ""teach to learn"" framework in which students teach characters in a computer game how to solve specific problems and use the feedback that they receive from the characters to evaluate how well they have understood the concepts themselves. The teachpad uses natural language input, meaning that the students can enter their instruction in plain English and can explain the solution to the problem as they would to a friend or sibling. A semantic parser extracts the useful information from this input and feeds it to a machine learning algorithm to teach a game character how to solve the problem. A model of what the character has learned is also visible to the student who can see how their instruction is working when the character is trying to solve a test problem. This project will implement the teachpad for the order of operations, and will determine how it compares to traditional classroom practices. Data will be  collected in authentic educational environments, through observations and interviews to study student engagement and usability, and pretests and posttests (once after the treatment and once when students return from summer break) to evaluate learning outcomes and long term retention."
"1839705","Collaborative Research: FW-HTF: Integrating Cognitive Science and Intelligent Systems to Enhance Geoscience Practice","DUE","FW-HTF-Adv Cogn & Phys Capblty, IUSE","10/01/2018","08/22/2018","Thomas Shipley","PA","Temple University","Standard Grant","Stephanie August","09/30/2021","$499,852.00","Alexandra Davatzes","TSHIPLEY@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","EHR","082Y, 1998","063Z, 9178","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>This project will make a significant contribution toward the support of future workers in geology. Understanding how geologists reason, plan to collect new data, consider three-dimensional spatial relations, and evaluate uncertainty are critically important for supporting scientists working on applied problems, such as natural resource exploration. This project will enhance existing efforts in geology to collect data using robot drones.  Drones allow access to important areas of the world too dangerous to access in person and not visible from satellite or plane.  The project will use machine learning to incorporate expert knowledge into drone flights to support effective autonomous data collection.  The data will yield improved geological understanding of an important fault system. Findings from the project will improve understanding of uncertainty in volumes and thus improve our understanding of earthquakes and the analyses of petroleum workers.  Understanding how expert geologists reason will support new exploration and mapping strategies for human-robot teams working in natural environments. The collaborative efforts of the interdisciplinary team will advance the fields of cognitive science, geology, and machine learning. The integration of cognitive science, robotics, and geology will develop new approaches to field work with human-autonomous systems teams that are faster and more effective than any either human or autonomous system would be acting alone. <br/><br/>The project will characterize expert spatial reasoning about 3D relations and uncertainty as geologists collect data to develop a 3D understanding of a new field area, make predictions about future observations, and construct geological models.  Errors in reasoning about 3D structures will be used to develop quantitative models of expert uncertainty. These models will be used to help explicitly visualize uncertainty for the experts and to construct cost functions for the robot navigation. The cost functions will include metrics that capture scientific value. The project will develop new approaches to drone exploration and mapping, including machine learning of features of interest to geologists. Drones will autonomously explore and map natural rock formations in canyon environments to support and speed up the data collection and interpretation efforts of field geologists.  The project will study the structural geology of the Mecca Hills area of California, a well exposed portion of the San Andreas fault system.  Robot drones will collect data about surface features to develop maps of subsurface structures. The cognitive science-infused robot design will employ successful expert strategies and focus on areas where experts are likely to make errors to prioritize exploration of those areas in navigation plans. The proposed strategies will enable 3D surface reconstruction of canyon surfaces. They will also enable better understanding of how to enhance planning and on-the-fly decision making of experts for collecting scientifically important data.   The project's foundational work aims to develop an interdisciplinary understanding of how geologists build a scientific understanding of a region over time. It also aims to design autonomous exploration strategies for human-robot teams, and test new ways to support the sequential decisions about where to collect data to maximize scientific impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823066","Collaborative Research: Human-Technology Partnership Supporting Career Path Exploration and Navigation","IIS","Cyberlearn & Future Learn Tech","09/01/2018","07/25/2018","Jason Levin","UT","Western Governors University","Standard Grant","Amy Baylor","08/31/2021","$147,398.00","","jason.levin@wgu.edu","4001 South  700 East","Salt Lake City","UT","841070000","8454891748","CSE","8020","063Z","$0.00","The employment landscape is changing and expanding, creating uncertainty as individuals navigate decisions regarding career preparation and continuing education.  This project develops technology to provide guidance for college students to access high quality information and guidance within a large and complex decision space. The approach includes integrating social support from human career coaches or peers to increase confidence and motivation in the problem solving and decision making process. The approach is fundamentally data driven, built upon computational modeling while informed by theories of human behavior, embodied within a novel machine learning paradigm termed Socially-Sensitive Reinforcement Learning (SSRL). Based on this new paradigm, the system will generate guidance to support students while being sensitive to student needs and preferences so that there is a high probability that students will accept and benefit from the guidance. The project targets nontraditional students who embark upon or advance STEM career paths and aims to broaden participation in STEM.<br/><br/>The project contributes to both human-computer interaction and machine learning and takes an interdisciplinary approach in its modeling of human interaction and sociotechnical support for learning. It comprises a three pronged solution: (1) a computational modeling strand proposes new computational paradigms that continually map best practices and inform data-driven recommendations in support of effective decision making; (2) a behavioral research strand provides insight through qualitative and quantitative investigations, uncovering properties of recommendations as well as recommenders that are associated with whether guidance is accepted and how it influences success in career advancement; and (3) an intelligent coach development strand embodies findings from the other two strands in the CareerScope Intelligent Coach Agent (ICA), designed as a sociotechnical solution leading to impact student decision making. The partnership with Western Governor's University (WGU) facilitates transition of research into practice at large scale through research with WGU students and deployment on the WGU platform. While the research will be housed within a single platform, the technical innovations will ultimately be integrated into a wide range of platforms. The results of this project will broaden understanding of the limitations and opportunities for ICAs to guide learners, particularly non-traditional learners, through their career paths. These results can inform both online and brick-and-mortar universities how to best coach/train/mentor students or integrate peer/professional mentorship and peer interaction into current advising practices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1907550","I-Corps Teams:  Invisible Light Field Messaging","IIP","I-Corps","12/01/2018","12/14/2018","Kristin Dana","NJ","Rutgers University New Brunswick","Standard Grant","Ruth Shuman","11/30/2020","$50,000.00","","kristin.dana@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is significant in the area of light field messaging.  The goal of this technology is invisible communication to a machine, and not subliminal communication to users who may observe the image. Information is embedded within images on a display so that the messages are machine-readable but invisible to humans. The electronic display, such as a billboard or kiosk, retains its purpose to convey visual information such as maps, advertisements and schedules.  Simultaneously, the display conveys a time-varying message to a camera-equipped computational system, such as a smartphone or robot. The near-term target application is interactive televisions, computer displays, and electronic billboards using existing cameras on smartphones.  With light field messaging, a phone-based mobile app could: retrieve product information from a television ad, obtain nearby traffic conditions from a roadside billboard, or obtain walking directions within a large airport from a kiosk.   This approach is expected to provide significant new avenues of interactive marketing and media. Additionally, the technology will provide novels methods of indoor localization, where GPS is not precise. Light field messaging for sending navigation cues to robotic systems and self-driving cars is a promising future application. <br/><br/>This I-Corps project uses an innovative deep learning architecture to embed a new message in each frame of video. The goal of this technology is invisible communication to a machine, and not subliminal communication to users who may observe the image. The approach is comprised of software-based algorithms with two main components: 1) Embedding method that embeds/codes hidden messages into photos and videos together and 2) Recovery method on the camera side that retrieves the message. Application software will run on display systems for embedding and on smartphones or other camera systems for obtaining messages from embedded imagery on electronic displays. Unlike older methods such as digital steganography, this approach uses modern neural networks to learn a robust coding method that overcomes the distortion of the light field transmission channel.  This messaging paradigm also improves on prior QR-codes because the code does not distract from the visible image and the code is dynamic, allowing significantly more information to be transmitted.   The message is fully contained within a single frame of video so that issues of time synchronization are avoided.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763896","CIF: Medium: Multi-Agent Consensus Equilibrium: Modular Methods for Integrating Disparate Sources of Expertise","CCF","Special Projects - CCF, Comm & Information Foundations","05/01/2018","09/17/2019","Charles Bouman","IN","Purdue University","Continuing Grant","Phillip Regalia","04/30/2022","$1,216,000.00","Gregery Buzzard, Garth Simpson, Stanley Chan","bouman@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","2878, 7797","7924, 7935, 9251","$0.00","Over the past decade, two major trends have reshaped data science: (i) a growing tidal wave of imaging and sensing devices and (ii) the rapid adoption of break-through machine learning technologies. From autonomous vehicles to medical devices, the ability to collect and analyze large quantities of data is changing the world. The goal of this research project is to create a new mathematical and computational framework for integrating distributed and multimodal sensor data into multiple types of emerging models in data science and machine learning so as to extract more information from data. The project team includes researchers from diverse disciplines who will address problems ranging from physical science and medicine to consumer imaging and industrial inspection. The research will result in theories, algorithms, and open software that can be used to integrate information from heterogeneous sensing systems to estimate and reconstruct signals and images.<br/><br/>The framework for model-data integration is based on a new theory of Multi-Agent Consensus Equilibrium (MACE). MACE allows for modular integration of multi-modal physical sensor information with information derived from data science models. At the core of this approach is the computational solution of the consensus equilibrium equations. These equations balance distributed sensor information with prior knowledge provided by machine learning models. The MACE framework is a generalization of the more traditional Bayesian or regularized inverse approach, but it allows for the use of non-traditional data science models such as deep convolutional neural networks in the solution of sensing and imaging problems. This project's contributions are in four areas: Thrust 1: Foundational Theoretical Methods; Thrust 2: Robust Sensor and Data Model Integration; Thrust 3: Multimodal and Networked MACE; and Thrust 4: Automated Experimentation. The project also includes integrated educational activities, engaging both graduate and undergraduate students in this research, as well as the development of new courses on consensus equilibrium and nonlinear optical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764032","AF: RI: Medium: Collaborative Research: Understanding and Improving Optimization in Deep and Recurrent Networks","IIS","Robust Intelligence","08/01/2018","06/25/2019","Nathan Srebro","IL","Toyota Technological Institute at Chicago","Standard Grant","Rebecca Hwa","07/31/2021","$549,101.00","","nati@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7495","7495, 7924, 9251","$0.00","Machine learning using deep neural networks has recently demonstrated broad empirical success. Despite this success, the optimization procedures that fit deep neural networks to data are still poorly understood. Besides playing a crucial role in fitting deep neural networks to data, optimization also strongly affects the model's ability to generalize from training examples to unseen data. This project will establish a working theory for why and when large artificial neural networks train and generalize well, and use this theory to develop new optimization methods. The utility of the new methods will be demonstrated in applications involving language, speech, biological sequences and other sequence data. The project will involve training of graduate and undergraduate students, and the project leaders will offer tutorials aimed at both the machine learning community, and other researchers and engineers using machine learning tools. <br/><br/>In order to establish a theory of why and when non-convex optimization works well when training deep networks, both empirical top-down and analytic bottom-up approaches will be pursued. The top-down approach will involve phenomenological analysis of large scale deep models used in practice, both when presented with real data, and when presented with data specifically crafted to test the behavior of the network. The bottom-up approach will involve precise analytic investigation from increasingly more complex models, starting with linear models, and non-convex matrix factorization, progressing through linear neural networks, models with a small number of hidden layers, and eventually reaching deeper and more complex networks. The theory developed aims to be both explanatory and actionable, and will be used to derive new optimization methods and modifications to architectures that aid in optimization and generalization. A particularly important testbed is the case of recurrent neural networks. Recurrent neural networks are powerful sequence models that maintain state as they process an input sequence and are used for sequence data. Particularly challenging to optimize, recurrent neural networks still leave much room for a stronger principled understanding, which the project aims to provide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801663","SaTC: CORE: Medium: Collaborative: Enabling Long-Term Security and Privacy through Retrospective Data Management","CNS","Secure &Trustworthy Cyberspace","08/01/2018","09/14/2019","Blase Ur","IL","University of Chicago","Continuing Grant","Sara Kiesler","07/31/2022","$316,000.00","","blase@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8060","025Z, 065Z, 7434, 7487, 7924, 9178, 9251","$0.00","Online data storage, everything from past conversations to tax returns to playdate invitations, may be retained at full fidelity for years or decades. Although the data being saved in online archives does not change, the personal and social contexts surrounding them do. Those life changes may necessitate changing or deleting stored data but, unfortunately, the vast quantity of data in users' online archives makes manual management infeasible. The goal of this project is to develop methods and tools that enable users to manage the data they have accumulated over many years, leveraging user-centered design and machine learning to partially automate the process. These tools will enable a better understanding of retrospective privacy in the context of modern long-lived online archives. They will also empower users to more effectively manage the risks embedded in these archives. The findings shared with the research community will advance discovery beyond this project. <br/><br/>The understanding of user conceptualizations of security and privacy over time, as contexts change, has been stymied by a lack of broad, carefully collected datasets within this domain. This project will collect anonymized datasets, with users' permission, that enable further research in this area. The team is conducting one of the first longitudinal studies of how desired security and privacy decisions change over time. The project will also gather qualitative insights about users' perceptions of risk and utility for long-term data, as well as the acceptability of retrospective management mechanisms. Furthermore, it is not currently understood how temporality impacts the use of machine learning techniques for privacy, nor how to capture concept drift to ensure that latent threats can be identified within immense archives. These tasks require new machine learning approaches and predictive models that can both account for the temporal dimension and minimize user burden when automating archive management. Finally, the project will design and implement novel user-centered interfaces that address the currently unmet need of helping users efficiently minimize security and privacy risks in their large, long-term online archives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818253","Computation for the Endless Frontier","OAC","Leadership-Class Computing","09/01/2018","09/21/2019","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","02/29/2024","$62,999,135.00","Dhabaleswar Panda, John West, Tommy Minyard, Omar Ghattas","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781","026Z","$0.00","Computation is critical to our nation's progress in science and engineering. Whether through simulation of phenomena where experiments are costly or impossible, large scale data analysis to sift the enormous quantities of digital data scientific instruments can produce, or machine learning to find patterns and suggest hypothesis from this vast array of data, computation is the universal tool upon which nearly every field of science and engineering relies upon to hasten their advance. This project will deploy a powerful new system, called ""Frontier"", that builds upon a design philosophy and operations approach proven by the success of the Texas Advanced Computing Center (TACC) in delivering leading instruments for computational science. Frontier provides a system of unprecedented scale in the NSF cyberinfrastructure that will yield productive science on day one, while also preparing the research community for the shift to much more capable systems in the future.  Frontier is a hybrid system of conventional Central Processing Units (CPU) and Graphics Processing Units (GPU), with performance capabilities that significantly exceeds prior leadership-class computing investments made by NSF.  Importantly, the design of Frontier will support the seamless transition of current NSF leadership-class computing applications to the new system, as well as enable new large-scale data-intensive and machine learning workloads that are expected in the future.  Following deployment, the project will operate the system in partnership with ten academic partners.  In addition, the project will begin planning activities in collaboration with leading computational scientists and technologists from around the country, and will leverage strategic public-private partnerships to design a leadership-class computing facility with at least ten times more performance capabilities for Science and Engineering research, ensuring the economic competitiveness and prosperity for our nation at large.<br/><br/>TACC, in partnerships with Dell EMC and Intel, will deploy Frontier, a hybrid system offering 39 PF (double precision) of Intel Xeon processors, complemented by 11 PF (single precision) of GPU cards for machine learning applications. In addition to 3x the per node memory of NSF's prior leadership-class computing system primary compute nodes, Frontier will have 2x the storage bandwidth in a storage hierarchy that includes 55PB of usable disk-based storage and 3PB of 'all flash' storage, to enable next generation data-intensive applications and support for the data science community.  Frontier will be deployed in TACC's state-of-the-art datacenter which is configured to supply 30% of the system's power needs from renewable energy.  Frontier will include support for science and engineering in virtually all disciplines through its software environment support for application containers, as well as through its partnership with ten academic institutions providing deep computational science expertise in support of users on the system. The project planning effort for a Phase 2 system with at least 10x performance improvement will incorporate a community-driven process that will include leading computational scientists and technologists from around the country and leverage strategic public-private partnerships.  This process will ensure the design of a future NSF leadership-class computing facility that incorporates the most productive near-term technologies, and anticipates the most likely future technological capabilities for all of science and engineering requiring leadership-class computational and data-analytics capabilities.  Furthermore, the project is expected to develop new expertise and techniques for leadership-class computing and data-driven applications that will benefit future users worldwide through publications, training, and consulting.  The project will leverage the team's unique approach to education, outreach, and training activities to encourage, educate, and develop the next generation of leadership-class computational science researchers. The team includes leaders in campus bridging, minority-serving institute (MSI) outreach, and data technologies who will oversee efforts to use Frontier to increase the diversity of groups using leadership-class computing for traditional and data-driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839686","Collaborative Research: FW-HTF: Integrating Cognitive Science and Intelligent Systems to Enhance Geoscience Practice","DUE","FW-HTF-Adv Cogn & Phys Capblty, IUSE","10/01/2018","11/25/2019","M. Ani Hsieh","PA","University of Pennsylvania","Standard Grant","Stephanie August","09/30/2021","$512,960.00","","m.hsieh@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","EHR","082Y, 1998","063Z, 8209, 9178","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>This project will make a significant contribution toward the support of future workers in geology. Understanding how geologists reason, plan to collect new data, consider three-dimensional spatial relations, and evaluate uncertainty are critically important for supporting scientists working on applied problems, such as natural resource exploration. This project will enhance existing efforts in geology to collect data using robot drones.  Drones allow access to important areas of the world too dangerous to access in person and not visible from satellite or plane.  The project will use machine learning to incorporate expert knowledge into drone flights to support effective autonomous data collection.  The data will yield improved geological understanding of an important fault system. Findings from the project will improve understanding of uncertainty in volumes and thus improve our understanding of earthquakes and the analyses of petroleum workers.  Understanding how expert geologists reason will support new exploration and mapping strategies for human-robot teams working in natural environments. The collaborative efforts of the interdisciplinary team will advance the fields of cognitive science, geology, and machine learning. The integration of cognitive science, robotics, and geology will develop new approaches to field work with human-autonomous systems teams that are faster and more effective than any either human or autonomous system would be acting alone. <br/><br/>The project will characterize expert spatial reasoning about 3D relations and uncertainty as geologists collect data to develop a 3D understanding of a new field area, make predictions about future observations, and construct geological models.  Errors in reasoning about 3D structures will be used to develop quantitative models of expert uncertainty. These models will be used to help explicitly visualize uncertainty for the experts and to construct cost functions for the robot navigation. The cost functions will include metrics that capture scientific value. The project will develop new approaches to drone exploration and mapping, including machine learning of features of interest to geologists. Drones will autonomously explore and map natural rock formations in canyon environments to support and speed up the data collection and interpretation efforts of field geologists.  The project will study the structural geology of the Mecca Hills area of California, a well exposed portion of the San Andreas fault system.  Robot drones will collect data about surface features to develop maps of subsurface structures. The cognitive science-infused robot design will employ successful expert strategies and focus on areas where experts are likely to make errors to prioritize exploration of those areas in navigation plans. The proposed strategies will enable 3D surface reconstruction of canyon surfaces. They will also enable better understanding of how to enhance planning and on-the-fly decision making of experts for collecting scientifically important data.   The project's foundational work aims to develop an interdisciplinary understanding of how geologists build a scientific understanding of a region over time. It also aims to design autonomous exploration strategies for human-robot teams, and test new ways to support the sequential decisions about where to collect data to maximize scientific impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764033","AF: RI: Medium: Collaborative Research: Understanding and Improving Optimization in Deep and Recurrent Networks","IIS","Robust Intelligence","08/01/2018","06/18/2018","Moritz Hardt","CA","University of California-Berkeley","Standard Grant","Rebecca Hwa","07/31/2021","$328,204.00","","hardt@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","7495, 7924","$0.00","Machine learning using deep neural networks has recently demonstrated broad empirical success. Despite this success, the optimization procedures that fit deep neural networks to data are still poorly understood. Besides playing a crucial role in fitting deep neural networks to data, optimization also strongly affects the model's ability to generalize from training examples to unseen data. This project will establish a working theory for why and when large artificial neural networks train and generalize well, and use this theory to develop new optimization methods. The utility of the new methods will be demonstrated in applications involving language, speech, biological sequences and other sequence data. The project will involve training of graduate and undergraduate students, and the project leaders will offer tutorials aimed at both the machine learning community, and other researchers and engineers using machine learning tools. <br/><br/>In order to establish a theory of why and when non-convex optimization works well when training deep networks, both empirical top-down and analytic bottom-up approaches will be pursued. The top-down approach will involve phenomenological analysis of large scale deep models used in practice, both when presented with real data, and when presented with data specifically crafted to test the behavior of the network. The bottom-up approach will involve precise analytic investigation from increasingly more complex models, starting with linear models, and non-convex matrix factorization, progressing through linear neural networks, models with a small number of hidden layers, and eventually reaching deeper and more complex networks. The theory developed aims to be both explanatory and actionable, and will be used to derive new optimization methods and modifications to architectures that aid in optimization and generalization. A particularly important testbed is the case of recurrent neural networks. Recurrent neural networks are powerful sequence models that maintain state as they process an input sequence and are used for sequence data. Particularly challenging to optimize, recurrent neural networks still leave much room for a stronger principled understanding, which the project aims to provide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821957","Verification, Validation, and Test of ML Systems (V-TML) Workshop","CCF","Special Projects - CCF, Software & Hardware Foundation","05/15/2018","05/09/2018","David Yeh","NC","Semiconductor Research Corporation","Standard Grant","Sankar Basu","04/30/2019","$49,960.00","","david.yeh@src.org","4819 Emperor Blvd.","Durham","NC","277035420","9199419400","CSE","2878, 7798","073Z, 7556, 7798, 7945, 7947","$0.00","Electronic systems are creating a renewed innovation cycle in many industries, including automobiles and manufacturing. These systems, which are at the heart of innovation in areas such as autonomous systems, often incorporate Machine Learning (ML) components to make decisions. Since safety, reliability, and predictability are paramount in these systems, this award funds a workshop to explore safe, reliable, and predictable applications of machine learning methods and systems, the performance of which often depends on factors that are not within the full control of the user or the system designer. These latter factors could range from uncertainties in training data, to lack of robust algorithms, to probabilistic aspects of inference mechanism etc. The organizing committee, as well as the participants are composed of individuals from academia, industry and the government, include diverse participation from minority, women and underrepresented groups.<br/><br/>Examples of technical aspects to be discussed at the workshop include, but are not limited to: (1) survey of relevant machine learning techniques, (2) data analytics currently used by electronic design automation, and problems in which similar paradigms are used, and (3) robustness, test, validation and verification in the context of ML. The outcomes of the workshop will be reported publicly and shared widely with the research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841805","Nonlinear dimensionality reduction and enhanced sampling in molecular simulation using auto-associative neural networks","CHE","CONDENSED MATTER & MAT THEORY, Chem Thry, Mdls & Cmptnl Mthds","07/01/2018","08/09/2018","Andrew Ferguson","IL","University of Chicago","Standard Grant","Evelyn Goldfield","05/31/2021","$304,490.00","","andrewferguson@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","1765, 6881","7433, 7573, 8084, 9216, 9263","$0.00","Andrew Ferguson of the University of Illinois at Urbana-Champaign is supported by an award from the Chemical Theory, Models and Computational Methods Program in the Chemistry Division to establish new theoretical approaches and computational tools to accelerate molecular simulations of protein folding. This project is cofunded by the Condensed Matter and Materials Theory Program in the Division of Materials Research. Proteins are molecular workhorses that perform the essential functions of life.  Proteins have evolved to adopt shapes that enable them to do these tasks. Determining the shape and motions of a protein can help reveal how it works and inform how to design new proteins to help treat disease, produce biofuels, or make new materials. Computer simulations of proteins are very useful in that they can identify the precise locations and motions of all the constituent atoms. For all but the smallest proteins, however, it is too computationally intensive to accurately predict their structure and motions even with powerful supercomputers. Ways to accelerate these simulations have been developed, but to work well they need good estimates of the structural rearrangements that the protein will make. This is a problem, since this is usually the question the simulations are trying to answer. In this work, Professor Ferguson is developing a new approach to accelerate protein folding simulations using a type of machine learning known as artificial neural networks so-called because they are based on the structure of neurons in the brain.  Neural networks allow computers to both determine these important structural pathways and use them to make simulations run faster. This new approach is being used to help understand large proteins involved in cancer and HIV infection.  It is also being incorporated into popular simulation software available for free public download. Professor Ferguson is providing research opportunities for undergraduates to work with him on this project and he is developing hands-on workshops in computational materials science as part of the Girls Learning About Materials (GLAM) summer camp at the University of Illinois. <br/><br/>The aim of this work is to establish a nonlinear machine learning approach to discover collective variables for protein folding and to use these variables to perform enhanced sampling in molecular dynamics simulations. The success of enhanced sampling techniques in accelerating conformational sampling is predicated on the availability of good collective variables (CVs) correlated with important molecular motions. Existing nonlinear dimensionality reduction techniques (e.g., diffusion maps, Isomap, land ocally linear embedding) can ably discover good CVs, but do not furnish the explicit coordinate mapping so that biased sampling must be conducted inefficiently and indirectly in proxy variables. This work establishes a new enhanced sampling approach based on auto-associative artificial neural networks (""autoencoders"") to discover CVs that are explicit differentiable functions of the atomic coordinates and to permit calculation of analytical biasing forces. This approach is termed MESA (Molecular Enhanced Sampling with Autoencoders). MESA is validated on the short peptides alanine dipeptide and tryptophan-cage, and deployed to discover metastable states and structural transitions in a kinase overexpressed in many cancers and an envelope protein presented on the surface of HIV. MESA is made broadly available to the molecular simulation community by collaborating with the developers of OpenMM and PLUMED to contribute the approach to future releases of these software packages. Positive research experiences have great benefits for undergraduate success and retention and this work supports 10-week summer research opportunities during each year of the award. Professor Ferguson is also developing new and exciting content for the highly successful Girls Learning About Materials (GLAM) summer camp at the University of Illinois to illustrate and promote computational materials science among female high school students and elevate female enrollment in STEM degree programs."
"1840120","Collaborative Research: FW-HTF: Augmented Cognition for Teaching: Transforming Teacher Work with Intelligent Cognitive Assistants","SES","FW-HTF-Adv Cogn & Phys Capblty","10/01/2018","09/06/2018","James Lester","NC","North Carolina State University","Standard Grant","Sara Kiesler","09/30/2022","$1,499,736.00","Bradford Mott","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","SBE","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>K-12 STEM teachers are critical to the US economy. With investments in teaching quality representing enormous economic value, the quality of education has been identified as a significant determinant of gross domestic product economic growth. However, the US teacher workforce is experiencing a crisis: teacher demand exceeds supply at every level, and attrition is extraordinarily high for new teachers.  Further, while STEM teaching represents one of the areas of highest need, STEM teachers leave the profession at high rates. These developments call for innovative workforce augmentation technologies to improve K-12 STEM teachers' performance and quality of work-life. To address this critical national need, the project will investigate how intelligent cognitive assistants for teachers can transform teacher work to significantly increase teacher performance and teacher quality of work-life. The project centers on the design, development, and evaluation of the Intelligent Augmented Cognition for Teaching (I-ACT) framework for intelligent cognitive assistants for teachers. With a focus on assisting K-12 STEM teachers in technology-rich inquiry teaching that supports collaborative, problem-based STEM learning, I-ACT cognitive assistants provide teachers with (1) prospective pedagogical guidance (preparation support preceding classroom teaching), (2) concurrent pedagogical guidance (real-time support during classroom teaching), and (3) retrospective pedagogical guidance (reflection support within a community of practice following classroom teaching). The project will culminate with an experiment conducted with a fully implemented version of I-ACT in public middle schools in North Carolina and Indiana.<br/><br/>The project realizes its objective through two primary thrusts. First, the research team will design and develop I-ACT cognitive assistants for K-12 STEM teachers and test them in public school classrooms. Utilizing AI-based multimodal learning analytics and a social constructivist theory of pedagogy, I-ACT cognitive assistants use machine-learned models of teacher orchestration to provide guidance throughout the full teaching workflow. I-ACT cognitive assistants operate in a tight feedback loop in which collected data will drive successive iterations of machine learning to train refined teacher support models for improved I-ACT cognitive assistant functionalities. Second, the research team will investigate how I-ACT cognitive assistants improve K-12 STEM teacher performance and teacher quality of work-life. The team will conduct focus groups, case studies, semi-structured interviews, and observations of teachers using I-ACT cognitive assistants in school implementations with middle school science teachers at the project's partner schools. The team will also conduct quasi-experimental studies to determine I-ACT impact on teacher performance and quality of work-life.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836914","BIGDATA: F: Metric-space Positioning Systems for Symbolic Data Science","IIS","Big Data Science &Engineering","10/01/2018","09/08/2018","Manuel Lladser","CO","University of Colorado at Boulder","Standard Grant","Sylvia Spengler","09/30/2021","$610,560.00","Rafael Frongillo","lladser@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8083","062Z, 7433, 8083","$0.00","Next-generation DNA sequencing technologies produce datasets that are the epitome of ""big data."" Resulting files are typically quite large, consisting almost entirely of symbolic (i.e., non-numeric) short DNA sequences. In contrast, the most widely used machine learning algorithms require numerical datasets to learn. Unfortunately, both traditional and cutting-edge methods to numerically represent symbolic data often suffer from high-dimensionality or substantial running time requirements, which hinder the application of powerful machine learning algorithms to modern biological questions. To overcome these crucial issues, this project addresses the fundamental problem of determining the ""right"" dimension in which to embed symbolic data for a data-mining or classification task. It does so by representing symbolic datasets numerically via a method reminiscent of Global Positioning Systems (GPS) but in a far more general setting. Besides exploring modern biology applications, the project will also investigate how to predict the source of a spread (i.e., ground zero) over large networks. This may assist administrators in determining how best to respond to new epidemics and cyber-threats. Additionally, the project will closely mentor undergraduate and graduate students to become mature data scientists. Its findings will be communicated as notes, open-source software, and video-lectures available to the general public, including students in the Colorado Data Science Team, which encourages the participation of women and under-represented minorities in Engineering education.<br/><br/>Much like GPS uses trilateration to locate a receiver anywhere on the planet, finite metric spaces contain resolving sets, that is sets of points that uniquely identify every point in the space via multilateration (i.e., the vector of distances to points in the set). Associated with any resolving set R, there is a one-to-one transformation from its ambient metric space to a Euclidean space of dimension |R|, the cardinality of R. The smallest resolving set thus induces the lowest-dimensional representation of its ambient space. Importantly, even when the ambient metric space is finite but exponentially large, its metric dimension is often much smaller than its cardinality. Determining the metric dimension is, however, an NP-hard problem in a variety of contexts. Building on this abstracted notion of multilateration, this project will: (1) assess the computational complexity of calculating the metric dimension of Hamming graphs, and characterize the metric dimension of various random graph models to guide the development of new and efficient algorithms to approximate this quantity; (2) explore relaxations and constraints of multilateration, including approximate and probabilistic algorithms, to expand the reach of applications of multilateration to other finite but large metric spaces; and finally (3) provide proofs-of-concept of multilateration to learn non-contiguous regions of dependencies in genomic sequences, develop classifiers for historically elusive virus targets, and identify the source of spread of information or disease in large networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750780","CAREER:  AF: Giving Form to Data with a Geometric Scaffold","CCF","Special Projects - CCF, Algorithmic Foundations","09/01/2018","07/24/2020","Benjamin Raichel","TX","University of Texas at Dallas","Continuing Grant","Joseph Maurice Rojas","08/31/2023","$310,486.00","","Benjamin.Raichel@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","2878, 7796","1045, 7926, 7929, 9251","$0.00","Using geometry to find structure in data is an old idea (Plato is quoted as saying, ""God ever geometrizes"") that gains fresh application as our data changes. Today's data sets, from areas such as machine learning, are often massive and high-dimensional: for example, when trying to classify news articles, each article may be represented as a point where the frequency of each word is a different dimension. This leads to geometries that are hard to grasp intuitively and hard to work with computationally (the ""curse of dimensionality""). Finding a smaller and lower dimensional subset of the data points that approximately preserves geometric structure not only reduces computation time but also can improve results by suppressing extraneous features. Representing inter-point distances of a general space in a more structured space can support new operations, such as data visualization by mapping the 2-D plane of the screen, or more efficient computation by mapping to the hierarchical structure of a tree. This project takes the age-old practice of teasing out geometric structure and applies it to the large- and high-dimensional data sets of the modern world. By taking a geometric approach to foundational problems in areas such as big data and machine learning, this project seeks to more closely connect computational geometry and these other areas, in turn both modernizing the classical field of computational geometry and advancing these other areas. The educational goals of this project will be achieved by directly supporting student research on the outlined topics, incorporating topics into developing new courses, and organizing regular seminars in order to grow the visibility and interdisciplinary nature of algorithms and theoretical computer science at the awardee institution.<br/><br/>Given a data set, the goal is to specify its geometric structure, use this structure to summarize and embed into simpler spaces where computations can be done efficiently, and when this is not possible, identify how to minimally fix the data to facilitate these tasks.  The project's focus is on three interrelated topics concerning geometric structure that lie at the intersection of big data, geometry, and machine learning: 1) data factorization and sparsification, 2) metric embeddings for structured spaces, and 3) metric violation distance.  The ultimate purpose is to develop better algorithms for handling data, ranging from better clustering algorithms to better classification algorithms. The ubiquity of such algorithms implies that any progress has the potential for significant real world impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826173","Designing Structural Alloys Through Interphase Boundary Segregation Engineering","CMMI","Special Initiatives, Materials Eng. & Processing","08/01/2018","07/24/2020","Srikanth Patala","NC","North Carolina State University","Standard Grant","Alexis Lewis","07/31/2021","$428,582.00","","spatala@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","ENG","088y, 1642, 8092","024E, 062Z, 077E, 083E, 088E, 091Z, 8021","$0.00","Structural alloys are crucial for the infrastructure and energy sectors, and have applications in the automotive, aerospace and nuclear industries. These alloy systems are usually metallic materials with certain impurity atoms added to improve their mechanical strength and thermal stability. One of the most commonly used strategy of improving mechanical strength, called precipitation hardening, relies on adding elements that form compounds, and impart excellent high-temperature strength. While a successful strategy, the temperature range at which these alloys can be utilized is limited. In particular, at high-temperatures, the strength degrades during the life-time of the component. This award supports research that will help increase the operating temperatures and thermal stability of precipitation-hardened structural alloys. New scientific knowledge about the behavior of these alloys during processing will enable the design of high-performance materials. The research approach combines computational simulation-based tools and machine-learning algorithms, and focuses on a model alloy system based on lightweight Magnesium, which has the potential to improve energy efficiency in the automotive industry. The broader impacts of this project thus combine commercial advances in structural materials, with a range of educational benefits in training secondary school teachers and graduate students involved with the project.<br/><br/>This work focuses on the development of a novel automated framework for simulating interphase boundaries using a machine learning approach to calculate interatomic potentials. The researchers will use the ternary Mg-Sn-Zn alloy for validating this framework but the approach can be generalized for screening potential ternary solute systems in any binary alloy system. This capability is crucial for designing precipitating systems with unprecedented thermal stability. This award also provides research opportunities to students from underrepresented groups, and the training of the next generation of secondary school teachers. Students from the NC State Education Department will participate in the scientific process through collaborative research projects on the themes of machine learning and atomistic simulations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755850","CRII: III: Interpretable Models for Spatio-Temporal Event Forecasting using Social Sensors","IIS","Info Integration & Informatics","08/01/2018","05/09/2018","Liang Zhao","VA","George Mason University","Standard Grant","Wei-Shinn Ku","07/31/2021","$174,990.00","","lzhao9@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7364","7364, 8228","$0.00","Significant events that occur at certain times and in specific locations, such as disease out-breaks and crime incidents, have tremendous impacts on our society. This strongly motivates the need to anticipate event occurrences in advance in order to reduce the potential social upheaval and damage caused. For example, for traffic congestion predictions based on social network reporting and traffic sensors, these methods would inform the authorities where the future congestion will occur and why certain ongoing traffic incident and congestion hot spots will worsen the problem on specific roads. In recent years, such model interpretability has attracted increasing attention as machine learning is beginning to be applied to ever more practical applications. As a domain with significant impact on society, the interpretability of spatio-temporal event forecasting models is particularly important in order to earn the trust of practitioners and become widely adopted in their everyday workflow. However, like conventional machine learning models, models for social event forecasting still primarily focus on prediction accuracy and are rapidly becoming too sophisticated and obscure to be easily understood by human operators. There is thus an urgent need to fill the increasing gap between data scientists and practitioners. To address it, this project focuses on developing a novel spatio-temporal social event forecasting framework that can jointly optimize the model accuracy and interpretability, and automatically illustrate the explanatory process of prediction generation. <br/><br/>To address challenges like spatial dependency and high-dimensional large data, the project aims at exploring the conditional independence and spatial topology to boost the sparsity of spatial dependence patterns. The project will then move on to exploit the hierarchical conjunction lattice of primitive data features to enforce the conciseness and sparsity of expository high-level representations of the data. To solve the formulated optimization problem for jointly maximizing accuracy and interpretability, this project also involves research on the corresponding optimization methods with rigorous theoretical analysis on efficiency and optimality. Finally, strategies for evaluating model interpretability in social event forecasting are systematically investigated. The success of this project will shed a light on the generic research in interpretable data mining and machine learning. The methods and tools developed in this project will help fill the gaps between data scientists and domain-specific forecasting experts. Finally, this project will provide valuable resources to support courses with new topics, datasets, techniques, and software, and gives more research opportunities for underrepresented students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809327","Visibility and Interactive Information Sharing in Collaborative Sensing Systems","ECCS","CCSS-Comms Circuits & Sens Sys","08/15/2018","08/15/2018","Gustavo de Veciana","TX","University of Texas at Austin","Standard Grant","Lawrence Goldberg","07/31/2021","$450,000.00","Haris Vikalo","gustavo@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","ENG","7564","153E","$0.00","Visibility and Interactive Information Sharing in Collaborative Sensing Systems<br/><br/>Self-driving vehicles and mobile robots have the potential to deliver transformative technological and societal changes. In order to make autonomous decisions, nodes need to have a reasonable degree of situational awareness achieved through recognition and tracking of entities in dynamic environments.  This may not be possible in partially occluded environments, where individual nodes may have limited visibility, unless nodes participate in collaborative sensing, i.e., share sensed information. Sharing raw/processed real-time sensing data with centralized resources in the cloud or at the network edge poses potentially high communication/computational burdens, particular in safety critical settings requiring low latency. This motivates the need to study distributed collaborative sensing frameworks leveraging powerful algorithms for tracking and deep learning models for reliable recognition/classification tasks. Of particular interest is a characterization of what collaborating sensors can ``see'' in occluded environments and how one should realize information sharing in resource constrained settings to fairly optimize what nodes ``know'', i.e., their situational awareness. The proposed research effort will advance the state-of-the-art in collaborative sensing systems which are expected to benefit the field and society more broadly, through planned efforts in education innovation, achieving diversity, engaging the community and industry, and disseminating results to a wider public.<br/><br/>This proposal centers on the study of collaborative sensing in obstructed/dynamic environments, such as might be used to enable self-driving vehicles and autonomous robots. The central challenge is to achieve an unprecedented level of real-time situational awareness based on distributed sensing resources in a possibly communication and/or computationally constrained setting. The proposed research integrates three research thrusts. The first is the advancement of the fundamental understanding what is visible to sets of distributed sensing units in stochastic environments. This work will leverage stochastic geometric models and analysis to provide robust quantitative performance assessment of `visibility' for typical random environments. The performance limits determined in this research thrust will inform what a distributed system can ``know"" in resource constrained settings. The second thrust is the development of fundamental underpinnings of distributed collaborative sensing with a focus on the optimization of interactive information sharing and/or adaptation to changing environmental contexts so as to jointly maximize situational awareness amongst autonomous yet collaborating nodes. We will provide new approaches driven by structural properties of the optimization problems (e.g., submodularity) and interactive information sharing protocols to facilitate distributed object recognition and tracking. The third thrust is the development of a scaled-down platform for controlled and reproducible experimentation of alternative collaborative sensing system designs. The last thrust is not only geared at providing platform to advance the research but is also an activity to engage a substantial number of undergraduates and a springboard to our educational efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827123","PFI-TT: Research and Development of a Novel Printer for Small Molecular-Based Medicines That Enhances Their Dissolution Properties and Cost-Effectiveness.","IIP","PFI-Partnrships for Innovation","08/01/2018","06/29/2020","Max Shtein","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Kaitlin Bratlie","04/30/2021","$199,947.00","","mshtein@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","1662","1662, 8042","$0.00","The broader impact/commercial potential of this PFI project will be to enhance the rate and cost-effectiveness of pharmaceutical research, development and manufacturing, ultimately lowering the cost of new, sophisticated medicines, as well as making it easier for people to take combinations of medicines required to combat some diseases. Many current practices in drug discovery and manufacturing are century-old, resulting in poor quality, poor patient compliance, poor scalability, and scarcity of many medicines. The proposed approach differs radically from existing practices in the pharmaceutical sector, enhances the properties of active ingre-dients in medicines, drives down the amount of solvents required for pre-clinical testing and thus reducing toxic chemical waste, yet is compatible with many existing drugs and dosage forms (e.g. pills, gel caps, patches, injections, etc.). If successful, the proposed technology could short-en by several years the drug development timeline, make it easier to combine multiple med-icines into a single pill or patch tailored to each patient, and reduce the amount of precious active ingredient being wasted due to body elimination or unused prescription medicines. <br/><br/>The proposed project has significant intellectual merit, in that it leverages techniques from the semiconductor industry and nanotechnology to address long-standing problems in pharmaceutical science, development, and manufacturing. The process at the core of this project can print active pharmaceutical ingredients (APIs) with precision and accuracy, while also enhancing their dissolution without resorting to strong or toxic solvents. It will advance understanding of thermal properties of pharmaceutical compounds, the link between molecular structure and crystal structure, and its influence on dissolution properties and bioa-vailability, which it enhances. While these capabilities have been shown for some compounds, they remain unavailable to the broader pharma research community. This multi-disciplinary project will make the capability widely available to molecular chemists, biologists, and pharmacologists, allowing them to devote more of their time to optimizing small molecular therapeutics for potency and site-specific binding, without facing solubility bottlenecks. The proposed work promises to unlock the therapeutic potential of millions of al-ready synthesized compounds that are languishing in material libraries due to their poor solubility, and to create combination therapies of unprecedented sophistication that will leverage deep learning- and data-driven medical science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828678","MRI: Development of an Underwater Mobile Testbed Using a Software-Defined Networking Architecture","CNS","Major Research Instrumentation, EPSCoR Co-Funding","10/01/2018","09/07/2018","Aijun Song","AL","University of Alabama Tuscaloosa","Standard Grant","Rita Rodriguez","09/30/2021","$300,000.00","Yang-Ki Hong, Fei Hu, Fumin Zhang","song@eng.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","1189, 9150","1189, 9150","$0.00","This project, developing one of the first Software Defined Network (SDN)-based underwater mobile testbeds to support the operation of marine robot fleets, aims to address a technological bottleneck, that of achieving integrated communications and navigation underwater. A fleet of Autonomous Surface Vehicles (ASV)s are directed to follow sampling Autonomous Underwater Vehicles (AUVs) to provide acoustic and Magnetic Induction (MI) communication over relatively short ranges. Launching the following effort thrusts: Acoustic & MI Communication; Control of ASVs & UAVs; SDN architecture; and Integration and evaluation.<br/><br/>The testbed will be designed to achieve cost-effectiveness, transferability, flexibility, and scalability and is expected to become a stable instrument that is accessible by multiple research communities that include ocean acoustics, communication and networking, robotics, oceanography and environmental sciences. The hybrid acoustic/MI communication will be used to achieve reliability and high data rates across the mobile network for ASVs and AUVs, while smooth autonomy of the fleet would be ensured by cooperative localization and real-time data transfer among the ASV-AUV pairs. The testbed is expected to enable various research directions, including underwater swarming, deep-learning-based underwater joint networking and navigation, and integrated oil spill responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1802627","EAGER: Improving our Understanding of Supercell Storms through Data Science","AGS","Physical & Dynamic Meteorology","01/15/2018","11/03/2017","Amy McGovern","OK","University of Oklahoma Norman Campus","Standard Grant","Jielun Sun","12/31/2019","$168,517.00","Cameron Homeyer, Corey Potvin","amcgovern@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","GEO","1525","7916, 9150","$0.00","This study seeks to apply novel data science techniques (such as tree-based classification models and deep learning) to four-dimensional (4D) weather radar observations of thunderstorm dynamics to enable identification of storms capable of producing tornadoes up to an hour prior to tornadogenesis. Real-time severe storm prediction is a challenging task that currently requires a human forecaster with a thorough understanding of the dynamics and current state of the atmosphere.  This study will develop and apply data science techniques to four-dimensional radar data from severe storms throughout the continental U.S. with the goal of identifying critical spatiotemporal relationships that can improve the understanding and prediction of tornadoes.  The long-term goal will be to develop techniques to fundamentally improve our understanding of severe storms in general (including hail, wind, and tornadoes) by analyzing the new knowledge identified by the data science models.<br/><br/>This study seeks to advance the scientific knowledge of tornadogenesis by identifying novel precursors to tornadoes in two unique 4D weather radar datasets. Data science has the potential to advance knowledge by processing and objectively evaluating a large amount of data in a relatively short period of time. This provides a mechanism by which large, complicated meteorological datasets can be assessed for their predictive capability or alternative applications without the need for time consuming subjective evaluation. The methods developed will enable others to evaluate existing Earth system data to a spatiotemporal extent that is not possible with established approaches.  The application of data science techniques to a novel domain will require the development of new techniques focusing on spatiotemporal 4D weather radar data."
"1801644","SaTC: CORE: Medium: Collaborative: Enabling Long-Term Security and Privacy through Retrospective Data Management","CNS","Secure &Trustworthy Cyberspace","08/01/2018","09/14/2019","Christopher Kanich","IL","University of Illinois at Chicago","Continuing Grant","Sara Kiesler","07/31/2022","$584,260.00","Elena Zheleva","ckanich@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","8060","025Z, 065Z, 7434, 7487, 7924","$0.00","Online data storage, everything from past conversations to tax returns to playdate invitations, may be retained at full fidelity for years or decades. Although the data being saved in online archives does not change, the personal and social contexts surrounding them do. Those life changes may necessitate changing or deleting stored data but, unfortunately, the vast quantity of data in users' online archives makes manual management infeasible. The goal of this project is to develop methods and tools that enable users to manage the data they have accumulated over many years, leveraging user-centered design and machine learning to partially automate the process. These tools will enable a better understanding of retrospective privacy in the context of modern long-lived online archives. They will also empower users to more effectively manage the risks embedded in these archives. The findings shared with the research community will advance discovery beyond this project. <br/><br/>The understanding of user conceptualizations of security and privacy over time, as contexts change, has been stymied by a lack of broad, carefully collected datasets within this domain. This project will collect anonymized datasets, with users' permission, that enable further research in this area. The team is conducting one of the first longitudinal studies of how desired security and privacy decisions change over time. The project will also gather qualitative insights about users' perceptions of risk and utility for long-term data, as well as the acceptability of retrospective management mechanisms. Furthermore, it is not currently understood how temporality impacts the use of machine learning techniques for privacy, nor how to capture concept drift to ensure that latent threats can be identified within immense archives. These tasks require new machine learning approaches and predictive models that can both account for the temporal dimension and minimize user burden when automating archive management. Finally, the project will design and implement novel user-centered interfaces that address the currently unmet need of helping users efficiently minimize security and privacy risks in their large, long-term online archives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839291","TRIPODS+X:EDU: Foundational Training in Neuroscience and Geoscience via Hackweeks","DMS","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","09/10/2018","Maryam Fazel","WA","University of Washington","Standard Grant","Tracy Kimbrel","09/30/2021","$176,190.00","Anthony Arendt, Aleksandr Aravkin, Ariel Rokem, Zaid Harchaoui","mfazel@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","041Y, 1253, 8624","047Z, 062Z, 8089, 8091","$0.00","Data-driven science and engineering requires close collaboration and coordination among researchers from different communities, including core sciences, statistics, and optimization. This project will build on and broaden the successful existing ""hackweek"" model to bring together participants from neuroscience and geoscience with experts in machine learning and optimization. The hackweeks will incorporate tutorials on core methods, hands-on sessions, and group activities designed to promote deeper understanding and closer collaboration of both data-driven scientific problems in neuroscience and geoscience, as well as fundamental methodologies and how they apply to these sciences. <br/><br/>In particular, the investigators plan to redesign geo-hackweek and neuro-hackweek, two events that the have been held annually at the University Washington by two of the PIs in recent years. Geo-hackweek will be redesigned to include the discussion of geophysical data interpolation and denoising, geophysical inverse problems, and Gaussian process models, and connecting these to techniques in optimization, including sparse and low-rank models, stochastic optimization, and PDE-constrained optimization. Neuro-hackweek will be augmented to include tutorials on the use of optimal transport models and Wasserstein distances in the analysis of neuroimaging data. This project aims to<br/><br/>(1) Expose participants from domain sciences to foundational topics, so they better understand data science tools, and in particular gain insight into how and when these algorithms work well (or do not work well);<br/>(2) Train participants to consider methods in the context of domain-specific problems, be able to identify domain-specific challenges, and think critically about how to effectively leverage optimization and machine learning tools for specific problem classes;<br/>(3) Expose students with foundations background to application domains, to understand practical challenges in application of machine learning tools;<br/>(4) Generate pedagogical material that can used in similar events;<br/>(5) Encourage collaborations between domain experts and experts on theory and methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830615","NRI: FND: Connected and Continuous Multi-Policy Decision Making","IIS","NRI-National Robotics Initiati","09/01/2018","08/23/2018","Edwin Olson","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","James Donlon","08/31/2021","$654,807.00","","ebolson@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8013","063Z, 8086","$0.00","The goal of this project is to create methods that allow robots to move and communicate in close proximity to other robots or humans. In these settings, a robot must understand how its behavior is likely to influence and change the behavior of other robots and people nearby.  The basic idea of this project is to allow the robot to select between several different strategies, picking the one that is most likely to work well in a given situation. For example, a robot might decide to veer towards the right because it predicts that an approaching wheelchair requires more room than a typical pedestrian. This project will also investigate how robots can coordinate with each other, deciding what information should be transmitted to teammate robots. This type of research is important in order to build robots that can safely and comfortably interact with regular people in everyday environments like their homes, schools, and hospitals. <br/><br/>The technical approach of this project is to extend a planning algorithm known as Multi-Policy Decision Making (MPDM).  Using an on-line forward roll-out process, candidate policies are evaluated from a ""library"" of options. The core tension in MPDM type systems is that larger libraries allow more flexible behaviors, but require greater computational resources. This project achieves expressivity in a different way than previous MPDM approaches: it allows policies to have one or more continuous parameters, and then efficiently computes good values of those continuous parameters. For example, whereas earlier MPDM work might have had several policies representing different nominal speeds of travel, this work allows robot designers to explicitly parameterize velocity. This continuous-valued parameter can be tuned using backpropagation methods similar to those used in deep learning networks.  The key advantage of this approach is that a single policy can generate a wider range of behaviors, which reduces the number of policies that must be explicitly considered. In turn, this reduces the computational complexity of the planning process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812535","EAGER - Integrating machine learning on autonomous platforms for target-tracking operations using stereo imagery","OCE","OCEAN TECH & INTERDISC COORDIN","01/01/2018","12/14/2018","Kakani Young","CA","Monterey Bay Aquarium Research Institute","Standard Grant","Kandace Binkley","12/31/2020","$323,007.00","","kakani@mbari.org","7700 SANDHOLDT RD","MOSS LANDING","CA","950399644","8317751803","GEO","1680","7916","$0.00","The ocean's midwaters (depths from 200 to 1000 meters where sunlight is dim) are increasingly becoming an area of interest for scientific discovery and study. Efforts to further explore this vast and incredibly important region in the ocean involves the development of small, nimble, autonomous underwater vehicles (AUVs) that can be used for a variety of missions. This proposal will use a large database in video images collected over 25 years to train the vehicle to identify and track targets in real-time using a pair of stereo cameras. This project will involve a Postdoctoral Researcher who will be mentored by collaborators at MBARI and Stanford, who are pioneers in applying machine learning algorithms to underwater imagery. Results of this effort will be disseminated via conferences, publications, and outreach through industry and media partners. Media programs at MBARI and National Geographic Society will produce YouTube videos and social media posts detailing the efforts, the project's personnel, methods, and discoveries.<br/><br/>The ocean's midwaters represent the largest ecosystem on earth with unique inhabitants and processes that link the surface waters to the seafloor. Efforts to further explore this vast and incredibly important region in the ocean involves development of AUVs that can be used for a variety of missions (e.g., transecting, tracking, fluid sampling). One of the key vehicle missions for these autonomous vehicles is to track targets in real-time. The tracking missions can be used for science questions as diverse as rates of marine snow sinking and its impact on biogeochemical cycling, the fate of rising methane from the benthos, and direct observations of organismal behavior to address their ecology and biomechanics. In order to conduct these tracking missions, robust algorithms are needed to identify and track targets as they change shape and state in realtime."
"1835389","NCS-FO: How Ecology Induces Cognition: Paleontology, Machine Learning, and Neuroscience","ECCS","IntgStrat Undst Neurl&Cogn Sys","09/15/2018","08/31/2018","Malcolm MacIver","IL","Northwestern University","Standard Grant","Shubhra Gangopadhyay","08/31/2022","$1,000,000.00","Daniel Dombeck","maciver@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","ENG","8624","8089, 8091, 8551","$0.00","We think of nervous systems as the means by which an animal organizes its world, but a deep time perspective suggests that it is rather the world of an animal that organizes its brain. Prior to the vertebrate invasion of land 385 million years ago, vision, our most powerful long-range sense, took in a largely blurry world at short range while underwater, with little variability in scene as the eyes move. Once on land, vision takes in a high contrast world at long range, with high variability as the eyes move. A possible reason for the greatly increased size and complexity of terrestrial vertebrate brains over those of fish is that this environment provides selective advantage to long sequences of actions toward distant goals, reaching its most complex form in varieties of prospective cognition in certain mammals and birds. A team of Northwestern researchers will conduct research into the computational, behavioral, and neural basis of planning, rooted in an evolutionary and computational sensory ecology perspective and a commitment to ethologically relevant behaviors. Planning is an immensely important capacity to understand the mechanistic basis of, as it participates in a diverse range of behaviors, and its diminishment favors impulsivity and reliance on the habit system. Up to now, laboratory studies of planning have typically relied on reduced environments and simple behaviors which are either appetitive or (more rarely) aversive, without a sentient target, the dynamics and unpredictability of which is likely key to the adequate analysis of prospective cognition. Methods from neuroengineering and data-intensive neuroscience will be brought to bear on the problem of making a more ethologically relevant, yet tightly controlled approach to investigating planning possible. The computational and behavioral work will be used to guide neurobiological interventions in two of the key brain structures that participate in reactive versus reflective decision making and choice: the striatum and hippocampus. <br/><br/>The team will pursue research with an unusually bold intellectual dynamic range well beyond a typical disciplinary approach, from its motivation rooted in evolutionary biology and computational sensory ecology, to the extension of the latest machine learning methods, through to single-cell resolution imaging of live animal behavior in a virtual reality system. The researchers will knit together parallel synergistic efforts in the simulation of planning, a mechatronically reconfigurable behavior arena with a robot predator, and two-photon single cell resolution imaging in a virtual reality system, resulting in an ethologically relevant context significantly more complex than current practice in laboratory settings. There are few areas of neuroscience that have as much potential to impact society as research on the neural basis of planning. Discussions of self-control, marshmallow tests, grit, and challenges we face in making long term plans such as retirement or adapting to changing climate for future generations fill the media. One of the team's research goals is to understand the manner in which the nervous system participates in constraining the temporal and spatial range of prospective cognition,which is clearly quite limited even in humans, toward a neuroscience of sustainability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826149","QRM: Hybrid Adversarial-Training Methods for 3D Virtual Microstructures","CMMI","AM-Advanced Manufacturing, Materials Eng. & Processing, DMREF, , ","09/01/2018","07/12/2019","Stephen Niezgoda","OH","Ohio State University","Standard Grant","Alexis Lewis","08/31/2022","$590,421.00","Dennis Dimiduk, Yunzhi Wang, Stephen Niezgoda","niezgoda.6@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","ENG","088Y, 8092, 8292, R210, S253","024E, 077E, 8021, 8400","$0.00","This grant will support research that will develop a rigorous scientific method in quantifying the internal structure, or microstructure, of engineering materials, advancing the frontiers of science and benefiting the US economy. Microstructure determines the properties and performance of engineering materials, but is often described in qualitative rather than quantitative terms. This award supports fundamental research to build the scientific tools to quantify microstructure, and enable the creation of digital representations of microstructure that can be used for computational simulations to predict material performance in structural and functional applications. This project uses state-of-the-art data science and machine-learning (ML), to build quantitatively accurate, high-fidelity virtual microstructures, and combines these tools with physics-based computational models to predict how materials in service respond to extreme environments. The resulting capabilities allow for the design of high-performance materials for manufacturing, infrastructure, and aerospace, and have the potential to transform how materials are brought from the laboratory to commercial applications.<br/><br/>This project uses state-of-the-art data science and machine-learning to build virtual microstructures with bounded uncertainty.  The approach builds upon ""shape-optimizers"" existing within the DREAM.3D synthetic microstructure software, and the nascent capabilities of generative adversarial networks (GAN).  These methods will be statistically coupled to experimental data for gains in quantitative representation accuracy.  The virtual microstructures represented in phase-field simulations will serve as one quality metric for the ML prediction.  The hybrid method explores and exploits use of the GAN methods to offset weaknesses within shape-optimizer and phase-transformation methods.  The goal is to obtain complete, realistic, and accurate virtual structures.  To demonstrate the gains from the new hybrid method, uncertainty quantification (UQ) techniques will be concurrently developed using both virtual ""materials phantoms,"" and evaluations of material descriptors from both synthetic structures and experiments.  The effort builds from open-source software and public datasets in the Materials Engineering and Machine Learning fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815254","AF: Small: Scalable Algorithms for Data and Network Analysis","CCF","Algorithmic Foundations","06/01/2018","05/30/2018","Shanghua Teng","CA","University of Southern California","Standard Grant","Tracy Kimbrel","05/31/2021","$500,000.00","","shanghua@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796","7923, 7926","$0.00","Data-based decision-making involving big input data sets require a lot of time for algorithms to process them. In computer science, efficient algorithms are generally considered to be the ones whose running time does not increase ""too fast"" when input size grows. ""Scalable algorithms"" are among the most efficient algorithms, because their running time is required to be nearly linear or even sub-linear with respect to the problem size. In other words, their complexity scales gracefully when the input size scales up. In the age of Big Data, efficient algorithms are in higher demand now more than ever before. While big data takes us into the asymptotic world envisioned by the pioneers of computer science, the explosive growth of problem size has also significantly challenged the classical notion of efficient algorithms: Algorithms that used to be considered efficient, according to the traditional polynomial-time characterization, may no longer be adequate for solving today's problems. It is not just desirable, but essential, that efficient algorithms should be scalable. Thus, scalability, instead of polynomial-time computability, should be elevated to the central complexity notion for characterizing efficient computation, and this will be the focus of algorithm design for network sciences and big data. This project will focus on the design and analysis of scalable algorithms.  One of its primary objective is to build bridges between the area of algorithm design and the fields of network sciences and machine learning.  If successful, the project will help to provide a rigorous algorithmic framework for designing new scalable data and network analysis algorithms.  By focusing on notions of algorithmic efficiency --- such as scalability --- that respect the sensibilities of researchers in these disciplines as to what constitutes a practical algorithm in the age of big data, this project aims to increase the value of theoretical analyses to researchers in these fields.  As this project combines ideas from many disciplines within one coherent research effort, lectures and tutorials presented on the fruits of the project will help cross-fertilize the disciplines within its scope.  The development of theoretical algorithms that might have practical applicability should simplify education in algorithms for students beyond theoretical computer science, and allow discussion of practically important heuristics at early stages of computer science education. The interdisciplinary nature of this research will enable broader engagements with PhD students and researchers in network sciences, machine learning, numerical analysis, and social science and hence will enhance the chance to be successful in supporting and working with women and minority researchers.<br/><br/>The technical goal of this project is to systematically extend the family of algebraic, numerical, and combinatorial techniques from the recent breakthroughs in Laplacian linear solvers and max-flows/min-cuts, to a wide-range of problems that arise in network analysis, data mining, and machine learning.  This project aims to show that these techniques --- including advanced sampling, sparsification, and local exploration of networks --- will play increasing roles in improving algorithmic scalability.  It contains several open questions and conjectures ranging from network analysis to distribution sampling to social influences towards this goal.  By providing new algorithmic insights into various dynamic processes over graphs, the project also aims to improve the understanding of network facets beyond static graph structures in order to develop better algorithmic theory for network sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761679","Collaborative Research: Enhancing Gait Dynamics via Physical Human-human and Human-Robot Interactions at the Hands","CMMI","M3X - Mind, Machine, and Motor","06/01/2018","05/30/2018","Irfan Essa","GA","Georgia Tech Research Corporation","Standard Grant","Robert Scheidt","05/31/2021","$319,231.00","Jun Ueda","irfan@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","058Y","063Z, 070E, 7632","$0.00","This project contributes to the understanding of how intelligent robotic systems can use a model of human intent, perception, and behavior to enhance human-robot cooperation, as may occur during rehabilitative gait training in physical therapy.  The project seeks to understand how small forces applied to the hand can be used to alter clinically-relevant human gait patterns.  The project is significant because it generates the experimental data and models of human gait and sensorimotor cognition needed to develop a robotic device that will interact physically and intuitively with individuals with mobility impairments to enhance their quality of life.  By developing a robotic test-bed system and the models of human sensorimotor behavior that will guide its actions, this project will serve the national interest and advance the NSF mission to promote the progress of science and to advance the national health.  <br/><br/>The project takes a three-stage approach to advancing the objectives of the NSF's Mind, Machine and Motor Nexus (M3X) program, which are to understand how the human mind controls body movements during the manipulation of machines, and how machine response can shape and influence both the mindset and movements of the human user. First, the project team will use novel instrumented devices and motion capture technology to measure interactive human-to-human hand forces and the resulting gait motions that occur during the proposed therapeutic intervention. In Stage 2 they will use machine learning techniques to develop a model of the motor and cognitive transformations that occurred during the human-to-human experiments of Stage 1. Finally, the team will embed the models within a novel robotic test-bed that will implement physical human-robot interactions at the hands. Human-robot experiments will evaluate the ability of the test-bed to promote desired changes in clinically-relevant gait characteristics such as gait speed, step length and step cadence.  Fundamental issues addressed by the project include: 1) identifying the relationships between hand forces, gait dynamics and perceived intents of the instructor (therapist) and student (patient) that arise during a simplified version of rehabilitative partner dance; 2) modeling those relationships so as to enable prediction of how small hand forces indirectly change human gait dynamics; and 3) developing a robotic gait coach capable of promoting desired changes in human gait dynamics.  In addition, the project supports education and promotes diversity through innovative outreach activities that will engage teams of students and older adults from underrepresented communities in the conception, design and prototyping of mobility-related assistive technologies. The project outcomes may have long-term impact on the quality of life of millions of Americans with deficits of gait by developing a robotic system that can assist therapists in their efforts to improve patient fitness, mobility, and independence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751161","CAREER: Building an Advanced Cyberinfrastructure for the Data-Driven Design of Chemical Systems and the Exploration of Chemical Space","OAC","CAREER: FACULTY EARLY CAR DEV, Chem Thry, Mdls & Cmptnl Mthds","03/01/2018","02/08/2018","Johannes Hachmann","NY","SUNY at Buffalo","Standard Grant","Alan Sussman","02/28/2023","$561,685.00","","hachmann@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1045, 6881","026Z, 062Z, 1045, 8084, 9263","$0.00","Innovation in chemistry and materials is a key driver of economic development, prosperity, and a rising standard of living.  It also offers solutions to pressing problems on energy, environmental sustainability, and resources that shape our society.  This research program is designed to boost the chemistry community's capacity to address these challenges by transforming the process that creates underlying innovation. The research promotes a shift away from trial-and-error searches and towards rational design.  These combine traditional chemical research with modern data science by introducing tools such as machine learning into the chemical context.  This project enables and advances this emerging field by building a cyberinfrastructure that makes data-driven research a viable and widely accessible proposition for the chemistry community, and thereby an integral part of the chemical enterprise.  Tools and methods developed in this research provide the means for the large-scale exploration of chemical space and for a better understanding of the hidden mechanisms that determine the behavior of complex chemical systems.  These insights can potentially accelerate, streamline, and ultimately transform the chemical development process.  The project also tackles the concomitant need to adapt education to this new research landscape in order to adequately equip the next generation of scientists and engineers, to build a competent and skilled workforce for the cutting-edge R&D of the future, and to ensure the competitiveness of US students in the international job market.  By promoting minority participation in this promising field, it contributes to a sustained push towards equal opportunity in our society.  This project thus promotes the progress of science and advances prosperity and welfare as stated by NSF's mission. <br/><br/>While there is growing agreement on the value of data-driven discovery and rational design, this approach is still far from being a mainstay of everyday research in the chemistry community.  This work addresses three key obstacles: (i) data-driven research is beyond the scope and reach of most chemists due to a lack of available and accessible tools, (ii) many fundamental and practical questions on how to make data science work for chemical research remain unresolved, and (iii) data science is not part of the formal training of chemists, and much of the community thus lacks the necessary experience and expertise to utilize it.  This research centers around the creation of an open, general-purpose software ecosystem that fuses in silico modeling, virtual high-throughput screening, and big data analytics (i.e., the use of machine learning, informatics, and database technology for the validation, mining, and modeling of resulting data sets) into an integrated research infrastructure.  A key consideration is to make this ecosystem as comprehensive, robust, and user-friendly as possible, so that it can readily be employed by interested researchers without the need for extensive expert knowledge.  It also serves as a development platform and testbed for innovation in the underlying methods, algorithms, and protocols, i.e., it allows the community to systematically and efficiently evaluate the utility and performance of different techniques, including new ones that are being introduced as part of this project.  A meta machine learning approach is being developed to establish guidelines and best practices that provide added value to the cyberinfrastructure.  The work is driven by concrete molecular design problems, which serve to demonstrate the efficacy of the overall approach.  The educational challenges that arise from the qualitative novelty of data-driven research and its inherent interdisciplinarity are addressesed by leveraging a new graduate program in Computational and Data-Enabled Science and Engineering for cross-cutting course and curricular developments, the creation of interactive teaching materials, and a skill-building hackathon initiative.  This award is jointly made with the Division of Chemistry's, Chemical Theory, Models and Computational Methods Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750667","CAREER: Addressing Scalability Challenges in Designing Next-generation GPU-Based Heterogeneous Architectures","CCF","Special Projects - CCF, Software & Hardware Foundation","02/01/2018","09/09/2019","Adwait Jog","VA","College of William and Mary","Continuing Grant","Yuanyuan Yang","01/31/2023","$272,558.00","","ajog@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","2878, 7798","1045, 7798, 7941","$0.00","Graphics processing units (GPUs) are becoming default accelerators in many domains such as high-performance computing (HPC), deep learning, and virtual/augmented reality. Their close integration with high-performance multi-core CPU architectures is also allowing very efficient heterogeneous computing. Going forward, it is imperative that such GPU-based systems scale both in terms of performance and energy efficiency to meet the exascale (and beyond) computing demands of the future. However, sustained scaling of these systems is challenging primarily because a) fabricating a single large die provides very low yield, making it prohibitively expensive, b) memory hierarchy remains a critical performance and energy efficiency bottleneck, and c) programmability and application scalability is hindered by inefficiencies in the shared virtual memory and multi-application support.<br/><br/>This project seeks to address these scalability challenges by rethinking the design of future large-scale GPU-based systems. In particular, this research project revolves around three major components: a) design space exploration of cores (including their organization) and the entire memory hierarchy, b) development of data movement optimization techniques by identifying and then exploiting cache locality via novel synergistic caching and scheduling techniques, and c) improving resource utilization of large-scale system resources by enhancing shared virtual memory and multi-application execution support. All three research components will be evaluated on a newly-developed comprehensive evaluation infrastructure. The findings of this research will be incorporated into new and existing undergraduate and graduate courses. It is expected that the insights resulting from this research would have a long-term positive impact on GPU-based computing, thereby making our daily lives more productive."
"1838650","SCH: INT: Collaborative Research: Novel Computational Methods for Continuous Objective Multimodal Pain Assessment Sensing System (COMPASS)","IIS","Smart and Connected Health","09/15/2018","02/03/2020","Richard Urman","MA","Brigham & Women's Hospital Inc","Standard Grant","Wendy Nilsen","08/31/2022","$406,411.00","Richard Urman, Kristin Schreiber","rurman@bwh.harvard.edu","75 Francis Street","Boston","MA","021156110","8572821670","CSE","8018","8018, 8062","$0.00","Few objective pain assessment techniques are currently available for use in clinical settings. Clinicians typically use subjective pain scales for pain assessment and management, which has resulted in suboptimal treatment plans, delayed responses to patient needs, over-prescription of opioids, and drug-seeking behavior among patients. This project will investigate science-based methods to build a robust Continuous Objective Multimodal Pain Assessment Sensing System (COMPASS) and a clinical interface capable of generating objective measurements of pain from multimodal physiological signals and facial expressions. COMPASS will allow objective measurements that can be used to significantly improve pain assessment, pain management strategies, reduce opioid dependency, and advance the field of pain-related research. The educational plan will include activities to engage patient training, K-12 students, minorities and underrepresented groups, as well as general public. These outcomes will also lead to development of a diverse work force needed to support advanced medical technologies and services.<br/><br/>Using advanced biosensing systems, data fusion algorithms and machine learning models, this project will develop a robust, reliable, and accurate pain intensity classification system, COMPASS, for estimating pain intensity experienced by patients in real-time on a 0-10 scale, which is the standard scale used by physicians in clinical settings. In the initial phase of the project, the team will conduct a pilot at Brigham and Women's Hospital to experiment with the different elements for developing the sensing systems and collect data to develop data fusion algorithms and machine learning models. In the later phase of the project, the team will collect an extensive set of data to train and validate the fully implemented COMPASS. Physiological sensor data from electroencephalograph, facial-expression, patient self-reported pain scales, and physician/nurse assessed pain scales will be collected from the subjects as they experience pain modulated by medical therapies that cause patients pain. The project will investigate evidence-based machine learning and feature extraction methods for physiological signals and facial-expression images. This highly interdisciplinary research will make significant contributions to the areas of pain assessment and management, human factors and patient safety.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816500","NeTS: Small:  Dynamic Predictive Streaming of 360 Degree Video","CNS","Networking Technology and Syst","10/01/2018","08/21/2018","Yong Liu","NY","New York University","Standard Grant","Darleen Fisher","09/30/2021","$499,901.00","Yao Wang","yongliu@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7363","7923, 9102","$0.00","Virtual Reality (VR) and Augmented Reality (AR) applications are projected to be the next wave of ""Killer Apps"" in the future Internet. VR/AR applications facilitate vivid immersive virtual and augmented reality experience and create tremendous new opportunities in many domains, including education, business, healthcare, and entertainment, etc. Many VR/AR applications involve streaming of 360-degree video scenes. Compared with the traditional video streaming, 360-degree video streaming requires much higher network bandwidth and much lower packet delivery latency, and user's quality of experience is highly sensitive to the dynamics in both network environment and user viewing behaviors. Addressing these unique challenges, this project will develop novel 360-degree video coding and delivery solutions to enable high quality interactive, on-demand, and live video streaming.   <br/><br/>The project includes several research thrusts to enable novel joint coding-and-delivery solutions for high quality and robust 360-degree video streaming. For interactive streaming, novel Field-of-View (FoV) adaptive coding structure will be designed to achieve low encoding and decoding latency. Realtime joint optimization of streaming rate adaption and video coding bits allocation based on the predicted FoV will be studied to maximize the rendered video quality.  For on-demand streaming, a two-tier video coding and delivery framework will be developed, and the rate allocation and video chunk scheduling between the two tiers will be investigated to strike the desired balance between the rendered video quality and streaming robustness. To facilitate predictive coding and delivery, the project will develop effective algorithms for predicting user FoVs, based on the past FoV trajectory and the audio and visual content through deep learning architectures. Personalized FoV prediction based on other users' view trajectories will also be explored under the framework of recommender systems. Fully-functional 360 video streaming prototypes will be developed and tested in controlled and real network environments to validate and improve the new designs. If successful, the research will lead to new theory and designs for 360-degree video coding and help enable the wide-spread deployment of high-quality and robust 360-video streaming systems. The research findings will be made available through publications, talks, open protocols, and open-source codes, allowing a multitude of developers, researchers, and companies to evolve 360-video streaming. The project will also create valuable research opportunities for graduate and undergraduate students, especially women and minority students. Interactions with industry will be facilitated through workshops and several research centers at the New York University.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760092","HARNESSING MACHINE LEARNING ALGORITHMS TO STUDY SCIENTIFIC GRANT PEER REVIEW","SMA","SciSIP-Sci of Sci Innov Policy","07/01/2018","04/04/2019","Anna Kaatz","WI","University of Wisconsin-Madison","Standard Grant","Cassidy Sugimoto","06/30/2021","$499,258.00","Anna Kaatz, You-Geon Lee","akaatz@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","SBE","7626","7626","$0.00","Armed with a $30 billion annual budget, the U.S. National Institutes of Health (NIH) leads the world in funding research to advance human health and treatments for disease. Like most funding agencies, NIH uses peer review to evaluate the merit of grant applications. Approximately three reviewers from a given review group (called a ""study section"") assign preliminary impact scores and write critiques to evaluate each application before attending study section meetings where all members contribute to a final priority score. Although NIH's review process is considered one of the best in the world, reports and self-studies show that racial/ethnic minorities and women have lower award rates for first time, and renewal applications, respectively, for NIH's largest funding mechanism, the R01 grant. This is problematic because R01s are critical for career advancement, and research conducted by racial/ethnic minorities and women is linked to technological innovation and is known to address costly education, economic, and health disparities. As a leader in efforts to diversify the science and medical workforce, NIH has called for studies to test for the possibility that bias may operate in its peer review process. This call brings to light the broad need for research on the effectiveness of peer review, which is used across all science and technology fields, and for more scientists to engage in such research. If factors unrelated to the quality of the proposed science negatively impact the outcome of a grant review, it runs counter to funding agencies' goals to select the best science, blocks expensive downstream federal efforts to broaden participation in science, and undermines the competitiveness of the U.S. scientific enterprise.<br/><br/>Our group was the first to show that, when combined with traditional analyses of scores and award rates, linguistic analysis of NIH peer reviewers' narrative critiques of R01 applications can show evidence of potential stereotype-based bias in reviewers' decision making. Although such bias is generally unintentional and impacts reviewers' judgment regardless of their own sex or race, it can lead reviewers to differentially enforce evaluation criteria. Controlled experiments show, for instance, that cultural stereotypes that racial/ethnic minorities and women lack intrinsic ability for fields like science, can lead reviewers to unconsciously require more proof to confirm their competence. Over the past decade machine learning technologies have made data-, text-, and video-mining into state-of-the-art analytic techniques, which, if applied to scientific peer review, could revolutionize the field. Long Short Term Memory (LSTMs) neural networks -- algorithms that function like the human brain to identify complex patterns in data -- in particular, have catapulted the application of computer science to the study of social and psychological phenomena. Using a large, demographically diverse set of NIH R01 application critiques, scores, and video of constructed study section discussions, this project is producing analytical tools that use LSTMs to capture evidence of stereotype-based bias in both written and oral discussion of grant applications. Resulting technologies are open-access, and available for applied use across scientific funding agencies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750698","CAREER:   Building an informative machine learning model of plant gene regulatory control","DBI","ADVANCES IN BIO INFORMATICS","09/01/2018","09/04/2019","Molly Megraw","OR","Oregon State University","Continuing Grant","Peter McCartney","08/31/2023","$222,205.00","","megrawm@science.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","BIO","1165","1045","$0.00","Despite the enormous potential of plant research for transformative advances in sustainable food and energy production, environmentally responsible materials biosynthesis, and safe effective medicines, a basic understanding of how plant genes are 'turned on' or 'turned off' in order to produce these useful products remains a primary scientific bottleneck. In order to understand how these genetic pathways function, one must identify and understand the patterns of DNA elements that ultimately control these genes. To address this fundamental scientific challenge in biological informatics, this project will use pattern recognition techniques in order to develop a detailed understanding of the way in which plant genes are controlled by DNA-encoded information. In order to do this, we will develop specific computational techniques for building and interpreting pattern recognition models. The most important aspect of this research is creating algorithms with the ability to recognize when a number of different DNA element patterns provide biologically relevant solutions for gene control. The algorithms, models, and biological outcomes developed by this project will be made available to the scientific community, including the plant science community. The underlying computational methods are expected to be applicable in many other fields of science and engineering. Teaching and training modules developed by this project will include outreach to our underserved communities of rural Oregon, including hands-on science experiences for high school students, as well as teacher training. The modules will draw special attention to the need for computationally trained researchers in the plant sciences, and to the need for a dramatically increased understanding of plant gene regulatory networks as a foundation for new drug discovery, agricultural, and materials biosynthesis challenges. <br/><br/>The combinatorial control of gene expression by Transcription Factors (TFs) is one of the most fundamental regulatory mechanisms across the eukaryotic tree of life, from plants to humans. The DNA region surrounding the start of a gene, called the promoter region, contains short regulatory sequence elements known as Transcription Factor Binding Sites (TFBSs). In plants, despite the many exciting potential advances that hinge on a detailed understanding of plant gene regulation, relatively little is known about the specific TFBS patterns that lead to gene expression. This project uses and extends a machine learning model developed by the Megraw lab that is able to predict the presence of Transcription Start Sites (TSSs) with high accuracy and resolution. A primary advantage of this model is that it suggests specific sets of TFBS:promoter interactions which have the potential to 'turn on' a particular gene. This model makes novel use of currently available high-throughput TSS data to examine the global structure of promoters in the model plant Arabidopsis, and in preliminary research suggests striking unanticipated differences between plant and animal DNA regulatory codes. The goal of this project is to rigorously test and expand this promising new computational approach for dissecting which TFBS sets are optimal predictors of gene up-regulation in a biological sample. The relative simplicity of the Arabidopsis genome makes this challenge tractable, yet capable of providing transformative insight into gene regulation in a multicellular organism. The results of this project can be found at http://megraw.cgrb.oregonstate.edu/projects/PlantReg<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837472","CPS: TTP Option: Medium: Collaborative Research: Trusted CPS from Untrusted Components","CNS","CPS-Cyber-Physical Systems","10/01/2018","09/19/2019","Bruce McMillin","MO","Missouri University of Science and Technology","Standard Grant","Phillip Regalia","09/30/2021","$962,695.00","Zhishan Guo, Rui Bo, Jonathan Kimball","ff@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","7918","7918, 7924, 9150","$0.00","The nation's critical infrastructures are increasingly dependent on systems that use computers to control vital physical components, including water supplies, the electric grid, airline systems, and medical devices.  These are all examples of Cyber-Physical Systems (CPS) that are vulnerable to attack through their computer systems, through their physical properties such as power flow, water flow, chemistry, etc., or through both. The potential consequences of such compromised systems include financial disaster, civil disorder, even the loss of life.  The proposed work significantly advances the science of protecting CPS by ensuring that the systems ""do what they are supposed to do"" despite an attacker trying to make them fail or do harm.  In this convergent approach, the key is to tell the CPS how it is supposed to behave and build in defenses that make sure each component behaves and works well with others.  The proposed work has a clear transition to industrial practice.  It will also enhance education and opportunity by opening up securing society as a fascinating discipline for K-12 students to follow. <br/><br/>The objective of the proposed project is to produce, from untrusted components, a trusted Cyber-physical system (CPS) that is resilient to security attacks and failures. The approach will rely on information flows in both the cyber and physical subsystems, and will be validated experimentally on high fidelity water treatment and electric power CPS testbeds. The project brings together concepts from distributed computing, control theory, machine learning, and estimation theory to synthesize a complete mitigation of the security and operational threats to a CPS. The proposed method's key difference from current methods is that security holes will be identified and plugged automatically at system design time, then enforced during runtime without relying solely on secure boundaries or firewalls. The system will feature the ability to identify and isolate a malfunctioning device or cyber-physical intrusion in real-time by validating its operation against fundamental scientific/engineering principles and learned behavior.  A combined mathematical/data science approach will be used to generate governing invariants that are enforced at system runtime.  Invariants are a scientific approach grounded in the system's physics coupled with machine learning and real-time scheduling approaches embedded in the CPS.  Robust state estimation will account for errors in measurement and automated security domain construction and optimization to reduce the cost of evaluation without sacrificing coverage. The successful outcome of this research will lead to improved national security across various CPS infrastructures which, in turn, will improve economic and population health and security.   The work can be taken to industry for deployment in critical infrastructures.  The project will stimulate interest in Science, Technology, Engineering and Mathematics (STEM) through the development of a water-themed tabletop exercise for K-12 and helping current college students develop an interest in outreach through the experiential learning aspects of developing the tabletop exercise.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837352","CPS: TTP Option: Medium: Collaborative Research: Trusted CPS from Untrusted Components","CNS","CPS-Cyber-Physical Systems","10/01/2018","09/17/2018","Aditya Mathur","IN","Purdue University","Standard Grant","Phillip Regalia","09/30/2021","$37,200.00","","apm@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7918","7918, 7924, 9150","$0.00","The nation's critical infrastructures are increasingly dependent on systems that use computers to control vital physical components, including water supplies, the electric grid, airline systems, and medical devices.  These are all examples of Cyber-Physical Systems (CPS) that are vulnerable to attack through their computer systems, through their physical properties such as power flow, water flow, chemistry, etc., or through both. The potential consequences of such compromised systems include financial disaster, civil disorder, even the loss of life.  The proposed work significantly advances the science of protecting CPS by ensuring that the systems ""do what they are supposed to do"" despite an attacker trying to make them fail or do harm.  In this convergent approach, the key is to tell the CPS how it is supposed to behave and build in defenses that make sure each component behaves and works well with others.  The proposed work has a clear transition to industrial practice.  It will also enhance education and opportunity by opening up securing society as a fascinating discipline for K-12 students to follow. <br/><br/>The objective of the proposed project is to produce, from untrusted components, a trusted Cyber-physical system (CPS) that is resilient to security attacks and failures. The approach will rely on information flows in both the cyber and physical subsystems, and will be validated experimentally on high fidelity water treatment and electric power CPS testbeds. The project brings together concepts from distributed computing, control theory, machine learning, and estimation theory to synthesize a complete mitigation of the security and operational threats to a CPS. The proposed method's key difference from current methods is that security holes will be identified and plugged automatically at system design time, then enforced during runtime without relying solely on secure boundaries or firewalls. The system will feature the ability to identify and isolate a malfunctioning device or cyber-physical intrusion in real-time by validating its operation against fundamental scientific/engineering principles and learned behavior.  A combined mathematical/data science approach will be used to generate governing invariants that are enforced at system runtime.  Invariants are a scientific approach grounded in the system's physics coupled with machine learning and real-time scheduling approaches embedded in the CPS.  Robust state estimation will account for errors in measurement and automated security domain construction and optimization to reduce the cost of evaluation without sacrificing coverage. The successful outcome of this research will lead to improved national security across various CPS infrastructures which, in turn, will improve economic and population health and security.   The work can be taken to industry for deployment in critical infrastructures.  The project will stimulate interest in Science, Technology, Engineering and Mathematics (STEM) through the development of a water-themed tabletop exercise for K-12 and helping current college students develop an interest in outreach through the experiential learning aspects of developing the tabletop exercise.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839966","Collaborative Research: FW-HTF: Augmented Cognition for Teaching: Transforming Teacher Work with Intelligent Cognitive Assistants","IIS","FW-HTF-Adv Cogn & Phys Capblty","10/01/2018","09/06/2018","Krista Glazewski","IN","Indiana University","Standard Grant","Tatiana Korelsky","09/30/2022","$1,499,985.00","Thomas Brush, Cindy Hmelo-Silver","glaze@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","082Y","063Z","$0.00","The Future of Work at the Human-Technology Frontier (FW-HTF) is one of 10 new Big Ideas for Future Investment announced by NSF. The FW-HTF cross-directorate program aims to respond to the challenges and opportunities of the changing landscape of jobs and work by supporting convergent research. This award fulfills part of that aim. <br/><br/>K-12 STEM teachers are critical to the US economy. With investments in teaching quality representing enormous economic value, the quality of education has been identified as a significant determinant of gross domestic product economic growth. However, the US teacher workforce is experiencing a crisis: teacher demand exceeds supply at every level, and attrition is extraordinarily high for new teachers.  Further, while STEM teaching represents one of the areas of highest need, STEM teachers leave the profession at high rates. These developments call for innovative workforce augmentation technologies to improve K-12 STEM teachers' performance and quality of work-life. To address this critical national need, the project will investigate how intelligent cognitive assistants for teachers can transform teacher work to significantly increase teacher performance and teacher quality of work-life. The project centers on the design, development, and evaluation of the Intelligent Augmented Cognition for Teaching (I-ACT) framework for intelligent cognitive assistants for teachers. With a focus on assisting K-12 STEM teachers in technology-rich inquiry teaching that supports collaborative, problem-based STEM learning, I-ACT cognitive assistants provide teachers with (1) prospective pedagogical guidance (preparation support preceding classroom teaching), (2) concurrent pedagogical guidance (real-time support during classroom teaching), and (3) retrospective pedagogical guidance (reflection support within a community of practice following classroom teaching). The project will culminate with an experiment conducted with a fully implemented version of I-ACT in public middle schools in North Carolina and Indiana.<br/><br/>The project realizes its objective through two primary thrusts. First, the research team will design and develop I-ACT cognitive assistants for K-12 STEM teachers and test them in public school classrooms. Utilizing AI-based multimodal learning analytics and a social constructivist theory of pedagogy, I-ACT cognitive assistants use machine-learned models of teacher orchestration to provide guidance throughout the full teaching workflow. I-ACT cognitive assistants operate in a tight feedback loop in which collected data will drive successive iterations of machine learning to train refined teacher support models for improved I-ACT cognitive assistant functionalities. Second, the research team will investigate how I-ACT cognitive assistants improve K-12 STEM teacher performance and teacher quality of work-life. The team will conduct focus groups, case studies, semi-structured interviews, and observations of teachers using I-ACT cognitive assistants in school implementations with middle school science teachers at the project's partner schools. The team will also conduct quasi-experimental studies to determine I-ACT impact on teacher performance and quality of work-life.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823037","SPX: Collaborative Research: Global Address Programming with Accelerators","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","John Owens","CA","University of California-Davis","Standard Grant","Tevfik Kosar","09/30/2021","$386,025.00","","jowens@ece.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","042Y","026Z","$0.00","Large-scale computing today is dominated by parallel computing, where a large task is divided into many smaller tasks and those smaller tasks run at the same time. Traditionally each of those tasks run independently up to a common stopping point, then they halt, exchange information, and continue. This global stop-and-communicate step is quite expensive. This project instead pursues a different approach, where individual tasks directly communicate with other tasks asynchronously, without having to wait for a global stopping point. This approach is likely to yield better performance on large-scale computing tasks, specifically on what is becoming the dominant large-scale machine, a heterogeneous machine with many CPUs and other<br/>many-core processors. The project will deliver a set of high-performance, open-source data structures and algorithm implementations to support irregular patterns of communication, notably those that arise in biology, graph analytics, and sparse linear algebra for machine learning. These will not only be directly useful for end users but also demonstrate how to design and engineer primitives for accelerator-equipped distributed-memory machines. The project also engages application developers (both in our groups and externally) to make the outcomes broadly useful.<br/><br/>The project will develop a programming environment for accelerator-based HPC systems that integrates accelerators into a Partitioned Global Address Space (PGAS) model, which will allow direct communication between GPUs in a manner that is well suited to both applications and the underlying hardware. Specifically, GPU programming will be integrated with the UPC++ PGAS programming model (""GPUPC++""). The project will thus advance the state of the art in algorithms, programming models, and low-level support for the heterogeneous large-scale computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1907563","Collaborative Research: Statistical Learning, Driving Simulator-Based Modeling, and Computationally Tractable Dynamic Traffic Assignment","CMMI","CIS-Civil Infrastructure Syst","08/01/2018","12/07/2018","Srinivas Peeta","GA","Georgia Tech Research Corporation","Standard Grant","Yueyue Fan","07/31/2021","$219,497.00","","srinivas.peeta@ce.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1631","036E, 1057","$0.00","Congestion is familiar to anyone who relies on a privately owned or rented automobile, taxi, or public transit for commuting, shopping and errand running. Historically, engineers and scientists exploring traffic networks frequently build mathematical models with the intent of coaxing from them insights revealing how congestion may evolve over time. Unfortunately, such models may easily become so large and complex that they are unwieldly, and simplifications are needed in order to provide passengers and drivers with accurate and rapidly computable information pertinent to route choice and departure time selection. Toward that goal, this project will employ modern statistics, simulation experiments, and notions of competition among traffic network users for available road capacity to better depict and more efficiently compute the behaviors of drivers who rely on road networks. The broader impacts of this research will be substantial. In particular, the results of this research will allow commuters and urban freight carriers to make more informed travel decisions, and governmental organizations to better regulate travel decisions within heavily congested major metropolitan regions. This study will also provide system-level experiential learning opportunities for students entering the transportation workforce. <br/><br/>Specifically, through a combination of experiments and machine learning and model development, this project will aim to depict the noncooperative exploration of available routes and departure times by drivers and passengers seeking to fulfill their travel demands via metropolitan road networks. A key goal of the intended research will be the efficient computation of solutions to the most prevalent type of dynamic traffic assignment (DTA), namely so-called dynamic user equilibrium (DUE). It is the lack of closed-form travel-delay operators that makes DUE computation tedious and slow. The plan is to replace the existing, differential algebraic equation (DAE) system representing travel delay with closed-form, approximate delay operators based on a form of statistical learning known as Kriging. Ad hoc experiments based on such an approach show great promise for small networks, but are not definitive. The PIs will develop the envisioned models and make developed software available as free-ware or inexpensive apps."
"1816497","SaTC: CORE: Small: Adversarial Learning via Modeling Interpretation","CNS","Secure &Trustworthy Cyberspace","08/01/2018","07/23/2018","Xia Hu","TX","Texas A&M Engineering Experiment Station","Standard Grant","Wei-Shinn Ku","07/31/2021","$500,000.00","James Caverlee, Guofei Gu","xiahu@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","8060","025Z, 7434, 7923","$0.00","Machine learning (ML) models are increasingly important in society, with applications including malware detection, online content filtering and ranking, and self-driving cars. However, these models are vulnerable to adversaries attacking them by submitting incorrect or manipulated data with the goal of causing errors, causing potential harm to both the decisions the models make and the systems and people who rely on them. Further, many common ML models make decisions in ways that are hard for humans to understand, leading to calls to develop modeling techniques that make the models more explainable and interpretable. This project sits at the intersection of adversarial and explainable ML, with the key insight that as models become more interpretable in terms of both the individual decisions they make and the rules they use to distinguish between different decisions, this interpretability will likely provide additional information that can be used to both create and defend against adversarial attacks.  The overall project goal is to test this insight and contribute to both the security and data mining communities by developing an adversarial learning framework that leverages interpretability of ML models and results to both identify and mitigate the risks of adversarial attacks, especially in the context of big data. The project also contains a significant educational component, including incorporating the research into curriculum development and providing research opportunities to undergraduate and underrepresented students.<br/><br/>The project consists of three research thrusts. The first is to develop effective attacking strategies by analyzing modeling interpretation from three aspects including instance level, class level, and a specific group of deep neural networks. This enables more effective attacks to be initiated through understanding the underlying working mechanisms of ML models. The second thrust is to focus on developing defensive strategies to improve the robustness of ML models against these adversarial attacks. The proposed defensive strategies are aimed at the three major steps in a typical knowledge discovery pipeline including training data refinement, model architecture modification, and test data filtering. While existing efforts are based on continuously probing built systems and updating model parameters once prediction mistakes are discovered, the proposed work provides a proactive way to tackle the problem. The third thrust is to develop adversarial learning algorithms to deal with challenges and take advantage of opportunities brought by big data. Specifically, the developed adversarial attacking and defensive algorithms will deal with large-scale, heterogeneous, and relational data. This will enable the proposed algorithms to scale to real-world applications demonstrating challenging data characteristics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823034","SPX: Collaborative Research: Global Address Programming with Accelerators","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","Katherine Yelick","CA","University of California-Berkeley","Standard Grant","Tevfik Kosar","09/30/2021","$465,000.00","Aydin Buluc","yelick@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","042Y","026Z","$0.00","Large-scale computing today is dominated by parallel computing, where a large task is divided into many smaller tasks and those smaller tasks run at the same time. Traditionally each of those tasks run independently up to a common stopping point, then they halt, exchange information, and continue. This global stop-and-communicate step is quite expensive. This project instead pursues a different approach, where individual tasks directly communicate with other tasks asynchronously, without having to wait for a global stopping point. This approach is likely to yield better performance on large-scale computing tasks, specifically on what is becoming the dominant large-scale machine, a heterogeneous machine with many CPUs and other<br/>many-core processors. The project will deliver a set of high-performance, open-source data structures and algorithm implementations to support irregular patterns of communication, notably those that arise in biology, graph analytics, and sparse linear algebra for machine learning. These will not only be directly useful for end users but also demonstrate how to design and engineer primitives for accelerator-equipped distributed-memory machines. The project also engages application developers (both in our groups and externally) to make the outcomes broadly useful.<br/><br/>The project will develop a programming environment for accelerator-based HPC systems that integrates accelerators into a Partitioned Global Address Space (PGAS) model, which will allow direct communication between GPUs in a manner that is well suited to both applications and the underlying hardware. Specifically, GPU programming will be integrated with the UPC++ PGAS programming model (""GPUPC++""). The project will thus advance the state of the art in algorithms, programming models, and low-level support for the heterogeneous large-scale computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828644","MRI:  Acquisition of a Mass Storage System for Data Intelligence Research","CNS","Major Research Instrumentation","10/01/2018","09/08/2018","Hao Ji","CA","Cal Poly Pomona Foundation, Inc.","Standard Grant","Rita Rodriguez","09/30/2021","$212,680.00","Hui Shi","hji@cpp.edu","3801 West Temple, Bldg 55","Pomona","CA","917682557","9098692948","CSE","1189","1189","$0.00","Data intelligence research on turning data into actionable intelligence and insights is at the heart of many disciplines, including computer science, mathematics, chemistry, business, and biology. This research instrumentation project involves the acquisition of a mass storage system to enhance the storage capacity of a High-Performance Computing (HPC) cluster.  This will enable researchers at the awardee and neighboring institutions to conduct studies in: scalable computational algorithms for big data analysis; finding answers to customized questions by reasoning over large-scale unstructured data; the development of deep learning techniques for two- and three-dimensional biological and medical image analysis; and other big data research challenges.   The instrumentation also creates an opportunity for students to participate in a variety of research and training activities related to data intelligence.<br/><br/>The project provides better storage capacity to process and share massive data. The system enables data intelligence research and training activities by providing flexible storage configurations, data preservation and recovery, high-performance, large-scale data operations, and secure data sharing for a broad range of projects. It supports collaboration across various universities in California, and enables faculty and students to develop modern scalable solutions to data intelligence problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751471","CAREER: Multiscale Modeling of Peptide Self-Assembly with Experiment Directed Simulation","CBET","Proc Sys, Reac Eng & Mol Therm","06/01/2018","05/30/2019","Andrew White","NY","University of Rochester","Standard Grant","Raymond Adomaitis","05/31/2023","$508,610.00","","andrew.white@rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","ENG","1403","1045, 9251","$0.00","PI: White, Andrew D. <br/>Proposal Number: 1751471 <br/>Institution: University of Rochester<br/>Title: CAREER: Multiscale Modeling of Peptide Self-Assembly with Experiment Directed Simulation<br/><br/><br/><br/>Self-assembly is the spontaneous organization of molecules into a supramolecular complex or new phase without the formation of chemical bonds. This process describes a wide range of phenomena from protein folding, to liquid crystal orientation, to polymer nanocomposite formation. Modeling of peptide self-assembly has been driven by applications in materials science and structural biology. In structural biology, research is motivated by the many disease causing peptides that self-assemble into toxic structures. The main objective of this proposal is the development of a framework for multiscale modeling of peptide assembly, in which molecular simulations are corrected using experimental data. The proposed research combines state-of-the art computer simulation techniques with the novel capability of using experimental data as an extra input to simulations to improve their accuracy.<br/><br/>Self-assembly is one of the most challenging problems for molecular modeling and simulation because it spans multiple length-scales and often multiple time-scales. Coarse-grain techniques which group atoms together in order to simulate larger-length scale interactions suffer from poor agreement with reference experiments and lack rigorous theory for correcting these discrepancies. This proposal aims to address these shortcomings by minimally biasing simulations to correct discrepancies between simulation predictions and experimental data.  The proposed new methodology is supported by preliminary data demonstrating improved accuracy and efficiency. These improvements allow a large number of distinct systems to be simulated and guarantee consistency with proposed parallel laboratory experiments. The increase in fidelity and simulation number may lead to the development of deep-learning models that will allow de novo design of self-assembling structures. The main objective of the proposal is to use this biasing technique along with established multiscale simulation methods to study multiple self-assembling peptides, with a particular emphasis on Amyloid beta peptide which is the toxic agent responsible for Alzheimer's disease. The overall scientific objective of the proposed research is to better understand the molecular details of the interplay between entropy, molecular structure and self-assembly through the use of computer simulations. Integration of research and education will involve the development of web-based applications for teaching undergraduate students about coarse-graining, experiment-directed simulation, and self-assembly and a virtual reality workshop to high school students participating in the University of Rochester's Kearns Center mentorship program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811323","I-Corps: Coordinates and Volumetrics in MRI Imaging","IIP","I-Corps","01/01/2018","01/23/2020","Nidhal Bouaynaya","NJ","Rowan University","Standard Grant","Pamela McCauley","08/31/2020","$50,000.00","","bouaynaya@rowan.edu","Office of Sponsored Programs","Glassboro","NJ","080281701","8562564057","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to fill a critical need in neuro-radiology, neuro-oncology, and radiation therapy, by developing a clinic-ready software suite that computes physician-approved volumes of key three-dimensional (3D) structures in the brain from Magnetic Resonance Imaging (MRI). Currently, physicians make a decision on tumor status by visual inspection and by measuring the largest perpendicular diameters from a single two-dimensional (2D) axial image. Radiation oncologists use manual segmentation to delineate the boundaries of the tumor regions; these contours are used for treatment planning. Current practice is associated with significant limitations that impede the quality of patient care. This software suite will potentially change the status-quo by generating 3D volumetric structures of tumors and other key regions in the brain. The physician can readily apply this information to better understand the disease and optimize care leading to early treatment, less morbidity, and longer survival times.<br/><br/>This I-Corps project produces a practical, clinic-ready suite of Magnetic Resonance (MR) analyses and display tools to solve the number one impediment to reliable use of MRIs for guiding radiation treatment of brain cancer: accurate and timely multimodal 3D segmentation of key structures in the brain. The main engine of the MR suite is a mathematical optimization framework that combines calculus of variation with deep learning techniques; thus amenable to pixel-level-accurate 3D segmentation in almost real-time. The proposed technology was developed from an algorithm for multimodal brain segmentation, which consists of (a) an automated, accurate and robust algorithm for 3D image segmentation, combined with (b) semi-automated and interactive multimodal labeling that requires physician approval."
"1752255","CAREER: Next-Generation Neural-Machine Interfaces for Electromyography-Controlled Neurorehabilitation","CBET","Disability & Rehab Engineering","04/01/2018","03/26/2018","Xiaorong Zhang","CA","San Francisco State University","Continuing Grant","Aleksandr Simonian","03/31/2023","$441,501.00","","xrzhang@sfsu.edu","1600 Holloway Ave","San Francisco","CA","941321722","4153387090","ENG","5342","1045, 9102, 9229","$0.00","The lives of millions of patients worldwide are severely impacted by upper extremity loss or impairment. An emerging technology, electromyography (EMG)-based Neural Machine Interface (NMI), offers enormous potential in the restoration of function through neuroprosthetics for this population, including amputees, stroke survivors, and cerebral palsy patients. The technology senses bioelectrical signals from muscles, interprets them to identify the intended movement of the patient, and makes decisions to control neurorehabilitation applications (e.g., a prosthetic limb). While neurorehabilitation system design has progressed remarkably over several decades, no system is currently capable of meeting all desired technical specifications for commercial and clinical implementation. This project takes a computer engineering approach toward improving EMG-based NMI technology functionality and robustness. Software will be developed for managing the sensor status and real-time responses, and novel computing platforms will be implemented to handle the large-scale, data-intensive computations required for responsive neurorehabilitation applications. The project integrates research and education through several avenues: enhancement of undergraduate curricula with embedded research experiences, development of a massive open online course on neural machine interface, and initiation of a K-12 through community college outreach program. <br/><br/>The PI's long-term career goal is to develop next-generation NMIs that will connect people and enable the exploration of big data and deep learning technologies in neurorehabilitation research. Toward this goal, the project's objectives are to (1) develop new hardware and software methods to enable the use of high-density grid sensing technology in real-time EMG-based NMIs to improve the functionality and robustness of the NMIs and (2) develop new computing technologies so that computing power and storage capacity are no longer barriers to the advancement of NMI neurorehabilitation research. The PI will first address the challenge of applying high-density EMG grids to real-time NMIs by employing a Grid Status Awareness and Response Engine to closely monitor the status of the EMD grids and respond accordingly. The issue of computational burden posed by the high-density EMG grids will then be tackled through the development of a neuromorphic computing system. Finally, a hierarchical computing platform that provides sufficient computational and storage capabilities to enable real-time response and portability will be developed. Insights and advancements made here in EMG-based NMI design will markedly improve reliability and functionality of EMG-controlled neurorehabilitation systems. Additionally, the developed NMI methods and tools are applicable to research fields beyond neurorehabilitation applications, such as brain-computer interfaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747845","SBIR Phase I: Comprehensive Online Platform to Enhance Care Coordination and Improve Behavioral Health Outcomes","IIP","SBIR Phase I","01/01/2018","06/12/2019","Anupam Khandelwal","CA","SageSurfer Inc","Standard Grant","Henry Ahn","06/30/2019","$225,000.00","","anupam@sagesurfer.com","651 Pinot Blanc Way","Fremont","CA","945398014","5103531045","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to increase treatment options and to improve health outcomes for patients using a comprehensive algorithm-based behavior health intervention. It can address Institute of Health Improvement's Triple AIM to improve population health, improve patients' experience of care, and reduce per capita healthcare cost. The proposed platform is aimed at optimizing treatment outcomes by enhancing motivations, maximizing treatment adherence, engaging families and natural supports, and supporting self-care management for individuals with chronic/serious behavioral health conditions. The proposed technology is expected to enable patients to receive an ancillary, active treatment that keeps them functioning in daily life. By enhancing the clinical experience for patients and clinicians, individuals with behavioral health conditions can self-monitor and communicate their experiences to their clinician and record them in real time, allowing monitoring and assessment by the behavioral health provider.   <br/><br/>The proposed project will develop and test a novel behavior health technology intervention on improving patients? ability to feel in control of their treatment and improve their well-being.  The technology is expected to enable ongoing, real-time feedback to the both the patient's personal and professional care team to monitor the impact of enabling/impending factors on treatment progress to continually refine the care plan and interventions so that optimal results can be achieved. The platform consists of a secure mobile and web platform used to provide proactive interventions, advanced algorithms used to personalize care and provide insights to the behavioral health provider. The team plans to assess whether the combination of personal support effectively in the patients' care together with advanced correlations using deep learning can help in predicting interventions including crisis episode, improve patient's treatment planning and wellness."
"1820076","SBIR Phase I:  Enhancing the Performance of Scientific Applications Through Intelligent Advice","IIP","SBIR Phase I","07/01/2018","06/19/2018","Brandon Nesterenko","CO","Crestone Computing LLC","Standard Grant","Peter Atherton","06/30/2019","$225,000.00","","bnesterenko09@gmail.com","19415 Lincoln Green Ln","Monument","CO","801328738","7192387076","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to allow new hardware platforms to be more quickly deployed and useful software products to be more readily made available on a wide variety of hardware to satisfy the needs of their users.  Additionally, researchers in science and engineering can reduce the runtime of their applications by hours or days on the machines inside high performance computing or data centers, enabling more scientific modeling and simulations to be completed within these centers, while allowing each individual scientist to concentrate more on his/her research, which in turn may more quickly benefit society.  For example, in the defense industry, many contracting companies develop Monte Carlo simulation software to predict outcomes from disasters. Increasing the throughput of these simulations would allow for more simulation results to be analyzed and tested, thereby resulting in better predictions and handling of disastrous situations. <br/><br/>This Small Business Innovation Research Phase I project is unique in its automated connection between existing performance modeling and prediction tools, and pattern-driven compiler optimization. Novel runtime monitoring and modeling techniques will be developed to automatically map performance bottlenecks discovered by existing performance analysis tools to potential opportunities of source code optimizations. Such opportunities again will be used to guide pattern-driven compiler optimizations, particularly to enhance the performance of finite element methods on both GPGPUs and multi-core/many-core CPUs. Deep learning neural networks will be used to automate the runtime behavior classification of computations. The new pattern-driven specialization of compiler optimizations will enable general-purpose compiler techniques to be aware of the higher-level semantics of library abstractions (e.g., data structures and algorithm abstractions) and allow them to be collectively customized and coordinated to attain the best performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818679","Broadening Participation Research Project: STEP into STEM, Investigating Successful Transitions and Effective Pathways into STEM","HRD","Hist Black Colleges and Univ","07/01/2018","07/09/2020","Gregory Monty","NC","North Carolina Agricultural & Technical State University","Standard Grant","Earnestine Psalmonds","06/30/2022","$349,998.00","Marwan Bikdash, Caroline Booth, Dukka KC","ghmonty@ncat.edu","1601 E. Market Street","Greensboro","NC","274110001","3363347995","EHR","1594","8212","$0.00","Researchers at North Carolina A&T State University will conduct a longitudinal study using big data analytics to advance the knowledge and evidence base for successful strategies to encourage underrepresented minorities to pursue STEM pathways. The goals of the project are to characterize and understand pathways to STEM, assess STEM intervention effectiveness at universities across the nation based on institutional system/program metrics for student success, and recommend ways to increase the number of underrepresented minority women pursuing STEM degrees and careers. The study will examine factors that influence transitions to STEM for undecided/non-STEM students and demonstrate specific systematic/programmatic interventions that are effective. The research will be guided by a triangulation of social cognitive career theory, student persistence theory, human capital theory, and vocational anticipatory socialization theory. The project is an innovative approach to better understanding factors that motivate students to elect STEM degree programs, and it is expected to develop a core set of best practices metrics for STEM transitions and pathways. <br/><br/>The primary data sources for the study are the High School Longitudinal Survey and the Beginning Postsecondary Students Survey. The data will be analyzed to examine student pathways into STEM, with a focus on students who are undecided about their majors upon entering community college and four-year degree programs.  The specific research questions are (1) what are the academic, socioeconomic, and demographic supports and barriers for underrepresented minorities (URM)and undecided/non-STEM major transitions to STEM?; (2) what effective pathways and higher education institutional systems/programs support URMs, undecided/non-STEM successful transitions to STEM?; (3) which institutions have data-based evidence to support their best practice intervention strategies?; (4) what implementable strategies will support successful transitions and effective pathways into STEM, particularly for URMs and undecided/non-STEM students from minority serving institutions?; and (5) what shared metrics are recommended for institutions to use nationally to assess successful transitions and effective pathways for URMs into STEM?  Analytical techniques will include Random Forest, Principal Component Analysis, XGBoost, and Deep Learning to analyze the large datasets and conduct the meta studies on the variables. The study results could impact policy and practice at higher education institutions at large.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818772","Partial Differential Equation Constrained Optimization:  Algorithms, Numerics, and Applications","DMS","COMPUTATIONAL MATHEMATICS","08/01/2018","07/26/2018","Harbir Antil","VA","George Mason University","Standard Grant","Leland Jameson","07/31/2021","$199,999.00","","hantil@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1271","9263","$0.00","The state of the art in modeling of the physical processes in science and engineering confronts us with solving increasingly complex optimization problems in the presence of constraints.  Think of designing the wing of an aircraft where it is essential that it can withstand a tremendous amount of pressure. This is just one of the example of an optimization problem with constrains. Such problems are inherently nonlinear due to constraints. This makes the development and analysis of algorithms and numerical techniques for the solution of these problems extremely challenging but rewarding. This project will create new optimization algorithms that allows us to incorporate constraints in a systematic and robust fashion. The targeted applications range from next generation micro- and nano-scale lab-on-chip devices to novel material and structural designs. Open source software will be created so that the research can be easily used by scientists working in other research areas.<br/> <br/>The applications under consideration can be modeled using partial differential equations (PDEs). These PDEs are  nonlocal (fractional); nonsmooth (contact problems); geometric, nonlinear, multiscale with an unknown domain, i.e. free boundary problems (FBPs). This project focuses on optimization problems with such PDE constraints - the so-called PDE constrained optimization problems. It aims to create new optimization schemes that will enable the solution of currently intractable optimization problems with nonsmooth features, including: optimization problems with nonlinear inequality constraints, contact problems, and risk-averse PDE constrained optimization. The resulting optimization algorithms will provide new insights into nonconvex nonsmooth problems. Optimization problems with surface tension is a new research field with great potential to enhance our understanding at the micro and nano-scales where surface effects dominate bulk effects. This will impact the design of next generation lab-on-chip, forensics and liquid lenses in astronomical telescopes. Open source software will be developed, which will not only benefit scientists in optimization, FBPs, and nonlocal problems but also scientists in nonlinear PDEs and data driven optimization problems. The results will be disseminated via two special topics courses on (i) PDE constrained optimization under uncertainty; (ii) Deep learning and PDE constrained optimization. One female student will get her PhD.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839900","CICI: SSC: Integrity Introspection for Scientific Workflows (IRIS)","OAC","Cybersecurity Innovation","09/01/2018","04/20/2020","Anirban Mandal","NC","University of North Carolina at Chapel Hill","Standard Grant","Robert Beverly","08/31/2021","$999,575.00","Ewa Deelman, Von Welch, Yufeng Xin, Cong Wang","anirban@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8027","","$0.00","Scientists use computer systems to analyze and store their scientific data, sometimes in a complex process across multiple machines in different geographical locations. It has been observed that sometimes during this complex process, scientific data is unintentionally modified or accidentally tampered with, with errors going undetected and corrupt data becoming part of the scientific record. The IRIS project tackles the problem of detecting and diagnosing these unintentional data errors that might occur during the scientific processing workflow. The approach is to collect data relevant to the correctness and integrity of the scientific data from various parts of the computing and network system involved in the processing, and to analyze the collected data using machine learning techniques to uncover errors in the scientific data processing. The solutions are integrated into Pegasus, a popular ""workflow management system"" - a software used to describe the complex process in a user-friendly way and that handles the details of processing for the scientists. The research methods will be validated on national computing resources with exemplar scientific applications from gravitational-wave physics, earthquake science, and bioinformatics. These solutions will allow scientists, and our society, to be more confident of scientific findings based on collected data.<br/><br/>Data-driven science workflows often suffer from unintentional data integrity errors when executing on distributed national cyberinfrastructure (CI). However, today, there is a lack of tools that can collect and analyze integrity-relevant data from workflows and thus, many of these errors go undetected jeopardizing the validity of scientific results. The goal of the IRIS project is to automatically detect, diagnose, and pinpoint the source of unintentional integrity anomalies in scientific workflows executing on distributed CI. The approach is to develop an appropriate threat model and incorporate it in an integrity analysis framework that collects workflow and infrastructure data and uses machine learning (ML) algorithms to perform the needed analysis. The framework is powered by novel ML-based methods developed through experimentation in a controlled testbed and validated in and made broadly available on NSF production CI. The solutions will be integrated into the Pegasus workflow management system, which is used by a wide variety of scientific domains. An important part of the project is the engagement with science application partners in gravitational-wave physics, earthquake science, and bioinformatics to deploy the analysis framework for their workflows, and to iteratively fine tune the threat models, ML model training, and ML model validation in a feedback loop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828315","MRI: Acquisition of a High Performance Computer for Computational Science at UC Santa Cruz","AST","Major Research Instrumentation","10/01/2018","08/16/2018","Brant Robertson","CA","University of California-Santa Cruz","Standard Grant","Zoran Ninkov","09/30/2021","$1,547,000.00","Nicholas Brummell, Piero Madau, Enrico Ramirez-Ruiz, Nicole Feldl","brant@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","MPS","1189","1206, 7697","$0.00","This proposal would fund a 500+TeraFLOP supercomputer for the University of California at Santa Cruz (UCSC).  300+ scientists and students will use it to study astrophysics, materials science, chemistry, applied math, computer science and climate science.  Astrophysicists will study colliding neutron stars for follow-up of LIGO events (NSF Big Idea:  Multi-messenger Astrophysics) and many other topics.  Materials scientists will use many body perturbation theory to study 2D materials as quantum emitters (NSF Big Idea: Quantum Leap).  Chemists will study reactions at liquid interfaces which are important in the energy and pharmaceutical industries.  Climate scientists will study feedbacks that may cause the rapidly warming Arctic (NSF Big Idea: Navigating the New Arctic).<br/><br/>High performance computing in the physical sciences has emerged as a critical method for achieving astounding breakthroughs.  State of the art computational resources have been instrumental in making the University of California, Santa Cruz (UCSC) one of the world's leading centers for astrophysical research.  Through this NSF MRI grant, University of California, Santa Cruz will expand the intellectual merit of its astrophysics research programs by acquiring a supercomputer to enable transformative studies in galaxy formation, astrophysical transients and gravitational wave sources, exoplanet formation and atmospheres, and cosmological structure formation. Computational research at UCSC in planetary science, applied mathematics, chemistry, and materials science will also be performed on the system. The supercomputer will create broader impacts on UCSC undergraduate and graduate training as a hands-on platform for teaching high performance computing and deep learning methodologies, with a focus on increasing the participation of women and underrepresented minorities in the field of computational astrophysics. This program will continue the success of previous NSF MRI-funded computational facilities, which have helped to train dozens of diverse members of the UCSC community in transferable research skills and have supported more than 300 users generate hundreds of refereed scientific publications that have garnered thousands of citations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758546","SBIR Phase II:  Energy-Efficient Perception for Autonomous Road Vehicles","IIP","SBIR Phase II","04/01/2018","09/19/2019","Forrest Iandola","CA","DeepScale, Inc","Standard Grant","Peter Atherton","10/31/2019","$760,000.00","","forrest@deepscale.ai","1232 Royal Crest Dr","San Jose","CA","951312912","6502000082","ENG","5373","165E, 5373, 8032, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be to allow more consumers to make use of assisted and autonomous driving systems in automobiles. Fully Autonomous Vehicles (AV) will reduce traffic collisions and enable humans to spend less time driving and more time on productive activities. Commercially deploying AVs requires a number of key technologies including sensing, perceptual systems, motion planning, and actuation. Our discussions with leaders and decisionmakers at automotive companies have shown that the development of robust, accurate, and energy-efficient perception systems is a major technical obstacle to creating mass-producible autonomous road vehicles. Of particular interest to automakers is scaling down the computational requirements of perceptual systems while preserving high levels of accuracy and robustness.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project will use deep learning to create perception systems that are (a) scalable across different computational platforms and (b) scalable across smaller or larger sensor sets. Specifically, the company will develop scalable systems from small compute platforms (used for Highly Automated Driving) to somewhat larger compute platforms (used for Fully Automated Driving). Further, the company will develop perceptual systems that scale from few sensors to many sensors. The goal is to ""do more with less,"" advancing the pareto-optimal frontier of efficiency-accuracy and price-accuracy tradeoffs. The company has already engaged with automotive OEMs and suppliers to develop partnerships and to define metrics for success in this endeavor.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816313","Mathematical Foundation for Signal Processing on Spatially Distributed Networks","DMS","APPLIED MATHEMATICS","09/01/2018","08/11/2018","Qiyu Sun","FL","The University of Central Florida Board of Trustees","Standard Grant","Victor Roytburd","08/31/2021","$195,231.00","","qiyu.sun@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","MPS","1266","","$0.00","A spatially distributed data network consists of a large number of cooperative agents that have data collecting, processing, and sharing capabilities.  It has been widely used in (wireless) sensor networks, smart power grids, and other engineering and science fields, because it offers some practical advantages over the more traditional form of data network that ships collected data to central processing facilities.  However, distributing the data processing tasks, such as filtering, denoising, and compressing, raises some new challenges.  While these have received much attention in recent years, there still is a gap between mathematical theory and engineering practice.  The investigator works to develop a mathematical foundation for signal processing on spatially distributed data networks that has practical consequences for engineering applications.  Graduate students participate in the research.  Graduate students participate in the research.<br/><br/>The topological structure of a spatially distributed data network is described by a sparse graph.  Data processing on a spatially distributed data network is performed by agents in the network; many of them can be described by well-localized matrices and integral operators, in terms of which the network behavior can in principle be described and optimized globally.  But this could be an impractically large optimization problem.  The principal investigator studies how to split a large network into a set of overlapping smaller ones whose separate global optimizations can be used to approximate the global optimization of the large network.  In this way, he aims to develop a mathematical foundation for signal processing on such networks that has engineering applications.  Here he emphasizes applied harmonic analysis and numerical analysis of well-localized matrices on graphs, design of stable spatially distributed data networks, fast algorithms implemented by agents in spatially distributed data networks, phaseless signal reconstruction, and deep learning coupled to signal processing on graphs.  Graduate students participate in the research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812114","Spectral Properties of Random Matrices","DMS","PROBABILITY","06/01/2018","04/01/2020","Paul Bourgade","NY","New York University","Continuing Grant","Tomek Bartoszynski","05/31/2021","$300,000.00","","bourgade@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","MPS","1263","","$0.00","This project will advance our knowledge of random matrices, a set of operators that serves as a paradigm for many high-dimensional systems. While the Gaussian distribution emerges universally from probabilistic models exhibiting independence, random matrix statistics appear from correlated systems, so that the scope of their applications have considerably expanded recently. For example, integrable systems, growth models, analytic number theory and quantum mechanics exhibit random matrix statistics. Moreover, random matrix techniques have improved our theoretical understanding of numerical analysis and deep learning networks. <br/><br/>Methods from probability (random walks and coupling), analysis (homogenization) and mathematical physics (loop equations) were recently advanced in connection with spectral analysis of random matrices, to prove eigenvalues and eigenvectors universality. The PI will further develop such tools for the following problems: <br/>(1) Delocalization for band matrices. The PI aims at understanding universality beyond mean field models. One key question concerns band matrices in dimension 1 up to the ``Anderson'' transition. A recent approach to this problem involves a mean field reduction and the connection with quantum unique ergodicity, and it will be developed further; <br/>(2) Universality of extremal statistics.  This includes extreme spectral spacings, large values of the characteristic polynomial, fluctuations of individual eigenvalues and more generally universal aspects of log-correlated random fields;<br/>(3) Coulomb gases, the Gaussian free field and Gaussian multiplicative chaos. Recent progress gave convergence of the electric field of 2D Coulomb gases to the Gaussian free field. A natural extension concerns convergence of the characteristic polynomial to Gaussian multiplicative chaos;<br/>(4) Eigenvectors of non-Hermitian random matrices. Physicists predicted in the 1990s the typical size of overlaps between eigenvectors from the Ginibre ensemble. This was recently upgraded to an explicit limiting distribution of diagonal overlaps, and to correlations of off-diagonal overlaps. Extension of this new integrability to the joint law of all overlaps will be studied.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848815","Multimessenger Astrophysics: Real-time Discovery at Scale","PHY","NUCLEAR THEORY, COMPUTATIONAL PHYSICS","09/01/2018","08/11/2018","Eliu Huerta Escudero","IL","University of Illinois at Urbana-Champaign","Standard Grant","Bogdan Mihaila","08/31/2019","$45,000.00","Daniel Katz","elihu@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","1285, 7244","026Z, 062Z, 069Z","$0.00","This award will provide support for participants to attend ""Deep Learning for Multi-Messenger Astrophysics (MMA): Real-Time Discovery at Scale"", a three-day workshop that will be held October 17-19, 2018 at the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign. The purpose of the workshop is to catalyze the creation of a community whose expertise is at the interface of two of NSF's 10 Big Ideas: Harnessing the Data Revolution and Windows on the Universe. Domain experts attending this event will inform the design of a roadmap of future collaborations to accelerate convergence between these two disparate frontiers of science and technology to fully realize MMA.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755761","CRII: III: Computational Methods to Explore the Role of Post-transcriptional Regulation in Cancer","IIS","Info Integration & Informatics","07/01/2018","02/01/2019","Wei Zhang","FL","The University of Central Florida Board of Trustees","Standard Grant","Sylvia Spengler","06/30/2021","$187,021.00","","wzhang.cs@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7364","7364, 8228, 9251","$0.00","This project will develop computational methods to investigate the role of post-transcriptional regulation in cancer patients using both high-throughput sequencing data and molecular networks to improve the understanding of cancer and assess the cause of the disease in a patient. Deregulation of gene expression is a hallmark of the human tumor cells. Post-transcriptional regulation is a pervasive mechanism in the relation of most human genes; its implication in cancer is only beginning to be appreciated. The central hypothesis underlying this project is that by considering post-transcriptional regulation events, estimated protein expressions can provide more accurate molecular signatures to detect complex disease mechanisms, compared to mRNA expressions. This project will study post-transcriptional regulation, with the goal of generating computational methods to predict the changes of protein expression level without doing large-scale proteomics experiments. The outcomes of the proposed research will lower the barriers for interacting with analyzing high-dimensional genomic profiles and cut time and costs spent on biomedical research. The new methods will enable biologists and biomedical researchers to perform comprehensive analysis with high-throughput sequencing data and biological networks together to investigate the impact of post-transcriptional regulation in certain cancer types.<br/><br/>This project to use a variety of sequences modalities has three phases: 1) Develop advanced machine learning methods to identify the genome-wide alternative polyadenylation (APA) events in the 3' untranslated region (3'UTR) of the mRNAs between cancer patients and matched normal samples with RNA-Seq data. 2) Develop biologically motivated graph-based learning models and efficient scalable algorithms to estimate the protein expression levels with microRNA-mRNA interaction networks and 3'UTR-APA events. 3) Investigate the prognostic power of the identified APA and estimated protein expressions compared to the canonical genomic features such as mRNA expression, mutation, and copy number variations. The project not only employs interpretable computational models which capture the causal relations in post-transcriptional regulation of protein-coding genes to better understand complex human diseases, but also develops unique machine learning methods and optimization techniques for computer science research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763814","SHF: Medium: Collaborative Research: Program Analytics: Using Trace Data for Localization, Explanation and Synthesis","CCF","Software & Hardware Foundation","06/15/2018","03/31/2020","Ranjit Jhala","CA","University of California-San Diego","Continuing Grant","Sol Greenspan","05/31/2022","$659,469.00","Sorin Lerner, Kamalika Chaudhuri","jhala@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7798, 7924, 7944","$0.00","Formal program analyses have long held out the promise of lowering the cost of<br/>creating, maintaining and evolving programs. However, many crucial analysis<br/>tasks, such as localizing the sources of errors or suggesting code repairs, are<br/>inherently ambiguous: there is no unique right answer. This ambiguity<br/>fundamentally restricts the wider adoption of formal tools by limiting users to<br/>those with enough expertise to effectively use such ambiguous results. The key<br/>insight is that data-driven machine-learning approaches, which have proved<br/>successful in other domains, can be applied to the data traces generated by<br/>programmers as they carry out development tasks. This research addresses the<br/>challenge of ambiguity by extending classical program analysis into modern<br/>program analytics. This extension enhances classical symbolic methods with<br/>modern data-driven approaches to collectively learn from fine-grained traces of<br/>programmers interacting with compilers or analysis tools to iteratively modify<br/>and fix software.<br/><br/>The research systematically develops program analytics by pursuing research<br/>along two dimensions: language domains and programming tasks. First, it studies<br/>different language domains, from dynamically typed languages (Python), to<br/>statically typed functional languages with contract systems (Haskell), to<br/>interactive proof assistants (Coq). Second, it targets different programming<br/>tasks, from localizing errors like null-dereferences, assertions or other<br/>dynamic type failures, to static type errors, to completing or fixing code to<br/>eliminate an error or to obtain some desired functionality. This approach takes<br/>advantage of a suite of new approaches that harness recent advances in<br/>statistical machine learning and fine-grained, domain specific programmer<br/>interactions. These advantages allow the research to address the fundamental<br/>problem of ambiguity in classical program analysis. This has potential to<br/>transform software development by yielding a new generation of program analysis<br/>tools that are efficient, applicable, and automatically customizable (e.g., to a<br/>particular company, project, group or even individual).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816449","Collaborative Research: Second-Order Variational Analysis in Structured Optimization and Algorithms with Applications","DMS","APPLIED MATHEMATICS","09/01/2018","08/07/2018","Jose Yunier Bello Cruz","IL","Northern Illinois University","Standard Grant","Pedro Embid","08/31/2021","$99,143.00","","yunierbello@niu.edu","301 Lowden Hall","De Kalb","IL","601152828","8157531581","MPS","1266","062Z","$0.00","This project focuses on developing advanced tools of mathematical analysis to investigate modern structured optimization problems and building efficient algorithms to solve them. These problems arise in different areas of science and engineering, including massive data analysis, machine learning, signal processing, medical image reconstruction, statistics, traffic and logistical networks, and operations research. Most of them share the irregular phenomenon of nonsmoothness or nonconvexity that challenges computation. Despite several practically successful algorithms recently proposed to solve such problems, the underlying fundamental theory is not quite understood and explored. Only analyzing the complexity and the deep mathematics behind these problems and algorithms provides practitioners across related, vital science and engineering areas new tools to comprehend their core features, be able to design more efficient algorithms, and attack more challenging problems arising from practice. The investigators develop such tools via a novel approach from a relatively young subfield of applied mathematics, variational analysis, which is naturally compatible with these nonsmooth and complex structures. Several topics from this project are integrated with teaching topic courses and training of students.<br/><br/>     This project is devoted to developing the theory of second-order variational analysis (SOVA) and using it to study the stability, sensitivity, and computational complexity of algorithms for solving structured optimization problems. The first part of this project serves as the theoretical foundation; it concerns the theory of SOVA with connections to stability and sensitivity analysis. More specifically, the investigators intend to study: (i) tilt stability and full stability for general optimization problems with connections to Robinson's strong regularity and Kojima's strong stability for conic programming via SOVA; (ii) metric (sub)regularity of the subdifferential and Kurdyka-Lojasiewicz property on nonsmooth (possibly nonconvex) functions via SOVA; and (iii) stability for parametric variational systems including Nash equilibrium systems and variational inequalities via SOVA. The second part of this project consists of designing and analyzing proximal algorithms for solving convex and nonconvex structured problems. Immediate applications include Lasso, group Lasso, elastic net, basic pursuit, sparsity, low-rank problems, and completion matrix problems that originate from compressed sensing, image reconstruction, machine learning, and data science. Stability theory developed in the first part plays a significant role here, especially in the complexity analysis of these algorithms. It explains why the development of many recent proximal algorithms is strongly influenced by the hidden power of SOVA. The specific objectives of this part are: (i) to accelerate the forward-backward splitting method and analyze the phenomenon of linear convergence encountered frequently in numerical experiments; and (ii) to design efficient methods of Douglas-Rachford splitting type for solving nonconvex optimization and feasibility problems. Other important applications include inverse problems corrupted by Poisson noise and total variation denoising models, both of which are well recognized in imaging science and statistical learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807814","Spin and Charge Dynamics: Competing Orders and Quasi-Particle Formation","DMR","CONDENSED MATTER & MAT THEORY","08/01/2018","07/24/2018","Adrian Feiguin","MA","Northeastern University","Standard Grant","Daryl Hess","07/31/2021","$330,000.00","","adrianfeiguin@gmail.com","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","MPS","1765","053Z, 7569, 8084, 9216","$0.00","NONTECHNICAL SUMMARY<br/><br/>Characterizing the properties of quantum materials and their different quantum phases is paramount in order to be able to develop new alternative technologies beyond the semiconductor paradigm for applications in energy conversion, electronic devices, and light harvesting. This award supports research and education that will not only have technological implications but will also tackle fundamental questions in physics that could lead to unanticipated discoveries.<br/><br/>The quest for materials with novel functionality is driven by the study of quantum systems where interactions among the constituent electrons are very strong. These interactions can give rise to complex and intriguing phenomena that take place when the physics is mostly governed by the collective behavior of the electrons inside the material.  One particular example occurs when considering electrons confined in low spatial dimensions: the confinement forces electrons to move by ""pushing"" instead of passing around each other. Same as oscillations of a string, the natural excitations of such a confined system of electrons can be better understood in terms of waves. As a consequence, electrons lose their individual identities as particles. This project focuses on understanding how electrons lose or recover their identities as individual excitations and how their behavior is subsequently transformed, giving rise to different phases in the material.<br/><br/>Even though these problems are theoretically very challenging due to their underlying complexity, they are amenable to numerical methods. The focus of the research is computational in nature and will also involve the development of new innovative algorithms based on quantum information and machine-learning ideas. New tools for scientific discovery will be developed and will be made available to the community as open-source software that may find applications in other disciplines beyond condensed matter physics.<br/><br/><br/>TECHNICAL SUMMARY<br/><br/>This award will support research and education focused on tackling urgent questions concerning different instabilities in strongly correlated materials due to the presence of antiferromagnetic long-range order and van Hove singularities. This project tackles a paradigmatic problem in condensed matter from a new perspective. In order to overcome the limitations of the Mermin-Wagner theorem, chains and ladders will be studied in the presence of long-range interactions that can realize actual spontaneous symmetry breaking and true antiferromagnetic order. The research will address questions of a very fundamental nature: i) How do spinons evolve into magnons and what are their signatures in the excitation spectrum? ii) Is it possible to realize fractionalized excitations and coherent excitations simultaneously in different regions of the Brillouin zone? iii) How do coherent quasiparticles form (if at all), in the presence of long-range order? Will we still observe signatures of spin-charge separation in the spectrum? iv) What is the interplay between long-range antiferromagnetic order and pairing? v) What terms in the Hamiltonian would give rise to competing instabilities such as those observed in superconducting cuprates? The method of choice will be the density matrix renormalization group and its time-dependent variants, which were co-developed by the PI and have had a remarkable success expanding our knowledge of correlation effects to the time domain.<br/><br/>This work is complemented by the development of novel computational approaches to tackle higher-dimensional systems based on new original ideas that bridge quantum information and machine learning: i) The use of lapped orthogonal orbitals that interpolate between real- and momentum-space representations, ii) Quantum disentanglers and natural orbitals to construct the wave functions of the elementary quasiparticle excitations, and iii) Using restricted Boltzmann machines to calculate the single-particle Green's functions, and therefore reconstruct the entire excitation spectrum of the quantum many-body problem. These new tools will be made available to the community as open-source software that may find applications in other disciplines beyond condensed matter physics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830225","ATD: Algorithm, Analysis, and Prediction for Nonlinear and Non-Stationary Signals via Data-Driven Iterative Filtering Methods","DMS","ATD-Algorithms for Threat Dete, ","09/01/2018","07/11/2019","Hao-Min Zhou","GA","Georgia Tech Research Corporation","Continuing Grant","Leland Jameson","08/31/2021","$200,000.00","","hmzhou@math.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","046Y, S100","6877","$0.00","Have you ever wondered how to separate two conversations recorded at the same time in one room or how to identify irregular patterns in a electrocardiogram reading? In signal processing, those tasks are called de-mixing, which is very challenging, especially when the signals are mixed in a time-varying manner (non-stationary), or when they are not simply added together (nonlinear). The standard methods, such as the algorithms based on classical Fourier analysis or wavelets, can work effectively for linear and stationary signals, but are not as satisfactory for nonlinear or non-stationary signals. Developing algorithms that can handle such signals becomes a timely task that inspires a wave of research in recent years. A new method, named adaptive local iterative filtering (ALIF) is proposed in this project. ALIF can decompose a non-stationary and nonlinear signal into finitely many components, each of which is called an intrinsic mode function (IMF) that reflects the local property at a certain frequency. ALIF is a nonlinear process that can be adaptive according to the input signals, and can separate different local features (frequencies) automatically.  ALIF will also be used together with a machine learning method called factorization machine (FM) to develop a novel signal prediction strategy. It is expected that the ALIF and its prediction algorithms can be used in various applications such as chemical and biological threat detections, ionospheric radio power scintillation in geophysics, data classification and prediction in social media, and financial data analysis.<br/><br/>Nonlinear and non-stationary signals are ubiquitous in real world applications, and they often cannot be handled effectively by the standard algorithms based on Fourier/wavelet transforms, due to the non-linearity and/or their time-varying nature. To capture features, especially the hidden ones, in these signals, it is necessary for the analysis methods to be local, adaptive and stable. The focus of this project has two parts: 1) designing data adaptive algorithms that involve techniques such as iterative filtering for the time frequency analysis; and 2) developing prediction strategies using adaptive iterative filtering techniques in conjunction with the neural networks on nonlinear and non-stationary signals. In the first part, an adaptive local iterative filtering (ALIF) is designed to decompose nonlinear and non-stationary signals, without knowing its instantaneous frequency information in advance. Accompanied with ALIF is a new way, based on dynamical system concepts, to calculate the instantaneous frequencies for decomposed signals. The second part of the project is on a feature prediction strategy, using ALIF together with the factorization machine from neural networks, to learn and predict useful features from noisy signals. In both parts, the mathematical properties, such as convergence and stability of the proposed algorithms, are at the center of studies along with various applications including chemical and biological threat detection, ionospheric radio power scintillation in geophysics, and financial data analysis. The research topic contains a wide range of problems that can be used as projects suitable for undergraduate and graduate education, and postdoctoral scholar training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1903831","Convergence HTF: Workshop on Converging Human and Technological Perspectives in Crowdsourcing Research","IIS","INSPIRE","10/16/2018","11/01/2018","Heng Xu","DC","American University","Standard Grant","Meghan Houghton","08/31/2020","$18,712.00","","xu@american.edu","4400 Massachusetts Avenue, NW","Washington","DC","200168001","2028853440","CSE","8078","060Z, 063Z","$0.00","Intelligent, interactive, and highly networked machines -- with which people increasingly interact -- are a growing part of the landscape, particularly in regard to work.  As automation today moves from the factory floor to knowledge and service occupations, research is needed to reap the benefits in increased productivity and increased job opportunities, and to mitigate social costs.  The workshop supported by this award will promote the convergence of computer science, data management, machine learning, education, and the social and behavioral sciences to define key challenges and research imperatives of the nexus of humans, technology, and work.  Convergence is the deep integration of knowledge, theories, methods, and data from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. This convergence workshop addresses the future of work at the human-technology frontier.<br/><br/>The specific focus of this workshop is on crowdsourcing -- the production of networked knowledge from public participation.  This is a new area of research, gaining attention from researchers who study human-computer interactions, data management, machine learning, human behavior, and business.  This workshop will bring together researchers from these and other relevant communities to (1) synthesize the diverse perspectives found in these different fields, (2) integrate different knowledge, theories and data to create a transdisciplinary and convergent research roadmap, and (3) catalyze new research directions and advance scientific discovery and innovation in crowdsourcing research.  The workshop will also contribute toward broadening participation in this area of research by proactively seeking inclusion of traditionally underrepresented persons."
"1814816","SaTC: CORE: Small: Collaborative: Understanding and Mitigating Adversarial Manipulation of Content Curation Algorithms","CNS","Secure &Trustworthy Cyberspace","07/15/2018","07/05/2018","Damon McCoy","NY","New York University","Standard Grant","Sara Kiesler","06/30/2021","$241,555.00","","dm181@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","8060","025Z, 7434, 7923, 9102","$0.00","Online social networks (OSNs) have fundamentally transformed how billions of people use the Internet. These users are increasingly discovering books, music bands, TV shows, movies, news articles, products, and other content through posts from trusted users that they follow. All major OSNs have deployed content curation algorithms that are designed to increase interaction and act as the ""gatekeepers"" of what users see. While this curation and filtering is useful and necessary given the amount of content available, it has also exposed people and platforms to manipulation attacks whereby bad actors attempt to promote content people would otherwise prefer not to see. This has driven the creation of an underground ecosystem that provides services and techniques tailored towards subverting OSNs' content curation algorithms for economic and ideological gains. This project will conduct open research to improve our understanding of current algorithmic curation attackers. The team will devise content curation algorithms and defenses which are hardened against manipulation and that can be adopted by these OSN platforms, providing a systematic approach to improving design and practice in an area of critical national importance. Technology transfer from this project will protect the integrity of social media discourse from adversarial manipulation. This project will train students with expertise in security and machine learning, areas of broad national need, and produce educational materials to engage both high school students and the public in these critical questions.<br/> <br/>The team will holistically explore the economic, social, and technical perspectives of machine learning-based content curation algorithms' weaknesses. The research comprises three main activities: 1) understand how OSNs are currently being successfully manipulated at large scales, 2) investigate the defenses OSNs have in place, and 3) design more resilient defenses. The team will build the first-ever taxonomy of manipulation services and techniques that are actively used to manipulate curation algorithms. Another thrust of the project is to create a framework for the external evaluation of deployed manipulation defenses based on the collection of both public data from the OSN's platform and external data to compare it against. The team will then develop robust and scalable algorithms to detect OSN manipulation within the collected data. Finally, the team will use the insights from the taxonomy of effective manipulation techniques and the exploration of the limitation of current defenses to design fundamentally resilient content curation algorithms. The project will explore both new curation algorithms and more effective mitigation techniques for existing algorithms. The project's findings will deepen our understanding of social network manipulation and adversarial learning and produce reliable approaches to algorithmic content curation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837821","BIGDATA: IA: Collaborative Research: Data-Driven, Multi-Scale  Design of Liquid Crystals for Wearable Sensors for  Monitoring Human Exposure and Air Quality","IIS","Special Initiatives, Big Data Science &Engineering","09/15/2018","09/06/2018","Nicholas Abbott","NY","Cornell University","Standard Grant","Christina Payne","08/31/2022","$656,531.00","","nabbott@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","1642, 8083","062Z, 8083","$0.00","Liquid crystals are responsive materials that can be used to manufacture low-cost and highly selective chemical sensors. Liquid crystals provide a potentially scalable approach toward deploying millions of wearable chemical sensors (e.g., in mobile phones or attached to clothing) that collect high-resolution data on human exposure to toxic contaminants in the air. This information is key to understanding health-risks associated with air quality, developing industrial practices that minimize workers' exposure to hazardous environments, and detecting point sources (e.g., fabrication of explosives). Liquid crystal sensors work by amplifying events that occur at the molecular-level into an optical signal when the sensor is exposed to a chemical environment. The amplification process involves a sequence of tightly coupled phenomena spanning multiple length and time scales. This span in scales lies beyond what is currently possible to characterize, model, and predict directly from first principles. This project seeks to combine first-principles and data-driven methodologies to overcome this technical challenge. The methods developed will enable the prediction of the influence of liquid crystal design variables on the information content of optical signals and will lead to a revolutionary impact on chemical sensing technologies and on the design of functional materials. The multidisciplinary nature of this project will train a new generation of engineers in the integration of data science into the design and analysis of advanced functional materials. K-12 students and the public will be engaged through development of hands-on liquid crystal sensors that respond to model target chemicals (e.g., carbon dioxide from sodas).<br/><br/>The project will investigate scalable machine learning techniques that enable the efficient use of large sets of experimental and first-principles simulation data to uncover and understand multi-scale phenomena that govern the performance of liquid crystals. Specifically, the project goals are to: i) Investigate the use of density functional theory and molecular dynamics simulations to identify nanoscale descriptors of the underlying spatiotemporal events occurring within and at liquid crystal interfaces (e.g., binding energies), ii) Establish feature extraction techniques to identify suitable macroscale descriptors of liquid crystal optical signals (e.g., optical response times and texture fields), and iii) Develop machine learning techniques that enable the creation of multi-scale models capable of mapping nanoscale and macroscale descriptors. These capabilities will be combined in a reinforcement learning framework that will help guide experimental data collection and identification of innovative liquid crystal system designs. The ultimate engineering goal of the project is to design LC sensors to infer exposure events involving carbon monoxide, ozone, and nitrogen and sulfur oxide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837812","BIGDATA: IA: Collaborative Research: Data-Driven, Multi-Scale Design of Liquid-Crystals for Wearable Sensors for Monitoring Human Exposure and Air Quality","IIS","Special Initiatives, Big Data Science &Engineering","09/15/2018","09/06/2018","Victor Zavala Tejeda","WI","University of Wisconsin-Madison","Standard Grant","Christina Payne","08/31/2022","$1,243,464.00","Emmanouil Mavrikakis, Reid Van Lehn, James Schauer","victor.zavala@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1642, 8083","062Z, 8083, 9102","$0.00","Liquid crystals are responsive materials that can be used to manufacture low-cost and highly selective chemical sensors. Liquid crystals provide a potentially scalable approach toward deploying millions of wearable chemical sensors (e.g., in mobile phones or attached to clothing) that collect high-resolution data on human exposure to toxic contaminants in the air. This information is key to understanding health-risks associated with air quality, developing industrial practices that minimize workers' exposure to hazardous environments, and detecting point sources (e.g., fabrication of explosives). Liquid crystal sensors work by amplifying events that occur at the molecular-level into an optical signal when the sensor is exposed to a chemical environment. The amplification process involves a sequence of tightly coupled phenomena spanning multiple length and time scales. This span in scales lies beyond what is currently possible to characterize, model, and predict directly from first principles. This project seeks to combine first-principles and data-driven methodologies to overcome this technical challenge. The methods developed will enable the prediction of the influence of liquid crystal design variables on the information content of optical signals and will lead to a revolutionary impact on chemical sensing technologies and on the design of functional materials. The multidisciplinary nature of this project will train a new generation of engineers in the integration of data science into the design and analysis of advanced functional materials. K-12 students and the public will be engaged through development of hands-on liquid crystal sensors that respond to model target chemicals (e.g., carbon dioxide from sodas).<br/><br/>The project will investigate scalable machine learning techniques that enable the efficient use of large sets of experimental and first-principles simulation data to uncover and understand multi-scale phenomena that govern the performance of liquid crystals. Specifically, the project goals are to: i) Investigate the use of density functional theory and molecular dynamics simulations to identify nanoscale descriptors of the underlying spatiotemporal events occurring within and at liquid crystal interfaces (e.g., binding energies), ii) Establish feature extraction techniques to identify suitable macroscale descriptors of liquid crystal optical signals (e.g., optical response times and texture fields), and iii) Develop machine learning techniques that enable the creation of multi-scale models capable of mapping nanoscale and macroscale descriptors. These capabilities will be combined in a reinforcement learning framework that will help guide experimental data collection and identification of innovative liquid crystal system designs. The ultimate engineering goal of the project is to design LC sensors to infer exposure events involving carbon monoxide, ozone, and nitrogen and sulfur oxide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746402","SBIR Phase I:  Democratizing Data Science Through Conversation","IIP","SBIR Phase I","01/01/2018","12/27/2017","Aaron Redlich","WI","DataChat Inc.","Standard Grant","Peter Atherton","03/31/2019","$224,928.00","","datachatrnd@gmail.com","1403 University Ave","Madison","WI","537151055","2622989678","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to dramatically improve the human productivity in gathering insights from data, and to democratize data analytics by making it available to a broad class of users within an enterprise. With DataChat, complex analyses can be carried out by simply conversing with a trained chatbot. The proposed approach has the potential to open a new vertical in the analytics market in which chatbots aid humans in carrying out the task of creating, deploying and running complex data science pipelines. This project could lead to the creation of a sub-market in the existing analytic software market, and it could also help improve the productivity of the (non-technology) sectors of the economy that increasingly require high-quality and fast insights from both their archival and real-time datasets.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will take on a number of technical challenges including designing and developing a method to allow programming the underlying program that powers chatbots. Another technical challenge that will be tackled is making it easy to load external data that may not have well-defined schemas. A type inferencing mechanism, and associated set of methods to learn over historical data, will be developed to address this research aspect. Another technical challenge is building good machine learning models, for which a set of mechanisms is proposed that will allow automatic exploration and ranking of machine learning models, aiding the user in picking the right model for the specific task at hand. Overall these technical components will collectively contribute to the different facets of data analysis that are needed to gather insights from data, and will power the overall chatbot approach."
"1838873","SCH: INT: Collaborative Research: Patient Specific Multisite Pacing of Diseased Human Hearts","IIS","Smart and Connected Health","10/01/2018","09/13/2018","Behnaam Aazhang","TX","William Marsh Rice University","Standard Grant","Wendy Nilsen","09/30/2022","$1,214,125.00","Yingyan Lin","aaz@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8018","8018, 8062","$0.00","Cardiovascular disease is the number one cause of death worldwide. For example, heart failure is a significant source of mortality within the U.S; it is responsible for 1% of all emergency room presentations and contributes to one in every nine deaths. A significant proportion of heart failure patients have concurrent conduction system disease, which will lead to their eventual death. Cardiac resynchronization has proven to be a useful therapy for improving cardiac function as well as reducing mortality in some patients. However, a significant number of patients fail to respond to resynchronization therapy, often due to inadequate pacemaker lead placement. There currently exists no pacemaker system that provides the 'potential' benefits of multisite temporally and spatially precise pacing for resynchronization. To reach the audacious goal of eliminating cardiovascular diseases, new technologies must be developed that will monitor the diseased heart with unprecedented temporal and spatial precision and will manage the pacing of the heart to restore a healthy function of the heart. This project will process data recorded from multiple sites and will generate a pacing therapy specific to the patient in real-time.<br/><br/>The award revisits the core foundation of pacemakers to develop temporally and spatially precise pacing at multiple sites for resynchronization. The researchers will develop a robust, well-annotated database of intra-cardiac electrograms (IEGM) from multiple cardiac sites. The focus will be on identifying challenging cases that are clinically difficult to differentiate and, thus, stand to reap the greatest benefit of being able to direct overall algorithm development. This information will be added to the associated metadata file. Data will be collected from a minimum of 150 patients with at least 50 patients from each identified pathophysiology. Pathology of each patient based on data from multiple intra-cardiac recording sites will be identified. The proposed machine-learning pipeline explores the representation of time series using wavelets and then learns transformations of multiple time series using the Lie group framework. This pipeline clusters time-series data to identify the right pathology for the specific patient. In addition, the project explores implementation as an application specific integrated circuit (ASIC) that will be implanted subcutaneously to continuously process intracardiac multi-site recordings as well as to generate temporally and spatially precise pacing patterns. The overall approach is to holistically develop a methodology that can address an extremely low-power implementation of machine learning and signal processing algorithms, by not only combining, but jointly optimizing, algorithmic-, circuit- and  architecture-level innovations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755871","CRII: RI: Using Large-Scale Neuroanatomy Datasets to Quantify the Mesoscale Architecture of the Brain","IIS","Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","09/01/2018","08/16/2018","Eva Dyer","GA","Georgia Tech Research Corporation","Standard Grant","Kenneth Whang","09/30/2020","$175,000.00","","evadyer@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495, 8624","7495, 8089, 8091, 8228","$0.00","Methods for revealing the global connections of the brain typically start by tracing a small number of neurons at a time. It is through performing many experiments, in different brain areas, and across many brains, that information can be aggregated and consolidated to produce detailed maps of the brain's global networks and architecture. The aim of this project is to develop new computational approaches for modeling the connectivity of the mouse brain, in order to reveal principles of wiring and information routing. The project will leverage whole-brain imaging datasets from the Allen Institute for Brain Science that each provide a small piece of the puzzle but when combined, can yield a picture of whole-brain connectivity. The outcomes of this research will be new maps of the global connectivity of the mouse brain, and a framework for studying the impact of disease and aging on whole-brain networks.<br/><br/>This project will develop a novel framework for analyzing whole-brain connectomics datasets to model high-level (mesoscopic) principles of wiring and architecture. To do this, tools from matrix factorization will be used to decompose large datasets from many brains into a collection of learned neural pathways or ""parts"" that, when combined, describe large volumes of  data as succinctly as possible. To address the size of the datasets, the use of subsampling-based approaches and randomized methods will be explored for massive-scale machine learning applications. Through intelligent and adaptive subsampling of the data, in combination with online methods for factorization, methods will be developed to process and learn from the entire Allen Institute Mouse Connectivity Atlas at less than 10-micron resolution. The outcomes of this project will be new tools for large-scale matrix factorization, models of the whole brain connectome, and discovery of wiring principles that could be useful in the development of next generation architectures for machine intelligence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755705","CRII: CIF: New Structure-Exploiting and Memory-Efficient Methods for Large-Scale Optimization and Data Analysis","CCF","Comm & Information Foundations","07/01/2018","01/29/2018","Paul Grigas","CA","University of California-Berkeley","Standard Grant","Phillip Regalia","06/30/2021","$175,000.00","","pgrigas@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7797","7935, 8228","$0.00","Large-scale optimization methods have been paramount to the successes of recent applications of machine learning and data analysis in a wide variety of domains. At the same time, certain structural properties of statistical models, such as sparsity or low-rank structure, have proven to be crucial for obtaining meaningful and accurate results in high dimensions. In addition to being highly scalable to large datasets, some optimization algorithms have the desirable property that they directly promote the aforementioned valuable structural properties of models. This project involves developing, analyzing, and implementing novel optimization algorithms that have such beneficial structure-exploiting and also memory-efficiency properties. This project directly involves the mentoring of graduate students, as well as integration of research results into an undergraduate level machine learning course and a graduate level course in optimization and statistical learning.<br/><br/>The foundation for this project is the Frank-Wolfe Method, a particular structure-exploiting first-order gradient optimization algorithm, and the related methodology of in-face directions. In-face directions automatically promote well-structured near-optimal solutions and have encouraging memory-efficiency properties. This research will investigate conditions whereby methods with in-face directions, as applied to convex relaxations of matrix completion and more general atomic norm regularization problems, are guaranteed to have a low memory footprint. Furthermore, this project will extend the reach of methods that incorporate in-face directions to new problem classes, including non-smooth objective functions, non-convex objective functions, and stochastic gradient estimates. The proposed optimization framework and in-face methodology applies very generally, and has potential for broader impact in several areas, including recommender systems, bioinformatics, customer segmentation, sentiment analysis, and medical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763674","SHF: Medium: Collaborative Research: Program Analytics: Using Trace Data for Localization, Explanation and Synthesis","CCF","Software & Hardware Foundation","06/15/2018","05/04/2020","Westley Weimer","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Sol Greenspan","05/31/2022","$225,586.00","","weimerw@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7798, 7924, 7944","$0.00","Formal program analyses have long held out the promise of lowering the cost of<br/>creating, maintaining and evolving programs. However, many crucial analysis<br/>tasks, such as localizing the sources of errors or suggesting code repairs, are<br/>inherently ambiguous: there is no unique right answer. This ambiguity<br/>fundamentally restricts the wider adoption of formal tools by limiting users to<br/>those with enough expertise to effectively use such ambiguous results. The key<br/>insight is that data-driven machine-learning approaches, which have proved<br/>successful in other domains, can be applied to the data traces generated by<br/>programmers as they carry out development tasks. This research addresses the<br/>challenge of ambiguity by extending classical program analysis into modern<br/>program analytics. This extension enhances classical symbolic methods with<br/>modern data-driven approaches to collectively learn from fine-grained traces of<br/>programmers interacting with compilers or analysis tools to iteratively modify<br/>and fix software.<br/><br/>The research systematically develops program analytics by pursuing research<br/>along two dimensions: language domains and programming tasks. First, it studies<br/>different language domains, from dynamically typed languages (Python), to<br/>statically typed functional languages with contract systems (Haskell), to<br/>interactive proof assistants (Coq). Second, it targets different programming<br/>tasks, from localizing errors like null-dereferences, assertions or other<br/>dynamic type failures, to static type errors, to completing or fixing code to<br/>eliminate an error or to obtain some desired functionality. This approach takes<br/>advantage of a suite of new approaches that harness recent advances in<br/>statistical machine learning and fine-grained, domain specific programmer<br/>interactions. These advantages allow the research to address the fundamental<br/>problem of ambiguity in classical program analysis. This has potential to<br/>transform software development by yielding a new generation of program analysis<br/>tools that are efficient, applicable, and automatically customizable (e.g., to a<br/>particular company, project, group or even individual).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837129","FMitF: Opening Up the Black Box of Probabilistic Program Inference","CCF","FMitF: Formal Methods in the F","12/01/2018","09/07/2018","Todd Millstein","CA","University of California-Los Angeles","Standard Grant","Rebecca Hwa","11/30/2022","$947,397.00","Guy Van den Broeck","todd@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","094Y","062Z, 071Z","$0.00","Probabilistic programming languages are an expressive means for creating, maintaining, and understanding a wide range of machine-learning models, and they have been successfully used by both researchers and major technology companies.  However, today's probabilistic programming languages impose strong limitations on the kinds of programs for which they are effective, thereby precluding their use for many machine-learning applications.  This project adapts and generalizes techniques from the formal methods community for reasoning about traditional programs, in order to develop general-purpose algorithms for probabilistic inference, which is the key task that a probabilistic programming language must perform.  The project is implementing these algorithms in the context of a new imperative probabilistic programming language and is providing educational opportunities in the burgeoning area of probabilistic programming for graduate, undergraduate, and high-school students.<br/><br/>This project has three main technical thrusts.  First, the project is developing exact inference algorithms for discrete probabilistic programs by exploiting the connection to techniques for symbolic model checking and weighted model counting.  Second, the project is developing techniques to automatically decompose a probabilistic inference query into multiple simpler sub-queries, each of which can be solved using the most appropriate inference method. The key enabler of this decomposition is a novel abstraction process for probabilistic programs.  Third, the project is investigating the use of abstractions as proposal distributions for probabilistic inference, resulting in new abstraction-guided approximate inference algorithms.  The results of this project will make probabilistic programming more effective by making probabilistic inference and learning tractable for a wider class of programs.  The artifacts that result from this research will be released open source, including a new probabilistic programming language that leverages the newly developed inference techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816386","Collaborative Research: Second-Order Variational Analysis in Structured Optimization and Algorithms with Applications","DMS","APPLIED MATHEMATICS","09/01/2018","08/07/2018","Nghia Tran","MI","Oakland University","Standard Grant","Pedro Embid","08/31/2021","$108,215.00","","nttran@oakland.edu","530 Wilson Hall","Rochester","MI","483094401","2483704116","MPS","1266","062Z","$0.00","This project focuses on developing advanced tools of mathematical analysis to investigate modern structured optimization problems and building efficient algorithms to solve them. These problems arise in different areas of science and engineering, including massive data analysis, machine learning, signal processing, medical image reconstruction, statistics, traffic and logistical networks, and operations research. Most of them share the irregular phenomenon of nonsmoothness or nonconvexity that challenges computation. Despite several practically successful algorithms recently proposed to solve such problems, the underlying fundamental theory is not quite understood and explored. Only analyzing the complexity and the deep mathematics behind these problems and algorithms provides practitioners across related, vital science and engineering areas new tools to comprehend their core features, be able to design more efficient algorithms, and attack more challenging problems arising from practice. The investigators develop such tools via a novel approach from a relatively young subfield of applied mathematics, variational analysis, which is naturally compatible with these nonsmooth and complex structures. Several topics from this project are integrated with teaching topic courses and training of students.<br/><br/>This project is devoted to developing the theory of second-order variational analysis (SOVA) and using it to study the stability, sensitivity, and computational complexity of algorithms for solving structured optimization problems. The first part of this project serves as the theoretical foundation; it concerns the theory of SOVA with connections to stability and sensitivity analysis. More specifically, the investigators intend to study: (i) tilt stability and full stability for general optimization problems with connections to Robinson's strong regularity and Kojima's strong stability for conic programming via SOVA; (ii) metric (sub)regularity of the subdifferential and Kurdyka-Lojasiewicz property on nonsmooth (possibly nonconvex) functions via SOVA; and (iii) stability for parametric variational systems including Nash equilibrium systems and variational inequalities via SOVA. The second part of this project consists of designing and analyzing proximal algorithms for solving convex and nonconvex structured problems. Immediate applications include Lasso, group Lasso, elastic net, basic pursuit, sparsity, low-rank problems, and completion matrix problems that originate from compressed sensing, image reconstruction, machine learning, and data science. Stability theory developed in the first part plays a significant role here, especially in the complexity analysis of these algorithms. It explains why the development of many recent proximal algorithms is strongly influenced by the hidden power of SOVA. The specific objectives of this part are: (i) to accelerate the forward-backward splitting method and analyze the phenomenon of linear convergence encountered frequently in numerical experiments; and (ii) to design efficient methods of Douglas-Rachford splitting type for solving nonconvex optimization and feasibility problems. Other important applications include inverse problems corrupted by Poisson noise and total variation denoising models, both of which are well recognized in imaging science and statistical learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813697","SaTC: CORE: Small: Collaborative: Understanding and Mitigating Adversarial Manipulation of Content Curation Algorithms","CNS","Secure &Trustworthy Cyberspace","07/15/2018","07/05/2018","Rachel Greenstadt","PA","Drexel University","Standard Grant","Sara Kiesler","07/31/2019","$249,999.00","","greenstadt@nyu.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8060","025Z, 7434, 7923, 9102","$0.00","Online social networks (OSNs) have fundamentally transformed how billions of people use the Internet. These users are increasingly discovering books, music bands, TV shows, movies, news articles, products, and other content through posts from trusted users that they follow. All major OSNs have deployed content curation algorithms that are designed to increase interaction and act as the ""gatekeepers"" of what users see. While this curation and filtering is useful and necessary given the amount of content available, it has also exposed people and platforms to manipulation attacks whereby bad actors attempt to promote content people would otherwise prefer not to see. This has driven the creation of an underground ecosystem that provides services and techniques tailored towards subverting OSNs' content curation algorithms for economic and ideological gains. This project will conduct open research to improve our understanding of current algorithmic curation attackers. The team will devise content curation algorithms and defenses which are hardened against manipulation and that can be adopted by these OSN platforms, providing a systematic approach to improving design and practice in an area of critical national importance. Technology transfer from this project will protect the integrity of social media discourse from adversarial manipulation. This project will train students with expertise in security and machine learning, areas of broad national need, and produce educational materials to engage both high school students and the public in these critical questions.<br/> <br/>The team will holistically explore the economic, social, and technical perspectives of machine learning-based content curation algorithms' weaknesses. The research comprises three main activities: 1) understand how OSNs are currently being successfully manipulated at large scales, 2) investigate the defenses OSNs have in place, and 3) design more resilient defenses. The team will build the first-ever taxonomy of manipulation services and techniques that are actively used to manipulate curation algorithms. Another thrust of the project is to create a framework for the external evaluation of deployed manipulation defenses based on the collection of both public data from the OSN's platform and external data to compare it against. The team will then develop robust and scalable algorithms to detect OSN manipulation within the collected data. Finally, the team will use the insights from the taxonomy of effective manipulation techniques and the exploration of the limitation of current defenses to design fundamentally resilient content curation algorithms. The project will explore both new curation algorithms and more effective mitigation techniques for existing algorithms. The project's findings will deepen our understanding of social network manipulation and adversarial learning and produce reliable approaches to algorithmic content curation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839441","EAGER: Real-Time: Search for dynamical dependencies and natural time-scales of physical processes","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2018","09/19/2018","Tryphon Georgiou","CA","University of California-Irvine","Standard Grant","Radhakisan Baheti","08/31/2021","$299,981.00","Efi Foufoula-Georgiou","tryphon@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","ENG","7607","092E, 7916","$0.00","A fundamental problem in physical sciences and engineering is to identify dependencies and dynamical relations between interacting processes for understanding causal relationships and advancing predictive modeling.  Indeed, historically, such dependences often formed the basis of physical laws. The abundance of large data-sets about complex natural or engineered systems in our modern technological world, has brought a sense of urgency to the need for reliable and versatile machine learning tools to detect relations between processes. The starting point of the project is the realization that often the observational time scale at which data is collected may not be the native time scale at which interactions occur and thus sampling might obfuscate the nature of such relations.  Indeed, linear dynamical relations between continuous-time processes may not be readily detectable from data collected at any finite sampling rate.  This project will develop methodologies that will allow to fully recover sought relations between processes at the natural time-scale from data at a (typically coarser) observational time-scale. Theory and statistical learning tools that will be developed for that purpose will be applied to geophysical processes, such as identifying relationships between climate variables using a suite of observations from ground and multi-satellite sensors at different spatio-temporal scales. <br/><br/>When relations between variables are dynamic (i.e., the interaction relies on memory in the system), sampling hides the nature of dynamical dependencies. Specifically, in linear stochastic processes, dynamical dependencies between vector-valued processes are reflected in the nullity of the power spectral density matrix when this is estimated at the natural time-scale of the process. At the observational sampling rate, the corresponding nullity no longer relates to the structure of the dynamical relations. Yet, with proper analysis the dynamical relations can be recovered by projecting sample-models to the natural time-scale of the processes involved.  In fact, for linear stochastic processes there is a fastest process time-scale that is consistent with the coarser scale observational data, and it is at that fine scale that models can be projected via solving suitable algebraic equations.  At any coarser scale, dynamical dependencies cannot be readily detected. The project will focus on how to learn and recover the natural time-space scale at which dynamical dependencies must be sought. Statistical learning tools will be developed and applied to geophysical data as proof of concept.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848701","CHE/DMS Innovation Lab: Learning the Power of Data in Chemistry","CHE","TRIPODS Transdisciplinary Rese, PROJECTS","09/01/2018","08/28/2018","Xiaoming Huo","GA","Georgia Tech Research Corporation","Standard Grant","Michelle Bushey","08/31/2020","$225,450.00","Paul Zimmerman","xiaoming@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","041Y, 1978","062Z, 9263","$0.00","With the support from the Division of Chemistry and the Division of Mathematical Sciences, Professor Xiaoming Huo of the Georgia Institute of Technology and Professor Paul Zimmerman of University of Michigan will bring together researchers in chemistry and data science for an immersive five-day Innovation Lab.  Innovation Labs are an evolution of ""sandpit"" workshops that have been used by research funding agencies for brainstorming and team formation, to accelerate scientific innovation. Over five days of in-person interaction, participants will learn to speak each others' languages through case study presentations and bootcamp activities, frame new research questions, form interdisciplinary teams around these ideas, and initiate collaborative projects that include scientists from both the chemistry and data science communities. The projects will utilize machine learning and data-driven statistical techniques to tackle challenging chemical problems and the analysis of complex chemical data sets. A primary objective of the Innovation Lab is to forge new connections between chemists and data scientists, and stimulate new research problems and new interdisciplinary collaborations. The Innovation Lab will include graduate students as well as participants at different career stages, to strengthen future workforce development and promote the participation of women and other underrepresented groups in the research community. <br/><br/>Dramatic advances in science can result when standard modes of research are replaced with completely new ways of thinking. The potential for such advances has been recognized at the interface of data science and chemistry, a key area where chemical sciences can greatly benefit from emerging strategies in statistics, computer science, and mathematics. This project brings together experts from historically unconnected disciplines to accelerate the development of the nascent field of chemical data science.  The outcome of the Innovation Lab is anticipated to be a deeper engagement of the data and computer science communities with chemistry, sparking new collaborations between the two communities, the initiation of new ways of thinking, and joint projects that extend well beyond the time frame of the Lab itself. These projects have the potential to achieve profound scientific impact in chemistry, inspire new methodological and algorithmic development, and serve as prime examples of the interplay between data-driven and chemical approaches. Fundamental mathematical and statistical principles will be applied to tackle important chemistry problems. Conversely, the challenging data sets and problems from chemistry may inspire unanticipated methodological developments in data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1742424","STEM TRansfer Students Opportunity for Nurtured Growth (STRONG)","DUE","S-STEM-Schlr Sci Tech Eng&Math","03/01/2018","01/11/2018","Mubarak Shah","FL","The University of Central Florida Board of Trustees","Standard Grant","Karen Keene","02/28/2023","$999,994.00","Malcolm Butler, Nazanin Rahnavard, Brian Moore, Gordon Chavis","shah@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","EHR","1536","9178, SMET","$0.00","With funding from the National Science Foundation's Scholarships in Science, Technology, Engineering and Mathematics (S-STEM) program, the STEM TRansfer Students Opportunity for Nurtured Growth (STRONG) is providing support to low-income students with demonstrated financial need and academic promise to succeed in STEM disciplines at the University of Central Florida (UCF). This S-STEM Track 2 project is funding up to 36 scholarships over 5 years for transfer students who are pursuing bachelor's degrees in STEM. Compared to first time in college (FTIC) students who begin their academic careers at a four-year institution, transfer students generally find the path to obtaining a bachelor's degree more challenging, and they are significantly less likely to graduate than their FTIC counterparts. Finding ways to improve the educational experiences of these students is an urgent national priority.  The aim of UCF's STRONG project is to substantially increase retention, graduation rates, and career success of STEM transfer students.  This goal is achieved in part by giving each student a role in various STEM activities within a community of high-achieving scholars, who are majoring in similar fields.  Providing financial support, mentorship, and opportunities for academic engagement outside the classroom promises to reduce the obstacles students face, keep them interested in their field of study, and motivate them to excel academically.  A thorough study of how the program affects its participants will maximize the impact of the intervention and informs future efforts to improve the educational experiences of STEM transfer students.<br/><br/>The UCF STRONG project consists of three components, which are all essential for reaching the program goal of increasing retention, graduation rates, and career success of STEM transfers. The first component is to design, implement, and improve a high-accuracy prediction model for identifying high-risk STEM transfers. Using known information about transfers, along with statistical analysis and machine learning, it will establish a means of predicting which students are least likely to complete a STEM degree, thereby helping UCF to know which students will benefit most from STRONG. The second component is to increase retention of transfers in STEM fields. STRONG scholars will be selected from financially needy incoming transfers who have been predicted to be unlikely to graduate, with the purpose of retaining them through graduation due to their participation in STRONG. This outcome is an expected result of increased student success through stimulation of academic integration and a cultivation of proficiency in their chosen fields.  More specifically, each student is given a role within a community of like-minded students from closely related fields; they also meet regularly with mentors and participate in STEM activities that promote their success.  The third component is to learn how the interventions influence STEM transfer students' ability to become active participants in their own learning. The project team will study the effects of the intervention on metacognition, motivation, and behavior, which are the three pillars of Self-Regulated Learning and are essential for student success.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830066","ATD: Collaborative Research: Theory and Algorithms for Real-Time Threat Detection from Massive Data Streams","DMS","ATD-Algorithms for Threat Dete, ","08/15/2018","08/05/2019","John Jasper","SD","South Dakota State University","Continuing Grant","Leland Jameson","07/31/2021","$98,222.00","","John.Jasper@sdstate.edu","1015 Campanile Ave","Brookings","SD","570070001","6056886696","MPS","046Y, S100","6877, 9150","$0.00","National security interests demand a heightened awareness of the actions of various adversaries. This voracious appetite for information results in an overwhelming stream of spatiotemporal data. New mathematics is necessary to effectively manage this data deluge; this research project aims to develop new theory and algorithms for this cause. The approach is guided by the following abstract description of the threat detection problem: Given a massive stream of spatiotemporal data, the task is to maintain a slowly evolving model of ""normalcy,"" any deviations from which are to be further investigated as potential threats. <br/><br/>The project will focus on the following two objectives: (1) develop algorithms and optimal encodings to process massive data streams, and (2) develop fast certificates and guarantees for cutting-edge learning algorithms. To this end, the research aims to solve some of the big open problems in optimization, frame theory, and machine learning: (a) to quickly solve convex relaxations of NP-hard unsupervised learning problems from streaming data; (b) to construct optimal line packings, including the packings conjectured to exist by Zauner; (c) to find sub-linear a posteriori approximation certificates for NP-hard learning problems; (d) to explain the well-behaved optimization landscapes exhibited by generative adversarial networks; and (e) to develop fast, after-the-fact explanations for black-box classification, enabling well-informed human decision making.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757153","Summer Program on Game Theory and Economics","SES","Economics","09/01/2018","10/22/2018","Yair Tauman","NY","SUNY at Stony Brook","Standard Grant","Nancy Lutz","08/31/2021","$233,640.00","Sandro Brusco, Camilo Rubbini, Pradeep Dubey, Abraham Neyman","amty21@gmail.com","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","SBE","1320","7556","$0.00","This award funds a series of conferences and workshops in game theory and applications of game theory. The conference, follow-on workshops, and lectures are held each summer as a coordinated program designed to bring together a distinguished group of senior scientists from around the globe with junior faculty and graduate students from across the U.S. Topics include new developments in game theory, areas in the intersection between game theory and computer science (such as Computation and Privacy), and applications of game theory to business and public policy. The activities advance science by giving young scholars the opportunity to form new research collaborations and present their work to a distinguished audience. <br/><br/>Topics include cooperative game theory, The Interface of Economics and Computing, Machine Learning and Data Science, Strategic Communication and Learning, Mechanism Design, Agents with Mis-specified models, Pricing in Complex Markets, Learning, Contagion, and Coordination in Networks, Delegation as an Economic Phenomenon, Game Theory and Computation, Matching Theory and Applications, Game Theory in Operations Research, Decision Theory, and Advances in Learning Theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815559","NSF-BSF:CIF:Small: Searching for the Rare: an Active Inference and Learning Approach","CCF","Comm & Information Foundations","07/01/2018","05/22/2018","Qing Zhao","NY","Cornell University","Standard Grant","Phillip Regalia","06/30/2021","$490,078.00","","qz16@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7923, 7935, 9102","$0.00","This project addresses the problem of searching for a few rare events of interest among a massive number of possibilities. The rare events may represent opportunities with exceptional returns or anomalies associated with high costs or potential catastrophic consequences. This problem arises in a broad range of applications, ranging from communications and infrastructure systems, cyber-security, to social-economic networks, and is particularly relevant in the era of increasing network size and abundance of data. The multidisciplinary nature of this project also provides a rich research experience for both undergraduate and graduate students.<br/><br/>The scientific objective is to develop general design methodologies for detecting rare events quickly and reliably when the total number of hypotheses is large, the observations are noisy, and the prior knowledge on the rare events may be as little as ""they are different from the nominal."" The project consists of three steps that represent a logical progression in scope and level of difficulty: (i) achieving optimal sample complexity with respect to detection accuracy through active hypothesis testing; (ii) achieving optimal sample complexity with respect to the dimension of the search space by exploiting inherent hierarchical structures; (iii) tackling unknown models by integrating online learning with active inference. The holistic treatment within a principled theoretic framework draws inspirations from and contributes to fundamental theories of active hypothesis testing and statistical and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829955","ATD: Collaborative Research: Theory and Algorithms for Real-Time Threat Detection from Massive Data Streams","DMS","ATD-Algorithms for Threat Dete, ","08/15/2018","08/05/2019","Dustin Mixon","OH","Ohio State University","Continuing Grant","Leland Jameson","07/31/2021","$76,730.00","","Dustin.Mixon@gmail.com","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","046Y, S100","6877","$0.00","National security interests demand a heightened awareness of the actions of various adversaries. This voracious appetite for information results in an overwhelming stream of spatiotemporal data. New mathematics is necessary to effectively manage this data deluge; this research project aims to develop new theory and algorithms for this cause. The approach is guided by the following abstract description of the threat detection problem: Given a massive stream of spatiotemporal data, the task is to maintain a slowly evolving model of ""normalcy,"" any deviations from which are to be further investigated as potential threats. <br/><br/>The project will focus on the following two objectives: (1) develop algorithms and optimal encodings to process massive data streams, and (2) develop fast certificates and guarantees for cutting-edge learning algorithms. To this end, the research aims to solve some of the big open problems in optimization, frame theory, and machine learning: (a) to quickly solve convex relaxations of NP-hard unsupervised learning problems from streaming data; (b) to construct optimal line packings, including the packings conjectured to exist by Zauner; (c) to find sub-linear a posteriori approximation certificates for NP-hard learning problems; (d) to explain the well-behaved optimization landscapes exhibited by generative adversarial networks; and (e) to develop fast, after-the-fact explanations for black-box classification, enabling well-informed human decision making.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810888","Collaborative Research: Consistent Risk Estimation under High-Dimensional Asymptotics","DMS","STATISTICS","08/01/2018","08/01/2018","Mohammad Ali Maleki","NY","Columbia University","Standard Grant","Gabor Szekely","07/31/2021","$119,880.00","","mm4338@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","Learning from large datasets has been the cornerstone of modern innovations and discoveries in science, medicine, and technology. Fast prediction of unseen events is a canonical goal in statistical learning. A classic approach to this end is leave-one-out cross-validation, a time-consuming routine of leaving a datum out,  fitting the model on the rest, and testing it on the left out datum, repeatedly. The recent emergence of massive data has exacerbated the computational infeasibility of such approaches. Moreover, in many recent instances, the number of features per observation can be extremely large, adding another challenging facet to the fast estimation of prediction error. To overcome these problems a new set of scalable and consistent risk estimators will be developed in this project. <br/><br/>The importance of risk estimation has motivated this project of different schemes, such as cross-validation, Stein's unbiased risk estimation (SURE), Generalized cross-validation, Akaike Information Criterion (AIC), and Bootstrap. The emergence of high-dimensional datasets has challenged most classical approaches to risk estimation. For instance, the large discrepancy between in-sample and out-of-sample prediction error, in applications involving predictions based on previously unseen features, makes it hard to rely on popular estimators, such as SURE or AIC, in high-dimensional regimes where the number of predictors is smaller than or at the same order as the number of observations. On the other hand, the information value of a datum in these regimes (as opposed to the information value of a datum in low-dimensional settings) casts doubt on the reliability of other techniques, such as 5-fold cross-validation. The project offers a novel theoretical framework to find the middle ground between scalability and reliability, and specifically, to obtain theoretically consistent and computationally efficient risk-estimation schemes under high-dimensional settings. Since risk estimation is at the core of areas including but not limited to machine learning, signal processing, medical imaging, neuroscience, and social and environmental sciences, any success in this project will lead to reliable and immediate scientific discoveries and better learning systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810880","Collaborative Research: Consistent Risk Estimation under High-Dimensional Asymptotics","DMS","STATISTICS","08/01/2018","08/01/2018","Kamiar Rahnama Rad","NY","CUNY Baruch College","Standard Grant","Gabor Szekely","07/31/2021","$69,999.00","","Kamiar.RahnamaRad@baruch.cuny.edu","One Bernard Baruch Way","New York","NY","100105526","6463122211","MPS","1269","","$0.00","Learning from large datasets has been the cornerstone of modern innovations and discoveries in science, medicine, and technology. Fast prediction of unseen events is a canonical goal in statistical learning. A classic approach to this end is leave-one-out cross-validation, a time-consuming routine of leaving a datum out,  fitting the model on the rest, and testing it on the left out datum, repeatedly. The recent emergence of massive data has exacerbated the computational infeasibility of such approaches. Moreover, in many recent instances, the number of features per observation can be extremely large, adding another challenging facet to the fast estimation of prediction error. To overcome these problems a new set of scalable and consistent risk estimators will be developed in this project. <br/><br/>The importance of risk estimation has motivated this project of different schemes, such as cross-validation, Stein's unbiased risk estimation (SURE), Generalized cross-validation, Akaike Information Criterion (AIC), and Bootstrap. The emergence of high-dimensional datasets has challenged most classical approaches to risk estimation. For instance, the large discrepancy between in-sample and out-of-sample prediction error, in applications involving predictions based on previously unseen features, makes it hard to rely on popular estimators, such as SURE or AIC, in high-dimensional regimes where the number of predictors is smaller than or at the same order as the number of observations. On the other hand, the information value of a datum in these regimes (as opposed to the information value of a datum in low-dimensional settings) casts doubt on the reliability of other techniques, such as 5-fold cross-validation. The project offers a novel theoretical framework to find the middle ground between scalability and reliability, and specifically, to obtain theoretically consistent and computationally efficient risk-estimation schemes under high-dimensional settings. Since risk estimation is at the core of areas including but not limited to machine learning, signal processing, medical imaging, neuroscience, and social and environmental sciences, any success in this project will lead to reliable and immediate scientific discoveries and better learning systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1734400","NRI: INT: Co-Multi-Robotic Exploration of the Benthic Seafloor - New Methods for Distributed Scene Understanding and Exploration in the Presence of Communication Constraints","IIS","NRI-National Robotics Initiati","01/01/2018","08/24/2017","Yogesh Girdhar","MA","Woods Hole Oceanographic Institution","Standard Grant","Ralph Wachter","12/31/2021","$1,337,101.00","Brian Claus, James Kinsey","ygirdhar@whoi.edu","183 OYSTER POND ROAD","WOODS HOLE","MA","025431041","5082893542","CSE","8013","8086","$0.00","This project addresses the control and communications among underwater robotic vehicles to explore and map in ocean environments, where the communications are inherently low bandwidth, may be degraded and even disrupted due to natural ocean phenomena.  The research focuses on coordinating robots and cooperating teams of such robots under such conditions with limited human intervention.  It is expected the principles learned from this project will be generalizable to deployment of teams of cooperating robots operating in harsh environments with similar communication challenges such as what might be expected in the aftermath of natural disasters.<br/> <br/>The project addresses technical challenges with robots learning to describe their environment using high-level descriptors, using state of the art unsupervised machine learning techniques, and exchanging compact messages with each other to keep the description model consistent across all the cooperating robots. This high-level scene description is used by the robots to efficiently communicate the state of the exploration to each other, and to the human operator as possible. Furthermore, the human operators can efficiently control the robot team by specifying their interests in terms of these learned scene descriptors. The automatically generated exploration trajectories aim to maximize the information content, human interest, and spatial coverage, while taking into account the difficult constraints imposed by communication range in such harsh environments."
"1815131","SaTC: CORE: Small: Collaborative: A Multi-Layer Learning Approach to Mobile Traffic Filtering","CNS","Secure &Trustworthy Cyberspace","10/01/2018","09/07/2018","Zubair Shafiq","IA","University of Iowa","Standard Grant","Alexander Sprintson","09/30/2021","$250,000.00","","zubair-shafiq@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","8060","025Z, 7434, 7923","$0.00","The mobile ecosystem has become an attractive target for various types of abuses. For instance, many mobile applications leak sensitive user information, such as email addresses and location, which is a privacy issue. Second, attackers routinely disguise malware in seemingly legitimate mobile apps to launch attacks, which poses security threats. Third, many mobile apps and sites push intrusive und undesirable ads, such as auto-play and pop-ups, which harm usability. The objective of this project is to defend against these abuses in the mobile ecosystem through real-time on-device filtering of network traffic on a mobile device.<br/><br/>The project performs multi-layer network traffic analysis with help of machine learning. It extracts features and train learning models to produce filtering rules, which can be applied on the device. Filtering on the mobile device is particularly challenging due to limited resources, lack of visibility into packet payload, and potential collateral damage on the affected apps. The filtering approach used is envisioned as universal (i.e., applicable across all apps), automated (compared to the current practice of manually maintained privacy related data), adaptive (to the ever-changing threats), and user-centric (i.e., customized per user) while leveraging collaboration among users. The general framework will be validate in three case studies against the following abuse scenarios in the mobile ecosystem: (i) intrusive ads (ii) privacy leaks (iii) malvertising. This project is excepted to enhance the usability, privacy, and security of the mobile ecosystem and will inform policies and practices on mobile data transparency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815666","SaTC: CORE: Small: Collaborative: A Multi-Layer Learning Approach to Mobile Traffic Filtering","CNS","Secure &Trustworthy Cyberspace","10/01/2018","09/07/2018","Athina Markopoulou","CA","University of California-Irvine","Standard Grant","Alexander Sprintson","09/30/2021","$249,999.00","","athina@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8060","025Z, 7434, 7923, 9102","$0.00","The mobile ecosystem has become an attractive target for various types of abuses. For instance, many mobile applications leak sensitive user information, such as email addresses and location, which is a privacy issue. Second, attackers routinely disguise malware in seemingly legitimate mobile apps to launch attacks, which poses security threats. Third, many mobile apps and sites push intrusive und undesirable ads, such as auto-play and pop-ups, which harm usability. The objective of this project is to defend against these abuses in the mobile ecosystem through real-time on-device filtering of network traffic on a mobile device.<br/><br/>The project performs multi-layer network traffic analysis with help of machine learning. It extracts features and trains learning models to produce filtering rules, which can be applied on the device. Filtering on the mobile device is particularly challenging due to limited resources, lack of visibility into packet payload, and potential collateral damage on the affected apps. The filtering approach used is envisioned as universal (i.e., applicable across all apps), automated (compared to the current practice of manually maintained privacy related data), adaptive (to the ever-changing threats), and user-centric (i.e., customized per user) while leveraging collaboration among users. The general framework will be validate in three case studies against the following abuse scenarios in the mobile ecosystem: (i) intrusive ads (ii) privacy leaks (iii) malvertising. This project is excepted to enhance the usability, privacy, and security of the mobile ecosystem and will inform policies and practices on mobile data transparency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755836","CRII:SCH:Computational Methods to Mine Multi-omic Data for Systems Biology of Complex Diseases","IIS","Smart and Connected Health","06/15/2018","06/04/2018","Jingwen Yan","IN","Indiana University","Standard Grant","Sylvia Spengler","05/31/2021","$174,831.00","","jingyan@iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8018","8018, 8228, 9102","$0.00","Recent advances in high throughput technologies have led to a substantial increase in multi-omic data characterizing various levels of molecular changes in the progression of disease, including genome, transcriptome, proteome and metabolome.  The availability of computational methods that are sufficiently powerful to handle the high dimensionality and heterogeneity of multi-omic data is still very limited. In addition, major findings generated from current -omics studies have been largely restricted to relatively simple patterns, e.g., individual biomarkers, possibly with few functional interactions, which present difficulties for validating these findings and relating them to downstream biology. This project, by coupling the multi-omic data and the systems biology networks, will develop novel computational methods to explore the functional network modules associated with disease quantitative traits. By enabling both strategic and efficient knowledge extraction from the vast biological landscape represented by multi-omic data, this research has may lead to unprecedented discovery of disease mechanisms and suggest surrogate biomarkers for therapeutic trials.<br/><br/>This work will develop new computational methods to enable the integration of large scale heterogeneous multi-omic data with rich domain knowledge for better biomarker and association discovery. Two interrelated tasks will be performed: 1) Develop a novel biological knowledge guided structured sparse learning model together with large-scale optimization methods to integrate -omic data and biological networks from multiple sources and discover -omic modules involving heterogeneous biomarkers for accurately predicting outcomes of interest; and 2) Couple multi-task learning with structured sparse association models to jointly learn the bi-multivariate associations between imaging phenotypes and -omic features with dense functional connections for multiple groups. The project will contribute to a new solution framework spanning the areas of machine learning, data mining and network science, and also provide novel perspectives as to how to effectively integrate the large-scale and heterogeneous -omic data for a systems biology of complex diseases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755946","CRII: III: Understanding Urban Vibrancy: A Geographical Learning Approach Employing Big Crowd-Sourced Geo-Tagged Data","IIS","Info Integration & Informatics","08/01/2018","03/26/2018","Yanjie Fu","MO","Missouri University of Science and Technology","Standard Grant","Sylvia Spengler","10/31/2019","$174,515.00","","yanjie.fu@ucf.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","7364","7364, 8228","$0.00","Vibrant communities are defined as places with the following features: permeability, vitality, variety, accessibility, identity, and legibility. Developing vibrant communities can help boost commercial activities, enhance public security, foster social interaction and, thus, yield livable, sustainable, and viable environments. With the advent of the mobile and sensing technologies, big crowd-sourced geo-tagged data (BCGD) are increasingly available from diverse sources (e.g., buildings, vehicles, human, sensors, devices) in urban space, and represent an invaluable source of intelligence for understanding urban vibrancy and enhancing smart growth. This project will develop novel, systematical, and effective analytical techniques to significantly advance critical problems in urban vibrancy by taking advantage of the wealth of BCGD. The algorithms and tools developed in this project will directly impact community planning,  city governance, and urban economics. The educational component of this project includes developing a new curriculum that incorporates research into the classroom and provides students from under-represented groups with opportunities to participate in research.<br/><br/><br/>This project will develop new analytical techniques to discover, analyze, and leverage the patterns within and the relationships among BCGD to understand and sustain urban vibrancy. Novel methodologies that are appropriate to urban vibrancy will be designed in three directions: measurements, patterns, and mechanism. In researching measurements, axioms that a metric of urban vibrancy should satisfy will be introduced, and principled metrics will be devised to evaluate community vibrancy and learn how it is distributed. In researching patterns, an analytic framework will be proposed to discover the complex patterns of spatial configuration from BCGD for urban vibrancy. This framework aims to identify the compatible dimensions and corresponding measuring methods, as well as optimal portfolios and geographic presentation of spatial configuration. In researching mechanism, new machine learning models will be developed to examine the impact of spatial configuration on urban vibrancy by exploiting the conformity between spatial view and mobility view and the regularity of geographic dependencies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827246","RCN-UBE:  Data-to-Design Course-based Undergraduate Research Experience ? protein modeling and characterization to enhance student learning and improve computational protein design","DBI","UBE - Undergraduate Biology Ed, IUSE","10/01/2018","08/10/2018","Justin Siegel","CA","University of California-Davis","Standard Grant","Sophie George","09/30/2020","$293,790.00","","jbsiegel@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","BIO","037Y, 1998","","$0.00","Incorporating undergraduate research experiences into laboratory courses vastly increases the reach and accessibility of these opportunities. The Data-to-Design Course-based Undergraduate Research Experience (D2D-CURE) network will train and support faculty to broaden research participation and expand high-impact educational practices. The network is designed to make this workflow accessible to a wide array of higher education institutions (community colleges, state schools, liberal arts schools, as well as research universities), as a high-impact module for existing and new biochemistry and bioengineering courses. Collectively and over time students who engage in this project will be generating datasets large enough to begin utilizing machine learning tools to improve protein design algorithms, while learning translatable skills, the process of science, and developing confidence in doing research. This network serves to both train future scientist and to advance the progress of science.<br/><br/>The Data-to-Design Course-based Undergraduate Research Experience (D2D-CURE) network's primary goal is to make a well-developed and tested molecular modeling and enzyme characterization workflow accessible to institutions nation-wide as an integratable curricular element for existing and new biology courses. The project has three major activities: The first is the coordination of workshops that will enable instructors from across the country to employ the D2D-CURE workshop at their home institutions. The second is the expansion of an annual conference, currently focused on protein modeling and design, to include a student development and engagement track. Third is the development of an online presence allowing the D2D-CURE participants to work in concert on cutting-edge scientific problems as a community as opposed to isolated institutions and programs. Despite widespread adoption of computational protein design, significant improvements are still needed to move efforts from being a tool used to enrich proteins with the desired function into a more accurate and predictive computer-aided design tool with a quantitative relationship between design and desired function. We plan to focus on enzymes since the only large data sets that currently exist are not quantitative, have a low dynamic range, and often convolve several independent physical measurements into a single value. To generate the data sets needed to develop a new generation of computational protein design software for enzymes, we are proposing a Research Coordination Network to engage students and their instructors in using industrially relevant techniques while generating acutely needed protein structure-function datasets.<br/><br/>This project is being jointly funded by the Directorate for Biological Sciences, Division of Biological Infrastructure, and the Directorate for Education and Human Resources, Division of Undergraduate Education as part of their efforts to address the challenges posed in Vision and Change in Undergraduate Biology Education: A Call to Action (http://visionandchange/finalreport/).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763434","III: Medium: Massively Parallel Data Analytics on Heterogeneous Architectures","IIS","Info Integration & Informatics","06/01/2018","06/26/2019","Samuel Madden","MA","Massachusetts Institute of Technology","Continuing Grant","Sylvia Spengler","05/31/2022","$1,154,000.00","Saman Amarasinghe, Joel Emer","madden@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7364","7364, 7924","$0.00","The rise of connected devices and the Internet has led to unprecedented growth in the volumes of data that computers must store and process. This enormous growth in data volumes coincides with a growing demand for immediate answers and interactive analytics. Increasingly companies need real-time reports of sales, network traffic, anomalies, and other business trends. Internet-connected devices, like cars and industrial plants, demand even more real-time analysis of data. These trends mean database and data analytics platforms must deliver ever-faster performance from machines, a fact that has driven the dramatic interest in scalable multi-node processing systems, like Hadoop and Spark, which distribute the processing of large data sets across clusters of machines. Unfortunately, because of the way these platforms are engineered, they provide shockingly poor utilization of the hardware resources on each node, often times yielding single-node throughput that is thousands of times lower than what the raw hardware is capable of. In this project, an orthogonal direction will be pursued; a system, called Proteus, will be built that will obtain performance that utilizes hardware to the fullest extent possible, focusing on yielding a scalable system that fully utilizes all available computing resources. If successful, this project will have broad impact because databases and data-intensive parallel computing systems are used by millions of enterprises around the world, both on-site and in computing clouds; optimized implementations of these systems that better exploit hardware will improve response times and reduce hardware and energy costs, resulting in billions of dollars of cost savings.<br/><br/>Proteus will parallelize across many cores on a single processor, as well as take advantages of many-core systems such as GPUs and Intel's Xeon Phi. In addition, Proteus will also be able to exploit large diverse clusters of hardware, but the aim is to do that without giving up this efficiency, rather than accepting inefficiency as a given of distributed computing. To do this, research in the Proteus project will focus on four key areas: <br/>(1) Developing optimized implementations of individual database algorithms, such as top-k sorts, sequential scans, random lookups, graph and machine learning algorithms for GPUs and CPUs. <br/>(2) Building cost models that predict the performance of these algorithms on heterogeneous architectures. <br/>(3) Developing intermediate languages that abstract details of the underlying hardware, to hide the nuances of these different platforms to but without giving up performance. <br/>(4) Building an optimizer that uses cost models to place these plans onto a heterogeneous mix of hardware to obtain the best overall performance for each query plan.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813823","RI: Small: Cache transition systems for sentence understanding and generation","IIS","Robust Intelligence","09/01/2018","08/15/2018","Daniel Gildea","NY","University of Rochester","Standard Grant","Tatiana Korelsky","08/31/2021","$399,998.00","","gildea@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7495","7495, 7923","$0.00","Graph-based semantic representations allow computers to store and process information in natural language text.  Such graphs contain nodes representing events and entities, and edges between nodes representing relations.  This project will develop algorithms that operate directly on graphs, and will allow statistical natural processing techniques to better represent semantic structures.  These advances can improve systems that extract information from text, translate between human languages such as English and Chinese, and interact with humans in natural language dialog.  Improved language understanding can help in accessing the enormous amount of information available in unstructured text on the web as well as in databases of newspapers and scanned books.  Improved translation between languages increases opportunities for trade as well as for dissemination of information generally between nations and cultures.<br/><br/>Graph-based representations of the meaning of natural language sentences are being used to an increasing degree for reasoning tasks including question answering, merging information from disparate sources, and generating responses in dialog systems.  However, automatic interpretation of sentences into such structures remains a very difficult task, despite recent progress in syntactic parsing.  This project will develop algorithms for parsing into and generating text from semantic graphs, focusing on Abstract Meaning Representation or AMR, although the techniques generalize to other representations.  Existing statistical systems for the AMR parsing task generally use ad-hoc algorithmic approaches; fundamentally new algorithms are necessary to advance the state of the art.  This project is based on a new transition system, called a cache transition system, tailored to the task of parsing into graph structures.  Preliminary experiments show that the system is a good match to real datasets of semantic graphs, in that it is able to produce the vast majority of graphs observed while at the same time simplifying the machine learning problem of predicting the next transition at each step.  This project aims to advance the state of the art in semantic parsing and generation by developing and training a neural version of the transition system to predict semantic graphs from input strings.  In its final year, the project will apply the parsing and generation methods to the task of machine translation, providing semantic graphs along with source language strings to a neural machine translation system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844492","EAGER: MATDAT18 Type-1: Collaborative Research: Data Driven Discovery of Singlet Fission Materials","DMR","TRIPODS Transdisciplinary Rese, DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY","09/01/2018","08/22/2018","Brian Reich","NC","North Carolina State University","Standard Grant","Daryl Hess","08/31/2021","$62,159.00","","brian_reich@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","MPS","041Y, 1712, 1765","054Z, 062Z, 7916, 7926, 9216","$0.00","NONTECHNICAL SUMMARY<br/>This award supports continued collaboration of materials researchers with data scientists kindled at the MATDAT18 Datathon event. The efficiency of organic solar cells may be enhanced significantly by harnessing singlet fission (SF), a quantum mechanical process that can lead to the generation of two conducting species, for example electrons, from one quantum of light. Presently, few materials are known to exhibit intermolecular SF in the solid state, and they belong to restricted chemical families. The vast number of possible molecules and crystals that could be made has not been explored for SF. The PIs will use computer simulations to search the many possibilities for new SF materials. To this end, a new approach will be developed, one that integrates cutting-edge advances in quantum mechanical simulations and machine learning. This research will advance both fields of materials science and data science. Graduate and undergraduate students will train in a collaborative cross-disciplinary environment at the interface of computational materials science and data science and acquire transferrable job skills in high demand. <br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports continued collaboration of materials researchers with data scientists kindled at the MATDAT18 Datathon event. Singlet fission (SF) is the conversion of one photogenerated singlet exciton into two triplet excitons. Recently, there has been a surge of interest in SF thanks to its potential to significantly increase the efficiency of organic solar cells by harvesting two charge carriers from one photon. However, few materials are presently known to exhibit intermolecular SF with high efficiency, hindering the realization of solid-state SF-based solar cells. The chemical compound space of possible chromophores is infinitely vast and largely unexplored. To enable computational discovery of SF materials, a new multi-fidelity screening approach will be developed, which integrates quantum mechanical simulations at different levels of fidelity with machine learning (ML) and database mining. ML algorithms will be used to analyze data generated by quantum mechanical simulations and to steer simulations for further data acquisition. High-cost high-fidelity evaluations of excited state properties of solid-state forms of candidate chromophores will be performed with many-body perturbation theory methods within the GW approximation and the Bethe-Salpeter equation. Lower-cost lower-fidelity evaluations of ground state features will be performed with density functional theory. Feature selection algorithms will then determine which descriptors are most predictive of the thermodynamic driving force for SF. These descriptors will be used to screen databases that contain crystal structures with no information or only partial information on their electronic properties. Optimization algorithms will be employed to decide which data points to sample and at what level of fidelity to maximize information gain. This research will advance the discovery of new intermolecular SF chromophores and will lead to advances in data science in the area of experimental design.  <br/><br/>The award is jointly funded through the Division of Materials Research and the Division of Mathematical Sciences in the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815347","CHS: Small: Robust Eye Tracking and Facial Context Sensing for Mobile Augmented Reality and Virtual Reality","IIS","HCC-Human-Centered Computing","09/01/2018","05/16/2019","Deepak Ganesan","MA","University of Massachusetts Amherst","Continuing Grant","Balakrishnan Prabhakaran","08/31/2021","$516,000.00","","dganesan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7367","7367, 7923, 9251","$0.00","Virtual and Augmented Reality headsets may soon become a major mass-market mobile consumer electronics device. However, there are challenges in making these headsets more portable and less power hungry, and in enabling a more immersive user experience by increasing awareness of the user's attention and emotional state. This project's goals are two-fold. The first goal is to enhance systems' ability to track the eye in a low-power yet robust manner by exploring new eye tracker designs combined with machine learning approaches. The second goal is to study methods to infer facial expressions and user emotion from wearable headsets by combining multiple sensing modalities including facial muscle activity and cameras. The project will also have educational impact on middle-school students and under-represented students through workshops. <br/><br/>The project involves activities and innovations in several areas. On the eye tracking side, the project will explore new hardware designs with stereo cameras and machine learning approaches to enhance robustness to face and eye shapes as well as eyeglass movements. On the facial expression sensing side, the team will explore multimodal sensing methods that combine electrooculography (EOG) and multiple camera views to infer expressions as well as to reduce power consumption. On the networked systems side, the project will look at leveraging eye tracking and facial expression sensing to enable wireless offload of rendering to a nearby compute node. The techniques to be researched can significantly lower the power consumed by AR and VR systems, enhance their ability to sense human expressions to enable more immersive experience, and reduce computational complexity by enabling predictive pre-fetch from a wirelessly connected edge cloud.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836948","FMitF: Collaborative Research: User-Centered Verification and Repair of Trigger-Action Programs","CCF","FMitF: Formal Methods in the F, Software & Hardware Foundation","09/01/2018","05/06/2019","Michael Littman","RI","Brown University","Standard Grant","Nina Amla","08/31/2022","$349,332.00","","mlittman@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","094Y, 7798","062Z, 071Z, 9150, 9251","$0.00","Modern data-centric systems, ranging from Internet-of-Things devices to online services, can benefit from helping people make clear their intent for how their devices and services should behave and interact with each other.  Generally, this requires people to engage in some amount of end-user programming, or programming by people who are not typically trained in programming.  Common examples of this include specifying that a light should only turn on when a room is occupied or that emails with certain words in the subject line should be routed into a particular folder.  Trigger-action programming (TAP), which consists of ""if-this-then-that"" rules, is the most common model for end-user programming because it is relatively easy to write simple TAP programs.  However, as the number and complexity of both rules and devices increases, TAP programs increasingly suffer from bugs and dependability problems and are hard to correct for inexperienced and trained programmers alike.  This project's goal is to make TAP programming, and thus people's ability to interact with devices that act on their behalf, more robust through developing a better understanding of end users' needs and abilities to write and debug TAP programs, computational techniques to both better model user intents and suggest TAP programs that meet them, and tools that use those techniques to help people more easily create correct TAP programs.  Apart from the potential benefits to people's well-being, the project will also provide educational benefits by developing course materials that increase awareness of both human aspects of, and formal methods for, programming.  Further, the tangible nature of such devices and the familiarity of popular online services are a fertile domain for engaging the public and training undergraduate students, K-12 students, and early-career graduate students in the computer science research lifecycle.<br/><br/>To accomplish these goals, the work combines techniques from formal methods, human-computer interaction, and machine learning. Contributions to formal methods include the design of systematic solutions to unique program repair, synthesis, and specification-refinement problems in the context of end-user programming. Contributions to cyber human systems include empirical studies and the design of data-driven interfaces for more accurately expressing intent. Specifically, the empirical human subjects studies seek to understand and improve the debugging process for trigger-action programming, create and distribute needed data sets of user-centric collections of trigger-action programs, and comparatively evaluate proposed interfaces. The interfaces developed in this work use data-driven methods to help users pinpoint and understand bugs in trigger-action programs, as well as to choose among candidates for automatically repaired trigger-action programs. Underlying these interfaces will be formal models of trigger-action programs, which are verified against specified properties written in linear temporal logic. The system developed will systematically synthesize program repairs, taking into account users' experiences and preferences. The system will also use a combination of machine learning and formal methods to automatically generate trigger-action programs and summarize specifications based on historical traces of user interaction with the system. In sum, helping non-technical users accurately communicate their intent through trigger-action programming benefits widely deployed end-user-programming systems for integrating internet-connected devices and online services.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1759522","Collaborative Research:  Innovation: Pioneering New Approaches to Explore Pangenomic Space at Scale","DBI","ADVANCES IN BIO INFORMATICS","07/15/2018","07/13/2018","Brendan Mumey","MT","Montana State University","Standard Grant","Peter McCartney","06/30/2021","$379,159.00","Indika Kahanda","brendan.mumey@montana.edu","309 MONTANA HALL","BOZEMAN","MT","597172470","4069942381","BIO","1165","9150","$0.00","This project develops new software tools for pangenomic analysis, which is a relatively new area of genomic research that studies large numbers of genome sequences from multiple organisms to understand how organisms adapt their genomes to their environments.  As the cost of DNA sequencing continues to decrease, it is now routine for multiple genomes per species to be available for analysis, giving much more information about the species. The approach makes use of a graph-based representation of a pangenome and exploits this representation to efficiently find both shared and unique regions of interest across genomes. Each individual?s genomic sequence corresponds to path in a graph data structure called a De Bruijn graph; these graphs are large and can have millions of nodes and edges.  The tools being developed are based on finding frequented regions (FRs) in De Bruijn graphs; these regions are hotspots that often represent features of interest in one or more genomes. Algorithms and software tools will be made available to the greater scientific community to facilitate new pangenomics research.  The project will provide support and training for a postdoc and an incoming PhD student at Montana State University. It will also support a summer intern in the last two years at the National Center for Genome Resources.  Aspects of the project will be incorporated into undergraduate and graduate courses at MSU, as well as integrated into several outreach and training activities at NCGR. In addition, MSU has several programs in place to serve American Indian students and the PIs will actively recruit from and engage this community.<br/><br/><br/><br/>The current trajectory of next generation sequencing improvements, including falling costs and increased read lengths and throughput, ensure that multiple genomes per species will be routine within the next decade. This project initiates work on a next generation of bioinformatics software that can exploit the increased information content available from multiple accessions and intelligently use the data for unbiased, species-wide analyses.  The proposed work will refine algorithms and develop software to address important problems in each of the identified areas. The research team has a variety of complementary expertise ranging from molecular biology, algorithms, machine learning and genomics research.  Pangenomic biology will be advanced through automatic identification of candidate regions of interest in a pangenome. Methods will be developed to discover regions that are conserved across evolutionary space, regions that are novel, and regions that have diverged due to positive selection.  Machine learning techniques will be used to search for interesting genomic regions. Lastly, this work will complement the work being done on the model plant, Medicago truncatula, contributing to research on its symbiotic relationships.  Results of the project can be found at: www.cs.montana.edu/pangenomics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755800","CRII: AF: Theoretical Problems in Quantum Computation","CCF","Quantum Computing","05/01/2018","03/09/2018","Xiaodi Wu","MD","University of Maryland College Park","Standard Grant","Almadena Chtchelkanova","04/30/2021","$175,000.00","","xwu@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7928","8228","$0.00","In anticipation of quantum computers being utilized in the near future, this project seeks to find quantum advantages in machine-learning-like tasks that are ubiquitous in our daily life. Future quantum computers will take advantage of entanglement, a quantum property, that digital computers do not use. It is important to understand the essential role of entanglement in the computational power of quantum computers to be able to use it efficiently. <br/><br/>This project consists of two sets of questions that touch upon central topics in quantum information (e.g., entanglement and its impact on complexity theory) and novel topics such as property testing. The first part of the project investigates the possibility of designing fast quantum algorithms for property testing of unknown classical distributions and quantum states, a well-motivated topic related to statistics, data analysis, and machine learning. The project aims to integrate techniques from classical property testing, quantum tomography, quantum walks, and so on. The second part of the project investigates the computational power of the absence of entanglement in the context of quantum Merlin-Arthur protocols with two provers (QMA(2)). It is a well-motivated problem due to its connection to the separability problem within quantum information as well as the Unique Games Conjecture in the field of approximation algorithm. Specific objectives include the study of (1) the k-extendible states and de Finetti theorems for the separability problem and (2) non-trivial error reduction schemes of QMA(2). The techniques used are primarily inspired by ideas from the sum-of-squares techniques in optimization, weak measurements and approximation theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835825","Collaborative Research: Elements: Software NSCI: Constitutive Relation Inference Toolkit (CRIKit)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2018","08/31/2018","Jed Brown","CO","University of Colorado at Boulder","Standard Grant","Bogdan Mihaila","08/31/2021","$293,441.00","","Jed.Brown@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1712, 8004","026Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales.  The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments.  This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.<br/><br/>The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses.  The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis.  This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability.  The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization.  The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829701","Collaborative Research: CyberTraining: CIU: Towards Distributed and Scalable Personalized Cyber-Training","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Blake Joyce","AZ","University of Arizona","Standard Grant","Alan Sussman","08/31/2021","$60,631.00","","bjoyce3@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","This project is addressing the challenge of providing distributed, scalable, and personalized training of cyberinfrastructures - systems that offer state-of-the-art cloud services for storing, sharing, and processing scientific data. Today, personalized training of these rapidly evolving, and hence relatively undocumented, systems requires trainer-supervised, hands-on use of these systems. These training sessions require trainees and trainers to be co-located and provide personalized training to a relatively small number of trainees. The project is developing new (a) domain-independent technologies in distributed collaboration and machine learning to reduce all three problems in a concerted manner, and (b) domain-dependent training material targeted at trainees in statistics, physical sciences, computer science, humanities, and medicine. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>A key technical insight in this work is that a cyberinfrastructure should not only support data science, but also make use of data science. The project is exploring two related innovations based on this insight: (1) Collaboration technologies that log, visualize and share the work of remote and local trainees to allow trainers to determine the need for remote or face-to-face assistance.  (2) Machine-learning technologies that mine trainee and trainer interactions so that trainees can be automatically instructed on how to solve their problems based on similar problems that have been previously solved by trainers and other trainees. The project is leveraging existing technologies and training techniques developed for a widely used NSF-supported cyberinfrastructure, called CyVerse. This system is domain-independent, but so far, its training material has been targeted mainly at plant-science research.  The project is extending the command interpreters and GUIs provided by CyVerse. The extended user-interfaces allow (a) trainees to announce difficulties and request recommendations, and (b) trainers to be aware of the progress of remote and local trainees, and remotely intervene when necessary. The functionality behind the user-interfaces is implemented by CyVerse-independent servers based on a general model of cyberinfrastructures, which includes the concepts of sharing and visualization of protected files, creation and execution of parameterized commands composed in workflows, and shareable, persistent work spaces. The project is adapting the CyVerse training material to cover new research domains including Geoscience, Political Science, and Biomedical Engineering. This expanded training material is being used to evaluate the proposed training technologies through training sessions for (a) students in a Statistics, Computer Science, Political Science, and interdisciplinary course, (b) attendees at three conferences targeted at Geoscientists, women, and Hispanics and Native Americans, respectively, (c) subjects in controlled lab studies, and (d) members of research groups at multiple institutes. The proposed qualitative and quantitative evaluation data gathered from these sessions are being used to assess not only the proposed technologies and training material, but also CyVerse and cyberinfrastructures in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750983","CAREER: Understanding and Combating Numerical Bugs for Reliable and Efficient Software Systems","CCF","Software & Hardware Foundation","07/01/2018","05/11/2020","Cindy Rubio Gonzalez","CA","University of California-Davis","Continuing Grant","Sol Greenspan","06/30/2023","$324,923.00","","crubio@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7798","1045, 7798, 7944, 9102, 9251","$0.00","The use of numerical software has grown rapidly over the past few years. From machine learning to safety-critical systems, a large variety of applications today make heavy use of floating point. Unfortunately, floating point introduces imprecision in numerical<br/>calculations. Analyzing, testing, and optimizing floating-point programs are difficult tasks. There is a large variety of numerical errors that can occur in such programs, including extreme sensitivity to roundoff, incorrectly handled exceptions, and nonreproducibility. This has led to numerous software bugs that have caused catastrophic failures. The goal of this research is to understand and combat numerical bugs. The intellectual merits are to advance the state of the art in analysis, testing and optimization of numerical software, and in extending these techniques to new domains beyond scientific applications. The importance of the research lies in the impact of the developed techniques and tools on improving the reliability and performance of real-world numerical programs, on which many other applications depend.<br/><br/>This research develops program analysis techniques and tools to (1) find frequent and impactful numerical bugs in programs, (2) propose<br/>fixes for these bugs, and (3) optimize numerical programs in different application domains to improve their performance. The research is driven by empirical studies that encompass several aspects of numerical software. First, a large-scale empirical study of numerical software is conducted to categorize real-world numerical bugs and their fixes. Second, an empirical study of test suites for numerical software is conducted to determine the effectiveness of testing in real-world numerical software. Based on the observations made through these empirical studies, a series of dynamic and static analyses are designed to detect and fix a variety of numerical bugs. These analyses are made available as part of an analysis and testing framework for numerical software. Novel precision tuning techniques are developed to enable scalable optimizations that lead to higher speedups, and to extend the scope of precision tuning to new application domains such as machine learning. The research has strong broader impacts in education and outreach. These include the development of new courses on software engineering and testing with a focus on numerical software, a Computer Science summer boot camp, and a mentoring program for underrepresented minorities especially focused on Latino students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839161","I-Corps: A Quantitative Approach to Detecting Meltdowns in Individuals with Autism Spectrum Disorder","IIP","I-Corps","07/01/2018","01/28/2019","Gloria Kim","FL","University of Florida","Standard Grant","Andre Marshall","07/31/2019","$50,000.00","","gloriakim@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will be the improvement of the quality of lives of both the individuals with Autism Spectrum Disorder (ASD) and their caregivers. The prevalence of autism has increased by 15% in 2014. Now, 1 in 59 children born in the US has ASD. This translates to roughly 5.5 million individuals in the US who are less likely to go to college or find a job after high school. Our user-friendly, discreet device can help affected individuals gain independence and create a supportive learning or working environment for them. Pre-emptive meltdown detection and care devices are virtually non-existent in the current marketplace. As the first to enter the market at an affordable price point, we expect to lead and spawn derivative products. Beyond autism, our platform technology can be utilized to push data to a healthcare professional, catalyzing informed and personalized preventive care. Other conceivable application is monitoring compliance and measuring the effectiveness of rehabilitation exercises for muscle engagement of stroke patients. Our innovation also has the potential to reach and assist those affected by other types of anxiety disorders. It can help people become more informed about their own mental health, intervene, and lead healthier lives. <br/><br/>This I-Corps project involves wearable technology for predicting, detecting, tracking, and mitigating meltdowns for individuals with Autism Spectrum Disorder (ASD). We have developed a biometric sensing suite comprising two components: The hardware portion consists of a detachable, water-resistant core unit for biosignal filtering, processing, and transmission as well as machine washable sensors that can be embedded into apparel; The software portion consists of our proprietary machine learning models for determination of when a meltdown is to occur, alerting the parent or caregiver of a high risk of a meltdown, and an assessment of mitigation techniques afterwards. Our innovation will advance autism research, as the technology has the potential to open the doors to a better understanding of behavioral phenotypes. Being able to tell what a non-verbal child is feeling can pave the way for more efficient and productive lessons for social/education development as well as broader phenotypical research. The assessment feature of our product will lead to the improvement of mitigation techniques for meltdowns. gaia's competitive advantage is in its pre-emptive nature of care. Proof-of-concept was demonstrated, and a beta prototype will be complete by the end of Summer 2018.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840740","I-Corps: Human Resource Optimization Software","IIP","I-Corps","07/01/2018","06/18/2018","Bart De Jonghe","PA","University of Pennsylvania","Standard Grant","Andre Marshall","05/31/2019","$50,000.00","","bartd@nursing.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project involves the optimization of human resource deployment via technology. More specifically, this endeavor leverages systems research to inform software with machine learning to inform how to organize work across industries and align that work with available human resources in a manner that optimizes outcomes. This technology can serve to benefit diverse industries; however, it offers unique value for healthcare settings which often have scare human resources deployed in rapidly changing environments. In clinical environments, how care is organized can have significant implications for cost, quality, and outcomes. This software will collect and analyze large volumes of data related to individuals and the environment to organize and assign work optimally (i.e. to improve clinical outcomes, decrease costs and increase efficiency). This technology has significant commercial potential beyond healthcare environments to any industry that routinely organizes and assigns work to a workforce where considerations of individual characteristics and environmental characteristics have significance.<br/><br/>This I-Corps project involves pursuits focused on better understanding the application potential of research-driven technology in various industries with a specific focus on inpatient clinical environments (i.e. hospitals). The technology automates and optimizes nurse staffing in hospital settings. It uses a data-driven algorithm informed by qualitative and quantitative research to make nurse-patient assignments that improve outcomes, save time, and reduce stress. The research driving this technology is inclusive of a review and synthesis of relevant nursing, health systems, and work science literature as well as qualitative research involving interviews of nurses and quantitative research assessing relationships between inpatient clinical work environments and patient outcomes. The technology integrates research data from nurses, patients, and hospital structural factors to inform algorithms driving nurse-patient matches. Machine learning elements of the software facilitate this optimization process. The software also serves as a mechanism for collecting a unique data set that records patient-nurse ?exposures? on the individual level and on an unprecedented scale. Such a data set is of great value to health services researchers, hospital quality improvement teams, and to our team as it seeks to improve and customize the algorithm with potential to decrease costs of care and improve patient and nurse outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1937978","CAREER: Quantifying diffusion and dynamics on healthcare, innovation and communication networks","IIS","Info Integration & Informatics","09/01/2018","07/30/2019","Edoardo Airoldi","PA","Temple University","Continuing Grant","Sylvia Spengler","08/31/2020","$104,853.00","","airoldi@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7364","1045","$0.00","Many modern data collections, gathered for the purpose of providing insights into matters of national interest such as medical and technological innovation, typically measure quickly evolving interactions, in addition to traditional unit-level measurements, in the context of a network. This project develops an integrated research and educational program to enable scientific and quantitative analyses of interactions and other combinatorial measurements as they change over time. Technical problems being addressed include, but are not limited to: an efficient representation that facilitates quantitative analyses of large-scale networks; models of how information and behavior evolve over time as a consequence of the network context they are embedded in; and fast algorithms to perform estimation of critical parameters in these models. These methods will be demonstrated on case studies exploring: the diffusion of medical innovations among physicians and its impact on health; technological innovation dynamics in the United States and the role of non-compete agreements; the estimation of point-to-point communications on a network, from aggregate traffic that is passively monitored.<br/><br/>The presence of interactions and other combinatorial measurements as a source of observed variation in the data creates new statistical and inferential challenges. For instance, generalized linear model theory needs to be extended to responses on a network. The analysis of processes on a network often induces constraints that make the inferential problems ill posed, since they involve a large number of unknown quantities to describe few observations. Estimation may require sampling from, and integrating over, extremely constrained parameter spaces. Importantly, interactions do not necessarily encode statistical dependence. In this sense, dealing with observed interactions requires original thinking; the data settings they entail are not amenable to analysis with classical methods, in which interactions are inferred as a means to encode dependence among unit-level observations. This project tackles technical challenges with a statistical and machine learning approach. Anticipated technical results include, but are not limited to: (1) a new wavelet decomposition of multivariate and dynamic networks; (2) statistical models of diffusion of information on a given network, and models of inhomogeneous network dynamics in continuous time; (3) scalable estimation algorithms for these models; and (4) theoretical foundations of inference with big data. This research will be evaluated qualitatively and quantitatively, at Harvard and in collaboration with industrial partners.<br/><br/>The proposed research is integrated with an interdisciplinary educational program, which will attract undergraduates to research at the intersection of statistics and computer science, in the context of problems of national importance. It will provide opportunities to actively encourage students from underrepresented groups to pursue careers in statistics and computer science. Key elements of the educational program include the development of a statistical machine learning curriculum; lectures on YouTube available to everyone; tutorials at national and international conferences and workshops; and a monograph. Outreach activities include open-source software and webtools for the community at-large, and a collaborative effort with industrial partners to leverage the new computational tools and algorithms for benefiting their pools of users worldwide. Additional details regarding the project can be found at: http://www.fas.harvard.edu/~airoldi/career.html."
"1822330","RESEARCH-PGR:  PanAnd - Harnessing convergence and constraint to predict adaptations to abiotic stress for maize and sorghum","IOS","Plant Genome Research Project","09/01/2018","08/21/2018","Edward Buckler","NY","Cornell University","Continuing Grant","Clifford Weil","08/31/2022","$2,727,270.00","Jeffrey Ross-Ibarra, Matthew Hufford, Elizabeth Kellogg, Adam Siepel","ed.buckler@ars.usda.gov","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","BIO","1329","1329, 7577, 9109, 9178, 9251, BIOT","$0.00","Maize, sorghum, sugarcane, and Miscanthus are the most productive and water efficient crops and biofuels in the world. This productivity is due to a shared physiology and genetic ancestry over the last 15 million years.  While these four crops will be extensively used, they are closely related to another 800 species that dominate grasslands across the world and are adapted to numerous environmental stresses including flooding, drought, heat, and frost. The project team will use modern genomics and machine learning to survey and analyze these related species, determining the most important genetic features they share that allow them to adapt to heat and drought.  The results of this work will be used by commercial and public sector plant breeders to make maize and sorghum more productive and resilient to extreme weather.  Key to this long-term impact is training the next generation of scientists in computational biology to address fundamental questions. These skills will be developed through hackathons and bioinformatics training workshops. The project will communicate this science to the general public through venues such as a traveling museum exhibit.<br/><br/>The Andropogoneae tribe of grasses contains a thousand species that collectively represent over a billion years of evolutionary history. It has used NADP-C4 photosynthesis and a wide range of adaptations to become a dominant clade on earth. This project will use the diversity and evolution across this tribe to understand the rules of adaptive convergence and constraint in plant genomes. The project team will sample and analyze the worldwide spectrum of genetic diversity in Andropogoneae to develop detailed models testing whether (1) quantitative estimates of evolutionary constraint improve predictions of fitness-related traits, and (2) convergent environmental adaptations shared across the Andropogoneae explain a substantial proportion of total adaptive variance. These hypotheses will be tested by assembling the gene and regulatory content of 57 species as well as whole genome sequencing of another 700 species. For eight species, diversity across their natural range of adaptation will be surveyed at the sequence level.  Evolutionary and machine learning models will be used to quantify the disruptive impact of a mutation in every ancestral genomic element.  The inter and intra-specific surveys will also permit an estimation of the prevalence of convergent evolution.   This project addresses two key elements of the genotype to phenotype problem - how to quantify the disruptive impact of mutations and how to determine whether adaptive solutions to environmental stresses are convergently shared across species.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827565","PFI-TT: Software for Automated Real-time Electroencephalogram Seizure Detection in Intensive Care Units","IIP","PFI-Partnrships for Innovation","08/01/2018","03/02/2020","Iyad Obeid","PA","Temple University","Standard Grant","Jesus Soriano Molla","01/31/2021","$215,999.00","Joseph Picone","iobeid@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","ENG","1662","1662, 8042, 9251","$0.00","The broader impact/commercial potential of this PFI project is that it will lead to improved clinical outcomes for neurological patients in intensive care units (ICUs). Although the data acquired using continuous electroencephalography (EEG) in the ICU is inexpensive to record and a rich source of information for guiding clinical decision making, it is often not used because it takes too long to be analyzed manually. The proposed technology will be capable of evaluating EEGs in real-time in order to alert doctors when clinically relevant events such as seizures occur. This will improve patient outcomes by allowing doctors to intervene with medications in a timelier and more precise fashion. This work will also have the broader impact of improving science's understanding of the fundamentals of how machine learning can be applied specifically to neural signal processing, which is currently a poorly understood area. <br/><br/>The proposed project will enable and accelerate the commercialization of software technology that detects seizures and abnormal brain activity in Intensive Care Unit patients. This will be accomplished with three main tasks. In the first task, the existing seizure detection software, which currently works offline, will be converted to work in real-time with a target latency of 20 seconds to detect a seizure. This will be accomplished through intelligent memory handling and by developing a low-latency, highly optimized post-processing algorithm. The second task will strengthen the existing seizure detection code to operate at clinically acceptable levels of sensitivity and false alarm rates. This will be achieved by retraining our algorithms on a significantly more diverse and complex EEG database in order to expose the software to as many variations of seizure presentation as possible. In the third and final task, extensive software testing will be conducted in order to optimize the machine learning configuration that maximizes the gains achieved in Tasks 1 and 2.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815525","NeTS: Small: A Scalable and Efficient Architecture for Exploiting Physical Layer Optics for High Performance Multicast in Data Centers","CNS","Networking Technology and Syst","10/01/2018","08/17/2018","T. S. Eugene Ng","TX","William Marsh Rice University","Standard Grant","Ann Von Lehmen","09/30/2021","$500,000.00","","eugeneng@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7363","7923","$0.00","As data centers scale up, there is an increasing need for fast data replication to distribute computational load across servers.   This is especially true of distributed machine learning algorithms, where a 'master computer' must distribute computational updates (i.e., do a multicast) to 100's or even 1000's of other servers. For example, for text mining, each iteration of a computation can easily result in more than a 1 Gigabyte of data being distributed, with the total exceeding 1 Terabyte. Unfortunately, the state-of-the-art solutions for transporting these massive data sets in the data center rely on application-layer overlays where the data is transmitted sub-optimally as a large number of repetitive flows. This project seeks to realize a fundamental breakthrough in data center multicast performance by leveraging optical transmission to perform the multicast function by simultaneously connecting a source machine rack to multiple receiver machine racks with a reconfigurable optical device. The results of this work will impact data analytics, which increasingly drives the US economy, through much more efficient operation of datacenters.   <br/><br/>This project develops a scalable architecture leveraging distributed small port-count optical cross connect switches and optical splitters to realize a data center network that efficiently supports both unicast and multicast traffic. To fully achieve the potential of this architecture, this project designs, analyzes and evaluates: (1) Scalable, high performance network connectivity for an optical splitter network, addressing difficulties resulting from heterogeneous packet switch port-counts, link speeds and splitter configurations; (2) Different routing algorithms and their performance trade-offs for different network connectivity and workload, addressing challenges such as failure recovery; (3) Different algorithms for multicast demand placement and optical multicast network resource scheduling that aim to optimize application level performance; (4) Different approaches to exploit the flexibility of the distributed small port-count optical circuit switches for adapting to dynamic shifts between unicast and multicast traffic hot-spots; (5) A variety of real big data applications operating over this new architecture, addressing the system design and implementation challenges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815718","SHF: Small: Architectural Techniques for Energy-Efficient Brain-Machine Implants","CCF","Software & Hardware Foundation","10/01/2018","08/23/2018","Abhishek Bhattacharjee","NJ","Rutgers University New Brunswick","Standard Grant","Yuanyuan Yang","04/30/2020","$465,995.00","","abhishek.bhattacharjee@yale.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798","7923, 7941, 8089, 9251","$0.00","This project focuses on the development of neural prostheses or brain implants to advance the scientific community's understanding of how the brain works, and to take a step towards devising treatment for neurological disorders. Brain implants are devices that are surgically embedded under the skull (of animals or humans in the context of scientific experiments and treatment of neurological disorders respectively) and placed on brain tissue, where they stimulate and record from hundreds of neurons. These devices are being used today to record neuronal electro-physiological data to unlock mysteries of the brain; to treat symptoms of Parkinson's disease, Tourette's syndrome, and epilepsy, with techniques like deep brain stimulation; and to offer treatment to those afflicted by paralysis or spinal cord damage via motor cortex implants. A key design issue with brain implants is that they are highly energy constrained, because they are embedded under the skull, and techniques like wireless power can heat up the brain tissue surrounding the implant. This project offers architectural techniques to lower the power consumption and energy usage of processing elements integrated on brain implants, whether they are general-purpose processors, customized integrated circuits, or programmable hardware. In tandem with its scientific studies, this project integrates an educational component to train high-school students, undergraduates, and PhD students on neuro-engineering techniques crucial to the society's continued efforts to shed light on how the brain works. <br/><br/>In terms of technical details, this project performs the first study on architectural techniques to improve the energy efficiency of embedded processors on implants by leveraging their existing low-power modes. Low-power modes can be used in the absence of interesting neuronal activity, which corresponds to periods of time when the implant is not performing useful work and the processor can be slowed down. A critical theme of this project is to show that hardware traditionally used to predict program behavior (e.g., branches or cache reuse) can also be co-opted to also predict brain activity, and hence anticipate interesting/non-interesting neuronal spiking. Such predictors can consequently be used to drive the implant processor in and out of low power mode. This project studies how to design hardware brain activity predictors that predict neuronal activity accurately, scalably, and efficiently, and how to integrate such predictors with low power modes on commodity embedded processors. The techniques are drawn from hardware machine-learning approaches for program prediction and consider neuronal spiking data extracted from brain sites on mice, sheep, and monkeys. Successful deployment of these approaches is expected to save as much as 85% of processor energy, effectively quadrupling battery lifetimes on implants being designed for mice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749857","CAREER: Something Old, Something New: Robust Statistics in the 21st Century","DMS","STATISTICS, Division Co-Funding: CAREER","09/01/2018","07/18/2020","Po-Ling Loh","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","08/31/2023","$233,130.00","","loh@ece.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 8048","1045","$0.00","Contemporary scientific problems involving large, high-dimensional datasets have ushered in a new era for statisticians. A largely unexplored area of research concerns incorporating ideas from robust statistics into the growing collection of methods for high-dimensional estimation and inference. Preliminary results hold much promise, but a plethora of theoretical and philosophical challenges abound when attempting to generalize notions from classical robust statistics to high-dimensional settings. The investigator will develop new statistical methodology for high-dimensional robust estimation and derive rigorous theory for the proposed estimators. This research project is highly interdisciplinary in nature, cutting across statistics, engineering, and computer science. Results of the research will be disseminated broadly, leading to cross-pollination between fields and revitalized interest in robust statistics. In addition, the investigator will refine and test her methods in radiology applications, instigating new scientific collaborations and leading to more robust medical imaging procedures for deployment in medical research. The investigator will also develop new educational material based on the research that will be incorporated into cross-listed classes in machine learning at the graduate and undergraduate levels. The investigator will work to improve the image of statistics and data science by engaging the wider community through public speaking engagements and visits to high school math circles across the state of Wisconsin.<br/><br/>Questions to be explored in this research project include: (1) How do existing notions of robustness apply to high-dimensional settings? (2) How should high-dimensional estimation procedures be modified to protect against deviations from distributional assumptions? (3) How might one quantify the relative robustness of various proposals? The project aims to generate novel theoretical results that advance the frontiers of both statistics and optimization theory. New algorithms will be devised for high-dimensional statistical estimation with guaranteed accuracy under a broader set of model assumptions. The theoretical analysis will involve studying a variety of non-convex estimators of independent interest in the optimization community, with emphasis on objectives and optimization algorithms that give rise to statistically consistent solutions. The research project will also address long-standing open questions in robust statistics involving optimization of low-dimensional non-convex objective functions, and the investigator will examine a variety of new problem settings arising in machine learning applications, including adversarially contaminated data, non-iid observations, and mislabeled datasets dichotomized into training and testing data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839250","RoL:FELS: Symposium: Evolutionary Biomechanics in the Era of Big Data, January 6, 2019, Tampa, Florida","IOS","Cross-BIO Activities","09/01/2018","08/06/2018","Martha Munoz","VA","Virginia Polytechnic Institute and State University","Standard Grant","Kathryn Dickson","08/31/2019","$21,868.00","","martha.munoz@yale.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","BIO","7275","068Z, 1228, 7556","$0.00","This award funds a one-day conference at the 2019 Annual Meeting of the Society for Integrative and Comparative Biology, to advance research and develop new collaborations in the interdisciplinary field of evolutionary biomechanics. Imagine going for a stroll in a nearby park. Near the ground, a fungus slowly collects water so it can ballistically launch its spores into the air. Nearby, falling maple seed pods create air vortices, allowing them to remain aloft during dispersal. Up above, swifts and hawks scan the ground for food, their wings differently shaped for either maneuvering or speed. What do the fungus, maple tree, and birds have in common? Their motion - all biological motion, in fact - is subject to the laws of physics. Evolutionary biomechanics is the study of how the physical rules of motion shape the evolution of biodiversity, a field that is undergoing a renaissance due to major advances in imaging technology, machine learning approaches, and molecular phylogenetics. Novel discoveries in evolutionary biomechanics have important applications, including those in engineering and biologically-inspired robotic design. The supported one-day symposium highlights emerging trends in the field of evolutionary biomechanics. Undergraduates, graduate students, and postdoctoral researchers at the conference will obtain advice for pursuing diverse career opportunities,  The major goal of the symposium is to galvanize new research and foster new collaborations in biomechanics. <br/><br/>All biological motion is guided by the laws of physics. The major features of evolutionary fitness - locomotion, feeding, and reproduction - are guided by mechanical laws. Thus, the physics of movement and the evolution of biodiversity are deeply connected. One of the major unanswered questions in biology is how causal and predictive the relationships are between mechanics and phenotypic diversity. This award will support a one-day symposium at the 2019 meeting of the Society for Integrative and Comparative Biology to highlight novel trends and discoveries in evolutionary biomechanics. Emerging leaders (of various backgrounds and ranks) in the field will discuss how they are using cutting-edge methods to improve our understanding of phenotypic evolution. New imaging methods allow the rapid generation, storage, and analysis of scans and videos. Machine learning techniques and crowd-sourcing platforms are accelerating the pace of data collection and pattern detection. Evolutionary analysis is being facilitated by Next Generation Sequencing and advances in comparative phylogenetics. Combined, these developments are rapidly elucidating the governing principles that causally, and predictably, link physics to phenotypic diversity. Students will be paired with symposium speakers in small groups, with the explicit goal of helping students plan their next career steps and forge new research collaborations. This award may help build an emerging interdisciplinary research network and encourage a new generation of scientists in evolutionary biomechanics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837120","FMitF: Collaborative Research: User-Centered Verification and Repair of Trigger-Action Programs","CCF","FMitF: Formal Methods in the F","09/01/2018","08/28/2018","Blase Ur","IL","University of Chicago","Standard Grant","Nina Amla","08/31/2022","$666,666.00","Shan Lu, Ravi Chugh","blase@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","094Y","062Z, 071Z","$0.00","Modern data-centric systems, ranging from Internet-of-Things devices to online services, can benefit from helping people make clear their intent for how their devices and services should behave and interact with each other.  Generally, this requires people to engage in some amount of end-user programming, or programming by people who are not typically trained in programming.  Common examples of this include specifying that a light should only turn on when a room is occupied or that emails with certain words in the subject line should be routed into a particular folder.  Trigger-action programming (TAP), which consists of ""if-this-then-that"" rules, is the most common model for end-user programming because it is relatively easy to write simple TAP programs.  However, as the number and complexity of both rules and devices increases, TAP programs increasingly suffer from bugs and dependability problems and are hard to correct for inexperienced and trained programmers alike.  This project's goal is to make TAP programming, and thus people's ability to interact with devices that act on their behalf, more robust through developing a better understanding of end users' needs and abilities to write and debug TAP programs, computational techniques to both better model user intents and suggest TAP programs that meet them, and tools that use those techniques to help people more easily create correct TAP programs.  Apart from the potential benefits to people's well-being, the project will also provide educational benefits by developing course materials that increase awareness of both human aspects of, and formal methods for, programming.  Further, the tangible nature of such devices and the familiarity of popular online services are a fertile domain for engaging the public and training undergraduate students, K-12 students, and early-career graduate students in the computer science research lifecycle.<br/><br/>To accomplish these goals, the work combines techniques from formal methods, human-computer interaction, and machine learning. Contributions to formal methods include the design of systematic solutions to unique program repair, synthesis, and specification-refinement problems in the context of end-user programming. Contributions to cyber human systems include empirical studies and the design of data-driven interfaces for more accurately expressing intent. Specifically, the empirical human subjects studies seek to understand and improve the debugging process for trigger-action programming, create and distribute needed data sets of user-centric collections of trigger-action programs, and comparatively evaluate proposed interfaces. The interfaces developed in this work use data-driven methods to help users pinpoint and understand bugs in trigger-action programs, as well as to choose among candidates for automatically repaired trigger-action programs. Underlying these interfaces will be formal models of trigger-action programs, which are verified against specified properties written in linear temporal logic. The system developed will systematically synthesize program repairs, taking into account users' experiences and preferences. The system will also use a combination of machine learning and formal methods to automatically generate trigger-action programs and summarize specifications based on historical traces of user interaction with the system. In sum, helping non-technical users accurately communicate their intent through trigger-action programming benefits widely deployed end-user-programming systems for integrating internet-connected devices and online services.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827700","PFI-RP: A Multi-Disciplinary Approach to Detecting Adolescent Online Risks.","IIP","GOALI-Grnt Opp Acad Lia wIndus, PFI-Partnrships for Innovation","09/15/2018","06/18/2020","Pamela Wisniewski","FL","The University of Central Florida Board of Trustees","Standard Grant","Jesus Soriano Molla","02/28/2022","$821,000.00","Sriram Chellappan, Munmun De Choudhury, Karl MacMillan","Pamela.Wisniewski@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","ENG","1504, 1662","019Z, 116E, 1662, 8042, 9251","$0.00","The broader impact/commercial potential of this PFI project is to develop improved social media tools to keep teens safer online. A multidisciplinary team of researchers, clinicians, and industry partners will build, evaluate, and commercialize state-of-the-art technologies that proactively detect adolescent online risk behaviors, including mental health issues, sexual solicitations, and online harassment, for the purpose of mitigating these risks and preventing harm. The team plans to release two separate technology offerings: First, an open source project that shares our risk detection algorithms with developers, so that they can build upon these solutions and form a community dedicated to promoting adolescent online safety. Second, a commercial product that combines these risk detection algorithms into an easy-to-use and accessible service that provides needed support and infrastructure to internet-based companies, who ultimately facilitate the risks teens encounter online, so that they can share in the joint responsibility of protecting the teens who use their platforms. It is imperative that as a society we become more proactive about protect teens from online risks, especially the most vulnerable teens who are at highest risk of engaging in online activities that can lead to physical harm or even death.<br/><br/>The proposed project addresses the critical and timely problem of adolescent online safety by leveraging a multi-disciplinary approach of human-centered machine learning to accurately detect risks teen encounter online. The work will be accomplished in four stages: 1) a user-centered phase that examines the types of online risk exposure that matter most to teens and their families. With this insight, the team will build evidence-based adolescent online risk models that identify key dimensions and patterns of risk. 2) a data-driven phase that draws upon these models to establish ground truth and annotate a rich training set of teen social media data, 3) a machine-learning phase that improves existing and builds new risk detection algorithms that are contextualized to teens and able to detect behavioral changes over time, and 4) a technology development phase that evaluates our solution and commercializes it within two separate product lines. The end result will be a suite of algorithms tailored to adolescents that detect objectionable content within social media and identify problematic behavioral patterns or changes that are indicative of impending risks over time. This work will enable real-time online safety interventions that protect and empower teen internet users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842655","RAISE-EQuIP: Chip-Scale Quantum Memories for Practical Quantum Communication Networks","ECCS","SSA-Special Studies & Analysis, CCSS-Comms Circuits & Sens Sys","10/01/2018","09/17/2018","LEE BASSETT","PA","University of Pennsylvania","Standard Grant","Dominique Dagenais","09/30/2021","$750,000.00","Rashid Zia, Firooz Aflatouni","lbassett@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","ENG","1385, 7564","049Z, 057Z, 093E, 094E, 095E, 096E, 100E, 106E","$0.00","RAISE-EQuIP: Chip-Scale Quantum Memories for Practical Quantum Communication Networks<br/><br/>This project is conceived in the context of the NSF's call for Research Advanced by Interdisciplinary Science and Engineering (RAISE), and specifically a Dear Colleague Letter for Engineering Quantum Integrated Platforms for Quantum Communication (EQuIP). It addresses a grand challenge of 21st-century science: leveraging modern capabilities in materials science, nanofabrication, signal processing, and integrated systems-on-a-chip to harness the computational power and sensitivity of quantum-coherent systems for practical applications. Motivated by the clear potential of spin-based quantum devices, this RAISE-EQuIP project adopts an engineering approach to address a series of technological roadblocks that currently limit their performance and scalability. The interdisciplinary approach harnesses state-of-the-art classical and quantum signal processing, electronic circuit design in silicon-based integrated platforms, machine learning optimization, and nanophotonic design, with the aim to transform spin-based quantum registers from a laboratory-scale experiments into compact, integrated systems that are available to power new applications and scientific investigations. With superior performance offered under real-world constraints, these devices can be deployed in testbed quantum communication networks and will enable future investigations of fundamental quantum physics. The collaborative project will engage many undergraduate and graduate students from diverse backgrounds; its research goals are coupled with a broad educational mission to educate students and the public about the emerging field of quantum science and technology. Through the realization of compact, robust, low-cost quantum devices, this project will support the design and deployment of hands-on activities for K-12 students and the public about spins, photons, and quantum communication, for use at venues that target large, diverse populations in Philadelphia, PA and Providence, RI.<br/><br/>Clusters of nuclear spins coupled to an optically addressable electron-spin qubit such as the nitrogen-vacancy (NV) center in diamond are leading platforms for quantum communication. The cluster constitutes a register of qubits that can be individually addressed, entangled, stored for times exceeding 1s, and utilized for quantum error correction. However, state-of-the-art experiments are currently performed on laboratory-scale setups consisting of customized optical cryostats, vibration-sensitive free-space optics, and racks of microwave electronics. Performance is further impeded by sub-optimal photon collection efficiency and labor-intensive calibration requirements for quantum control sequences. This RAISE-EQuIP project will tackle these challenges on multiple levels, drawing on complementary expertise of the collaborating researchers in diamond NV quantum control and device engineering (Bassett), high-speed analog circuit design and signal processing (Aflatouni), and computational physics and nanophotonics (Zia). We will design and build compact, fiber-coupled diamond devices featuring nanofabricated optical metalenses and impedance-matched microwave antennas to transmit optical and spin-resonance signals, respectively, and integrate these devices with custom-fabricated silicon CMOS chips that process the necessary analog and digital signals for spin resonance, photon counting, and real-time adaptive feedback control. Computational machine learning methods will enable efficient mapping and control of the unknown coupled-spin Hamiltonian. The resulting quantum-register devices will exhibit performance superior to state-of-the-art laboratory systems, but with a fraction of the size, cost, and energy requirements. Components of the modular, hybrid-integrated system are generalizable to other quantum architectures based on spins, ions, photons, and superconducting qubits, so these devices can serve as a framework for future generations of portable quantum technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755791","CRII: AF: RUI: Faster and Cache-Efficient Similarity Filters and Searches for Big Data","CCF","Algorithmic Foundations","02/15/2018","02/15/2018","Mayank Goswami","NY","CUNY Queens College","Standard Grant","A. Funda Ergun","01/31/2021","$175,000.00","","Mayank.Goswami@qc.cuny.edu","65 30 Kissena Blvd","Flushing","NY","113671575","7189975400","CSE","7796","7796, 7926, 8228","$0.00","Analysing and organising the massive amount of data collected everyday is one of the biggest challenges faced by computer scientists. This impacts almost every field of modern human life, such as health care, military applications, smart cities, transportation, networks, etc. To quickly analyse and organise such huge amounts of data, one needs to develop space and time efficient algorithms, that accelerate the search for information while minimising memory requirements. This project aims to develop space-efficient similarity filters, which quickly filter out queries that have very little similarity from the records existing in the database. This has applications in security, databases, machine learning, file systems, vision, pattern recognition, compression and networks. Apart from the targeted impact of these applications to society, this project also aims at producing the next generation of researchers and promoting under-represented groups in science and technology. Minority undergraduate and high school students will be involved in this project. Queens College, CUNY is a diverse institution, representing a wide range of ethnic minorities and has 28% Hispanic students. In an effort to engage more undergraduate students and women in research (Queens College has roughly 56% female students), the PI plans to offer a new course that develops an understanding of approximate membership and similarity search data structures in the setting of big data.<br/><br/><br/>Similarity searching is a fundamental problem in this context, where a massive data of records needs to be organised so as to answer quickly queries of the form: given a record, is it similar to some record in the data? Similarity search, modeled as near(est) neighbor search (NNS) is an important and well-studied problem with numerous applications: Given a set P of n points lying in some d-dimensional metric space, the goal is to preprocess P so as to return for a given query point q the point p in P that is closest to q. Exact versions of this problem are hard to solve, and all solutions to the approximate version of the problem require superlinear (more than nd) space. This space usage is prohibitive for many big data settings, where not just n, but d could be huge (e.g., the number of pixels in an image). This project is aimed at developing the theory and applications of Similarity Filters, which are decision (yes/no) based data structures. Given a query q, a (c, r)-similarity filter always returns yes if q is within a distance r of some input point, and returns no with a small error probability (thus some false positives are allowed) if all input points are at least cr away from the query. Recent results from the researcher shows that such filters can be built with sublinear (asymptotically less than nd) space. The project takes on two tasks: 1) to understand the approximation factor-space-query tradeoff for similarity filters, developing tight upper and lower bounds, especially in the sublinear space regime, and 2) to develop I/O efficient Similarity Filters and nearest neighbor data structures, akin to quotient or cascade filters for the approximate membership problem (where the in-RAM Bloom Filter is the standard data structure, used in many networking applications). The overarching goal is to use similarity filters on RAM in conjunction with a disk-based data structure: faraway queries will be filtered quickly, and nearby queries will have their neighbor(s) returned following a slower but cache friendly search. The availability of such a two-layered space-and-time-efficient data structure will greatly boost the scope of applications when the amount of data is massive, especially in databases, networks, security and machine learning."
"1737591","SCC-IRG Track 1: Sociotechnical Systems to Enable Smart and Connected Energy-Aware Residential Communities","CNS","S&CC: Smart & Connected Commun","01/01/2018","08/30/2017","Panagiota Karava","IN","Purdue University","Standard Grant","Sandip Roy","12/31/2021","$3,581,912.00","Torsten Reimer, Leigh Raymond, James Braun, Ilias Bilionis","pkarava@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","033Y","042Z","$0.00","This project aims to develop a new paradigm for smart and connected residential communities that engages inhabitants in understanding and reducing their home energy use while increasing their environmental awareness, responsiveness to collective goals, and improving their quality of life. The research will lead to discoveries concerning how individuals, groups, and residential communities make decisions related to their home energy consumption. Based on this knowledge, this project will develop feedback mechanisms integrated into user-interactive smart devices to enable optimal energy management. The Indiana Housing and Community Development Agency, several industry stakeholders and various community action groups will be engaged in this work throughout the lifetime of the project. Smart and connected (S&C) technology will be implemented in several hundred households in multiple residential communities that will be used as research test-beds and will cover a wide range of demographics, locations, and construction. The research outcomes will be integrated in teaching modules that support curriculum and workforce development as well as capacity-building in engineering, social and economic science, and polytechnic schools at Purdue. Through sociotechnical research advances, community engagement, and dissemination, this project will create a national model for ""S&C energy-aware residential communities"" in the housing sector, and by example point the broader research community toward S&CC research frontiers that enhance community functioning and national prosperity.<br/><br/>Fundamental advances in machine learning and mechanism design, along with integrative research in human-machine interaction, behavioral and social sciences, and building energy systems will lead to discoveries that challenge our current understanding of behavior and response to feedback both at the individual and community-level. Given the large population size and the range of learning and community-based feedback mechanisms along with audio/visual end-user systems, the findings pertaining to the use of customized feedback and S&C technology to influence behavior will lead to general principles of human behavior that can be transferred to domains beyond energy use. The researchers will establish a new sociotechnical modeling approach based on Bayesian multi-scale clustering algorithms and game-theoretic models that will impact multiple disciplines and research activities in different S&CC application domains, such as energy, water, transportation, economic development, environmental quality, and urban planning."
"1830782","I-Corps:  Low False Negative 3-D Facial Recognition","IIP","I-Corps","04/01/2018","04/04/2018","Thomas Sterling","IN","Indiana University","Standard Grant","Andre Marshall","09/30/2019","$50,000.00","","tron@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is the expansion of facial recognition technologies into the emissive infrared bands and the reduction of false negatives in the presence of large facial pose variation.  Visible 2-D image-based machine learning techniques dominate the commercial facial recognition landscape while heterogeneous facial recognition techniques including 3-D to 2-D matching and infrared to visible matching have not seen similar widespread commercial deployment.  The patented technology originates from an NSF Award and brings a crucial algorithmic component for commercial deployment that enhances heterogeneous facial recognition techniques while not relying on machine learning nor its concomitant large training database requirements.  This in turn opens up the potential for improved automated surveillance against a given watch-list deployable in a host of public and private spaces and under a wider range of surveillance conditions than currently possible using 2-D commercial matching algorithms.<br/><br/>This I-Corps project is a result of technology developed under an NSF Award which enables a radical improvement in 3-D facial recognition by reducing the computational costs for facial implicit surface generation and biometric capture originating from multispectral (visible and infrared) sources.  The technology results in robust recognition in the presence of large facial pose variation and occlusion using 3-D facial implicit surfaces.  The improvement over existing 2-D commercial facial recognition algorithms is evident under conditions typical of automated surveillance where large facial pose variation and poor lighting result in false negatives using conventional practice.  The method and apparatus for 3-D facial recognition underlying this research, however, performs equally well under large facial pose variation and is able to seamlessly integrate multiple band infrared input along with visible image input for vastly lower false negative facial recognition even in the presence of low-light and nighttime conditions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834332","Collaborative Research: Reliable Materials Simulation based on the Knowledgebase of Interatomic Models (KIM)","DMR","DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY, CDS&E","10/01/2018","08/27/2018","Mark Transtrum","UT","Brigham Young University","Continuing Grant","Daryl Hess","09/30/2022","$283,929.00","","mktranstrum@byu.edu","A-285 ASB","Provo","UT","846021231","8014223360","MPS","1712, 1765, 8084","013E, 026Z, 027E, 054Z, 062Z, 7726, 8084, 9216, 9263","$0.00","NONTECHNICAL SUMMARY<br/>This award supports OpenKIM, a cyberinfrastructure component of the research community that uses computer simulations of atoms based on Newton's Laws and models for the interaction between atoms, to attack problems in materials science, engineering, and physics, and to enable the discovery of new materials, design new devices, to advance the understanding of materials-related phenomena, and much more. Recent years have seen significant advancement in the areas of materials knowledge, discovery, and manufacturing methodologies.  This includes, for example, the development of graphene (a single atomic layer of carbon atoms, which has exceptional mechanical, thermal, and electrical properties) and the related class of two-dimensional materials with unprecedented material properties now being extensively studied by scientists and engineers.  Another example is the advent of three-dimensional printing techniques that allow engineers to design new materials from the ground up that can be tailor-made for their specific application.  Computer simulation of materials at the atomic-scale is one of the key enabling technologies driving the current materials revolution.  Although the most accurate atomic-scale simulations employ the equations of quantum mechanics, such computations take so long to complete, even on today's powerful computers, that practically they are limited to a few thousands of atoms.  This is simply not enough for the study of materials properties, which requires the simulation of interactions between millions and even billions of atoms.  Thus, materials researchers rely on faster more approximate equations, known as interatomic models (IMs), to describe atomic interactions.  These models are fast, but typically they are only accurate for a restricted range of material properties.  This limited range of applicability necessitates the creation of many IMs, even for a single material such as silicon.  Organizing, sharing, and evaluating the range of applicability of these IMs has been a long-standing challenge for the materials research community.  In most cases researchers have no way of knowing which IM is suitable for their particular application. Further, the proliferation of IMs, often designed to work only with specific simulation programs, makes it difficult to share and exchange IMs, and to reproduce other researchers' work, which is how science evolves and self corrects.<br/><br/>The Knowledgebase of Interatomic Models (KIM) is a project that is working to solve these challenges.  To date, the KIM project has developed an online framework at https://openkim.org to address the issues of IM provenance, selection, and portability.  IMs archived on this website are exhaustively tested and can be used in plug-and-play fashion in a variety of major simulation codes that conform to a standard developed as part of the KIM project. The development activity of the current project will extend the KIM framework by broadening the number and types of supported IMs, and will add new capabilities and educational resources that will make it easy for researchers to integrate the IMs and materials data available on openkim.org into their daily research workflow. Further, emerging techniques in information topology and machine learning will be applied to study and quantify the inherent uncertainty in predictions made by IMs, and to assist materials researchers to select the best IM for their application.  Together the development, educational, and research activities of this project are expected to significantly increase the userbase and broader impact of the KIM project.  <br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports OpenKIM, a community Knowledgebase of Interatomic Models (KIM) for simulation. KIM is a project for normalizing the use of IMs in molecular simulations of materials.  An IM, often referred to as a ""potential"" or ""force field,"" is an approximate method for computing the energy and its derivatives for an atomic configuration.  This project addresses both traditional ""physics-based"" IMs and the new class of ""data-driven"" IMs introduced in recent years. In a sustained effort, the KIM project has developed a systematic framework to address the IM provenance, selection, and portability problems faced by materials researchers.  Before KIM, these challenges were the cause of significant inefficiencies and inaccuracies in the research pipeline.  Today, an IM available on openkim.org is subjected to a rigorous set of ""Verification Checks"" that aim to ensure that its implementation conforms to a high software-engineering standard, and to an extensive set of ""Tests,"" each of which computes a well-defined material property for assessing the IM's accuracy.  A researcher can come to openkim.org and explore the predictions of KIM Models in comparison with experimental or quantum ""Reference Data"" to select a suitable IM for their application. The current project is aimed at extending KIM to become an integral component of the workflow of researchers engaged in molecular simulation to make their work more efficient and their results more reliable and reproducible.  To achieve this vision, the Principal Investigators (PIs) will pursue the following program of cyberinfrastructure R&D and basic research related to IM usage and science.  The cyberinfrastructure R&D will include extensions to KIM standards to support additional common IM features (such as long-range fields) and added support for IMs having cutting-edge features that cannot yet be standardized.  Further, KIM will be integrated into existing simulation tools so that researchers may query and retrieve data archived on openkim.org as part of their daily workflow.  This approach reduces errors, ensures reproducibility, uses a standard tested method (embodied in a KIM Test) to obtain the desired property, and firmly integrates the KIM framework into the workflow of computational materials researchers.  The basic research component of the project includes three research thrusts requiring advances to enhance the reliability of molecular simulations: (1) IM Uncertainty: The PIs will use ideas from information topology and differential geometry to automatically generate IM ensembles for obtaining estimates of the inherent uncertainty of the IM. (2) IM Transferability: The PIs plan to adapt a multi-task machine learning approach to predict an IM's accuracy for different applications.  This will lead to a rigorous, objective criterion to assist researchers with IM selection. (3) IM Heuristics: By mining IM predictions and Reference Data archived on openkim.org, it is possible to identify correlations similar to empirical heuristics such as Vegard's rule and connections between microscopic properties and macroscopic features.  Detection of such heuristics will provide insights into the limitations of IMs, help design optimal training sets, and lead to better understanding of the properties of IMs generally.  In terms of broader impacts, the scope of the KIM project is unusually large - far beyond materials science - due to the prevalence of molecular simulations across the physical sciences from microbiology to geology.  The project aims to maximize its impact by (1) expanding the KIM user base, (2) engaging the materials research community directly and through targeted research and educational efforts, and (3) developing new relationships and collaborations with other materials modeling cyberinfrastructures and organizations.<br/><br/>This award is jointly supported by the Division of Materials Research in the Directorate for Mathematical and Physical Sciences and the Civil, Mechanical and Manufacturing Innovation Division in the Engineering Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812904","Design of Gradient-Based Methods for Solving General and Huge Convex Optimization Problems","DMS","APPLIED MATHEMATICS","08/15/2018","08/14/2018","James Renegar","NY","Cornell University","Standard Grant","Pedro Embid","07/31/2021","$316,798.00","","renegar@orie.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1266","","$0.00","Optimization modeling, and algorithms for solving the models, have been key to gains in efficiency in the US economy since the late 1940's.  The relevance of optimization has increased manyfold alongside advances in computer design, and recently alongside the availability of vast and complex datasets arising from internet applications.  Optimization has become a central part of machine learning.  However, even the most efficient optimization algorithms are unable to succeed for complicated models populated by huge datasets possessing little special structure, the main problem being the limited core memory available on (even large) computers.  While Moore's Law accurately predicted exponentially-increasing computing power, the surprise has been that the sizes of datasets are increasing much faster.  A central focus of the project is the design of algorithms capable of solving a complicated optimization model populated with a huge dataset, by breaking apart the model into a sequence of computational problems, each relying on only a portion of the data, not too much for core memory.  Graduate students participate in the research.<br/><br/>The general approach makes use of the most elemental of algorithms for convex optimization, namely, subgradient methods, dating to the 1960's.  In tandem, the approach makes use of -- and advances -- a framework promoted by the investigator in recent years, whereby a general convex optimization problem is transformed into an equivalent convex optimization problem whose only constraints are linear equations and for which the objective function is Lipschitz continuous (thereby allowing direct application of subgradient methods).  Exploration is being done initially for linear programming, an ambitious goal being to solve problems even beyond the reach of the simplex method (in cases where the basis-inverse matrix is larger than core memory permits).  Focus also is being given to problems involving an objective function that is itself the sum of many functions, a common setting in machine learning.  Here the goal is to devise algorithms that are able to choose the summand functions in a principled (and efficient) manner, unlike incremental (sub)gradient methods, which choose a summand uniformly at random.  Additionally, attempts are being made to extend the investigator's framework so as to provide, for example, a way to transform a continuously-differentiable objective function, possibly with bounded domain, into an entire function possessing Lipschitz-continuous gradient, thereby allowing accelerated methods to be applied easily.  A particularly important aspect of the project is the design of practical schemes for speeding up first order methods when the optimization problem being solved has some particular kind of geometrical structure (such as ""sharpness,"" where the objective function grows linearly with the distance to optimality).  The goal is to design schemes that require no knowledge of parameters governing the geometrical structure, and yet that are guaranteed to achieve optimal speedup whenever the structure is present (regardless of whether the user knows the structure is present).  Graduate students participate in the research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822221","Advancing Sub-Seasonal Weather Predictability Through Machine Learning Techniques","AGS","Climate & Large-Scale Dynamics","09/01/2018","06/04/2018","Timothy Delsole","VA","George Mason University","Standard Grant","Eric DeWeaver","08/31/2021","$459,667.00","","tdelsole@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","GEO","5740","","$0.00","Current operational weather forecasting systems can produce valuable predictions of day-to-day weather, but such predictions are only skillful up to a week or so in advance.  On the longer subseasonal timescale, say between two and eight weeks, useful forecasts of mean conditions and the likelihood of extreme events may also be possible. But basic science questions, including the sources and mechanisms of subseasonal variability, their potential predictability, and the essential elements necessary for robust prediction, have not yet been resolved.  These questions are of practical as well as scientific interest, as guidance from subseasonal forecasts could have a variety of uses including agricultural planning and emergency management.<br/><br/>This project seeks to identify empirical predictive relationships between local climate variability of interest and large-scale patterns in relevant predictors such as sea surface temperature (SST), soil moisture, and atmospheric circulation. Prior work by the Principal Investigator (PI) and colleagues demonstrated such a relationship between heat waves in Texas, including the heat wave associated with the 2011 drought, and an SST pattern covering much of the North Pacific.  A key limitation to such prediction methods is that the observed record is too short to identify statistically significant predictive relationships.  Thus methods which seem successful when tested on past cases may fail when used for realtime prediction. The PI's strategy for circumventing this limitation is to identify predictive relationships using output from weather and climate models in place of observations, as many thousands of years of simulated weather and climate variability are available from a variety of modeling projects. Model output used here comes from the North American Multimodel Ensemble (NMME), the Subseasonal to Seasonal (S2S) Prediction Project, and the Coupled Model Intercomparison Project (CMIP).  <br/><br/>A further concern in developing empirical prediction methods is the need for regularization, meaning a way to eliminate spurious small-scale features in predictor patterns which arise due to the large number of data points used to represent the predictor fields. Such features are not usually consistent from model to model or between model output and observations.  The PI uses a regularization scheme in which eigenvectors of the Laplacian operator serve to factor out small scales, leading to more robust predictive relationships.  To further ensure robust predictions, the PI applies an innovative cross validation technique which bypasses the best statistical model identified in cross validation in favor of the simplest model which is within a standard deviation of the best model.<br/><br/>Once robust predictive relationships are identified that hold across different models and in observations, the sources and mechanisms responsible for the relationships will be explored.  If the empirical methods can reproduce hindcasts from the models in the NMME archive then the model output can be used to understand the time-evolving dynamical processes linking the predictor pattern to the predicted variability.<br/><br/>In addition to the societal benefits of subseasonal forecasts, the project provides support and training to a postdoc, thereby promoting workforce development in this research area. The project also addresses public scientific literacy through public  seminars by the PI on sub-seasonal prediction, a topic of interest to the general public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746461","SBIR Phase I:  Development of An Accurate Low-Cost Wearable Ultraviolet Dosimeter For The General Population","IIP","SBIR Phase I","01/01/2018","12/27/2017","Emmanuel Dumont","NY","YouV Labs","Standard Grant","Rick Schwerdtfeger","07/31/2018","$225,000.00","","em@shade.io","476 Central Park West apt 4C","New York","NY","100258913","9174101191","ENG","5371","5371, 8033, 8034","$0.00","The broader impact of this Small Business Innovation Research (SBIR) Phase I project is to equip consumers with a scientific tool to measure and control their exposure to ultraviolet (UV) radiation, thereby mitigating their risk of getting skin cancer while enjoying the benefits of sunlight such as Vitamin D and outdoors activities. In the United States, skin cancer has become a major public health issue with an estimated 3.5M people being treated each year for a cost of over $8B. The rate of Vitamin D deficiency in the US has been estimated to be over 40% leading to increased risk for depression, cardiovascular disease, and cancer. Up until now, accurate measurement of ultraviolet exposure remains confined to research laboratories. This project aims at carrying a scientific breakthrough in UV dosimetry and bring a laboratory-grade technology to consumers.<br/><br/>The proposed project aims to achieve a scientific breakthrough in UV dosimetry by combining several detectors, machine learning algorithms, and customized calibration. We anticipate that this breakthrough will lead to a drastic improvement in accuracy to closer match that of laboratory-grade equipment, while keeping the size and cost of our instrument in line with consumers? expectations. The scientific challenge is to have the UV sensor be accurate when it measures most solar spectra, and there is an infinity of these based on location, weather, and time of the year. Our strategy is to stay away from incremental improvements (e.g. filter optimization) or expensive developments (e.g. a full-blown spectrometer). Instead, we are testing the hypothesis that the combination of machine learning algorithms and a small set of carefully chosen detectors will enable us to build a small low-cost instrument able to identify the local solar spectrum and provide correctly calibrated real-time measurements. To achieve this, we propose to apply clustering techniques to find representative spectra of solar UV and train several detectors to recognize them and correct the measurement. The execution of this project requires top-level R&D among diverse collaborators, whom we have gathered for this project."
"1821942","I-Corps: A novel diagnostic method of detecting eye diseases using a smartphone","IIP","I-Corps","04/01/2018","01/25/2018","Jo Woon Chong","TX","Texas Tech University","Standard Grant","Ruth Shuman","09/30/2020","$50,000.00","","j.chong@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is in the development of smartphone-based eye disease detection technology for potential clients and partners. Potential commercial clients for the technology include 1) ophthalmologists and eye clinics, 2) medical centers, 3) customers living in rural or suburban areas with limited medical services, 4) senior living and retirement communities, 5) health insurance companies, and 6) ophthalmic instrument manufacturers. Eye diseases are usually detected in clinics with ophthalmic devices, e.g. optical coherence tomography, corneal topography and slit lamp, which are large, expensive and not portable, and need to be operated by trained technicians. However, our proposed smartphone-based eye disease detection method is small, affordable, portable, and it can be operated by patients in a convenient way, which will overcome the limitations mentioned above. The proposed algorithm can detect eye diseases or monitor eye healthiness in a proper and timely manner, and it can share the monitoring information with ophthalmologists. Hence, eye diseases can be detected in the earlier stage with our technology.  Moreover, ophthalmologist can focus more on severe treatments or surgeries. A 5- to 10-fold lower cost is an added potential benefit. <br/><br/>This I-Corps project will explore the commercial potential of a new eye disease detection technology using a smartphone and make it broadly available for scientific discovery and medical applications. The project further develops a smartphone-based eye disease detection technology that is more accurate and convenient using image processing and machine learning techniques. The proposed smartphone-based eye disease detection technology makes use of panoramic images or short video recordings of the eye at different angles. The recorded eye images are divided into iris, lens, sclera, and cornea components using automatic image cropping techniques. Each divided component is then analyzed using shape detection and color matching techniques to recognize the shape and to detect the abnormality of each component, respectively. Preliminary finding shows that the proposed technology detects keratoconus with more than 90% accuracy from 30 subjects. To detect diverse types of eye diseases and to increase the accuracy of eye disease detection, we will collect additional eye data from patients and normal subjects to form a larger database, and apply machine learning techniques to the accumulated database. Bringing these innovative capabilities to the commercial market will significantly improve discovery output in academia and industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812197","Decision Theoretic Bayesian Computation","DMS","STATISTICS","08/01/2018","08/13/2019","Vinayak Rao","IN","Purdue University","Continuing Grant","Gabor Szekely","07/31/2021","$98,345.00","Harsha Honnappa","varao@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1269","","$0.00","Decision-makers, whether in business, policy-making, or engineering systems, face the problem of taking action without complete knowledge of the state of the world. Examples of such situations include controlling industrial plants, maneuvering autonomous vehicles, developing new drugs, making investment decisions or staffing decisions in service systems. Modern decision-makers typically use sophisticated probabilistic models to capture uncertainty, and take optimal actions within the framework of such models. In general, the models themselves involve unknown parameters which must be estimated from data. While large datasets improve the estimation of the parameters, leading to more accurate decisions, these big-data settings also raise computational challenges that call for approximations in the estimation. Current methodology typically proceeds in two steps: (1) use the vast statistical and machine learning literature to approximately estimate model parameters, and (2) use the resulting approximations to compute the best possible action. This two-stage procedure can result in sub-optimality of actions, as the approximations computed in the first stage are not tailored to the decision-making problem in the second stage. The objective of this project is to develop and study a methodological framework for approximate computation that puts decision-making at its center, recognizing that the ultimate goal of most big-data analyses is to help decide among actions in the face of uncertainty. The project will provide tools and theory to accurately account for trade-offs between statistical accuracy, decision-theoretic utility and computational complexity, and will integrate decision-making into the computational revolution that has driven much of modern data-science. The tools and theory potentially impact a large range of data-driven decision-making problems. <br/><br/>This project works in the overarching framework of Bayesian statistics, where the primary object of interest is the posterior distribution over the unknown parameters and variables. The research focuses on theoretical and methodological challenges arising from approximate computation for Bayesian decision theory. The investigators consider two complementary problems, (a) Decision-theoretic variational Bayes, and (b) Robust decision-making. The former task analyzes and extends variational methods, developed in the machine learning community to approximate intractable Bayesian posterior distributions, from a decision-theoretic viewpoint. The investigators will theoretically study the optimality of such algorithms with respect to decision-making rather than prediction, and develop novel `loss-calibrated' algorithms that search for approximations using decision-theoretic, rather than inferential criteria. Task (b) recognizes that a model is always an approximation to reality, and is therefore misspecified. As a consequence, a Bayesian posterior distribution, even if calculated exactly, might not actually characterize the distribution over future observations. The investigators explore connections with approximations from the first task, and move from uncertainty about parameters and variables under a specified model, to uncertainty about the choice of model itself. They develop and analyze methodology that allows robust and principled decisions in the face of such `Knightian' uncertainty.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848868","I-Corps:  Residential Energy Management and Analytics","IIP","I-Corps","09/15/2018","08/16/2018","Srinivas Shakkottai","TX","Texas A&M Engineering Experiment Station","Standard Grant","Andre Marshall","02/29/2020","$50,000.00","","sshakkot@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project derives from its promise to positively affect not only consumers, but also enhance efficiencies in the electric energy marketplace as a whole. The basic functionality of matching users to optimal electric plans will result in both higher consumer satisfaction, and result in Retail Electric Providers (REPs) offering more competitive rate packages.  Identifying the actual usage profile of appliances over time and providing actionable information to consumers will enable consumers to take informed decisions on appliance purchases, as well as manufacturers to optimize designs with real-world inputs.   The platform will enable the solution of system-wide problems faced by utility companies like peak period demand surges to be countered through demand response initiatives such as incentivizing customers to modify their usage patterns to smoothen the load curve.  Each function of the proposed system is geared toward addressing a specific source of friction in the electric energy marketplace, and consequently also possesses commercial potential.  Thus, the overall economic impact will be on consumers, REPs, utility companies and appliance manufactures, while promoting a greater knowledge and engagement among the electricity consumers.<br/><br/>This I-Corps project explores the value of creating a bundled energy management system aimed at residential users.  The system uses machine-learning-based analytics of the customer's daily, weekly, monthly, and seasonal energy usage trends to offer potential savings through (1) recommending the best retail energy service provider plan matching the customer's usage patterns, (2) incentivizing customers to enable full integration with smart home devices, including smart thermostats, (3) identifying and predicting the electricity consumption of different appliances and providing actionable information on their optimal usage and maintenance, and (4) services to allow customers to navigate through the process of switching plans. A smartphone app available for the iOS and Android platforms forms the customer interface to the system.  The key novelties of this project lie in the development and integration of machine learning tools and behavioral economics ideas into the domain of residential energy management.  The project is founded on research into the design, development and validation of such tools in contexts such as predicting residential energy usage over time, disaggregating usage on a per-appliance basis, and experimentation on how best to motivate users to engage in energy usage behavior that induces efficient grid operation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801727","RET Site: Cross-disciplinary Research Experiences on Smart Cities for Nevada Teachers: Integrating Big Data into Robotics","EEC","RES EXP FOR TEACHERS(RET)-SITE","09/01/2018","06/22/2018","Kostas Alexis","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Amelia Greer","08/31/2021","$581,073.00","Lei Yang","kalexis@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","ENG","1359","115E, 9150, 9177","$0.00","This Research Experiences for Teachers (RET) Site at the University of Nevada Reno aims to deliver a unique holistic research experience to K-12 teachers of Nevada in relation to future smart cities and the cutting edge topics of robotics and big data, as well as their intersection. The overarching goal is to enable a critical mass of teachers in Nevada to be able to help their students develop a passion for these domains, strengthen their critical thinking, and broadly nurture an interest to excel in STEM fields. Research in the respective domains is currently of paramount importance in order to address the multitude of open challenges to achieve truly autonomous driving and the realization of smart cities. The relevance and significance of cross-disciplinary research in robotics and big data cannot be overstated in the context of STEM education and its potential is daily growing. This site will attract K-12 teacher participants to STEM education by providing tangible research experiences and appropriately designed knowledge modules, in turn enhancing the STEM education for their students and further designing and deploying transferable research and education tools for the school environment.<br/><br/>This project aims to nurture an interest and passion for robotics and big data research to K-12 teachers of all levels, especially from the perspective of their intersection for smart cities. The RET Site will provide a tangible experience through a) educational activities on the fundamental principles of autonomous driving, machine learning, and big data, b) a custom-designed, miniaturized, and transferable test-bed of smart cities called ""Robocity"", and c) hands-on activities on an instrumented electric bus and a drive-by-wire Lincoln MKZ autonomous car. Through the envisioned activities, it is anticipated that Nevada teachers will be actively engaged and gain knowledge and research experiences in robotic perception and machine learning, control, path planning, big data analytics, and big data system design. To enhance the quality of STEM education in local K-12 schools at all levels, a team with both senior and young faculty has been assembled to support the development of curricular modules and provide a set of mechanisms that allow research to be seamlessly transferred to the school environment. The project will engage a thorough plan for project evaluation and a participant recruitment process that focuses on underrepresented groups and respective student populations. To ensure that the developed skills acquired by teachers are transferred to the classroom, the PI team will engage in follow-up activities with the RET participants to support the installation of Robocity. The PI team will also offer demonstrations and tangible exercises at schools, as well as workshops at the University of Nevada, Reno.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842538","EAGER: Data-driven Koopman Operator Techniques for Chaotic and Non-Autonomous Dynamical Systems","AGS","Climate & Large-Scale Dynamics","09/01/2018","11/16/2018","Dimitrios Giannakis","NY","New York University","Standard Grant","Varavut Limpasuvan","08/31/2021","$300,000.00","","dimitris@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","GEO","5740","7916","$0.00","This project aims to develop data-driven Koopman methodologies for analyzing and identifying spectral properties of coherent patterns and predicting their dynamical evolutions in dynamical systems with both oscillatory modes and continuous spectrum as well subject to secular trends. Koopman operators were derived in the early 1930s and used to prove an important theorem on the behavior of idealized dynamical systems. But they remained largely unknown beyond dynamical systems theory until the early 2000s, when their usefulness for complicated, data intensive problems was recognized. Since then, the idea of Koopman operators in combination with machine learning algorithms has been applied to various problems in fluid dynamics, stability analysis of power networks, the development of financial trading algorithms, and brain activity research. Like these applications, climate research is often data-driven and involves analysis of a complex system, thus Koopman operators could be a valuable addition to the climate research toolkit. <br/><br/>The advances in pattern extraction and predictive capabilities for complex systems stemming from this project will have great impacts in the field of nonlinear dynamical systems and will be applicable to various disciplines that deal with time-evolving phenomena, from climate science, fluid dynamics, to neuroscience and economics. The project will contribute towards STEM workforce development through the training of a postdoc and new course development for on data-driven dynamical systems modeling at Principal Investigator's home institution.<br/><br/>This project will advance machine learning techniques and spectral Galerkin methods for identifying Koopman eigen-frequencies and eigenfunctions of complex dynamical systems that are subject to both stochastic forcing and external forcing (such as the Earth's climate system) from time series and spatiotemporal data without knowledge of the underlying governing equations and boundary conditions. The team will apply the data-driven methods to climate simulations of El Nino Southern Oscillation to study the interactions of natural variability and forced response of the climate system. This will demonstrate the ability of the methods to reveal and predict many aspects of the temporal evolutions of dominant modes in data, which cannot be achieved from conventional covariance-based methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820103","Toward the molecular unpacking of the PaaC domain, a novel Plasmodesmata-association & activation Cassette","MCB","Cellular Dynamics and Function","08/01/2018","05/02/2020","Jung-Youn Lee","DE","University of Delaware","Continuing Grant","Charles Cunningham","07/31/2021","$800,000.00","Li Liao","lee@dbi.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","BIO","1114","7465","$0.00","Many human diseases and developmental defects are associated with malfunction in communication between cells. Similarly, plants also suffer from uncontrolled spread of diseases, inappropriate seasonal or environmental responses, or improper organ development when cell-to-cell communication goes unchecked. This research, building on unique expertise in cell and molecular biology and computational modeling and machine learning, aims to unravel how important protein regulators are targeted to the channels that connect neighboring cells and fine tune cell communications in plants. New concepts and discoveries made by this research will be introduced to both graduate and undergraduate students as integrated new bioinformatics courses as well as laboratory modules. Furthermore, the software developed from this project will be made available free online to the public together with the source code. In addition, the newly produced data, results, and tools will be incorporated into an existing webserver with user-friendly interface to better serve the research and education community. The fundamental knowledge gained by this study has the potential to be utilized in the long run to engineer genetically encoded synthetic proteins to effectively control nutrient distribution, flow, and fitness in agriculturally important crop plants.<br/><br/>Given the fundamental role that plasmodesmata play in the biology of plants as intercellular communication channels, we know so little about the molecular architecture and mechanisms underlying the plasmodesmal association and regulation. Numerous plant viral proteins, both soluble and membrane-attached, have been studied for a few decades in search for some clues about their plasmodesmal targeting signals and mechanisms. However, no universal signal or mechanism was identified. To address this gap in the current knowledge, the research team aims to unveil molecular mechanisms underlying plasmodesmal-targeting/retention using innate plasmodesmal regulators called the Plasmodesmata-located Proteins (PDLPs). These protein families are found to share an evolutionarily conserved feature which is required for their plasmodesmal localization and function; this novel domain is here named the Plasmodesmata-association & activation Cassette (PaaC). The outcomes of in-depth investigations of the PaaC domain, will provide unprecedented mechanistic insights into how plasmodesmata-associated integral membrane proteins are targeted and anchored at plasmodesmata, and how they may self-assemble to form functional protein complexes. To achieve these goals, the research team will take an integrative computational approach utilizing newly developed machine learning tools and techniques in combination with the state-of-the-art bioimaging techniques. It is anticipated that computational extraction and identification of complex features underlying the PaaC and PaaC-like signals will also serve as a valuable means to identify previously unrecognized plasmodesmal proteins that are encoded by various plant genomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815840","AF: Small: Data Stream Algorithms with Application to Linear Algebra","CCF","Algorithmic Foundations","06/01/2018","05/21/2018","David Woodruff","PA","Carnegie-Mellon University","Standard Grant","Joseph Maurice Rojas","05/31/2021","$433,978.00","","dwoodruf@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7926","$0.00","Many important problems in machine learning, scientific computing, and statistics benefit from fast procedures for solving numerical linear algebra problems. At the same time, many large-scale datasets, such as internet search logs, network traffic, and sensor network data, have been studied in data-stream literature. Surprisingly, a number of fast procedures used in numerical linear algebra have been made possible by exploiting tools that were originally developed in the data-stream literature. These developments are based on a technique called sketching, which is a tool for quickly compressing a problem to a smaller version of itself, for which one can then afford to run a much slower procedure on the smaller problem. A major goal of this project is to study foundational problems in the data-stream domain, and develop their connections to problems in numerical linear algebra. The algorithms developed here will be accessible to graduate and undergraduate students from computer science, machine learning, and mathematics, and the investigator plans to integrate the results of the project into a graduate course on algorithms for big data, as well as undergraduate algorithms courses. The investigator is actively working with underrepresented minority and undergraduate researchers on topics directly related to this project.<br/><br/>The first main thrust of this project is to develop new techniques for fundamental problems in data streams where the goal is to use minimal amount of memory while running algorithms over one pass on a data stream. Such problems include statistical problems - such as estimating the variance, moments, most frequent items, heavy hitters - for many of which optimal memory bounds are still unknown. Another challenge in this domain is that of processing massive streams for real-world graphs, such as geometric intersection graphs, which arise in cellular networks and scheduling theory. By studying a breadth of problems in different corners of data streams, the investigator plans to develop new techniques and build unexpected applications. The second main thrust of the project is to develop new algorithms and hardness results for fundamental problems in numerical linear algebra, connecting such problems to the study of data streams. CountSketch, which is a low-memory heavy hitters algorithm, paved the way for obtaining time-optimal algorithms for regression. This project will continue to push forward such connections, for example, by studying CountSketch in the context of tensors, which is a new application domain. The project will also investigate many deterministic (instead of randomized) data stream algorithms, and understanding their role in linear algebra. A major goal is to understand the limitations of speedups to linear algebra problems. One particular problem of interest here is to obtain low rank approximation with spectral norms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854312","CIF21 DIBBs: PD: Cyberinfrastructure Tools for Precision Agriculture in the 21st Century","OAC","Hydrologic Sciences, Data Cyberinfrastructure","06/01/2018","05/22/2020","Michela Taufer","TN","University of Tennessee Knoxville","Standard Grant","Amy Walton","06/30/2021","$513,105.00","","taufer@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1579, 7726","062Z, 077Z, 7433, 8048, 9150","$0.00","This interdisciplinary project applies computer science approaches and computational resources to large multidimensional environmental datasets, and synthesizes this information into finer resolution, spatially explicit products that can be systematically analyzed with other variables.  The main emphasis is ecoinformatics, a branch of informatics that analyzes ecological and environmental science variables such as information on landscapes, soils, climate, organisms, and ecosystems.  The project focuses on synthesis/computational approaches for producing high-resolution soil moisture datasets, and the pilot application is precision agriculture. The effort combines analytical geospatial approaches, machine learning methods, and high performance computing (HPC) techniques to build cyberinfrastructure tools that can transform how ecoinformatics data is analyzed.<br/><br/>The investigators build upon publicly available data collections (soil moisture datasets, soil properties datasets, and topography datasets) to develop: (1) tools based on machine-learning techniques to downscale coarse-grained data to fine-grained datasets of soil moisture information; (2) tools based on HPC techniques to estimate the degree of confidence and the probabilities associated with the temporal intervals within which soil-moisture-base changes, trends, and patterns occur; and (3) data- and user- interfaces integrating data preprocessing to deal with data heterogeneity and inaccuracy, containerized environments to assure portability, and modeling techniques to represent temporal and spatial patterns of soil moisture dynamics. The tools will inform precision agriculture through the generation and use of unique information on soil moisture for the coterminous United States.  Accessibility for field practitioners (e.g., local soil moisture information) is made possible through lightweight virtualization, mobile devices, and web applications.<br/> <br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Earth Sciences within the NSF Directorate for Geosciences."
"1829752","Collaborative Research: CyberTraining: CIU: Toward Distributed and Scalable Personalized Cyber-Training","OAC","CyberTraining - Training-based","09/01/2018","06/11/2020","Prasun Dewan","NC","University of North Carolina at Chapel Hill","Standard Grant","Alan Sussman","08/31/2021","$471,286.00","Alison LaGarry, Sreekalyani Bhamidi","dewan@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","044Y","026Z, 062Z, 7361, 9179, 9251","$0.00","This project is addressing the challenge of providing distributed, scalable, and personalized training of cyberinfrastructures - systems that offer state-of-the-art cloud services for storing, sharing, and processing scientific data. Today, personalized training of these rapidly evolving, and hence relatively undocumented, systems requires trainer-supervised, hands-on use of these systems. These training sessions require trainees and trainers to be co-located and provide personalized training to a relatively small number of trainees. The project is developing new (a) domain-independent technologies in distributed collaboration and machine learning to reduce all three problems in a concerted manner, and (b) domain-dependent training material targeted at trainees in statistics, physical sciences, computer science, humanities, and medicine. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>A key technical insight in this work is that a cyberinfrastructure should not only support data science, but also make use of data science. The project is exploring two related innovations based on this insight: (1) Collaboration technologies that log, visualize and share the work of remote and local trainees to allow trainers to determine the need for remote or face-to-face assistance.  (2) Machine-learning technologies that mine trainee and trainer interactions so that trainees can be automatically instructed on how to solve their problems based on similar problems that have been previously solved by trainers and other trainees. The project is leveraging existing technologies and training techniques developed for a widely used NSF-supported cyberinfrastructure, called CyVerse. This system is domain-independent, but so far, its training material has been targeted mainly at plant-science research.  The project is extending the command interpreters and GUIs provided by CyVerse. The extended user-interfaces allow (a) trainees to announce difficulties and request recommendations, and (b) trainers to be aware of the progress of remote and local trainees, and remotely intervene when necessary. The functionality behind the user-interfaces is implemented by CyVerse-independent servers based on a general model of cyberinfrastructures, which includes the concepts of sharing and visualization of protected files, creation and execution of parameterized commands composed in workflows, and shareable, persistent work spaces. The project is adapting the CyVerse training material to cover new research domains including Geoscience, Political Science, and Biomedical Engineering. This expanded training material is being used to evaluate the proposed training technologies through training sessions for (a) students in a Statistics, Computer Science, Political Science, and interdisciplinary course, (b) attendees at three conferences targeted at Geoscientists, women, and Hispanics and Native Americans, respectively, (c) subjects in controlled lab studies, and (d) members of research groups at multiple institutes. The proposed qualitative and quantitative evaluation data gathered from these sessions are being used to assess not only the proposed technologies and training material, but also CyVerse and cyberinfrastructures in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826810","IRES Track 1 Graduate Research In Industrial Projects for Students - Berlin","OISE","IRES Track I: IRES Sites (IS)","09/01/2018","08/10/2018","Dimitri Shlyakhtenko","CA","University of California-Los Angeles","Standard Grant","Maija Kukla","08/31/2021","$233,235.00","Christian Ratsch","shlyakht@ipam.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","O/D","7727","5936, 5980","$0.00","This project will support the ""Graduate-level Research in Industrial Projects for Students-Berlin"" (GRIPS) led by the Institute for Pure and Applied Mathematics (IPAM). The program will offer graduate students in mathematics and related disciplines the opportunity to work on industry-sponsored research problems in Berlin through a collaboration with Research Campus MODAL in Berlin, Germany. MODAL has existing industrial partners affiliated with their laboratories that provide their research groups with interesting and challenging research problems.  The program will be eight weeks in length; eight US graduate students will participate each summer.  Half of the US students each year will be women, and one or more will be a member of an underrepresented ethnic group. MODAL will  recruit 8 European students to form 4 research groups of two US and two European students each, and provide academic mentors. The program will not only produce interesting research in applications of mathematics to real-life problems, but will give a diverse group of US students an insight into industrial research and provide them with hands-on experience of working on industrial problems. One of the goals of the program is to make the students aware of the multiple career pathways open to them and highlight the value and adaptability of their mathematics skills as applied to interesting real-life problems.  The students will also receive invaluable experience collaborating with foreign colleagues and companies. The projects will be the natural outgrowth of MODAL's existing industry research partnerships, and will be selected in collaboration with IPAM.    Participants will produce a final report on their work, and will be encouraged to submit their work for conference presentations and/or publication in technical journals.  It is expected that many of the participants will continue the research and collaborations they started in the program.<br/><br/>Applied mathematics research has become increasingly important and relevant to engineering problems and, indeed, to everyday life. Increasingly, core applied mathematics topics, ranging from applied probability, to partial differential equation, to optimization, find important applications in areas such as materials science, mathematical biology, operations research, and, more recently, machine learning. Applications of mathematical techniques to real-life problems require both a deep understanding of mathematical theory, as well as acute awareness of both the physical and technical aspects of the problem, as well as implementation details of any numerical algorithms or simulations. For this reason, such successful applications are often a combination of individual and team efforts, with the main problem being divided into more specialized parts, and specific expertise of each team member brought to bear on each such  component.The exact topics of the research projects will change from year to year and will depend on the interest of industry and academic researchers involved.  Generally speaking, the projects will apply mathematical techniques to questions of optimization (such as optimization of rail network throughput), efficient simulation (such as simulation of modern nanophotonic devices) and machine learning (such as analysis of medical mass data sets).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813444","RI: Small: Statistically Sound and Computationally Efficient Data Analysis Through Algorithmic Applications of Rademacher Averages","IIS","Robust Intelligence","09/01/2018","05/21/2020","Eli Upfal","RI","Brown University","Continuing Grant","Rebecca Hwa","08/31/2021","$466,000.00","","Eliezer_Upfal@Brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7923, 9150, 9251","$0.00","Machine learning and data mining are among the most influential contributions of computer science in the last decade.  Given sufficiently large datasets and computational power one can discover patterns and make reasonably accurate predictions.  While there has been tremendous progress in designing efficient algorithms for analyzing massive datasets, there has been less progress in providing rigorous measures of statistical significance or robustness of the analysis. As we analyze large and noisy datasets to model complex relationships in data, it is critical to develop formally proven methods with clear performance guarantees.  This project advocates a responsible approach to data analysis, based on well-founded mathematical and statistical concepts. Such an approach enhances the effectiveness and reliability of evidence- based decision making in medicine, policy and other social applications of big data analysis. Capacity-building activities of this project include: (1) Creation and dissemination of algorithms and software that implement rigorous, interpretable, and usable computational and statistical approaches to big data analysis; and (2) Educational initiatives at the graduate and undergraduate level to build a bigger and more diverse workforce of data scientists with the appropriate foundational skills both to apply analytical tools to existing datasets and to develop new approaches to future datasets.<br/><br/>The goal of this project is developing practical data analysis algorithmic applications based on the theoretical machine learning concept of Rademacher complexity. This project is motivated by preliminary results that have shown that the analytical properties of the Rademacher complexity, combined with its efficient sampling properties, provide a unique opportunity to develop general tools to begin bridging the gap between theory and practice in large scale data analysis. In particular, the project is focused on the following aims: improve the efficiency of rigorous data analysis algorithms through better sample complexity bounds; improve multi-comparisons and overfitting control through Rademacher generalization bounds; develop theory and practical applications of Cartesian and Chaos Rademacher Complexities; develop efficient algorithms for estimating the empirical Rademacher complexity; and explore new rigorous data analysis algorithms through the application of Rademacher theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758650","SBIR Phase II:  User-Centered System for Improved Coordination across the Continuum of Care","IIP","SBIR Phase II","03/01/2018","05/20/2020","Thaddeus Fulford-Jones","MA","Radial Analytics, Inc.","Standard Grant","Alastair Monk","02/28/2021","$980,899.00","","thaddeus@radialanalytics.com","50 Beharrell Street Suite A","Concord","MA","017420000","6178558214","ENG","5373","019Z, 116E, 169E, 5373, 8042, 8240, 9231, 9251","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project focuses on using analytics and technology to benefit patients who require additional care to fully recover - or simply to maintain their health - after being discharged from the hospital. Whether subacute or community-based care, significant opportunity exists to leverage machine learning, decision science, and advance data mining techniques to better support patients. State and federal budgets cover the costs of care for millions of Americans every year. This expenditure is growing faster than inflation, prompting the development of both optional and mandatory payment reform efforts that stand to simultaneously reduce costs and improve care outcomes. If successful, this project will help reduce costs of care for healthcare providers, payers, and government/society.<br/><br/>The proposed project aims to incorporate and improve upon user-centered decision science in healthcare. The proposed platform will combine advanced machine learning techniques with a patient/family-centered business model. The innovation will harness multiple streams of healthcare data, such as claims/billing data from acute, subacute, and community care settings. If successful, this research will impact the state-of-the-art in healthcare analytics and outcomes measurement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824660","Data-based Iterative Control using Complex-Kernel Regression for Precision SEA Robots","CMMI","Special Initiatives, Dynamics, Control and System D","09/15/2018","10/29/2018","Santosh Devasia","WA","University of Washington","Standard Grant","Irina Dolinskaya","08/31/2021","$387,250.00","","devasia@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","ENG","1642, 7569","030E, 034E, 116E, 8024, 9178, 9231, 9251","$0.00","This grant will support research that will contribute new knowledge related to increased automation in high-demand, low-volume manufacturing sectors, such as aerospace. In contrast to full automation, there is a need for growing-convergence research on semi-autonomous approaches for low-volume manufacturing, which exploit the combination of human adaptability and machine precision and speed, to be cost effective. Robots with series-elastic actuators (SEA) have soft joints, which enables precision control over the forces applied to the environment and are therefore, considered to be inherently safe for human-robot collaboration. This inherent safety facilitates easy adoption by workers who can directly program the robots by physical demonstrations, which in turn reduces the amount of training needed for new workers.  Nevertheless, this increased control over forces comes at the cost of lower positioning precision, which limits their use in manufacturing, where precision is important. The results from this research will increase the precision of such inherently-safe robots, and enable their use by relatively-novice workers. Moreover, the use of robotic solutions for manufacturing in confined spaces, rather than a human crawling inside, can lead to thinner, lighter and more efficient aircraft wings, with lower operating costs. Thus, the work will directly impact US competitiveness in the aerospace manufacturing sector with a substantial number of high-paying jobs. This research involves the integration of control theory and advanced robotics in manufacturing. Due to substantial and growing interest in manufacturing and robotics, the efforts will help to increase participation by underrepresented groups in research, and strengthen engineering education.<br/><br/>Relatively-soft, series elastic actuators along with low-impedance control improves control authority over the force exerted by such robots on the environment, and has the potential to enable human-robot collaboration in the manufacturing environment. Nevertheless, a central issue is that the flexural systems in such robots result in non-minimum phase dynamics and high gains (for improved precision) can lead to instability. Moreover, accurate modeling for increased precision can be challenging due to substantial friction nonlinearities, backlash, and contact-related effects in series elastic actuators robots. This research will fill the knowledge-gap on data-based iterative machine learning approaches to improve the precision of such systems. The research will use uncertainty estimates from the kernel-based learning approach to develop conditions on the size of the iteration gain for guaranteed convergence.  The approach will be experimentally evaluated with a confined-space manufacturing testbed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835792","Collaborative Research: Elements: Software: NSCI: Constitutive Relation Inference Toolkit (CRIKit)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2018","08/31/2018","Tobin Isaac","GA","Georgia Tech Research Corporation","Standard Grant","Bogdan Mihaila","08/31/2021","$299,136.00","","tisaac@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1712, 8004","026Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales. The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments. This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.<br/><br/>The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses. The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis. This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability. The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization. The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1825348","Matching Problems in Refugee Resettlement","CMMI","OE Operations Engineering","08/15/2018","06/19/2019","Andrew Trapp","MA","Worcester Polytechnic Institute","Standard Grant","Georgia-Ann Klutke","07/31/2021","$400,157.00","","atrapp@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","ENG","006Y","073E, 078E, 116E, 5514, 9178, 9179, 9231, 9251","$0.00","This project will promote the progress of science and contribute to the national prosperity and welfare by advancing analytical decision tools tackling the operational challenges of refugee resettlement in the United States. The goal of resettlement is to progressively integrate refugees into host societies, while balancing limitations of communities with the needs of refugees. This research will augment current manual refugee resettlement decision-making by using analytical methods that include machine learning and mathematical optimization. These technologies will improve humanitarian decision-making and have the potential to transform how domestic and worldwide resettlement decisions are made. Host communities will benefit by integrating refugees that bring new skills, youth and diversity to the matched communities. An established collaboration with a US-based refugee organization will guide the research and enable validation of the developed models in a real-world setting.<br/><br/>This project will make two main methodological contributions. First, it will explore the algebraic and geometric properties of integral monoids to better understand and capitalize on their structure. It is believed that integral monoids can excel in contexts with flexible capacity and multiple objectives. This research will develop appropriate algorithms and data structures to efficiently and judiciously encode and retrieve monoid information, and will include algorithmic analyses to ensure the computational tractability. If successful, this research will contribute to new advances in optimization methodology, specifically the algorithmic use of integral monoids to solve other hard linear and nonlinear matching, knapsack, generalized assignment, and packing problems. Second, this research will construct novel objective functions by leveraging supervised machine learning techniques on existing refugee placement and integration (outcome) data. These new objective functions will guide the search toward more successful resettlement outcomes. The predictive modeling will also reveal previously undiscovered insights into how demographic and regional factors contribute to refugee integration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834251","Collaborative Research: Reliable Materials Simulation based on the Knowledgebase of Interatomic Models (KIM)","DMR","DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY, CDS&E","10/01/2018","08/27/2018","Ellad Tadmor","MN","University of Minnesota-Twin Cities","Continuing Grant","Daryl Hess","09/30/2022","$2,105,161.00","George Karypis, Ryan Elliott","tadmor@aem.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1712, 1765, 8084","013E, 026Z, 054Z, 062Z, 7726, 8084, 9216, 9263","$0.00","NONTECHNICAL SUMMARY<br/>This award supports OpenKIM, a cyberinfrastructure component of the research community that uses computer simulations of atoms based on Newton's Laws and models for the interaction between atoms, to attack problems in materials science, engineering, and physics, and to enable the discovery of new materials, design new devices, to advance the understanding of materials-related phenomena, and much more. Recent years have seen significant advancement in the areas of materials knowledge, discovery, and manufacturing methodologies.  This includes, for example, the development of graphene (a single atomic layer of carbon atoms, which has exceptional mechanical, thermal, and electrical properties) and the related class of two-dimensional materials with unprecedented material properties now being extensively studied by scientists and engineers.  Another example is the advent of three-dimensional printing techniques that allow engineers to design new materials from the ground up that can be tailor-made for their specific application.  Computer simulation of materials at the atomic-scale is one of the key enabling technologies driving the current materials revolution.  Although the most accurate atomic-scale simulations employ the equations of quantum mechanics, such computations take so long to complete, even on today's powerful computers, that practically they are limited to a few thousands of atoms.  This is simply not enough for the study of materials properties, which requires the simulation of interactions between millions and even billions of atoms.  Thus, materials researchers rely on faster more approximate equations, known as interatomic models (IMs), to describe atomic interactions.  These models are fast, but typically they are only accurate for a restricted range of material properties.  This limited range of applicability necessitates the creation of many IMs, even for a single material such as silicon.  Organizing, sharing, and evaluating the range of applicability of these IMs has been a long-standing challenge for the materials research community.  In most cases researchers have no way of knowing which IM is suitable for their particular application. Further, the proliferation of IMs, often designed to work only with specific simulation programs, makes it difficult to share and exchange IMs, and to reproduce other researchers' work, which is how science evolves and self corrects.<br/><br/>The Knowledgebase of Interatomic Models (KIM) is a project that is working to solve these challenges.  To date, the KIM project has developed an online framework at https://openkim.org to address the issues of IM provenance, selection, and portability.  IMs archived on this website are exhaustively tested and can be used in plug-and-play fashion in a variety of major simulation codes that conform to a standard developed as part of the KIM project. The development activity of the current project will extend the KIM framework by broadening the number and types of supported IMs, and will add new capabilities and educational resources that will make it easy for researchers to integrate the IMs and materials data available on openkim.org into their daily research workflow. Further, emerging techniques in information topology and machine learning will be applied to study and quantify the inherent uncertainty in predictions made by IMs, and to assist materials researchers to select the best IM for their application.  Together the development, educational, and research activities of this project are expected to significantly increase the userbase and broader impact of the KIM project.  <br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports OpenKIM, a community Knowledgebase of Interatomic Models (KIM) for simulation. KIM is a project for normalizing the use of IMs in molecular simulations of materials.  An IM, often referred to as a ""potential"" or ""force field,"" is an approximate method for computing the energy and its derivatives for an atomic configuration.  This project addresses both traditional ""physics-based"" IMs and the new class of ""data-driven"" IMs introduced in recent years. In a sustained effort, the KIM project has developed a systematic framework to address the IM provenance, selection, and portability problems faced by materials researchers.  Before KIM, these challenges were the cause of significant inefficiencies and inaccuracies in the research pipeline.  Today, an IM available on openkim.org is subjected to a rigorous set of ""Verification Checks"" that aim to ensure that its implementation conforms to a high software-engineering standard, and to an extensive set of ""Tests,"" each of which computes a well-defined material property for assessing the IM's accuracy.  A researcher can come to openkim.org and explore the predictions of KIM Models in comparison with experimental or quantum ""Reference Data"" to select a suitable IM for their application. The current project is aimed at extending KIM to become an integral component of the workflow of researchers engaged in molecular simulation to make their work more efficient and their results more reliable and reproducible.  To achieve this vision, the Principal Investigators (PIs) will pursue the following program of cyberinfrastructure R&D and basic research related to IM usage and science.  The cyberinfrastructure R&D will include extensions to KIM standards to support additional common IM features (such as long-range fields) and added support for IMs having cutting-edge features that cannot yet be standardized.  Further, KIM will be integrated into existing simulation tools so that researchers may query and retrieve data archived on openkim.org as part of their daily workflow.  This approach reduces errors, ensures reproducibility, uses a standard tested method (embodied in a KIM Test) to obtain the desired property, and firmly integrates the KIM framework into the workflow of computational materials researchers.  The basic research component of the project includes three research thrusts requiring advances to enhance the reliability of molecular simulations: (1) IM Uncertainty: The PIs will use ideas from information topology and differential geometry to automatically generate IM ensembles for obtaining estimates of the inherent uncertainty of the IM. (2) IM Transferability: The PIs plan to adapt a multi-task machine learning approach to predict an IM's accuracy for different applications.  This will lead to a rigorous, objective criterion to assist researchers with IM selection. (3) IM Heuristics: By mining IM predictions and Reference Data archived on openkim.org, it is possible to identify correlations similar to empirical heuristics such as Vegard's rule and connections between microscopic properties and macroscopic features.  Detection of such heuristics will provide insights into the limitations of IMs, help design optimal training sets, and lead to better understanding of the properties of IMs generally.  In terms of broader impacts, the scope of the KIM project is unusually large - far beyond materials science - due to the prevalence of molecular simulations across the physical sciences from microbiology to geology.  The project aims to maximize its impact by (1) expanding the KIM user base, (2) engaging the materials research community directly and through targeted research and educational efforts, and (3) developing new relationships and collaborations with other materials modeling cyberinfrastructures and organizations.<br/><br/>This award is jointly supported by the Division of Materials Research in the Directorate for Mathematical and Physical Sciences and the Civil, Mechanical and Manufacturing Innovation Division in the Engineering Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841758","BIGDATA: IA: Collaborative Research: In Situ Data Analytics for Next Generation Molecular Dynamics Workflows","IIS","Big Data Science &Engineering","06/01/2018","08/22/2018","Michela Taufer","TN","University of Tennessee Knoxville","Standard Grant","Almadena Chtchelkanova","09/30/2021","$979,987.00","","taufer@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8083","7433, 7942, 8083, 9150","$0.00","Molecular dynamics simulations studying the classical time evolution of a molecular system at atomic resolution are widely recognized in the fields of chemistry, material sciences, molecular biology and drug design; these simulations are one of the most common simulations on supercomputers.  Next-generation supercomputers will have dramatically higher performance than do current systems, generating more data that needs to be analyzed (i.e., in terms of number and length of molecular dynamics trajectories). The coordination of data generation and analysis cannot rely on manual, centralized approaches as it does now.  This interdisciplinary project integrates research from various areas across programs such as computer science, structural molecular biosciences, and high performance computing to transform the centralized nature of the molecular dynamics analysis into a distributed approach that is predominantly performed in situ. Specifically, this effort combines machine learning and data analytics approaches, workflow management methods, and high performance computing techniques to analyze molecular dynamics data as it is generated, save to disk only what is really needed for future analysis, and annotate molecular dynamics trajectories to drive the next steps in increasingly complex simulations' workflows. <br/><br/>The investigators tackle the data challenge of data analysis of molecular dynamics simulations on the next-generation supercomputers by (1) creating new in situ methods to trace molecular events such as conformational changes, phase transitions, or binding events in molecular dynamics simulations at runtime by locally reducing knowledge on high-dimensional molecular organization into a set of relevant structural molecular properties; (2) designing new data representations and extend unsupervised machine learning techniques to accurately and efficiently build an explicit global organization of structural and temporal molecular properties; (3) integrating simulation and analytics into complex workflows for runtime detection of changes in structural and temporal molecular properties; and (4) developing new curriculum material, online courses, and online training material targeting data analytics. The project's harnessed knowledge of molecular structures' transformations at runtime can be used to steer simulations to more promising areas of the simulation space, identify the data that should be written to congested parallel file systems, and index generated data for retrieval and post-simulation analysis. Supported by this knowledge, molecular dynamics workflows such as replica exchange simulations, Markov state models, and the string method with swarms of trajectories can be executed ?from the outside? (i.e., without reengineering the molecular dynamics code)."
"1844484","EAGER: MATDAT18 Type-1: Collaborative Research: Data Driven Discovery of Singlet Fission Materials","DMR","TRIPODS Transdisciplinary Rese, DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY","09/01/2018","08/22/2018","Noa Marom","PA","Carnegie-Mellon University","Standard Grant","Daryl Hess","08/31/2020","$237,841.00","","nmarom@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","041Y, 1712, 1765","054Z, 062Z, 7916, 7926, 9216","$0.00","NONTECHNICAL SUMMARY<br/>This award supports continued collaboration of materials researchers with data scientists kindled at the MATDAT18 Datathon event. The efficiency of organic solar cells may be enhanced significantly by harnessing singlet fission (SF), a quantum mechanical process that can lead to the generation of two conducting species, for example electrons, from one quantum of light. Presently, few materials are known to exhibit intermolecular SF in the solid state, and they belong to restricted chemical families. The vast number of possible molecules and crystals that could be made has not been explored for SF. The PIs will use computer simulations to search the many possibilities for new SF materials. To this end, a new approach will be developed, one that integrates cutting-edge advances in quantum mechanical simulations and machine learning. This research will advance both fields of materials science and data science. Graduate and undergraduate students will train in a collaborative cross-disciplinary environment at the interface of computational materials science and data science and acquire transferrable job skills in high demand. <br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports continued collaboration of materials researchers with data scientists kindled at the MATDAT18 Datathon event. Singlet fission (SF) is the conversion of one photogenerated singlet exciton into two triplet excitons. Recently, there has been a surge of interest in SF thanks to its potential to significantly increase the efficiency of organic solar cells by harvesting two charge carriers from one photon. However, few materials are presently known to exhibit intermolecular SF with high efficiency, hindering the realization of solid-state SF-based solar cells. The chemical compound space of possible chromophores is infinitely vast and largely unexplored. To enable computational discovery of SF materials, a new multi-fidelity screening approach will be developed, which integrates quantum mechanical simulations at different levels of fidelity with machine learning (ML) and database mining. ML algorithms will be used to analyze data generated by quantum mechanical simulations and to steer simulations for further data acquisition. High-cost high-fidelity evaluations of excited state properties of solid-state forms of candidate chromophores will be performed with many-body perturbation theory methods within the GW approximation and the Bethe-Salpeter equation. Lower-cost lower-fidelity evaluations of ground state features will be performed with density functional theory. Feature selection algorithms will then determine which descriptors are most predictive of the thermodynamic driving force for SF. These descriptors will be used to screen databases that contain crystal structures with no information or only partial information on their electronic properties. Optimization algorithms will be employed to decide which data points to sample and at what level of fidelity to maximize information gain. This research will advance the discovery of new intermolecular SF chromophores and will lead to advances in data science in the area of experimental design.  <br/><br/>The award is jointly funded through the Division of Materials Research and the Division of Mathematical Sciences in the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832121","HBCU-Excellence in Research: Understanding Atmospheric Moist Convection and Organization Using Automatic Feature Identification and Tracking","AGS","Hist Black Colleges and Univ, Integrat & Collab Ed & Rsearch","09/01/2018","08/17/2018","Xiaowen Li","MD","Morgan State University","Standard Grant","Chungu Lu","08/31/2021","$299,827.00","Monir Sharker","xiaowen.li@morgan.edu","1700 East Cold Spring Lane","Baltimore","MD","212510002","4438853200","GEO","1594, 7699","1525, 4444, 7699, 9178","$0.00","Convective updraft core and its lifecycle are fundamental processes in atmospheric moist convection. Cumulus parameterization is a critical component in global climate prediction. The ultimate goal of the study is to improve cumulus parameterization using a physically based, time-variant convective updraft core concept. The close collaborations between the PI and Co-PI will add new research field (atmospheric sciences) and strengthen the current research and teaching (computer sciences) at Morgan State University, an HBCU (Historically Black Colleges and Universities) that has been recently designated as Maryland's preeminent public urban research university. An education component is built into the study, where a graduate student and an undergraduate student will be supported and trained in STEM (Science, Technology, Engineering, and Mathematics) field at Morgan State University.<br/><br/>Coherent updraft cores are the engine for convective heat and moisture transport. However, systematic study of convective updraft cores from a Lagrangian point of view has not been performed before. This project will take advantages of existing computer image recognition and machine learning algorithms, adapt them to high resolution cloud-resolving model simulations, in order to automatically identify updraft core features and track them throughout their full life cycle. Three sets of progressively more sophisticated simulations will be carried out: ensemble simulations of triggered single convection, quasi-equilibrium state simulation, and case studies using observed large-scale forcing. Convective updraft core characteristics such as their sizes, depths, lifespan and spatial distributions will be derived from these simulations. Model sensitivity tests that perturb environmental conditions, e.g., stability, water vapor, wind shear, will reveal environmental controls on updraft core characteristics. <br/><br/>Convection organization will also be studied in the context of updraft cores through their congregation, merging and splitting.  Although not immediately achievable in the framework of current project, the ultimate goal is to use the time-variant convective core concept established in this study to improve cumulus parameterization in global climate predictions. A database including thousands of convective updraft cores and their lifecycles will be compiled and made public through this project for further study.<br/><br/>This is an interdisciplinary study that takes advantage of existing data mining and machine learning algorithms in information sciences and applies them to 3D wind fields where convection is embedded. Identifying and tracking updraft cores is a novel approach that will provide new insights to dynamics and physics of moist convection and its organization. The time-dependent updraft core concept can potentially be used to replace the steady state plume model in current GCM cumulus parameterizations, allowing for a scale-independent cumulus parameterization with a physical underlying concept.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811315","Statistical Machine Learning Methods for Complex Data Sets","DMS","STATISTICS","08/01/2018","08/03/2018","Kean Ming Tan","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","10/31/2019","$119,988.00","","keanming@umich.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1269","","$0.00","Recent advances in science and technology have led to the generation of massive amounts of large-scale data with complex structures, including genomics, neuroimaging, and microbiology data. These large-scale datasets pose significant statistical and computational challenges to data analysis. Firstly, widely used statistical methods yield unstable estimates and are not computationally scalable to modeling large-scale data sets. Secondly, complex data sets are often accompanied by outliers due to possibly measurement error or heavy-tailed random noise. For instance, in genomic studies, it has been observed that the distribution of gene expression levels is generally heavy-tailed, that is, the data contain a lot of extremely large values. Classical statistical methods will yield biased estimates and spurious scientific discovery if these outliers are not taken into account during model estimation and inference. This project aims to develop scalable and robust multivariate statistical methods to address the aforementioned problems. <br/><br/>In this project, the investigator uses a combination of regularization and statistical optimization techniques to develop novel multivariate statistical methods for analyzing complex high-dimensional data sets. The first part of the project concerns the sparse generalized eigenvalue problem, which arises naturally in many statistical models such as partial least squares, canonical correlation analysis, sufficient dimension reduction, and Fisher's discriminant analysis. The investigator will develop a general framework for solving the sparse generalized eigenvalue problem and make available a wide range of statistical models for analyzing high-dimensional data. Furthermore, the investigator will study the theoretical properties of sparse generalized eigenvalue problem, and this will lead to the understanding of various statistical models that are previously not well understood in the high-dimensional setting. The second part of the research project focuses on a class of robust sparse reduced rank regression models. The investigator will develop efficient algorithms and high-dimensional asymptotic analysis for the resulting estimators under the Huber loss function, and quantify the bias-robust tradeoff between using Huber loss and squared error loss. This research project will also deliver easy-to-use software packages for fitting the developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1759462","Collaborative Research:  Innovation:  Pioneering New Approaches to Explore Pangenomic Space at Scale","DBI","ADVANCES IN BIO INFORMATICS","07/15/2018","02/07/2020","Joann Mudge","NM","National Center for Genome Resources","Standard Grant","Peter McCartney","06/30/2021","$283,181.00","Thiruvarangan Ramaraj","jm@ncgr.org","2935 Rodeo Park Drive East","Santa Fe","NM","875050000","5059827840","BIO","1165","9150","$0.00","This project develops new software tools for pangenomic analysis, which is a relatively new area of genomic research that studies large numbers of genome sequences from multiple organisms to understand how organisms adapt their genomes to their environments.  As the cost of DNA sequencing continues to decrease, it is now routine for multiple genomes per species to be available for analysis, giving much more information about the species. The approach makes use of a graph-based representation of a pangenome and exploits this representation to efficiently find both shared and unique regions of interest across genomes. Each individual?s genomic sequence corresponds to path in a graph data structure called a De Bruijn graph; these graphs are large and can have millions of nodes and edges.  The tools being developed are based on finding frequented regions (FRs) in De Bruijn graphs; these regions are hotspots that often represent features of interest in one or more genomes. Algorithms and software tools will be made available to the greater scientific community to facilitate new pangenomics research.  The project will provide support and training for a postdoc and an incoming PhD student at Montana State University. It will also support a summer intern in the last two years at the National Center for Genome Resources.  Aspects of the project will be incorporated into undergraduate and graduate courses at MSU, as well as integrated into several outreach and training activities at NCGR. In addition, MSU has several programs in place to serve American Indian students and the PIs will actively recruit from and engage this community.<br/><br/><br/><br/>The current trajectory of next generation sequencing improvements, including falling costs and increased read lengths and throughput, ensure that multiple genomes per species will be routine within the next decade. This project initiates work on a next generation of bioinformatics software that can exploit the increased information content available from multiple accessions and intelligently use the data for unbiased, species-wide analyses.  The proposed work will refine algorithms and develop software to address important problems in each of the identified areas. The research team has a variety of complementary expertise ranging from molecular biology, algorithms, machine learning and genomics research.  Pangenomic biology will be advanced through automatic identification of candidate regions of interest in a pangenome. Methods will be developed to discover regions that are conserved across evolutionary space, regions that are novel, and regions that have diverged due to positive selection.  Machine learning techniques will be used to search for interesting genomic regions. Lastly, this work will complement the work being done on the model plant, Medicago truncatula, contributing to research on its symbiotic relationships.  Results of the project can be found at: www.cs.montana.edu/pangenomics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757952","REU Site: Data Science in the Life Sciences, Environmental Science and Engineering","DMS","WORKFORCE IN THE MATHEMAT SCI","05/15/2018","11/19/2019","Lisette de Pillis","CA","Harvey Mudd College","Standard Grant","James Matthew Douglass","04/30/2021","$349,101.00","Susan Martonosi, Tanja Srebotnjak","depillis@hmc.edu","301 Platt Boulevard","CLAREMONT","CA","917115901","9096218121","MPS","7335","9250","$0.00","This Research Experiences for Undergraduates (REU) in Data Science in the Life Sciences, Environmental Science and Engineering at Harvey Mudd College will provide nine undergraduate students per year from across the United States with opportunities to learn and apply data science concepts and tools to projects in the (i) biological and life sciences, (ii) environmental science and (iii) engineering and industrial applications. The students can apply to the REU program by ranking their favorite projects and recruitment will consider the applicant's skills and likelihood of having a successful experience as well as attracting students from traditionally underrepresented population groups, including women, African American and Hispanic/Latino students. The participating students will spend a total of 10 summer weeks at Harvey Mudd College (a member of the 5 Claremont Colleges that also include Claremont McKenna, Pomona, Pitzer and Scripps) and will work on their research projects with an experienced faculty member. They will be supported by state-of-the-art infrastructure such as computing facilities, libraries and laboratories. Students will also engage with their peers in a series of data science and professional skill workshops. Social events and educational field trips round out the program. Domain-specific research using data science methods and tools provide the students with important skills that prepare them for graduate studies and are useful for analyzing and solving problems in many disciplines and environments. The professional skill modules, including research ethics, time management and scholarly publishing, will complement and enrich the technical training and further equip students with competencies needed to become well-rounded, successful researchers. As computational capacity continues to expand at a rapid pace, there is an increased need for data science literacy among all scientists and engineers. Exposing the students to relevant concepts and tools in data science through this REU program is aimed at encouraging them to pursue careers in STEM-related fields and helping fill the persistent skill and labor gap in the U.S. by producing graduates who can have an impact on science and technology in the public and private sectors. <br/><br/>The research projects that are part of the Harvey Mudd College REU program in Data Science in Life Sciences, Environmental Science and Engineering address new and open problems in different STEM disciplines using computational, mathematical and statistical methods and tools. The participating faculty mentors maintain active research activities in these tracks that offer data and hypothesis-rich environments for exploration and in-depth analysis. Example projects include the analysis of flow cytometry data, modeling of epidemiological and public health data such as surgical cataract coverage in developing countries, spatial data modeling and analysis for health risk assessments related to unconventional oil and gas development, testing hybrid mathematical models in atmospheric chemistry, and developing predictive models for sports coaching such as real-time coaching recommendations based on play-by-play basketball data. These projects involve high-dimensional data analytics and use different techniques to extract insights such as simulated data to validate mathematical models of effective health intervention coverage, multivariate spatial regression and Kriging, time series analysis of cloud-chamber data on air pollution models, and algebraic analysis of partially ranked data coupled with predictive techniques used in machine learning to predict player performance. In addition, students will also receive hands-on training in R, Python, MATLAB, become well versed in Linux command line processing, batch scripting, data movement, use of XSEDE supercomputers, and version control. They will be exposed to big data environments such as Hadoop and Spark, learn to use relational databases and write SQL and PostgreSQL queries, and work with ESRI's ArcGIS to model high-resolution spatial data. While the technical training and participation in real research processes is the main component of the REU program, necessary soft skills for successfully developing and running research programs are taught as well. This includes the group's participation in Harvey Mudd College's successful weekly Stauffer lecture and open lab series as well as a series of custom-tailored workshops involving additional personnel from the College's academic departments and the Writing Center.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763070","Data-Driven Learning and Geometric Embedding for Reduction and Control of Complex Heterogeneous Networks","CMMI","Dynamics, Control and System D","08/15/2018","07/24/2018","Jr-Shin Li","MO","Washington University","Standard Grant","Robert Landers","07/31/2021","$325,000.00","","jsli@seas.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","ENG","7569","030E, 034E, 8024, 9150","$0.00","Complex systems in which multiple agents (components) affect each other dynamically are prevalent in nature and human society in different scales, such as neurons in the brain, bees in a hive, and human beings in a social network. Undesirable behavior of such systems, in the form of disease, economic collapse, rumor spreading, and social unrest, has generated considerable interest in understanding the dynamic structures of such complex networks and devising ways to control them. Despite the abundance of data, ease of access, and advances in data science, obtaining reliable models of such networks remains a very challenging problem. The scale of these emerging complex systems also poses a great difficulty. These obstacles also form a bottleneck for analyzing and engineering the dynamic structures (e.g., synchrony and clustering) and for controlling the collective behavior in such complex networks. This project will develop a unified data-driven framework to investigate fundamental questions regarding how to extract dynamics of a large-scale complex system or network from its simulation or measurement data, and how to control this system if the dynamics reconstruction is successful and reliable. The project will also support new initiatives to promote interdisciplinary education for students from traditionally underserved populations in local high schools in the city of St. Louis, MO, through the creation of summer research opportunities.<br/><br/>By bridging systems and control theory with concepts and methods from algebraic geometry, time-series analysis, and machine learning, a unified data-driven framework will be established. Specifically, a novel approach based on spectral decomposition will be developed to extract the dynamics of a complex system and decode the topology of a complex network using its time-series data. The properties of the reconstructed network, e.g., connectivity and the coupling strength of nodes, will then be utilized to synthesize a dynamically-proximate reduced network that is tractable for control-theoretic analysis and design. Furthermore, novel topological and geometrical approaches will be derived to construct local and global embedding of high-dimensional data to low-dimensional manifolds, which will reveal hidden topological structures in large data sets and characterize transitions of flow of the underlying dynamical system. In collaboration with researchers in biology and chemistry, the network inference, dimensionality reduction, and control techniques will be applied to a diverse set of complex systems from cells to societies, for example, for decoding functional connectivity in cellular networks and analyzing social synchronization in groups of animals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1855501","CRII: III: Computational Methods to Explore Big Bioassay Data for Better Compound Prioritization","IIS","CRII CISE Research Initiation","10/01/2018","10/25/2018","Xia Ning","OH","Ohio State University","Continuing Grant","Sylvia Spengler","04/30/2021","$107,886.00","","ning.104@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","026Y","7364, 8228","$0.00","Bioassay data represent an extremely valuable source of experimental Big Data with rich content that have been substantially produced in the early stages of drug discovery for testing chemical compound bioactivities and identifying promising drug candidates. However, the power of such Big bioassay data has not been fully unleashed, particularly for the purposes of discovering novel knowledge and improving drug development. This is largely due to the fact that the exploration of a much larger space of bioassays has been fundamentally hindered by the less developed ability to identify and utilize the relations across bioassays. In this project, the PI and her team will develop novel computational methods and tools that can effectively explore a wide range of heterogeneous bioassays, identify experimentally unrevealed relations among them, and utilize the novel knowledge derived from them so as to improve compound prioritization. The research will bring scientific impacts and shed light on fully utilizing the existing wealth of Big Data, stimulating knowledge distillation in innovative manners, establishing visionary conceptual hypotheses and developing novel analytical techniques correspondingly. This research aims to solve critical problems in drug discovery through Big Data means, and has a great potential to improve drug candidate identification through accurate compound prioritization, and thus it will have far-reaching economic and societal impacts. <br/><br/>The PI and her team will develop a computational framework to produce better compound ranking for each bioassay. This framework will consist of a local structure learning component and a global structure learning component to discover and leverage the compound ranking within a bioassay and ranking relations across bioassays, respectively. They will also develop new methods to better rank compounds under a combination of criteria. In particular, they will solve compound ranking based on activity and selectivity simultaneously by leveraging ranking difference across bioassays. The research will be innovative, both in terms of employing original computational models and methods into important problems in drug discovery, and in terms of developing unique methodologies and computational techniques for core Computer Science research. For drug discovery, the research will provide novel perspectives and methodologies as to how researchers can utilize the large-scale experimental data to solve important problems in drug discovery. For core Computer Science, the research will contribute a new solution framework and methods spanning the areas of data mining and machine learning. Specifically, the research will lead to novel methods for boosting ranking performance by actively including additional data, incorporating relevant information within a regularized optimization framework, deploying iterative procedures and greedy strategies for large-scale problems with multiple simultaneous tasks, etc. All these methods are generalizable to a variety of other Computer Science applications. For further information see the project web page: http://cs.iupui.edu/~xning/compRank.html"
"1800940","EAGER: Distributed Iterative Control of Soft Robotic Arms","CMMI","Dynamics, Control and System D","04/01/2018","03/27/2020","Panagiotis Polygerinos","AZ","Arizona State University","Standard Grant","Robert Landers","03/31/2021","$159,100.00","Wenlong Zhang","Polygerinos@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","7569","030E, 034E, 116E, 7916, 8024, 9178, 9231, 9251","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) will create a new method of controlling compliant robot manipulators resembling octopus arms or elephant trunks, and will experimentally validate these new control laws. This project will explore two innovative aspects. First, control will be distributed. That is, a large number of sensors and actuators will be spread throughout the manipulator, and information gathered by each sensor will be used to control only a small number of nearby actuators. This distributed control strategy avoids the transmission of large amounts of data across the structure, and eliminates the need for a powerful central computer to process this flood of data. Second, the control law will be iterative. That is, the soft manipulator will learn to achieve its objectives over multiple attempts. Although the approach taken is not explicitly based on biological behavior, both these features are present in animals that manipulate objects using compliant appendages. Computer simulations will be used to study performance of the new control laws on extremely complex robots, with hundreds of individually controllable segments. Physical experiments will be conducted on systems with between five and ten such segments. The modeling and control approaches will advance the national health and economic prosperity, by enabling reliable autonomous operation of soft multi-segment robots, to increase their usability in numerous applications, including manufacturing, surgery, search and rescue, navigation, and personal home assistance.<br/><br/>This project will explore novel distributed planning and control approaches for multi-segment soft robotic systems. In contrast to the large amounts of training data required by machine learning, this approach aims to create intuitive and computationally efficient algorithms with high robustness and scalability. Specific research goals include i) novel 2-D and 3-D goal reaching and obstacle avoidance algorithms, using only local sensor measurements and information processing, ii) feasible high-fidelity dynamic models to capture non-linear material properties and pressure dynamics, iii) a robust iterative learning control algorithm for gravity compensation and motion control of the soft arm, and iv) validation of the control laws using a physical multi-segment soft robotic arm.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820862","EAGER: An Interactive Learning Analytics Framework based on a Student Sequence Model for understanding students, retention, and time to graduation","IIS","Big Data Science &Engineering","08/01/2018","07/26/2018","Mary Lou Maher","NC","University of North Carolina at Charlotte","Standard Grant","Wu He","07/31/2021","$298,486.00","Mohsen Dorodchi, Wenwen Dou, Xi Niu","marylou.maher@gmail.com","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","8083","7916, 8083","$0.00","This is a project to personalize academic advising with the goal of better identification of student risk and success in retention and graduation in higher education. The personalization is accomplished through the development of an interactive system using a novel approach to data modeling. The advisor is given the sequence of courses, labs, and sections that the student has taken, along with grades and any other assessment data that is available. This information is then fed to a data mining application that identifies the degree of success that the student has had and a prediction of any additional assistance the student may require in order to achieve academic success. The data mining application uses machine learning technology and interactive input from faculty, advisors, and academic leadership to accurately model student achievements.<br/><br/>More precisely, the interactive framework enables the discovery of actionable knowledge to improve student success by including the domain experts in data-driven discovery and decision-making from heterogeneous and longitudinal student data. The approach is to integrate and iterate the feature extraction, analytics, and interpretation processes within a single interactive user experience. Through the use of explorative interactive visualization of data and data patterns, the target user communities, including academic leadership, faculty, and advisors will be empowered to explore a broader range of meaningful hypotheses and derive specific actionable insights given the large and complex data that is being collected about students' performance and campus life. This will transform the ability to create policy, curriculum changes, and interventions that can address specific critical issues in universities more proactively than traditional analyses can provide for affecting retention, time to graduation, and student success.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1727618","Collaborative Research: Emerging Optimization Methods for Planning and Operating Shared Mobility Systems under Uncertain Budget and Market Demand","CMMI","CIS-Civil Infrastructure Syst","01/01/2018","04/18/2019","Siqian Shen","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yueyue Fan","12/31/2020","$304,010.00","Ruiwei Jiang","siqian@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","1631","029E, 036E, 039E, 116E, 9102, 9178, 9231, 9251","$0.00","Advances in networked communication systems have enabled the use of shared mobility forms including carsharing and ridesharing. Both government and private sectors engaged in planning shared mobility systems must choose between introducing new, shared mobility programs versus expanding existing ones, under uncertain budget and market demands.  In this project, the PI will address the issue of demand uncertainty in design of shared mobility strategies considering shared fleet size, type, location design, and operational activities such as real-time vehicle routing, redistribution, and charging for carsharing systems; in addition to service region planning and shared mobility. The success of this project will: (i) advance both the theoretical and the computational frontiers of optimization methods for use in solving new transportation problems; and (ii) impact applications of shared mobility that relate to critical civil infrastructures, supply chain & logistics, and other service industries. Education plans, including promoting female and underrepresented minority groups in science, engineering, and management, will be collaboratively undertaken through the PIs' involvements in various education initiatives at the University of Michigan and Purdue University.<br/><br/>The objective of this collaborative research is to derive high-fidelity, data-driven mathematical models and provably efficient numerical algorithms that innovatively combine optimization and reinforcement learning for shared mobility system design and operations. In specific, we characterize shared mobility demand response as a multi-stage information revealing process, and abstract the corresponding decision process as sequential resource planning, allocation, and task prioritization, adjusted by varying decisions in later stages. We derive models and solution methods based on single-, two- and multi-stage stochastic optimization and dependent on full knowledge of demand distributions. We also investigate data-driven distributionally robust optimization methods and machine learning approaches to address ambiguous demand distributions and budget uncertainty. The derivation, validation, and calibration of this study aim at (i) formulating appropriate optimization-based models to characterize decision-data interdependence in complex shared mobility systems; (ii) deploying data-driven, distribution-free approaches for handling the distribution ambiguity of multi-sourced uncertainties in emerging shared mobility services; (iii) integrating learning approaches to dynamically adapt to endogenous system information and enhancing solutions from multi-stage optimization processes; (iv) designing efficient computational methods with solution quality guarantees to enable practical use of the models."
"1727478","Collaborative Research: Emerging Optimization Methods for Planning and Operating Shared Mobility Systems under Uncertain Budget and Market Demand","CMMI","CIS-Civil Infrastructure Syst","01/01/2018","08/04/2017","Mengshi Lu","IN","Purdue University","Standard Grant","Yueyue Fan","12/31/2020","$125,000.00","","mengshilu@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","1631","029E, 036E, 039E","$0.00","Advances in networked communication systems have enabled the use of shared mobility forms including carsharing and ridesharing. Both government and private sectors engaged in planning shared mobility systems must choose between introducing new, shared mobility programs versus expanding existing ones, under uncertain budget and market demands.  In this project, the PI will address the issue of demand uncertainty in design of shared mobility strategies considering shared fleet size, type, location design, and operational activities such as real-time vehicle routing, redistribution, and charging for carsharing systems; in addition to service region planning and shared mobility. The success of this project will: (i) advance both the theoretical and the computational frontiers of optimization methods for use in solving new transportation problems; and (ii) impact applications of shared mobility that relate to critical civil infrastructures, supply chain & logistics, and other service industries. Education plans, including promoting female and underrepresented minority groups in science, engineering, and management, will be collaboratively undertaken through the PIs' involvements in various education initiatives at the University of Michigan and Purdue University.<br/><br/>The objective of this collaborative research is to derive high-fidelity, data-driven mathematical models and provably efficient numerical algorithms that innovatively combine optimization and reinforcement learning for shared mobility system design and operations. In specific, we characterize shared mobility demand response as a multi-stage information revealing process, and abstract the corresponding decision process as sequential resource planning, allocation, and task prioritization, adjusted by varying decisions in later stages. We derive models and solution methods based on single-, two- and multi-stage stochastic optimization and dependent on full knowledge of demand distributions. We also investigate data-driven distributionally robust optimization methods and machine learning approaches to address ambiguous demand distributions and budget uncertainty. The derivation, validation, and calibration of this study aim at (i) formulating appropriate optimization-based models to characterize decision-data interdependence in complex shared mobility systems; (ii) deploying data-driven, distribution-free approaches for handling the distribution ambiguity of multi-sourced uncertainties in emerging shared mobility services; (iii) integrating learning approaches to dynamically adapt to endogenous system information and enhancing solutions from multi-stage optimization processes; (iv) designing efficient computational methods with solution quality guarantees to enable practical use of the models."
"1818650","Enabling Computer and Information Science and Engineering Research and Education in the Cloud Workshop","IIS","Big Data Science &Engineering","01/15/2018","04/03/2020","Jennifer Rexford","NJ","Princeton University","Standard Grant","Sylvia Spengler","12/31/2021","$49,999.00","","jrex@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8083","7556, 8083","$0.00","-Part 1:<br/><br/>Cloud computing can transform computer science research and education through on-demand, elastic, and self-serve access to computation and storage resources at scale, coupled with contemporary hardware (e.g., graphical and tensor processing units), advanced software stacks (e.g., machine-learning libraries), and shared data sets. Researchers and educators can avoid the substantial time, energy, and expense of building and maintaining local infrastructure, students can be better prepared for the cloud-centered world they will enter upon graduation, and research projects across institutional boundaries can more easily share and maintain data and the results of their analysis. <br/><br/>Although academic adoption of the cloud is increasing, significant challenges remain. Researchers worry about the relative costs compared to local infrastructure, especially when some local costs are hidden (e.g., ""free"" rack space, cooling, and power, as well as lower overhead charges on grants for equipment purchases). Graduate students worry about causing run-away costs if their experiments in the cloud run amok, and the risk of escalating costs near major paper submission deadlines. Some projects may not be able to use the public cloud, due to privacy issues (or the perception of increased privacy risk) concerning the underlying data. Plus, for both researchers and educators, making the transition to the cloud requires overcoming a ""learning curve"" to select a particular cloud offering---and learn how to use it effectively. <br/><br/>This workshop will identify opportunities and challenges for Computer and Information Science and Engineering (CISE) researchers and educators to use the cloud, and recommend steps the major stakeholders can take to lower the barriers to cloud adoption. By convening a diverse community of stakeholders who can contribute to the strategic development of a cloud-computing roadmap, this workshop will lower the barriers for cloud adoption for academic research and education in CISE. The perspectives and insights shared from industry, government, and academia, coupled with discussions focused on actionable challenges and measurable outcomes, will serve to advance CISE research and education. The workshop will identify high-priority topics and high-impact opportunities for future research, training, and collaboration in conjunction with cloud computing.<br/><br/>-Part 2<br/><br/>The workshop will engage about 35-45 participants from academic, industry, and government, including principal investigators, educators, cloud-computing researchers, campus CIOs, public cloud providers, and government funding agencies.  The workshop will focus on several key issues including, (1) CISE research and the cloud: What types of CISE research are (or are not) best enabled by cloud platforms? (2) CISE education and the cloud: What types of CISE education are best enabled by cloud platforms? What novel concepts can be taught? (3) Key challenges with cloud usage: What are the barriers to cloud adoption and how should the stakeholders address these issues? (4) Relationship among various types of resources: What roles should on-campus resources, national resources, and cloud-provided resources each play in supporting research and teaching? (5) Cloud costs: How can researchers and educators best manage the costs of cloud usage, including the risks of runaway costs or costs that continue after a grant funding a project has ended? (6) Education for prospective cloud users: What are good ways to educate and train researchers, students, and campus IT staff on using the cloud effectively? (7) Ongoing user support: What ongoing support is needed for academic users, whether locally or at the national level? <br/><br/>The workshop will host a series of presentations and working sessions with live note-takers, resulting in a post-meeting white paper describing key themes, insights, and recommendations from the discussions. Lowering the barriers to academic use of the cloud has the potential to significantly improve how CISE researchers and educators conduct their work. Using the cloud can reduce IT costs, foster cross-institution and interdisciplinary research collaborators, and better train students for a cloud-centric world. Innovations in cloud computing services can help cloud providers recognize new opportunities ahead of future commercial demands, while better supporting academic research and education."
"1916805","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/01/2019","Madhav Marathe","VA","University of Virginia Main Campus","Standard Grant","Robert Beverly","10/31/2023","$2,880,000.00","","mvm7hz@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835598","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Jurij Leskovec","CA","Stanford University","Standard Grant","Robert Beverly","10/31/2023","$540,000.00","","jure@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800452","I-Corps: Intelligent Traffic Management System","IIP","I-Corps","01/01/2018","11/17/2017","Anuj Sharma","IA","Iowa State University","Standard Grant","Pamela McCauley","06/30/2018","$50,000.00","","anujs@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will be significant reductions in traffic congestion, vehicle crash risk, and fuel consumption. This will potentially have large economic benefits as traffic congestion causes significant costs to the economy. It is anticipated that intelligent traffic incident management system will be used by state departments of transportation (DOTs) to reduce the duration and impacts of incidents and improve the safety of motorists, crash victims, and emergency responders. Additional benefits to the DOTs will be: reduced personnel training needs, improved workload conditions, and increased worker retention rates.  State, municipal and city agencies managing traffic will use this solution as a smart and reliable decision-assist system to monitor traffic conditions in real time, proactively control risk using advisory control, quickly detect traffic incidents, identify the location and potential cause of incidents, suggest traffic control alternatives, and minimize cognitive bottlenecks for traffic incident management operators.<br/><br/>This I-Corps project is focused on understanding the product-market fit for intelligent traffic management systems. The proposed system uses novel machine learning techniques and graph-based trend filtering approaches for anomaly detection and state estimation for massive, spatially correlated, multi-dimensional time series data obtained from sensors that monitor the traffic networks. These approaches have been shown to be superior to the state-of-the-art approaches for detecting faulty sensors and quickly reporting traffic incidents. An advanced human-machine interface will also be provided for the Intelligent Traffic Management system with the aim to reduce the Visual, Auditory, Cognitive and Psychomotor (VACP) workload of the Traffic Incident Managers. The system data architecture uses state-of-the-art data pipelines for data ingestion, massively parallel methods for stream and batch analytics, distributed databases for scalable data storage, and GPU-augmented methods for fast data visualization of large volumes of data."
"1835631","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Geoffrey Fox","IN","Indiana University","Standard Grant","Robert Beverly","10/31/2023","$500,000.00","","gcf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835660","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Madhav Marathe","VA","Virginia Polytechnic Institute and State University","Standard Grant","Robert Beverly","09/30/2019","$2,880,000.00","Edward Fox, Naren Ramakrishnan, Catherine Amelink, Christopher Kuhlman","mvm7hz@virginia.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835441","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Albert Esterline","NC","North Carolina Agricultural & Technical State University","Standard Grant","Robert Beverly","10/31/2023","$40,000.00","","esterlin@ncat.edu","1601 E. Market Street","Greensboro","NC","274110001","3363347995","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835439","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Richard Alo","MS","Jackson State University","Standard Grant","Robert Beverly","10/31/2023","$40,000.00","Natarajan Meghanathan","richard.alo@famu.edu","1400 J R LYNCH ST.","Jackson","MS","392170002","6019792008","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809830","Sustainable Integration of Distributed Energy Resources in Distribution Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","07/01/2018","05/30/2018","Lang Tong","NY","Cornell University","Standard Grant","Radhakisan Baheti","06/30/2021","$390,592.00","Timothy Mount","ltong@ece.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","ENG","7607","155E","$0.00","The proposed research addresses a major obstacle to a large-scale integration of distributed energy resources that have transformative impacts on the distribution systems and the overall energy economics of this century. The goal is to develops system and control theoretic approaches, economic analysis, and data analytical tools for the sustainable integration of distributed energy resources (DER) such as behind-the-meter solar, distributed storage, and demand response in distribution systems. The focus is on the dynamics of DER adoption as results of engineering innovation and market interactions among regulated utilities, emerging energy service providers, community DER, and prosumers. Using nonlinear system theory, optimal control, and machine learning tools, the proposed research aims to characterize the stability of DER adoption, the potential of death spiral induced by the endogenous positive feedback, the impacts of policy prescriptions such as net-metering and tax incentives, and economic implications on equity and fairness. Complementing the theoretic analysis are empirical studies of prevailing pricing mechanisms and new tariff proposals. To this end, this research has a significant component in developing data analytic and machine tools for extracting models from online data. <br/><br/>This research addresses an emerging problem in DER adoption in distribution system. The proposed Research has potential to contribute a fundamental understanding of the interactions among consumers, regulated utilities, and service providers. The research will provide a set of novel system and control theoretic tools in analyzing different DER integration models, market designs, and policy prescriptions. The proposed research advances the state of the art theory and practices and helps to unleash the full potential of distributed energy resources through an innovative, economically viable, and operationally secure pathway to a new paradigm of power distribution. The multidisciplinary nature of this research provides rich experiences for undergraduate and graduate students and enriches curriculum developments in a critical area engineering education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838833","ECDI: Secure Fog Robotics Using the Global Data Plane","CNS","Information Technology Researc, Special Projects - CNS, Networking Technology and Syst","10/01/2018","09/18/2019","John Kubiatowicz","CA","University of California-Berkeley","Continuing Grant","Darleen Fisher","09/30/2021","$1,741,644.00","Ken Goldberg, Anthony Joseph, Joseph Gonzalez","kubitron@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1640, 1714, 7363","7363, 9102","$0.00","This project investigates new ways of structuring and securing information by using cryptographically hardened bundles of data, called DataCapsules.  The need for a new approach stems from the proliferation of data-driven technology and cyber-physical systems that control physical devices, such as robots and manufacturing machines, and use information from widely disparate sources. The consequences of data exposure, breach, or corruption can lead to identity theft, property loss, or (increasingly) bodily harm.  Unfortunately, common approaches to protecting information are ad-hoc, buggy, and subject to a variety of attacks and failure modes. In contrast, the DataCapsule infrastructure provides a standardized approach to sequencing, securing every bit of information while also including explicit provenance (stating who generated it). DataCapsules may move freely from place to place in the network while retaining their integrity, thereby enabling secure computation at the edge of the network.  Further, this project investigates techniques to ease the transition of application writers from current practice to use of the DataCapsule infrastructure. The benefits of standardization around DataCapsules are many fold, including (1) more uniform application of best practices for data security; (2) secure edge computing infrastructures that fluidly interact with authorized entities in the core of the network (cloud); and (3) an opportunity for new networking environments that respect information privacy and security while optimizing for performance and quality of service.<br/><br/>This project explores the use of DataCapsules to improve the security and performance of robotic and machine-learning applications operating in edge computing environments. DataCapsules are secured bundles of information with unique, self-certifying names that are transported over a data-centric 'narrow-waist' infrastructure called the Global Data Plane (GDP). This project investigates the design of DataCapsules as well as an architecture for the GDP that provides flat-address routing from authorized clients to DataCapsules, allowing DataCapsules to be replicated and reside anywhere within the GDP.  DataCapsules consist of standardized metadata wrappers anchoring hash-chain-linked histories of transactions labeled by signatures. As universal 'ground-truths' for data storage applications, DataCapsules share some advantages of block-chains, including publicly verifiable integrity. Above the DataCapsule layer, application writers benefit from uniform security while continuing to utilize common storage access patterns, such as filesystems, databases, and key-value stores. The GDP partitions the network into Trust Domains (TDs) to allow clients to reason about the trustworthiness of hardware. This architecture includes overlay switches connected via a tunneling protocol and a scalable location resolution infrastructure. Each TD is responsible for a subset of the DataCapsules and provides data location facilities that serve 'location delegation' certificates (mapping names to network locations) for these DataCapsules. For scalability, this project investigates several name resolution mechanisms, including one based on distributed hash table (DHT) principles. This project also utilizes secure enclave technologies (e.g. Intel SGX) to provide secure computation at the edge of the network.  By promoting best practice labeling and secure management of information, the DataCapsule infrastructure promises to lead to an overall reduction in data breaches and safer public and private cyberspace infrastructure. Further, it will allow application writers to trust the security of information at the edge of the network, thus leading to new and better application of data-driven techniques at the network edge while simultaneously improving privacy; this, in turn, will lead to better applications, such as robotic and smart manufacturing.  Finally,  in addition to educational activities, the project, in collaboration with the University of California Berkeley's Lawrence Hall of Science, will produce open-access videos to raise awareness of information vulnerability and provenance with youth and the public at large.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758539","SBIR Phase II:  Development of a Safety System for Individuals with Alzheimer's Disease and Related Dementias","IIP","SBIR Phase II","04/01/2018","07/27/2020","George Netscher","CA","SafelyYou Inc.","Standard Grant","Alastair Monk","03/31/2023","$1,559,998.00","Alexandre Bayen","gnetscher@gmail.com","2935 MLK Jr Way, Unit C","Berkeley","CA","947032166","7138226924","ENG","5373","096Z, 165E, 169E, 5373, 8018, 8032, 8042, 8089, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is a video-based safety system for improving the quality and reducing the cost of dementia care. Alzheimer's Disease (AD) affects 5.4M in the US, including 1 in 9 over 65 and 1 in 3 over 85, and represents two thirds of all those affected by dementia. It is the single most expensive disease in the US ($236B direct; $221B indirect costs). The size of the US population with AD doubles every 15 years. The drug failure rate for AD is among the highest, currently 99.6. Fall accidents account for 26% of all AD related hospitalizations and are a major concern and key cost contributor, with an average fall rate of 4 falls per year and 3 in 4 repeated falls. Unfortunately, safety products developed for falls were developed for cognitively aware adults and not designed specifically for individuals with AD. The proposed system will change the quality of care and operations in memory care facilities, increase quality of life for patients and families, and help the medical profession gain a better understanding of dementia. <br/><br/><br/>The proposed project addresses this critical gap in Alzheimer's care by detecting safety critical events, based on camera video where events can be reviewed by a human assistant in real-time and after the fact. It is expected to achieve two main objectives that advance science and technology: (1) Demonstrating the ability for our machine learning algorithms to automatically perform safety critical tasks, by learning over a sufficiently rich set of data. These extend existing methods for fall detection to include (a) fall prediction; (b) wandering detection; (c) bed sores detections; (2) enabling the Company to demonstrate that the system proposed can run with a fully operational HIL paradigm running at scale with up to 1000 patients in 100 memory care facilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837253","CPS: Medium:  Rethinking Communication and Control for Low-Latency, High Reliability loT Devices","ECCS","CPS-Cyber-Physical Systems","09/01/2018","08/29/2018","George Pappas","PA","University of Pennsylvania","Standard Grant","Radhakisan Baheti","08/31/2021","$1,000,000.00","Alejandro Ribeiro, Hamed Hassani","pappasg@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","ENG","7918","7918","$0.00","The internet-of-things (IoT) revolution is bringing millions of physical devices online (e.g. cars, UAVs, homes, medical devices), enabling them to connect to each other in real-time, as well as to cloud services. Wireless communication will be critical in providing IoT connectivity.  There are three main strategic directions that are envisioned in the future wireless networking. First, enhanced mobile broadband will result in even larger data rates for future 3D applications such as virtual reality. Second, Smart City/Community applications require large number of sensors that communicate sporadically over large urban or rural areas in a scalable, asynchronous, and energy efficient manner. The previous two directions, while important, are not the focus of our proposal. Instead, our proposal focuses on low-latency and ultra-reliable communications and networking that is critical for latency-sensitive, closed-loop control applications, like vehicle to vehicle communications, collaborative swam planning, and industrial control. In such latency sensitive applications, we need to rethink the networking stack, coding, networking architecture, and control design to enable communications and networking that can provide ultra-low latency (99.999%). This is far beyond what is currently possible. But even more importantly, we do not know what is possible and what are the fundamental limits for control system design over low-latency, high-reliability communications.<br/><br/>In this proposal, we will be rethinking the scientific foundations for ultra-reliable, low-latency wireless communications for latency sensitive control applications. We propose to achieve our scientific agenda by addressing three intellectual challenges: 1) Low-latency channel coding, where the goal is to focus on short packet codes for control loops 2) Control over low latency-aware communication channels, where the goal is to understand the what is the optimal tradeoff of latency to reliability for control loops and 3) Learning for Large Scale Wireless Control Networks, where machine learning will perform resource allocation for large numbers of control loops with competing latency/reliability requirements We intend to evaluate the proposed research agenda by leveraging our existing Intel Science and Technology Center (ISTC) on Wireless Autonomous Systems and demonstrate our ideas in future wireless protocols (IEEE 802.11ax) and experimentally demonstrate it in high-speed V2V and fast formation control with aerial swarms.  On the educational front, the University of Pennsylvania is planning to offer a Micro-Masters program in Internet of Things (IoT) on the edX MOOC platform. Longer term, our goal is to create a new community of researchers that focus on control over low-latency wireless networks for IoT devices. Towards this goal, we plan on leveraging departmental efforts to increase and diversify the PhD students working on this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758688","Collaborative Research: Bayesian Estimation of Restricted Latent Class Models","SES","Methodology, Measuremt & Stats","08/15/2018","09/11/2018","Yinghan Chen","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Continuing Grant","Cheryl Eavey","07/31/2021","$37,286.00","","yinghanc@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","SBE","1333","9150","$0.00","This research project will advance statistical methods known as cognitive diagnosis models (CDMs). CDMs are the statistical machinery that link cognitive theory with applications in online learning technology. They serve as a framework for providing fine-grained classification of the skills and attributes needed for success in the classroom and beyond. Robust cognitive theory is central to ensure accurate inferences with CDMs. This project will develop new statistical methods for validating cognitive theory in the context of CDMs. The modern classroom generates a wealth of longitudinal information from computerized student assessments. The innovations from this project will provide a framework for human development that harvests the available information to track skill development and to support teachers' instructional decisions in real time. The new methods will be applicable more broadly to other disciplines, such as the social sciences, neuroscience, medicine, and business, as an approach to gain more detailed and nuanced information about cognitive processes underlying human judgment and decision making. Software developed during the course of the project to implement the developed procedures will be made publicly available.<br/><br/>The project will advance statistical and psychometric theory by developing Bayesian methods for estimating the Q matrix for a general class of models. Cognitive theory will be incorporated into CDMs by specifying a Q matrix that catalogues the skills required by each task. The general unavailability of cognitive theory Q matrices for most content areas and research domains poses a barrier to widespread application of CDMs. The project will offer several advances to existing research. The project will develop procedures for estimating Q for the most general restricted latent class model. Bayesian estimation methods will be employed that explicitly enforce identifiability conditions to ensure consistency and accuracy of parameter recovery. Stochastic processes and irreducible transitions will be created to estimate Q when the number of latent attributes is unknown a priori. The project also will consider methods that incorporate expert knowledge in the statistical model to improve estimation of Q and to enhance interpretation of uncovered attributes. Psychometric insights gained from working on this important problem can lead to significant developments in the underlying statistical theory for estimation of cognitive diagnosis model Q matrices and could potentially have an impact on the broad spectrum of applications beyond education and psychology, such as machine learning applications that seek to cluster binary data according to a set of underlying features.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827592","EAGER: Hybrid Precoding for Massive MIMO Communication Networks","ECCS","CCSS-Comms Circuits & Sens Sys","09/15/2018","09/19/2018","Chengshan Xiao","PA","Lehigh University","Standard Grant","Lawrence Goldberg","08/31/2021","$200,000.00","","xiaoc@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","ENG","7564","153E, 7916","$0.00","EAGER: Hybrid Precoding for Massive Multiple-Input Multiple-Output Millimeter Wave Wireless Communication Networks<br/><br/><br/>Future smart and connected communities will place tremendous demand on wireless communications due to the ever-growing popularity of smartphones, autonomous vehicles, and mobile devices. The massive Multiple-Input Multiple-Output (MIMO) technology, in combination with millimeter wave (mmWave) spectrum utilization, is considered as a key breakthrough for enabling enormous data-rate increase for next generation wireless networks.  In massive MIMO systems, a very large number of antennas is employed at the base station to communicate many mobile users simultaneously. However, this large number of antennas can lead to prohibitive cost and power consumption if conventional approach which requires one radio frequency (RF) chain per antenna is adopted. This project focuses on novel hybrid precoding designs which will not only reduce the number of RF chains but also maximize the data rate. The hybrid precoding consists of analog and digital precoders, where the digital precoder is realized by a small amount of RF chains, and the analog precoder is realized by phase shifters. Therefore, the cost, complexity and power consumption of massive MIMO systems can be reduced dramatically. The proposed research can have a large impact on the design and development of future generation of wireless networks, for which massive MIMO and mmWave are key enabling technologies. The research results will be integrated into the classes for electrical engineering and computer engineering majors through designing new course projects. Research findings will be broadly disseminated through conference presentations and journal publications. Moreover, the project will help increase participation of under-represented minorities and enhance outreach activities to attract female students to careers in engineering. <br/><br/>This project aims to investigate hybrid precoding design methods that can drastically reduce the cost, complexity and power consumption while approaching the optimal performance of fully-connected, unconstrained massive MIMO systems. The project formulates the hybrid precoding design of multi-user massive MIMO systems into a joint optimization of analog and digital precoders with dynamic resource allocation which includes subarray selection, power allocation, and modulation-coding-rate selection. The joint optimization will enable the hybrid system with a significantly reduced number of RF chains to achieve similar performance of fully-connected massive MIMO systems at a fractional cost. The dynamic resource allocation will help to achieve best throughput for given channel conditions. Furthermore, the proposed approach utilizes finite-alphabet inputs and statistical channel state information (CSI) instead of the idealistic Gaussian inputs and instantaneous CSI, thus improving the robustness of the optimized precoders for practical systems. The objective of this project is expected to be accomplished by three specific tasks. First, theoretical studies of the achievable data rates will address the fundamental tradeoff between performance and cost of hybrid precoding.  Second, algorithm research will derive low-complexity solutions to solve the NP-hard optimization problems. Third, machine learning techniques will be applied to learn the features and transition probabilities of the Markov decision processes governing the online resource allocation and hybrid precoding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814947","AF: Small: Multiparty Communication, Polynomials, and Noise","CCF","Algorithmic Foundations","06/01/2018","05/15/2018","Alexander Sherstov","CA","University of California-Los Angeles","Standard Grant","Tracy Kimbrel","05/31/2021","$500,000.00","","sherstov@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7796","7923, 7926, 7927","$0.00","Communication complexity theory studies the minimum amount of communication, measured in bits, required in order to compute functions whose arguments are distributed among several parties. In addition to the basic importance of studying communication as a bottleneck resource, the theory has found a vast number of applications in many areas, including machine learning, mechanism design, streaming algorithms, data structures, pseudo-random generators, and VLSI layouts. This project tackles fundamental questions whose resolution will have a significant impact on the discipline. This work will exploit insights from, and contribute new ideas to, other areas such as quantum computing, computational learning, and approximation theory. The project is an ample source of research problems at various levels of difficulty and will be used in advising graduate and undergraduate students.  The investigator will integrate this research into his graduate and undergraduate teaching, take an active part in scientific dissemination, and promote theoretical computer science in Southern California.<br/><br/>This project comprises two related components.  First, the investigator will tackle longstanding open problems in the study of multiparty communication, such as settling the communication requirements of the set disjointness problem and breaking the logarithmic barrier for multiparty communication lower bounds. The second, complementary component of this project will advance the study of analytic representations of Boolean functions. Here, the investigator aims to obtain tight lower bounds for the polynomial approximation and sign-representation of the k-element distinctness function, constant-depth circuits, and Boolean formulas of arbitrary depth.  The two research components of this project are intimately related in that they require the same class of analytic techniques. Indeed, major advances in communication complexity over the past two decades have been obtained by transforming, explicitly or implicitly, communication protocols into multivariate polynomials of comparable complexity and by analyzing the resulting approximation questions. The planned research on multiparty communication and polynomials is further unified by a focus on noise, in the sense of imperfect output or adversarial corruption of intermediate computations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752170","CAREER:Feedback-Controlled Microfluidic Chips with Integrated Sensor Networks for Blood Analysis","ECCS","CCSS-Comms Circuits & Sens Sys","03/15/2018","03/05/2018","Ali Fatih Sarioglu","GA","Georgia Tech Research Corporation","Standard Grant","Shubhra Gangopadhyay","02/28/2023","$500,000.00","","sarioglu@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","7564","1045, 104E","$0.00","Efficient exchange of metabolic signals with other tissues turns the blood into an opportunity to monitor and diagnose physiological and pathological conditions. Among the constituents of blood, white blood cells represent a particularly rich source of information due to their active involvement in the immune response of the body. As such, technologies that can rapidly characterize blood samples and extract reliable information are in ever-increasing demand for both clinical and basic research applications. The proposed work aims to develop smart microchips that can reliably analyze white blood cells from small blood samples without any sample preparation. These microchips will be low-cost, disposable, and will include built-in electrodes that can convert the chemical information from white blood cells into electrical signals to be interpreted by a smartphone and transmitted to the healthcare provider. The proposed research therefore has the potential to revolutionize healthcare delivery by enabling people to self-administer blood tests at home or in mobile settings. Besides his research, the PI is fully committed to the educational aspects of his profession and aspires to be a role model for next-generation engineers. The PI's educational goal is to create application-focused multidisciplinary courses, research opportunities and learning experiences for students. To this end, the PI proposes (1) to organize innovation tournaments to develop micro/nanotechnologies for solving biomedical challenges, (2) to implement a laboratory module in the graduate- and undergraduate-level courses, (3) to involve and mentor undergraduate and graduate students in conducting the research activities of this proposal, (4) to mentor high school teachers to attract K-12 and High School Students and underrepresented groups to science, technology, engineering and mathematics (STEM) education.<br/><br/>Despite being highly effective in manipulating cells, microfluidic devices lack native sensing schemes and hence often act as upstream sample preparation elements before quantitative measurements typically performed with a laboratory instrument. The disconnect between microfluidic manipulation and quantitative measurements is an important limitation that hampers the widespread adoption of these tools outside of academic research laboratories, for example in resource-limited or in point-of-care settings, where they can be truly transformative in healthcare delivery. The PI's career goal is to develop polymer-based lab-on-a-chip (LoC) platforms with built-in sensor networks, whose purposely simple hardware will be augmented by complex computational algorithms, to function as content-aware, autonomous microfluidic devices for quantitative cell analysis. To achieve this goal, the PI will adopt a highly multidisciplinary approach combining traditionally-distant disciplines such as microsystem engineering, information theory, data science, and biomedicine. This proposal will (1) design and fabricate plastic microfluidic chips wired with networks of<br/>interconnected electrical micro-sensors, each individually designed to produce a signature response that can be recognized among others through computation, (2) develop computational algorithms to process compressed data from the sensor network by utilizing both model-based signal processing and machine learning approaches, (3) develop open- and closed-loop controlled microfluidic systems, where a host of<br/>actuators are combined with spatiotemporal data generated by networked sensors to extract biological information from the sample under test, (4) combine all of the developed concepts to create an autonomous and adaptive microfluidic system that can analyze whole blood samples by label-free immunophenotyping of white blood cells.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1855759","RI: Small: Collaborative Research: A Topological Analysis of Uncertainly Representation in the Brain","IIS","Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","08/26/2018","11/01/2018","Chao Chen","NY","SUNY at Stony Brook","Standard Grant","Rebecca Hwa","07/31/2020","$254,827.00","","chao.chen.1@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7495, 8624","7495, 7923, 8089, 8091","$0.00","Characterizing how brain regions activate, collaborate, and interact in cognition empowers us with advanced approaches to help humans make the right decisions on high stress jobs, prevent drug abuse, and treat neurological disorders. This project will study cognitive control in terms of the uncertainty representation, namely, how brains execute the same cognitive task with different levels of uncertainty. Based on theory and algorithms in topology data analysis, the project will analyze brain functional MRI images using novel topological descriptors, which directly model global interactions between brain regions in a principled manner. These descriptors will be used in novel learning models to discover brain activity patterns that are crucial for uncertainty representation. The outcome of the project will include (1) new knowledge in uncertainty representation, e.g., fine-scale activity patterns and interactions between brain regions correlated to the uncertainty level; (2) new topological analysis tools for brain imaging study. This project will bring research and educational opportunities to graduate and undergraduate students from both computer science and neuroscience. The PIs will also mentor students from underrepresented groups and high school students through the CUNY College Now program.<br/><br/>This project will create new computational topology algorithms to extract rich information from the intrinsic structure of data. Novel machine learning methods will be created in order to leverage the topological structures for not only prediction, but also knowledge discovery. A novel interactive data exploration platform based on topological features will be developed for brain imaging study. These techniques and software will be validated on task-evoked fMRI data to produce quantitative assessments of accuracy and to characterize advantages and limitations of these approaches. Domain experts will validate the quality of the approach in validating scientific hypotheses and data exploration."
"1827183","IRES Track I: Collaborative Research: Interdisciplinary Research in Korea on Applied Smart Systems (IRiKA) for Undergraduate Students","OISE","IRES Track I: IRES Sites (IS)","10/01/2018","08/16/2018","Jin-Woo Choi","LA","Louisiana State University","Standard Grant","Maija Kukla","09/30/2021","$87,660.00","","choijw@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","O/D","7727","5942, 5978","$0.00","Interdisciplinary Research in Korea on Applied smart systems (IRiKA) for Undergraduate Students will provide a cohort of five US undergraduate students per year with the opportunity to conduct research for 8 weeks at Korea's top-ranked universities with state-of-the-art research facilities: Seoul National University, Korea Advanced Institute of Science and Technology and Ewha Womans University. Over the lifetime of this 3-year project, 15 students will participate. Students from underrepresented groups will be recruited. The unifying research theme of IRiKA is smart systems with the subtopics of sensors, emerging electronics, and materials and process development. In addition to lab work and weekly cohort research meetings, IRiKA students will visit Korea's government research institutions and global leaders in the tech industry such as Samsung, LG, and Hyundai. The distinctive features of IRiKA are: 1) A cohort experience bringing the US participants together; 2) Vetted and structured professional development program tailored for both US students and Korean mentors; and 3) Availability of follow-on collaborative projects in US and Korea to facilitate a sustained global network of mentorship. Students will gain formative research skills and learn how smart systems brings together interdisciplinary technological solutions for manufacturing, healthcare, energy, safety and security, transportation, and logistics. The international aspect of the IRiKA will help students recognize their place in the global scientific community. The participants will present their work at their home institutions upon their return and will be incentivized to publish in a peer-reviewed journal or present at a conference for broader dissemination. <br/><br/>Interdisciplinary Research in Korea on Applied smart systems (IRiKA) for Undergraduate Students will engage students in interdisciplinary research, help them develop a global perspective on collaboration, and motivate them to pursue a career in STEM research. Efforts will be made to attract students underrepresented in STEM and/or with limited STEM research opportunities. IRiKA takes a scaffolded mentorship approach that fosters students' growth from a relatively dependent status to as independent a status as their competence warrants. Smart systems incorporate sensing, actuation, wireless connectivity, and machine learning. Examples of research projects that individual students will conduct during 8 weeks in Korea include: Development of an air-borne particle sensing system for health monitoring and air quality monitoring; Development of miniature and micro power generation systems to enable autonomous sensor systems; and Development of a light-weight, flexible point-of-care device consisting of microfluidic channels and reduced graphene oxide-based biosensors. Upon return to the US, students will be able to engage in follow-on projects in areas such as convergent IoT systems at University of Florida's NSF I/UCRC Multi-functional Integrated Systems Technology (MIST) center and wearable sensor systems at Louisiana State University.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830372","ATD: Collaborative Research: Adaptive and Rapid Spatial-Temporal Threat Detection over Networks","DMS","ATD-Algorithms for Threat Dete, ","09/01/2018","06/14/2019","Sarah Holte","WA","Fred Hutchinson Cancer Research Center","Continuing Grant","Leland Jameson","08/31/2021","$62,450.00","","sholte@fredhutch.org","1100 FAIRVIEW AVE N J6-300","Seattle","WA","981094433","2066674868","MPS","046Y, S100","6877","$0.00","This project aims to develop innovative machine learning and statistical algorithms for detecting, preventing, and responding to threats over networks. Two concrete applications are monitoring the threat of multi-antibiotic-resistant (MDR) gonorrhea from a network of clinics across the United States and monitoring HIV transmission in clusters of patients. The research has impact in many other practical applications, including biosurveillance, engineering, homeland security, finance, and public health, where large-scale spatial-temporal data streams are collected with the aim of rapid detection and prevention of threats. The research aims to develop crucial scalable algorithms and methods to effectively and efficiently monitor, analyze, and optimize responses in these situations. In addition, the project will integrate research and education by infusing the research findings into the curriculum and by involving Ph.D. students in research.  <br/><br/>This project aims to develop innovative algorithms for rapid threat detection by combining spatial-temporal models, ordinary differential equation (ODE) models with change-point detection, and multi-armed bandit and ensemble methods when monitoring large-scale spatial-temporal data over networks. In particular, efficient scalable algorithms are developed in three interrelated research tasks, including (1) rapid detection of threats by combining a ""background + anomaly + noise"" decomposition framework with sequential change-point detection; (2) predictive analytics of threats by applying multi-armed bandit algorithms and adaptive sampling in the changing environments to assess increasing risks at the population level; and (3) prescriptive analytics of threats by developing nested ensemble models based on calibrated ODE and data-driven spatial-temporal models so as to better assess the effects of prevention/intervention actions. Results of the project are expected to significantly advance the state of the art in spatial-temporal models, online learning, streaming data analysis, and large-scale inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763268","RI:Medium:Collaborative Research: Object-Centric Inference of Actionable Information from Visual Data","IIS","Robust Intelligence","09/01/2018","08/14/2018","Leonidas Guibas","CA","Stanford University","Standard Grant","Jie Yang","08/31/2021","$475,000.00","","guibas@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","7495, 7924","$0.00","This project will create novel algorithms and learning architectures suitable for understanding how to plan and execute actions in an environment for purposeful object manipulation. Such understanding is indispensable for autonomous agents operating in unstructured environments, and it is also valuable in providing automated assistance to humans during the execution of various physical tasks. The project will computationally ""imagine"" changes that actors with human-like manipulation capabilities can make on that environment and generate plans that can accomplish the desired manipulations. Such tools facilitate the creation of smart environments, where for example a perception system watching an elderly person can infer the task the person is trying to accomplish and offer advice/assistance. They also allow the creation of automated instructional videos customized to a particular environment that can be used for efficient training of unskilled workers. The project will provide mentoring and research opportunities for a diverse set of students, including members of groups typically under-represented in computer science.<br/><br/>This research will study environments formed by objects, some of which can be manipulated, while others define obstacles to be avoided or support surfaces to be used. Manipulating an object typically means interacting with small parts of the object, referred to as its active sites: handles, buttons, levers, graspable or pushable regions, etc. A deep challenge is to develop tools for identifying and classifying these active sites on objects at large scale, and to codify the types of interactions they partake of based on dynamic 2D/3D imagery, building a vocabulary of elementary actions. This requires novel machine learning methods and deep architectures for processing large-scale dynamic visual and geometric data. It also requires characterizing manipulations at a more abstract level so that they can be used by a variety of effectors, robotic or human, on different object geometries and physical characteristics. A further challenge is the accumulation and update of actionable information as more visual data is received in online object model repositories, such as ShapeNet. A final but key step of the approach will be the development of tools for transporting such action knowledge to new settings that are similar but not identical to the capture settings, using a variety of mathematical tools including functional maps.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834305","Travel Support:  XI Iberoamerican Conference on Phase Equilibria and Fluid Properties for Process Design, Equifase 2018","CBET","Proc Sys, Reac Eng & Mol Therm","05/15/2018","05/15/2018","Walter Chapman","TX","William Marsh Rice University","Standard Grant","Triantafillos Mountziaris","04/30/2019","$30,000.00","","wgchap@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","ENG","1403","7556, 8248","$0.00","New thermodynamic data and models are essential for optimizing the performance of conventional chemical processes and for developing new energy efficient and environmentally sustainable ones. Furthermore, thermodynamic data and models are central to scientific advances in designing and manufacturing advanced materials.  This grant will provide partial support for US-based junior faculty and young researchers from diverse backgrounds to attend the XI Iberoamerican Conference on Phase Equilibria for Process Design (EQUIFASE 2018). The conference will be held from October 22 - 25, 2018 in the city of Carlos Paz, in the province of Cordoba, Argentina. The goal of this international conference is to identify new opportunities in research and education and to promote international collaboration related to basic data for chemical process design and optimization. The conference is unique in the wide participation of researchers and students from South American countries, providing the only meeting that brings much of this community together with researchers from the international community.<br/><br/>EQUIFASE 2018 will focus on improved thermodynamic data and advanced computational models to enable optimal design of processes related to energy conversion and storage, chemical processing, materials fabrication, to maximize energy efficiency and environmental sustainability. Intellectual merits of the conference include: (1) Identification of key challenges in conventional process industries and in new areas of process intensification related to processing of energy, chemicals, pharmaceuticals, and bio-chemicals. (2) Development of a roadmap to address these challenges by promoting collaboration that integrates methods of experimental design, molecular modeling and computation, and modern approaches of machine learning. Education will be a key topic of the conference with focus on international collaborations in distance learning and open courses.  The young conference participants, many from underrepresented groups in STEM, will benefit from state-of-the-art reviews and presentations by leading experts, by exchanging the latest advances on frontier research subjects relating to molecular thermodynamics and chemical processes, and by debating controversial points with their peers. The conference will also provide an excellent opportunity for interaction and cooperation among academic and industrial researchers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746694","SBIR Phase I:  A Blood-Based Test to Identitfy Patients with Intracranial Aneurysm","IIP","SBIR Phase I","01/01/2018","12/20/2017","Vincent Tutino","NY","Neurovascular Diagnostics, Inc.","Standard Grant","Henry Ahn","12/31/2018","$224,032.00","","vtutino@nvdiag.com","8210 Golden Oak Cir","Buffalo","NY","142218504","5857030490","ENG","5371","5371, 8038","$0.00","This SBIR Phase I project aims to develop a novel blood diagnostic to detect unruptured intracranial aneurysms (IA) in asymptomatic patients. About 2-5% of the U.S. population (about 6-17 million Americans) have an unruptured IA, and these individuals are largely asymptomatic and thus unaware of the potential danger they are in. Currently, no good screening tools to identify patients with unruptured IAs exist. As a result, about 30,000 Americans suffer IA rupture each year without warning, 10-15% of whom die on the way to the hospital and another 30-40% of whom die within a month. The diagnostic screening technology developed in this project will identify people who have unruptured IAs, thus enabling patients to be monitored and receive preventative treatment, which can drastically reduce the rate of rupture. In addition to the health benefits of this non-invasive test, it will also result in massive savings for the healthcare system. The estimated lifetime healthcare costs for annual cases of patients with ruptured IA is about $3 billion, and more than $885 million for patients with unruptured IAs. Plus, the annual lost wages of surviving ruptured IA patients and their caretakers combined is an estimated $138 million.<br/><br/>This project aims to develop a molecular diagnostic to detect biomarkers of unruptured aneurysms using the transcriptomes of circulating neutrophils. Preliminary results have shown that circulating neutrophils isolated from blood samples could be used to predict unruptured IA presence with 80% accuracy. This Phase I project will increase the sample size of the previous discovery and validation cohorts to give more confidence in the discovered biomarkers as well as increase the accuracy of the proposed diagnostic. Transcriptomes of neutrophil RNA from patients with and without aneurysms will be obtained through next-generation sequencing. The transcriptome data will be used to further develop biomarker models by employing a machine learning pipeline that uses supervised learning combined with 10-fold cross-validation to prevent over-fitting. The created biomarker models will be validated using neutrophil RNA expression from an independent cohort of patients. Predictive accuracy of 90% with an AUC greater than 0.80 will be used to measure success. Furthermore, to established feasibility of assessing the biomarkers via an inexpensive test, differential expression of biomarker genes will be tested using RT-qPCR, a cheaper, more facile technique than RNA sequencing. <br/>"
"1816499","RI: Small: Dynamics of repulsion and reinforcement in point process, latent variable, and trajectory models","IIS","Robust Intelligence","08/15/2018","08/11/2018","Vinayak Rao","IN","Purdue University","Standard Grant","Rebecca Hwa","07/31/2021","$234,003.00","","varao@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7495","7495, 7923","$0.00","Most traditional ways of analyzing data assume that data points are sampled independently of one another.  In many problems, however, this assumption is incorrect.  This project focuses on data where one observation influences others, either as reinforcing (likely to have a similar value) or repulsing (likely to have a greatly different value). Such interactions might arise between static measurements, between trajectories evolving in space/on a network, or may be desirable biases in algorithms to promote goals like robustness, diversity or fairness. These might arise as a consequence of competition for finite resources, because of rich-get-richer dynamics from propagating social influence, because of interacting processes in physical and biological systems, or out of a desire to learn compact representations of complex systems. Examples include the locations of cells or service stations, interactions among particles or populations, traffic trajectories, users navigating social media, the spiking of neurons or the spread of disease. The research brings together applied problems and theoretical ideas from fields like machine learning, statistics, physics and computer science. Such tools open new avenues to data-summarization, exploration and visualization, and allow practitioners to explore trade-offs between interpretability and predictive accuracy. The applied aspects of this project provide an opportunity for undergraduate research and for the integration of research and teaching through an undergraduate course on stochastic processes and simulation.<br/><br/>At a technical level, this project develops principled statistical models and efficient algorithms that relax assumptions of independence among observations lying on a shared space. It considers interactions for three classes of problems: 1) point process models, 2) latent variable models and 3) trajectory models. Central to the work are two kinds of stochastic process models: the Hawkes process for reinforcement, and the Matern type-III process for repulsion. Both processes share intuitive and mechanistic generative schemes from an underlying Poisson process, whose rate is modulated by event history. This allows a framework that jointly models richer repulsive and reinforcing interactions in stationary and trajectory data. The connection with the Poisson process allows novel models and mechanisms of reinforcement and repulsion, as well as new, scalable algorithms, allowing investigations into the fundamental role of non-Poissonness in real applications. Incorporating repulsive priors into latent variables of hierarchical models also allow novel repulsive latent variable models with biases towards parsimony and interpretability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829729","Collaborative Research: CyberTraining: CIC: Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP)","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","08/01/2018","07/02/2018","G.J. Peter Elmer","NJ","Princeton University","Standard Grant","Alan Sussman","07/31/2021","$375,041.00","Ian Cosden","gelmer@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","044Y, 7244","026Z, 062Z, 7361, 7569, 9102, 9179","$0.00","High-energy physics (HEP) aims to understand the fundamental building blocks of nature and their interactions by using large facilities such as the Large Hadron Collider (LHC) at the European Laboratory for Particle Physics (CERN) in Switzerland and the Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) planned for the 2020s at Fermilab, in Illinois, as well as many smaller experiments. These experiments generate ever increasing amounts of data and rely on a sophisticated software ecosystem consisting of tens of millions of lines of code that is critical to mine this data and produce physics results. People are the key to developing, maintaining, and evolving this software ecosystem for HEP experiments over many decades. Building the necessary software requires a workforce with a mix of HEP domain knowledge and advanced software skills. The Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP) project provides a training path from a researcher's first steps through active contribution to software training and workforce development. The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure and impacts STEM disciplines in terms of much needed and sought after software training.<br/><br/>The FIRST-HEP project directly organizes training activities and works with partners to leverage and bring synergy to disparate existing efforts in order to maximize their collective impact. It brings together an extended set of partners from the community to build not only missing basic training elements like introductory programming skills in Python, git and Unix but  also use of HEP data formats like ROOT and advanced topics including parallel programming, performance tuning, machine learning and data science for Ph.D. students. It works to build a community of instructors and experiments around the software training material and transforms the approach for research software training in HEP. It builds the workforce required for the cyberinfrastructure challenges of running and planned HEP facilities and experiments in the coming years.The FIRST-HEP education and training activities include specific goals to educate minorities in HEP, K- 12 educators and the broader STEM workforce. The K-12 teachers learn very basic skills of Unix including file management, programming languages, such as C+ and shell scripting. FIRST-HEP harnesses the potential of the underrepresented groups and works to ensure that the pool meets or exceeds the diversity in the larger HEP graduate student population when selecting both training participants and instructors for the HEP fundamental training sessions and the advanced computing schools. FIRST-HEP includes a dedicated outreach activity on cybertraining to the local Puerto Rico public. FIRST-HEP leverages engagement with the Software Carpentries to host training of  K-12 teachers at UPRM in basic Software Carpentry skills and who in turn train their students. This encourages the teachers and school authorities to consider incorporating the basic carpentries into the high school curriculum. The training and cyber skills gained during the FIRST-HEP fundamental training courses directly contribute to the broader STEM workforce and trains students to pursue data science careers and other research areas besides HEP, such as Astronomy, where similar software skills are required.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819166","Fast and Reliable Hierarchical Structured Methods for More General Matrix Computations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2018","06/21/2018","Jianlin Xia","IN","Purdue University","Standard Grant","Leland Jameson","07/31/2021","$199,148.00","","xiaj@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1271","062Z, 9263","$0.00","Large matrix computations play a critical role in modern scientific computing tasks and engineering simulations. Realistic computations usually involve enormous amounts of data due to large dense matrices or dense intermediate matrix blocks, which makes classical matrix methods impractical. Hierarchical structured methods provide an effective and reliable way to compress and process large matrix data. In such methods, dense matrix blocks are approximated by compact structured forms that are convenient to handle. This research project aims to develop theoretical foundations for understanding multiple hierarchical structured techniques and for designing new hierarchical structured algorithms. These algorithms are expected to be applicable to more general matrix computations and challenging applications where usual structured methods are not suitable or effective.<br/><br/>Hierarchical structured methods exploit inherent structures in matrix computations to gain high efficiency while ensuring superior stability. This project is concerned with the design, analysis, and application of fast and reliable hierarchical structured methods for broad classes of challenging computations. A unified framework will be provided to understand multiple types of hierarchical structured methods, design new hierarchical methods with enhanced applicability, and analyze their accuracy and stability. State-of-the-art fast and stable solvers will be developed for tackling challenges such as large data sizes, ill conditioning, high frequencies, and multiple frequencies. The new solvers will be applicable to a wide range of matrix computations. Based on data sparsity and enhanced stability, the solvers will significantly improve the efficiency and reliability of many computations in PDE solution, large data analysis, network, machine learning, imaging, seismic modeling, electromagnetics, etc. The research will also make fast and stable structured solvers widely accessible to broader fields and industries. The data will be included in data repositories for unrestricted access. Open-source packages and educational/tutorial materials will be freely available. The multidisciplinary project will provide excellent opportunities for graduate and undergraduate students from diverse backgrounds to closely interact and to learn critical computational and mathematical skills from multiple fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830344","ATD: Collaborative Research: Adaptive and Rapid Spatial-Temporal Threat Detection over Networks","DMS","ATD-Algorithms for Threat Dete, ","09/01/2018","06/14/2019","Yajun Mei","GA","Georgia Tech Research Corporation","Continuing Grant","Leland Jameson","08/31/2021","$119,168.00","","yajun.mei@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","046Y, S100","6877","$0.00","This project aims to develop innovative machine learning and statistical algorithms for detecting, preventing, and responding to threats over networks. Two concrete applications are monitoring the threat of multi-antibiotic-resistant (MDR) gonorrhea from a network of clinics across the United States and monitoring HIV transmission in clusters of patients. The research has impact in many other practical applications, including biosurveillance, engineering, homeland security, finance, and public health, where large-scale spatial-temporal data streams are collected with the aim of rapid detection and prevention of threats. The research aims to develop crucial scalable algorithms and methods to effectively and efficiently monitor, analyze, and optimize responses in these situations. In addition, the project will integrate research and education by infusing the research findings into the curriculum and by involving Ph.D. students in research.  <br/><br/>This project aims to develop innovative algorithms for rapid threat detection by combining spatial-temporal models, ordinary differential equation (ODE) models with change-point detection, and multi-armed bandit and ensemble methods when monitoring large-scale spatial-temporal data over networks. In particular, efficient scalable algorithms are developed in three interrelated research tasks, including (1) rapid detection of threats by combining a ""background + anomaly + noise"" decomposition framework with sequential change-point detection; (2) predictive analytics of threats by applying multi-armed bandit algorithms and adaptive sampling in the changing environments to assess increasing risks at the population level; and (3) prescriptive analytics of threats by developing nested ensemble models based on calibrated ODE and data-driven spatial-temporal models so as to better assess the effects of prevention/intervention actions. Results of the project are expected to significantly advance the state of the art in spatial-temporal models, online learning, streaming data analysis, and large-scale inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756029","CRII: SCH: A Framework for Optimizing Exoskeleton-Assisted Walking Performance in Children with Cerebral Palsy","IIS","Smart and Connected Health","06/01/2018","05/02/2019","Zachary Lerner","AZ","Northern Arizona University","Standard Grant","Wendy Nilsen","05/31/2021","$190,285.00","","Zachary.Lerner@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","8018","8018, 8228, 9251","$0.00","The ability to walk is essential for health and well-being, particularly during childhood development. Children with cerebral palsy have neuromuscular impairments that often diminish walking capacity. Wearable exoskeletons may offer a compelling compliment to surgery and standard physical therapy. However, to be effective, exoskeleton assisted rehabilitation must be finely tailored to each individual. By seeking to improve the effectiveness of exoskeleton-assisted gait training, knowledge gained from this proposal will provide the basis for future exoskeleton intervention studies that will seek to improve underlying gait function and increased levels of habitual physical activity in cerebral palsy. The outcomes of this research have the potential to transform the treatment of pediatric gait disorders, lead to improved health, and reduce the economic burden for individuals with cerebral palsy. In addition to the scientific and societal impacts, this project seeks to broaden research participation from individuals underrepresented in STEM fields by offering interdisciplinary training to a diverse cohort of students. This research will positively impact Northern Arizona University by elevating the research profile of the Center of Bioengineering Innovation and Bioengineering PhD program.<br/><br/>Machine learning techniques have begun to demonstrate potential in improving human-wearable robot interactions but investigations to date have focused on single joints and short durations. The purpose of this proposal is to utilize computational intelligence to establish a framework suitable for optimizing patient-specific assistance and enhancing neuro-muscular participation during training with wearable exoskeletons. Investigators will implement, in an exoskeleton platform, an optimization framework that can learn from biomechanical responses to exoskeleton assistance to quickly converge on an optimal control strategy for the timing and magnitude of assistive joint torques. The first goal is to establish the optimization algorithms and parameters of an exoskeleton control algorithm that suitably adapts knee and ankle assistance based on walking performance. The second goal is to conduct a pilot study to determine how knee and ankle exoskeleton assistance, optimized for improving lower-extremity posture, affects gait biomechanics and neuromuscular control during consecutive daily gait training in children with cerebral palsy. It is anticipated that this research will lead to an improved understanding of the algorithm parameters needed to account for stride-to-stride and session-to-session variability for longitudinal optimization of exoskeleton control for gait rehabilitation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758631","Collaborative Research: Bayesian Estimation of Restricted Latent Class Models","SES","Methodology, Measuremt & Stats","08/15/2018","07/08/2019","Steven Culpepper","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Cheryl Eavey","07/31/2021","$300,000.00","Feng Liang, Jeffrey Douglas, Yuguo Chen","sculpepp@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","SBE","1333","","$0.00","This research project will advance statistical methods known as cognitive diagnosis models (CDMs). CDMs are the statistical machinery that link cognitive theory with applications in online learning technology. They serve as a framework for providing fine-grained classification of the skills and attributes needed for success in the classroom and beyond. Robust cognitive theory is central to ensure accurate inferences with CDMs. This project will develop new statistical methods for validating cognitive theory in the context of CDMs. The modern classroom generates a wealth of longitudinal information from computerized student assessments. The innovations from this project will provide a framework for human development that harvests the available information to track skill development and to support teachers' instructional decisions in real time. The new methods will be applicable more broadly to other disciplines, such as the social sciences, neuroscience, medicine, and business, as an approach to gain more detailed and nuanced information about cognitive processes underlying human judgment and decision making. Software developed during the course of the project to implement the developed procedures will be made publicly available.<br/><br/>The project will advance statistical and psychometric theory by developing Bayesian methods for estimating the Q matrix for a general class of models. Cognitive theory will be incorporated into CDMs by specifying a Q matrix that catalogues the skills required by each task. The general unavailability of cognitive theory Q matrices for most content areas and research domains poses a barrier to widespread application of CDMs. The project will offer several advances to existing research. The project will develop procedures for estimating Q for the most general restricted latent class model. Bayesian estimation methods will be employed that explicitly enforce identifiability conditions to ensure consistency and accuracy of parameter recovery. Stochastic processes and irreducible transitions will be created to estimate Q when the number of latent attributes is unknown a priori. The project also will consider methods that incorporate expert knowledge in the statistical model to improve estimation of Q and to enhance interpretation of uncovered attributes. Psychometric insights gained from working on this important problem can lead to significant developments in the underlying statistical theory for estimation of cognitive diagnosis model Q matrices and could potentially have an impact on the broad spectrum of applications beyond education and psychology, such as machine learning applications that seek to cluster binary data according to a set of underlying features.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836469","Supporting U.S.-Based Students to Participate in the 2018 IEEE International Conference on Data Mining (ICDM 2018)","IIS","Info Integration & Informatics","06/01/2018","05/16/2019","Wei Ding","MA","University of Massachusetts Boston","Standard Grant","Maria Zemankova","05/31/2020","$25,000.00","","wei.ding@umb.edu","100 Morrissey Boulevard","Dorchester","MA","021253300","6172875370","CSE","7364","7364, 7556","$0.00","This grant provides international travel support for approximately sixteen U.S. based graduate student participants to attend the 2018 International Conference on Data Mining, to be held in Singapore, November 17-20, 2018. ICDM is the world's premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative and practical development experiences. The total number of ICDM participants in the past has been in excess of 500, with a majority of the participants from the U.S., then Europe and Asia. Besides the technical program, ICDM 2018 features workshops, tutorials, panels, demos, data challenge competition, and a Ph.D. forum. The conference recognizes the importance of recruiting and engaging students in data mining research in early stage of their graduate study. The Ph.D. forum is designed to provide an interactive environment in which Ph.D. students can meet, exchange their ideas and experiences both with peers and with senior researchers from the data mining community in an international scope. The supported students will advance their skills and knowledge, learn about the latest research, and develop their research networks in data mining. A strong representation of U.S. researchers at the conference is useful in maintaining U.S. competitiveness in this important area. <br/><br/>The ICDM conference series promotes novel, high-quality research findings, and innovative solutions to challenging data mining problems, and seeks to advance the state-of-the-art in data mining. The conference covers all aspects of data mining, including algorithms, software, systems, and applications. ICDM draws researchers, application developers, and practitioners from a wide range of data mining related areas such as statistics, machine learning, pattern recognition, databases, data warehousing, data visualization, knowledge-based systems, and high-performance computing. The conference proceedings are published by IEEE. The recipients of the NSF travel support will be listed at the ICDM 2018 conference website, together with additional ICDM 2018 information.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1764078","RI:Medium:Collaborative Research: Object-Centric Inference of Actionable Information from Visual Data","IIS","Robust Intelligence","09/01/2018","08/14/2018","Hao Su","CA","University of California-San Diego","Standard Grant","Jie Yang","08/31/2021","$425,000.00","","haosu@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7495, 7924","$0.00","This project will create novel algorithms and learning architectures suitable for understanding how to plan and execute actions in an environment for purposeful object manipulation. Such understanding is indispensable for autonomous agents operating in unstructured environments, and it is also valuable in providing automated assistance to humans during the execution of various physical tasks. The project will computationally ""imagine"" changes that actors with human-like manipulation capabilities can make on that environment and generate plans that can accomplish the desired manipulations. Such tools facilitate the creation of smart environments, where for example a perception system watching an elderly person can infer the task the person is trying to accomplish and offer advice/assistance. They also allow the creation of automated instructional videos customized to a particular environment that can be used for efficient training of unskilled workers. The project will provide mentoring and research opportunities for a diverse set of students, including members of groups typically under-represented in computer science.<br/><br/>This research will study environments formed by objects, some of which can be manipulated, while others define obstacles to be avoided or support surfaces to be used. Manipulating an object typically means interacting with small parts of the object, referred to as its active sites: handles, buttons, levers, graspable or pushable regions, etc. A deep challenge is to develop tools for identifying and classifying these active sites on objects at large scale, and to codify the types of interactions they partake of based on dynamic 2D/3D imagery, building a vocabulary of elementary actions. This requires novel machine learning methods and deep architectures for processing large-scale dynamic visual and geometric data. It also requires characterizing manipulations at a more abstract level so that they can be used by a variety of effectors, robotic or human, on different object geometries and physical characteristics. A further challenge is the accumulation and update of actionable information as more visual data is received in online object model repositories, such as ShapeNet. A final but key step of the approach will be the development of tools for transporting such action knowledge to new settings that are similar but not identical to the capture settings, using a variety of mathematical tools including functional maps.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820253","SBIR Phase I:  A discovery and prediction platform for microbial data","IIP","SBIR Phase I","07/01/2018","02/12/2019","Minseung Kim","CA","PIPA LLC","Standard Grant","Ruth Shuman","03/31/2019","$225,000.00","","minseung@pipacorp.com","2030 Rehrmann Dr.","Dixon","CA","956202510","6099020399","ENG","5371","5371, 8038","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) project is to develop a software platform to automate the integration and analysis of large sets of biological data. This project will help lift the barrier that exists in data analysis and experimental planning by providing the latest computational advances in machine learning and bioinformatics. The goal is to create a cohesive data universe for microbial organisms, where each new dataset can be easily integrated, compared, and ultimately leveraged to increase collective knowledge. Such homogenized sets of structured biological data are ideal training sets for predictive models that can accelerate discovery times by transforming the traditional ""shot-in-the dark"" way of experimentation to guided, well-connected hypotheses, generated from the complete set of data at hand. <br/><br/>This SBIR Phase I project proposes to build a software tool for automated hypothesis generation based on biological datasets. First, it will support the design of a novel multi-omics engine that will allow the creation of a cohesive compendium of the omics and interaction universe for microbial organisms. Second, it will lead to the development of predictive modeling tools that will interrogate and transform this data compendium into a semantic web of actionable predictions. Third, it will support the evaluation of novel active learning methodologies for omics data. This project will lead to a software architecture that can put all of these tools under one platform to empower commercial clients as well as the academic community. Once completed, the product will offer an automated, end-to-end environment and decision support engine fueled by the collective knowledge. In that sense, it will be a personalized experience on an ecosystem of pre- and post-processing services for biological data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833035","Collaborative Proposal: NSF Summer School on Computational Modeling of Disordered Materials for Historically Black Colleges and Universities","DMR","DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY","09/01/2018","07/27/2020","Parthapratim Biswas","MS","University of Southern Mississippi","Standard Grant","Daryl Hess","08/31/2021","$115,459.00","","Partha.Biswas@usm.edu","2609 WEST 4TH ST","Hattiesburg","MS","394015876","6012664119","MPS","1712, 1765","029Z, 1711, 9150","$0.00","SUMMARY<br/><br/>This award supports an intensive training workshop for students and faculty members from Historically Black Colleges and Universities (HBCU) and Minority Serving Institutions (MSI) on computational modeling of complex materials, with an emphasis on the fundamental concepts of physics, chemistry and materials science. The participants will learn and employ a variety of modern structural modeling techniques and widely used computer simulation methodologies to understand and analyze the properties of highly complex and technologically important disordered materials. The workshop will adopt a lecture-laboratory approach, whereby each lecture is followed by hands-on computational ""laboratory"" exercises, aimed at illustrating how information from the lecture can be directly employed for practical simulations of complex materials. The topics to be addressed in the laboratory exercises are: structural modeling techniques on the length scale of atoms, analyses of structural properties, geometry optimization, electronic and vibrational properties, as well as thermodynamic properties of different types of disordered materials. The participants will also be introduced to the fundamentals of emerging data-centric and machine-learning techniques in materials modeling. The workshop will: (i) expose students to a wide range of state-of-the-art computer simulation techniques, and the applications of these techniques to real-world problems; (ii) bridge the gap between the standard teaching curriculum at the undergraduate and graduate levels and the actual expertise and knowledge that are needed for practicing computational materials science even at its basic level; (iii) develop collaborative initiatives between HBCUs and state-funded and private universities to initiate research in computational modeling of complex materials. The PIs will provide post-workshop support for a period of six months so that participants may develop small publishable research projects on their return to their home institutions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844524","EAGER: Reconciling Model Discrepancies in Human-Robot Teams","IIS","NRI-National Robotics Initiati","09/01/2018","08/13/2018","Yu Zhang","AZ","Arizona State University","Standard Grant","David Miller","08/31/2021","$249,929.00","","Yu.Zhang.442@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8013","063Z, 7916, 8086","$0.00","Robots greatly complement human capabilities in many domains, but having them function as teammates has often proven to be challenging. This is due in part to a lack of understanding of robots, which often leads humans to find that their expectations of robotic teammates are not met in reality.  The proposed research addresses this by having robots reason about the models people have of them and using those models to either behave in ways that meet the expectations more closely or explain the perceived differences.  The work can have a significant impact on critical domains that involve human-in-the-loop AI systems, such as decision support, automated manufacturing, eldercare, and medical robotics. <br/><br/>To become reliable teammates, robots must understand the expectations of their human partners regarding the robots' tasks and abilities, and be able to seek reconciliation between the expected and actual models. In this project, reconciliation will be achieved either by 1) biasing the robot's behavior to implicitly accommodate model differences; or 2) communicating to explicitly reduce the differences. The first approach, termed model reconciliation planning, will be formulated as an optimization problem that generates a plan for the robot to execute while minimizing its distance to the expected plan that the human envisions. Heuristic search methods will be developed to accommodate this approach. The second approach will generate explanations to update the human's model of the robot in such a way that the robot's plan more closely matches that of the human's expectation in the updated model.  In addition, machine learning will be used to approximate the model of human expectation when not provided explicitly, and methods for model reconciliation with these learned and incomplete models will be developed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829707","Collaborative Research: CyberTraining: CIC: Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP)","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Sudhir Malik","PR","University of Puerto Rico Mayaguez","Standard Grant","Alan Sussman","07/31/2021","$124,342.00","","sudhir.malik@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","044Y","026Z, 062Z, 7361, 9102, 9150","$0.00","High-energy physics (HEP) aims to understand the fundamental building blocks of nature and their interactions by using large facilities such as the Large Hadron Collider (LHC) at the European Laboratory for Particle Physics (CERN) in Switzerland and the Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) planned for the 2020s at Fermilab, in Illinois, as well as many smaller experiments. These experiments generate ever increasing amounts of data and rely on a sophisticated software ecosystem consisting of tens of millions of lines of code that is critical to mine this data and produce physics results. People are the key to developing, maintaining, and evolving this software ecosystem for HEP experiments over many decades. Building the necessary software requires a workforce with a mix of HEP domain knowledge and advanced software skills. The Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP) project provides a training path from a researcher's first steps through active contribution to software training and workforce development. The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure and impacts STEM disciplines in terms of much needed and sought after software training.<br/><br/>The FIRST-HEP project directly organizes training activities and works with partners to leverage and bring synergy to disparate existing efforts in order to maximize their collective impact. It brings together an extended set of partners from the community to build not only missing basic training elements like introductory programming skills in Python, git and Unix but  also use of HEP data formats like ROOT and advanced topics including parallel programming, performance tuning, machine learning and data science for Ph.D. students. It works to build a community of instructors and experiments around the software training material and transforms the approach for research software training in HEP. It builds the workforce required for the cyberinfrastructure challenges of running and planned HEP facilities and experiments in the coming years.The FIRST-HEP education and training activities include specific goals to educate minorities in HEP, K- 12 educators and the broader STEM workforce. The K-12 teachers learn very basic skills of Unix including file management, programming languages, such as C+ and shell scripting. FIRST-HEP harnesses the potential of the underrepresented groups and works to ensure that the pool meets or exceeds the diversity in the larger HEP graduate student population when selecting both training participants and instructors for the HEP fundamental training sessions and the advanced computing schools. FIRST-HEP includes a dedicated outreach activity on cybertraining to the local Puerto Rico public. FIRST-HEP leverages engagement with the Software Carpentries to host training of  K-12 teachers at UPRM in basic Software Carpentry skills and who in turn train their students. This encourages the teachers and school authorities to consider incorporating the basic carpentries into the high school curriculum. The training and cyber skills gained during the FIRST-HEP fundamental training courses directly contribute to the broader STEM workforce and trains students to pursue data science careers and other research areas besides HEP, such as Astronomy, where similar software skills are required.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761699","Point Processes in Healthcare and Security Analytics: Nonparametric Estimation and Efficient Optimization","CMMI","OE Operations Engineering","08/15/2018","02/11/2019","Niao He","IL","University of Illinois at Urbana-Champaign","Standard Grant","Georgia-Ann Klutke","07/31/2021","$324,883.00","Negar Kiyavash","niaohe@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","ENG","006Y","073E, 5514, 8023, 9102","$0.00","The advancement of point process models in healthcare and security analytics, such as in predicting disease occurrences, crime and cyber security attacks, has the potential to significantly improve public health and safety.  Point processes are an important but underappreciated class of models in engineering that incorporate temporal dynamics, information diffusion, and recurrent behavior.  This project will establish theoretical foundations and computational methods that enable efficient modeling and estimation of point processes from real-time and large-scale transactional data.  To address emerging data-rich challenges in healthcare and security analytics, this project aims to break the modeling and computational limitations of current theory and practice in point processes, providing a powerful new set of tools for these significant engineering challenges.  The PIs are committed to devoting their efforts to facilitate the education and training for next-generation engineers and data scientists, especially women and underrepresented minorities.<br/><br/>The development of efficient inferential analysis and decision-making for point process models is far from reaching the same level of maturity as that of Gaussian models.  In this project, the PIs will leverage elements from many fields - optimization, machine learning, nonparametric statistics, and information theory, to address several fundamental modeling and computational hurdles in the context of multivariate Hawkes processes.   The project has four main research thrusts:  (i) investigation of nonparametric models to significantly advance the capability of point processes in capturing large-scale and complex event data; (ii) development of novel theoretically grounded and practically efficient learning and optimization algorithms for statistical inference in point processes; (iii) exploration of domain-specific structure to achieve the optimal trade-off between likelihoods and regularizations; (iv) development of online optimization schemes and tools to facilitate real-time prediction and inference for streaming event data.  The techniques developed in this project will be used to advance the modeling and prediction of specific healthcare and security related applications, such as discovery of disease relationships, tracking of adverse drug reactions, and detection of criminal and terrorist activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816777","RI: Small: Variation and self-organization in multi-agent systems","IIS","Robust Intelligence","09/01/2018","07/03/2019","Annie Wu","FL","The University of Central Florida Board of Trustees","Standard Grant","Roger Mailler","08/31/2021","$488,434.00","","aswu@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7495","7495, 7923, 9229","$0.00","This project seeks to understand how to use inter-agent variation to improve the ability of swarm-based systems to solve the decentralized task allocation problem.  Swarm-based systems consist of large numbers of independent agents that act collectively to accomplish goals beyond the scope of a single agent.  Such systems may be applied to problems such as herding, forest fire containment, crowd control, perimeter protection, and hazardous waste clean up, which consist of multiple tasks with demands that may vary over time.  The agents in a swarm may be physical robots or virtual software agents.  Because these systems are decentralized and have no central controller, each agent decides independently what task to take on and when.  Effective and efficient allocation and reallocation of agents among tasks (as task demands change over time) is crucial to good swarm performance.  In addition to maintaining an appropriate number of agents on each task at any given time, swarms must also avoid or minimize problems such as extreme responses in which too many or too few agents respond, wasted energy when agents undo and redo each others' work, and deadlocks which may prevent accomplishment of the overall goal altogether.  Studies on social insect societies indicate that inter-agent variation is a necessary element for effective and efficient division of labor in biological swarms.  Taking inspiration from biology, this work investigates how inter-agent variation affects the decentralized task allocation problem in computational swarms and what types of variation are most effective in producing stable, robust, and adaptable swarms.<br/><br/>This project consists of three phases.  First, carry out a systematic study to build a model of the relationship between inter-agent variation and the self-organizing behavior of swarms.  This phase will examine how different types of variation in agent decision making characteristics affect system level stability and adaptability.  Second, investigate methods for and trade offs of dynamically evolving system variation.  This phase will investigate how principles from machine learning methods such as evolutionary computation may be used to dynamically learn and adjust inter-agent variation in response to changing task demands.  Third, apply and test conclusions from phase one and two to a multi-agent herding problem.  The multi-agent herding problem is representative of a range of problems that require division of labor involving search, gathering, containment, and coordinated movement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829740","CyberTraining: CIU: The LSST Data Science Fellowship Program","OAC","CyberTraining - Training-based, SPECIAL PROGRAMS IN ASTRONOMY","08/01/2018","06/29/2018","Adam Miller","IL","Northwestern University","Standard Grant","Alan Sussman","07/31/2021","$499,251.00","","amiller@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","044Y, 1219","026Z, 062Z, 1207, 7361, 9179","$0.00","This National Science Foundation (NSF) Training-based Workforce Development for Advanced Cyberinfrastructure award supplements graduate education in astronomy by providing in-depth training in the skills necessary to make scientific discoveries using big data. Ongoing and future surveys, such as the NSF's flagship optical telescope project, the Large Synoptic Survey Telescope (LSST), are producing data at an unprecedented rate. The sheer size of these data sets requires new working practices: sophisticated computational software and data mining procedures are necessary to fully exploit the rich information present in the data. However, these skills are not typically a core component of the astronomy and astrophysics graduate curriculum. The LSST Data Science Fellowship Program (DSFP) supplements traditional educational programs by training students in a variety of data science methods to work with and ultimately analyze big data. DSFP students are selected from a wide variety of universities using an innovative admissions procedure that increases the participation of students from underrepresented groups. Furthermore, DSFP students are trained in science communication and receive a certification in teaching data science so they can tutor peers and lead training workshops in the material learned as part of the program. The project serves the national interest, as stated by the National Science Foundation's mission: to promote the progress of science, by training the next generation of astronomers to have the computing skills necessary to derive scientific insights from the largest telescopic surveys that have ever been conducted.<br/><br/>DSFP students attend six week-long sessions over the course of two years as part of their program training. Each session is hosted by a different institution and designed to focus on a single topic including: the basics of managing and building code, statistics, machine learning, scalable programming, data management, image processing, visualization, and science communication. This curriculum empowers trainees to ask broader questions of their data, prepares them for the technical challenges associated with LSST, and exposes them to the tools and methods necessary to advance fundamental science research. Student participants spread the adoption of data science tools, methods, and resources via the aforementioned teaching workshops, fostering new pathways to discovery in the broader research community. Students must work in collaborative groups, which in conjunction with their science communication training, enhances their leadership and mentoring skills. To reach a broad audience, all materials developed as part of the program are made available to the public, and a guide to convert the material into a semester-long course at the undergraduate or graduate level is provided. This program prepares students for success in a wide range of careers, providing education in data science methodologies, domain-specific<br/>considerations, and professional skill development in research, teaching, and communication.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830363","ATD: Collaborative Research: Adaptive and Rapid Spatial-Temporal Threat Detection over Networks","DMS","ATD-Algorithms for Threat Dete, ","09/01/2018","06/13/2019","Hao Yan","AZ","Arizona State University","Continuing Grant","Leland Jameson","08/31/2021","$68,378.00","","haoyan@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","MPS","046Y, S100","6877","$0.00","This project aims to develop innovative machine learning and statistical algorithms for detecting, preventing, and responding to threats over networks. Two concrete applications are monitoring the threat of multi-antibiotic-resistant (MDR) gonorrhea from a network of clinics across the United States and monitoring HIV transmission in clusters of patients. The research has impact in many other practical applications, including biosurveillance, engineering, homeland security, finance, and public health, where large-scale spatial-temporal data streams are collected with the aim of rapid detection and prevention of threats. The research aims to develop crucial scalable algorithms and methods to effectively and efficiently monitor, analyze, and optimize responses in these situations. In addition, the project will integrate research and education by infusing the research findings into the curriculum and by involving Ph.D. students in research.  <br/><br/>This project aims to develop innovative algorithms for rapid threat detection by combining spatial-temporal models, ordinary differential equation (ODE) models with change-point detection, and multi-armed bandit and ensemble methods when monitoring large-scale spatial-temporal data over networks. In particular, efficient scalable algorithms are developed in three interrelated research tasks, including (1) rapid detection of threats by combining a ""background + anomaly + noise"" decomposition framework with sequential change-point detection; (2) predictive analytics of threats by applying multi-armed bandit algorithms and adaptive sampling in the changing environments to assess increasing risks at the population level; and (3) prescriptive analytics of threats by developing nested ensemble models based on calibrated ODE and data-driven spatial-temporal models so as to better assess the effects of prevention/intervention actions. Results of the project are expected to significantly advance the state of the art in spatial-temporal models, online learning, streaming data analysis, and large-scale inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763469","CHS: Medium: Collaborative Research: Manipulation Assistance for Activities of Daily Living in Everyday Environments","IIS","HCC-Human-Centered Computing","08/01/2018","05/11/2020","Holly Yanco","MA","University of Massachusetts Lowell","Continuing Grant","Ephraim Glinert","07/31/2022","$405,018.00","","holly@cs.uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","7367","7367, 7924, 9251","$0.00","While many people with disabilities need help with activities of daily living (ADLs) in their homes or at other locations, they care deeply about maintaining their sense of independence, which implies limiting the tasks that professional or family caregivers are asked to provide. There is the potential for robots to have a huge impact here, by enabling people to live independently for longer. The goal of this research is to develop a robotic wheelchair-manipulator system (RoWMan) consisting of a power wheelchair with a robotic arm mounted on it, that will help its user perform ADLs either as an assistive device or by performing manipulation tasks autonomously. In assistive mode, the user would ride in the wheelchair, with the RoWMan system manipulating items as requested. Whereas in autonomous mode, the user could ask RoWMan to navigate on its own through the house, retrieve items, and place them as directed. This project will necessitate the development of new user interfaces as well as an array of new machine learning and robotics techniques that will enable successful autonomous robotic navigation and manipulation in unstructured environments. To ensure broad impact, project outcomes will be evaluated with a user population at Crotched Mountain Rehabilitation Center.<br/><br/>In recent focus groups it was found that users want a number of capabilities, including the ability to pick up something from the floor, the ability to unlock and open a door, the ability to manipulate items on a tightly packed shelf, etc. RoWMan will be designed so as to enable users to perform these sorts of tasks, by focusing on two areas: robotic manipulation and human-robot interaction. The manipulation work will develop new algorithms that perform well with novel objects in unstructured environments. Traditionally, manipulation planners assume that the shapes of the objects involved are known in advance or can be estimated on the fly, but these assumptions often cause problems in practice. The focus here will be to develop new algorithms based on deep reinforcement learning that can perform manipulation tasks reliably even when the geometry of the world is unknown in advance. The project will also support research into a new class of human-robot interaction based on laser pointers. Recent work suggests that laser pointing can be very effective for the target user community because it enables users to point directly in the environment rather than on a screen which induces additional cognitive load. This project will develop new ways of communicating sophisticated intent using a combination of environmental context, laser pointing, and laser gestures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763878","CHS: Medium: Collaborative Research: Manipulation Assistance for Activities of Daily Living in Everyday Environments","IIS","HCC-Human-Centered Computing","08/01/2018","08/23/2018","Robert Platt","MA","Northeastern University","Continuing Grant","Ephraim Glinert","07/31/2022","$389,825.00","","r.platt@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7367","7367, 7924","$0.00","While many people with disabilities need help with activities of daily living (ADLs) in their homes or at other locations, they care deeply about maintaining their sense of independence, which implies limiting the tasks that professional or family caregivers are asked to provide. There is the potential for robots to have a huge impact here, by enabling people to live independently for longer. The goal of this research is to develop a robotic wheelchair-manipulator system (RoWMan) consisting of a power wheelchair with a robotic arm mounted on it, that will help its user perform ADLs either as an assistive device or by performing manipulation tasks autonomously. In assistive mode, the user would ride in the wheelchair, with the RoWMan system manipulating items as requested. Whereas in autonomous mode, the user could ask RoWMan to navigate on its own through the house, retrieve items, and place them as directed. This project will necessitate the development of new user interfaces as well as an array of new machine learning and robotics techniques that will enable successful autonomous robotic navigation and manipulation in unstructured environments. To ensure broad impact, project outcomes will be evaluated with a user population at Crotched Mountain Rehabilitation Center.<br/><br/>In recent focus groups it was found that users want a number of capabilities, including the ability to pick up something from the floor, the ability to unlock and open a door, the ability to manipulate items on a tightly packed shelf, etc. RoWMan will be designed so as to enable users to perform these sorts of tasks, by focusing on two areas: robotic manipulation and human-robot interaction. The manipulation work will develop new algorithms that perform well with novel objects in unstructured environments. Traditionally, manipulation planners assume that the shapes of the objects involved are known in advance or can be estimated on the fly, but these assumptions often cause problems in practice. The focus here will be to develop new algorithms based on deep reinforcement learning that can perform manipulation tasks reliably even when the geometry of the world is unknown in advance. The project will also support research into a new class of human-robot interaction based on laser pointers. Recent work suggests that laser pointing can be very effective for the target user community because it enables users to point directly in the environment rather than on a screen which induces additional cognitive load. This project will develop new ways of communicating sophisticated intent using a combination of environmental context, laser pointing, and laser gestures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1664644","QuBBD: From Personalized Predictions to Better Control of Chronic Health Conditions","DMS","CDS&E-MSS","06/01/2018","06/01/2018","Ioannis Paschalidis","MA","Trustees of Boston University","Standard Grant","Branislav Vidakovic","05/31/2021","$899,999.00","Christos Cassandras, Rebecca Mishuris","yannisp@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","8069","8083","$0.00","The United States spends twice as much annually on health care than the next-highest spender but significantly under-performs in quality of care metrics, such as life expectancy and infant mortality. Hospital care accounts for about a third of U.S. health care spending. It has been estimated that nearly $30 billion in hospital care costs each year are potentially preventable, with about half of that amount due to hospitalizations related to the two major chronic diseases: heart diseases and diabetes. Electronic Health Records, and the emerging digital data from home-based devices, smart phones, and wearables, offer a great opportunity to develop a systematic approach towards better management of chronic conditions in an outpatient setting and the prevention of hospitalizations required to treat acute episodes resulting from poor control of a patient's condition. This project will utilize digital health data to develop predictive models that anticipate future undesirable events, such as hospitalizations, re-admissions, and transitioning to an acute stage of a disease. These predictions will be used to trigger personalized interventions, ranging from increased monitoring and doctor visits to optimized treatment policies adapted to each patient. The project supports a collaboration between mathematical scientists and a physician at a major safety-net hospital, which treats a significant percentage of low-income and underrepresented groups.<br/><br/>The research will focus on two broad tasks: (1) predictive analytics, and (2) personalized interventions. Task 1 develops methods for predictions in two time scales, long and medium. These predictions target hospitalizations and rely upon new supervised machine learning approaches that combine classification with clustering as a way of enhancing performance and offering interpretable results. In addition, anomaly detection methods are proposed for shorter-term predictions. Task 2 focuses on interventions seeking to prevent events predicted under Task 1. Interventions include increased monitoring and optimizing treatment policies using Markov Decision Processes and perturbation analysis methods. Methodological advances will include methods for joint clustering and classification, anomaly detection, learning and improving policies for Markov Decision Processes, and perturbation analysis techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811818","Statistical Methodology and Applications to Engineering, Economics, and Health Analytics","DMS","STATISTICS","07/01/2018","06/29/2018","Tze Lai","CA","Stanford University","Standard Grant","Gabor Szekely","06/30/2021","$250,000.00","","lait@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","1269","","$0.00","A long-term objective of the proposed research is to develop innovative statistical methodologies and combine them with technological advances for resolving fundamental problems in engineering, economics, and health care. In particular, the past seven years have witnessed the beginning of a big data era in the US health care system, following the health care reform legislation enacted in 2010, and the Precision Medicine Initiative of 2015. This era poses new challenges and opens up new opportunities for the mathematical (including statistical, computational, and data) sciences and their interactions with the biomedical, engineering, and economic sciences. The project will address some of these challenges, and its broader impact includes (i) direct applications in engineering, economics and finance, health, and medicine, and (ii) training the next generation of scientists in academia, industry, and government by involving graduate students in all phases of the research and developing new advanced courses and revising the curriculum in financial and risk modeling, statistics and data science, and clinical trials and biostatistics.<br/><br/>The project is broadly divided into three areas. The first is the development of valid and efficient post-selection multiple testing in the big data era, in which some machine learning/feature engineering/variable selection algorithms are typically used to extract features/variables for subsequent hypothesis generation and statistical testing. The proposed research will address the reproducibility issues and ""replication crisis"" with this data-dependent choice of features and hypotheses for statistical inference from biomedical big data by resolving foundational issues concerning valid post-selection inference.  Initial investigations have already started by considering samples of fixed size, and will proceed with extensions to group sequential designs and then to sequential detection and diagnosis for multistage manufacturing processes, multicomponent systems, and multiple data streams from financial and production networks. The second area is the statistical foundation of gradient boosting, which also has applications to the first area because of its effectiveness in tackling high-dimensional nonlinear and generalized linear models. The third area covers biomarker-guided adaptive design of clinical trials for the development and testing of personalized therapies and in the closely related subject of contextual multi-armed bandits in sequential analysis and reinforcement learning. Innovations in this area can lead to advances toward the Precision Medicine Initiative. Also covered are innovative study designs and analyses of point-of-care trials and observational studies, and development of mobile health platforms and wearable devices to improve and facilitate evidence-based management of chronic diseases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750339","CAREER: Using Metamodeling to Enable High-Fidelity Modeling in Risk-based Multi-hazard Structural Design","CMMI","Engineering for Natural Hazard, CAREER: FACULTY EARLY CAR DEV","09/01/2018","03/12/2018","Seymour Spence","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Joy Pauschke","08/31/2023","$500,000.00","","smjs@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","014Y, 1045","036E, 039E, 040E, 043E, 1045, 1057, 1576, 7231, CVIS","$0.00","To assess the risk and resiliency of seismic and wind excited buildings in the United States, the use of high-fidelity computational models is paramount to characterizing the building performance.  However, the need to propagate uncertainty through the system when estimating state-of-the-art risk/resiliency metrics significantly hinders, if not precludes, the use of such models. This difficulty becomes exasperated in risk-based decision-making where multiple building design solutions must be evaluated and compared over several hazards. The research goal of this Faculty Early Career Development Program (CAREER) award is to overcome this fundamental limitation through the investigation of a new simulation paradigm based on the optimal fusion of low-/intermediate-fidelity metamodels with high-fidelity structural models. By defining the metamodels through domain independent approaches, multi-hazard assessment will be naturally encompassed and will enable new approaches for rapidly identifying the optimal tradeoff solutions to multi-hazard risk-based decision problems. These advances will provide models and procedures for enabling a full transition to optimal risk-based design, while promoting the rational use of computational resources through rigorous optimization. Risk-based design will benefit national welfare and prosperity through enhancing the safety of the built environment against wind and seismic events to better protect life and property during extreme events and to maintain essential services and business continuities during response and recovery. The educational goals of this CAREER award are to increase the number of women in engineering and professionals with expertise in wind loss mitigation. This will be achieved through a high school outreach program that leverages the link between risk-based engineering and societal benefit to inspire a diverse student pool to pursue careers in engineering, the development of an undergraduate wind engineering program at the University of Michigan, and undergraduate student research opportunities. To implement the high school outreach program, project-based learning modules that connect risk-based engineering and societal benefit through basic science will be created. Dissemination of these materials will be achieved through a teacher training workshop. Data from this project will be archived in the NSF-supported Natural Hazards Engineering Research Infrastructure (NHERI) Data Depot (https://DesignSafe-ci.org). <br/><br/>This research will create a new class of parametric metamodels (surrogate models) through identifying orthogonal subspaces for each high-fidelity computational model of the simulation environment. This will provide a setting in which both physics-based and data-driven reduced-order parametric metamodels can be defined through hyper-reduction and machine learning. The combined space of the high-fidelity and parametric metamodels will provide an enriched simulation environment in which multi-fidelity uncertainty propagation models can be defined for rapidly estimating high-fidelity probabilistic risk/resiliency metrics. The parametric nature of the metamodels will enable the creation of new adaptive multi-objective optimization schemes that will allow the rapid identification of high-fidelity multi-hazard Pareto fronts, which are central for effective risk-based decision-making. The models identified through this effort will directly benefit a number of other disciplines, including aerospace and biomedical engineering, atmospheric sciences, and the automotive industry, where rapid high-fidelity computation plays a key role in scientific discovery. The research will use the NHERI wind tunnel facility at the University of Florida.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1903673","I-Corps:  Connected digital health platform with integrated delivery model to improve patient adherence to health goals using insights from data science and behavioral science","IIP","I-Corps","12/01/2018","12/11/2018","Ravi Radhakrishnan","PA","University of Pennsylvania","Standard Grant","Andre Marshall","05/31/2021","$50,000.00","","rradhak@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to help pre-diabetics use and adhere to diabetes prevention measures to avoid a full-blown episode of diabetes. <br/><br/>86 million Americans are pre-diabetic with risk for developing type 2 diabetes and a cost of $2.3T. Evidence-based research has resulted in a protocol called diabetes prevention program (DPP), taught by diabetes coaches and covered by insurance. But usage/adherence to DPP is low. 9 out of 10 patients are unaware. Programs are underutilized, patients drop off and coaches are not paid by insurance due to non-compliance.<br/><br/>The company?s technology solution will use data and behavioral science to improve usage and adherence for DPP. This solution empowers doctors, diabetes coaches and patients to collectively improve long term self-care. By identifying risk of diabetes and connecting at-risk patients to programs, the company improves usage. By helping patients adhere to programs, the company helps programs comply and get paid. The company will collect software as a service (SaaS) fee from programs by helping them with revenue generation. The company will prove its value proposition in the diabetes prevention market with well-defined goals set by CDC/Medicare and use this evidence to move to other markets beyond diabetes. <br/><br/>This I-Corps project is a connected digital health platform with integrated delivery to improve patient adherence to health goals using insights from data science and behavioral science. It comprises of a web dashboard for health providers and a synced mobile app for patients. By aggregating and learning from decades of research in behavioral science, machine learning and medical adherence and combining it with connected technologies and computational power that have recently become available, the company is developing a better way to help patients adhere to their long term health goals. With smartphone usage at 77% (Pew Research, 2018), it is now easier to collect data automatically and continuously, intervene, support and track patients virtually. <br/><br/>The company is leveraging its team?s 75+ years of combined experience in healthcare, technology, behavioral psychology and entrepreneurship to tackle this challenge. The company will streamline data collection and management through wearables and connected technologies. The company will use this patient generated data to develop new predictive algorithms and visualize data to offer insights to patients and healthcare providers. Additionally, the company will implement behavioral science methodologies and tools such as gamification and nudging to reinforce healthy behaviors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1904444","Exploring Clouds for Acceleration of Science (E-CAS)","OAC","CYBERINFRASTRUCTURE","11/15/2018","09/13/2019","Howard Pfeffer","DC","INTERNET2","Cooperative Agreement","Kevin Thompson","10/31/2021","$3,030,955.00","James Bottum, Ana Hunsinger","hpfeffer@internet2.edu","1150 18th St NW","Washington","DC","200363825","7349134264","CSE","7231","","$0.00","Internet2 leads the ""Exploring Clouds for Acceleration of Science (E-CAS)"" project in partnership with representative commercial cloud providers to accelerate scientific discoveries. The effort seek to demonstrate the effectiveness of commercial cloud platforms and services in supporting applications that are critical to growing academic and research computing and computational science communities, and seeks to illustrate the viability of these services as an option for leading-edge research across a broad scope of science. The project helps researchers understand the potential benefit of larger-scale commercial platforms for scientific application workflows such as those currently using NSF's High-Performance Computing (HPC). It also explores how scientific workflows can innovatively leverage advancements provided by commercial cloud providers.  The project aims to accelerate scientific discovery through integration and optimization of commercial cloud service advancements; identify gaps between cloud provider capabilities and their potential for enhancing academic research; and provide initial steps in documenting emerging tools and leading deployment practices to share with the community.<br/><br/>Cloud computing has revolutionized enterprise computing over the past decade and it has the potential to provide similar impact for campus-based scientific workloads. The E-CAS project explores this potential by providing two phases of funded campus-based projects addressing acceleration of science. Each phase is followed by a community-led workshop to assess lessons learned and to define leading practices. Projects are selected from two categories; time-to-science (to achieve the best time-to-solution for scientific application/workflows that may be time or situation sensitive) and innovation (to explore innovative use of heterogeneous hardware resources, serverless applications and/or machine learning to support and extend application workflows). The project is guided by an external advisory board including leading academic experts in computational science and other fields, commercial cloud representatives, NSF program officers, and others. It leverages prior and concurrent NSF investments while creating a new model of scalable cloud service partnerships to enhance science in a broad spectrum of disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752125","CAREER:  Dynamic Decision-Making Under Uncertainty via Distributionally Robust Optimization","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","03/15/2018","03/09/2018","Grani Adiwena Hanasusanto","TX","University of Texas at Austin","Standard Grant","Radhakisan Baheti","02/28/2023","$500,000.00","","grani.hanasusanto@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","ENG","7607","092E, 1045","$0.00","A wide spectrum of decision problems arising in process control, energy systems operation, supply chain management, investment planning, project management, engineering, economics, etc., involve uncertain parameters whose values are unknown to the decision maker when the decisions are made. Ignoring this uncertainty typically leads to inferior solutions that perform poorly in practice due to the notorious flaw of averages, whereby plans based on the assumption that average conditions pre-vail are usually wrong. These decision problems are often also dynamic in nature they span across multiple time stages and involve high dimensional non-anticipative recourse decisions which further increase the problem complexity. Thus, effective and efficient solution schemes for these decision problems are highly desirable. Traditional solution schemes, however, suffer from the curse of dimensionality and are extremely challenging to solve. Recent advances in distributionally robust optimization (DRO) have been successful in mitigating the intractability of various single-stage decision problems under uncertainty. In DRO, we seek a decision that performs best in view of the most adverse distribution of uncertain parameters that is consistent with the available statistical and structural information. Thus, DRO not only improves computational tractability but also alleviates the overfitting effects characteristic of the traditional solution schemes. By leveraging and inventing new techniques in DRO, the proposed research work aims to significantly advance the state-of-the-art methodologies for addressing the challenges of dynamic decision problems and to initiate the effort for industrial-size applications. The research outputs of this work will have a significant and immediate practical impact on important applications in energy, engineering, machine learning, operations management, finance, etc., and on learning problems in robotics and automatic control. This CAREER work will also advance the state of pedagogy by developing an integrated curriculum that bridges the gap between the deep theory of decision-making under uncertainty and the real-life practice. The proposed curriculum is aimed at future practitioners and researchers, and is designed to equip these experts with the analytical skills and tools to deal with real-life decision-making problems under uncertainty.<br/><br/>The proposed research work is aimed at addressing a major gap in the theory and practice of decision-making under uncertainty. It concentrates on four main research thrusts: 1) Derive exact mixed-integer conic programming (MICP) reformulations for convex dynamic problems as well as for dynamic problems with discrete decisions 2) Deal with the case of endogenous uncertainty whose representation depends explicitly on the chosen decisions 3) Systematically integrate data into the description of uncertainty. Obtain provable out-of-sample performance guarantees from the resulting data-driven DRO models 4) Derive exact MICP reformulations for inverse optimization problems in the dynamic setting. The proposed research effort endeavors to develop more powerful solution schemes which leverage standard off-the-shelf MICP solvers for various intractable decision-making problems under uncertainty. The work will establish a new connection between generic dynamic DRO models and renowned classes of mixed-integer conic programs. The resulting connection will give us a better understanding of the inherent difficulty of the decision problems and enable us to derive attractive performance guarantees for the new solution schemes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757923","REU Site: Computational and Mathematical Modeling of Complex Systems","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2018","07/19/2019","Cristopher Moore","NM","Santa Fe Institute","Standard Grant","Alan Sussman","02/28/2021","$332,821.00","","moore@santafe.edu","1399 HYDE PARK ROAD","SANTA FE","NM","875018943","5059462727","CSE","1139","9150, 9250","$0.00","The SFI Research Experiences for Undergraduates (REU) program is a ten-week residential research opportunity in which students develop innovative research projects in collaboration with mentors. The program asks students to discard traditional disciplinary boundaries, and learn computational modeling and data analysis techniques that can apply across the physical, natural, and social sciences. This allows students to ask big questions about real-world systems using rigorous mathematical and computational methods. Projects range from simulation to machine learning to proving theorems. The program supports the goals of science education and diversity in science by emphasizing engagement with students from non-elite institutions with limited research opportunities, women, and under-represented minorities (URMs). Early career scientists act as mentors in the program, gaining valuable experience as educators in mentoring. Research performed by SFI REUs has directly advanced the progress of science, and has focused on solving problems of direct relevance to society and the national health, including vaccination strategies for whooping cough, sustainability, economics of higher education, and social network analysis, among other topics. <br/><br/>In every STEM field, computational and mathematical modeling are rapidly becoming essential skills: translating a real-world system into a quantitative model, designing and coding computational experiments, analyzing these experiments statistically, and comparing their results with data. Projects of this kind are an ideal opportunity for undergraduate training that builds students' technical and analytical skills, connects them with the wider scientific world, and links scientific thinking with real-world contexts and applications. The SFI REU program is designed around the strengths of being a leading transdisciplinary research center. Undergraduates are recruited from multiple departments including computer science, physics, mathematics, biology, and the social sciences, and paired with mentors from many different scientific backgrounds. Recent projects include epidemiology and public health, digital humanities and topic modeling, social network structure, cell biology and proteomics, smart cities and urban data, and statistical physics. Methods utilized range from simulation to data analysis to theorem-proving, and in many cases have produced publishable work. Throughout the summer, students are offered tutorials on the basics of data analysis, algorithms, network theory, statistics, and programming in Python and C++. Tutorials are also offered on science writing, presenting research, applying for jobs in science and picking a graduate school or industry carrer, and dealing with impostor syndrome and implicit bias.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1741306","BIGDATA: IA: Predictive Analytics of Driver's Engagement for Injury Prevention","IIS","Big Data Science &Engineering","01/01/2018","04/29/2020","Christopher Yang","PA","Drexel University","Standard Grant","Wendy Nilsen","12/31/2021","$1,015,993.00","Jonathan Antin, Helen Loeb, Weimao Ke, Santiago Ontanon, Sheila Klauer","ccy24@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8083","7364, 7433, 8083, 9251","$0.00","Over 30,000 people are killed in motor vehicle crashes on US roadways every year. Driver distraction from secondary in-vehicle activities, particularly among young drivers, has emerged as a major cause of motor vehicle crashes. A substantial amount of research has been focused on analyzing small sets of naturalistic driving or simulated data to study a small number of features for detecting the driver's engagement. However, many meaningful dependencies and patterns can only be discovered by large collections of data. In this project, the goal is leveraging the two petabytes federal database of naturalistic driving data to develop predictive analytics for detecting a driver's disengagement from the driving tasks in order to provide alerts to drivers and reduce the risk of motor vehicle crashes. <br/><br/>In this project, data pre-processing techniques are investigated for the large volume of heterogeneous data with over 100 variables in Strategic Highway Research Program 2 (SHRP 2). Two scalable predictive analytics algorithm families based on instance-based learning and heterogeneous network mining for predictive modeling are developed. In addition, a novel distributed computing infrastructure to support the scalable predictive analytics in performing pattern mining of driving behavior analysis, modeling, and prediction are developed. The research outcomes of this project shed a significant amount insight into current work of injury prevention due to motor vehicle crashes. The project extends the capability of machine learning, sensor informatics, and driving behavior analytics. The integrated education plan includes incorporating the research findings in courses offered at the Master of Science program in Health Informatics. The outreach plan involves organizing workshops, conferences, and seminars to disseminate the research outcomes."
"1757785","REU Site: Psychology Research Experience Program","SMA","RSCH EXPER FOR UNDERGRAD SITES, SCIENCE RESOURCES STATISTICS","09/01/2018","04/08/2020","Bradley Postle","WI","University of Wisconsin-Madison","Continuing Grant","Josie S. Welkom","08/31/2021","$221,994.00","Tim Rogers","postle@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","SBE","1139, 8800","062Z, 9250, 9251","$0.00","This award was provided as part of NSF's Social, Behavioral and Economic Sciences Research Experiences for Undergraduates (REU) program and is also supported by SBE's National Center for Science and Engineering Statistics (NCSES). It has both scientific and societal benefits, and integrates research and education. The explicit motivation for this REU program is to address the well-documented achievement gap whereby members of underrepresented populations, racial and ethnic minorities, low-income, and first-generation college students: (1) apply to PhD programs in psychology and neuroscience and (2) successfully complete their doctoral training, in proportions far lower than their representation in the overall population. The Psychology Research Experience Program (PREP) at the Department of Psychology of the University of Wisconsin, Madison provides intensive mentoring and experience in scientific research and in professional development to undergraduates who have expressed and demonstrated an interest in a career in scientific psychology. The intellectual focus for PREP is integrating principles and methods of data science into the study of psychology and neuroscience. This focus reflects in all branches of science relating to behavior, including data mining and meta analysis, large-scale collection of data, and the increasing appreciation that individual data sets can constitute, ""big data"" which methods from machine learning and other branches of engineering can be applied. PREP's combination of a mentored research experience, training in the tools of data science, and professional development and networking, is designed to equip its participating students with the skills that are necessary for a successful career in STEM. Thus, it will broaden the participation of underrepresented groups in STEM-related careers.<br/><br/>PREP features a balance of mentored laboratory research and a curriculum of scientific instruction, professional development, and networking opportunities. Each PREP student is paired with a professor whose research falls into one or more of these domains of psychology research: Biological, Clinical, Cognition and Cognitive Neuroscience, Developmental, and Social. Tiered mentorship includes graduate student or postdoctoral fellow from the host lab, as well as by a graduate student in the Department of Computer Science or the Department of Electrical and Computer Engineering. These PREP research mentors, themselves, participate in a weekly mentor training workshop. In addition to its primary goal of broadening participation in STEM-related careers, PREP contributes quantitative and qualitative survey research to the Student Assessment of their Learning Gains (SALG) instrument to improve teaching and mentoring in the STEM disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827173","IRES Track I: Collaborative Research: Interdisciplinary Research in Korea on Applied Smart Systems (IRiKA) for Undergraduate Students","OISE","IRES Track I: IRES Sites (IS)","10/01/2018","03/15/2019","Yong-Kyu Yoon","FL","University of Florida","Standard Grant","Maija Kukla","09/30/2021","$213,000.00","","ykyoon@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","O/D","7727","5942, 5978","$0.00","Interdisciplinary Research in Korea on Applied smart systems (IRiKA) for Undergraduate Students will provide a cohort of five US undergraduate students per year with the opportunity to conduct research for 8 weeks at Korea's top-ranked universities with state-of-the-art research facilities: Seoul National University, Korea Advanced Institute of Science and Technology and Ewha Womans University. Over the lifetime of this 3-year project, 15 students will participate. Students from underrepresented groups will be recruited. The unifying research theme of IRiKA is smart systems with the subtopics of sensors, emerging electronics, and materials and process development. In addition to lab work and weekly cohort research meetings, IRiKA students will visit Korea's government research institutions and global leaders in the tech industry such as Samsung, LG, and Hyundai. The distinctive features of IRiKA are: 1) A cohort experience bringing the US participants together; 2) Vetted and structured professional development program tailored for both US students and Korean mentors; and 3) Availability of follow-on collaborative projects in US and Korea to facilitate a sustained global network of mentorship. Students will gain formative research skills and learn how smart systems brings together interdisciplinary technological solutions for manufacturing, healthcare, energy, safety and security, transportation, and logistics. The international aspect of the IRiKA will help students recognize their place in the global scientific community. The participants will present their work at their home institutions upon their return and will be incentivized to publish in a peer-reviewed journal or present at a conference for broader dissemination. <br/><br/>Interdisciplinary Research in Korea on Applied smart systems (IRiKA) for Undergraduate Students will engage students in interdisciplinary research, help them develop a global perspective on collaboration, and motivate them to pursue a career in STEM research. Efforts will be made to attract students underrepresented in STEM and/or with limited STEM research opportunities. IRiKA takes a scaffolded mentorship approach that fosters students' growth from a relatively dependent status to as independent a status as their competence warrants. Smart systems incorporate sensing, actuation, wireless connectivity, and machine learning. Examples of research projects that individual students will conduct during 8 weeks in Korea include: Development of an air-borne particle sensing system for health monitoring and air quality monitoring; Development of miniature and micro power generation systems to enable autonomous sensor systems; and Development of a light-weight, flexible point-of-care device consisting of microfluidic channels and reduced graphene oxide-based biosensors. Upon return to the US, students will be able to engage in follow-on projects in areas such as convergent IoT systems at University of Florida's NSF I/UCRC Multi-functional Integrated Systems Technology (MIST) center and wearable sensor systems at Louisiana State University.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832998","Collaborative Conference: NSF Summer School on Computational Modeling of Disordered Materials for Historically Black Colleges and Universities","DMR","DMR SHORT TERM SUPPORT, CONDENSED MATTER & MAT THEORY","09/01/2018","08/15/2018","Raymond Atta-Fynn","TX","University of Texas at Arlington","Standard Grant","Daryl Hess","08/31/2019","$16,750.00","","r.attafynn@gmail.com","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","MPS","1712, 1765","029Z, 1711","$0.00","SUMMARY<br/><br/>This award supports an intensive training workshop for students and faculty members from Historically Black Colleges and Universities (HBCU) and Minority Serving Institutions (MSI) on computational modeling of complex materials, with an emphasis on the fundamental concepts of physics, chemistry and materials science. The participants will learn and employ a variety of modern structural modeling techniques and widely used computer simulation methodologies to understand and analyze the properties of highly complex and technologically important disordered materials. The workshop will adopt a lecture-laboratory approach, whereby each lecture is followed by hands-on computational ""laboratory"" exercises, aimed at illustrating how information from the lecture can be directly employed for practical simulations of complex materials. The topics to be addressed in the laboratory exercises are: structural modeling techniques on the length scale of atoms, analyses of structural properties, geometry optimization, electronic and vibrational properties, as well as thermodynamic properties of different types of disordered materials. The participants will also be introduced to the fundamentals of emerging data-centric and machine-learning techniques in materials modeling. The workshop will: (i) expose students to a wide range of state-of-the-art computer simulation techniques, and the applications of these techniques to real-world problems; (ii) bridge the gap between the standard teaching curriculum at the undergraduate and graduate levels and the actual expertise and knowledge that are needed for practicing computational materials science even at its basic level; (iii) develop collaborative initiatives between HBCUs and state-funded and private universities to initiate research in computational modeling of complex materials. The PIs will provide post-workshop support for a period of six months so that participants may develop small publishable research projects on their return to their home institutions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746142","STTR Phase I:  Accelerating the dissemination of healthcare interventions that improve care for high-need/high-cost patients","IIP","STTR Phase I","01/01/2018","07/11/2018","Bianca Finkelstein","VA","Health Network Research Group","Standard Grant","Henry Ahn","02/28/2019","$225,000.00","Fei Li","bfinkel@conduitinsight.com","11710 Plaza America Drive","Reston","VA","201904743","5713062538","ENG","1505","1505, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase 1 project includes: accelerating the pace of healthcare improvement by making information on high-need/high-cost patients instantly accessible and individually tailored to health care providers; transforming health intervention databases into active and dynamic learning communities about caring for high-need/high-cost patients; reducing the burden on safety net providers to sort and sift through dozens of information sources about delivering care to high-need/high-cost patients. Health Information Networking Tool (HINT) will be a unique combination of customized algorithms that will fill in critical gaps in knowledge, especially around health disparities, the social determinants of health and underserved conditions; boost opportunities for safety net providers to connect with peers to engage in collaborative problem-solving; reduce duplication and repetition of errors and failed interventions across safety net healthcare organizations; increase public recognition for safety net institutions that develop promising interventions; and enable technical advancements in machine learning to suggest models of care for high-need/high-cost patients. The commercial impact for HINT includes: reducing the cost and improving the quality of care delivered to high-need/high cost populations; and creating opportunities for safety net institutions and providers to market their expertise on caring for complex, underserved patients.<br/><br/>The proposed project was conceived on the belief that ingenious solutions in caring for society's most vulnerable populations occur daily across the safety net health system; that HINT will accelerate innovation by bringing an unprecedented resource to disseminate voluminous and constantly changing healthcare information; that the proposed information network will reduce the fragmentation of information and duplication in errors and failed interventions that currently occur; and finally that unlocking and disseminating innovations and advice from peers - in similar institutions and caring for similar patients - will accelerate successful practices to improve the health of high-need/high-cost patients. HINT proposes to develop unique crawling, clustering, text mining, collaborative filtering, bipartite matching, and ranking algorithms. The team will integrate the six algorithms into a customized social content management system, and address challenges around design and functionality."
"1813940","AF: Small: RUI: Competitive Search, Evacuation and Reconfiguration with Coordinated Mobile Agents","CCF","Algorithmic Foundations","06/01/2018","05/21/2018","Sunil Shende","NJ","Rutgers University Camden","Standard Grant","A. Funda Ergun","05/31/2021","$237,473.00","","shende@camden.rutgers.edu","311 N. 5th Street","Camden","NJ","081021400","8562252949","CSE","7796","7923, 7929, 7934, 9229","$0.00","In recent years, innovations in special-purpose sensor hardware and machine-learning algorithms have led to rapid advances in robotics and autonomous vehicle technology. This project aims to develop and analyze algorithms for an ensemble of mobile agents working in parallel to achieve coordinated, goal-directed motion in simple geometric spaces that are abstractions of complex terrains. The agents cooperate among themselves to efficiently search, explore, synchronize and reconfigure into patterns using distributed algorithms with limited memory, sensing and communication capabilities. The research will contribute to a deeper understanding of coordination protocols to complete navigational tasks, even when some of the agents may be faulty. Results from the research will provide critical insights that can be used in robot-assisted applications such as search-and-rescue in hostile or unknown terrains, or surface exploration needed to develop and maintain future colonies and human outposts on other planets. The study of reconfiguration problems will also provide ideas that can shed light on previously unexplained aspects of flocking phenomena in birds, such as murmurations. The project will involve undergraduate students in theoretical research and will train them to develop and maintain an open-source repository of distributed algorithms for robot coordination problems and their animations. In addition, results from the project will be archived as a permanent online resource for the parallel and distributed computing community at large. Two specific annual initiatives for outreach will be a part of the project: firstly, participation by the investigator and undergraduate research students in the Rutgers Future Scholars program, and secondly, conducting workshops for K-12 teachers to teach basic computational and algorithm design skills.<br/><br/>The project addresses fundamental theoretical problems of efficient, coordinated exploration and reconfiguration by a team of mobile agents as they work together to accomplish a computational task in simple geometric spaces.  Three basic problem settings are considered: (a) the search problem, where an unknown target location, which can only be sensed when reached, has to be found by at least one robot in the team, (b) the evacuation problem, that requires some subset of the robots to reach an unknown exit location in the region after it has been found, and (c) the reconfiguration problem, where the robots must rearrange themselves in a certain desired configuration based on either a given arrangement or on local rules. Within each problem setting, the goal is to develop an algorithm for the robots so that they can attain the desired configuration or complete the task defined by the problem with the minimum amount of resource usage possible, e.g. total distance traveled or time taken. Many interesting variants of the problems will also be studied, such as the peculiarities of the region's geometry, constraints on the communication capabilities of the robots, and handling faulty robots that may be benign or malicious.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831137","SBIR Phase II:  Envimetric - Soil and water contamination predictive modeling tools","IIP","SBIR Phase II","09/15/2018","07/23/2020","Jason Dalton","VA","Azimuth1, LLC","Standard Grant","Anna Brady-Estevez","08/31/2020","$800,000.00","","jason.dalton@azimuth1.com","501 Church St NE","Vienna","VA","221804711","7036188866","ENG","5373","5373, 8030, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovative Research (SBIR) Phase II project is a significant reduction in the cost and time to remove hazardous contaminants from the soils and groundwater impacting communities. <br/><br/>Properties observed from thousands of contaminated sites serve as inputs to a computerized mathematical model of the site, forecasting the most likely shape and depth of a contaminant plume. This machine learning model gives remediation planners access to a fast delineation of volume to be remediated as well as the uncertainty of the modeled estimate. This saves time and money searching for these contaminants that are deep underground and in groundwater. This Phase II project will expand on the Phase I prototype, creating an operational product capable of reaching the needs of environmental engineers and scientists around the globe, providing the stimulus to cut remediation time and cost in half.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830498","NRI: FND: Rapid Operator Awareness via Mobile Robotics (ROAMR), Customizable Human Safety using Mobile and Wearable Co-Robots","IIS","NRI-National Robotics Initiati","09/01/2018","04/22/2020","Anirban Mazumdar","GA","Georgia Tech Research Corporation","Standard Grant","David Miller","08/31/2021","$497,946.00","Aaron Young","anirban.mazumdar@me.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","063Z, 8086, 9251","$0.00","Human operators in construction sites, disaster zones, and other rapidly changing environments risk injury from collisions with moving machinery and falling components.  The risk of collisions can reduce human performance by forcing them to work very slowly and keep track of multiple situations simultaneously.  This project aims to explore how teams of co-robots can improve human safety by detecting potential collisions that the humans may not be aware of, alerting them to the danger, and helping them move away from the danger.  The project will develop methods for communicating potential threats to human operators via a human-worn exoskeleton, which can also assist the people in moving to safety as rapidly as possible.<br/><br/>The approach will augment human awareness, performance, and safety in unstructured, outdoor, and varying environments, where dynamic objects pose threats of imminent, unnoticed collision.  Specifically, the project will develop a new safety architecture where mobile systems rapidly detect collisions and plan a safe response, a wearable exoskeleton communicates the situation to the human, and the exoskeleton helps to achieve a safe response.  The project will focus on 1) developing and characterizing a physical ""language"" for wearable co-robots to provide situational awareness based on visual displays, audio guidance, vibro-tactile sensations, and physical signals from the exoskeleton actuators; 2) investigating how wearable sensing can infer human motions using machine learning to predict human behavior by accounting for automated obstacle avoidance plans, human motion strategies, and measurements of joint kinematics, ground contact forces, and muscle activation; and 3) researching how a wearable exoskeleton can accelerate safe responses in real-time by using targeted exoskeleton torques to enhance propulsive forces and increase speed of response by reducing the apparent inertia of the limbs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831475","SCC: Community-Based Automated Information for Urban Flooding","CNS","S&CC: Smart & Connected Commun","10/01/2018","09/09/2018","Mikhail Chester","AZ","Arizona State University","Standard Grant","Bruce Hamilton","09/30/2021","$1,500,000.00","Thomas Meixner, Robert Pastel, Benjamin Ruddell, Christopher Lowry","Mikhail.Chester@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","033Y","042Z","$0.00","Flooding is the most damaging natural hazard in the U.S. and around the world, and most flood damage occurs in cities. Yet the ability to know when flooding is happening and communicate that risk to the public and first responders is limited. At the same time there is a surge in digitally connected technologies, many at the fingertips of the general public (e.g., smartphones). The need is for new flood information that can be generated from primary observations that are collected in exactly the right places and times to be coupled with the ability to more effectively communicate this risk to communities. This project will develop the Integrated Flood Stage Observation Network (IFSON), a system that can take in crowd-sourced information on flooding (from cameras, a smartphone app, and social media), intelligently assess flood risk (using machine learning), and communicate those risks in real time. IFSON will be scalable to any community or city and will provide a backbone for new crowd-sourced technologies.<br/><br/>This project will i) integrate several new technologies (each that directly engages with different communities) to provide new insights into and communication capacity around urban flooding hazards, ii) connect a range of communities to each other in near-realtime (from the general public to first responders to infrastructure managers) and develop flood sensing and avoidance capacities that can be used anywhere in the U.S. or even internationally, iii) develop new insights into how urban morphology contributes to flood risk, and iv) leverage prior funding by connecting practitioners from existing sustainability research networks and sending data to CUAHSI and eRams. Additionally, this research will develop outreach activities that will educate the public and practitioners on how flooding hazards occur, their impacts, and how to mitigate risks. The research will directly empower and engage local citizens in flood event reporting and response, and explores a concrete model for what it would mean to have a ""smart and connected community"" for minimizing flood risk. Although driven by a number of novel technologies and techniques, the central focus of this work is on the interface of community with technology and, in particular, how modern network technologies can engage and bring together ordinary citizens, city planners, first responders, and other local stakeholders within a shared, collaboratively constructed information space; a broad range of educational and outreach opportunities are included to engage stakeholders and amplify project impact. In addition to training students through research positions, the project will create a summer Research Experience for Undergraduates (REU) program. It will also connect with national, state, and local societies across a number of disciplines. For example, the project will work with the City of Phoenix during their Monsoon Preparedness day to educate first responders on how to use project results. Interdisciplinary course modules that show how to engage various communities (including the public, first responders, and infrastructure managers) in mitigating flood risk will be developed and disseminated. Additionally, infrastructure managers will be recruited to participate in workshops on how project data will reveal new insights into the condition of infrastructure and what strategies can be employed to reduce hazards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758596","SBIR Phase II:  Automating the Design of Photonic Devices using Cloud-Based Computational Electromagnetics","IIP","SBIR Phase II","02/01/2018","01/07/2020","Ardavan Oskooi","CA","SIMPETUS, LLC","Standard Grant","Steven Konsek","07/31/2021","$909,999.00","","oskooi@simpetus.com","1515 Sutter St Ste 333","San Francisco","CA","941095348","4159203025","ENG","5373","169E, 5373, 8029, 8032, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is to provide quick and cost-effective access to anyone seeking cutting-edge, electromagnetic, simulation tools for research and development. This will involve automating the most technically intensive tasks required to accurately, reliably, and efficiently design and prototype photonic and optoelectronic devices. Current simulation tools require everything to be set up by hand which, even for trained experts, is time consuming and error prone. Also, the computational design and prototyping of photonic devices involves too much trial and error. Our turn-key, on-demand, cloud platform will: (1) reduce design time by 50%, (2) lower the unit cost of simulations by an order of magnitude, and (3) enable first-time users with no prior training in electromagnetic simulations to get up and running with manufacturing-ready designs in a matter of weeks not months. Our advanced simulation tools will facilitate enterprises and entrepreneurs with bringing products to market in a variety of industries critical to national security, health, and education. Easy-to-use and affordable design tools will also reduce the need to fabricate and test as many iterations thereby conserving environmental resources. This is the next generation of computer-aided design (CAD) tools to accelerate photonics innovation and discovery. <br/><br/>The proposed project involves the development of a turn-key, electromagnetic, simulation platform which automates complex, multi-step tasks involved in the design and prototyping of photonic and optoelectronic devices. Photonics, the science of light, underlies critical technologies in telecommunications, networking, photovoltaics, biomedicine, photolithography, imaging, displays, and solid-state lighting. Phase II involves automating the deployment of finite difference time-domain (FDTD) simulations in four key areas: (1)large-scale shape optimization for devices involving tens to hundreds of degrees of freedom, (2) sensitivity analysis to assess the impact on device performance of manufacturing errors and predicting manufacturing yields, (3) launching simulation jobs using on-demand, scalable, high-performance computing (HPC) in the public cloud, and (4) seamlessly importing and exporting planar device geometries based on the standard Graphic Database System (GDSII) file format widely supported by electronic design automation (EDA) tools and semiconductor foundries. This will be made possible by combining advances in machine learning, nonlinear optimization, and computational electromagnetics."
"1836318","CBMS Conference: Mathematical Molecular Bioscience and Biophysics","DMS","INFRASTRUCTURE PROGRAM","09/01/2018","08/06/2018","Shan Zhao","AL","University of Alabama Tuscaloosa","Standard Grant","Swatee Naik","02/29/2020","$35,000.00","Weihua Geng","szhao@ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","MPS","1260","7556, 9150","$0.00","This award supports the 2019 NSF-CBMS conference ""Mathematical Molecular Bioscience and Biophysics,"" hosted by University of Alabama, Tuscaloosa, during May 13-17, 2019. The conference will feature Professor Guowei Wei of Michigan State University as the Principal Lecturer. Mathematical molecular bioscience and biophysics has been emerging as a promising interdisciplinary research area at the interface of mathematics and biology, driven by the trends of contemporary life sciences that transform biosciences from macroscopic to microscopic or molecular, and from qualitative and phenomenological to quantitative and predictive. The conference will promote biological studies for solving cutting edge problems at molecular level, so that mathematics can play a more important role in addressing fundamental challenges in molecular biosciences and biophysics. This conference consists of ten principal lectures, together with supplemental presentations by other experts and round table discussions. The conference aims to attract many junior mathematicians, including undergraduate and graduate students, postdoctoral fellows, and young faculty, to enter this new interdisciplinary field. In addition, the meeting will benefit the hosting university by enhancing its research program and raising its visibility to peer institutes in the southeastern region. <br/><br/>Mathematical Molecular Bioscience and Biophysics (MMBB) concerns the development of mathematical theories, models, methods, schemes, and algorithms for elucidating molecular mechanisms and for solving open problems at the forefront of molecular biosciences and biophysics. The lecture series will provide a thorough overview of the MMBB literature to mathematical and biological societies. Numerous areas of mathematics, including differential equations, functional analysis, harmonic analysis, Lie groups, Lie algebras, geometry, graph theory, topology, combinatorics, multiscale modeling, inverse problems, optimization, machine learning, stochastic analysis, uncertainty quantification, fuzzy logic, statistical inference, and nonparametric regression, have found important applications in MMBB and many successful applications will be illustrated in this conference. Biological open problems at the forefront of the MMBB, such as those associated with drug design and discovery, which will stimulate new research directions in modeling and computation of biomolecular structure, function, dynamics and transport, will be identified. For more information, please refer to the conference webpage: http://cbms.ua.edu<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763786","AF: Medium: Collaborative Research: Foundations of Adaptive Data Analysis","CCF","Special Projects - CCF, Algorithmic Foundations","03/01/2018","09/17/2019","Adam Smith","MA","Trustees of Boston University","Continuing Grant","A. Funda Ergun","02/28/2021","$260,000.00","","ads22@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","2878, 7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816242","CHS: Small: Innovation Through Analogical Search","IIS","HCC-Human-Centered Computing","08/15/2018","08/08/2018","Aniket Kittur","PA","Carnegie-Mellon University","Standard Grant","Ephraim Glinert","07/31/2021","$500,000.00","Chu Sern Joel Chan","nkittur@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","014Z, 7367, 7923","$0.00","Comparisons between two things are useful to support creative thinking and reasoning. Many important breakthroughs in science and technology were driven by analogy: observing water led the Greek philosopher Chrysippus to speculate that sound was a wave phenomenon; an analogy to a bicycle allowed the Wright brothers to design a steerable aircraft. Opportunities to find analogical breakthroughs are exploding with the increased availability of online repositories of ideas ranging from scientific papers to patents to the entire web. This project will use computational tools to facilitate searching for analogies drawn from one domain to help with innovative thinking and reasoning in another domain. The goal is to dramatically accelerate innovation and discovery across domains as diverse as science, humanities, mathematics, law, and design. <br/><br/>The planned research bridges the gap between the power of large-scale text mining approaches, which excel at detecting surface similarity, and the depth of human cognition, which is currently unsurpassed at detecting deep analogical similarity. Investigators will explore how representations with weaker structures are robust to issues of language complexity, hierarchies of purposes, and levels of abstraction that are present in real-world documents like research papers and R&D documents. Using these representations, investigators will build computational tools enabling users to connect problems in one field with solutions from another field based on deep structure. Results and algorithms could spur the development of new types of machine-learning techniques that focus on deep structure, and may contribute back to theory in the fields of creativity, problem solving, and innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755823","CRII: CHS: Identifying When People Need a Robot's Assistance","IIS","HCC-Human-Centered Computing","03/15/2018","03/09/2018","Henny Admoni","PA","Carnegie-Mellon University","Standard Grant","Ephraim Glinert","02/28/2021","$175,000.00","","hadmoni@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","7367, 8228","$0.00","Robot collaborators and assistants have the potential to improve lives by helping people perform physical tasks more safely, quickly, and effectively. For example, wheelchair-mounted assistive robot arms can help people with motor impairments perform activities of daily living (like eating) independently, increasing their self-sufficiency and quality of life. However, robot assistance is limited by the fact that robots cannot always recognize when people want or need help. The goal of this research is to develop algorithms that enable robots to recognize when a person is having difficulty with a physical task, based on their behavior before they reach a failure point, and then provide the necessary assistance to complete the task. This work will draw from psychology to explore how nonverbal behaviors like eye gaze, body posture, and facial expression can reveal people's need for assistance. The project will include a data collection study of nonverbal behavior during robot operation. The nonverbal behavior collected during this study will be open sourced to enable other researchers to draw insights about human behavior during human-robot interactions. The work will improve the usefulness of collaborative and assistive robots and lead to better integration of personal robots in workplaces, homes, and assistive care environments.<br/><br/>The main research question in this work is: can robots recognize that a person needs assistance based on their nonverbal behaviors during a human-robot interaction? To investigate this, the project has four goals. Goal 1: Recognize the need for assistance. The work will begin with a large-scale data collection of people's nonverbal behavior (eye gaze, body posture, and facial expressions) during an assistive human-robot manipulation task. Using machine learning approaches, the project team will train predictors on the data that can use nonverbal behavior patterns to recognize when people need assistance. Goal 2: Provide assistance. By monitoring real-time nonverbal behaviors during a human-robot interaction, this system will use the trained predictors from Goal 1 to identify when a person needs help. Once the system predicts that assistance is required, it should be able to provide that assistance seamlessly and in real time using shared autonomy. Goal 3: Evaluate the system. Individual system components will be validated separately, then a full-scale evaluation will be conducted to measure the utility of the implemented system in a real-world assistive human-robot interaction. Goal 4: Create and disseminate an open source data set. A major goal of this project is to collect and share a data set of nonverbal behavior during assistive human-robot interaction, which represents a novel contribution to the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841450","Planning IUCRC at University of Pittsburgh:  Center for Data Science for Materials Reliability and Degradation (MDS-Rely)","IIP","IUCRC-Indust-Univ Coop Res Ctr","12/01/2018","11/07/2018","Paul Leu","PA","University of Pittsburgh","Standard Grant","Prakash Balan","11/30/2019","$15,000.00","","pleu@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","ENG","5761","5761","$0.00","The University of Pittsburgh (Pitt) and Case Western Reserve University (CWRU) are planning to form a new IUCRC named the Center on Materials Data Science for Reliability and Degradation (MDS-Rely). The primary goal of MDS-Rely is to apply data science-informed research to better understand the reliability and lifetime of essential materials. This IUCRC builds on established centers at both locations focusing on Energy Materials, Advanced and Additive Manufacturing, and in Data Science and Analytics. Recent advances in materials data science have the potential to transform this field in creating new technologies that enable unprecedented lifetimes, durability in extreme environments, and understanding of various degradation and failure mechanisms. Our established relationships with industry and our multidisciplinary capabilities in protocols, modeling, and applications are key to taking on this grand challenge of long-lived technologies. <br/><br/>MDS-Rely will engage a diverse cohort of students, and develop broad data science capabilities to respond to employers' needs. MDS-Rely will harness relationships between industry and academia to make headways in applying data science to materials development, design, and reliability.  MDS-Rely will include three Thrusts: (1) Enhanced Reliability Study Protocols, (2) Modeling & Service Life Prediction, and (3) Reliability and Degradation Applications. Thrust 1 will focus on designing data informed experimental studies of materials and developing a data science-based experimental design with both non-destructive and destructive evaluations to monitor material degradation. Thrust 2 will focus on degradation and reliability modeling of materials and will encompass graph-based network modeling, machine learning, image analysis, and time-series analysis. Data analytic pipelines will be created to inform future analysis. These models will give insight back to the design of data informed experimental studies. Finally, Thrust 3 will inform materials design choices for different stress conditions by understanding the main contributors of degradation in a material system. This will also allow for much more rapid material selection in the material choice portion of a system design. The workshop meetings further define specific projects to direct respond to industry needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842971","EAGER: Personal Models for a Navigational Approach to Health","IIS","Info Integration & Informatics","09/01/2018","08/09/2018","Ramesh Jain","CA","University of California-Irvine","Standard Grant","Wendy Nilsen","01/31/2020","$199,512.00","","jain@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7364","7364, 7916","$0.00","Scientific and technological advances in the last century have helped humankind move past infectious disease as the primary cause of mortality.  Average lifespan has doubled from 40 years to above 80 years in developed countries.  Today, chronic disease presents our greatest health challenge. Current healthcare practice based on the episodic infectious disease health model is not a good approach to treating chronic disease.  Instead, healthcare needs to focus on diet, exercise, and other lifestyle activities.  Future health systems will use rich, multi-modal data to sense health state continuously and provide guidance in making daily lifestyle decisions.  The lifestyle navigational approach is a close-loop sense-estimate-act cycle based on sensors, computing elements, and actuators and closely guided by the personal model of the individual.  To provide guidance, predict the future, or understand preferences of an individual, we must build a model that is specific to an individual. The personal model contains not only ordinary medical record data, but also information about disease propensity and lifestyle behaviors.<br/><br/>This project will develop methods to easily and accurately track behaviors that have an impact on health and chronic disease, and methods for incorporating such information into a personalized lifestyle health model.  These models will be critical in developing a navigational approach to health in order to facilitate precision medicine based on predictive and preventive approaches. Nutrition is one of the major components of lifestyle, and yet there are no fully satisfactory approaches for food logging. This proposal will specifically create a  novel approach to food logging based on using a wearable sensor to detect heart rate changes associated with eating, which triggers an ecological momentary assessment (EMA) for food logging.  A food log compared with already available activity logs provides enough quantitative information to understand and analyze lifestyle quantitatively for a person.  This project will develop novel event mining techniques by extending machine learning approaches for building lifestyle related causal models for a person that will be used in predictive approaches as well as in guiding the person's lifestyle decisions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1917537","SaTC: CORE: Medium: Implicit One-handed Mobile User Authentication by Induced Thumb Biometrics on Touch-screen Handheld Devices","CNS","Secure &Trustworthy Cyberspace","08/15/2018","03/14/2019","Lina Zhou","NC","University of North Carolina at Charlotte","Standard Grant","Balakrishnan Prabhakaran","07/31/2021","$626,030.00","","lzhou8@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","8060","025Z, 7434, 7924, 9102","$0.00","People often store private and sensitive data on their mobile devices, and the security of these devices is essential. This project advances and develops a new process for verifying a user's legitimate right to access a mobile device. Existing research has not made this process very usable for many people who lack dexterity or the use of both hands. This research aims to design and develop a method for one-handed authentication on a touch-screen mobile handheld device. The objective is to improve both security and usability of authentication. The proposed methods also will detect unauthorized access to a mobile device in a continuous manner, even if the password is stolen. The interdisciplinary nature of this work will promote teaching, training, and education in mobile security and privacy, human-computer interaction, mobile accessibility, machine learning, and behavioral science. The researchers will actively engage students at both graduate and undergraduate levels in their research activities, and make a strong effort to engage women and underrepresented minorities.<br/><br/>The project will support one-handed mobile authentication on a touch-screen mobile handheld device by inducing thumb biometrics and by enabling one-handed text entry based on thumb strokes. This project will advance authentication research and practice by: (1) laying the groundwork for one-handed authentication in support of both point-of-entry and implicit continuous authentication; (2) introducing a new venue for improving the security of one-handed authentication by inducing and fusing thumb biometrics from user interactions with a touch-screen mobile device; (3) creating new design principles for improving the usability of mobile authentication; and (4) addressing accessibility challenges for users with situational or visual impairments via the support of keypress-less text entry on a mobile touch screen. This project will lend itself to a new solution that can address the common security-usability tradeoff of mobile authentication methods."
"1754268","Collaborative Research: Refining Geothermobarometry in Pyroxenes using In Situ Measurements of Fe3+","EAR","Petrology and Geochemistry","07/01/2018","06/21/2018","Molly McCanta","TN","University of Tennessee Knoxville","Standard Grant","Jennifer Wade","06/30/2021","$146,000.00","","mmccanta@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","GEO","1573","9150","$0.00","This project promotes the progress of science by providing calibrations for use at a national facility, the Advanced Photon Source at Argonne National Laboratory; these results will also be useful for users of other synchrotron facilities worldwide. The calibrations address the scientific question of how much oxygen was present when a mineral crystallized by analyzing a proxy, which is the oxidation state of the multivalent element iron. This understanding has vital importance for understanding how oxygen controls the crystallization path and composition of cooling magmas, also providing insight into processes that may have operated on and in the magma as it moved to the surface. Graduate and undergraduate students at Mount Holyoke College, the University of Tennessee at Knoxville, and the University of Idaho will be supported by this project, including at least three women.<br/> <br/>The ability to measure redox states at sub-nm and pm scales is a formidable analytical challenges due to the inherent anisotropy of most rock-forming minerals, which causes their crystal structures to interact with photons differently according to crystal orientation. This project studies oriented single crystals to explore the effects of crystallographic orientation on x-ray absorption spectroscopy measurements, focusing on the pyroxene mineral group, one of the most common phases in igneous rocks. The team will use these data to build a universal calibration appropriate for quantifying the partial pressure of oxygen (oxygen fugacity) in pyroxene-bearing samples. This work has the potential to advance knowledge in multiple geological disciplines. It will also enable synchrotrons around the world that use the Athena software package to quickly and easily determine Fe3+ contents of pyroxene minerals with known precision and accuracy. Finally, this project applies machine learning techniques to the study of spectroscopic data. Development of this methodology is exportable to other fields, such as medical, biological, and forensic uses of spectroscopy, where it has the potential for societal impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812240","Concentration, Convexity, and Structure","DMS","PROBABILITY","06/15/2018","06/10/2019","Grigoris Paouris","TX","Texas A&M University","Continuing Grant","Tomek Bartoszynski","11/30/2021","$150,000.00","","grigoris@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1263","","$0.00","In various scientific disciplines such as mathematics, statistical mechanics, quantum information, and others, high-dimensional structures play a central role. It has been observed that these distinct areas share the common feature that basic probabilistic principles govern the underlying high-dimensional behavior. In most cases, efficient approximation and study is facilitated by (non-asymptotic) high-dimensional probability. The investigator intends to work on several questions related to the most widely applied principle in high-dimensional probability: the concentration of measure phenomenon. This principle is commonly the main reason behind the frequently-observed tendency of high-dimensional systems to congregate around typical forms.  To quantify this phenomenon, one needs precise inequalities for high-dimensional objects (for instance, measures or random vectors), where independence properties can be lacking. The questions under study have a strong geometric component. Results of the study will have implications in disciplines that depend vitally on high-dimensional objects, including asymptotic geometric analysis, geometric probability, machine learning, sparse recovery, random matrices, and random polynomial theory.<br/><br/>The main goal of the project is to find the quantities or to isolate characteristics of a function that govern its concentration (say with respect to the Gaussian measure); in particular, to determine the quantities that control small fluctuations (variance) and small ball probabilities. The project undertakes a systematic study of this problem and initiates some new methods to compute deviation inequalities (especially in the small ball regime).  It is planned to test these methods on more general measures such as log-concave probability measures. The project will also investigate limit theorems for geometric quantities that complement concentration inequalities at the asymptotic level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840598","Planning Grant: Engineering Research Center for Advanced Materials Manufacturing and Discovery for Extreme Environments (CAM2DE2)","EEC","ERC-Eng Research Centers","09/01/2018","08/29/2018","Raymundo Arroyave","TX","Texas A&M Engineering Experiment Station","Standard Grant","Sandra Cruz-Pol","08/31/2021","$100,000.00","L. Brinson, Richard Neu, Ibrahim Karaman, Preet Singh","rarroyave@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","1480","123E, 1480","$0.00","The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program. Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>Technology advances are resulting in increasingly complex engineering systems for energy, biomedical devices, infrastructure, transportation, and space exploration. These advances also require materials that can perform in extreme environments. These requirements currently exceed the operating limits of available materials. This project envisions an ERC that brings together experts from multiple areas and perspectives to tackle this issue at two levels. The first level is the need to have input from the areas of materials, process, product, and life cycle design research to tackle these challenges. The second level looks at how advanced materials and manufacturing modeling, data analytics, and design methodologies interact in a comprehensive solution. The vision of this planning grant is to establish a joint industry-academia research and innovation team to identify overarching fundamental scientific issues that limit current material capabilities. The goal of this ERC planning grant will be to bring these research areas and approaches together to discover and design advanced materials that can extreme conditions. The proposed ERC would impact society through the development of materials and infrastructure that are durable and resilient, improving the safety and welfare of society against extreme events. <br/><br/>Understanding the behavior of materials and engineered systems under extreme conditions and translating this knowledge into the design of robust and sustainable materials requires a full description of the multi-scale phenomena associated with the interaction between materials and environment. The proposed ERC planning grant will provide a fertile platform to bring stakeholders together through three regional workshops. The scientific and technological problems identified at these workshops will serve to enable a fundamental paradigm shift in materials and manufacturing design for extreme environments. This shift will incorporate computational materials/product design, machine learning, and stochastic approaches. This ERC planning grant will enable comprehensive formulation of a sustainable vision, focus, and ERC structure on the proposed topic. Key material systems and testbeds will be identified, in collaboration with targeted industry partners, professional societies, federal regulatory bodies, and end users. Long-term plans for integrated education, entrepreneurship, technology transfer, and broadening participation activities will be developed. Applications will span from healthcare, transportation, and energy generation, to infrastructure applications. These activities will engage a wide range of stakeholders, from K-12 to professionals as well as industry partners. Established partnerships and collaborations with minority-serving institutions will provide opportunities for broadening participation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755687","CRII: SCH: Applying Motor Control Theories for Ambulatory Monitoring of 3D Upper-Limb Movement","IIS","Smart and Connected Health","07/15/2018","07/09/2018","Sunghoon Lee","MA","University of Massachusetts Amherst","Standard Grant","Wendy Nilsen","06/30/2020","$174,228.00","","sunghoonlee@umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8018","8018, 8228","$0.00","Accurate monitoring of three dimensional (3D) upper limb posture and motion in a community setting has been of paramount importance in rehabilitation. This work is aimed at developing a system that can provide an objective assessment of individually-tailored therapeutic treatments. Specifically, in movement disorders with a long recovery time, such as stroke or traumatic brain injury, continuous monitoring of movement using a minimally-invasive sensing has been a goal to support long-term adherence. A wrist-worn inertial sensor has been the most commonly used wearable sensor due to its immediacy, ubiquity, and acceptance for sustained use. However, developing a precise understanding of upper limb movements based on a single wrist-worn device is challenging. Data from these devices tends to drift over time; that is, a small error in the sensor measurements grows rapidly as the data are integrated to estimate the movement. This project aims to establish a system that allows accurate monitoring of upper limb movement using a single wrist-worn inertial sensor by specifically addressing the issue of drift.<br/><br/>The proposed effort will advance the state-of-the-art in ambulatory monitoring of upper limb motion by exploiting the unique kinematic properties of voluntary upper limb movements mediated by the human central nervous system (CNS) and the physical properties of the musculoskeletal structure. The project starts from prior knowledge regarding the unique kinematic characteristics of limb motion that would eliminate the need for the second integration and many of the drift errors. This model will provide unique opportunities to develop a novel computational algorithms for precise measurement of upper limb motion and kinematics. This study will address the following scientific challenges: 1) development of mathematical models and computational algorithms to estimate the dynamically changing body direction, 2) establishment of a new machine learning framework to estimate the 3D position trajectory of the sensing unit without double integration by leveraging the motor control theories, and 3) development of a sequential algorithm to estimate the most likely kinematic profiles of limb joints based on the human musculoskeletal properties. The success of this project will lead to a major breakthrough in precise gesture monitoring in the free-living setting, opening a new door leading to previously unexplored datasets and potentially new development of personalized disease management via unobtrusive monitoring of motor functions in movement disorders. This project will also embrace the integration of interdisciplinary research and undergraduate/graduate training among the areas of wearable computing, signal processing, data science, and smart health.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815323","CSR: Small: A Just-in-Time, Cross-Layer Instrumentation Framework for Diagnosing Performance Problems in Distributed Applications","CNS","CSR-Computer Systems Research","10/01/2018","08/03/2018","Raja Sambasivan","MA","Trustees of Boston University","Standard Grant","Samee Khan","02/29/2020","$460,249.00","Ayse Coskun, Orran Krieger","raja@cs.tufts.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7354","7923","$0.00","Distributed applications running in data centers are critical to society (e.g., for shopping, banking).  Engineers must diagnose and fix problems observed in data centers quickly; however, doing so is extremely challenging.  A significant hurdle is that engineers must spend significant time and effort exploring what instrumentation (e.g., log messages about specific application behaviors) is needed to provide visibility into a new problem.  To assist in this front, this project will develop an instrumentation framework that, in response to a new problem, will automatically search the space of possible instrumentation choices and enable the instrumentation needed to provide insight into it.<br/><br/>This project addresses fundamental challenges associated with creating an automatic instrumentation framework: (a) What algorithms and heuristics are suited for automatically and efficiently exploring the instrumentation search space?  (b) What architectural support is needed within the framework to enable automatic exploration?  (c) How can the search space be explored without significantly impacting application performance?  The proposal will explore the utility of algorithms based on operator knowledge, statistics, and machine learning to explore the search space.  It will build on end-to-end tracing, as this will enable the framework to work for problems that affect different sets of requests.<br/><br/>This project will inform the architecture of next-generation instrumentation frameworks, which are needed to keep pace with the ever-increasing complexity of distributed applications.  The critical issues identified in popular open-source distributed applications while evaluating the framework will improve their robustness.  Researchers will be able to leverage the software artifacts released by this project to create novel distributed-application-management tools that leverage the framework's unique capabilities.  They will be able to deploy the framework in research clouds to obtain valuable workload traces from them.  The project will generate course modules on diagnosis practices for distributed applications.<br/><br/>The artifacts produced by this project, including framework source code, workload traces, instrumented applications, and research results, will be freely disseminated online at: https://massopen.cloud and https://www.rajasambasivan.com.   All software artifacts will be stored in Github as well.  All artifacts will be available for a minimum of seven years from the start of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821780","SaTC-EDU: PHIKS - PHysical Inspection and attacKs on electronicS","DGE","CYBERCORPS: SCHLAR FOR SER, Secure &Trustworthy Cyberspace","08/01/2018","02/06/2019","Navid Asadi","FL","University of Florida","Standard Grant","Li Yang","07/31/2021","$308,000.00","Domenic Forte","nasadi@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","EHR","1668, 8060","025Z, 7254, 7434, 9178, 9179, 9251, SMET","$0.00","The hardware security community has grown significantly over the past decade.  However, research on the most advanced topics such as reverse engineering, physical attacks, and counterfeit detection remains stuck in the early stages of investigation and/or development.  First, owning and maintaining the required instrumentation is too prohibitive for academia, thereby limiting research to a few focused labs in industry and government.  Second, the knowledge and expertise needed to fully understand these topics and to address their challenges lies at the intersection of multiple non-overlapping scientific fields including microscopy, physics, material science, and computer engineering.  Third, electronic hardware spans multiple levels (device, chip, printed circuit board, and system), which are each subject to unique attack modes and requirements.  Although some book chapters and surveys have attempted to bridge these gaps, they only introduce the basic concepts to academic researchers, students, and practitioners.<br/><br/>The goal of this project is to create a course that exposes students to advanced topics in hardware security.  Specifically, ten educational modules will be created that cover (1) the background and motivation for destructive and non-destructive reverse engineering, invasive and non-invasive probing, fault injection, and counterfeit detection;  (2) the fundamentals of inspection/attack equipment including optical microscope, scanning electron microscope (SEM), focused ion beam (FIB), photon emission microscope (PEM), and X-ray microscope (XRM);  and (3) an introduction to the image processing, machine learning, and classification techniques needed to make sense of microscopy data in the context of hardware security.  The components of each module will include one or more lectures, an interactive equipment demonstration, an archetypal data set, and multiple forms of assessment (quiz, lab experiment, and exam). Data sets and demos will be generated based on prior research experiments as well as new experiments using the world class instruments available at University of Florida. By making the educational materials available to the academic community, this project will have a significant impact on education and workforce development in cybersecurity, failure analysis, and electronics testing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750931","CAREER: Mathematical Modeling and Computational Tools for in vivo Astrocyte Activity","IIS","Robust Intelligence","06/01/2018","06/16/2020","Guoqiang Yu","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Kenneth Whang","05/31/2023","$529,249.00","","yug@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7495","1045, 7495, 8089, 8091","$0.00","This project aims to develop new computational tools to interpret and analyze the activity of astrocytes. Astrocytes, one type of glial cells and the most populous cells in brain, have recently been found to have much more active functionality than previously thought. Astrocytes closely listen to and proactively regulate the nervous system, yet their exact roles in normal and pathological brains remain elusive. Recent technological progress makes it possible to monitor astrocyte activity with unprecedented spatial and temporal resolution, but considering the complexity and scale of astrocyte activity data, rigorous computational modeling is critically needed. This project will establish a solid foundation for quantitative analysis of astrocyte activity, enabling in vivo analysis, greater reproducibility, and  benefits for understanding brain disorders. The computationally challenging problems formulated in this project are expected to be valuable for other areas of computer science, serving as examples to build new statistical models and develop powerful generic machine-learning theory and algorithms. <br/><br/>The proposed research will develop a comprehensive data-driven framework to model astrocyte activity, with the specific goals of automatically detecting calcium events, identifying functional independent units and characterizing their individual and systems manifestation. Three research objectives include: (1) developing computational approaches to detect calcium events (2) systems modeling and quantification of astrocyte activity, and (3) application to in vivo astrocytes of mouse and other model animals to  showcase the usefulness of the proposed methodology by addressing several different current biological questions. The educational objective is to help maintain the competitive vitality of the U.S. computational neuroscience workforce, and to deepen the understanding of core computational concepts, by incorporating cutting-edge computational neuroscience problems into the engineering curriculum, by improving the recruitment and retention of women and minority students, by reaching out to K-12 students to inspire their interest in this interdisciplinary field, and by interacting with undergraduate students to offer opportunities and encourage them to choose computational neuroscience as a career.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761990","Spokes: MEDIUM: NORTHEAST: Collaborative: Advancing a Data-Driven Discovery and Rational Design Paradigm in Chemistry","OAC","BD Spokes -Big Data Regional I, OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, PROJECTS","09/01/2018","08/23/2018","Johannes Hachmann","NY","SUNY at Buffalo","Standard Grant","Lin He","08/31/2021","$700,000.00","Alan Aspuru-Guzik, Geoffrey Hutchison, Marcus Hanwell","hachmann@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","024Y, 1253, 1712, 1978","028Z, 054Z, 062Z, 7433, 8083","$0.00","With support from the Division of Chemistry (CHE) and the Division of Materials Research (DMR) in the Directorate for Mathematical and Physical Sciences (MPS) and the Directorate for Computer & Information Science & Engineering (CISE), this project aims to advance the use of modern data science in chemistry. In particular, the project will advance the field of data-driven chemical research by promoting the use of machine learning and other data mining techniques in the molecular sciences and by fostering and coalescing a community of stakeholders. The work of the project and the community it represents aims to transform chemistry's ability to tackle challenging discovery and design problems. This approach can dramatically accelerate and streamline the process that leads to chemical innovation -- an important factor in economic and technological advancement -- and thus result in an improved return on public and private investments. The project also addresses corresponding questions of training and workforce development needed in chemistry, thus insuring the US's international competitiveness. <br/><br/>The mission of this project is to assert the role of big data research in the chemical domain, i.e., to promote, enable, and advance the ideas of data-driven discovery and rational design. The project aims to create a community-driven roadmap as well as facilitate concrete solutions that are beyond the scope of the disjointed efforts of its individual stakeholders. The Big Data Hubs and Spokes ecosystem is the ideal framework to realizing this vision and accelerating progress in this high-priority area of research. The effort at hand sets out to implement some of the key findings of the recent NSF Division of Chemistry workshop on Framing the Role of Big Data and Modern Data Science in Chemistry. The four signature initiatives of this Spoke project include (i) the planning, coordination, integration, and consolidation of community-developed software tools for big data research in chemistry as well as the formulation of guidelines, best practices, and standards; (ii) the organization of workshops for community building, to connect solution seekers with solution providers, and to address questions ranging from strategic to technical; and (iii) the creation and dissemination of community-developed teaching materials as well as the formulation of course, program, and curricular recommendations for education and workforce development that reflect the changing, data-centric approach in chemical research; (iv) providing access to a shared hardware infrastructure for community data sets, on-site data mining capacity, and the exploration of domain specific method and hardware issues.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817073","SHF: Small: Indy: Toward Safe and Fast Compiler Flags","CCF","Software & Hardware Foundation, Software Institutes","10/01/2018","04/09/2019","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Nina Amla","09/30/2021","$497,430.00","","ganesh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798, 8004","026Z, 7923, 7942, 8004, 8206, 9251","$0.00","Rapid advances in computer architecture are necessitated by the growing demand for computing speed and also energy efficiency. Unfortunately, these advances require significant adaptations of existing software and their recompilation, and when applied to software in scientific simulations and engineering domains, the numeric outputs of these software can change. This issue is of significant concern, as the whole scientific and engineering enterprise rests on reproducible results building on top of prior contributions. This research project develops methods to detect and isolate those software components responsible for changing answers, and produces actionable evidence in terms of the software routines that need to be rewritten. This ensures that scientific software can truly become an asset that serves generations of research, and not be obviated every three or so years when hardware changes.<br/><br/>The project develops methods to compile a users' software application or library using different compilers and executing on different platforms. Given a collection of acceptance tests, the tools developed in this work (collectively called ""Indy"", connoting ""Fast and Safe Compiler Flags"") help detect those components that produce unacceptably deviant answers. Then, employing code bisection methods, the Indy tools root-cause the source of variability to the level of single file or even to a single function symbol. Given that the methods used by Indy can be computationally intensive, the project also explores machine-learning methods and other static analysis methods to avoid wholesale recompilation and re-execution. The Indy tools are maintained on public repositories, and the investigative team maintains strong connections with user-groups to ensure tool adoption, feedback, and refinement. The project will release Indy as open source and build a user community around Indy by ensuring that interested researchers are able to contribute to the code-base. This will allow a wider growth of the project. This aspect is of special interest to the software cluster in the NSF Office of Advanced Cyberinfrastructure, which is co-funding this award.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837869","2018 Workshop: Integrating Neurophotonics, Statistical Physics, and Control Theory for Advancing Neuroscience","PHY","PHYSICS OF LIVING SYSTEMS","11/01/2018","10/19/2018","David Boas","MA","Trustees of Boston University","Standard Grant","Krastan Blagoev","10/31/2019","$88,350.00","","dboas@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","7246","7237, 7556, 9183","$0.00","The 50-person, two-day 2018 Workshop: ""Integrating Neurophotonics, Statistical Physics, and Control Theory for Advancing Neuroscience"" will identify research efforts that will be needed to close the gaps between the future potential and the current state-of-the-art in neurophotonic-sensing and modulation and big data as it relates to advancing our understanding of the rules of the functioning brain. The discussions among the multi-disciplinary participants will help to define the challenges and questions that should be addressed given the recent explosive growth in neurophotonics and big data technological capabilities for studying the brain. Inclusion of young scientists in the workshop will bring fresh perspective to the discussions and foster their career development through the interactions and networking. Specific major outcomes of the workshop will include the increased awareness of research, education, and technology transfer related to neurophotonics and big data; ideas enabling the transformation of research and STEM education; partnership opportunities among government, academia, industry, and the general public; and synergism for transformative activities in the application of neurophotonics and big data technology.<br/><br/>The Workshop will provide a forum for disseminating information and sharing ideas about the frontiers of neuro-sensing and modulation with light and the integration with statistical modeling, machine learning, and control theory. The workshop will contribute toward defining the important topics and questions to be addressed with future research, and will identify research directions and opportunities that, if pursued, can initiate new developments in neurotechnology and computational neuroscience / big data modeling that will lead to new break-through neuroscientific studies. The workshop will result in a comprehensive publishable document that will be widely distributed in the SPIE journal Neurophotonics. This workshop will be a two-day event on October 22-23, 2018, in Alexandria, Virginia. The workshop will focus on three themes: reviewing the frontiers of neuro-sensing and modulation with light and the intersection with big data methods; exploring the opportunities for neurophotonic methods to integrate with big data methods to significantly advance the impact on neuroscientific studies; and defining novel neuroscientific studies to guide the integration and advancement of these technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812600","SaTC: STARSS: Small: Analysis of Security and Countermeasures for Split Manufacturing of Integrated Circuits","CNS","Secure &Trustworthy Cyberspace","08/01/2018","07/20/2018","Azadeh Davoodi","WI","University of Wisconsin-Madison","Standard Grant","Sandip Kundu","07/31/2021","$312,210.00","","adavoodi@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8060","021Z, 025Z, 7434, 7923, 9102","$0.00","Integrated circuit fabrication has spread across the globe, with over 90% of the world's fabrication capacity controlled by non-US companies. This project is on studying the security of chip fabrication by an untrusted foundry. The fabrication technique, known as split manufacturing, is based on partial sharing of the chip design information with the untrusted foundry in order to protect the intellectual property of the chip. With this technique, only a challenging portion of the chip is manufactured at the untrusted foundry. The remaining is completed by a trusted and smaller-scale foundry. <br/><br/>Specific research tasks include: (a) use of machine learning techniques to decide an appropriate ""split level"" to divide the chip design files into trusted and untrusted portions; (b) developing design-aid tools to obfuscate the chip layout files in order to make reverse engineering by the untrusted foundry infeasible; (c) development and public release of a software tool to generate various split-manufactured instances from the design files of the full chip, in order to promote research in this area.<br/><br/>Besides the release of the software tool, research findings will be published across communities related to the fields of hardware security and Integrated Circuit design. Specific components of the project are designed to promote involvement of undergraduate students in research. Involvement of women and underrepresented students in research will be pursued at all levels.<br/><br/>Data and software tools will be publicly accessible from http://homepages.cae.wisc.edu/~adavoodi/split-man.htm and will remain active for at least two years following the end date of the award.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828187","MRI: Acquisition of an HPC System for Data-Driven Discovery in Computational Astrophysics, Biology, Chemistry, and Materials Science","OAC","Major Research Instrumentation, Information Technology Researc","10/01/2018","08/24/2018","Srinivas Aluru","GA","Georgia Tech Research Corporation","Standard Grant","Stefan Robila","09/30/2021","$3,699,317.00","Deirdre Shoemaker, Surya Kalidindi, Charles Sherrill, Richard Vuduc","aluru@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1189, 1640","026Z, 1189","$0.00","The project funds the purchase of a high-performance computing and storage system at the Georgia Institute of Technology. This computing instrument will support data-driven research in astrophysics, biosciences, computational chemistry, materials and manufacturing, and computational science. These projects contribute to national initiatives in big data, strategic computing, materials genome, and manufacturing partnership; and NSF supported observatories such as the gravitational wave observatory and the South Pole neutrino observatory. The system also serves as a springboard for developments of codes, software prototyping, and scalability studies prior to using national supercomputers. Advances made in computational methods and scientific software are disseminated in the form of open-source codes and data analysis portals. Over 33 faculty, 54 research scientists/postdocs, 195 graduate students, and 56 undergraduate students will immediately benefit from the instrument. In addition, the system provides training opportunity at all levels from undergraduate students to early career researchers, in important interdisciplinary areas of national need. A fifth of the system capacity is utilized to enable research activities of regional partners, researchers from minority serving institutions, and other users nationally through XSEDE participation. The project involves undergraduate student participation from historically black colleges from Atlanta metropolitan area. Public outreach efforts are planned through videos of public interest and local events such as the Atlanta Science Festival.<br/><br/>The cluster will combine regular compute nodes with others configured to emphasize one of the following: big memory, big local storage, solid state storage, Graphics Processing Units (GPU), and ARM processors. In doing so, the system can be employed by a diversity of projects. In astrophysics, the instrument bolsters data-driven research including detection of gravitational waves, astrophysical neutrinos, and gamma rays. It does it by leveraging data from leading astroparticle observatories and contributing to their mission. It also leads to improved insights into formation of supermassive black holes and large-scale structure of the universe. The computing system also aids the development of parallel software in computational genomics, systems biology, and health analytics. Important applications in assembly and network analysis of plant genomes, and environmental metagenomics are pursued. The instrument also enables next generation algorithms and software for computational chemistry and expands the boundaries of molecular simulation. The system enables advances in density function theory, enhances studies of crystal defects and nanostructures, and injects novel use of machine learning techniques in computational chemistry. It also fosters the development of data science methodologies to identify building blocks of materials at multiple scales, thus significantly reducing the development and deployments cycles for new materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816577","SHF: Small: Hyperscaling Data Analytics for High-Performance Computers","CCF","Software & Hardware Foundation","07/01/2018","07/02/2018","Spyros Blanas","OH","Ohio State University","Standard Grant","Almadena Chtchelkanova","06/30/2021","$460,000.00","","blanas.2@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7798","7923, 7942","$0.00","Data analytics extracts insights from massive datasets, often with the assistance of machine learning techniques. The goal of this project is to allow domain experts, including data scientists, to analyze massive datasets quickly using the most powerful supercomputing systems in the world. The problem is that state-of-the-art data processing algorithms that filter data, summarize results and combine information from different sources have inherent scalability bottlenecks. This project designs hyperscalable data processing algorithms that harness the unprecedented compute, storage and networking concurrency of a high-performance computer. This project also develops an open-source data processing engine to disseminate prototype implementations of these algorithms to the public. Another contribution is the creation of a massively parallel data processing module and associated teaching materials for undergraduate data science curricula, such as the diverse Data Analytics undergraduate major at The Ohio State University.<br/><br/>The confluence of extreme compute parallelism, fast networking and growing memory capacities in the modern datacenter presents an opportunity to design a hyperscalable data processing kernel for warehouse-scale computers. This project sits at the intersection of data management and high-performance computing; it develops scalable join and aggregation algorithms, topology-conscious query planning and optimization techniques, and interference-aware data access methods for shared cold storage. This is accomplished by carefully overlapping communication and computation, identifying and avoiding unscalable all-to-all communication, accounting for network path congestion and variability in remote memory access latency, and judiciously using inter-process coordination to accelerate data ingestion from a massively parallel shared file system. These research activities lay the intellectual foundation to make data analytics scalable and efficient in warehouse-scale computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811109","SHF: Small:Extremely Energy-Efficient Monolithic 3D System Architectures","CCF","Software & Hardware Foundation","07/01/2018","05/18/2018","Niraj Jha","NJ","Princeton University","Standard Grant","Sankar Basu","06/30/2021","$450,000.00","","jha@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7798","7923, 7945","$0.00","With the slowing down of Moore's Law, traditional 2D scaling is not expected to deliver the area, power, and performance benefits the semiconductor industry has been counting on.  Thus, the time has come to go vertical, accommodating processor cores, accelerators, cache, and main memory in the same package. There are two ways of doing this: use through-silicon vias (TSVs) or monolithic 3D integration. TSVs do not have memory-on-logic stacking success stories.  Monolithic 3D integration uses monolithic inter-tier vias that have a much smaller diameter than TSVs, and enable many different design styles: transistor-level monolithic, gate-level monolithic, and block-level monolithic.  While fabrication and test techniques for monolithic 3D integration are maturing, monolithic 3D system architectures have not been investigated in depth.  The work on this project fills this gap. The methodologies and tools developed under this grant will be made available on the web. They will also be made available to the industry.  The material will be included in course materials, and under-represented graduate students will be attracted to this research through Princeton Fellowships.  Results will be disseminated through research articles and seminars.<br/><br/>There are various ""walls"" confronting computer system architects these days.  The Power Wall constrains the portion of the chip that can be powered on.  This is also known as the dark silicon problem.  The Memory Wall prevents efficient access to off-chip memory. Monolithic 3D integration has the potential to significantly alleviate the problems associated with these walls, especially for abundant-data problems, such as machine learning and inference, which are becoming commonplace. This project seeks to improve the energy efficiency of monolithic 3D system architectures by a factor of 500 relative to traditional system architectures.  It will do so by exploiting synergies across the device, logic, memory, accelerator, micro-architecture, chip multiprocessor, and monolithic 3D IC levels of the design hierarchy, providing a common computation platform from high-performance mobile devices to data centers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762595","GOALI/Collaborative Research: Control-Oriented Modeling and Predictive Control of High Efficiency Low-emission Natural Gas Engines","CMMI","Special Initiatives, Dynamics, Control and System D","09/01/2018","03/27/2020","Javad Mohammadpour Velni","GA","University of Georgia Research Foundation Inc","Standard Grant","Robert Landers","08/31/2021","$275,991.00","","javadm@uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","ENG","1642, 7569","030E, 034E, 091Z, 1504, 8024","$0.00","About 200 million internal combustion engines (ICEs) are produced in the world every year and used in energy, transport and service sectors. Furthermore, ICEs account for over 22% of the U.S. total energy consumption and produce the largest portion of CO2 greenhouse gas emissions in urban areas. Dual fuel natural gas (NG) engines in advanced low temperature combustion regimes represent the state-of-the-art ICE technology with some of the highest reported fuel conversion efficiencies and 25% lower CO2 emissions compared to conventional engines. However, achieving a robust and high-efficiency performance of these engines on a broad operational range using existing control technologies is not possible due to their highly nonlinear and uncertain dynamic behavior. This research aims at developing fundamental tools for dynamic modeling and control of nonlinear systems and applying them to high-efficiency low-emission advanced ICEs. The project will provide wide-ranging societal benefits through three major impact areas: first, by advancing research in nonlinear control systems, and mixing and reactive flow including combustion systems; second, by providing direct benefits for control of combustion engines, commonly used in power generation, automotive, locomotive, marine, oil and gas drilling, construction, utilities and manufacturing industries; and third, through educational and outreach activities delivered at industry sites, local communities and science fairs. <br/><br/>This project is a collaborative effort between Michigan Technological University, University of Georgia, and the industry partner, Cummins Inc. The project intends to develop a suite of innovative control-oriented modeling and stochastic predictive control design tools to address control challenges for advanced dual fuel natural gas engines, as well as a broad range of other nonlinear and stochastic dynamic systems. The outcomes of this project result in six main components that include: (i) characterizing the dynamics of dual fuel NG engines in advanced combustion regimes, (ii) building the first physics-based control-oriented model for advanced dual fuel NG engines, (iii) developing new analytical tools for deriving models through the powerful fusion of machine learning and classical multivariate methods, (iv) providing solutions to fill the gaps between first-principles models and data-driven methods for estimating an accurate model, (v) bridging the gaps between parameter-varying systems and stochastic controls, and (vi) constructing, testing, and validating the combustion controllers for dual fuel NG engines. The outcomes from these six theoretical, modeling and experimental contributions will be generic dynamic modeling and predictive control design tools for nonlinear and stochastic industrial systems that are demonstrated on engine test-beds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815054","III: Small: Combining Stochastics and Numerics for Improved Scalable Matrix Computations","IIS","Info Integration & Informatics","09/01/2018","08/17/2018","Michael Mahoney","CA","International Computer Science Institute","Standard Grant","Wei Ding","08/31/2021","$500,000.00","","mmahoney@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7364","7364, 7923","$0.00","Data are often modeled as matrices.  As a result, linear algebraic algorithms, and in particular matrix decompositions, have proven extremely successful in the analysis of datasets in the form of matrices.  RandNLA (Randomized Numerical Linear Algebra), which integrates the complementary perspectives that theoretical computer science and numerical linear algebra bring to matrix computations, has led to nontrivial theory and high-quality implementations, and it has proven useful in a range of scientific and internet applications.  This project will addresses statistical properties of RandNLA algorithms, and how these algorithms are used in downstream convex and non-convex optimization pipelines.  This project will facilitate the development of algorithmic methods for the extraction of knowledge from large genetic, medical, internet, financial, astronomical, and other scientific data sets, and it will also focus on broader interdisciplinary educational opportunities, including undergraduate courses on the mathematics of data science. <br/><br/>Examples of technical challenges of interest include that the randomness inside the algorithm can lead to implicit regularization, and that it can also lead to usefulness in downstream applications that is not captured by existing theory.  These and other challenges will be addressed in several complementary ways.  First, by developing bootstrapping methods for core RandNLA algorithms.  Second, by developing improved statistical analysis of core RandNLA algorithms.  Third, by developing non-linear leverage scores for more general statistical objectives.  Fourth, by developing methods to combine in a principled manner SGD and RandNLA.  And fifth, by providing implementations addressing scientific data analysis applications, and also by considering longer-term directions of interdisciplinary interest.  In each case, there will be a focus on complementary stochastic and numerical aspects of RandNLA algorithms, as well as on how RandNLA primitives are used in realistic convex and non-convex machine learning pipelines.  This will lead to new insights in algorithmic and statistical theory, as well as more useful algorithms in practical implementations and applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746758","SBIR Phase I:  Building a Global Community to Crowdsource-Clean the Planet","IIP","SBIR Phase I","01/01/2018","12/21/2017","Jeff Kirschner","NC","Litterati, LLC","Standard Grant","Rajesh Mehta","06/30/2018","$225,000.00","","jeff@litterati.org","131 Turvey Ct.","Chapel Hill","NC","275145260","4154845880","ENG","5371","5371, 8031, 8033","$0.00","The broader impact/commercial potential of this SBIR Phase I project stems from developing a crowdsourced solution to litter - one of the world's most pervasive and toxic problems. To many, it's dirty, disgusting, and someone else's problem to solve. Unfortunately, we all suffer the consequences, as litter impacts our economy, degrades the environment, demoralizes community pride, kills wildlife, and poisons the food system. This project aims to develop a mobile technology that empowers anyone to identify, map, and collect the world?s litter, while simultaneously connecting to a broader community of associated brands, cities, schools. By crowdsourcing the data and cleanup, there is great potential in collecting massive amounts of information which can be used for everything from infrastructure improvement, to resource allocation, brand packaging redesign, and even individual responsibility and behavioral change.<br/><br/><br/>The intellectual merit of this project is derived from building a global database of litter. And one critical need to achieving such a monumental task is the ability to quickly (and accurately) identify any piece of litter, anytime, anywhere even if the litter is in a deep state of decay and decomposition. This project further aims to integrate that information with other data sets including location, time, proximity to schools, and the watershed. By leveraging technologies such as image recognition and machine learning, the project will further empower the people who are crowdsourcing the data and cleanup to collect a vast amount of identifiable information."
"1750760","CAREER: Compiler and Runtime Support for Multi-Tasking on Commodity GPUs","CNS","Special Projects - CNS, CSR-Computer Systems Research","05/01/2018","04/28/2020","Bo Wu","CO","Colorado School of Mines","Continuing Grant","Matt Mutka","04/30/2023","$305,183.00","","bwu@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","1714, 7354","1045, 7354, 9251","$0.00","General-purpose Graphics Processing Units (GPU) computing has become mainstream, as witnessed in various domains such as machine learning, graph analytics, and scientific simulation. One notable trend is employing GPUs in data centers and cloud computing infrastructures to satisfy users' increasing demand to accelerate their applications. In such multi-tasking environments, applications from different users contend to use the shared GPU, leading to unpredictable and unacceptable performance degradation. This CAREER project aims at developing a set of compiler and runtime techniques to support multi-tasking on commodity GPUs in a transparent and efficient manner. The compiler techniques circumvent the hardware limitations to enable a set of features, such as preemption, and the runtime system schedules applications to utilize the potential of the GPU and guarantees quality of service. In addition, the investigator advances GPU education in the University to target both Computer Science (CS) and non-CS students based on a GPU education center.<br/><br/>Specifically, the project investigates how to integrate compiler and runtime techniques to support multi-tasking on GPUs by building a system that achieves three goals. First, the system addresses GPU core contention by enabling flexible GPU kernel preemption. The compiler transforms the GPU program to be a preemptable form by circumventing the limitation imposed by the hardware thread scheduler. The runtime intercepts all GPU kernel launch requests and makes global preemption and scheduling decisions to maximize performance. Second, the system supports fine-grained sharing for threads from different applications to fully utilize hardware resources within GPU streaming multi-processors. The runtime guarantees the QoS of user-facing applications while optimizing overall throughput aided by performance prediction. Third, the system addresses GPU memory contention by coordinating GPU memory transfers, which considers memory access patterns and array reuse patterns.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931895","EAGER: Using Search Engines to Track Impact of Unsung Heroes of Big Data Revolution, Data Creators","IIS","Info Integration & Informatics, NSF Public Access Initiative","11/29/2018","04/26/2019","Adam Godzik","CA","University of California-Riverside","Standard Grant","Sylvia Spengler","09/30/2019","$154,535.00","","adam@godziklab.org","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7364, 7414","7364, 7916","$0.00","Efficient mechanisms of data exchange are increasingly central to science (and society) in the midst of the deluge of complex data. The pace of data production and its complexity mean that a large amount of data is often not adequately analyzed by the data producers, and thanks to its rapid dissemination, the broad community is participating in its analysis. This model, however, carries a risk of neglecting the input of the original data creators, as attention shifts to data integrators and analyzers. The current paradigm of information dissemination and assigning credit in science, based on peer-reviewed publications and formal acknowledgment of third-party contributions in the form of citations, is biased toward high-profile, well-known scientists and research centers who participate in the latter stages of knowledge creation and dissemination. We propose to use unbiased internet searches to identify uncredited use of datasets and resources in research literature, allowing data creators and researchers participating in early stages of data creation to claim credit for their work. Using machine-learning and text-mining techniques, the PI seeks to extract relevant information from noisy results of general-purpose search engines and develop easy-to-use interfaces for public use of such resources to supplement official bibliometric resources.<br/><br/>Acceptance and citation biases have a significant impact on careers of researchers outside the central foci of funding and publications, which are also typically places with more-diverse research forces. First, the probability of rejection in peer review is significantly biased against less-famous scientists and those at less-research-intensive institutions. These biases are less likely to affect data creation as databases typically accept data without peer review and the value of data can be measured by its use. The same biases affect number of citations, where people tend to cite more-famous, established scientists, or cite reviews that are often invited and only leading scientists would have been invited to write the review. As a result, both publications and citations are heavily biased toward already-recognized scientists that represent less-diverse populations, both in personal terms and in terms of the institutions where they work, as compared to the general population of scientists. Such biases affect careers and ability to obtaining grant funding for young scientist operating outside of the tight collaboration networks at best research institutions and creates a classical rich getting richer and poor getting poorer loop. The new internet-based information exchange paradigm has already had a profound effect on researchers? ability to getting their results in prominent, public view. This proposal aims at approaches that would further alleviate publication and citation bias, not by addressing it directly, but by developing more openways of evaluating contributions to science."
"1804843","Collaborative Research: Understanding and manipulating the solvent microenvironment for selective, catalytic amination of renewable oxygenates","CBET","Catalysis","09/01/2018","08/13/2019","Jesse Bond","NY","Syracuse University","Continuing Grant","Robert McCabe","08/31/2021","$148,911.00","","jqbond@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","ENG","1401","","$0.00","The initial steps of biomass refining involve breakdown of the raw material to a biocrude oil containing a mixture of building block chemicals.  The building block chemicals can be further refined to higher value products, often in the liquid phase, with the aid of a solvent and a solid catalyst.  This project will investigate the transformation of one of those building block chemicals, 3-hydroxybutryolactone (3HBA), to several higher-value chemicals.  Theoretical analysis and experimental methods will be combined to understand how the solvent influences the performance of the catalyst in promoting conversion of 3HBA to the desired products.  Results of the study can be applied more generally to other bio-based chemicals to support a growing bio-refining industry relevant for the transition to renewable chemical production.  The project will contribute to a highly trained workforce of experts in biomass processing, while also adding to U.S. technical prominence in biomanufacturing of chemicals.  <br/><br/>A major goal of heterogeneous catalysis research is to identify active sites and to understand how they interact with reactants, products, and the bulk environment to facilitate chemical transformations.  While most catalyst studies focus on catalyst discovery, it is often the bulk reaction environment that benefits most from redesign. The focus on solvation effects in heterogeneous catalysis has recently expanded with the trend toward liquid-phase, catalytic processing of biomass. Motivated by this shift, the project focuses on developing the scientific foundations needed for the rational design of solvent systems for catalytically processing renewable oxygenates. Specifically, the proposed research aims at understanding how the nature of the solvent microenvironment impacts activity and selectivity of ruthenium (Ru) catalysts during reductive amination of 3-HBA to form 2-amino-3-hydroxytetrahydrofuran and 3-aminotetrahydrofuran. The proposed combination of computational and experimental research is structured around (1) state-of-the-art density functional theory calculations, (2) machine learning tools for accelerating complex reaction network investigation, (3) microkinetic reactor modeling under various experimental reaction conditions, (4) vapor phase catalyst evaluation and kinetic isotope effect studies, (5) catalyst evaluations in condensed phases of water, ethanol, 1,4-dioxane, and cyclohexane, and (6) systematic correlation of experimental data with computational models through Bayesian statistical analysis. An iterative research loop is proposed, with experimental observations leading to hypotheses that motivate new computations, while computational models will rationalize experimental findings and guide new investigations. The research program includes undergraduate outreach, and research results will be integrated into undergraduate and graduate electives and the core chemical engineering curriculum at both Syracuse University and the University of South Carolina.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811614","Properties of Approximate Inference for Complex High-Dimensional Models","DMS","STATISTICS","07/01/2018","08/02/2019","Iain Johnstone","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2022","$297,592.00","David Donoho","imj@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","1269","","$0.00","Modern scientific data acquisition often creates datasets with many measurements on a large sample size of individuals.  The structure of interest in the data may be low dimensional or sparse. Examples arise in scientific domains from econometrics to genomics and image and signal processing and beyond.  The project explores approximate methods of statistical inference for such structure in a representative set of contemporary settings: high dimensional estimation, generalized linear mixed models and low rank multivariate models. It aims to develop approximate methods backed by an optimality theory and/or performance guarantees.  The work is expected to provide new theoretical insights into current problems in specific application domains such as Magnetic Resonance Imaging and quantitative genetics.<br/><br/>The project will bring ideas from classical decision theory to compressed sensing and robust linear modeling, rigorously solving nonconvex optimization problems and obtaining reconstruction performance rigorously better than traditional convex optimization methods.  It will exploit decision theoretic ideas in the proposer's previous work to provide new theoretical insights into a pressing practical problem in compressed sensing -- deriving optimal variable-density sampling schedules applicable to Magnetic Resonance Imaging and NMR spectroscopy.  The project will also study theoretically the statistical performance of some methods for deterministic approximate inference popular in machine learning, but for which little attention has been given to properties such as consistency, asymptotic normality and efficiency.  The study will begin with concrete examples in the realm of generalized linear mixed models, from a frequentist perspective, and seek to establish first-of-kind results for asymptotic efficiency of Expectation Propagation.  Finally, the project will study approximate inference for the eigenstructure of highly multivariate models with low dimensional structure.  It will adapt James' classical framework for multivariate analysis to a broad class of multispike models.  Through collaboration with quantitative geneticists, it will develop methods for inference for low dimensional structure in high dimensional genetic covariance matrices.  In both cases, methods from random matrix theory will be essential.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810927","Collaborative Research: Automated observations of phytoplankton communities from open water moorings","OCE","OCEAN TECH & INTERDISC COORDIN","01/15/2018","12/15/2017","Heidi Sosik","MA","Woods Hole Oceanographic Institution","Standard Grant","Kandace Binkley","12/31/2020","$69,815.00","","hsosik@whoi.edu","183 OYSTER POND ROAD","WOODS HOLE","MA","025431041","5082893542","GEO","1680","7916","$0.00","Phytoplankton are a diverse group of microscopic organism living near the ocean surface. They play essential role in marine food webs and global biogeochemical cycles. The Imaging FlowCytobot(IFCB) has been used for a number of years but because of the high power requirements deployment has been limited to these types of nearshore sites with power and internet connection. This EAGER proposal will test deploy the Imaging FlowCytobot on an open ocean mooring.<br/><br/>The ability to determine the composition and temporal dynamics of phytoplankton communities is important but traditional methods for collecting samples is time consuming. The Imaging FlowCytobot (IFCB) was developed at WHOI as a submersible flow cytometer that can resolve particles in the 10-150 micrometer size range, including critical phytoplankton groups such as diatoms, dinoflagellates, and coccolithophores, and can capture up to 30,000 high resolution images per hour. To date IFCB?s have been deployed at coastal fixed structures (piers, tower) with power, internet, and regular maintenance. This EAGER proposal will expand the operational capabilities of IFCB by testing it on an open ocean mooring. If successful, a new generation of autonomous plankton samplers could be deployed in geographically diverse locations throughout the global ocean in ecologically and biogeochemically significant locations far from the coastlines. This project will fund a graduate student from SIO who will be trained to operate and maintain the IFCB, as well as analyze data using machine learning tools and image analysis software."
"1757493","REU Site: Computational Modeling and Simulation in Applied Sciences","DMS","WORKFORCE IN THE MATHEMAT SCI","09/01/2018","07/11/2018","Wandi Ding","TN","Middle Tennessee State University","Standard Grant","James Matthew Douglass","08/31/2021","$241,470.00","Rachel Leander","wding@mtsu.edu","1301 E. Main","Murfreesboro","TN","371320001","6154947848","MPS","7335","9250","$0.00","This interdisciplinary REU Site program will prepare undergraduates for research, employment, and advanced study in the emerging discipline of Computational Science - a discipline that is critical not only for the advancement of science, but for our nation's economic growth and security. With the advent of more powerful computing tools, computational science is often described as the third pillar of science, joining the traditional pillars of theory and experiment. Students will work in small groups and with faculty guidance while employing innovative computational tools to solve problems in applied science.  Students will acquire essential technical and professional skills by participating in faculty-led workshops. One-on-one interactions with faculty mentors, collaboration between students, independent inquiry, and guided training will provide students with valuable skills in programming, computational modeling, simulation, data analysis, visualization, research management, and communication. The research conducted through this REU Site program has the potential to produce high-impact, publication-quality results in the fields of materials science, epidemiology, cellular biology, and molecular dynamics. Targeted recruiting and selection efforts will ensure that under-represented constituencies, including women, minority, first generation college, and veteran students, are well represented in the program.<br/><br/>The objective of this REU Site program is to recruit undergraduate students into the fast growing, and interdisciplinary field of computational science. Faculty will mentor students as they integrate computation and mathematical/statistical theory to solve problems in molecular dynamics, epidemiology, materials science, and cellular biology. Research problems will form and test mechanistic hypotheses about the structure of biological proteins, yield insights into the dynamics and control of diseases that pose a major threat to public health, address computational topics in the design of optical and acoustical metamaterials, and investigate fundamental principles of cellular decision-making. These research projects, together with a series of bi-weekly workshops, will support innovation by training students in a) Python programming, including best practices for readability, documentation, sharing, and structuring of code and b) tools (e.g. MDAnalysis, Comsol, and Pymol) and techniques (e.g., machine learning, sensitivity analysis, and parameter estimation) for model development, data analysis, and visualization.  Students with a strong background in mathematics, computer science, physics, or biology, and an intense interest in applied science and computation are encouraged to apply.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758628","SBIR Phase II:  A Security, Privacy and Governance Policy Enforcement Framework for Big Data","IIP","SBIR Phase II","04/01/2018","02/13/2019","Fahad Shaon","TX","Data Security Technologies LLC","Standard Grant","Peter Atherton","03/31/2021","$759,993.00","","fahad@datasectech.com","PO Box 836088","Richardson","TX","750836088","9727299582","ENG","5373","5373, 8032, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be the creation of a new tool that could prevent the loss of sensitive data stored in big data management systems due to cyber-attacks. Furthermore, the proposed cybersecurity tool can allow organizations to audit their big data usage to prevent data misuse and comply with various privacy regulations. Recent attacks have shown that the leakage/stealing of stored data may result in enormous monetary loss and damage to organizational reputation, and increased identity theft risks for individuals. Furthermore, in the age of big data, protecting the security and privacy of stored data is paramount for maintaining public trust, and getting the full value from the collected data. The company's proposed tool will potentially have significant impact by addressing these important societal needs with respect to big data security and privacy. Based on customer discovery findings, this tool will also address an important customer need found in many different industries and has the potential to have significant commercial impact as more and more companies are adopting big data technologies.<br/><br/>This Small Business Innovation Research Phase II project will commercialize a novel big data privacy, security and governance management tool that provides efficient data sanitization, attribute-based access control, accountability and governance policy enforcement capabilities for protecting sensitive data stored in big data management systems. In addition, the proposed product will provide novel data sensitivity aware intrusion detection capabilities. The Phase II research objectives are: 1) to develop an efficient attribute-based access control framework to prevent unauthorized access to sensitive data; 2) to develop data sanitization capabilities for complying with various regulations; 3) to develop a scalable audit log capture, storage and querying framework for increasing accountability for big data usage; and 4) to develop a data sensitivity aware intrusion detection framework to quickly detect potential attacks against sensitive data. These objectives pose significant research challenges with respect to scaling to big data without impacting the existing workflow of the companies. The company proposes to address these challenges by using novel code injection techniques combined with risk aware audit log generation and data sensitivity aware machine learning based intrusion detection techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750102","CAREER: Data-driven Models of Human Mobility and Resilience for Decision Making","CNS","S&CC: Smart & Connected Commun, CPS-Cyber-Physical Systems","04/01/2018","09/01/2019","Vanessa Frias-Martinez","MD","University of Maryland College Park","Continuing Grant","Sylvia Spengler","03/31/2023","$306,388.00","","vfrias@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","033Y, 7918","1045, 1640, 7918, 9102","$0.00","This project envisions mobile cyber-physical systems (CPS) where people carrying cell phones generate large amounts of location information that is used to sense, compute and monitor human interactions with the physical environment during environmental dislocations.  The main objective will be to identify the types of reactions populations have to a given type of shock, providing decision makers with accurate and informative data-driven representations they can use to create preparedness and response plans. Additionally, the outcomes of this project will allow for the development of tools to assess and improve the effectiveness of different types of preparedness and response policies through feedback loops in the mobile CPS. These feedback loops could show how community behaviors during shocks change when policies are re-defined based on the computations of the CPS, and vice-versa. Previous work by the PI and others has already showed that CPS integrating people and cell phones as sensing platforms can be used to collect location information at large scale and to compute, using data mining and machine learning techniques, human mobility behaviors during shocks. However, most of the results are very limited and ad-hoc, lacking any type of serious applicability from a preparedness and response policy. This project will advance the state of the art by developing accurate methods and effective tools for decision-making during shocks in mobile CPS. From a broader impacts perspective, the proposed research will contribute in two areas: (a) real-world deployments, to promote data-driven policy development, data-driven analyses of human behavior, and the use of feedback loops in mobile CPS for decision-making assessment; and (b) the creation of an educational plan and training opportunities in the areas of data science for social good and mobile CPS for decision making.<br/><br/>The main outcomes of the project will include novel data-driven methods for mobile CPS that will reliably characterize and predict human mobility patterns and resilience during shocks so as to improve preparedness and response policies. The project will make use of cell phone metadata and social media to achieve the following three objectives: (1) to characterize the types of reactions that communities have to different kinds of shocks using real-time data from mobile CPS, which would allow for the development of more adequate preparedness policies to be ready for future events;  (2) to create predictive methods to forecast the impact that shock management policies would have on human mobility behaviors and community resilience during a shock,  using human behavioral information from the CPS feedback loop when different policies are applied (either in real-time or in batch processing); and (3) to evaluate the transferability of the types of reactions and predictive methods across different shocks, spatio-temporal scales and data sources in mobile CPS, which would provide decision makers with the possibility of analyzing behaviors and resilience in communities where cell phone metadata in the CPS is not fully available. From an intellectual merit perspective, the proposed methods will advance the state of the art in data analytics and real-time systems for CPS in the area of Smart and Connected Communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820488","SBIR Phase I:  Early Detection of Anomalies in Large-Scale Gas Networks","IIP","SBIR Phase I","06/15/2018","06/18/2018","Krishna Karambakkam","CA","EigenPatterns Inc.","Standard Grant","Anna Brady-Estevez","02/28/2019","$224,400.00","","krishna@eigenpatterns.com","1527 Ilikai Avenue","San Jose","CA","951181943","7324292804","ENG","5371","5371, 8030","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) project is to dramatically reduce the incidence of natural gas pipeline failures across the country, within the next 3 to 5 years. Every year there are a few hundred ""significant"" pipeline accidents (fatalities or significant property damage) causing massive damage to life and property, dispersing hazardous materials and disrupting gas distribution services. These events result in many hundreds of millions of dollars in repair and recovery, and large fines that can be up to a billion dollars or more. This scalable and economical capability will significantly reduce the likelihood of such failures, without requiring additional infrastructure. Performance has been validated at a large utility company, and the prototype has demonstrated the ability to capture a substantial fraction of previously undetected events with significant advance warning (90 minutes or more). This outcome represents a clear performance improvement over existing systems and is enabled by advanced models customized for the gas-utility domain. The methods developed in this project can be directly applied to improve detection accuracy in other contexts such as power-grid networks, computer cluster management and financial fraud detection.<br/><br/> This SBIR Phase I project proposes to detect anomalies in large-scale gas-utility networks through statistical inference from continuously observed time-series data on pressure, prevailing temperature, and other characteristics of the network. Anomalies within gas-utility networks occur due to a variety of reasons, e.g., sulphur or ice buildup in the pipelines, and corrosion/aging of hardware, and are often preceded by detectable signatures in the time-series of gas-pressure data. A premise of the project is that the early detection of such signatures, leading to advance warning of 90 minutes or more, allows corrective action within the utility network to avoid significant property damage, loss of life, and service disruption. The project proposes new methods for the rapid estimation of short and medium timescale models of gas pressure behavior from voluminous streaming data, along with methods for constructing prediction bands through Monte Carlo and stochastic optimization techniques. Such methods are non-generic and their success relies crucially on exploiting specific structural properties that are unique to network-level gas-pressure time series, along with modern trends in statistical machine learning. The proposed stochastic optimization techniques will probabilistically classify identified anomalies into ``failure type,"" allowing the prioritizing of network level emergency operations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1901588","RAPID: Collaborative Research: Assessing chemical and microbiological contamination in environmental waters in Eastern North Carolina after Hurricane Florence","CBET","EnvE-Environmental Engineering, Special Initiatives","12/01/2018","11/29/2018","Angela Harris","NC","North Carolina State University","Standard Grant","Karl Rockne","05/31/2020","$99,694.00","Ryan Emanuel, Natalie Nelson, Joshua Kearns, Mahmoud Sharara","aharris5@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","ENG","1440, 1642","080Z, 7914","$0.00","Hurricane Florence passed through southeast North Carolina in 2018 resulting in widespread flooding. The counties most affected by flooding house thousands of swine and poultry confined animal feeding operations (CAFOs). These areas are also home to many low-income, African American, Hispanic, and American Indian communities. The objective of this research is to study the spread of contaminants into the environment caused by Hurricane floodwaters. The pollutants assessed include heavy metals from coal ash, emerging chemical contaminants related to industrial and municipal wastewater, and pathogens and nutrients associated with CAFOs. This project will result in improved understanding of the risks posed by extreme flooding. Local communities affected by the flooding will be engaged in this research through the identification of sampling sites and communication of research results. The results will have broader impact through the identification of waste management operations that can be modified to prevent such releases in future flooding events.<br/><br/>The central aim of this work is to assess microbial and chemical contamination of the environment near livestock and industrial operations flooding caused by Hurricane Florence floodwaters. A secondary aim is to understand the relative impacts of different land-usage and livestock waste management practices on contaminant loading. To achieve these aims, this research project has the following objectives: (1) Collect samples to quantify spatiotemporal variability in molecular targets for microbial contamination (i.e., human-, poultry-, and swine-specific fecal microbial source tracking targets and antimicrobial resistance genes), nutrients (N and P), heavy metals, and chemical contaminants of emerging concern; (2) Identify spatial relationships between contamination, flooding extent, land-use (e.g., CAFO densities, industrial sites, urban areas) and manure management practices using satellite imagery and machine learning techniques; (3) Assess persistence of biological and chemical species in the natural environment post-flooding. To achieve these objectives, the research team will sample across 50 sites in multiple coastal plain river basins where intense flooding occurred after Hurricane Florence to map advanced chemical and biological water quality indicators in high spatial resolution.  The team sampled within a week after the hurricane, and then will sample after ~1 month, ~2 months, and ~6 months. Knowledge gained from this study will directly inform improvements to emergency management protocols post-hurricane in landscapes with many contaminant sources. The work will have broader impact through the promulgation of waste management recommendations that account for the increasing likelihood and severity of extreme flooding events expected in the future as our climate changes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822028","Planning IUCRC Oakland University Center for Digital Joining and Repair","IIP","IUCRC-Indust-Univ Coop Res Ctr","09/01/2018","08/28/2018","Sayed Nassar","MI","Oakland University","Standard Grant","Prakash Balan","08/31/2019","$15,000.00","Lianxiang Yang","nassar@oakland.edu","530 Wilson Hall","Rochester","MI","483094401","2483704116","ENG","5761","5761","$0.00","This award supports a planning workshop for a new Industry-University Cooperative Research Center (IUCRC) aimed at improving composite material joining and repair (CJAR). Composite materials are vital to the health and competitiveness of several important economic sectors, including automotive, manufacturing, energy, and aerospace. As composites have different compositions and element concentration/orientation and tolerances that vary widely, current CJAR practices are highly specialized, labor-intensive, and require experienced technicians to complete the work successfully and reliably. Partner institutions Oakland University, Georgia Institute of Technology, and the University of Tennessee-Knoxville will work collaboratively with both the civilian and defense industries, in the automotive, aerospace, and energy sectors of the US economy.  The Oakland University D-CJAR will primarily work with firms in the automotive (GM, Ford, Fiat Chrysler, BMW, General Dynamics Land Systems), manufacturing (John Deere), and civil aircraft engine (Pratt & Whitney) industries, and also with the National Center for Manufacturing Sciences and the U.S. Naval Research Laboratory. Faculty and student teams at the partner universities will work with industry members to develop and disseminate basic and applied precompetitive research on methodologies, technologies, tools, and workforce innovations to facilitate rapid, reliable, and cost-effective composite joining and repair, with an overall goal of reducing costs, cycle time, and variation of CJAR operations within 10 years.<br/><br/>The scope of work for the Oakland University D-CJAR center will include components of automotive, aircraft, infrastructure (wind turbine blades, etc.) and protective equipment that are wholly or partially made with composite materials, with the goal of transforming the current labor-intensive and specialized processes into science-based, automated, digital CJAR processes. We expect advances in several fields and knowledge domains around CJAR, including (1) design and analysis, (2) materials and process engineering, (3) testing and NDE, and (4) data analytics. We will apply diverse advanced digital techniques, including advanced computational modeling, sensing, materials characterization, and machine learning to practical CJAR cases that will also advance the education and workforce preparedness of students working on projects at the partner universities. Development of new materials and processes would facilitate standardization, modeling, and automation of many CJAR tasks and processes, producing cost savings, faster cycle times, and enhanced performance for industry partners across the entire composites supply chain, contributing to the maintenance of U.S. global leadership in this growing sector.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758207","Doctoral Dissertation Research: Remote Sensing of Urban Tree Species and Tree Stress","BCS","Geography and Spatial Sciences","04/01/2018","03/05/2018","Brenden McNeil","WV","West Virginia University Research Corporation","Standard Grant","Jacqueline Vadjunec","09/30/2019","$18,000.00","Fang Fang","Brenden.McNeil@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","SBE","1352","1352, 9150, 9169, 9179","$0.00","This doctoral dissertation research project will develop a robust methodology to map the species and stress symptom level of individual broadleaf deciduous trees in North American cities.  The project will provide new insights regarding ways to integrate information about spectral-temporal variability from remotely sensed images with field inventories of tree species and stress symptom levels.  By furthering the potential to reduce labor-intensive fieldwork and instead use new, high-resolution remote sensing imagery to assess tree species and stress symptom levels, this project can help advance research on the interplay of different factors on the health of urban forests.  The project will help advance the practice of urban forest management, which has significant societal value because healthy urban forests improve urban air quality, enhance ground water maintenance, and moderate urban air temperatures.  Although this project will focus on a case study in Washington, D.C., the development of an accurate, robust, and repeatable methodology for using widely available, state-of-the-art remote sensing data to update forest inventories of tree species and tree stress will have utility in a much broader set of urban and rural settings.  As a Doctoral Dissertation Research Improvement award, this award also will provide support to enable a promising student to establish a strong independent research career.<br/><br/>More than half of the world's population now lives in cities, and the biodiversity and health of trees has been widely recognized as central to the sustainability of urban environments.  The doctoral student conducting this project will develop a robust and accurate method to identify tree species and stress symptom level to improve urban sustainability using optimized suite of remote sensing data and machine-learning algorithms.  She will seek answers to two questions related to phenological variations in the foliage of different kinds of trees at different times of the year:  (1) With the leaf pigment-induced changes in the visible-light spectral bands, will spring leaf emergence and fall senescence phenology periods be most informative to predict tree species?  (2) Will the magnitude of decline in near infrared reflectance spectral bands during late summer be the most predictive component to predict changes in stress symptom level within a certain tree species?  This project will pair large field inventories from the District of Columbia Department of Transportation with a suite of WorldView-3 satellite images to map the tree species and stress symptom levels of more than 7,000 trees along Washington streets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831682","Excellence in Research: Time Domain Astronomy in the Caribbean: Detecting and Characterizing the Fastest Astrophysical Transient Events","AST","HBCU-EiR - HBCU-Excellence in","09/01/2018","08/13/2018","Antonino Cucchiara","VI","University of The Virgin Islands","Standard Grant","Hans Krimm","08/31/2021","$498,125.00","","antonino.cucchiara@uvi.edu","#2 John Brewers Bay","Charlotte Amalie","VI","008026004","3406931202","MPS","070Y","1207, 9150","$0.00","This research project seeks to exploit the unique location of the University of the Virgin Islands Robotic Telescope (VIRT) to monitor and detect short-lived (transient) and explosive astronomical events.  The VIRT is in a ""blind spot"" in the global coverage of astronomical transient events, and its robotic capability allows it to respond within minutes of the identification of such events. This will allow new discoveries as well as rapid monitoring of known transient events from ongoing multi-wavelength observational surveys, allowing astronomers to characterize transient events at multiple wavelengths and understand the mechanisms responsible for them.  By supporting a postdoctoral researcher, this project will strengthen the astronomy research program at the University of the Virgin Islands (UVI).  It will also provide research opportunities for students in the Physics program at UVI. <br/><br/>The research team will implement a real-time data analysis pipeline for the VIRT, create a lightcurve template database, and build a modern machine-learning infrastructure for rapid classification of astronomical transient events.  The project will bolster the research and education programs at UVI by supporting a dedicated postdoctoral researcher and involving undergraduate students in several aspects of the project.  It will thus help realize UVI?s potential in research and help establish it as a major research and education institution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822155","Planning IUCRC Southern Illinois University Carbondale: Center for Networked Embedded, Smart and Trusted Things (NESTT)","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/01/2018","08/04/2018","Spyros Tragoudas","IL","Southern Illinois University at Carbondale","Standard Grant","Behrooz Shirazi","07/31/2019","$14,999.00","","spyros@engr.siu.edu","Ofc. of Sponsored Projects Admin","Carbondale","IL","629014308","6184534540","CSE","5761","5761","$0.00","Southern Illinois University Carbondale plans to work together with Arizona State University, the University of Arizona, the University of Southern California, and the University of Connecticut to form a new NSF Industry University Cooperative Research Center (IUCRC) for Networked, Embedded, Smart and Trusted Things (NESTT). The vision of the NESTT Center is to contribute to the development of an equitable, safe and secure connected world, focusing on creating holistic Internet of Things (IoT) solutions by integrating technology disciplines with expertise in other areas such as law, business, and humanities.<br/><br/>This planning grant's objective is to organize with industry partners and the participating universities to outline a research agenda and explore industry commitment.  At the IUCRC planning meeting, SIUC faculty will present their interdisciplinary research interests, spanning the technology areas of embedded systems, networking, edge and cloud computing, smart grid, machine learning, data analytics, and security. Innovative high performance, trustworthy, and secure IoT solutions for transportation, smart cities, energy, and healthcare will be developed. Researchers in non-engineering disciplines, particularly privacy, trade secret rights, and contractual rights, may engage in the Center's activities.<br/><br/>If the planning meeting is successful, the efforts of SIUC and partner universities in NESTT could help implement IoT solutions that will improve the quality of life and sustainability. Lowering the barriers to the adoption of IoT technologies through holistic multidisciplinary design and innovation in embedded, networked, trusted, and secure things, our efforts will help realize the full potential of the IoT, and deliver significant economic, societal and environmental returns. The planning meeting will explore ways to broaden the participation of underrepresented students. For example, SIUC will engage with the Success in Engineering through Excellence and Diversity program.<br/><br/>The agenda, list of participants, and activities of the NESTT IUCRC planning meeting and workshops will be posted on SIUC's website for the Consortium for Embedded Systems (CES), https://ces.siu.edu. Once NESTT is established, CES will be merged with the NESTT site.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749025","Collaborative Research:  The MegaAttitude Project: Investigating selection and polysemy at the scale of the lexicon","BCS","Linguistics, Cross-Directorate  Activities","09/01/2018","09/07/2019","Benjamin Van Durme","MD","Johns Hopkins University","Continuing grant","Joan Maling","02/28/2022","$123,650.00","Kyle Rawlins","vandurme@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","SBE","1311, 1397","1311, 7298, SMET","$0.00","This project addresses how humans draw complex inferences from the thousands of English predicates that combine with subordinate clauses -- ""think"", ""know"", ""say"", ""tell"", ""remember"", ""forget"", etc. -- when the structural characteristics of the clauses they combine with vary. For example, the sentence ""John forgot that he bought milk"" is similar to the sentence ""John forgot to buy milk""; but from the first sentence, a listener infers that John bought milk, while from the second, a listener infers that he didn't. This inference pattern is only one among many such patterns in English; yet, in spite of this variety, there appears to be substantial regularities across predicates and subordinate clause structures that prior work has only scratched the surface of. Investigating the systematicities in how humans compute these inference patterns sheds light on how the human cognitive system constructs complex meanings from simpler parts and supports the development of intelligent computational systems for comprehending and reasoning about natural language in human-like ways.<br/><br/>The current project approaches this investigation in two parts. First, it develops and deploys multiple scalable, crowd-sourced annotation protocols, based on experimental methodologies from psycholinguistics, in order to collect data about a wide variety of inference patterns triggered by all of the thousands of English predicates that combine with subordinate clauses. Second, it leverages recent advances in multi-task machine learning to build a unified computational model of the relationship between such predicates, the structure of their subordinate clauses, and the inferences that they trigger, which is trained on these data. This model not only helps to reveal systematicities in how humans compute the inference patterns of interest; it can also be straightforwardly incorporated into applied technologies for natural language understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832613","Collaborative Research: Chemomechanical Degradation of Oxide Cathodes in Li-ion Batteries: Synchrotron Analysis, Environmental Measurements, and Data Mining","DMR","CERAMICS","09/01/2018","08/27/2018","Feng Lin","VA","Virginia Polytechnic Institute and State University","Standard Grant","Lynnette Madsen","08/31/2022","$334,220.00","","fenglin@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","MPS","1774","7237, 8396, 8399, 8614","$0.00","NON-TECHNICAL DESCRIPTION: Rechargeable batteries are key to electric automobiles and the integration of renewable energy. Mechanical degradation of the ceramic electrodes is an issue that causes capacity fade of the battery materials. Despite the steady progress in the battery technology, the understanding of mechanical aging and failure mechanisms lags behind, mostly due to the intrinsic complexity of electroceramic chemistry, structural/compositional heterogeneity, and sensitivity on the environment and operation conditions. This project seeks to elucidate the aging mechanisms of ceramic battery materials. The research creates fundamental knowledge on the material science of batteries via a close integration of novel experimental and modeling approaches. On the education front, the multifaceted collaboration between Purdue University, Virginia Tech, and SLAC National Accelerator Laboratory provides unique training opportunities for the students in this project. Efforts will continue to encourage underrepresented graduate and undergraduate students to join the project. Outreach efforts in collaboration with the Women in Engineering Program at Purdue University and Destination Areas at Virginia Tech will continue to promote the participation of women in science and engineering.<br/><br/>TECHNICAL DETAILS: The overarching goal of this project is to understand the defect inception, accumulation, and growth at the interface or within the grains, and the inter-relationship of defect-microstructure-performance of Li-ion batteries using the operando synchrotron X-ray analytical techniques, environmental nanoindentation, and data mining. The research effort includes (i) characterizing the local redox reactions, chemical states of matter, and local composition over multiple length scales using synchrotron X-ray analytical techniques, (ii) determining the intragranular and intergranular defect growth and morphological evolution of the nanostructured electrodes by state-of-the-art X-ray tomography and transmission X-ray microscopy, (iii) identifying the dependence of the structural and mechanical degradation of oxide cathodes on the state of charge, charging protocol, and cycling history using in-house-developed environmental nanoindentation, and (iv) identifying the composition-chemistry-morphology correlation through machine learning and data analysis of the library of experimental output.. The research draws a conceptually radical spectrum of the electro-chemo-mechanics and establishes a scientific basis for developing ceramic oxides with resilient electrochemical and mechanical performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829458","Collaborative Research: RUI: Isolating neural mechanisms of perceptual awareness from post-perceptual processes","BCS","Cognitive Neuroscience","09/01/2018","08/06/2019","Michael Pitts","OR","Reed College","Standard Grant","Kurt Thoroughman","08/31/2021","$235,318.00","","mpitts@reed.edu","3203 S E Woodstock Blvd","Portland","OR","972028138","5037711112","SBE","1699","1699, 8089, 9229, 9251","$0.00","Understanding the neural mechanisms that give rise to perceptual awareness is a long-standing and fundamental endeavor in human cognitive neuroscience. For decades, cognitive neuroscientists have tried distinguishing between neural activity patterns associated with conscious and unconscious processing. Answering this question can shed light on whether the fundamental conscious experience is linked to activity in lower-level sensory systems, higher-level brain systems in frontal cortex that developed later (in both phylogeny and ontogeny), or interactions between the two.  This investigation is also relevant for understanding various disorders of consciousness such as persistent vegetative state or minimally conscious state, in which individuals are non-responsive, and where such findings could be a first step towards construction of computer algorithms that could indicate the presence of conscious states. Finally, with the adoption of machine learning and neural networks, understanding the neural processes that lead to conscious experience in the human brain could be useful in developing new network architectures designed to more closely mimic human intelligence. <br/><br/>The goal of the proposed research is to identify neural correlates of perceptual awareness using a variety of methods such as electroencephalography and functional magnetic resonance imaging. To examine this issue, the researchers will compare conscious versus unconscious processing in the brains of awake human observers. Specifically, they will investigate the differences in neural activity when observers consciously perceive visual items compared to when those same items go unnoticed.  A critical problem in comparing conscious versus unconscious processing is that it is difficult to separate the neural events associated with conscious perception from the neural events associated with task performance such as memory, decision-making, or verbal and manual responses. Therefore, the researchers will use newly developed ""no-report paradigms"" that include conditions in which stimuli are sometimes seen and sometimes unseen, but observers are not required to report their perceptual experience. By comparing differential neural activity for seen versus unseen stimuli when observers report their experience versus when they do not, the researchers will have a unique opportunity to test several leading theories of consciousness and to identify minimally sufficient neural correlates of perceptual awareness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814840","Collaborative Research:  Photometric redshifts via Bayesian functional data analysis","AST","EXTRAGALACTIC ASTRON & COSMOLO","07/01/2018","06/13/2018","Thomas Loredo","NY","Cornell University","Standard Grant","Nigel Sharp","06/30/2021","$537,398.00","David Ruppert","loredo@astro.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1217","1206","$0.00","Many projects mapping the sky require precise estimates of the speed at which galaxies are receding.  A wide-spread technique derives this number, the redshift, from carefully measuring galaxy colors, using an analysis called ""photo-zs"".  Unfortunately, current estimation methods are not precise enough to achieve major survey science goals.  The present team of astronomers and statisticians will improve both the precision and the reliability of photo-zs.  This work will provide key enabling technology for large surveys in progress and in development, which represent a considerable investment in astronomy.  This project will increase the return on that investment.  The research requires innovation in both astronomy and statistics and the development of new statistical methods likely to have significant additional impact.  The research will help to train a diverse population of students and postdocs in advanced statistics via summer schools and other special sessions, and historically, many such trainees have gone on to pursue careers in data science.<br/><br/>Current and forthcoming automated digital sky surveys aim to push further into ""precision cosmology"" territory by meticulously mapping the distribution and properties of hundreds of millions of galaxies, and measuring the details of thousands of supernovae.  This requires accurate and precise estimation of the redshifts of galaxies using broad-band photometric data, by the technique known as photometric redshifts, or photo-zs for short.  Unfortunately, current estimation methods do not enable the most complete science return from these surveys.  The present project unites astronomers and statisticians to improve the precision as well as the reliability of photo-zs, by modeling spectral energy distributions of galaxies, using Bayesian functional data analysis (FDA), an approach that emphasizes predictive modeling and thorough propagation of information and uncertainty across hierarchical, multi-stage discovery chains.  The project will use a modular, hierarchical modeling framework and account for similarity and diversity, with both conventional parameterizations, and new data-driven parameterizations.  Because this framework will produce probabilistic photo-z estimates, with possibly complex uncertainties, the team will also study how optimally to provide such estimates and to use them for cosmological science.  Open-source implementations of their algorithms will be accelerated by graphics processing units where appropriate.  The research will provide valuable inter-disciplinary training to a graduate student, while developing new statistical methods by innovatively combining FDA, machine learning, and high-performance computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747780","Planning IUCRC George Mason University: Center for Hardware and Embedded System Security and Trust (CHEST)","CNS","IUCRC-Indust-Univ Coop Res Ctr","02/01/2018","01/25/2018","Houman Homayoun","VA","George Mason University","Standard Grant","Dmitri Perkins","01/31/2019","$14,994.00","","hhomayoun@ucdavis.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","5761","5761","$0.00","Integrated circuit and embedded hardware devices are critical to most modern military and industrial systems for defense, energy, healthcare, banking, communication, transportation, and other sectors.  These devices are more vulnerable than ever to malicious tampering by untrusted entities, with threats over a broad range of attack vectors.  Malicious functionality can be added to the devices during the design process, fabrication, or assembly, leading to compromised systems, often without detection.  This collaborative effort seeks to further the national defense by developing novel methodologies and technologies that enhance trust and security of these devices and their respective systems.<br/><br/>The proposed Center will enable researchers and practitioners from diverse areas -- integrated circuits, architecture, cryptography, system safety and resilience, etc. -- to position device security as a coherent, collaborative discipline.  It will be a hub for industry-focused research and a repository for related data.  It will foster dialog to promote advances across disciplinary boundaries. The site will lead collaborative efforts in security of modern computer architecture platform deploy in Internet of Things and Cloud environments, as well as application-specific integrated circuit (ASIC) and field-programmable gate array (FPGA) security. It will adopt advanced machine-learning solutions, to understand the behavior of hardware attacks at various level of abstraction, to design effective defense mechanisms at the heart of the architecture and hardware.<br/><br/>The Center will influence the design, protection, and resilience of cyber physical systems from vulnerabilities associated with device security. It will also help with workforce development needed for industry, government, and military.  The Center will disseminate research results and synthesized theories on device security through both professional/scholarly activities and courses taught at the six sites.  The results are expected to bring a significant return on investment, in terms of improving the readiness of industry for emergent conditions and improving resilience to threats.<br/><br/>The collaborative group for the Industry-University Cooperative Research Center for Hardware and Embedded System Security and Trust is composed of George Mason University, Northeastern University, University of Connecticut, University of Texas at Dallas, University of Virginia, and Wright State University.  The collaborators host a website at https://www.vdl.afrl.af.mil/programs/PG_CHEST_IUCRC with meeting materials, program information, publications, etc. The website is made available indefinitely or until the Center transitions to the next phase."
"1829394","Sensory-Biased Working Memory & Attention Networks in the Human Brain","BCS","COGNEURO","09/01/2018","08/21/2018","David Somers","MA","Trustees of Boston University","Standard Grant","Kurt Thoroughman","08/31/2021","$591,061.00","","somers@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","SBE","1699","1699","$0.00","Abstract<br/><br/>Humans experience the world via multiple sensory modalities --  we see, we hear, and we touch. Each sensory modality has unique strengths and weaknesses in its ability to represent the world around us. Our minds are able to flexibly recruit visual or auditory brain structures when their strengths correspond to the task at hand. In addition, our experience at any moment depends both on sensory input and on working memory and attention mechanisms in the brain. These mechanisms have very limited capacities, which in turn limit cognition. The overarching goal of this project is to examine the human brain mechanisms that support attention and working memory in vision, hearing, and touch. The research team will perform functional MRI experiments to study the brain activity of healthy adults while they perform demanding sensory working memory tasks. Preliminary studies by the research group suggest that there are extensive brain networks, extending into the frontal lobes of the cerebral cortex, that are specialized for each sensory modality, as well as a shared network that supports and unifies these three senses. The current research program will examine individual differences in working memory performance and brain network organization. It will also develop advanced computational models that can predict the functional organization of an individual's brain from their unique pattern or 'fingerprint' of brain connectivity. The project will facilitate other research efforts through the dissemination of new models and computational tools, and will recruit and train young scientists, including members of groups that are under-represented in STEM, in cognitive neuroscience research.<br/><br/>This proposal has 4 primary intellectual goals: (1) identify the fine-scale organization of tactile, visual, auditory, and modality-independent attention & working memory (WM) regions within human cerebral cortex; (2) reveal the specificity of coding of WM information across cortical regions for each modality; (3) detail the network organization of attention & WM circuits; and (4) test hypotheses about content-specific WM mechanisms and cross-modality WM coding. Individual subject fMRI analyses permit fine-scale observation of distinct functional regions. Drawing on subject-specific maps of cortical organization, the research group will re-examine the highly debated question of which brain structures support stimulus-specific working memory for each modality. It will investigate the specificity of sensory modality biased regions by examining functional networks in the resting-state within individual participants. The research team will leverage its findings to probe these networks in 1200 subjects from the Human Connectome Project dataset. The research group will also test and validate a machine-learning approach, Connectome Fingerprinting, for predicting the location of modality-specific working memory regions in individual brains from their unique functional connectome. Vision excels in coding spatial information, but codes timing less reliably; conversely, the auditory system performs high-fidelity temporal coding but coarse spatial coding. The research team has observed cross-modal recoding of WM information into cortical structures that prefer the 'appropriate modality' - auditory spatial WM recruits visual-biased regions and visual temporal WM recruits auditory-biased regions. To probe content-specific WM mechanisms, the researchers will examine interactions between sensory modality WM and space/time WM. The research group hypothesizes that visual and spatial WM stores are distinct, but that auditory and timing WM stores are shared. Collectively, these studies will elucidate the human brain networks and mechanisms that support sensory working memory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817272","Understanding Semidefinite Programming Duality Using Elementary Reformulations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2018","07/18/2018","Gabor Pataki","NC","University of North Carolina at Chapel Hill","Standard Grant","Leland Jameson","07/31/2021","$150,000.00","","gabor@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","MPS","1271","9263","$0.00","Semidefinite programs (SDPs) are some of the most versatile, useful, and interesting  optimization problems to emerge in the last few decades. They find uses in engineering, economics, and machine learning, to name just a few areas. In the last few decades thousands of papers have been published on SDPs. However, SDPs are often pathological: they may not attain their optimal values, and/or their optimal value may differ from that of their dual. Such SDPs  often defeat even  the best SDP solvers, which fail or report an incorrect solution.  Can we understand these pathologies using something as simple as row operations inherited from Gaussian elimination? The project aims to answer this question affirmatively and lead to both theoretical and computational advances in semidefinite programming, and more broadly, in convex optimization. The PI will broadly disseminate the results, both in international and domestic conferences, and by training doctoral students. <br/> <br/>A linear system of equations can be pathological in the sense that it may not have a solution. We can understand this pathology by  transforming the system into a standard form that contains an impossible equation. The transformation is based on elementary row operations. The proposed project aims to use the same operations to transform SDPs into a canonical form, from which their pathology  (say positive duality gap) is easy to see. Thus, on the theoretical side  the project will show that elementary row operations - a staple tool in linear algebra - are useful to understand a much more general class of problems, SDPs, and even more broadly, convex optimization problems. On the computational side the project will develop a useful problem library to test SDP solvers, and other conic optimization solvers. Thus, besides developing theory, the project will contribute to the development of solution methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755808","CRII: CIF:  Next-Generation Group Testing for Neighbor Discovery in the IoT via Sparse-Graph Codes","CCF","Comm & Information Foundations","04/01/2018","12/18/2017","Ramtin Pedarsani","CA","University of California-Santa Barbara","Standard Grant","Phillip Regalia","03/31/2020","$174,938.00","","ramtin@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7797","7935, 8228","$0.00","Group testing is a fundamental inference problem that aims to detect a set of defective items from a larger set of items via group tests. Group testing has a variety of applications in different fields including communications, computer science, machine learning, biology, and signal processing. The main challenges in group testing are to design a small number of tests such that the defective items can be reliably recovered, and to efficiently recover the defective items using a low-complexity decoding algorithm. The goal of this project is to address both challenges by developing fast and near-optimal group testing schemes. The application area that the project focuses on is active neighbor discovery in the Internet of Things (IoT). In an IoT setting, there is an abundance of low-energy devices that collect and transmit information. The main challenge in such systems is to enable a massive number of devices to communicate via a scalable and low-complexity random access scheme. This project addresses this challenge by designing large-scale active neighbor discovery protocols based on group testing.<br/><br/>The key idea of this research is to view the group testing problem from a coding-theoretic lens to develop recovery algorithms with near-optimal sample complexity (number of tests) and optimal decoding complexity. The main ingredients of this coding-theoretic approach are to: (i) design the tests based on a sparse-graph code; (ii) develop a fast peeling-based decoder with sublinear computational complexity for detecting the defective items; and (ii) leverage powerful tools from modern coding theory such as density evolution to minimize the sample complexity of the algorithm. As a concrete application, by addressing the fundamental challenge of scale in the theory of group testing, the proposed work aims to develop active user detection schemes for large-scale communication systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808382","Instrument Development: 4-D Super Time Resolved Microscopy (4-D STReM) for Understanding Dynamics in Porous Materials","CHE","Chemical Measurement & Imaging","07/01/2018","07/06/2018","Christy Landes","TX","William Marsh Rice University","Continuing Grant","Lin He","06/30/2021","$466,300.00","","cflandes@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","6880","1108","$0.00","With support from the Chemical Measurement and Imaging Program in the Division of Chemistry, Professor Landes at Rice University is working to understand and optimize processes that occur within porous materials. The goal of the project is to develop a new type of microscope with unprecedented space and time resolution. The Landes group's new microscope allows the study of how rare events impact the efficiency of porous materials that are important for catalysis, separations science, corrosion, and biology. It has been established that it is possible to manipulate light as it interacts with molecules and proteins. For example, Professor Landes has already shown that by shaping light's phase, events faster than the camera frame rate can be imaged. By incorporating new mathematical and physical tools, the current project will result in a new instrument to image and track fast dynamics in porous materials with optimized 3-D space and time resolution. The interdisciplinary nature of this research effort provides participating students with a unique experience at the interface of spectroscopy and materials science, as well as image processing and modern information theory, and continues the strong history of cross-disciplinary activities in science and technology at Rice University. This grant supports Professor Landes to provide training opportunities to high school teachers to incorporate cutting edge science into their course materials, as well as her new effort to create a summer scientific programming course. <br/><br/>Recently, a new microscopy technique called super temporal-resolved microscopy (STREM) was developed. Proof-of-concept measurements showed that STREM can improve the time resolution of traditional wide-field cameras by at least twenty times. This development, if combined with recent advances in 3-D imaging methods and signal processing, represents an opportunity to resolve the multiscale, nonlinear dynamics that drive a range of interfacial materials properties. Thus, the current project's objective is to develop and optimize 4-D STREM, a chemical imaging method for quantifying the nonlinear dynamics and structures in porous materials. It is hypothesized that better 3-D sub-diffraction spatial information, coupled with improved time resolution and signal processing algorithms, reveals heterogeneous mass transport, chemical, and biological mechanisms occurring at porous interfaces. The project will involve innovations in both hardware and software to improve the temporal and 2-D spatial resolution. Additionally, a new algorithm is to be developed to track in 3-D. Finally, the new microscope is to be used to acquire and curate a machine learning library capable of differentiating among common analyte, sample, and instrument conditions. A new instrument optimized for characterizing the multiscalar physics and chemistry that underlie separations in porous media, by improving both spatial and temporal resolution is obtained in this project. Further, the project will result in new algorithms to extract information from large 3-D data sets. In terms of applications, a more detailed description of mass transport in pores and channels is a step towards predictive separations, which are currently optimized empirically, amounting to billions of dollars each year for industry, government, and academic purposes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822517","CRCNS Research Proposal: Stochastic Processes Driving the Ascending Reticular Activating System","DMS","Cross-BIO Activities, MATHEMATICAL BIOLOGY, Leadership-Class Computing","08/15/2018","04/18/2019","Skirmantas Janusonis","CA","University of California-Santa Barbara","Standard Grant","Junping Wang","07/31/2021","$498,832.00","Bangalore Manjunath, Nils-Christian Detering","janusonis@ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","7275, 7334, 7781","7327, 8091","$0.00","All brain circuits, including those that underlie consciousness and perception, are physically embedded in a dense meshwork of microscopic, meandering fibers. These fibers are active and release signaling molecules that profoundly affect the brain. However, individual fibers do not have well-defined destinations and appear to be randomly oriented within brain tissue, frequently changing their direction. This property makes them very different from point-to-point nerve connections. Because of the unpredictability of individual fiber trajectories, brain research to date has focused on overall fiber ""densities."" These densities vary across brain regions and have been associated with normal and altered functions of the brain. This research project aims to reveal how local, random-like decisions by individual fibers lead to specific fiber densities in brain regions. The research also intends to determine whether the behavior of the fibers has remained fundamentally the same in the last 250 million years of vertebrate evolution. This conceptually-novel, interdisciplinary approach brings together recent developments in high-resolution microscopy, automated image analysis (including machine learning), and stochastic processes. The causal models developed in the project are expected to provide essential tools for the prediction of fiber densities from the dynamics of single fibers and will lay a theoretical foundation for fiber density manipulations in fundamental and applied neuroscience. The project's data and analyses will be used in a newly created interdisciplinary laboratory course and will also strengthen an undergraduate training program in bio-image informatics. The project naturally brings together neurobiology, engineering, and mathematics; this and its visual appeal make it well-suited for planned STEM-oriented presentations for K-12 students, as well as for public talks. Since serotonergic fibers present unique computational challenges, the project will contribute to the development of image analysis algorithms. The project will also build a rigorous theoretical foundation for the structure of ""diffuse"" neurotransmission that is affected in several mental disorders, including depression, schizophrenia, and autism spectrum disorder.<br/><br/>Virtually all neural processes in vertebrate brains take place in a dense matrix of fibers that release serotonin, norepinephrine, and other neurotransmitters. This ancient system originates in the brainstem and is known as the ascending reticular activating system (ARAS). Since ARAS fibers do not form well-defined projections and have extremely meandering trajectories, their current descriptions fall outside the scope of connectomics projects and are based on observed fiber ""densities."" This interdisciplinary project seeks to reconstruct the fundamental self-organizing process that builds and supports the ARAS in the brain. In a radical departure from current descriptive approaches, it hypothesizes that the behavior of single serotonergic fibers can be described by a three-dimensional stochastic process, which determines the resultant fiber density (as an emergent phenomenon). Vertebrate brains, spanning some 250 million years of evolution, will be used to elucidate this process. Single ARAS fibers will be visualized with immunohistochemistry (including tissue expansion) and imaged with confocal laser scanning microscopy. Long fiber trajectories in mice will be visualized using tissue-clearing techniques and whole-brain imaging with light-sheet microscopy. An image analysis algorithm will be developed to automatically detect and trace individual fiber trajectories in the 3D-space. The trajectories will be used to build an optimal stochastic model, by taking advantage of advanced computational methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821691","EAGER: Efficient Utilization of FPGAs in HPC Centers and the Cloud:  A Software/Hardware Approach","CNS","Special Projects - CNS, CSR-Computer Systems Research","05/01/2018","06/24/2020","Lina Sawalha","MI","Western Michigan University","Standard Grant","Erik Brunvand","04/30/2021","$215,435.00","","lina.sawalha@wmich.edu","1903 West Michigan Avenue","Kalamazoo","MI","490085200","2693878298","CSE","1714, 7354","7354, 7916, 9251","$0.00","Cloud and high-performance computing (HPC) are essential for advancing science and education.  Field programmable gate arrays (FPGAs) are being integrated into data centers and cloud computing due to their power and performance benefits.  This proposal focuses on filling the gap between the current use of FPGAs and the use of such computing infrastructures for mainstream computing.  Exact solution for hardware/software partitioning is known to be a computationally difficult problem which is often solved based on static analysis without taking program runtime behavior into account.  This work aims to take full advantage of FPGAs in cloud computing via automated partitioning to speed up HPC while minimizing energy consumption.<br/><br/>The project integrates techniques from computer architecture, compilers, machine learning and computer systems to allow for an efficient automated task partitioning and tuning. The cross-layer techniques will exploit CPU-FPGA systems and will be based on hardware-software cooperation with new innovative architectures.  This work could result in solutions for adaptable, scalable, energy-efficient and high-performance use of heterogeneous architectures for cluster and cloud computing.  In addition, the research ideas developed in this project are expected to significantly speed the pace of discovery in many computing domains from cloud computing and cyber physical systems to mobile computing and embedded systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750109","CAREER: Enabling Predictable Performance in Cloud Computing","CNS","CSR-Computer Systems Research","09/01/2018","06/12/2020","Anshul Gandhi","NY","SUNY at Stony Brook","Continuing Grant","Erik Brunvand","08/31/2023","$217,015.00","","anshul@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7354","1045","$0.00","Cloud computing allows tenants, such as Netflix and Expedia to economically rent compute and storage resources from providers. To enable low resource prices, providers consolidate multiple tenants onto a single physical server. However, this sharing of physical resources among tenants often leads to contention, resulting in unpredictable performance. Worse, tenants cannot observe resource contention due to the opaque nature of cloud computing. This project will develop novel performance models to estimate resource contention in opaque cloud deployments. These models will then be leveraged to develop solutions for cloud tenants that mitigate performance variation, thus enabling predictable performance in clouds.<br/><br/>To realize predictable performance, the project will proceed along two integrated fronts. On the theoretical front, the project will develop uncertainty-aware stochastic performance models. These models will then be integrated with control-theoretic and machine learning techniques to infer, at runtime, the unobservable model parameters in a cloud environment. On the systems front, armed with the uncertainty-aware models, the project will develop solutions, including task schedulers and resource managers, that alleviate application performance variation. The solutions will be designed to dynamically detect and diagnose performance interference. All models and solutions will be experimentally evaluated in public and private clouds.<br/><br/>The interdisciplinary nature of the project provides unique opportunities for integrated education and outreach. The primary benefit of the project will be increasing cloud adoption and promoting its broader impact on energy efficiency. To facilitate this goal, the project will develop open-source solutions for platforms such as OpenStack. The project will advance interdisciplinary education by developing performance analysis lectures and modules that will be integrated with existing courses taught in the departments of Computer Science and Applied Mathematics and Statistics, and the College of Business. Outreach activities will focus on creating research opportunities for local area high school students.<br/><br/>All data produced as a result of this project, including models, software solutions, publications, and courseware, will be made publicly available at the project repository: http://www.pace.cs.stonybrook.edu/predictable-clouds.html. The project data will be maintained and made available for at least 10 years, and even longer, if needed. Data will be stored and hosted on local web servers, and will also be replicated on external public web servers, such as those provided by github, which offer long-term durability and reliability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828105","MRI Collaborative: Development of ESPRIT - Emerging systems' performance and energy evaluation instruments and testbench","CNS","Major Research Instrumentation, Special Projects - CNS","10/01/2018","09/08/2018","Krishna Kavi","TX","University of North Texas","Standard Grant","Rita Rodriguez","09/30/2021","$300,000.00","Song Fu, Hui Zhao","krishna.kavi@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","1189, 1714","1189","$0.00","Future computing nodes will most likely rely on heterogeneous processing and memory systems as well as networking technologies. Identifying the most suitable computing system for a given application requires the cumbersome task of evaluating the application's performance on as many alternatives as possible. This project develops ESPRIT (Emerging Systems PeRformance and Energy Evaluation Instrument and Testbench), a computing system capable of evaluating the most suitable system for specific classes of applications. If applications can be classified into groups based on their similarities along a wide range of performance characteristics, it may be possible to determine the system best suited for a specific class of applications. This work will help large-scale computing systems be configured for more efficient operation and lower energy use.<br/><br/>The ESPRIT project consist of state of the art computing nodes; system, memory, and power and energy simulators; benchmarks from different applications; a suite of measuring instruments; models for investigating application behaviors; statistical clustering and other machine learning techniques.The merit of this project resides in the development of instruments to evaluate applications along a number of performance characteristics of behaviors and classifying them into clusters in order to identify the most suitable design for energy efficiencies by varying capacities as well as technology scales. ESPRIT could be used to investigate new design choices, or tune applications for specific designs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839007","EAGER:REAL-D: Smart Decision Making using Data and Advanced Modeling Approaches","CBET","Proc Sys, Reac Eng & Mol Therm, GOALI-Grnt Opp Acad Lia wIndus, Special Initiatives","09/01/2018","08/06/2019","Marianthi Ierapetritou","NJ","Rutgers University New Brunswick","Standard Grant","Raymond Adomaitis","12/31/2020","$255,000.00","Shantenu Jha, Rohit Ramachandran","mgi@udel.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","ENG","1403, 1504, 1642","019Z, 7916, 8037","$0.00","The proposed exploratory research project aims to develop a next-generation autonomous manufacturing process for pharmaceutical production that integrates product and process informatics with knowledge management. The integration of process data, process models, and information management tools will enable adaptive adjustment to the operating conditions to compensate for variability in raw materials and changing product needs. The research team will take advantage of the facilities of the Center for Structured Organic Particulate Systems (C-SOPS) at Rutgers University for proof of principle studies and generation of experimental data for advancing fundamental understanding of each process.<br/><br/>To enable the transition towards more autonomous and de-centralized decisions across the entire manufacturing supply chain, it is imperative to develop an integrated platform to: (a) acquire data regarding process and product operations from the manufacturing facility using data historian platforms; (b) utilize the data to extract further knowledge on process understanding; and (c) use this knowledge to dynamically and adaptively improve process operations. For task (a), the use of a data management system, such as OSI PI, is proposed with the ability to receive data from multiple sources including the control platform as well as the Process Analytical Technology (PAT) data management tool. This platform has the capability to build up recipe hierarchical structure using Event Frame functionality and periodically push the data into a cloud system for permanent enterprise-wide data storage and efficient sharing. For task (b), the use of advanced statistical and machine learning methods is proposed, in combination with data reconciliation methods.  Finally, for task (c), information acquired will be utilized to adapt the model feasible space by building accurate surrogate models and adaptively refine them using the online data acquisition. Although the focus will be on pharmaceutical production processes, the proposed work, if successful, can have significant broader impacts on a variety of industrial processes. Two PhD students will be trained on the development of a cutting-edge framework for autonomous manufacturing processes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828280","MRI: Development of a System for High-Resolution Uninterrupted Capture of Complex Animal Motions","CMMI","Major Research Instrumentation, Dynamics, Control and System D","10/01/2018","02/21/2019","Rolf Mueller","VA","Virginia Polytechnic Institute and State University","Standard Grant","Joanne Culbertson","09/30/2021","$265,666.00","Amos Abbott, Alexander Leonessa, John Socha, Hongxiao Zhu","rolf.mueller@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","ENG","1189, 7569","030E, 034E, 116E, 1189, 9178, 9231, 9251","$0.00","This major research instrumentation award supports the development of an integrated camera array that will be customized to meet the challenges of capturing fast and highly complex animal motions--providing an enabling tool for fundamental research to understand and model the dynamics of motion. The instrument will permit uninterrupted motion tracking for hundreds of points on an animal as it executes even the most complex motions, such as midair somersaults that bats perform in pursuit of prey. The unprecedented detail, quality, and quantity of the data to be generated will provide the basis for new fundamental research into how animals use freedom of movement to attain unmatched levels of performance in maneuverability and energy efficiency. The large volume of quantitative data produced by the instrument will bring data-intensive methods--from non-linear dynamics and machine learning-- to bear on the field of animal motion. A deeper understanding of the principles behind animal motion will be key to the development of next-generation mobile robots that can handle unconstrained, natural environments. These highly dexterous, mobile robots will enhance productivity in applications such as manufacturing, health care, disaster response, precision agriculture, forestry and environmental monitoring and clean-up. This instrument will also enable fundamental research on the motion of man-made structures, such as the complex dynamic motions inherent in flutter in aerodynamic systems. Knowledge gleaned from this instrument will also help veterinarians to diagnose disease and pain from animals' motion patterns. Graduate and undergraduate students will be involved in instrumentation development and the instrumentation will enable interdisciplinary research training opportunities in engineering and biology. <br/><br/><br/><br/>The instrument will combine high spatial and temporal resolution with the ability to view a moving animal from many different angles at the same time. It will consist of 48 high-speed video cameras that can deliver a 1280x1024-pixel image resolution at 1057-Hz frame rate. High-quality illumination will be provided by 8 specialized lights so that no part of a moving animal will ever be hidden from view. All cameras in the array will be synchronized (precision < 10 nanoseconds) and operated automatically to allow for efficient capture of large motion data sets. A recording of 5 seconds, for example, will result in over 250,000 images with 332 Gigabytes of raw data. The project team's automated image processing methods will allow reliable tracking of several hundred landmark points across such large image sets. The instrument and its accompanying suite of software tools will be used for the study of previously unexplained animal motion capabilities such as the highly articulated flight of bats, gliding of snakes, and lizards running on vertical substrates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810958","Estimation of Smooth Functionals of Covariance and Other Parameters of High-Dimensional Models","DMS","STATISTICS","07/01/2018","05/07/2018","Vladimir Koltchinskii","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","06/30/2021","$250,000.00","","vlad@math.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","1269","","$0.00","A crucial problem in statistical inference for complex, high-dimensional data is to develop statistical estimators of parameters represented by high-dimensional vectors or large matrices. Optimal error rates in such estimation problems are often rather slow due to the ``curse of dimensionality"", and it becomes increasingly important to identify low-dimensional structures and features of high-dimensional parameters that could be estimated efficiently with error rates common in classical, ``low-dimensional"" statistics. Such features are often represented by functionals that depend smoothly of unknown parameters and the goal is to take advantage of their smoothness to develop efficient estimation procedures. The problems of this nature often occur in a variety of applications such as signal and image processing, machine learning and data analytics.  The purpose of this project is to study these problems systematically and to develop new approaches to efficient estimation of smooth functionals. The project is in an interdisciplinary area between mathematics, statistics and computer science and it includes a number of activities to facilitate interactions with researchers in these areas and to ensure the impact of proposed research on education. <br/><br/>The main focus of the project is on the development of general methods of estimation of smooth functionals of covariance operators based on high-dimensional or infinite-dimensional observations. It is expected that these methods will be applicable to other important high-dimensional  models such as Gaussian shift models (both in vector and in matrix case); linear regression models (including trace regression and regression models in quantum state tomography); some non-linear models. The methods to be developed include a new approach to bias reduction in smooth functional estimation problems based on iterative application of bootstrap (bootstrap chains) and concentration and normal approximation bounds needed to establish asymptotic efficiency of estimators with reduced bias. The goal is to determine optimal smoothness thresholds for functionals of interest that ensure their efficient estimation, in particular, in a dimension free high-complexity setting, with complexity of the problem characterized by the effective rank of the true covariance. Other directions include the study of efficient estimation of smooth functionals under regularity assumptions on the parameter set and applications of methods of functional estimation to hypotheses testing for high-dimensional parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828149","NRT-HDR: Intersecting computational and data science to address grand challenges in plant biology","DGE","NSF Research Traineeship (NRT), Project & Program Evaluation","09/01/2018","05/24/2019","Shin-Han Shiu","MI","Michigan State University","Standard Grant","John Weishampel","08/31/2023","$2,999,967.00","C. Robin Buell, Erich Grotewold, Karen Cichy, Brian O'Shea","shius@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","EHR","1997, 7261","062Z, 7361, 9179, SMET","$0.00","Plants are indispensable for life on earth, providing food, energy, and oxygen, as well as the basis for many man-made products. A better understanding of plant science will lead to more secure plant resources, which is even more important given the rapidly increasing global population.  Genomics research has significantly advanced our understanding about how plants function, with the application of genomics yielding datasets that could revolutionize plant science and lead to safe, reliable, and sustainable production of food and biofuels. To achieve these outcomes, there is a critical need for scientists with both an understanding of plant biology and computational skills. This National Science Foundation Research Traineeship (NRT) award to Michigan State University will address this demand by training doctoral students who can employ advanced computational and data science approaches to address grand challenges in plant biology. The project anticipates training approximately seventy (70) PhD students, including thirty-eight (38) funded trainees from plant biology and computational data science programs. <br/><br/>Trainees will engage in research and coursework that emphasize tackling ""grand challenge"" questions in plant biology by leveraging computational approaches. Training will go beyond the traditional genomics and bioinformatics approaches in plant biology to include the advanced training in computation and modeling required to handle increasingly heterogeneous, multi-scale data from the molecular to ecosystem levels.  This type of training will allow students to tackle complex questions such as investigating genotype-phenotype relationships across the Plant Tree of Life or machine learning for high-dimensional plant data.  In addition, the traineeship features professional development opportunities, outreach activities, and industry/governmental internships that serve to broaden trainees' career options while also improving their ability to communicate with a wide range of audiences. Upon completion of the training program, trainees will have a core understanding of plant and computational sciences, excel in interdisciplinary biological and computational research, and possess effective communication, leadership, management, teaching, and mentoring skills. Trainees will be co-advised by experts in plant science and computational/data science. To accomplish the training goals, trainees will participate in a program consisting of: (1) curricular and research activities that will create a cohort of trainees with dual expertise in computational sciences and plant biology, (2) a biweekly forum to encourage scientific interactions, (3) a trainee-led annual symposium that engages a wider scientific audience and builds organizational and leadership skills, (4) internship opportunities in industry and government agencies, (5) professional development activities tailored to individual career goals, including entrepreneurship, and (6) public engagement through outreach activities, further bolstering the ability of trainees to communicate with a wide range of audiences. <br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training. The program is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas through comprehensive traineeship models that are innovative, evidence-based, and aligned with changing workforce and research needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836909","ICE-T: RC: Millimeter Wave Communications and Edge Computing for Next Generation Tetherless Mobile Virtual Reality","CNS","Special Projects - CNS","10/01/2018","09/10/2018","Jacob Chakareski","AL","University of Alabama Tuscaloosa","Standard Grant","Alexander Sprintson","08/31/2020","$299,883.00","","jacobcha@njit.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","1714","9150","$0.00","Virtual and augmented reality (VR/AR) technologies hold tremendous potential to advance our society, having impact on quality of life, environmental and energy conservation, and the world economy. However, two main challenges stand in the way of realizing this vision. These applications are hyper-data-intensive and require ultra-low latency, neither of which can be met by current and upcoming conventional networking methods and systems. These presently limit VR/AR applications to an offline operation, synthetic content, high-end wired equipment, and gaming/entertainment settings. This project envision a novel system at the intersection of millimeter-wave communication (mmWave) and edge computing that aims to overcome these challenges to bring us closer to the next generation tetherless VR/AR societal applications. The project will make notable contributions to the emerging area of networked VR/AR application systems and communications, leading to advances in numerous socially relevant applications, e.g., search and rescue, and disaster response. It will also facilitate fundamental research in the general application area of high-volume high-speed/low-latency data transfer in emerging settings. Beyond the direct scientific and technology impacts and their broader effects on society, educational, outreach, international collaboration, and scientific leadership activities will be pursued as an integral part of the project.<br/><br/>Overcoming the broad performance gap between present and upcoming networked systems capabilities and anticipated requirements of next generation applications will require novel holistic approaches to capture, coding, networking, and reconstruction/navigation of VR/AR data. Towards this objective, the project will investigate a futuristic 5G heterogeneous cellular network system that integrates radio frequency (RF) and millimeter wave communication, and viewport-adaptive space-time scalable VR signal tiling, for multi-path streaming of 360-degree tetherless mobile VR applications. In this setting, the project will pursue the following synergistic investigations: (1) Navigation-aware scalable VR signal tiling to enable interactive streaming of only the data truly needed by the user during navigation; (2) Deep machine learning for user navigation prediction to assist the envisioned resource allocation methods. (3) Space-time scalable rateless code construction for effective source-channel VR signal representation to protect against prospective transmission errors. (4) Dynamic rate-distortion optimized strategies for hybrid RF-mmWave multi-path VR streaming and analysis of the foundations of the interdependencies between the VR signal tiling design and the characteristics of the two network paths. (5) Analysis of the fundamental trade-offs between edge computing and communication that arise here and pursuit of optimization methods that will leverage them to maximize the system efficiency. (6) Graph-theoretic analysis of the problem of dynamic mmWave transmitter to VR user assignment. (7) Network slicing for parallel operation with other applications. Extensive integration and experimentation will be carried out to assess, validate, and prototype the enabled research advances in practical settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815653","EFFICIENT TESTING AND POST-MANUFACTURE TUNING OF BEAMFORMING MIMO WIRELESS COMMUNICATION SYSTEMS: ALGORITHMS AND INFRASTRUCTURE","ECCS","CCSS-Comms Circuits & Sens Sys","08/15/2018","08/15/2018","Abhijit Chatterjee","GA","Georgia Tech Research Corporation","Standard Grant","Lawrence Goldberg","07/31/2021","$360,000.00","","chat@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","7564","153E","$0.00","Efficient Parallel Testing and Tuning of Beamforming MIMO Wireless Systems<br/><br/>There has been a revolution in the use of multiple-input multiple-output (MIMO) wireless communication systems over the last decade. It is expected that mobile data traffic will increase by up to 1000X by 2020 as compared to 2010. Future 5G wireless systems (communication data rates > 50Gbps) will deploy massive MIMO systems with large numbers of transmit and receive antennas and novel RF transceiver architectures that admit RF beamforming. Research on 5G massive MIMO systems is moving forward at an electrifying pace. It will be possible to point electromagnetic beams towards moving targets while simultaneously communicating at extremely high speeds and minimizing interference with other users. Downloading a high definition film will be possible in less than a second. However, with the dramatically increasing levels of circuit complexity and higher operating speeds, the underlying electronics will be highly susceptible to manufacturing process variations, electrical degradation and defects. At high data rates of communication, the effects of device non-idealities on RF system performance can be dramatic. Power consumption will be a major issue since a large number of transmitter and receiver chains will be involved. Such massive MIMO systems will need to be tested extensively and tuned for quality prior to sale.  In the extreme, such systems will need to possess built-in self-testing and self-tuning capability to automatically compensate for field wear and tear due to electrical, thermal and mechanical stress.<br/><br/>The first problem to solve is efficient low-cost manufacturing production test of a range of MIMO systems with the capacity to handle 5G systems with 10-100 RF chains. State of the art test methods require that test signals to individual RF chains have frequency separation for the individual RF chain non-linearities to be assessed independent of nonlinearities in other chains. Conversely, given a set of frequencies that can be generated, only a certain maximum number of RF chains can be tested in parallel. A key goal is to design ""frequency-efficient"" tests and back-end response analysis algorithms that do not require such frequency separation allowing large numbers of RF chains to be tested in parallel.  A second key goal is parallel gain and phase tuning of as many RF chains as possible using intelligent testing and response-analysis algorithms. One way to speed up the tuning procedure is to use the parallel testing procedure described above, to perform parallel tuning of the MIMO system well. Such parallel tuning can be supported by machine learning algorithms that predict the best tuning knob configurations for each chain based on specific time and frequency domain response features extracted from parallel testing techniques. To enable testing and tuning, high-speed signals need to be captured and analyzed for signal fidelity. To this end, the use of proposed incoherent undersampling for acquisition of test response signals provides a significant avenue for reducing testing costs by significantly simplifying the hardware required for high-speed device testing and characterization. Overall, the use of the proposed techniques will allow massive MIMO systems to be tested and tuned, post-manufacture and in the field, without the need for complex test instrumentation in 10s of ms test time, significantly reducing test cost while increasing product yield and field reliability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818886","Iteratively Regularized Broyden-Type Algorithms for Nonlinear Inverse Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2018","07/18/2018","Alexandra Smirnova","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Yuliya Gorb","08/31/2021","$100,000.00","Xiaojing Ye","asmirnova@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","MPS","1271","9263","$0.00","The goal of this project is to tackle major computational challenges faced by scientists and engineers in their quest to improve the accuracy and efficiency of numerical algorithms for solving large-scale inverse problems. This is a scenario where direct measurements of the unknown quantities are not feasible, and one needs to identify ""cause from effect"" by using (generally nonlinear) mathematical and statistical models. The resulting problems are notoriously ill-posed (or unstable), in a sense that even small measurement errors in the input data may give rise to a substantial noise propagation in the recovered solution, to the extent that this solution gets entirely destroyed. For this reason, special techniques called ""regularization"" must be combined with high-speed optimization procedures, so that reliable information on the unknown effect could be obtained from the available data. The key areas of application include imaging and sensing technology, machine learning, gravitational sounding, ocean acoustics, and data sciences.<br/><br/>This project aims at the development of iteratively regularized Broyden-type numerical algorithms for solving nonlinear ill-posed inverse problems in either finite or infinite dimensional spaces. A family of new regularization methods will be designed to solve large-scale unstable least squares problems, where the Jacobian of a discretized nonlinear operator is difficult or even impossible to compute. To overcome this obstacle, PIs consider a family of Gauss-Newton and Levenberg-Marquardt algorithms with the Frechet derivative operator recalculated recursively by using Broyden-type single rank updates. To balance accuracy and stability, the pseudo-inverse for the derivative-free Jacobian is regularized in a problem-specific manner at every step of the iteration process. A variety of filters will be investigated, yielding greater flexibility in the use of qualitative and quantitative a priori information available for each particular applied problem. The proposed iteratively regularized methods will be studied in both deterministic and stochastic settings. For stochastic processes, the minimization functionals are evaluated subject to stochastic errors due to inexact computations to lower per-iteration cost, and/or unavoidable environmental noise and fluctuations. In the framework of the proposed research, PIs will conduct comprehensive convergence analysis of the new algorithms, including convergence rates and optimal policies for the selection of regularization parameters and step sizes. In addition to the theoretical investigation, a significant component of this project is to evaluate the proposed algorithms using extensive numerical experiments on real-world nonlinear inverse problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841374","EAGER: Social Dynamics of Organizational Behavior in Temporary Virtual Teams","IIS","HCC-Human-Centered Computing","08/15/2018","08/03/2018","Brian Keegan","CO","University of Colorado at Boulder","Standard Grant","William Bainbridge","07/31/2021","$199,864.00","","brian.keegan@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7367","7367, 7916","$0.00","This research leverages the large-scale, detailed, international, and unobtrusive data logged by online electronic sports (e-sports) to understand the organizational behavior of temporary virtual teams.  Teamwork and collaboration are essential for success in contemporary organizations, and as teams become increasingly distributed, virtual, self-assembled, cross-functional, and temporary, existing frameworks for supporting effective teamwork need to be revised. Multiplayer online e-sports are models for developing new and more effective frameworks for technologically-mediated teamwork: clear and consistent performance metrics, detailed and public behavioral data for quantitative analysis, a large and international user base, and extensive randomization, repeated observations, and matching for making strong causal inferences. The inferences made from detailed behavioral records of high-tempo, naturalistic decision making can be extended to many other settings such as disaster response or breaking news. The findings from this research could provide the empirical basis for identifying under-utilized expertise in noisy social systems, optimizing team assembly algorithms to improve performance, improving decision making in temporary virtual teams, and using online e-sports as diagnostic tools for existing teams. The findings from this project will also inform the design and governance of e-sports that already attract tens of millions of users around the world. <br/><br/>The project pursues two initiatives to understand how to improve the performance of temporary virtual teams. The first initiative examines how team assembly decision-making in high-tempo contexts influences team performance. The second initiative examines how software and database patches disrupt mental models and decision-making. This research will triangulate between (1) existing organizational theories and constructs about team processes, (2) quantitative methods from data mining, machine learning, and econometrics for analyzing unobtrusive observational data about user behavior, and (3) the unique affordances of several popular e-sports to examine the variables and mechanisms that influence team performance in a naturalistic setting.  The results of this research project will (1) provide comparative empirical insights into a rapidly growing cultural and economic phenomenon; (2) develop frameworks and models to increase engagement in sociotechnical systems; and (3) provide generalizable recommendations for improving the performance of temporary virtual teams. This project will create free software libraries, data collections, and supporting tutorials and documentation enabling other researchers to develop and evaluate organizational theories using data drawn from e-sports.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841910","I-Corps: Factor graph computing for data-driven decision-making","IIP","I-Corps","09/15/2018","08/16/2018","Roman Lubynsky","MA","Massachusetts Institute of Technology","Standard Grant","Andre Marshall","02/29/2020","$50,000.00","","rml@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to democratize the power of machine learning and predictive analytics to help non-engineering business personnel realize the full value of their data. Enabling this will drive better organizational decision making at all levels and the creation of additional business value for commercial and governmental organizations of all kinds, and of all sizes. Data-driven decisions using big data analytics holds massive promise, but has largely remained unfulfilled primarily because, in its current form, it is unaffordable to all but the most technologically advanced organizations. Data-driven businesses have 5-6% higher productivity and can potentially add $3 Trillion in value globally, per year. A wide range of sectors, such as manufacturing, retail, finance, healthcare, security, and governmental services will see significant commercial impact including higher productivity, better utilization of resources, and the acceleration of the deployment of new products, services, and technologies. By enabling commercial and governmental entities to more effectively utilize their data and resources, businesses and government agencies will increase their productivity, more effectively utilize their personnel, improve their competitiveness, and eliminate waste resources while increasing the flow of products and services that benefit society at large. <br/><br/>This I-Corps project is based on ground breaking technology meant to realize a scalable, flexible and easy-to-use data-processing infrastructure. At the highest level, building such a platform requires: (a) coming up with the right abstraction or language that accommodates all sorts of computation at scale; (b) implementing the architecture to realize such computation at scale; (c) the ability to go from ""sandboxing"" or ""prototyping"" to a production environment instantly; and (d) the ability to work in heterogenous data environments instantly. We have put forth a novel computational language, called factor graph computing. Such a computation framework is ""Turing complete"". Factor graph computing allows for performing data transformation, predictive modeling, and optimization at scale to enable data-driven decisions. Such a platform eliminates the need for data engineering, provides flexibility to build predictive models instantly, allows for seamless evolution by bringing in new datasets in the mix, and requires minimal support to maintain the infrastructure and allow for generating prescriptive decisions at scale.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828470","MRI: Development of a Wide-Field Infrared Camera for Robotic Surveys of the Dynamic Astronomical Sky","AST","Major Research Instrumentation","10/01/2018","08/16/2018","Robert Simcoe","MA","Massachusetts Institute of Technology","Standard Grant","Zoran Ninkov","09/30/2021","$982,882.00","Mansi Kasliwal, Gabor Furesz","simcoe@space.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1189","1207, 7697","$0.00","The US astronomical community has embarked on a systematic effort to monitor the dynamic sky, leveraging advances in robotic telescope operation, remote-imaging sensors, large-scale computing and machine learning to find, classify, and unveil transient cosmic sources. Numerous surveys already operate at visible wavelengths, but the high cost of conventional sensors has discouraged similar efforts in the infrared (IR), despite strong scientific motivation.  Advances in material growth driven by the defense and surveillance markets have yielded lower-cost IR sensors of the semiconductor InGaAs, with sufficiently low noise to detect faint astronomical sources. The Wide-field Infrared Transient Explorer (WINTER) will mosaic six High Definition format InGaAs detectors to produce the world's widest-area IR camera and conduct a deep robotic survey of the dynamic infrared sky. In addition to its primary science mission, studying how the heaviest elements of the periodic table are synthesized in colliding neutron stars identified via gravitational waves, WINTER will devote observing time to support undergraduate research and laboratory education at the intersection of Big Data and the physical sciences.<br/><br/>WINTER is an infrared camera with a 1.2 x 1.0 degree field of view that will be installed on a dedicated 1-meter robotic telescope at Palomar Observatory. A simple prototype shows that thermo-electrically cooled InGaAs behind an ambient temperature optical train yields broadband images (at 0.9-1.7 microns) limited by shot noise from the sky, rather than sensor noise. This removes the requirement for cryogenic optics and vacuum cryostats, greatly reducing system cost and complexity relative to traditional IR imagers with HgCdTe sensors. During each year of a 5-year baseline survey, WINTER will monitor the entire Northern sky to a depth of J=19.2 (single visit), or J=20 (combined), and release calibrated images and transient alerts to the public. WINTER is sensitive to emission from the r-process enhanced ejecta of kilonovae over the full horizon of Advanced LIGO, and will autonomously interrupt its sky survey to search for the isotropic infrared counterparts of gravitational-wave events.  WINTER's IR capability is uniquely sensitive to emission from both binary neutron star mergers and neutron star-black hole mergers independent of viewing angle, opacity, mass ratio and remnant lifetime.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814894","Optimization and Equilibria with Expectation Functions: Analysis, Inference and Sampling","DMS","PROBABILITY, APPLIED MATHEMATICS","09/01/2018","08/13/2018","Shu Lu","NC","University of North Carolina at Chapel Hill","Standard Grant","Victor Roytburd","08/31/2021","$272,407.00","Amarjit Budhiraja","shulu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","MPS","1263, 1266","","$0.00","Mathematical optimization and equilibrium problems have prominent applications in machine learning, statistics, economy and business, health care, and many branches of science and engineering.  Solving these problems helps to gain knowledge about the nature and structure of complex systems, and to better design and control these systems by making efficient use of scarce resources.  There are numerous parameters in the formulation of each such problem.  In many cases the exact values of some parameters are not available due to the lack of complete information, especially when such parameters describe future events.  An effective way to manage the long-term behaviors of complex systems under such data uncertainty is to introduce probability distributions on the parameters and use expectation functions in the problem formulations.  To numerically solve those problems with expectation functions, certain types of approximations are commonly used.  The investigators study properties of optimization and equilibrium problems defined with expectation functions, relations between these problems and their approximations, and methods to solve them.  Of particular interest is application of these ideas to study the electricity market competition between renewable and nonrenewable energy sources.  Results from this project can be used to evaluate the well-posedness of a given problem, measure the reliability of a solution obtained from a numerical procedure, and solve certain types of these problems.<br/><br/>The investigators analyze the structure and properties of optimization and equilibrium problems defined by expectation functions, develop inference procedures and sampling-based optimization methods, and study an application to the electricity market.  Their first goal is to develop an efficient inference method for the solution to the true optimization or equilibrium problem based on a solution to its sample average approximation (SAA) problem.  They expect this method to work for a general framework that allows the SAA functions to be nonsmooth, the SAA solution to be inexact, and the SAA asymptotic distribution to follow a piecewise normal structure as in the cases of general constrained optimization problems.  They apply this method to predict the out-of-sample performance of the SAA solutions.  Their second goal is to revisit existing importance sampling techniques to efficiently incorporate them into an iterative optimization algorithm for minimizing the probability of a rare event under some design parameters, and study properties of the probability function and compare solutions of the original problem with those of its convex substitutes.  The third goal of the investigators is to study a stochastic equilibrium model of the competition behavior between different types of energy generators in the electricity market and to provide a novel generalization of the classical Nash-Cournot equilibrium when the payoff functions are not concave.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817077","SHF: Small: Collaborative Research: Exploring Nonvolatility of Emerging Memory Technologies for Architecture Design","CCF","Software & Hardware Foundation","08/01/2018","07/16/2018","Jishen Zhao","CA","University of California-San Diego","Standard Grant","Yuanyuan Yang","07/31/2021","$250,861.00","","jzhao@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7923, 7942, 9102","$0.00","In modern computers, by combining the speed of traditional cache technology, the density of traditional main memory technology, and the non-volatility of flash memory, a new class of emerging byte-addressable nonvolatile memories (NVMs) have great potential to be used as the universal memories of the future. Such memory types include technologies such as phase-change memory, spin-transfer-torque magnetoresistive memory, and resistive memory. As these emerging memory technologies mature, it is important for computer architects to understand their pros and cons in a comprehensive manner in order to improve the performance, power, and reliability of future computer systems incorporating these systems which will be used in various application domains. Yet, most of previous research on NVM architecture is focused only on the performance, power, and density benefits and how to overcome challenges, such as write overhead and wearout issues. The non-volatility characteristic of NVM technologies is not fully explored. Therefore, this project examines how to exploit the non-volatility characteristic that distinguishes the emerging NVM technologies from traditional memory technologies, and investigate new memory architecture design with novel applications.<br/><br/>The goal of this project is to advance the memory architecture design of various types of computer systems with a full exploration of the non-volatility characteristic of NVM technologies across architecture, system, and application levels. To this end, the project explores the design space of various types of computer systems, ranging from severs to embedded systems.  In particular, the project identifies and addresses design issues in nonvolatile cache architecture, re-architects main memory structure to leverage the non-volatility characteristic to improve system performance and energy consumption, supports persistent memory systems in various use cases with emerging NVM technologies, and studies near-data-computing techniques applied for these NVM technologies. The successful outcome of this research is expected to provide the design guidelines for enabling both large capacity and fast-bandwidth nonvolatile memory/storage, which are beyond the present state-of-the-art. Consequently, the research will spawn new applications involving the computation on the exascale of data, e.g., data mining, machine learning, visual or auditory sensory data recognition, bio-informatics, etc.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1855919","CAREER: Overcoming limitations to approximating combinatorial optimization problems","CCF","Algorithmic Foundations","04/04/2018","06/24/2019","Alexandra Kolla","CO","University of Colorado at Boulder","Continuing Grant","Tracy Kimbrel","04/30/2021","$296,006.00","","alexkolla@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7796","1045, 7927","$0.00","This proposal seeks to develop a comprehensive understanding of the limitations of approximation algorithms for combinatorial optimization problems. Combinatorial optimization problems are of great importance to various areas such as operations research, machine learning, VLSI design, computational biology and statistical physics. The task of finding algorithms for combinatorial optimization problems arise in countless applications, from billion-dollar operations to everyday computing tasks. Many optimization problems are NP-hard and thus cannot be solved exactly in polynomial time unless P=NP. However, the majority of such problems are key elements to practical applications where often, if the optimal solution is hard to  find, producing an approximate solution su;ffices.<br/><br/>The research proposed will study the limitations of approximation algorithms posed by the Unique Games Conjecture and the limitations posed by the presence of unlikely worst-case instances. The PI aims to design a methodology to partially or fully overcome such limitations by addressing both of those factors. The proposal outlines a challenging plan focusing on research in a broad cross-section of spectral graph theory, convex optimization, multi-commodity flows, and harmonic analysis.   The contributions of the work described in this proposal will have great impact on the theory of approximability as well as real world problems for which efficient, exact algorithms will be provided for semi-random instances and will naturally result in collaborations between researchers across many different  fields such as mathematics, theory of computer science, industrial engineering, operations research and networking."
"1825444","Collaborative Research: Knowledge and Data-driven Design of Mechanical Metamaterials","CMMI","EDSE-Engineering Design and Sy","09/01/2018","08/27/2018","Meredith Silberstein","NY","Cornell University","Standard Grant","Kathryn Jablokow","08/31/2021","$299,075.00","","ms2682@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","ENG","072Y","067E, 068E, 073E, 9102","$0.00","Traditionally, materials have been designed through choices of molecular level composition and structure. With the advent of increasingly sophisticated material-forming techniques like additive manufacturing, structures repeated at microscale are also now being used to realize effective overall properties; these materials are termed ""metamaterials"". The most obvious gain for metamaterials versus traditional fully dense materials is in weight and material consumption, but there are also responses that are simply not achievable with fully dense polymers. In particular, 3D printing of polymers has reached a critical threshold of quality, speed, and size at which it can be used for production rather than just prototyping. The geometry within the repeated structural cell of a metamaterial critically influences the overall properties. Determining the optimal geometry requires a design framework distinct from that used for dense materials. This work will explore innovative ways of combining expert knowledge (i.e., physical laws, models, heuristics) and databases of actual and simulated material behaviors, using advanced machine learning and search algorithms to foster the discovery of metamaterials with desired properties. Progress in the project will promote the new field of data-driven design as well as advance the national health, prosperity, and welfare by facilitating the design of advanced materials with hitherto unknown, yet desirable combination of properties. Beyond this technological impact, this grant will serve to prepare the next generation of students for a new era of design for intelligent materials and structures. Doctoral, undergraduate, high school, and middle school students will be reached through in-lab research experiences and design outreach activities.<br/><br/>The central objective of this work is to create a design method for 3D printable elastomeric metamaterials that leverages both available engineering knowledge and data. The design space of interest will include two distinct geometry classes -- lattice materials and minimum energy surfaces. The methodology in this project will leverage physics-based models, existing knowledge, and data to minimize the resources needed to reach an acceptable design. The intermediate research objectives are to: (1) formulate and validate a comprehensive set of low computational cost mechanics models for lattice and minimum surface energy style metamaterials, together with a set of heuristics for designing such materials; (2) develop data-driven surrogate models and identify sources of and quantify uncertainty in predicted mechanical properties of 3D printed mechanical metamaterials; (3) develop knowledge representations and data fusion strategies to incorporate expert knowledge including physical laws, heuristics, and beliefs into the design of 3D printed metamaterials. In contrast to the current state-of-the-art for metamaterial design, the design framework that is produced by this grant will be well oriented to accommodate large deformation. This will facilitate design of printed metamaterials for properties such as toughness and failure strain.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842577","EAGER: ISN: Advanced Analytics, Intelligence and Processes for Disrupting Operations of Illicit Supply Networks","CMMI","OE Operations Engineering, Special Initiatives","09/01/2018","04/14/2020","Steven Simske","CO","Colorado State University","Standard Grant","Georgia-Ann Klutke","08/31/2021","$387,969.00","Jon Kellar, Grant Crawford","steve.simske@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","ENG","006Y, 1642","078E, 091Z, 116E, 7916, 8024, 9178, 9231, 9251","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) will contribute to the advancement of national prosperity and economic welfare by studying how illicit supply chain networks operate, and in particular when using the processes and practices of legal networks. Identifying the ways in which counterfeiting and other fraudulent parties operate will help redress this multi-billion-dollar drain on the US economy, which results in both loss of income for the affected legitimate parties and loss of tax income for the US government. Counterfeit goods supply chains take advantage of legitimate distribution networks but are discoverable because their transactional signatures are different from those of legal transactions.  This project involves comparing the analytics collected for both physical inventory and cyber activity to determine the patterns of illicit activities that distinguish them from those of lawful trade. These patterns arise from sales figures, labor analytics, and differences from expected reported trade values in time and locations.  By effectively characterizing suspicious digital transactions and better distinguishing between legitimate and illegitimate enterprises, this research will lead to more effective countermeasures in the digital space used by the majority of commercial enterprises.  The project team involve cross-disciplinary expertise in computer science, operations engineering, and forensic materials science, and will provide opportunities for graduate students in this multi-faceted effort.     <br/><br/>This project will provide the multi-tiered cyber-physical processes for statistical-based discovery of illicit activities and their enabling illicit networks. The starting point is determining statistically relevant deviations from the licit supply chains that illicit trade is dependent on for efficiency and for simulating legitimacy. Physical and cyber forensic processes will be investigated and compared: hybrid machine learning and analytics of public information such as web sales and product pricing sites, and privileged information such as projected and realized regional and seasonal sales, allow the analytics to identify the most salient threat surface points in the illicit supply chains. Fundamental understanding of the operations of illicit networks - to explore, expose and exploit their vulnerabilities - involves the collection of evidence to lead back to the source. In the physical space, this project will investigate the use of an imaging-fueled forensic service (IFFS) tied to fast-onramp sources of information (returns centers, secret shopping, online activity) and capable of determining compliance with serialization, copy prevention, and anti-tamper. These physical forensics are used to provide ""independent accounting"" for the on-line analytics focused on sales, pricing, and supply chain analytics.  Based on indications of illicit activity, the  project will evaluate means to steer potentially illicit users into revealing themselves as non-legitimate based on their ability to reproduce the legitimate supply chain.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762520","GOALI/Collaborative Research: Control-Oriented Modeling and Predictive Control of High Efficiency Low-emission Natural Gas Engines","CMMI","Dynamics, Control and System D","09/01/2018","01/30/2020","Mahdi Shahbakhti","MI","Michigan Technological University","Standard Grant","Robert Landers","08/31/2021","$269,976.00","Hoseinali Borhan, Mahdi Shahbakhti, Jeffrey Naber","mahdish@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","ENG","7569","030E, 034E, 1504, 8024","$0.00","About 200 million internal combustion engines (ICEs) are produced in the world every year and used in energy, transport and service sectors. Furthermore, ICEs account for over 22% of the U.S. total energy consumption and produce the largest portion of CO2 greenhouse gas emissions in urban areas. Dual fuel natural gas (NG) engines in advanced low temperature combustion regimes represent the state-of-the-art ICE technology with some of the highest reported fuel conversion efficiencies and 25% lower CO2 emissions compared to conventional engines. However, achieving a robust and high-efficiency performance of these engines on a broad operational range using existing control technologies is not possible due to their highly nonlinear and uncertain dynamic behavior. This research aims at developing fundamental tools for dynamic modeling and control of nonlinear systems and applying them to high-efficiency low-emission advanced ICEs. The project will provide wide-ranging societal benefits through three major impact areas: first, by advancing research in nonlinear control systems, and mixing and reactive flow including combustion systems; second, by providing direct benefits for control of combustion engines, commonly used in power generation, automotive, locomotive, marine, oil and gas drilling, construction, utilities and manufacturing industries; and third, through educational and outreach activities delivered at industry sites, local communities and science fairs. <br/><br/>This project is a collaborative effort between Michigan Technological University, University of Georgia, and the industry partner, Cummins Inc. The project intends to develop a suite of innovative control-oriented modeling and stochastic predictive control design tools to address control challenges for advanced dual fuel natural gas engines, as well as a broad range of other nonlinear and stochastic dynamic systems. The outcomes of this project result in six main components that include: (i) characterizing the dynamics of dual fuel NG engines in advanced combustion regimes, (ii) building the first physics-based control-oriented model for advanced dual fuel NG engines, (iii) developing new analytical tools for deriving models through the powerful fusion of machine learning and classical multivariate methods, (iv) providing solutions to fill the gaps between first-principles models and data-driven methods for estimating an accurate model, (v) bridging the gaps between parameter-varying systems and stochastic controls, and (vi) constructing, testing, and validating the combustion controllers for dual fuel NG engines. The outcomes from these six theoretical, modeling and experimental contributions will be generic dynamic modeling and predictive control design tools for nonlinear and stochastic industrial systems that are demonstrated on engine test-beds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763191","AF: MEDIUM: Collaborative Research: Foundations of Adaptive Data Analysis","CCF","Algorithmic Foundations","03/01/2018","04/25/2020","James Zou","CA","Stanford University","Continuing Grant","Tracy Kimbrel","02/28/2021","$276,000.00","","jamesz@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758536","SBIR Phase II:  A Cloud-Based Development Framework and Tool Suite for Quantum Computing","IIP","SBIR Phase II","04/01/2018","05/13/2019","Randall Correll","CA","QC Ware Corp.","Standard Grant","Peter Atherton","09/30/2020","$1,421,129.00","","randall.correll@qcware.com","550 Hamilton Ave","Palo Alto","CA","943012010","6508660087","ENG","5373","116E, 165E, 169E, 5373, 8032, 8808, 9231, 9251","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will to enable inexpensive access to quantum computing (QC) and to take the complexity out of the programming and application hosting tasks, which currently pose a major barrier to entry for potential users. QC technology is expected to disrupt significant portions of the high-performance computing environment for optimization problems, which has previously been characterized by slow and incremental performance improvements. This project would yield a platform that both increases the efficiency and lowers the cost of analyzing complex optimization problems, which could spur fast-paced innovation in wide areas of the economy that tackle such issues. <br/><br/>This Small Business Innovation Research (SBIR) Phase II project addresses the need for a cloud-based platform for using QC technology. Early-generation quantum computers have been introduced by multiple hardware vendors. Despite advances in performance of QC processors, little effort has been directed toward developing programming environments and applications that can provide simple and inexpensive access to QC capabilities and that can exploit the power that QC systems will have in the near future. This project will develop a suite of front-end and back-end tools that efficiently transform high-level computing problems into formulations for circuit-model QC systems, abstracting away the physical low-level details and domain knowledge currently necessary to build QC applications. The project will further develop a set of applications in optimization, search, and machine learning. The proposed research will explore the best software tools and platform methods for integrating emerging QC capabilities into enterprise and research workflows by streamlining and making affordable the decomposition and formulation of real-world problems into implementations that run on quantum processors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814388","SaTC: CORE: Small: Practical methods for detecting  access permission vulnerabilities caused by sysadmin's configuration errors","CNS","Secure &Trustworthy Cyberspace","09/01/2018","08/24/2018","Yuanyuan Zhou","CA","University of California-San Diego","Standard Grant","Sol Greenspan","08/31/2021","$500,000.00","","yyzhou@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8060","025Z, 7434, 7923, 9102","$0.00","As data center systems become ever so complex, it has been ever so daunting for system administrators to configure various permission correctly without accidentally opening up permissions for unintended users (and also malicious users) and resulting in catastrophic security disasters. Since data centers have been used to store and manage data not only for financial, business, communication, but also our daily life such as emails, photos, even home appliances and automobile data, it has become ever so important to prevent human errors (system administrator errors) in access permission configurations to avoid security attacks and privacy leaks. This project will develop new methods to detect and prevent permission configuration errors.  The project will involve various educational and outreach activities for students, especially women students in computer science; the investigator has been a role model and a mentor for women high school students, undergraduates, graduates and junior faculty.<br/><br/>To address this access-control misconfigurations problem, the project has three main objectives: (i) providing sysadmins with precise, complete information, (ii) detecting suspicious accesses after access permission changes and (iii) eliminating access-control configuration mistakes. These three objectives will be achieved by using a combination of static program analysis, binary instrumentation, profiling, static and quantitative methods, decision tree machine learning, software testing, etc. The proposed research includes the following three synergistic thrusts: (1) Informative Logging for Access Permission-Related Errors.  (2) Intelligent monitoring and detection of suspicious accesses. (3) Holistic Cross-component Access-Control Management. The three thrusts together well cover the important security problem that has never been addressed by prior work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816112","NeTS: Small: Towards mmWave-based Vehicular Communications","CNS","Networking Technology and Syst","10/01/2018","08/23/2018","Ahmed Ibrahim","FL","Florida International University","Standard Grant","Ann Von Lehmen","09/30/2021","$500,000.00","Elias Alwan, Kemal Akkaya","aibrahim@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7363","7923","$0.00","A new wave of industrial revolution is being witnessed, where vehicles evolve from being controlled by human beings to fully autonomous cyber-physical systems. Having connected vehicles that can communicate with each other, as well as with highway infrastructure and management, is vital to the success of such revolution. Therefore, the goal of this proposal is to design vehicular communications and networks that can provide high data-rate, low-latency, and security. Towards fulfilling this goal, the millimeter wave (mmWave) frequency band will be utilized as the fundamental enabler to achieve the required high data rates for vehicular communications. The proposed project will benefit the global society by helping to realize safe and secure self-driving cars. <br/><br/>The main challenges facing the design and validation of the proposed vehicular communication system are spreading across the implementation of mmWave Radio Frequency (RF) front-ends, baseband processing, and the network management. The design of mmWave RF front-ends requires high-gain and low-profile antenna array for inconspicuous integration on the vehicular platforms, RF beamforming techniques, and finally low-cost, hardware-reduced, and power efficient electronics behind the antenna array. At the baseband processing level, the main obstacles include physical layer secrecy, RF impairment mitigation, and enabling simultaneous multi-vehicle to infrastructure communication. Finally at the network level, novel network architectures are needed to enable low-latency and fast local decision making. The proposed research is transformative as it will advance fundamental knowledge in: 1) High-data-rate RF transmission over mmWave through the design and validation of mmWave multiple-antenna transceiver architectures along with RF beamforming schemes that exploit both spatial and spectral diversity; 2) Enhanced physical layer processing through the design and validation of digital beamforming schemes that achieve physical layer secrecy, mitigate RF impairments, and achieve high-data-rate simultaneous multi-vehicle communication; and 3) Edge computing via a low-latency dynamic network management architecture by utilizing the emerging SDN paradigm, which involves intelligent nodes with local decision and machine learning capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821233","Collaborative research: Design and Analysis of Data-Enabled High-Order Accurate Multiscale Schemes and Parallel Simulation Toolkit for Studying Electromagnetohydrodynamic Flow","DMS","CDS&E-MSS","09/01/2018","08/07/2018","Guang Lin","IN","Purdue University","Standard Grant","Christopher Stark","08/31/2019","$50,000.00","","lin491@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","8069","9263","$0.00","Various magnetohydrodynamics (MHD) approximations have served scientists and engineers well for studying problems in astrophysics, space physics and engineering such as tokamaks, plasma propulsion, and plasma instability in engineering devices. Even so, the limitations in this approximation have now become evident. Especially when dealing with dilute plasmas, charge separation cannot be accommodated in MHD. To match the full range of observational data and experiments, it is imperative to provide the plasma physics community with a capability that goes beyond the MHD approximation. The work aims at developing high-order accurate, efficient and easy-to-use numerical methods for simulation-driven discoveries related to multiscale electromagnetohydrodynamic problems on complex geometry. Indeed, the methods developed will actually offer high-order accuracy and extremely robust performance for any conservation law beyond electromagnetics or elasticity applications. Therefore, several other fields of great importance in science and engineering, and indeed of great importance to the NSF mission, will be directly benefited by the methods developed here. Training a new generation of computational scientists capable of conducting interdisciplinary research is one of the central activities of the work. Courses relevant to the research such as numerical partial differential equations, advanced scientific computing, uncertainty quantification and machine learning have been introduced by the investigators and will be renovated by incorporating outcomes from the project into course materials. <br/> <br/>The specific objectives of this project are to develop, analyze and evaluate data-enabled high-order accurate and robust computational modeling tools for simulating multiscale high energy density plasma flows containing both continuum and rarefied regimes in complex geometry. Both new high-order divergence-constraint-preserving central discontinuous Galerkin (DG) scheme on overlapping unstructured grid cells for simulating continuum plasma coupled with Maxwell's equations, and asymptotic preserving central DG scheme for solving Vlasov-Maxwell-Boltzmann (VMB) equations to model the dilute plasma flow will be developed. An innovative data-enabled stochastic concurrent coupling algorithm combining these schemes will be also devised for multiscale simulations. In this coupling algorithm, a novel data-enabled stochastic heterogeneous domain decomposition method to exchange statistical distribution at the interface of continuum and rarefied regimes will be developed. This will be the first attempt to stochastically couple continuum and kinetic plasma flow models. There is no current capability that integrates these unique advances, and the investigators will be the first group to deliver such a forward-looking capability to the plasma physics community. All numerical simulations will be validated by advanced data-enabled uncertainty quantification method developed in this project. A large-scale parallel code with these capabilities will be developed and released to the plasma physics community. It will not only enable the plasma physics community to carry out transformational simulations for new discoveries related to the multiscale electromagnetohydrodynamic physics for the first time but also lower the threshold for new computational scientists to use the new cutting-edge numerical methods for other applications such as nonlinear optics. Simulations will be used to explain new observations such as enhanced electron transport, which are difficult to study experimentally in a harsh plasma environment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833294","I-Corps:  Transformational Nutrient Curation and Management for Hydroponic Systems","IIP","I-Corps","04/01/2018","04/04/2018","John Blaho","NY","CUNY City University of New York","Standard Grant","Andre Marshall","09/30/2019","$50,000.00","","john.blaho@mail.cuny.edu","205 East 42nd Street","New York","NY","100175706","6466648910","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project focuses on the development of a data driven approach combined with automation equipment to optimize plant nutrition in real time.  The project goal is the development of an efficient and practical machine learning system for nutrient curation and management that may revolutionize commercial farming.  Current technologies on the market lack adequate performance specifications and are not cost effective for commercial applications.  As a result of changing climate conditions and economic factors, many farms struggle to maintain profitability due to new operational challenges and risks.  Worldwide, arable outdoor farmland is dramatically being reduced due to changing climate conditions.  The proposed I-Corps project will initially explore the warehouse farm sector that grows produce indoors close to the point where this produce is consumed.  Direct beneficiaries of this project include the US agriculture industry, as well as those responsible for national food security.  Development of technologies to assist new forms of farming are now required to ensure access to fresh and more nutritionally dense produce in the decades ahead. <br/><br/>This I-Corps project is based on the premise that the rational addition of nutrients to hydroponic systems based on real time feedback optimizes plant growth.  The core technology includes an automated nutrient dosing and pH balancing system which collects data and acts on environmental data.  This system consists of a sensor module monitoring hydroponic nutrient reservoir conditions and pump modules, combined with a cloud based data storage and processing component that adjusts chemical conditions as needed.  The driving algorithms utilize analytical techniques that model nutrient factor usage and predicts their effects on cellular gene expression and overall plant health.  Together, the hardware and software components derive a variety of plant growth metrics that will form the basis for a larger framework of environmental controls to dictate optimal plant growth.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831770","SCC: Landslide Risk Management in Remote Communities: Integrating Geoscience, Data Science, and Social Science in Local Context","CNS","S&CC: Smart & Connected Commun, Info Integration & Informatics","10/01/2018","09/12/2018","Robert Lempert","CA","Rand Corporation","Standard Grant","Sylvia Spengler","09/30/2021","$2,100,974.00","Lisa Busch, Ryan Brown, Phebe Vayanos, Joshua Roering","Robert_Lempert@rand.org","1776 MAIN ST","Santa Monica","CA","904013297","3103930411","CSE","033Y, 7364","042Z, 7364","$0.00","Communities worldwide, including many throughout the United States, struggle to predict and manage significant landslide risk. Landslides prove difficult to predict because they are infrequent and their occurrence may depend strongly on the specific soil, rain, and wind conditions in each location. Effective warning proves hard to disseminate because community members have different risk perceptions and tolerances, and even the best scientific predictions of landslide risk are often imprecise. In this project, a team of geo, information, and social science research institutions, the Sitka Sound Science Center, and the Sitka Tribe of Alaska, will design a novel landslide risk warning system for Sitka, Alaska, a small, diverse coastal town of 9,000 pressed against the steep, landslide-prone slopes of the Tongass National Forest. Working with local students and other residents acting as citizen scientists, the project will deploy small, inexpensive, networked moisture sensors on the slopes above Sitka that, when combined with new methods for integrating diverse data streams, will improve landslide prediction. The project will map Sitka's social networks and residents' understanding of risk and will then use this information, along with new influence maximization methods, which identify well-connected 'key influencers' in each social network, to design effective dissemination channels for landslide warning. The project will use decision support tools to facilitate community deliberations and workshops with government officials on the appropriate design of the physical and social components of a warning system that will best balance timely warning with reduced incidence of disruptive false alarms. While focused on Sitka, this project's results should be widely applicable worldwide, especially in other small or remote towns or communities with landslide risk.<br/><br/>This project will advance geoscience, social science, information science, and risk management through innovative incorporation of multiple data streams from sources such as historical records and imagery, hydrologic sensors, and social networks. The project will advance information science by showing how diverse sources of data (of disparate time scales, dimensionalities, and levels of noise) can be integrated to improve decision-making and policy-making in highly uncertain environments. These diverse streams of data will allow us to utilize both existing machine learning methodologies, as well as novel influence maximization models for communicating natural hazard risk. The project will advance geoscience by improving predictive models through direct measurement of landslide triggering conditions and region-specific threshold calibration, and by testing how a vast increase in the number of in-situ sensors affects the design, implementation, and performance of landslide early warning systems. The project will advance social science through an improved understanding of risk perception and communication in social and cultural contexts. It will be among the first to study how network influence maximization can improve community education and natural hazard response. By linking an understanding of social networks and cultural frames of risk perception with a participatory, quantitative decision support system, this project will improve understanding of how data can be used to facilitate a fair, accountable, integrative, and transparent risk management process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839317","TRIPODS+X:RES: Collaborative Research:Privacy-Preserving Genomic Data Analysis","DMS","TRIPODS Transdisciplinary Rese, Secure &Trustworthy Cyberspace","10/01/2018","08/14/2019","Abhradeep Guha Thakurta","CA","University of California-Santa Cruz","Standard Grant","Tracy Kimbrel","09/30/2021","$527,124.00","Dimitris Achlioptas, Russell Corbett-Detig","aguhatha@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","MPS","041Y, 8060","025Z, 047Z, 062Z","$0.00","Much of modern day medicine is driven by genomic data, with the size and complexity of genomic datasets increasing at a rapid pace. Naturally, any use of human genomic data raises grave privacy concerns. This is because the power to query multiple genomic databases with seemingly innocuous questions such as ""Do you contain any genome that has mutation X?"" is enough to determine whether an individual's genome is present in the databases. Such re-identification attacks have raised a germane question: can one implement privacy protection for genomic data so that meaningful data analysis remains possible, but attacks such as these become impossible? The main idea of this project is to achieve this goal by making and exploiting statistical assumptions about the data, such that if the assumptions are false, data analysis will suffer but privacy will not. The project will also generate curricular material for a graduate class at the intersection of data privacy, machine learning, and genomics.<br/><br/>The project considers three major research questions on preserving privacy in the context of genomic data. The notion of privacy used is differential privacy, which provably protects against re-identification attacks, and has found large-scale adoption in both academia and industry. The first research question is the estimation of allele frequencies, and of linkage disequilibrium, while preserving individual privacy. Given a set of human genomes, the objective of allele frequency estimation is to estimate the frequency of the different mutations across various locations in the chromosome. Linkage disequilibrium is the deviation from independence for pairs of alleles. The second question is haplotype sampling. Haplotypes correspond to sets of genetic variations (typically extending over multiple genes), that tend to be inherited together.  In haplotype sampling, the objective is to generate synthetic haplotypes given a data set of human genomes, while respecting biology behind these genetic variations. Finally, the project aims to estimate pathogenic variants of breast cancer genes. Variants of the BRCA 1 and 2 genes are known to be pathogenic for breast cancer. However, a lot of the variants are still not classified as pathogenic / non-pathogenic and are VUSs - Variants of Unknown Significance. The objective is to develop a privacy-preserving system to gather statistics about the VUSs from individually sequenced genes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831669","SCC: Smart Water Crowdsensing: Examining How Innovative Data Analytics and Citizen Science Can Ensure Safe Drinking Water in Rural Versus Suburban Communities","CNS","S&CC: Smart & Connected Commun","10/01/2018","09/09/2018","Dong Wang","IN","University of Notre Dame","Standard Grant","Ralph Wachter","09/30/2021","$1,466,428.00","Na Wei, Diogo Bolster, Danielle Wood, Jennifer Tank","dwang5@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","033Y","042Z","$0.00","Monitoring drinking water contamination is vitally important to inform consumers about water safety, identify source water problems, and facilitate discussion of public health and the environment of our drinking water. The overall goal of this project is to develop a framework for reliable and timely detection of drinking water contamination to build sustainable and connected communities.   It focuses on communities that use private wells for drinking water without the benefit of a central utility to monitor water quality. It engages the community to participate, leveraging advances in data analytics, exploring the technological and social dimensions to answer a public health question: Is the drinking water in the community safe?<br/><br/>This project advances the role of public participatory scientific research, also referred to as citizen science, in data gathering. It develops new inference models using approaches from machine learning and statistics to improve accuracy, reliability, trustworthiness and value of the data, gathered through public participation.  It improves understanding of key socio-demographic factors that influence public participation and data quality in contrasting community types. It demonstrates the potential role of citizen science in eliciting changes in behavior, and how that influences programmatic and regulatory practices, e.g., in this study of groundwater quality for healthy and sustainable communities.  This framework, known as the Smart Water Crowdsensing (SWC) framework, developed by this project for communities in Indiana studying water quality, should serve as an exemplar for communities nationwide seeking community public participation in studying local public health questions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829255","Collaborative Research: US Forest Service Decisions and Innovations","SMA","SciSIP-Sci of Sci Innov Policy","09/01/2018","07/18/2018","Forrest Fleischman","MN","University of Minnesota-Twin Cities","Standard Grant","Mark Fiegener","08/31/2021","$374,239.00","Michael Dockry","ffleisch@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","SBE","7626","7626","$0.00","Effective public decision-making requires an evidence-based approach, and leading scientists frequently call for producing policy-relevant science, yet little is known about how government officials use science in their day-to-day work. This project will fill this gap by conducting a systematic analysis of use of science in thousands of decisions about environmental policy made at multiple levels of the US federal government. We focus on the contents of Environmental Impact Statements (EISs) prepared by the US Forest Service (USFS) under the National Environmental Policy Act. Providing a more detailed picture of how scientific information is used in government decision-making will enable government agencies to improve the processes through which they incorporate scientific information. At the same time, providing details of what types of scientific information are utilized in public decision-making processes will help scientists, universities, and funding agencies prioritize science that is policy-relevant. To forward these goals, the project will publish an index that will assist scientists in comparing the policy impact of different publications and will conduct outreach activities with environmental planners to help them understand the implications of the research.<br/><br/>This project will use web scraping and computational text analysis to collect and analyze the use of science in all USFS EISs produced between 2006 and 2018. These EISs record the scientific basis for at least 2,000 decisions made about public land management in a wide variety of contexts. The project has two objectives: (1) Understand what drives the inclusion of innovative scientific information (data, facts, evidence) in USFS EISs; and (2) Understand what drives the adoption of innovative scientific practices (operating procedures, protocols, norms) by USFS decision-makers preparing EISs. Using computational text analysis and machine learning tools, we will measure various aspects of the scientific content of each document such as citations, key concepts, and analytical practices. We will combine this information with publicly available data about the scientific information in each document, such as place of publication, employer of the scientists producing the document, extent of public participation in the document's preparation, and the government office using the scientific information. This analysis will help in understanding how characteristics of scientific information and characteristics of government offices influence how science is used in public decision-making processes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808962","Examining Priming in State Legislative Elections","SMA","(SPRF-FR) SBE Postdoctoral Res, Political Science","08/01/2018","06/10/2019","Joshua Meyer-Gutbrod","CA","Meyer-Gutbrod           Joshua         L","Standard Grant","Josie S. Welkom","07/31/2020","$138,000.00","Bruce Bimber","","","Goleta","CA","931172709","","SBE","040Y, 1371","7137","$0.00","This award was provided as part of NSF's Social, Behavioral and Economic Sciences Postdoctoral Research Fellowships (SPRF) program and supported by SBE's Political Science program. The goal of the SPRF program is to prepare promising, early career doctoral-level scientists for scientific careers in academia, industry or private sector, and government. SPRF awards involve two years of training under the sponsorship of established scientists and encourage Postdoctoral Fellows to perform independent research. NSF seeks to promote the participation of scientists from all segments of the scientific community, including those from underrepresented groups, in its research programs and activities; the postdoctoral period is considered to be an important level of professional development in attaining this goal. Each Postdoctoral Fellow must address important scientific questions that advance their respective disciplinary fields. Under the sponsorship of Dr. Bruce Bimber at the University of California, Santa Barbara this postdoctoral fellowship award supports an early career scientist investigating the role of implicit and explicit racial priming within American state legislative campaigns. Both conservative and liberal social movements have increasingly integrated race-based identity claims into their agendas. While existing literature has examined the impact of racial rhetoric within national politics, little work has been done to assess the impact on state politics. This project will explore how this growing racial divide has impacted policy conversation within the American States by developing an expansive data set that catalogues state legislative campaign content by issue arena and rhetorical frame. In doing so, this project aims to inform efforts to counteract racial rhetorical frames and improve democratic engagement for all citizens. This dataset will also provide a critical data source for developing projects for future research on state politics, public policy, political campaigns, and race. Finally, by processing campaign statements into issue coverage summaries for state legislative matchups, this project aims to provide accessible information to better inform voters in lower information elections of a candidate's willingness to engage with salient issues.   <br/><br/>In order to fully investigate racial rhetoric, this project will employ web-scraping techniques to preserve state legislative candidate campaign websites from 2014-2018. Websites provide a holistic approach to the analysis of campaign rhetoric as candidates possess extensive control over the content of a website while space and costs issues are at a minimum. A random sample of candidate issue statements and self-reported legislative records will be catalogued and coded by trained coders according to issue arena and content. This will provide a critical training dataset that will be used to classify the remaining statements according to issue arena using machine learning classification algorithms. The final dataset will provide an unprecedented account of state legislative campaign rhetoric by providing: 1) Character-counts that measure dedicated issue space by topic and provide a critical metric for issue salience and priming; 2) A comprehensive list of issue statements relevant to a particular policy arena that will be used to examine statements for implicit and explicit racially driven framing; 3) A complete collection of website images that provide evidence for visual priming. This data will provide evidence for variation in issue selection as a product of candidate qualities and constituent demographics, specifically racial heterogeneity of district constituents and neighboring districts. In addition, specific policy arenas, including welfare/public assistance, criminal justice, and immigration, can be examined for variation in implicit and explicit racial rhetoric across states and districts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751255","CAREER: Towards Automated Security Vulnerability and Patch Management for Power Grid Operations","CNS","Secure &Trustworthy Cyberspace, EPSCoR Co-Funding","05/01/2018","05/04/2018","Qinghua Li","AR","University of Arkansas","Continuing Grant","Phillip Regalia","04/30/2023","$242,030.00","","qinghual@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","8060, 9150","025Z, 1045, 7434, 9150","$0.00","The power grid is a critical infrastructure for national security, the economy, and daily life, and faces many cybersecurity threats. A proof-of-concept attack hit the Ukraine in 2015, and cut off the power supply to hundreds of thousands of people for several hours. In many successful cyber attacks so far, security vulnerabilities in software have played an important role, exposing systems to attackers who aim to compromise and hence control the system. Vulnerabilities may exist in controllers, meters, circuit breakers, and control computers, and as such, electric utilities usually use vulnerability and patch management to monitor the security vulnerabilities of assets, analyze the remediation and mitigation action for each vulnerability (e.g., applying a patch), and deploy the action to secure the vulnerabilities before attackers exploit them. This remains a heavily manual process in the energy sector, where electric utilities spend a tremendous amount of time on such analyses over a large number of vulnerabilities, and increases the time window in which vulnerabilities are known but not mitigated, putting the system into a high risk of being attacked. This problem has received insufficient attention and has defied commercial solutions. This project will address this problem through automating this analysis process. If successful, it will drastically reduce the human resources and time spent by electric utilities on vulnerability and patch management, and increase the security of the nation's energy infrastructure through mitigating vulnerabilities much more quickly. This project will also develop a training course for security operators in electric utilities and a graduate-level course on vulnerability and patch management. Industry workshops and tutorials on security conferences will be developed to disseminate research discoveries. Undergraduate students and underrepresented students will be involved in the research.<br/> <br/>This project aims to develop methodologies for automated vulnerability and patch management in electric utilities that can expedite decision making of how to remedy and mitigate security vulnerabilities. It has several research tasks. (i) Develop an automated solution that can predict whether a vulnerability should be patched immediately or mitigated in other ways. A prediction model will be built using machine learning methods to predict human operators' remediation actions for vulnerabilities, and easy-to-verify rationale will be provided so that security operators can validate the predictions if needed. (ii) Design a quantitative approach for automating mitigation action analysis. This task will devise a data flow model to capture the interactions between mitigation actions and system components and the interactions among system components, and formulate mitigation action selection as an optimization problem, where the goal is to minimize the negative impact of the selected mitigation actions for a given set of vulnerabilities to the power grid. (iii) Develop a prototype of the proposed tools and conduct field tests in electric utility partners. (iv) Develop recommendations for the vulnerability and patch management ecosystem, including vendors, third-party services, regulation authorities, and standardization organizations. The recommendations to the whole ecosystem developed in this project aims for a systematic, multi-party approach for mitigating vulnerabilities, offering the potential to transform vulnerability and patch management practices in the energy sector from manual to automated operations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755781","CRII:RI: Adaptive and Practical Algorithms for Personalization","IIS","Robust Intelligence","05/01/2018","04/13/2018","Haipeng Luo","CA","University of Southern California","Standard Grant","Rebecca Hwa","04/30/2021","$175,000.00","","haipengl@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","7495, 8228","$0.00","Intelligent personalization systems, such as those in news, advertising, search, online shopping, and clinical trials, are playing an increasingly important role in daily lives, bringing to us tremendous convenience as well as increasing the productivity of society. The main challenge in developing algorithmic solutions for these systems lies in the fact that only feedback for the recommended options, but not the other ones, is provided by the users. Many simple heuristics have been used in practice, and there are also some recent advances on more rigorous approaches based on the ""contextual bandit"" model, referring to an analogy with the objective to maximize the sum of rewards earned through a sequence of lever pulls where an encoding of past performance provides context. However, there is still great room for improvement in terms of both practicality and performance guarantees. This project seeks to develop more practical and adaptive contextual bandit algorithms for such systems. The success of this project requires developing new algorithmic techniques as well as mathematical tools from statistics, optimization, machine learning, and their combinations in an innovative way, which advances the theory and practice of the field of online decision making. Education is integrated into the project through curriculum development and student mentoring. Outreach activities include collaborations with other universities as well as with industry, and also organizing related workshops at top conferences. <br/><br/>Specifically, the project aims at designing a family of practical contextual bandit algorithms which not only enjoy some information-theoretic worst-case guarantees but can also achieve much better performance when the problem exhibits some kind of ""easiness"". First, the project systematically studies different kinds of ""easiness"" measurements and develops and analyzes specific algorithms for each of these measurements. Second, the project further considers the question of whether it is possible to have a single algorithm that is optimal against all problem instances, where optimality is in terms of the best performance among a reasonable class of algorithms. Finally, the project implements all the developed algorithms and conducts empirical evaluation on benchmark datasets, with the goal of releasing easy-to-use and publicly available software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839283","TRIPODS+X:RES: Collaborative Research:Privacy-Preserving Genomic Data Analysis","DMS","TRIPODS Transdisciplinary Rese","10/01/2018","09/10/2018","Vishesh Karwa","OH","Ohio State University","Standard Grant","Tracy Kimbrel","10/31/2019","$72,564.00","","vishesh.karwa@temple.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","041Y","047Z, 062Z","$0.00","Much of modern day medicine is driven by genomic data, with the size and complexity of genomic datasets increasing at a rapid pace. Naturally, any use of human genomic data raises grave privacy concerns. This is because the power to query multiple genomic databases with seemingly innocuous questions such as ""Do you contain any genome that has mutation X?"" is enough to determine whether an individual's genome is present in the databases. Such re-identification attacks have raised a germane question: can one implement privacy protection for genomic data so that meaningful data analysis remains possible, but attacks such as these become impossible? The main idea of this project is to achieve this goal by making and exploiting statistical assumptions about the data, such that if the assumptions are false, data analysis will suffer but privacy will not. The project will also generate curricular material for a graduate class at the intersection of data privacy, machine learning, and genomics.<br/><br/>The project considers three major research questions on preserving privacy in the context of genomic data. The notion of privacy used is differential privacy, which provably protects against re-identification attacks, and has found large-scale adoption in both academia and industry. The first research question is the estimation of allele frequencies, and of linkage disequilibrium, while preserving individual privacy. Given a set of human genomes, the objective of allele frequency estimation is to estimate the frequency of the different mutations across various locations in the chromosome. Linkage disequilibrium is the deviation from independence for pairs of alleles. The second question is haplotype sampling. Haplotypes correspond to sets of genetic variations (typically extending over multiple genes), that tend to be inherited together.  In haplotype sampling, the objective is to generate synthetic haplotypes given a data set of human genomes, while respecting biology behind these genetic variations. Finally, the project aims to estimate pathogenic variants of breast cancer genes. Variants of the BRCA 1 and 2 genes are known to be pathogenic for breast cancer. However, a lot of the variants are still not classified as pathogenic / non-pathogenic and are VUSs - Variants of Unknown Significance. The objective is to develop a privacy-preserving system to gather statistics about the VUSs from individually sequenced genes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751487","CAREER: MULTIPHASE FLUID-MATERIAL INTERACTION: CAVITATION MODELING AND DAMAGE ASSESSMENT","CBET","FD-Fluid Dynamics","01/01/2018","06/05/2019","Kevin Wang","VA","Virginia Polytechnic Institute and State University","Standard Grant","Ron Joslin","12/31/2022","$565,833.00","","kevinwgy@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","ENG","1443","1045, 9251, CL10","$0.00","Cavitation describes the formation, growth, and violent collapse of bubbles in a liquid when exposed to rapid pressure variations. If carefully controlled, cavitation can be a unique approach for high-precision material modification and fabrication, as it allows high-intensity energy pulses to be generated safely, released remotely, and focused within a small target region. The goal of this project is to understand how cavitation affects solid materials close to the bubbles; and to use this knowledge to be able to predict how cavitation modifies nearby material. The proposed research will provide new scientific insight for a broad range of engineering and biomedical applications, from fabricating materials to curing diseases. The educational and outreach component of the project will directly impact the education of K-12 schoolchildren in Central and Western Virginia, through collaboration with the Center for the Enhancement of Engineering Diversity at Virginia Tech and the Science Museum of Western Virginia in Roanoke, VA.<br/><br/>Previous research on cavitation has primarily focused on either the fluid part of the problem, without considering the material's response, or the material part, particularly the macroscopic fracture (e.g., pits, cracks, holes) after multiple cycles of bubble collapse. This project will start with developing and experimentally validating a computational fluid dynamics/computational solid dynamics - coupled model, which will enable direct numerical simulation of up to hundreds of bubbles interacting with a broad range of materials, including fluid-induced damage and fracture. Next, the comprehensive characterization of single bubbles, tandem bubbles, and small bubble clusters collapsing near various materials will create a theoretical foundation for clarifying the micro-scale mechanisms underlying cavitation-induced material damage. Further, the direct numerical simulation model will be used to examine simplified bubbly flow models and, in combination with machine learning, design new models with improved predictive capability."
"1822185","Planning IUCRC University of Connecticut: Center for Networked Embedded, Smart and Trusted Things NESTT","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/01/2018","08/04/2018","Georgios Bollas","CT","University of Connecticut","Standard Grant","Behrooz Shirazi","07/31/2019","$14,999.00","","george.bollas@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","5761","5761","$0.00","The University of Connecticut will work together with the Arizona State University, University of Southern California, University of Arizona, and Southern Illinois University to form a new NSF Industry University Cooperative Research Center (IUCRC) for Networked Embedded, Smart and Trusted Things (NESTT). The vision of NESTT is to contribute to the development of an equitable, safe and secure connected world. NESTT will achieve this vision by focusing on creating holistic IoT solutions, integrating technology disciplines with expertise in law, business, and humanities.<br/><br/>The objective of the UConn's site planning project is to hold workshops with industry partners and NESTT partner universities. The goal of these workshops is to outline a research agenda and identify industry support for an IUCRC focused on providing IoT solutions specific to cyber-physical systems, IOT requirements and architecture, cybersecurity, industrial IOT for smart manufacturing, smart buildings, grid modernization, and transportation. These application areas will be supported by UConn's expertise in systems engineering, distributed edge and cloud computing, formal methods, machine learning, data analytics, cyber-physical security, prognostics and diagnostics.<br/><br/>The multidisciplinary research at NESTT aims to integrate societal and technological aspects of IoT as a systems engineering and design problem. New scientific discoveries in NESTT and their integration with industrial practice could enable economic growth, and provide societal and environmental benefits. Access and opportunities to students who are underrepresented in STEM disciplines and relevant technology areas will be a priority. UConn's NESTT will engage with the UConn Multicultural Scholars Program, the McNair Scholars Undergraduate Research Program, liaisons at HBCUs, the National GEM Consortium, the Northeast Alliance for Graduate Education and the Professoriate, and Summer Research Programs for minority students.<br/><br/>The agenda, list of participants and outcomes of the UConn's NESTT IUCRC planning workshops will be posted online at UConn under the auspices of the UTC Institute for Advanced Systems Engineering (http://utc-iase.uconn.edu ) and promoted through its social media venues.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749269","Workshop: Linguistic Computation Electrified","BCS","Linguistics","02/01/2018","02/07/2018","Tamara Swaab","CA","University of California-Davis","Standard Grant","Tyler Kendall","07/31/2019","$47,340.00","John Henderson, Fernanda Ferreira, Matthew Traxler","swaab@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","SBE","1311","1311, 7556","$0.00","Psycholinguistic and linguistic theories have yielded testable hypotheses about language mechanisms, but revealing how these mechanisms are implemented in the biology of the brain remains a central challenge. This project will support a workshop bringing together leading scientists of brain and language to advance the understanding of the brain mechanisms of language from a cross-disciplinary viewpoint, focusing on the novel use of electrical brain recording methods in humans. Understanding language processes from their computational to their neural bases is essential for advancing critical national goals in education, in the medical treatment of language disorders and hearing loss, and in the creation of analytic approaches that will boost U.S. competitiveness in science and engineering, public and private sector research and development, and national security.<br/><br/>The project will connect electrophysiological and computational approaches for understanding human brain language processing. A first aim is to advance the effectiveness of electrophysiological tools for investigating language computations in the brain during comprehension. Language processes unfold rapidly, on a time scale of fractions of a second, and electrical recordings of brain activity are able to track this fast time course. A second aim is to develop frameworks for creating novel applications and approaches, such as the integration of machine learning methods with measures of brain electrical activity to obtain highly selective views of brain processes supporting specific aspects of language understanding. A third aim is to evaluate three main functions in linguistic computation that are being shown to be amenable to study using electrophysiological recordings, namely combinatorial processing, bilingualism, and predictive processing during language comprehension and production. The project will also provide training in leading-edge methods, thus supporting national efforts to develop the work force, and enhance science education and scientific rigor."
"1831322","Dimensions US-BIOTA-Sao Paulo:  Collaborative Proposal:  Traits as predictors of adaptive diversification along the Brazilian Dry Diagonal","DEB","Dimensions of Biodiversity","09/01/2018","08/10/2018","Kelly Zamudio","NY","Cornell University","Standard Grant","Douglas Levey","08/31/2023","$399,999.00","","krz2@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","BIO","7968","7968","$0.00","The Brazilian Dry Diagonal (BDD) is a broad region of diverse and unique habitats sandwiched in between the wetter Amazon basin to the west and the moist tropical and subtropical Atlantic Forest to the east. Although the BDD can appear superficially to be wasteland with little variety, in fact it harbors many unique species, comprise the most endangered set of habitats in Brazil, and is being rapidly being lost to agriculture and other human encroachment. This research will be the first to synthesize information from species' traits, communities, and genetics to understand how the unique traits and communities of organisms living in the BDD evolved. Using a diverse set of approaches to study in detail a variety of animals, plants and fungi that today inhabit this broad swath of dry habitats in central Brazil, the researchers will evaluate: 1) what sorts of traits - such as body size, shape or specific behaviors - allow a group of organisms to succeed in a novel environment? 2) how the novel environment determines the set of organisms - the community - that we see today? 3) how organisms adapt genetically and morphologically to that novel environment? and 4) how novel traits, communities, and genetic changes interact to produce the variety of organisms that are seen in different environments today? Answers to these questions will improve scientists' ability to predict the impacts of a changing world on biodiversity. The project will foster new international research collaborations by engaging a large team of experts from the United States and Brazil and will also provide interdisciplinary research training opportunities for undergraduate students, graduate students and post-doctoral researchers.   <br/><br/>To accomplish the four aims listed above, the project will first use a machine learning approach applied to hundreds of traits and lineages in the BDD to determine which traits predict evolutionary success - the tendency for xeric-adapted species to undergo evolutionary diversification. The project will then employ community phylogenetics approaches to understand how functional trait variation is distributed in the BDD, and will determine the evolutionary patterns and over- or under-dispersion of trait values observed in particular communities inhabiting the BDD today, as well as how traits are filtered across habitat gradients. Finally, the project uses a variety of genomic technologies, including whole genome sequencing and transcriptomics, to understand how organisms adapt to the more xeric habitats of the BDD as compared to close relatives living in adjacent mesic biomes. By linking evolutionary patterns in traits, communities and genes, the project will synthesize the functional, phylogenetic and genetic dimensions of biodiversity of the BDD and present a comprehensive portrait of its origins and evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831182","Dimensions US-BIOTA-Sao Paulo: Collaborative Proposal: Traits as predictors of adaptive diversification along the Brazilian Dry Diagonal","DEB","Dimensions of Biodiversity","09/01/2018","08/10/2018","Xianfa Xie","VA","Virginia State University","Standard Grant","Douglas Levey","08/31/2023","$366,154.00","","xxie@vsu.edu","1 Hayden Drive","Petersburg","VA","238060001","8045246987","BIO","7968","7968","$0.00","The Brazilian Dry Diagonal (BDD) is a broad region of diverse and unique habitats sandwiched in between the wetter Amazon basin to the west and the moist tropical and subtropical Atlantic Forest to the east. Although the BDD can appear superficially to be wasteland with little variety, in fact it harbors many unique species, comprise the most endangered set of habitats in Brazil, and is being rapidly being lost to agriculture and other human encroachment. This research will be the first to synthesize information from species' traits, communities, and genetics to understand how the unique traits and communities of organisms living in the BDD evolved. Using a diverse set of approaches to study in detail a variety of animals, plants and fungi that today inhabit this broad swath of dry habitats in central Brazil, the researchers will evaluate: 1) what sorts of traits - such as body size, shape or specific behaviors - allow a group of organisms to succeed in a novel environment? 2) how the novel environment determines the set of organisms - the community - that we see today? 3) how organisms adapt genetically and morphologically to that novel environment? and 4) how novel traits, communities, and genetic changes interact to produce the variety of organisms that are seen in different environments today? Answers to these questions will improve scientists' ability to predict the impacts of a changing world on biodiversity. The project will foster new international research collaborations by engaging a large team of experts from the United States and Brazil and will also provide interdisciplinary research training opportunities for undergraduate students, graduate students and post-doctoral researchers.   <br/><br/>To accomplish the four aims listed above, the project will first use a machine learning approach applied to hundreds of traits and lineages in the BDD to determine which traits predict evolutionary success - the tendency for xeric-adapted species to undergo evolutionary diversification. The project will then employ community phylogenetics approaches to understand how functional trait variation is distributed in the BDD, and will determine the evolutionary patterns and over- or under-dispersion of trait values observed in particular communities inhabiting the BDD today, as well as how traits are filtered across habitat gradients. Finally, the project uses a variety of genomic technologies, including whole genome sequencing and transcriptomics, to understand how organisms adapt to the more xeric habitats of the BDD as compared to close relatives living in adjacent mesic biomes. By linking evolutionary patterns in traits, communities and genes, the project will synthesize the functional, phylogenetic and genetic dimensions of biodiversity of the BDD and present a comprehensive portrait of its origins and evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829495","Neurobiologically-informed risk assessment: An empirical examination","BCS","Decision, Risk & Mgmt Sci, LSS-Law And Social Sciences, Cognitive Neuroscience","09/01/2018","08/28/2018","Eyal Aharoni","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Kurt Thoroughman","08/31/2021","$538,827.00","Kent Kiehl","eaharoni@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","SBE","1321, 1372, 1699","1321, 1372, 1699","$0.00","The need to anticipate who will reoffend, relapse, or recover is an important responsibility of clinical and legal practitioners and a prerequisite for the provision of effective healthcare and social services to adjudicated individuals with different risk-needs. It is widely accepted that the brain plays an essential role in shaping antisocial behaviors, but the precise nature of these processes is poorly understood. Previous scholarship has raised the possibility that methods of assessing risk in adjudicated individuals could be improved by including measures of brain function along with traditional behavioral and social measures. This project examines whether the inclusion of noninvasive brain measures can enhance the ability to correctly distinguish between those inmates most and least likely to experience antisocial outcomes, such as rearrest. The project also supports the training and professional development of students in cognitive neuroscience. The results of this project are expected to support the development of clinical tools, procedures and treatments for assessing and remediating risk in forensic populations, thereby reducing the associated costs to society. <br/> <br/>The objectives of this project are achieved by conducting the first large, out-of-sample longitudinal test of a neurocognitive model of persistent antisocial behavior. The model is developed in a sample of 600 adult criminal offenders who have undergone task-driven functional magnetic resonance imaging and been assessed for reoffense risk 12 months following release from prison (the training sample). This model is then used to classify a separate sample of 100 criminal offenders who are similarly scanned, released, and followed (the testing sample). The project employs both theory-driven (null hypothesis testing of a neurocognitive model of impulse control) and data-driven (e.g., whole-brain machine learning classification) analytical approaches. This project will advance basic and clinical knowledge of how neurobiological and behavioral risk factors interact to produce antisocial behavior by characterizing their combined and relative utility in assessing risk. This knowledge can, in turn, be used to inform the development of treatment approaches that are more sensitive to individual defendants' unique risk needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832707","Collaborative Research: Chemomechanical Degradation of Oxide Cathodes in Li-ion Batteries: Synchrotron Analysis, Environmental Measurements, and Data Mining","DMR","CERAMICS","09/01/2018","08/27/2018","Kejie Zhao","IN","Purdue University","Standard Grant","Lynnette Madsen","08/31/2022","$300,780.00","","kjzhao@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1774","7237, 8396, 8399, 8614","$0.00","NON-TECHNICAL DESCRIPTION: Rechargeable batteries are key to electric automobiles and the integration of renewable energy. Mechanical degradation of the ceramic electrodes is an issue that causes capacity fade of the battery materials. Despite the steady progress in the battery technology, the understanding of mechanical aging and failure mechanisms lags behind, mostly due to the intrinsic complexity of electroceramic chemistry, structural/compositional heterogeneity, and sensitivity on the environment and operation conditions. This project seeks to elucidate the aging mechanisms of ceramic battery materials. The research creates fundamental knowledge on the material science of batteries via a close integration of novel experimental and modeling approaches. On the education front, the multifaceted collaboration between Purdue University, Virginia Tech, and SLAC National Accelerator Laboratory provides unique training opportunities for the students in this project. Efforts will continue to encourage underrepresented graduate and undergraduate students to join the project. Outreach efforts in collaboration with the Women in Engineering Program at Purdue University and Destination Areas at Virginia Tech will continue to promote the participation of women in science and engineering.<br/><br/>TECHNICAL DETAILS: The overarching goal of this project is to understand the defect inception, accumulation, and growth at the interface or within the grains, and the inter-relationship of defect-microstructure-performance of Li-ion batteries using the operando synchrotron X-ray analytical techniques, environmental nanoindentation, and data mining. The research effort includes (i) characterizing the local redox reactions, chemical states of matter, and local composition over multiple length scales using synchrotron X-ray analytical techniques, (ii) determining the intragranular and intergranular defect growth and morphological evolution of the nanostructured electrodes by state-of-the-art X-ray tomography and transmission X-ray microscopy, (iii) identifying the dependence of the structural and mechanical degradation of oxide cathodes on the state of charge, charging protocol, and cycling history using in-house-developed environmental nanoindentation, and (iv) identifying the composition-chemistry-morphology correlation through machine learning and data analysis of the library of experimental output.. The research draws a conceptually radical spectrum of the electro-chemo-mechanics and establishes a scientific basis for developing ceramic oxides with resilient electrochemical and mechanical performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814370","CDS&E: Reconstruction of universe's initial conditions with galaxies","AST","EXTRAGALACTIC ASTRON & COSMOLO","09/01/2018","08/26/2018","Uros Seljak","CA","University of California-Berkeley","Standard Grant","Bevin Zauderer","08/31/2021","$520,827.00","","useljak@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1217","1206, 8084","$0.00","The universe evolved from a simple state where matter was almost uniformly distributed in space. In the present day the matter is very strongly clustered into galaxies, clusters of galaxies, and even larger structures. This evolution is governed by gravity and by additional processes such as formation of stars in galaxies. There is enormous amount of information about the universe origins, content, and future evolution hidden in the galaxy distribution.  This information is difficult to access in the present-day form because it has been scrambled by gravity and other processes. The goal of this project is to use simulations to reconstruct the initial conditions of our universe. When these are evolved in time with known laws of physics, they give rise to our visible universe. Ultimately this will allow a movie to made of our universe starting from the initial smooth distribution and ending in images of actual galaxies such as the Hubble Deep Field. A major benefit of this method is that information about our universe can be simply extracted from the initial conditions.  More broadly, an aim of this project is to impact other communities where similar problems arise such as machine learning via the methods and tools developed   <br/><br/>The primary goal of this project is to develop and apply a new set of theoretical and computational instruments, including new statistical methods, algorithms, and computational implementations, to optimally reconstruct the initial condition of our universe from the spatial distribution of galaxies. Galaxies are a primary probe of the large scale structure of the universe that are or will be observed by surveys such as the Sloan Digital Sky Survey (SDSS), the Dark Energy Survey (DES), the Large Synoptic Survey Telescope (LSST), the Dark Energy Spectroscopic Instrument (DESI), EUCLID and the Wide Field Infrared Survey Telescope (WFIRST). This project will extend a hierarchical probabilistic generative model developed by the PI's team to the modelling of galaxies. The framework attempts to solve an exact probabilistic model for the initial conditions that is conditioned on the data with a process that combines elements of numerical optimization in high dimensions and analytic marginalization to find the best solution and their covariance matrix. The proposed research will apply this method to galaxy redshift catalogs and their surrounding dark matter information inferred from weak lensing. The method will be developed using realistic simulations of both dark matter and of galaxies populated in the dark matter and hydro simulations, before being applied to real data.  This research will explore best methods to achieve fast convergence in the search for local and global minimum and aims to have an impact more broadly to research areas (e.g. neural networks) outside astronomy in the tools developed for non-convex optimization in very high dimensions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841187","Collaborative Research: Novel Computational and Statistical Approaches to Prediction and Estimation","DMS","CDS&E-MSS","06/01/2018","07/05/2018","Alexander Rakhlin","MA","Massachusetts Institute of Technology","Continuing grant","Christopher Stark","11/30/2018","$48,335.00","","rakhlin@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","8069","7433, 8084, 9263","$0.00","Terabytes of data are collected by companies and individuals every day. These data possess no value unless one can efficiently process them and use them to make decisions. The scale and the streaming nature of data pose both computational and statistical challenges. The objective of this research project is to develop novel approaches to making online, real-time decisions when data are constantly evolving and highly structured. In particular, this project focuses on online prediction problems involving multiple users in dynamic networks. The project also aims to tackle the privacy issues arising in such multi-user scenarios.<br/><br/>In recent years, it was shown that a majority of online machine learning algorithms can be viewed as solutions to approximate dynamic programming (ADP) problems that incorporate one additional datum per step. Along with directly addressing the computational concerns, the ADP framework also provides guaranteed performance on prediction problems. This project is to use and extend the ADP framework to develop prediction algorithms that simultaneously address the issues of computation, robustness, non-stationarity, privacy, and multiplicity of users."
"1819314","SBIR Phase I:  In Situ Mitigation of Dendrite Formation in Lithium Metal Batteries Using Software and Electronics","IIP","SMALL BUSINESS PHASE I","06/15/2018","06/15/2018","Daniel Konopka","CO","Alligant Scientific, LLC","Standard Grant","Anna Brady-Estevez","11/30/2018","$223,052.00","","DanielK@AlligantScientific.com","640 Plaza Dr Ste 120","Highlands Ranch","CO","801292399","7342239010","ENG","5371","5371, 8030","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) project is the creation of a control system that enables the next generation of high capacity lithium metal batteries to replace current lithium ion battery technology. The global lithium-ion battery market is expected to grow to $67.70 billion USD by end of 2022 from $31.17 billion USD in 2016. However, lithium ion batteries cannot store sufficient energy required by the applications contributing most to that growth (i.e., electric vehicles) as demonstrated by the slow adoption within the largest battery powered product markets today.  Lithium metal batteries were conceived decades ago and are capable of storing three times the energy of lithium ion batteries.  Yet inherent chemical instability renders them extremely dangerous to recharge, preventing their use.  This project is the next phase of work to develop a system that monitors and maintains the stability of lithium metal batteries during charging, enabling safe and reliable use by consumers, businesses, and government.  The complete solution will consist of licensable hardware and software which can be tailored to specific battery powered applications, integrating with battery cells or charging systems for consumer electronics, long range electric vehicles, medical devices, and grid storage systems.  <br/><br/>This SBIR Phase I project funds the continued development of a new paradigm in battery healing: maintaining battery electrode health from the outside in. The system uses software and electronics that control surface issues on battery electrodes which otherwise cause permanent loss of capacity and life during normal use.  As an important part of the overall solution being developed, the key technical hurdles addressed by this proposed SBIR project are focused on real-time electrode surface sensing and mapping capabilities and control strategies to suppress dendrites, as well as advanced characterization methods to monitor and share electrode health information with other components to ensure safety, reliability and durability of the overall energy storage system. The R&D plan will include development of live mapping of electrochemically active surfaces, control software to develop an algorithm and feedback system, and machine learning to improve sensing-mapping-control strategies.  The most promising set of solutions will be demonstrated and validated in an operando visualization test cell that allows observation of the formation and suppression of dendrites on lithium metal electrodes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1753171","CAREER: Calibrating Regularization for Enhanced Statistical Inference","DMS","STATISTICS","07/01/2018","07/01/2019","Daniel McDonald","IN","Indiana University","Continuing Grant","Gabor Szekely","06/30/2023","$159,915.00","","dajmcdon@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","MPS","1269","1045","$0.00","Volumes of data that seemed unimaginable only a decade ago are now ubiquitous in scientific research. Investment decisions are based on prices, updated every microsecond, for thousands of securities; atmospheric scientists use multiresolution satellite images to understand climate change; and internet companies exploit massive music collections to infer trends in tastes and preferences. Rigorous analysis of these large datasets requires a balance between computational constraints and statistical performance, and operationalizing such tradeoffs involves a combination of algorithmic and design-based approximations. For example, decreasing the resolution of an image or subsampling high-throughput data enables faster computations but removes potentially valuable information. At the same time, elaborate explanations of the scientific process are credible because they account for real-world complexity, but estimating complex models requires both more data and larger computers. The proposed work investigates the viability of modern statistical and machine learning methodologies for answering applied scientific questions. The project will create new algorithms and open-source software for combining computational approximations with regularization for analyzing large datasets as well as providing theoretical justification for their statistical properties. A more complete picture of the interplay between statistical regularization, computational approximation, and scientific parsimony will enable fundamental scientific advancement. The PI will employ the methodologies developed in this project to facilitate novel science with large datasets in climate science, biology, music analysis, astronomy, economics, and chemistry. Furthermore, the PI will carefully integrate the research aims with educational and outreach objectives to engage elementary and high school music students, introducing them to modern statistics and computer science, as well as integrating underrepresented populations in research.<br/><br/><br/>Computational tractability and statistical efficiency for large datasets necessitate approximation or regularization, either of which heuristically balances fidelity to the data with scientific goals like parsimony, smoothness, sparsity, or interpretability. The PI seeks to elucidate the dual roles of regularization and approximation as tools for better scientific understanding. Current research in computer science has focused on improving algorithms so as to enable computation with minimal approximation. Meanwhile, statisticians have developed regularization techniques in order to take advantage of simple structures - graph topologies, sparse linear models, smooth functions - that, if representative of the truth, will improve inference and prediction. The challenge is to understand the consequences of this coupling for scientific interpretability. The PI seeks to address two important issues (1) how do we select tuning parameters when computations are at a premium? and (2) how does the accuracy and stability of scientific conclusions relate to the approximation/regularization methods used to obtain those conclusions? This project seeks to enable fundamental scientific progress by understanding the connections between computational approximations and statistical regularization, thereby facilitating improved inferences. The PI will fill this gap by deriving practical algorithms with accompanying theoretical justification under more reasonable statistical assumptions. These tools will be tightly coupled with applications in neuroscience, genetics, atmospheric science, and music.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751278","CAREER: User-Based Simulation Methods for Quantifying Sources of Error and Bias in Recommender Systems","IIS","HCC-Human-Centered Computing","08/01/2018","03/24/2020","Michael Ekstrand","ID","Boise State University","Continuing Grant","William Bainbridge","07/31/2023","$206,353.00","","michaelekstrand@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","7367","1045, 7364, 7367, 9150, 9251","$0.00","Systems that recommend products, places, and services are an increasingly common part of everyday life and commerce, making it important to understand how recommendation algorithms affect outcomes for both individual users and larger social groups.  To do this, the project team will develop novel methods of simulating users' behavior based on large-scale historical datasets.  These methods will be used to better understand vulnerabilities that underlying biases in training datasets pose to commonly-used machine learning-based methods for building and testing recommender systems, as well as characterize the effectiveness of common evaluation metrics such as recommendation accuracy and diversity given different models of how people interact with recommender systems in practice.  The team will publicly release its datasets, software, and novel metrics for the benefit of other researchers and developers of recommender systems.  The work also will inform the development of computer science course materials about the social impact of data analytics as well as outreach activities for librarians, who are often in the position of helping information seekers understand the way search engines and other recommender systems affect their ability to get what they need.<br/><br/>The work is organized around two main themes.  The first will quantify and mitigate the popularity bias and misclassified decoy problems in offline recommender evaluation that tend to lead to popular, known recommendations.  To do this, the team will develop simulation-based evaluation models that encode a variety of assumptions about how users select relevant items to buy and rate and use them to quantify the statistical biases these assumptions induce in recommendation quality metrics. They will calibrate these simulations by comparing with existing data sets covering books, research papers, music, and movies.  These models and datasets will help drive the second main project around measuring the impact of feature distributions in training data on recommender algorithm accuracy and diversity, while developing bias-resistant algorithms.  The team will use data resampling techniques along with the simulation models, extended to model system behavior over time, to evaluate how different algorithms mitigate, propagate, or exacerbate underlying distributional biases through their recommendations, and how those biased recommendations in turn affect future user behavior and experience.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830969","EFRI CEE: Epigenomic Regulation Over Multiple Length Scales: Understanding Chromatin Modifications Through Label Free Imaging and Multi-Scale Modeling","EFMA","Genetic Mechanisms, EFRI Research Projects","09/01/2018","08/28/2018","Juan De Pablo","IL","University of Chicago","Standard Grant","Manju Hingorani","08/31/2022","$1,999,920.00","Hao Zhang, Ali Shilatifard","depablo@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","ENG","1112, 7633","7465","$0.00","This project will develop novel computational and imaging technologies for a new level of understanding about chromatin organization. Chromatin is the condensed complex of DNA and proteins in every cell nucleus whose structure and function is a core determinant of gene expression, and as such controls every aspect of the growth, development and health of organisms. The computational work will be used to inform experimental imaging and the imaging will correspondingly inform the computation to develop validated models of chromatin structure at multiple length scales. These models will enable scientists to address key questions about how local changes in DNA at the atomic scale relate to changes in chromatin at the nanometer scale, and ultimately alter gene expression. The new technologies developed will be made publicly available, which will facilitate research across disciplines spanning physics and engineering to biology, and support the National Science Foundation's goal of Understanding the Rules of Life. The project will involve multiple cross-disciplinary teams of graduate students and postdoctoral scholars who will be trained for successful careers in science and engineering. A 6-week summer course is also planned to inspire and train high school students for the STEM workforce.<br/><br/>The project will start by examining the single chromatin fiber problem through exploitation of the ""heat shock response"" that enables cells to survive high temperatures through activation of heat shock genes. This fundamental process can be used to detect changes in chromatin structure associated with changes in gene expression. Through state-of-the-art imaging, relying on spectroscopic intrinsic-contrast photon-localization optical nanoscopy (SICLON), this work seeks to resolve chromatin structure in vivo to as low as 5 nm length scale. To extract the dynamics of chromatin in this region, computer representations of the images will be generated through combined machine learning, evolutionary optimization strategies and finite difference time domain methods. The workflow will iteratively solve chromatin configurations and infer molecular events for any epigenetic response. The project will build upon a recently developed multiscale chromatin model ideally suited for molecular-based interpretation of experimental observations. As a secondary effort, the project will scale up systems for identifying structural effects of heterochromatin domains by investigating the impact of methyltransferase activity on these domains. Again, this aspect of the work will rely on further development of label-free imaging technologies to achieve sub-10 nm resolution, with the overall goal of quantitative and predictive understanding the 3D organization of chromatin in the cell.<br/><br/>This award is co-funded by the Genetic Mechanisms Cluster in the Division of Molecular and Cellular Biosciences in the Biological Sciences Directorate and by the Emerging Frontiers in Research and Innovation Program in the Division of Emerging Frontiers and Multidisciplinary Activities in the Engineering Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1748969","Collaborative Research: The MegaAttitude Project: Investigating selection and polysemy at the scale of the lexicon","BCS","Linguistics, Cross-Directorate  Activities","09/01/2018","09/01/2019","Aaron White","NY","University of Rochester","Continuing Grant","Joan Maling","02/28/2022","$284,280.00","","awhite48@ur.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","SBE","1311, 1397","1311, 7298, 9251, SMET","$0.00","This project addresses how humans draw complex inferences from the thousands of English predicates that combine with subordinate clauses -- ""think"", ""know"", ""say"", ""tell"", ""remember"", ""forget"", etc. -- when the structural characteristics of the clauses they combine with vary. For example, the sentence ""John forgot that he bought milk"" is similar to the sentence ""John forgot to buy milk""; but from the first sentence, a listener infers that John bought milk, while from the second, a listener infers that he didn't. This inference pattern is only one among many such patterns in English; yet, in spite of this variety, there appears to be substantial regularities across predicates and subordinate clause structures that prior work has only scratched the surface of. Investigating the systematicities in how humans compute these inference patterns sheds light on how the human cognitive system constructs complex meanings from simpler parts and supports the development of intelligent computational systems for comprehending and reasoning about natural language in human-like ways.<br/><br/>The current project approaches this investigation in two parts. First, it develops and deploys multiple scalable, crowd-sourced annotation protocols, based on experimental methodologies from psycholinguistics, in order to collect data about a wide variety of inference patterns triggered by all of the thousands of English predicates that combine with subordinate clauses. Second, it leverages recent advances in multi-task machine learning to build a unified computational model of the relationship between such predicates, the structure of their subordinate clauses, and the inferences that they trigger, which is trained on these data. This model not only helps to reveal systematicities in how humans compute the inference patterns of interest; it can also be straightforwardly incorporated into applied technologies for natural language understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814941","Topological Analysis of Pattern-Forming Systems","DMS","APPLIED MATHEMATICS","09/01/2018","08/13/2018","Patrick Shipman","CO","Colorado State University","Standard Grant","Victor Roytburd","08/31/2021","$392,486.00","Richard Bradley","shipman@math.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1266","7237, 9251","$0.00","Diverse phenomena in nature and the laboratory give rise to patterns such as ripples, squares, or hexagons.  Examples range from laboratory models of climate in which a fluid is heated from below to hexagonal arrays of firing neurons in the region of the brain responsible for spatial memory.  Two classes of pattern-forming systems motivate the work of this project.  The first involves nanoscale patterns produced by bombarding a solid surface with a broad ion beam.  This can produce a wide variety of self-organized nanoscale patterns.  The self-assembly of nanoscale patterns that occurs when solids are irradiated is not just fascinating: in the future, ion bombardment may prove to be an important tool in the fabrication of nanostructures.  It is widely believed that the burgeoning field of nanotechnology will lead to advances that will transform fields as disparate as energy, electronics, and medicine.  The second class of patterns involves color changes in chemical systems in which a vapor reacts with a solid or liquid.  For example, a colored pattern may appear in a solution of an important class of plant pigments called anthocyanins as it is exposed to common atmospheric pollutants.  These patterns are a part of the sponsored outreach to elementary and high school students.  In both of these systems, the patterns vary from highly ordered ripples or lattices with a few defects to patterns so dominated by defects that a lattice structure may not be easily recognized.  These defects limit the utility of nanostructures produced by ion bombardment.  They are also indicative of the underlying chemical or physical mechanism by which the patterns form, and therefore help the investigators to understand those mechanisms.  In this work, the investigators develop mathematical tools to understand the formation of defects.  The tools are also applied to help propose experimental methods to eliminate defects in nanopatterns produced by ion bombardment.  The tools also provide insight into the mechanisms driving pattern formation.  The research involves undergraduate and graduate students in integrated theoretical and experimental work.<br/><br/>Defects are often prevalent in patterns produced in nature and the laboratory, so that the patterns are far from ideal ripples or hexagonal lattices.  These defects can be interpreted as data sets that have topological characteristics.  In this project, the investigators apply methods of topological data analysis (TDA) to patterns modeled by nonlinear partial differential equations.  In particular, the investigators and their colleagues develop methods to quantify the order in a pattern using the output of various TDA methods.  Experiments and simulations suggest that long-wavelength deformations (i.e., the zero mode) can play a significant role in the persistence of defects in a developing pattern.  The investigators test this hypothesis using a multidimensional extension of TDA methods.  The stability of defects is probed by deriving equations for the amplitude and phase of the patterns.  Methods of predicting where defects will form as a pattern evolves are developed using TDA.  Finally, combining TDA with machine learning tools, the team determines parameters in models of pattern formation from experimental data.  The methods are applicable to any pattern-forming system.  However, two classes of systems provide focus for this work.  The first is the formation of nanoscale patterns when a solid surface is bombarded by a broad ion beam.  Using TDA, the investigators determine the nature of the instability that leads to the formation of a hexagonal array of nanodots when the surface of a binary material is bombarded, a subject of considerable debate.  The second class is a set of reaction-diffusion systems that we call vaporchromatic experiments.  In these experiments, vapor interacts with a solution containing a polymer or pigment that changes color upon interaction with the vapor.  The team develops mathematical reaction-diffusion-convection models for the formation of vaporchromatic patterns.  The methods of analyzing patterns using TDA are motivated by and tested on patterns produced both by experiments and by numerical simulations of partial differential equation models.  Graduate and REU undergraduate students participate in the research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750155","CAREER: Software Adaptation and Synthesis Techniques for Internet of Things Systems","CNS","CSR-Computer Systems Research","07/01/2018","06/18/2020","Octav Chipara","IA","University of Iowa","Continuing Grant","Erik Brunvand","06/30/2023","$300,077.00","","octav-chipara@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7354","1045","$0.00","Wearable Internet-of-Things (IoT) devices, such as smart watches, smart glasses, and cloud-enabled hearing aids promise to improve our everyday lives. However, developing such systems is challenging. System developers must account for the differences in users and hardware platforms, and anticipate how a system will respond to the changing operating conditions. For example, a hearing aid may use its onboard processor to augment speech in relatively quiet surroundings but rely on more powerful cloud computing resources to enhance speech as users encounters more noisy environments. Today, developers lack effective programming languages and tools to help them create and improve such IoT systems.<br/><br/>This project investigates how adaptive IoT systems may be developed using policy-driven software adaptation and synthesis.  Central to our approach is to separate the functionality of a system (encapsulated in software components) from its run-time adaptation (specified as a policy).  Developers will be provided with a language to write policies that control when components are executed, their concurrency, and dynamically selects which alternate component implementation should be used. The policy language will be combined with data-driven techniques that integrate simulation, program analysis, and machine learning techniques to configure the parameters of a policy as well as synthesize new policies.<br/><br/>The research undertaken as part of this proposal will provide a solid foundation for building adaptive IoT systems. The primary research activities will help reduce the development time and improve the robustness of IoT systems, bringing this technology closer to fruition.  The primary educational activities include incorporating new topics on IoT systems into the Operating Systems curriculum and topics on software adaptation and synthesis into the Mobile Computing curriculum. These efforts will create a pipeline of undergraduate and graduate students that are broadly interested in systems research. <br/><br/>The publications, educational materials, software artifacts, and results obtained as part of this project will be made publically available at http://cs.uiowa.edu/~ochipara/career for five years following the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811740","Statistical and Computational Guarantees of Three Siblings: Expectation-Maximization, Mean-Field Variational Inference, and Gibbs Sampling","DMS","STATISTICS","08/01/2018","07/22/2020","Huibin Zhou","CT","Yale University","Continuing Grant","Gabor Szekely","07/31/2021","$299,999.00","","huibin.zhou@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","MPS","1269","9263","$0.00","Three sibling algorithms, expectation-maximization (EM), mean-field variational inference, and Gibbs sampling, are among the most popular algorithms for statistical inference. These iterative algorithms are closely related: each can be seen as a variant of the others. Despite a wide range of successful applications in both statistics and machine learning, there is little theoretical analysis explaining the effectiveness of these algorithms for high-dimensional and complex models. The research presented in this project will significantly advance the theoretical understanding of those iterative algorithms by unveiling the statistical and computational guarantees as well as potential pitfalls for statistical inference. The wide range of applications of EM, mean-field variational inference, and Gibbs sampling ensure that the progress we make towards our  objectives will have a great impact in the broad scientific community which includes neuroscience and social sciences. Research results from this project will be disseminated through research articles, workshops and seminar series to researchers in other disciplines. The project will integrate research and education by teaching monograph courses and organizing workshops and seminars to help graduate students and postdocs, particularly minority, women, and domestic students and young researchers, work on this topic. In addition, the PI will work closely with the Yale Child Study Center and the Yale Institute for Network Science to explore appropriate and rigorous algorithms for neuroscience, autism spectrum disorder, social sciences, and data science education.<br/><br/>The PI studies these iterative algorithms by addressing the following questions: 1) what is the sharp (nearly necessary and sufficient) initialization condition for the algorithm to achieve global convergence to optimal statistical accuracy? 2) how fast does the algorithm converge? 3) what are sharp separation conditions or signal strengths to guarantee global convergence? 4) what are the estimation and clustering error rates and how do they compare to the optimal statistical accuracy? There are three stages to developing a comprehensive theory for analyzing iterative algorithms: 1) studying statistical and computational guarantees of EM for Gaussian mixtures for both global parameter estimation and latent cluster recovery, 2) extending EM to mean-field variational inference and Gibbs sampling, and considering a unified analysis for a class of iterative algorithms, 3) extending Gaussian mixtures and Stochastic Block Models to a unified framework of latent variable models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760102","A Graph Theoretic Approach for Spatial Dependence in Quality Control and Prediction","CMMI","OE Operations Engineering","07/01/2018","06/15/2018","Dorit Hochbaum","CA","University of California-Berkeley","Standard Grant","Georgia-Ann Klutke","06/30/2021","$398,763.00","","dhochbaum@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","006Y","071E, 073E, 9102","$0.00","This project will contribute to the advancement of science and will benefit the national prosperity and welfare, by enhancing manufacturing system monitoring and quality control. Expensive high-tech manufacturing processes require early detection of process disturbances and accurate yield prediction. Early detection allows for faster diagnosis of the nature and cause of the disturbances and their correction in order to improve quality and reduce production costs. This project will devise and test new prediction methods for diverse applications that exhibit spatial and/or temporal dependencies.  By exploiting these dependencies, this project is expected to enhance quality and reduce production costs in manufacturing.  The project also has relevance to other domains that exhibit spatial and spatio-temporal dependencies, such as control of the spread of communicable disease and enhanced protection of individuals on social networks by detecting patterns of adverse link behavior, such as spam. The fundamental concepts of this work and the new outlooks on prediction approaches will be incorporated into educational course materials. Both undergraduate and graduate students will be involved in the research and implementation in the areas of manufacturing and health care.<br/><br/>This project utilizes graph theoretic optimization techniques to explicitly incorporate spatiio-temporal dependencies in problems of  prediction and estimation.  The graph-theoretic approach employs a separation-deviation model where the objective is to minimize a penalty function involving deviation functions associated with nodes and separation functions associated with edges.  Efficient parametric cut algorithms for convex deviation and bilinear separation will be extended and improved.  Separation functions for integrated circuit manufacturing yield prediction based on priors from actual wafer defect data will be examined.  This work will make fundamental contributions to the theoretical development of models and computational algorithms for extensions of the basic separation-deviation model, which is used extensively in Bayesian estimation, machine learning, and isotonic regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812503","Mining Multi-Layer Protein-Protein Association Networks: An Integrated Spectral Approach","DMS","MATHEMATICAL BIOLOGY","08/01/2018","07/31/2018","Lenore Cowen","MA","Tufts University","Standard Grant","Junping Wang","07/31/2021","$210,000.00","Xiaozhe Hu","cowen@eecs.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","MPS","7334","","$0.00","This project is focused on strategies which help us to obtain information for associated pairs of proteins or genes in biological systems.  In addition to the collection of lots of information about the role of different genes or proteins in the cell, there is also increasing information about pairs of proteins or genes that are related, either because there is evidence that they cooperate in the cell, or evidence that they otherwise have common attributes. The information about associated pairs can be mathematically described as a heterogeneous collection of networks, but designing efficient and effective machine learning and computational mathematics algorithms to integrate the diverse information sources to explore and make sense of these networks is a difficult unsolved problem.  The new mathematical methods that will be developed in this project will be customized for the computational biologists and systems biologists who would like to use network analysis to boost the statistical significance of the signal of important genes and pathways in their data, with applications to gene function prediction, and the identification of sets of genes that are important in complex diseases such as type II diabetes and Crohn's disease. The project supports one graduate student and two undergraduate students. Through training and collaborating with investigators and other experts in the field, they will become involved in the broader research communities of scientific computing and biology.<br/><br/>Effective and efficient inference and computational methods will be developed, analyzed, and implemented for mining multi-layer PPI networks via an integrated spectral approach and the generalizations of diffusion-based distance metrics. More precisely, spectral multilayer analysis methods based on dimension reduction and multilevel optimization methods will be designed in order to provide high-quality integration tools of multiple networks that can be used to mine this massive graph collection. The methods will be benchmarked and tested on a substantial new biological network testbed, connected with the recent DREAM disease module identification challenge. Furthermore, implementations of the tools will be made generally available to the community, for mining heterogeneous network collections in general, which will lead to new insights related to core problems in computational biology, including the identification of disease modules within the datasets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1753167","CAREER: New strategy to outsmart antimicrobial resistance: Mastering evolution of beta-lactamases catalytic mechanism through reaction pathways alignment, simulation, and analysis","CHE","Chem Thry, Mdls & Cmptnl Mthds","06/15/2018","06/01/2018","Peng Tao","TX","Southern Methodist University","Standard Grant","Evelyn Goldfield","05/31/2023","$584,262.00","","ptao@smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","MPS","6881","062Z, 1045, 8084, 9263","$0.00","Peng Tao of Southern Methodist University is supported by an award from the Chemical Theory, Models and Computational Methods program in the Chemistry Division to develop new computational methods to model and predict the evolution of beta-lactamase catalytic mechanisms that lead to dangerous antibiotic resistant strains. How proteins are evolved is a vital question to answer for all living systems on earth. Previous studies focusing on protein sequences and structures provided great insight into protein evolution. However, there is a lack of understanding about functional mechanisms for proteins, especially for enzymes. This prevents deeper understanding of protein evolution. To meet this challenge, Professor Tao and his research group are developing an advanced theoretical framework and computational techniques to connect the evolution of enzyme sequences and structure to the evolution of their functions, catalytic activity, and mechanisms. They focus on beta-lactamases as enzymes that hydrolyze beta-lactam antibiotics and serve as one main cause of antimicrobial resistance. This work will provide fundamental understanding about the evolution of beta-lactamases' catalytic mechanisms, and could help develop a new generation of antibiotics with low resistance. The award also supports the development of educational materials to boost basic science education and outreach on social media. These freely accessible materials will benefit and shape the next generation of interdisciplinary scientists and engineers from economically disadvantaged student populations.<br/><br/>The project focuses on developing a theoretical framework to describe enzymatic mechanism landscapes, which are defined as the structural analysis of aligned reaction pathways of evolutionarily related enzymes using statistical and machine-learning methods. This theoretical framework is helping to predict how new mutations in enzymes could lead to changes in catalytic mechanisms and activities. New computational methods are being developed by Professor Tao and his research group to explore enzyme structure-function relations and provide the missing connection between the evolution of protein structure and the variations in enzyme mechanism that are less amenable to experiment. They are also developing online-based chemistry education channels and outreach platforms to enhance general chemistry education through collaboration with teachers from other schools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816695","AF: Small: Provable Quantum Advantages in Optimization","CCF","OFFICE OF MULTIDISCIPLINARY AC, CYBERINFRASTRUCTURE, QIS - Quantum Information Scie","10/01/2018","07/16/2018","Xiaodi Wu","MD","University of Maryland College Park","Standard Grant","Almadena Chtchelkanova","09/30/2021","$450,000.00","","xwu@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","1253, 7231, 7281","057Z, 7203, 7281, 7923, 7928","$0.00","The project aims to investigate the landscape of provable quantum advantages in optimization and machine learning, which is ubiquitous in our daily life, and to build a solid theoretical foundation for applications of quantum computing, especially with near-term quantum devices and in the establishment of quantum supremacy. By integrating modern tools in both optimization and quantum algorithm design, the project aims to design quantum algorithms for convex optimization and various semidefinite program classes, by quantizing the state-of-the-art classical optimization algorithms. The results obtained will be disseminated through a variety of venues, including conferences, new course materials, expository writings, and high school open days aimed at exposing young computer scientists to the frontiers of quantum information research. This project is jointly supported by the Algorithmic Foundations (AF) Program in the Division of Computing and Communications Foundations in the Directorate for Computer and Information Science and Engineering, and the Quantum Information Science (QIS) Program in the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This research project investigates provable quantum advantages in solving convex optimization, general and positive semidefinite programs, as well as variational optimization methods executable on near-term quantum devices. Specific targets include: (1) the sampling-based approach and the membership-to-separation approach for convex optimization; (2) optimal semidefinite program solvers based on the width-dependent and width-independent approaches as well as the interior point method. The investigator also aims to design new quantum optimization algorithms on near-term quantum devices as well as to provide theoretical justifications of quantum optimization algorithms based on the variational method and the quantum approximate optimization algorithm (QAOA).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750953","CAREER: Amplifying Intelligence in Mobile Networked Systems","CNS","Networking Technology and Syst","07/01/2018","07/18/2020","Chunyi Peng","IN","Purdue University","Continuing Grant","Alexander Sprintson","06/30/2023","$381,308.00","","chunyi@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7363","1045, 9102","$0.00","The mobile Internet revolution is ongoing, as the technology evolves from the fourth generation (4G) to 5G and beyond, with a need to incorporate a substantially higher degree of intelligence than is present in networks today. This need is driven by the observation that current 4G systems do not provide sufficient information on network behaviors. Consequently, it poses challenges to optimize performance, diagnose failures, and dis-incentivizes deployment of new applications such as virtual and augmented realities (AR/VR), autonomous vehicles, massive Internet of things. This project seeks to enhance intelligence to the 4G/5G signaling subsystem, offering critical network control utilities, such as radio resource control and mobility management. These new designs will lead to improvements in performance and reliability to the mobile Internet infrastructure for our society. The investigator will interact closely with mobile network companies for possible technology transfer. The project will also seek to influence the upcoming 5G and beyond 5G-standardization. It will recruit and train a new generation of engineers and students, including those from groups under-represented in this area.<br/><br/>This project explores a novel approach of Amplifying Intelligence in Mobile networked systems (called AIM). AIM will tackle two challenges: (1) lack of knowledge at the mobile clients: The mobile operating system and application programs lack information on the underlying ""black-box"" network operations; (2) High complexity and lack of verification in the infrastructure: The infrastructure suffers from complex designs and operations without proper verification. AIM is a multi-disciplinary solution that applies machine learning, data science, distributed systems and computing theory to mobile networking. Through its data-driven and verifiable design, AIM offers a viable, new solution suite in four concrete technical thrusts: (T1) protocol and function analytics for data-driven clients; (T2) provable and simplified control-plane signaling for data access; (T3) enhanced error-handling design with new investigative capability of tracing; and (T4) mobile VR for data-plane signaling intelligence. This project complements the ongoing efforts on wireless access (5G New Radio) and architectural innovations (e.g., network slicing and Network Function Virtualization).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829393","Identifying coral reef 'bright spots' from the global 2015-2017 thermal-stress event","OCE","BIOLOGICAL OCEANOGRAPHY","09/01/2018","06/10/2019","Robert van Woesik","FL","Florida Institute of Technology","Continuing Grant","Daniel J. Thornhill","08/31/2021","$500,558.00","Deron Burkepile","rvw@fit.edu","150 W UNIVERSITY BLVD","MELBOURNE","FL","329016975","3216748000","GEO","1650","1097, 8214, 8556","$0.00","Coral reefs are one of the world's most diverse ecosystems that provide goods and services, such as fisheries and storm protection, for inhabitants of tropical and subtropical regions. However, the current rapid rate of climate change threatens the existence of coral reefs as they degrade because of thermal-stress events. Consequently, the coverage and coral composition of many coral reefs is changing. Most global models suggest that few if any reef corals will survive beyond the 2.5 degree Celsius temperature rise predicted for the tropical oceans within the next hundred years. Such predictions differ from recent field studies on coral reefs that show pockets where corals do not bleach and die. The disagreement between the global models and field assessments is a consequence of ignoring climate-change refuges; it is critical to locate the climate-change refuges and determine what circumstances are conducive for coral survival. The investigators will examine the global response of coral reefs to thermal stresses over the last two decades, and focus on the 2015-2017 El Nino event, which caused considerable thermal stress and coral bleaching. The investigators ask the question: Where are the coral reef 'bright spots' from the thermal-stress events? 'Bright spots' are considered as places with less than expected bleaching. The team will also assess why some localities are potential 'bright spots'. Identifying coral reef bright spots will help guide future conservation decisions by enabling managers to target reefs with specific characteristics, which could be protected from human encroachment and be designated as potential refuges from coral bleaching as climate change progresses. This project includes training of a post-doctoral fellow and a Ph.D student, and host a coral-bleaching workshop. This study will be of relevance to all persons that live and work near coral reefs. What happens to reef corals has cascading consequences on other reef-associated organisms, and also influences whether reefs can keep up with sea-level rise. <br/><br/>The current rapid rate of climate change threatens the existence of coral reefs as they degrade by thermal-stress events. A glimmer of optimism lies in the observation that thermal stresses vary spatially and temporally across the oceans, with the consequence that coral communities in different geographic regions, and under different local conditions, are likely to inherently differ in their capacity to tolerate thermal stress. One of the most transformative aspects of this work is in analyzing the extent to which the bleaching patterns differed from model predictions. This work will capitalize on the recent progress on Bright-Spots Analysis to assess unexpected outcomes. The investigators will take two approaches. First, the project will use a machine-learning algorithm, boosted regression trees to examine the relationships between coral bleaching and the environmental predictor variables of interest. Second, a series of generalized mixed effects models, within a hierarchical Bayesian framework, will be used to identify where geographically 'bright spots' from thermal stress are located and why some coral reefs are more susceptible to thermal stresses than others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747778","Phase 1 IUCRC Rutgers-New Brunswick: Center for Accelerated Real Time Analytics (CARTA)","CNS","IUCRC-Indust-Univ Coop Res Ctr","06/01/2018","03/16/2018","Dimitris Metaxas","NJ","Rutgers University New Brunswick","Continuing Grant","Ann Von Lehmen","05/31/2023","$300,000.00","Vladimir Pavlovic, Kostas Bekris, Mubbasir Kapadia","dnm@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","5761","5761","$0.00","Nearly every research field and industry sector is struggling with extracting useful information from massive and dynamic data in a timely way. Developing tools and technologies in this realm of real-time and accelerated analytics contributes to promoting the progress of science and to advancing the national prosperity and welfare. Success in this realm hinges on balancing fundamental research, technological know-how, and commercial market intelligence. To address this challenge, this project joins industry members with academic centers to conduct multidisciplinary science and research towards extracting value from massive and moving data  and enabling better decision making of complex, dynamic data.<br/><br/>The Center of Accelerated Real Time Analytics (CARTA) project explores the ways in which relatively-high-risk fundamental developments can be leveraged to help organizations that have longer-term, more complex analytic needs. The focus of CARTA is on horizontal foundational technologies that would create an infrastructure capable of powering applications of national significance. In this context, at Rutgers-New Brunswick (RU-NB), the CARTA/RU-NB site will enable new application domains through innovative machine learning, statistical, modeling methods and technologies for accelerated and real-time analytics. Having these technologies and tools will be key in achieving the goals of the overall CARTA center. <br/><br/>The broader impact of the work of the CARTA center will be in addressing the future advanced, real-time analytics needs of the industry and society. The techniques developed by CARTA can be applied across industry sectors, including national security, healthcare, manufacturing, energy, and business intelligence. The fundamental research done at CARTA will be translated into technology developments, delivering practical solutions to hard problems. The ultimate success of this paradigm shift by the analytics industry will rest on the ability of CARTA universities to prepare experts to take advantage of the science and technologies to solve a variety of real-life applications. <br/><br/>CARTA research may involve sensitive academic and industrial data along with public domain data.  This data and resulting research outputs will be maintained using appropriate best practices for each type of data for a period of three years after the closing of CARTA. A central repository, suitably tagged for appropriate referencing and documentation, will be set up at https://carta.umbc.edu for maintaining the acquired and generated data from Center projects. Access to all models and project results will be stored on line and made available for downloading in near real time to respond to approved user requests.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750175","CAREER: Quality of Experience and Network Management in the Encrypted Internet","CNS","Networking Technology and Syst","08/01/2018","08/12/2019","Zubair Shafiq","IA","University of Iowa","Continuing Grant","Darleen Fisher","07/31/2023","$188,659.00","","zubair-shafiq@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7363","1045","$0.00","While encryption is essential for security and privacy, it limits the ability of content providers (e.g. Netflix and YouTube) and mobile network operators (e.g. AT&T and Verizon) to effectively and efficiently deliver content to users. First, encryption limits the ability of content providers to maintain effective control over modern applications such as augmented reality or virtual reality (AR/VR) video streaming. Second, encryption limits the ability of mobile network operators to efficiently share the scarce radio spectrum among their many users. These issues can significantly degrade users' Quality of Experience (or QoE). The goal of this project is to improve QoE of modern applications in the increasingly encrypted Internet. <br/><br/>This project will investigate implicit and explicit cooperation between network operators and content providers for QoE and network management in the encrypted Internet. First, the project will investigate implicit cooperation between network operators and content providers through machine learning based analysis of encrypted traffic. Second, the project will investigate in-band and out-of-band protocols and API frameworks for explicit information exchange between network operators and content providers. The project investigates a spectrum of innovative approaches for cooperative QoE and network management, ranging from clean slate approaches to the ones that are backward compatible and incrementally deployable. The proposed research will help improve Internet experience of modern high-throughput low-delay applications such as AR/VR video streaming, especially for users in underserved areas with poor Internet connectivity. The project will also engage undergraduates and under-represented minority students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827652","PFI-RP: From the Ground Up: Using Soft Robotic Sensors to Create a Foot and Ankle Wearable that Accurately Captures Real-time, Kinematic and Kinetic Data During Athletic Training","IIP","PFI-Partnrships for Innovation, EPSCoR Co-Funding","09/15/2018","03/20/2020","Reuben Burch","MS","Mississippi State University","Standard Grant","Kaitlin Bratlie","08/31/2021","$935,490.00","Rajkumar Prabhu, Harish Chander, John Ball, Mark Ronay","burch@ise.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","ENG","101y, 1662, 9150","019Z, 1662, 8042, 9102, 9150, 9251","$0.00","The broader impact/commercial potential of this PFI project will advance the health and welfare of the American public by using a new type of wearable technology solution. This liquid metal sensors-based wearable is designed to move human movement evaluation from laboratory environments to where people normally perform work: the real world. This project will create an affordable foot-ankle wearable to properly assess wearer gait and leg symmetry for people of all shapes and sizes without requiring expensive, bulky equipment or a practitioner's expertise. Moreover, the proposed tool is accurate, yet is also accessible meaning that this capability could improve quality of life via proper prognosis and diagnosis within competitive athletics as well as for medical patients during real-world task assessments. This project can also be used to support the national defense of the United States by providing the ability to assess movement measures, such as step count, acceleration, and ground reaction force estimation of military personnel, aiding in tactical planning and identification of oncoming injuries to limb segments. Over time, joints will fatigue and failure to identify these stressors can have a severe impact on the battlefield just as in the practice courts or industrial environments. <br/><br/>The proposed project repurposes soft robotic sensors to accurately and noninvasively capture movement and force data in near real-time at the foot-ankle to provide meaningful performance and risk assessment for tailoring training regimens. This wearable solution is in response to collegiate and professional-level strength and conditioning coaches and trainers desire for a precise solution that captures movement data ""from the ground up"". The proposed solution will accurately capture ankle kinematic and kinetic data outside of the lab, giving coaches, trainers, medical staff, and researches a wealth of information that was previously not available or trusted by field practitioners. Intellectual merits include: (a) expanding state-of-the-art wearable solutions to measure absolute foot-ankle angles, (b) using custom liquid metal sensors to measure foot forces, and (c) combining complex angle and force measurements into machine learning systems estimating ankle injury risk thereby allowing practitioners to effectively monitor repetitive movements. The project goal is to create an accurate yet affordable wearable system that can identify potential risks of the most common injury, ankle strain. Collegiate student athletes will be used for design validation as initial project scope is designed for competitive sports; however, the solution will be expandable to industrial and military repetitive motions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815328","AF: Small: Approximate Counting, Stochastic Local Search and Nonlinear Dynamics","CCF","Algorithmic Foundations","06/01/2018","05/21/2018","Alistair Sinclair","CA","University of California-Berkeley","Standard Grant","Joseph Maurice Rojas","05/31/2021","$500,000.00","","sinclair@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7796","7923, 7926","$0.00","Theoretical Computer Science (TCS) is concerned with core questions in algorithms and complexity theory, and also with many questions in other scientific disciplines viewed through the so-called ""computational lens."" This refers to the study of many scientific phenomena that are fundamentally computational in nature and can therefore benefit from a TCS perspective. This project addresses both core topics and connections with other disciplines, notably statistical physics and population genetics. A unifying theme in the project is techniques for the analysis of random and physical processes arising in all of these fields. The project will engage as collaborators experts in fields such as complex analysis, applied probability and statistical physics to work on these interface areas. The project affords many opportunities for graduate student and postdoctoral training and ideas from this research will find their way into course curricula. The investigator also maintains a strong interest in undergraduate and K-12 teaching, the latter through his involvement with the Berkeley Math Circle.<br/><br/>On a more technical level, the project contains three broad themes:<br/><br/>1. Approximate counting: the development of approximation algorithms for counting problems, with special emphasis on the emerging technique of ""geometry of polynomials"" and its connections to the more classical techniques of Markov chain Monte Carlo and correlation decay. Applications include generating functions in combinatorics, partition functions in statistical physics, the computation of volumes, and the analysis of graphical models in machine learning.<br/><br/>2. Stochastic local search algorithms: the study of a novel framework, arising from recent advances on the algorithmic Lovasz Local Lemma, for the design and analysis of stochastic local search algorithms, which search for combinatorial structures with desired properties by successively eliminating ""flaws"". The new framework is based on matrix norms and is inspired by linear time invariant analysis in control theory.<br/><br/>3. Nonlinear dynamics: the study of computational aspects of two distinct, but related nonlinear dynamical systems: mass action kinetics and the ""red-queen"" dynamics. Mass action kinetics is a standard model for chemical reaction networks, and also captures other processes such as the Boltzmann equation in statistical physics, genetic algorithms and recombination in population genetics. The more speculative red-queen dynamics is a novel model for the evolution of multiple species under selection.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762918","Collaborative Research: Downburst Fragility Characterization of Transmission Line Systems Using Experimental and Validated Stochastic Numerical Simulations","CMMI","Engineering for Natural Hazard","08/01/2018","05/14/2018","Abdollah Shafieezadeh","OH","Ohio State University","Standard Grant","Joy Pauschke","07/31/2021","$209,596.00","","shafieezadeh.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","ENG","014Y","036E, 039E, 040E, 1057, 7231, CVIS","$0.00","Downbursts, referring to downward high intensity winds associated with thunderstorms, pose a major threat to power transmission grids in many parts of the United States.  Considering the distributed size of the power transmission infrastructure, significant investments will be needed in order to upgrade the existing systems and properly design new infrastructure to resist downburst wind forces. The goal of this research is to develop an integrative experimental and numerical framework that is capable of characterizing the extent of vulnerability of the power transmission infrastructure against downbursts, and identify the most critical components. The framework will provide knowledge that can help reduce outage-induced societal disruptions caused by downbursts, and thus enable continued national prosperity and welfare following a downburst event. Using this framework, various significant factors for the downburst performance of transmission systems and potential causes of past failures will be investigated. Moreover, this research will develop the first generation of downburst fragility models for transmission tower-line systems (TLSs) using experimentally validated numerical models. These fragility models are crucial for risk-informed decision making for design and management of the transmission grid in order to mitigate future failures and enhance the resiliency of the grid against extreme weather events. The numerical and experimental studies will provide the knowledge needed to enhance design methodologies to include downburst wind loads for transmission line systems.  The research findings will be integrated into undergraduate and graduate courses at Florida International University (FIU) and The Ohio State University to better prepare the future generation of infrastructure engineers. Project data will be archived and publicly shared in the NSF-supported Natural Hazards Engineering Research Infrastructure (NHERI) Data Depot (https://www.DesignSafe-ci.org/). <br/><br/>This research will develop an integrative experimental and numerical framework to characterize the fragility of transmission line systems against downbursts. The experimental research will involve developing a versatile downburst simulator at the NSF-supported NHERI Wall of Wind (WOW) Experimental Facility (EF) at FIU. Using this simulator, a scaled, aeroelastic multi-span TLS will be tested; the results will be used to experimentally validate high-fidelity finite element models of coupled transmission tower-insulator-conductor-foundation systems. Through these experimental and numerical investigations, new knowledge will be gained with regard to the aerodynamic behavior of conductors and drag and shielding effects on lattice tower sections under non-synoptic downburst wind fields. Moreover, these investigations will reveal extreme nonlinear behaviors of transmission line systems in post-elastic regimes when towers are damaged or conductors fail. The research also will provide new physics-based insights into the role of uncertainties in the downburst performance of TLSs. Failure modes that are unique to or are more likely to occur under non-synoptic downburst loadings, as compared to those under synoptic hurricane loadings, will be identified and characterized. The produced data and models will be integrated to develop the first generation of multi-dimensional demand and fragility surfaces for TLSs under downbursts at component- and system-levels using highly efficient and accurate machine learning techniques. In addition, the experimental downburst simulator at the NHERI WOW EF at FIU will provide the natural hazards community with a unique testbed with dual simulation capabilities to generate both non-synoptic and synoptic winds and analyze the impacts on buildings and other structural systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801865","NeTS: Large: Collaborative Research: ASTRO: A Platform for 3-D Data-Driven Mobile Sensing via Networked Drones","CNS","Special Projects - CNS, Networking Technology and Syst, CPS-Cyber-Physical Systems","08/15/2018","07/09/2020","Edward Knightly","TX","William Marsh Rice University","Continuing Grant","Deepankar Medhi","07/31/2023","$1,064,934.00","Robert Griffin, Clifford Dacso, William Reed, Yingyan Lin","knightly@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","1714, 7363, 7918","7363, 7925, 9251","$0.00","The driving vision of this project is to detect Volatile Organic Compounds (VOCs) through ASTRO, a platform for autonomous 3-D data-driven mobile sensing via networked drones equipped with gas sensors. VOCs are hazardous to human health and the environment; they are released by explosions, gas leaks, and industrial accidents prevalent in low-income and under-resourced urban neighborhoods in close proximity to industrial processing plants, chemical refineries, and other sources of airborne pollutants. The project is located in an economically disadvantaged area of Houston, Texas. With Technology For All (TFA), the project team has a history of engaging the local community via broadband access, technology training, and connected health. The TFA wireless network already serves 1000's of community members in several square kilometers in Houston's East End via a mix of commercial Wi-Fi and software defined radios. The project targets realizing a high-resolution ground truth of environmental conditions in low-income urban areas which can impact emergency response procedures and environmental justice via policy and law. The project will develop a mobile app that alerts community residents of hazardous VOC concentrations near their current location. This project will impact urban areas with a demonstration of fusing next generation environmental sensing with next generation wireless access via networked drones. <br/> <br/>The project's objective is to realize an unprecedented resolution in VOC sensing by development and demonstration of ASTRO, a system for networked drone sensing missions without ground control. ASTRO will realize the unique capability to dynamically move sensors in 3-D according to real-time measurements. Consequently, networks of drones with on-board sensors can find and track VOC plumes, solely by coordinating among themselves, and without requiring a centralized ground controller. Two inter-related thrusts will realize this vision. The first is target detection, tracking, and modeling high VOC concentration clusters, targeting health and environmental safety. The second is development of the underlying principles and methodologies for data-driven mobile missions via drone networks. The project's outcomes will include lightweight machine learning methods that provide foundations for real-time distributed autonomous sensing with environmental and health objectives. These data sets will yield development of atmospheric models of VOCs at a finer resolution than is possible today. Moreover, the outcomes will also include methods for adaptive communication among the networked drones via software defined radios that can adapt their network topology and spectrum usage to realize mission objectives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819251","Extraction of Information from Scientific Simulations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2018","07/09/2020","Stephen Becker","CO","University of Colorado at Boulder","Continuing Grant","Leland Jameson","07/31/2021","$149,993.00","","stephen.becker@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","MPS","1271","9263","$0.00","Numerical simulations are a cornerstone of the modern scientific process, and considerable expense and energy, both literally and figuratively, is put into simulations. The two objectives of this project are to (1) shorten simulation time, and (2) ask new questions of the data given the output of an existing simulation (but not its full history) in order to avoid running a new simulation.  To shorten simulation time, a naive approach leads to less accurate estimates of certain variables that are used for both prediction of new material properties and for validation of numerical models against known properties. This program investigates super-resolution methods that do not suffer this loss of accuracy. The second objective records appropriate randomized snapshots or ""sketches"" from simulations.  Because it may not be known in advance which variables should be directly calculated, this program develops techniques to use the sketches to estimate variables that were not originally calculated. The technique is designed to ""future-proof"" simulations, and gives datasets a second chance at being useful without requiring a new simulation to be run. Achieving either or both objectives in this project will lead to fewer unnecessary computer simulations, saving energy and reducing impact on the environment.<br/><br/>This project brings together ideas from physical chemistry, high-performance computing, optimization, machine learning, digital signal processing, and time-series statistics. More specifically, the first objective is to estimate spectral variables from a shortened simulation. Shortening the time range usually has the effect of broadening the spectral lines. Under appropriate assumptions on the spectra, recent advances in harmonic analysis show that by solving a semi-definite program, one can super-resolve the spectral lines. These advances have been limited to signal-processing applications, and with appropriate modification, they will have a great impact on spectral estimation from simulations.  The project will develop efficient algorithms to tackle this problem, as well as show how it can be adapted to the situation of scientific computing, and theoretically extended to correctly handle uncertainty. The second objective is compressive parameter estimation, which has only recently been explored and mainly in the context of digital signal processing. The project will develop particular estimators for given statistics, as well as explore a general approach to apply to a broad class of statistics. The approach for storing snapshots of data will be applied primarily to magnetohydrodynamic (MHD) simulations of solar convection, a field of great importance due to the effect of the sun's weather on cosmic rays and photons incident on Earth.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822420","Chequamegon Heterogeneous Ecosystem Energy-balance Study Enabled by a High-density Extensive Array of Detectors","AGS","Physical & Dynamic Meteorology, Hist Black Colleges and Univ","08/01/2018","07/15/2019","Ankur Desai","WI","University of Wisconsin-Madison","Continuing Grant","Chungu Lu","07/31/2021","$1,138,002.00","Mark Schwartz, Grant Petty, Philip Townsend, Stefan Metzger","desai@aos.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","GEO","1525, 1594","4444, 9178, 9251, SMET","$0.00","The living biosphere interacts with atmospheric processes at a multitude of scales. Understanding these processes requires integration of multiple observations for comparison to theories embedded in atmospheric models. But, all observations mismatch the scale of all models. Therefore, spatial and temporal scaling of surface fluxes is fundamental to how we evaluate theories on what happens within the sub-grid of atmospheric models and how those feed back onto larger scale dynamics. The Chequamegon Heterogeneous Ecosystem Energy-balance Study Enabled by a High-density Extensive Array of Detectors (CHEESEHEAD) is an intensive field-campaign designed specifically to address long-standing puzzles regarding the role of atmospheric boundary-layer responses to scales of spatial heterogeneity in surface-atmosphere heat and water exchanges.<br/><br/>Intellectual Merit:<br/>The high-density observing network is coupled to large eddy simulation (LES) and machine-learning scaling-experiments to better understand sub-mesoscale responses and improve numerical weather and climate prediction formulations of sub-grid processes. This project will advance spatiotemporal scaling methods for heterogeneous land surface properties and fluxes and theories on the scales at which the lower atmosphere responds to surface heterogeneity. CHEESEHEAD aims to provide a level of observation density and instrumentation reliability never previously achieved to test and develop hypotheses on spatial heterogeneity and atmosphere feedbacks.<br/><br/>Broader Impacts:<br/>The experiment generates knowledge that advances the science of surface flux measurement and modeling, relevant to many scientific applications such as numerical weather prediction, climate change, energy resources, and computational fluid dynamics. The research will train next generation land-atmosphere graduate and undergraduate students. Field support outreach and teacher training is included via middle, high school, and undergraduate student involvement at nearby schools and colleges in coordination with UCAR's (University Corporation for Atmospheric Research) GLOBE program, Northland College, and local school districts. The database of observations and models will be made immediately available to the community and public for general use for further scientific advancement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828123","MRI Collaborative: Development of ESPRIT - Emerging systems' performance and energy evaluation instruments and testbench","CNS","Major Research Instrumentation, HCC-Human-Centered Computing","10/01/2018","09/08/2018","Chase Wu","NJ","New Jersey Institute of Technology","Standard Grant","Rita Rodriguez","09/30/2020","$100,000.00","","chase.wu@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","1189, 7367","1189, 7367","$0.00","Future computing nodes will most likely rely on heterogeneous processing and memory systems as well as networking technologies. Identifying the most suitable computing system for a given application requires the cumbersome task of evaluating the application's performance on as many alternatives as possible. This project develops ESPRIT (Emerging Systems PeRformance and Energy Evaluation Instrument and Testbench), a computing system capable of evaluating the most suitable system for specific classes of applications. If applications can be classified into groups based on their similarities along a wide range of performance characteristics, it may be possible to determine the system best suited for a specific class of applications. This work will help large-scale computing systems be configured for more efficient operation and lower energy use.<br/><br/>The ESPRIT project consist of state of the art computing nodes; system, memory, and power and energy simulators; benchmarks from different applications; a suite of measuring instruments; models for investigating application behaviors; statistical clustering and other machine learning techniques.The merit of this project resides in the development of instruments to evaluate applications along a number of performance characteristics of behaviors and classifying them into clusters in order to identify the most suitable design for energy efficiencies by varying capacities as well as technology scales. ESPRIT could be used to investigate new design choices, or tune applications for specific designs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819626","SBIR Phase I:  In-home Monitoring of Sleep Fragmentation and Micro-Arousals by Characterizing Leg Movements","IIP","SBIR Phase I","07/15/2018","09/24/2018","Cody Feltch","MD","TANZEN MEDICAL, INC.","Standard Grant","Henry Ahn","02/29/2020","$225,000.00","","cfeltch@tanzenmed.com","502 Scrimshaw Way","Severna Park","MD","211461423","4107775251","ENG","5371","5371, 8038","$0.00","This SBIR Phase I project uses novel technologies to produce an attractive, affordable and commercial product and service, RestEaZe translating to the home the sleep lab evaluations of restless sleep for millions of adults with insomnia and children with ADHD and RLS. An analysis of significant sleep features that should be evaluated for at-home sleep monitoring indicated that at least these should include total sleep time, brief arousals, wake during sleep, sleep efficiency, periodic leg movements in sleep and sleep position. Moreover these need to be developed and validated for adults as well as children. The analyses indicate that sleep monitoring should be based on leg and not arm movement. <br/><br/>RestEaZe, for the first time ever, both phenotypes and also measures the leg movements of sleep (LMS) and using logistic regression and advanced machine learning identifies LMS characteristics that accurately predict these significant sleep measures. RestEaZe translates advanced sleep science from the lab to the home. The user gets an immediate scientifically accurate view of quiet vs. restless sleep and changes in sleep over several nights in relation to user entered sleep behavior. RestEaZe also provides professionally developed interactive guides for improving sleep based on RestEaZe results. The doctor gets advanced movement and sleep analyses with interactive graphic and numeric summaries of the patient's sleep over several days. The analyses will reveal both sleep quality, circadian pattern and PLMS. The PLMS reveal possible medical conditions such as sleep apnea, restless legs syndrome (RLS), cardiac morbidity and REM behavior disorder.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747748","Planning IUCRC Northeastern University: Center for Hardware and Embedded System Security and Trust (CHEST)","CNS","IUCRC-Indust-Univ Coop Res Ctr","02/01/2018","01/30/2018","Yunsi Fei","MA","Northeastern University","Standard Grant","Behrooz Shirazi","01/31/2020","$15,000.00","","yfei@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","5761","5761","$0.00","Integrated circuit and embedded hardware devices are critical to most modern military and industrial systems for defense, energy, healthcare, banking, communication, transportation, and other sectors.  These devices are more vulnerable than ever to malicious tampering by untrusted entities, with threats over a broad range of attack vectors.  Malicious functionality can be added to the devices during the design process, fabrication, or assembly, leading to compromised systems, often without detection.  This collaborative effort seeks to further the national defense by developing novel methodologies and technologies that enhance trust and security of these devices and their respective systems.<br/><br/>The to-be-planned Center is intended to enable researchers and practitioners from diverse areas -- integrated circuits, architecture, cryptography, system safety and resilience, etc. -- to position device security as a coherent, collaborative discipline.  It will be a hub for industry-focused research and a repository for related data.  It will foster dialog to promote advances across disciplinary boundaries. The site will lead collaborative efforts in security of cyber-physical and large-scale systems, secure architecture and virtualization, and embedded system security and privacy. It will adopt advanced machine-learning algorithms, and investigate analysis and mitigation of side channels and covert channels in mobile systems, Graphical Processing Units, and nano-devices.<br/><br/>The Center is expected to influence the design, protection, and resilience of cyber physical systems from vulnerabilities associated with device security and workforce development needed for industry, government, and military.  The Center will disseminate research results and synthesized theories on device security through both professional/scholarly activities and courses taught at the six sites.  The results should bring a significant return on investment, as they improve the readiness of industry for emergent conditions and improve resilience to frequencies, natures, and severities of threats.<br/><br/>The collaborative group for the Industry & University Collaborative Research for Hardware and Embedded System Security and Trust is composed of George Mason University, Northeastern University, University of Connecticut, University of Texas at Dallas, University of Virginia, and Wright State University.  The collaborators host a website at https://www.vdl.afrl.af.mil/programs/PG_CHEST_IUCRC with meeting materials, program information, publications, etc. The website is made available indefinitely or until the Center transitions to the next phase."
"1841426","Planning IUCRC at Case Western Reserve University: Center on Data Science for Materials Reliability and Degradation (MDS-Rely)","IIP","IUCRC-Indust-Univ Coop Res Ctr","12/01/2018","11/07/2018","Roger French","OH","Case Western Reserve University","Standard Grant","Prakash Balan","11/30/2019","$15,000.00","Laura Bruckman","roger.french@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","ENG","5761","123E, 5761","$0.00","Case Western Reserve University (CWRU) and the University of Pittsburgh (Pitt) are planning to form a new IUCRC named the Center on Materials Data Science for Reliability and Degradation (MDS-Rely). The primary goal of MDS-Rely is to apply data science-informed research to better understand the reliability and lifetime of essential materials. This IUCRC builds on established centers at both locations focusing on Energy Materials, Advanced and Additive Manufacturing, and in Data Science and Analytics. Recent advances in materials data science have the potential to transform this field in creating new technologies that enable unprecedented lifetimes, durability in extreme environments, and understanding of various degradation and failure mechanisms. Our established relationships with industry and our multidisciplinary capabilities in protocols, modeling, and applications are key to taking on this grand challenge of long-lived technologies. MDS-Rely will engage a diverse cohort of students, and develop broad data science capabilities to respond to employers' needs. MDS-Rely will harness relationships between industry and academia to make headways in applying data science to materials development, design, and reliability.  <br/><br/>MDS-Rely will include three Thrusts: (1) Enhanced Reliability Study Protocols, (2) Modeling & Service Life Prediction, and (3) Reliability and Degradation Applications. Thrust 1 will focus on designing data informed experimental studies of materials and developing a data science-based experimental design with both non-destructive and destructive evaluations to monitor material degradation. Thrust 2 will focus on degradation and reliability modeling of materials and will encompass graph-based network modeling, machine learning, image analysis, and time-series analysis. Data analytic pipelines will be created to inform future analysis. These models will give insight back to the design of data informed experimental studies. Finally, Thrust 3 will inform materials design choices for different stress conditions by understanding the main contributors of degradation in a material system. This will also allow for much more rapid material selection in the material choice portion of a system design. The workshop meetings further define specific projects to direct respond to industry needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822148","Planning IUCRC at University of Tennessee/Knoxville:  Center for [Digital Composite Joining and Repair]","IIP","IUCRC-Indust-Univ Coop Res Ctr","09/01/2018","08/28/2018","Uday Vaidya","TN","University of Tennessee Knoxville","Standard Grant","Prakash Balan","08/31/2019","$14,999.00","Dibyendu Mukherjee, Elizabeth Barker","uvaidya@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","ENG","5761","5761","$0.00","This award supports a planning workshop for an Industry-University Cooperative Research Center (IUCRC) aimed at improving composite material joining and repair (CJAR) technologies. Composite materials are vital to the competitiveness of several sectors, including automotive, manufacturing, energy, biomedical, defense and aerospace. Current CJAR practices are highly specialized, labor-intensive, and require experienced technicians for critical inspection, maintenance and repair. Partner institutions including The University of Tennessee-Knoxville, Georgia Institute of Technology and Oakland University will collaborate with various industrial companies, in the automotive, aerospace, defense, energy, and biomedical sectors of the US economy. The University of Tennessee site of D-CJAR will engage the Institute for Advanced Composites Manufacturing Innovation (IACMI), Oak Ridge National Laboratory (ONR) - (Manufacturing Demonstration Facility (MDF) and Carbon Fiber Technology Facility (CFTF)), material suppliers, equipment manufacturers and end users.  Examples include Magnum Venus Products (MVP), Resource Fiber, Mini-Fibers, HTS IC among others. Faculty and student teams at the partner universities will work with industry members to conduct pre-competitive research to develop and disseminate basic and applied technologies/knowledge to facilitate rapid, reliable, and cost-effective composite joining and repair, with an overall goal of significantly reducing costs, cycle time, and variation of CJAR operations within ten (10) years.<br/><br/>With the goal of transforming the current labor-intensive and specialized processes into science-based, automated, and digital CJAR processes, the University of Tennessee site of D-CJAR will primarily focus on developing CJAR technologies for multi-material joining and repair of advanced composites, metals and hybrids. We expect advances in several fields and knowledge domains around CJAR, including (1) design and analysis of multi-materials joining, (2) process innovations, (3) materials and process engineering, and (4) testing and nondestructive evaluation (NDE). We will apply advanced digital techniques, including advanced computational modeling, sensing, materials characterization, and machine learning to practical CJAR cases that will also advance the education and workforce preparedness of students working on projects at the partner universities. Development of new materials and processes would facilitate standardization, modeling, and automation of many CJAR tasks and processes, producing cost savings, faster cycle times, and enhanced performance for industry partners across the entire composites supply chain, contributing to the maintenance of U.S. global leadership in this growing sector.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822169","Planning IUCRC Arizona State University: Center for Networked Embedded, Smart and Trusted Things NESTT","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/01/2018","08/04/2018","Sarma Vrudhula","AZ","Arizona State University","Standard Grant","Behrooz Shirazi","12/31/2019","$14,999.00","","vrudhula@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","5761","5761","$0.00","Arizona State University will collaborate with The University of Arizona, University of Southern California, Southern Illinois University Carbondale and the University of Connecticut to plan for the formation of a new Industry University Cooperative Research Center (IUCRC), called Center for Networked Embedded, Smart and Trusted Things (NESTT).  Companies from a wide spectrum of industries will be recruited and will work jointly with faculty to create a portfolio of industry-ranked, multi-disciplinary research projects to develop innovative solutions to fundamental technological and societal challenges posed by Internet of Things (IoT). The outcome will be a proposal to establish NESTT as an IUCRC. <br/><br/>ASU will work with its partners to organize workshops involving industry leaders and academic researchers to draft the research agenda for NESTT, aimed at accelerating IoT technology development and transfer to industry, and make opportunities from the IoT equitable, safe and secure for all. ASU expertise will include edge and Fog computing; safe and secure cyber-physical systems; machine learning, data analytics; trustworthy, networked embedded systems; robotics and industrial IoT; smart cities and transportation systems; sensors and wearable electronics; IoT governance, technology-related law and ethics, and business models. NESTT at ASU will further the participation of underrepresented students in STEM disciplines.  <br/><br/>The IoT will become the foundational technology for every major industry.  NESTT?s technological innovations and holistic multi-disciplinary design will lower the barriers to the adoption IoT technologies which will accelerate the delivery of significant economic, societal and environmental returns. The enormous wealth of data generated from such IoT systems will also lead to new discoveries and inventions in many scientific disciplines as scientific communities further embrace data-driven research.  The planning for NESTT will include working with industry partners to develop innovative ways for recruiting (in industry and academia) underrepresented students in STEM disciplines. <br/><br/>The agenda and documentation of activities for the NESTT IUCRC planning meetings will be made available on ASU's website for the Center for Embedded Systems (https://embedded.asu.edu ). It will be maintained until the establishment of NESTT, and then merged with the NESTT site.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815757","SaTC: CORE: Small: Models and Measurements for Website Fingerprinting","CNS","Secure &Trustworthy Cyberspace","09/01/2018","07/18/2018","Nicholas Hopper","MN","University of Minnesota-Twin Cities","Standard Grant","James Joshi","08/31/2021","$499,659.00","","hopper@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8060","025Z, 7434, 7923","$0.00","Many private interactions between individuals and their friends, families, employers, and institutions are now carried out on the Internet; disclosure of the contents of these interactions or even the mere associations between these parties can expose people to real financial or physical risks. As a result, encryption and services such as virtual private networks or the Tor project that conceal the connection between a user and the websites they visit are growing in popularity.  Website fingerprinting attacks use information that is not concealed by these techniques, such as file sizes and download times, to re-identify the websites a user visits, but while these attacks work in a lab environment, it is a challenge to evaluate them in practical settings and develop effective protections against them.  This project will apply statistics and machine learning to develop new probabilistic models of the ""fingerprint"" of a website, metrics of the amount of information these models reveal, and privacy preserving algorithms and datasets to test these models.  The results of these models and tests will be used to assess the threat posed by website fingerprinting and inform the design of new defense mechanisms.  As a result, users will benefit from improved protection techniques, while other researchers can use the resulting models, datasets, and metrics to study the effectiveness of website fingerprinting defenses.  The work and data will also be used to support both undergraduate and graduate education through both courses and research training.<br/><br/>This project will seek to address three key challenges in website fingerprinting research -- privacy-preserving characterization of background traffic, maintaining fingerprint databases, and evaluation and comparison of defenses -- by developing new representations of website fingerprints that can assign a likelihood to any fingerprint being generated by a specific type of download.  Using these representations, the project will pursue four main thrusts.  First, the project will use these models to determine the extent to which website fingerprints can directly infer identifying features of a download without requiring a database of all possible web pages, moving from existing closed world approaches to website fingerprinting toward more broadly applicable open world approaches.  Second, the project will develop algorithms to train the models on live traffic while preserving the privacy of individual users using concepts from differential privacy.  Third, using the trained models, the project will provide the first assessment of attacks and defenses on realistic data, and use metrics from information theory to compare those attacks and defenses on an equal footing.  Finally, the project will use the results of these evaluations to develop new defensive techniques that can be applied directly to the content of privacy-sensitive sites and to systems designed to protect users' downloads such as the Tor network.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839537","Workshop: Computing, Information Science, and Access to Justice","SES","LSS-Law And Social Sciences, Networking Technology and Syst","08/15/2018","06/30/2020","Tanina Rostain","DC","Georgetown University","Standard Grant","Reggie Sheehan","10/31/2020","$93,550.00","Ellen Zegura, Jon Gould","tr238@law.georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","SBE","1372, 7363","7556, 9179","$0.00","Title: Workshop: Computing, Information Science, and Access to the Justice System<br/><br/>Social science research demonstrates that low and middle-income people face significant barriers to obtaining access to the civil justice system because of an inability to afford a lawyer. While experiments with new approaches to access to the legal system have garnered much attention from courts, legal aid organizations, and the bar, there is little research about the efficacy of these new approaches. In other fields of human activity, computer scientists are creating new and powerful technologies to study how people interact with private and public institutions, as well as new communication tools, such as apps and social media, to improve these interactions. These computing-based approaches can lead to new knowledge about interactions with the civil justice system and new tools to advance the goal of improving access to the justice system. <br/><br/>This workshop will bring together several recent scientific, administrative, and technological trends to develop a new research agenda focused on computing and access to the justice system. These trends include emerging research on the civil justice system, the expanding digitization of information in legal institutions and organizations, and the recent rise of computational social science and the fields of human-centered computing, machine learning, and computing connectivity. The workshop will convene researchers engaged in access to justice scholarship, data scientists, human-centered computing experts, and representatives of courts, legal service providers, and other organizations embedded in the civil justice system, to generate a collaborative research agenda focused on the workings of the legal system. The workshop will produce a framework for research that applies computing methods to the scholarship on access to justice, while generating actionable knowledge about how to improve the civil justice system and strengthen its role in the United States democratic system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762968","Collaborative Research: Downburst Fragility Characterization of Transmission Line Systems Using Experimental and Validated Stochastic Numerical Simulations","CMMI","Engineering for Natural Hazard","08/01/2018","05/14/2018","Amal Elawady","FL","Florida International University","Standard Grant","Joy Pauschke","07/31/2021","$307,752.00","","aelawady@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","ENG","014Y","036E, 039E, 040E, 1057, 7231, 9102, CVIS","$0.00","Downbursts, referring to downward high intensity winds associated with thunderstorms, pose a major threat to power transmission grids in many parts of the United States.  Considering the distributed size of the power transmission infrastructure, significant investments will be needed in order to upgrade the existing systems and properly design new infrastructure to resist downburst wind forces. The goal of this research is to develop an integrative experimental and numerical framework that is capable of characterizing the extent of vulnerability of the power transmission infrastructure against downbursts, and identify the most critical components. The framework will provide knowledge that can help reduce outage-induced societal disruptions caused by downbursts, and thus enable continued national prosperity and welfare following a downburst event. Using this framework, various significant factors for the downburst performance of transmission systems and potential causes of past failures will be investigated. Moreover, this research will develop the first generation of downburst fragility models for transmission tower-line systems (TLSs) using experimentally validated numerical models. These fragility models are crucial for risk-informed decision making for design and management of the transmission grid in order to mitigate future failures and enhance the resiliency of the grid against extreme weather events. The numerical and experimental studies will provide the knowledge needed to enhance design methodologies to include downburst wind loads for transmission line systems.  The research findings will be integrated into undergraduate and graduate courses at Florida International University (FIU) and The Ohio State University to better prepare the future generation of infrastructure engineers. Project data will be archived and publicly shared in the NSF-supported Natural Hazards Engineering Research Infrastructure (NHERI) Data Depot (https://www.DesignSafe-ci.org/). <br/><br/>This research will develop an integrative experimental and numerical framework to characterize the fragility of transmission line systems against downbursts. The experimental research will involve developing a versatile downburst simulator at the NSF-supported NHERI Wall of Wind (WOW) Experimental Facility (EF) at FIU. Using this simulator, a scaled, aeroelastic multi-span TLS will be tested; the results will be used to experimentally validate high-fidelity finite element models of coupled transmission tower-insulator-conductor-foundation systems. Through these experimental and numerical investigations, new knowledge will be gained with regard to the aerodynamic behavior of conductors and drag and shielding effects on lattice tower sections under non-synoptic downburst wind fields. Moreover, these investigations will reveal extreme nonlinear behaviors of transmission line systems in post-elastic regimes when towers are damaged or conductors fail. The research also will provide new physics-based insights into the role of uncertainties in the downburst performance of TLSs. Failure modes that are unique to or are more likely to occur under non-synoptic downburst loadings, as compared to those under synoptic hurricane loadings, will be identified and characterized. The produced data and models will be integrated to develop the first generation of multi-dimensional demand and fragility surfaces for TLSs under downbursts at component- and system-levels using highly efficient and accurate machine learning techniques. In addition, the experimental downburst simulator at the NHERI WOW EF at FIU will provide the natural hazards community with a unique testbed with dual simulation capabilities to generate both non-synoptic and synoptic winds and analyze the impacts on buildings and other structural systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814969","SHF: Small: A Scalable Architecture for Ubiquitous Parallelism","CCF","Software & Hardware Foundation","10/01/2018","07/20/2018","Daniel Sanchez Martin","MA","Massachusetts Institute of Technology","Standard Grant","Yuanyuan Yang","09/30/2021","$450,000.00","","sanchez@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7798","7923, 7941","$0.00","With cost-performance gains predicted by Moore's Law slowing down, future computer systems will need to harness increasing amounts of parallelism to improve performance. Achieving this goal requires new techniques to make massive parallelism practical, as current multicore systems fall short of this goal: they squander most of the parallelism available in applications and are exceedingly hard to program. To address these challenges, this project is investigating a novel parallel architecture that efficiently scales to thousands of cores and is almost as easy to program as sequential systems. It achieves these benefits by exploiting ordered parallelism, which is general and abundant but is hard to mine in current systems. The technologies being investigated will make future parallel systems more versatile, scalable, and easier to program. These techniques will especially benefit hard-to-parallelize irregular applications that are key in emerging domains, such as graph analytics, machine learning, and in-memory databases. The prototyping efforts will bring the benefits of ordered parallelism to existing systems. Finally, the infrastructure developed as part of this project will be released publicly, enabling others to build on the results of this work.<br/><br/>Towards the goal of efficiently parallelizing the vast majority of applications while retaining the programming simplicity of sequential systems, this project is investigating and developing the following techniques: (1) distributed data-centric execution, which scales fine-grained ordered parallelism and speculative execution to rack-scale systems with tens of thousands of cores; (2) an expressive execution model that supports seamless combinations of speculative and non-speculative tasks, improving efficiency and parallelism; (3) adaptive speculation and resource management techniques that avoid performance pathologies, reduce wasted work, and make more efficient use of this novel architecture; and (4) an FPGA-based prototype of this architecture that leverages these techniques to exploit ordered parallelism and accelerate important applications. In this architecture, programs consist of tiny tasks with order constraints. The system executes tasks speculatively and out of order, and efficiently speculates thousands of tasks ahead to uncover ordered parallelism. Tasks are distributed to run close to their data, reducing data movement and allowing the system to scale across multiple chips and boards. An early 256-core design demonstrates near-linear scalability on programs that are often deemed sequential, outperforming state-of-the-art algorithms by one to two orders of magnitude.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815034","Predictive monitoring of aperiodic sources","AST","EXTRAGALACTIC ASTRON & COSMOLO","07/01/2018","06/07/2018","Matthew Graham","CA","California Institute of Technology","Standard Grant","Nigel Sharp","06/30/2021","$299,873.00","Stanislav Djorgovski","mjg@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","MPS","1217","1206, 1207, 7480","$0.00","Many studies of objects in the sky that vary in brightness look only at periodic variation, with a regular and repeating change with time.  However, most variable astronomical sources are aperiodic, showing erratic and irregular changes.  They remain poorly studied in comparison to periodic sources, even though they can play a key role in our understanding of complex dynamic physical environments, from stars, to gas and dust flows, to galaxies.  This work studies aperiodic variability using the data from two NSF-supported projects, the Catalina Real-time Transient Survey (CRTS), and the newly begun Zwicky Transient Facility (ZTF).  This is the first large-scale systematic study of these phenomena, and it will be the definitive study, well into the next decade.  It is an excellent case study for data-intensive science, applying state-of-the-art machine learning techniques to real data.  It will expose students to cutting-edge data science, and its products will contribute to projects that will train the next generation in how to handle big astronomical data.<br/><br/>Even though the majority of variable sources are aperiodic, they are poorly understood, and for well-known examples like quasars and young stellar objects many fundamental questions remain about the physical mechanisms behind their optical variability.  New sky surveys are enabling systematic studies of variability and discovering many new phenomena, including sub-parsec separated quasar binaries, multi-year long flares attributable to microlensing of explosive activity in the accretion disk, and changing-state sources indicative of variable accretion rates.  These extreme behaviors should be easily discoverable with modern robust statistical methods.  This work will create generative data-derived models with novel non-parametric discriminating features from CRTS data, and use the models to predict the future behavior of aperiodic sources, which can then be monitored in real-time using ZTF and other synoptic facilities.  Prior work focused on robust statistical characterization and identifying extreme variable classes.  This will be extended to more general aperiodic sources and more sophisticated non-parametric generative models.  The unprecedented sky coverage of ZTF joins with the unequalled archival coverage of CRTS to make this the definitive study.  In keeping with the CRTS Open Data and ZTF alert policies, all transitioning objects identified will be released to the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750970","CAREER: Computational Optics and Photonics for Deep Imaging of Live Tissue","OAC","CAREER: FACULTY EARLY CAR DEV, EPSCoR Co-Funding","05/01/2018","04/30/2018","Heidy Sierra","PR","University of Puerto Rico Mayaguez","Standard Grant","Alan Sussman","04/30/2023","$498,905.00","","heidy.sierra1@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","1045, 9150","026Z, 1045, 9102, 9150","$0.00","Innovation in non-invasive imaging techniques is part of the effort to provide rapid screening, diagnosis as well as to guide treatment in numerous settings that aspire to offer affordable and efficient healthcare.  Most of the existing high-resolution methods are effective primarily on thin and nearly homogeneous transparent samples or over tissue surface.  In most realistic scenarios, it is important to acquire information at depth within tissue.  High-resolution volumetric imaging approaches may require expensive computational tools for data analysis and complex hardware configurations.  Computational optics grounded on signal processing and image reconstruction concepts offers promising alternatives.  This research contributes to advance the related state-of-the-art in translational cyberinfrastructure and biomedical technology.  Results from this research can improve non-invasive imaging systems for research and patient care while supporting the NSF mission to promote the progress of science and advance the national health.  The development of this project involves multidisciplinary efforts from computer science, bioengineering and electrical engineering as well as educational activities with the participation of students from underrepresented groups.   <br/><br/>This project focuses on providing a framework to support advances on optical imaging techniques that can perform at the needed resolution and speed for various scenarios such as healthcare and biomedical research.  The research plan is geared to creating an advanced cyberinfrastructure with simulation and analysis tools to build a computational optical system for deep imaging of live tissues.  The components of the framework include three-dimensional optical imaging models employing nonlinear scattering theory that integrate tissue optical properties to characterize their effect into the imaging resolution performance.  Additionally, it includes the integration of light-tissue-interaction modeling parameters with compressive sensing concepts and machine learning algorithms for advanced data management.  This project targets realistic challenges in biomedical research, including (i) a gap between complex physics of light propagation in tissues and the design of efficient high-resolution imaging systems, (ii) computational optics and photonics for deep imaging of live tissues, and (iii) integration with reliable and state-of-the-art data analytics and visualization environments.  The simulations and computational optics tools focus on confocal imaging of skin tissue, which is widely used in biomedical research, and is potentially adoptable in the clinic to guide diagnosis of skin conditions.  The education plan addresses three major areas: i) research training and experiences for graduate and undergraduate students, i) course development in topics related with computational optics and data analytics, and iii) outreach to K-12 students and professionals to introduce research issues and opportunities in computational imaging and data analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763665","AF: Medium: Collaborative Proposal: Foundations of Adaptive Data Analysis","CCF","Algorithmic Foundations","03/01/2018","04/25/2020","Cynthia Dwork","MA","Harvard University","Continuing Grant","Tracy Kimbrel","02/28/2021","$286,000.00","","dwork@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757945","REU Site: Research Experiences for Undergraduates - Emerging Issues in Cybersecurity","CNS","RSCH EXPER FOR UNDERGRAD SITES, RES EXP FOR TEACHERS(RET)-SITE, Special Projects - CNS","02/01/2018","07/12/2019","Jun Zheng","NM","New Mexico Institute of Mining and Technology","Standard Grant","Harriet Taylor","01/31/2021","$295,984.00","Dongwan Shin","jun.zheng@nmt.edu","801 Leroy Place","Socorro","NM","878014681","5758355496","CSE","1139, 1359, 1714","9150, 9250","$0.00","This funding establishes a new Research Experiences for Undergraduates (REU) Site at the New Mexico Institute of Mining and Technology.  This REU Site will host a diverse group of undergraduate students who will spend their summer working on research problems dealing with emerging areas of cybersecurity.  Sound approaches to cybersecurity are essential for protecting the computer and communication devices of individual consumers as well as public and private companies and organizations and the government from the increasing threats of cyber attacks while also preserving the privacy concerns of citizens. The projects are led by faculty from the Computer Science and Engineering Department who are actively engaged in cybersecurity research. The university faculty will collaborate with RiskSense, a cybersecurity company, to develop research projects that have real-world relevance and that can impact the practices of the cybersecurity industry. The site will provide mentoring and other professional development opportunities that should impact the future academic and professional careers of the students. The site plans to attract talented undergraduate students from universities in New Mexico and the surrounding southwestern area of the country and target students from groups traditionally under-represented in the computing fields.   <br/><br/>The research projects in this REU Site address a range of topics including document format malware detection, machine learning for botnet detection, access control for smart grid, privacy-preserving location-based services, secure data logging for mobile devices, and developing user mental models against semantic attacks.  Students will use state-of-the-art facilities and current methods and techniques to develop solutions to fundamental challenges in cybersecurity.  The goals of this site include providing students experience in cutting edge research in cybersecurity, broadening participation in computing, motivating students to pursue graduate studies in computer science, and preparing the future cybersecurity workforce. This site provides an immersive research environment and cutting-edge research projects to undergraduate students who have limited access to research experiences and cybersecurity experts.  The site will be one of the first REU Sites in the southwest region of the country focusing on cybersecurity. The site has the potential to impact the cybersecurity workforce of the region. Through participation in the summer research program, the undergraduate students should acquire skills that will lead to rewarding professional careers in a field of emerging importance and impact."
"1750669","CAREER: A Policy-Agnostic Programming Framework for Statistical Privacy","CNS","Secure &Trustworthy Cyberspace","04/01/2018","08/02/2019","Jean Yang","PA","Carnegie-Mellon University","Continuing Grant","Sol Greenspan","03/31/2024","$103,595.00","","jyang2@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8060","025Z, 1045, 7434","$0.00","This project develops a new programming model that incorporates a theory of differential privacy. Differential privacy is a formulation of statistical privacy that protects individual data values while still allowing the release of results from privacy-preserving analyses. Prior work on language-based techniques for differential privacy has focused on preventing leaks, rejecting programs either statically, before they run, or dynamically, as they run, before they leak too much information. This project uses an approach that allows the compiler and runtime to enforce privacy requirements by construction. The objective is two-fold: to make it easier for programmers to implement privacy-preserving data analytics, and to provide provable privacy guarantees. The approach facilitates the programming of differentially algorithms, while allowing non-experts to build up intuitions about what makes programs differentially private. The project integrates research with education by developing a framework to educate non-experts about statistical privacy, by disseminating results to both the academic community and collaborators, and incorporating the techniques into the security curriculum.<br/><br/>Central to the technical approach is the concept of policy-agnostic programming, where a programmer can write policy-enforcing code that looks similar to (simpler) policy-free code and relies on the runtime environment to customize program behavior to enforce policies. The project develops the theory and infrastructure for a new programming framework called Jostle that supports privacy-agnostic programming through exposing fine-grained algorithmic choices to the programmer. The compiler and runtime, rather than the programmer, is responsible for navigating the space of privacy and accuracy trade-offs. Making this work involves (1) a dynamic semantics for policy-agnostic differential privacy, (2) a decidable probabilistic relational type system, and (3) a compilation framework for policy-agnostic differential privacy that uses the results of (1) and (2) for statically and dynamically exploring privacy/accuracy tradeoffs. The resulting system supports implementations of complex machine learning algorithms that are agnostic to the differential privacy concerns, and allow the programmer to rely on the compiler and runtime to modify programs to satisfy privacy requirements. The approach is intended to be sufficiently general to support different formulations of statistical privacy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839833","EAGER: Real-Time: Effective Power System Operation during Hurricanes using Historical and Real-Time Data","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2018","09/14/2018","Mostafa Sahraei Ardakani","UT","University of Utah","Standard Grant","Anil Pahwa","08/31/2021","$298,681.00","Ge Ou, Zhaoxia Pu","mostafa.ardakani@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","ENG","7607","155E, 1653, 7916","$0.00","Hurricanes often lead to large and long-lasting blackouts, with serious social and economic consequences. This project aims at developing models and software tools to provide guidance for the smart and preventive operation of power systems, using available weather forecast data. It is anticipated that the developed tools will better estimate the impacts of hurricanes on the electric power grid and identify operation strategies that are resilient to the damages induced by the hurricane. Ultimately, the outcomes of this project will help reduce the size and duration of power blackouts during hurricanes. The avoided blackout costs can save the U.S. economy hundreds of millions to several billion dollars each year. This is an interdisciplinary research project, where electrical engineering, civil engineering, and atmospheric science students will closely collaborate and broaden their skill sets, thus advancing their education and empowering the nation's trained workforce.<br/><br/>This project exploits the availability of weather forecast data to guide preventive power system operation during hurricanes.  Currently, weather forecast data is not systematically integrated into the power system operation and, thus, preventive operation is not possible. The current research approaches employ an integrated framework, mainly based on physical and engineering models, which uses the hurricane forecast information to predict the component damage scenarios for the power system. The preventive operation decisions are, then, determined with stochastic optimization, through explicit modeling of the damage scenarios. The existing models have two main shortcomings: 1) they ignore the weather forecast uncertainties; and 2) they are extremely computationally demanding. There are substantial levels of inherent uncertainties in weather forecast data and component damage models, which affect both the effectiveness of the final preventive operation strategies as well as the model?s computational efficiency. Moreover, the computational time in real-time power system operation is extremely limited. The hypothesis governing this project is that a hybrid approach, which exploits the availability of data, while also relying on physical and engineering principles, can overcome the two challenges, mentioned above. First, using historical and real-time data, this project reduces uncertainties in order to improve both the solution quality and the computational needs. Second, by applying machine learning techniques on synthetic, historical, and real-time data, the project replaces computationally-demanding stochastic optimization with effective proxy deterministic models, to achieve computational tractability for real-time operation. Thus, this project will enable, for the first time, appropriate integration of uncertain weather forecast data within power system operation during hurricanes, to quickly identify effective preventive operation strategies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811552","Innovated Statistical Inference for Complex and High-Dimensional Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2018","08/09/2018","Lingzhou Xue","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","08/31/2021","$130,000.00","","lzxue@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","MPS","1269, 7454","068Z","$0.00","Increasing amounts of high-dimensional data are being collected and analyzed in a diverse range of research areas. In practice, data scientists face significant analytic challenges when exploring and understanding the complex and high-dimensional data. Statistical inference of high-dimensional data is essential in theoretical and applied research of statistics, biostatistics, econometrics, geoscience, machine learning, signal processing, and many others. Big Data has rapidly reshaped statistical modeling and revolutionized statistical analysis. There exist many challenges and open problems, whose solutions require innovative ideas and techniques. This project will address new challenges arising in high-dimensional hypothesis testing. <br/><br/>Testing high-dimensional structural parameters plays a vital role in estimating and quantifying uncertainty, making informed choices, and discovering knowledge from Big Data. In this project, novel statistical methods and theory are developed to study three important topics of high-dimensional hypothesis testing: (1) power enhancement tests for high-dimensional covariance matrices, (2) power enhancement tests for high-dimensional mean vectors, and (3) nonlinear statistical dependence of high-dimensional data. The research outcomes will provide powerful analytic tools for solving open problems in three research topics. The methods and theory are general, and they can be directly extended to address other important hypothesis testings for high-dimensional data such as testing in high-dimensional spiked models. Software packages will be developed to make the research outcomes readily available to other researchers and practitioners.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1801697","Commutative Algebra:  F-Regularity in Algebraic Geometry and Non-Commutative Algebra","DMS","ALGEBRA,NUMBER THEORY,AND COM","07/01/2018","08/08/2019","Karen Smith","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Janet Striuli","06/30/2021","$217,027.00","","kesmith@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","1264","9251","$0.00","This project investigates polynomials and the geometric shapes they define, called algebraic varieties.  The problems belong to the field of mathematics called algebraic geometry.  Algebraic geometry has many applications throughout industry and national security, including to error-correcting codes, machine learning, cryptography, computer aided design, and 3D printing. Some of these applications are concerned with studying the sets where polynomials are zero module certain prime numbers, the particular focus of this project. This project is basic research into understanding the singularities of algebraic varieties defined over finite fields, including criteria for understanding how far the varieties are from being smooth, and techniques for circumventing the failure of smoothness in some cases. The project includes research designed to be student-ready to provide training for the next generation of mathematicians.<br/><br/>In more precise terms, the project investigates strong F-regularity in several contexts in non-commutative algebra, combinatorics, and birational algebraic geometry. It investigates whether or not, for a strongly F-regular local ring R of prime characteristic, the algebra A of R-linear endomorphisms of F*R, where F is the Frobenius map, is a non-commutative resolution of singularities in the sense of Van den Bergh. In addition, a theory of derived functors of differential operators will be investigated, with an eye toward using it to show that rings of differential operators over fields of characteristic zero can be reduced to characteristic p under some conditions. Finally, building on recent work in valuation rings, the project aspires to show that certain section rings of varieties are always finitely generated, an important step in the minimal model program in characteristic p.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1754261","Collaborative Research: Refining Geothermobarometry in Pyroxenes using In Situ Measurements of Fe3+","EAR","Petrology and Geochemistry","07/01/2018","06/21/2018","Melinda Dyar","MA","Mount Holyoke College","Standard Grant","Jennifer Wade","06/30/2021","$172,000.00","","mdyar@mtholyoke.edu","50 College Street","South Hadley","MA","010756456","4135382000","GEO","1573","","$0.00","This project promotes the progress of science by providing calibrations for use at a national facility, the Advanced Photon Source at Argonne National Laboratory; these results will also be useful for users of other synchrotron facilities worldwide. The calibrations address the scientific question of how much oxygen was present when a mineral crystallized by analyzing a proxy, which is the oxidation state of the multivalent element iron. This understanding has vital importance for understanding how oxygen controls the crystallization path and composition of cooling magmas, also providing insight into processes that may have operated on and in the magma as it moved to the surface. Graduate and undergraduate students at Mount Holyoke College, the University of Tennessee at Knoxville, and the University of Idaho will be supported by this project, including at least three women.<br/> <br/>The ability to measure redox states at sub-nm and pm scales is a formidable analytical challenges due to the inherent anisotropy of most rock-forming minerals, which causes their crystal structures to interact with photons differently according to crystal orientation. This project studies oriented single crystals to explore the effects of crystallographic orientation on x-ray absorption spectroscopy measurements, focusing on the pyroxene mineral group, one of the most common phases in igneous rocks. The team will use these data to build a universal calibration appropriate for quantifying the partial pressure of oxygen (oxygen fugacity) in pyroxene-bearing samples. This work has the potential to advance knowledge in multiple geological disciplines. It will also enable synchrotrons around the world that use the Athena software package to quickly and easily determine Fe3+ contents of pyroxene minerals with known precision and accuracy. Finally, this project applies machine learning techniques to the study of spectroscopic data. Development of this methodology is exportable to other fields, such as medical, biological, and forensic uses of spectroscopy, where it has the potential for societal impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1901202","RAPID: Collaborative Research: Assessing Chemical and Microbiological Contamination in Environmental Waters in Eastern North Carolina after Hurricane Florence","CBET","EnvE-Environmental Engineering, Special Initiatives","12/01/2018","11/29/2018","Diego Riveros-Iregui","NC","University of North Carolina at Chapel Hill","Standard Grant","Karl Rockne","11/30/2019","$50,303.00","Jill Stewart","diegori@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","ENG","1440, 1642","080Z, 7914","$0.00","Hurricane Florence passed through southeast North Carolina in 2018 resulting in widespread flooding. The counties most affected by flooding house thousands of swine and poultry confined animal feeding operations (CAFOs). These areas are also home to many low-income, African American, Hispanic, and American Indian communities. The objective of this research is to study the spread of contaminants into the environment caused by Hurricane floodwaters. The pollutants assessed include heavy metals from coal ash, emerging chemical contaminants related to industrial and municipal wastewater, and pathogens and nutrients associated with CAFOs. This project will result in improved understanding of the risks posed by extreme flooding. Local communities affected by the flooding will be engaged in this research through the identification of sampling sites and communication of research results. The results will have broader impact through the identification of waste management operations that can be modified to prevent such releases in future flooding events.<br/><br/>The central aim of this work is to assess microbial and chemical contamination of the environment near livestock and industrial operations flooding caused by Hurricane Florence floodwaters. A secondary aim is to understand the relative impacts of different land-usage and livestock waste management practices on contaminant loading. To achieve these aims, this research project has the following objectives: (1) Collect samples to quantify spatiotemporal variability in molecular targets for microbial contamination (i.e., human-, poultry-, and swine-specific fecal microbial source tracking targets and antimicrobial resistance genes), nutrients (N and P), heavy metals, and chemical contaminants of emerging concern; (2) Identify spatial relationships between contamination, flooding extent, land-use (e.g., CAFO densities, industrial sites, urban areas) and manure management practices using satellite imagery and machine learning techniques; (3) Assess persistence of biological and chemical species in the natural environment post-flooding. To achieve these objectives, the research team will sample across 50 sites in multiple coastal plain river basins where intense flooding occurred after Hurricane Florence to map advanced chemical and biological water quality indicators in high spatial resolution.  The team sampled within a week after the hurricane, and then will sample after ~1 month, ~2 months, and ~6 months. Knowledge gained from this study will directly inform improvements to emergency management protocols post-hurricane in landscapes with many contaminant sources. The work will have broader impact through the promulgation of waste management recommendations that account for the increasing likelihood and severity of extreme flooding events expected in the future as our climate changes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819006","International Congress on Mathematical Software (ICMS 2018)","DMS","COMPUTATIONAL MATHEMATICS","07/15/2018","07/16/2018","Jonathan Hauenstein","IN","University of Notre Dame","Standard Grant","Leland Jameson","06/30/2019","$20,000.00","","hauenstein@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1271","7556, 9263","$0.00","The International Congress on Mathematical Software (ICMS) consists of a diverse community in the broadly defined area of computational sciences including researchers and practitioners in computational mathematics, algorithms and complexity, and software engineering.  This National Science Foundation award provides support for the 2018 meeting of the International Congress on Mathematical Software (ICMS 2018) to be held July 24-27, 2018 at the University of Notre Dame in Indiana.  ICMS 2018 is the 6th meeting of ICMS and the first to be hosted in the United States.  This conference, as with the 5 previous meetings of ICMS, will bring together a wide range of computational scientists along with practitioners to discuss recent progress and current challenges in software, algorithms, complexity, and applications in computational mathematics and computational science.  This conference provides an opportunity for cross-fertilization between the mathematical sciences and other areas of science and engineering through software development and utilization.  The plenary speakers for this conference are Folkmar Bornemann (TU Munich), Thomas Hales (University of Pittsburgh), and William Stein (SageMath, Inc.).  This award will support travel grants for graduate students, post-doctoral researchers, young faculty members, members of under-represented groups, and researchers without federal support to attend ICMS 2018 to meet with leaders in software development for mathematics, science, and engineering applications, and interact with each other who will be the next generation of leaders in this area.<br/><br/>ICMS 2018 is an international meeting for interaction between computational mathematicians, scientists, engineers, and software developers.  This creates a unique atmosphere for sharing ideas and discussing new trends and challenges in computational mathematics and software development. For example, plenary speaker William Stein will provide attendees of ICMS 2018 insights into successes and challenges of open-source mathematical software development.  Apart from the plenary speakers, all talks and posters at ICMS 2018 are selected from responses to an open call for submissions which are organized into sessions.  ICMS 2018 features 18 sessions such as machine learning for mathematical software, software for mathematical reasoning, computational algebraic geometry, post-quantum group-based cryptography, and management of mathematical software, mathematical knowledge, and research data.  ICMS 2018 also includes a poster session which is scheduled for the first day of the conference.  The conference website is http://icms-conference.org/2018/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830129","Towards Modeling & Simulation-Enabled Design of Intelligent Robots  A Meeting Dedicated to Identifying Opportunities, Summarizing Challenges, and Brainstorming for Impactful Di","IIS","NRI-National Robotics Initiati","04/01/2018","03/09/2018","Dan Negrut","WI","University of Wisconsin-Madison","Standard Grant","David Miller","03/31/2020","$29,917.00","","negrut@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8013","7556, 8086","$0.00","Robots are becoming increasingly expected to operate in complex, unstructured environments.  To successfully achieve their tasks, they must use models to plan and control their actions.  For reliable performance, these models should adequately capture the types of interactions the robot has with the world.<br/>This workshop brings together researchers in the areas of robotics, modeling and simulation (M&S), and machine learning (ML), to examine the roles that M&S and ML play in robotic planning and control, and how these two different approaches can be combined to produce robust robot behavior. Approximately 35 researchers from academia and industry, from various backgrounds, are invited to participate in open discussion and brainstorming sessions.  <br/>The one-day format, held at the National Institute of Standards and Technology (NIST), includes overview talks and breakout sessions.  The outcome will be a report summarizing the main ideas and themes that emerge from the discussions, with the aim of publishing the results in a robotics journal.<br/><br/><br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812987","CSR: Small: IOQL: an I/O Interface for Near-Data Processing","CNS","CSR-Computer Systems Research","08/15/2018","06/29/2018","Hung-Wei Tseng","NC","North Carolina State University","Standard Grant","Sandip Kundu","08/31/2019","$499,515.00","","htseng@ucr.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","7354","7923","$0.00","As data sets grow, the overhead of moving data around different units in computers significantly affect the execution time of applications. Performing computation near the source location of data as much as possible can fundamentally minimize the demand of data movement. However, doing this is difficult with the existing computer systems. This project will develop a Input/Output Query Language (IOQL) to provide a natural language and interface for users to easily describe the tasks to compute near data storage. The IOQL infrastructure optimizes and coordinates the allocation of all tasks in the system. IOQL will boost the performance of many critical datacenter applications. <br/><br/>IOQL will achieve its goal through presenting a query language with easy-to-understand syntax, application programming interfaces for popular programming languages, a query engine that optimizes and assigns tasks among heterogeneous computing resources during runtime, kernel modules to interact with data storage/memory devices with computing facilities and architectural support of IOQL. IOQL will evaluate the proposed concepts by implementing real system prototype with extended data storage devices and memory controllers. This project will measure the performance, power and energy consumption for data-intensive compute kernels, machine learning frameworks and database systems.<br/><br/>IOQL will be made compatible with existing computing systems, especially data centers, to address the demand of real-world applications for financial (e.g., business intelligence), scientific (e.g., simulation), and public health (e.g., genomics) applications. The design of IOQL minimizes the amount of additional costs of supporting the proposed tasks but maximizes extensibility of the framework, making IOQL an ideal long-term solution for similar problems.<br/><br/>This project will publish research results in related peer-reviewed conferences, journals and workshops that allow public to access. This project will also make the stabilized source code of IOQL and all related modules available on https://github.com/ESCALNCSU. The research group plans to retain all related data for at least three years.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761068","CDS&E: Better by Design: Establishing Modeling and Optimization Techniques for Producing New Classes of Biomimetic Nanomaterials","CMMI","CONDENSED MATTER & MAT THEORY, CDS&E","09/15/2018","05/20/2019","Wenxiao Pan","WI","University of Wisconsin-Madison","Standard Grant","Joanne Culbertson","08/31/2021","$554,732.00","Benjamin Peherstorfer","wpan9@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","1765, 8084","024E, 026Z, 067E, 7234, 7237, 8084, 9216, 9263","$0.00","Novel nanomaterials with precisely tailored characteristics can enable innovation in areas ranging from manufacturing to energy storage and drug delivery but designing such materials can be a challenge. This project develops modeling and optimization techniques that will enable researchers to use desired properties to drive materials and process selection. The researcher harnesses the power of mesoscale modeling techniques and computational methods based on Bayesian machine learning and stochastic optimization to search the vast universe of options to identify promising candidates. This approach gives the community an enabling predictive tool for analysis and design of polymer-based nanomaterials. The knowledge gained from this research will be broadly disseminated through publications, conference presentations and by organizing symposia. Educational and outreach programs will be developed to train a diverse STEM workforce and to broaden participation of underrepresented students in the fields of engineering and computational science.<br/> <br/>Integrating a mesoscale coarse-graining method with stochastic optimization provides an enabling tool in soft materials and advances knowledge about design exploration in high-dimensional search spaces and design optimization under uncertainty. The goal of this project is to significantly reduce the cost of simulating the molecular self-assembly process and the characteristics of assembled materials. The researcher will develop a mesoscale model that describes the dynamics of self-assembly and simulates and predicts the structures and mechanical properties of assembled materials. A coarse-grained approach balances the need for accuracy in material properties, which is the basis for optimization, and the computational efficiency needed to make the optimization feasible. The computational framework, which includes the mesoscale modeling, classification, and optimization steps, will be validated by comparison with experiments on peptoids. This research will enable inverse design of peptoid-based biomimetic nanomaterials with precisely tailored structures and properties for applications such as chemical/biological sensors, biomimetic nanodevices and water/ion transport membranes. The computational methodology will be shared through GitHub and as LAMMPS subroutines and the computer codes will be released to the scientific community as open-source software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816833","SHF:SMALL:Collaborative Research: Exploring Nonvolatility of Emerging Memory Technologies for Architecture Design","CCF","Software & Hardware Foundation","08/01/2018","07/16/2018","Yuan Xie","CA","University of California-Santa Barbara","Standard Grant","Yuanyuan Yang","07/31/2021","$249,000.00","","yuanxie@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798","7923, 7942","$0.00","In modern computers, by combining the speed of traditional cache technology, the density of traditional main memory technology, and the non-volatility of flash memory, a new class of emerging byte-addressable nonvolatile memories (NVMs) have great potential to be used as the universal memories of the future. Such memory types include technologies such as phase-change memory, spin-transfer-torque magnetoresistive memory, and resistive memory. As these emerging memory technologies mature, it is important for computer architects to understand their pros and cons in a comprehensive manner in order to improve the performance, power, and reliability of future computer systems incorporating these systems which will be used in various application domains. Yet, most of previous research on NVM architecture is focused only on the performance, power, and density benefits and how to overcome challenges, such as write overhead and wearout issues. The non-volatility characteristic of NVM technologies is not fully explored. Therefore, this project examines how to exploit the non-volatility characteristic that distinguishes the emerging NVM technologies from traditional memory technologies, and investigate new memory architecture design with novel applications.<br/><br/>The goal of this project is to advance the memory architecture design of various types of computer systems with a full exploration of the non-volatility characteristic of NVM technologies across architecture, system, and application levels. To this end, the project explores the design space of various types of computer systems, ranging from severs to embedded systems.  In particular, the project identifies and addresses design issues in nonvolatile cache architecture, re-architects main memory structure to leverage the non-volatility characteristic to improve system performance and energy consumption, supports persistent memory systems in various use cases with emerging NVM technologies, and studies near-data-computing techniques applied for these NVM technologies. The successful outcome of this research is expected to provide the design guidelines for enabling both large capacity and fast-bandwidth nonvolatile memory/storage, which are beyond the present state-of-the-art. Consequently, the research will spawn new applications involving the computation on the exascale of data, e.g., data mining, machine learning, visual or auditory sensory data recognition, bio-informatics, etc.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827230","RAPID: Coping in Families Affected by California Wildfires and Flooding","BCS","Social Psychology","04/01/2018","04/11/2018","Charles Benight","CO","University of Colorado at Colorado Springs","Standard Grant","Steven J. Breckler","03/31/2021","$158,604.00","Terrance Boult","benight@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","SBE","1332","7914","$0.00","Severe natural disasters carry great economic costs and threaten the health and well-being of many people.  Severe natural disasters during the past decade include unprecedented flooding, catastrophic hurricanes, and devastating wildfires.  The worst wildfire in California history struck Ventura and Santa Barbara counties in December 2017.  The Thomas Fire in those counties burned 282,000 acres, took two lives, and incinerated or damaged over one thousand structures. Because of its location to major population centers, over 100,000 residents were evacuated, many more than once.  In the aftermath of the fire, rain produced a massive mudslide cascading through the city of Montecito killing 23 people and destroying 115 homes.  These catastrophic events within a geographically concentrated region offer a unique opportunity to better understand how individuals and families cope with and recover from profound stress.  The focus of this project is to gain a richer understanding of how individuals and families manage post-disaster recovery and what predicts both positive and negative outcomes. By bringing together a multi-disciplinary team of computer scientists and disaster psychologists, new theories and computational methods will be evaluated to better understand human response to extreme events and to inform the development of future interventions.<br/><br/>Using Smart Phone technology and online surveys over a six-month period of time, 100 parent/child pairs and 100 non-parent survivors in Ventura and Santa Barbara counties will be followed.  The surveys and the daily ""check-ins"" will provide critical data on how people cope with stress across time.  By cultivating a very rich set of data on the same individuals over time, it will be possible to discern patterns of recovery in ways that past research has not done.  Measuring outcomes on a daily basis will generate significant data and support the use of ""big data"" analysis methods (e.g., machine learning techniques) that can identify unique changes or shifts in functioning predicted by the guiding theoretical framework (self-regulation shift theory).  This project extends what is currently understood related to post-disaster recovery by targeting key coping mechanisms of change and highlighting possible critical targets for supportive interventions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828066","MRI: Acquisition of a Hybrid Real-Time Simulator for Real-Time Power Grid Simulations","ECCS","Major Research Instrumentation","09/15/2018","09/14/2018","Dongliang Duan","WY","University of Wyoming","Standard Grant","Anil Pahwa","08/31/2021","$1,033,396.00","Matt Donnelly, Shaun Wulff, John Pierre, Suresh Muknahallipatna","dduan@uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","ENG","1189","1189, 9150","$0.00","This MRI proposal requests funds for the acquisition of a hybrid real-time digital simulator, which is a combination of specialized high-performance computing (HPC) hardware and software tools capable of performing real-time power grid and hardware-in-loop simulations with various external devices. The equipment will be located at the University of Wyoming (UWyo) and made accessible online for open-access use by investigators at UWyo, Montana Tech, other academic institutions and power companies worldwide. The acquisition of the requested hybrid real-time power system digital simulator will promote interdisciplinary collaboration efforts to build our next-generation smart power grid. In addition, the curricula in the power engineering area will be updated with better demonstrations of power system operations to students and therefore attract and provide competent workforce for the challenges of our next-generation power system. The open accessibility of the equipment will also promote collaborations among researchers. Meanwhile, the integration of research and education will help attract undergraduate students for higher education. Furthermore, given that UWyo is the only provider of baccalaureate and graduate education and research in Wyoming, acquisition of the proposed instrument is expected to have a tremendous impact on the training of future scientists at the high school, community college, and the undergraduate level. UWyo actively promotes college preparedness, access, and success among students traditionally underrepresented in STEM fields by focusing special programs on first-generation, female, low-income, and ethnic minority students.<br/><br/>The acquisition of the hybrid real-time digital simulator will provide a great platform for research efforts in many different areas including data mining, statistical signal processing, high-performance computing and wind energy towards power grid modernization. Specifically, the capability of the proposed instrumentation to perform both real-time power grid and hardware-in-loop simulations is vital for a variety of projects currently underway in several departments at the UWyo and Montana Tech, including 1) the development of machine learning and statistical signal processing algorithms using synchrophasor measurements for reliability analysis and dynamic wide-area situational awareness; 2) the invention of advanced measurement and analyzing techniques based on point-on-wave measurements for both transient and steady-state analysis; 3) the application of modern high-performance computational technologies for real-time analysis of the grid; 4) the integration of improved system models including renewable generation sources for different time scale power system stability studies; and 5) other large system simulation studies. The research team at UWyo and Monata Tech has an excellent record of contributions to power system stability analysis and high-performance computation over decades. However, their past research has mostly been based on actual field measured data and on smaller power grid models implemented in software. This new hybrid simulator will provide them a powerful tool to take this research to a more in-depth level in multiple ways. It will enable them to simulate larger power grids in finer detail and more complex scenarios. A hybrid simulator also enables them to interface with actual hardware devices such as PMUs used in grid monitoring. Furthermore, the hybrid simulator has the capability to interface with other real-time digital simulators as a network simulator to conduct smart grid communication co-simulations. All these add tremendous new dimensions to their research, putting them in a position to continue to make high impact contributions to the US power grid.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808266","Collaborative Research: Data driven control of switched systems with applications to human behavioral modification","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2018","08/27/2018","Constantino Lagoa","PA","Pennsylvania State Univ University Park","Standard Grant","Radhakisan Baheti","08/31/2021","$250,000.00","David Conroy","lagoa@engr.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","7607","092E","$0.00","Dramatically increasing health care costs threaten the nation's economy. Over 80% of those costs are due to chronic illnesses which can be prevented or mitigated through lifestyle change. Physical activity is also a key behavioral component of ideal cardiovascular health. This suggests that promoting physical activity through the personalized virtual health advisors can lead to substantial health improvements across a broad spectrum of the population. Motivated by these observations, this proposal seeks to develop a tractable, practical framework for designing personalized behavior monitoring systems, aimed at maintaining optimal levels of physical activity. This is accomplished by embedding the problem into a more general, systems-theoretic one:  design of controllers with provable performance for systems characterized by a collection of models where neither the number of models nor their parameters are a priori known and must be obtained from experimental data, collected from multiple sensors with large variations in quality. Education is proactively integrated into this project, starting with STEM summer camps projects for urban middle school students on data driven modeling and continuing at the college level with a multi-disciplinary program that uses personalized medicine to link a full range of distinct subjects ranging from machine learning to systems theory and optimization. At the graduate level, these activities are complemented by recruitment efforts that leverage the resources of Penn State's McNair Scholars Program and Northeastern University's Program in Multicultural Engineering to broaden the participation of underrepresented groups in research. <br/><br/>Motivated by the problem of designing effective behavioral interventions, this proposal seeks to develop a comprehensive, computationally tractable framework for synthesizing data driven control laws for a class of systems described by switched difference inclusions. These models arise in a broad class of domains, ranging from resilient infrastructures to health care, characterized by large amounts of uncertainty and abruptly changing dynamics. The research addresses both the identification and control design problems in a unified framework based on polynomial optimization and its connections to the problem of moments. Contributions to the field of identification include the development of a tractable framework for robust identification of uncertain switched systems that exploits the underlying structure of the problem to substantially reduce the computational complexity and can handle both worst case and risk-adjusted descriptions. Contributions to control include a new framework for chance constrained control of uncertain switched systems that maximizes the probability of achieving a desired final state, while, at the same time, minimizing the probability of entering bad sets. As a proof-of-principle, the resulting framework is applied to the problem of designing smartphone based virtual health advisors capable of providing individualized optimal physical activity strategies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1825521","Collaborative Research: Knowledge and Data-driven Design of Mechanical Metamaterials","CMMI","EDSE-Engineering Design and Sy","09/01/2018","08/27/2018","Daniel Selva Valero","TX","Texas A&M Engineering Experiment Station","Standard Grant","Kathryn Jablokow","08/31/2021","$280,463.00","","ds925@cornell.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","072Y","067E, 068E, 073E","$0.00","Traditionally, materials have been designed through choices of molecular level composition and structure. With the advent of increasingly sophisticated material-forming techniques like additive manufacturing, structures repeated at microscale are also now being used to realize effective overall properties; these materials are termed ""metamaterials"". The most obvious gain for metamaterials versus traditional fully dense materials is in weight and material consumption, but there are also responses that are simply not achievable with fully dense polymers. In particular, 3D printing of polymers has reached a critical threshold of quality, speed, and size at which it can be used for production rather than just prototyping. The geometry within the repeated structural cell of a metamaterial critically influences the overall properties. Determining the optimal geometry requires a design framework distinct from that used for dense materials. This work will explore innovative ways of combining expert knowledge (i.e., physical laws, models, heuristics) and databases of actual and simulated material behaviors, using advanced machine learning and search algorithms to foster the discovery of metamaterials with desired properties. Progress in the project will promote the new field of data-driven design as well as advance the national health, prosperity, and welfare by facilitating the design of advanced materials with hitherto unknown, yet desirable combination of properties. Beyond this technological impact, this grant will serve to prepare the next generation of students for a new era of design for intelligent materials and structures. Doctoral, undergraduate, high school, and middle school students will be reached through in-lab research experiences and design outreach activities.<br/><br/>The central objective of this work is to create a design method for 3D printable elastomeric metamaterials that leverages both available engineering knowledge and data. The design space of interest will include two distinct geometry classes -- lattice materials and minimum energy surfaces. The methodology in this project will leverage physics-based models, existing knowledge, and data to minimize the resources needed to reach an acceptable design. The intermediate research objectives are to: (1) formulate and validate a comprehensive set of low computational cost mechanics models for lattice and minimum surface energy style metamaterials, together with a set of heuristics for designing such materials; (2) develop data-driven surrogate models and identify sources of and quantify uncertainty in predicted mechanical properties of 3D printed mechanical metamaterials; (3) develop knowledge representations and data fusion strategies to incorporate expert knowledge including physical laws, heuristics, and beliefs into the design of 3D printed metamaterials. In contrast to the current state-of-the-art for metamaterial design, the design framework that is produced by this grant will be well oriented to accommodate large deformation. This will facilitate design of printed metamaterials for properties such as toughness and failure strain.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828163","MRI: Acquisition of Hardware for the Enhancement of the ELSA High Performance Computing Cluster to Enable Computational Research at The College of New Jersey","OAC","Major Research Instrumentation","09/01/2018","08/23/2018","Joseph Baker","NJ","The College of New Jersey","Standard Grant","Stefan Robila","08/31/2021","$651,032.00","Paul Wiita, Wendy Clement, Michael Ochs, Michael Bloodgood","bakerj@tcnj.edu","P.O. Box 7718","Ewing","NJ","086280718","6097713255","CSE","1189","026Z, 1189","$0.00","The College of New Jersey (TCNJ) will acquire equipment to significantly upgrade and enhance the Electronic Laboratory for Science and Analysis (ELSA) High Performance Computing cluster. TCNJ is a primarily undergraduate institution promoting a deep engagement of undergraduate students in research. Many of TCNJ's School of Science faculty members are working at the cutting edge of computational research in their fields, which include a broad range of areas including biochemistry/biophysics, genetics, bioinformatics, astrophysics, machine learning, and mathematical biology. In order to maintain a diverse and state of the art resource that meets the current and future computational needs of TCNJ's faculty and undergraduate students the current ELSA cluster requires targeted hardware enhancements. The new instrument will (1) enhance the research capacity and resulting scientific discovery of TCNJ's School of Science faculty members and their undergraduate research teams; (2) expose a greater number of undergraduate students and researchers to this powerful computational infrastructure through a series of newly developed High Performance Computing and data visualization short courses and workshops; and (3) improve access to the ELSA cluster for students traditionally underrepresented in STEM, as well as to researchers beyond TCNJ through a new collaboration with Open Science Grid.<br/><br/>This project will expand the research programs of more than 13 faculty members (many of whom are early career faculty) spanning all five of TCNJ's School of Science departments. The computationally intensive work that will be supported through this project includes a diverse array of scientific efforts, including studies of pilus biomechanics, estimations of cell signaling processes, methods for investigating the strength of passwords and security of password systems, and improving our understanding of the most energetic objects in the universe. Currently, the research programs of TCNJ faculty members in these and other areas are restricted by inadequate graphic processing unit (GPU) resources and by the slow speed aging central processing unit (CPU) servers part of the current instrumentation. The new ELSA cluster will allow faculty and student researchers at TCNJ to run workflows ranging from embarrassingly parallel computations, to those that necessitate high levels of parallelization over hundreds of cores, intensive GPU computations, and remote visualization of simulation results. The instrument will thus ensure that these research programs are able to reach their full potential. This project will also benefit nearly 100 undergraduate student researchers each year who are part of these labs and work directly on the cluster. As a result, in addition to improving the capacity for scientific discovery, the proposed acquisition will help TCNJ meet the demands of developing an undergraduate workforce that is ready to leverage increasingly powerful High-Performance Computing resources, now and in their future careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815643","SHF: Small: Energy Saving in Heterogeneous Data Centers","CCF","Software & Hardware Foundation","10/01/2018","07/02/2018","Daniel Wong","CA","University of California-Riverside","Standard Grant","Yuanyuan Yang","09/30/2021","$499,375.00","Laxmi Bhuyan","danwong@ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7941","$0.00","Many critical online services are turning to cloud infrastructure to meet scalability demands. In order to sustain cloud computing growth, it is necessary to scale the computational capacity, and improve the energy efficiency, of data centers. Modern data centers increasingly integrate accelerators, such as Graphical Processing Units (GPUs), with traditional CPUs to provide unprecedented parallelism and order-of-magnitude improvement to computational throughput. In order to manage software workloads and hardware resources at cloud-scale, data centers are increasingly being virtualized to provide ease of software and hardware management. However, existing energy efficiency techniques are not well-suited for virtualization technology and heterogeneous hardware. This project provides fundamental insights and solutions towards achieving energy-efficient computing for emerging virtualized heterogeneous data centers. This project has wide-reaching benefits for the computational engines behind many workloads of national interest, such as weather forecasting and machine learning. Results of this research are being integrated into the existing undergraduate and graduate courses.<br/><br/>The research consists of three main thrusts: 1) development of heterogeneous benchmarks to evaluate server energy profile and metrics to quantify heterogeneous server energy proportionality in order to investigate the implications of virtualization and heterogeneous architectures, 2) development of software support for container live migration and performance-aware migration strategies to minimize live migration overhead, 3) development of management techniques to maximize energy proportionality of heterogeneous CPU and multi-accelerator systems, such as dynamic load balancing and automatic runtime task migration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811729","AF: Small: RUI: New Directions in Kolmogorov Complexity and Network Information Theory","CCF","Algorithmic Foundations","10/01/2018","05/21/2018","Marius Zimand","MD","Towson University","Standard Grant","Joseph Maurice Rojas","09/30/2021","$236,222.00","","mzimand@towson.edu","8000 York Road","Towson","MD","212520001","4107042236","CSE","7796","7923, 7926, 7927, 9229","$0.00","Data communication in systems with multiple senders and receivers raises difficult problems caused by the possibility of complex data correlation patterns, network topologies, and interaction scenarios. Traditionally, these problems have been tackled using tools from Information Theory (IT), which assumes that the data has been produced according to a certain generative model.  In practice, most of the time the generative model is not known. Even if it is known, it is often more complicated than the models typically used in theoretical studies. This project will use tools from Algorithmic Information Theory (AIT, also known as Kolmogorov complexity), which equates the information in a piece of data with its minimal description length.  The advantage is that the new approach does not rely on any model, and consequently the results obtained this way are valid in more general circumstances. The problems that will be studied are of interest to computer scientists, electrical engineers, and mathematicians. The project will promote a deep and dynamic exchange of ideas between these communities. This project will produce new insights in Kolmogorov complexity and communication complexity. These areas have applications in computational complexity, machine learning, constructive combinatorics, and other fields. The results will likely have an impact in many of these areas. The project will allow undergraduate and graduate students to participate in research activities that have a strong theoretical flavor and the promise of real-world applications.<br/><br/>The project is timely and realistic because it has recently become apparent that some interesting questions (for instance, source coding of non-ergodic sources), which cannot be approached with tools based on Shannon entropy, can be solved in the AIT framework. In the other direction, there has been recent progress in some classical problems in Kolmogorov complexity inspired from results and techniques from IT. The project will: (1) isolate, understand, and develop certain aspects of Kolmogorov complexity that are particularly relevant for data communication; (2) use the newly-developed tools to make progress in outstanding problems in network communication such as channel coding, network coding, interactive protocols, and others; and (3) use the insights from the communication setting to advance the theory of Kolmogorov complexity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763314","AF: MEDIUM: Collaborative Research: Foundations of Adaptive Data Analysis","CCF","Algorithmic Foundations","03/01/2018","04/25/2020","AARON ROTH","PA","University of Pennsylvania","Continuing Grant","Tracy Kimbrel","02/28/2021","$378,000.00","Weijie Su","aaroth@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1748988","CAREER: Associative In-Memory Graph Processing Paradigm: Towards Tera-TEPS Graph Traversal In a Box","CCF","Software & Hardware Foundation","02/01/2018","09/17/2019","Jing Li","WI","University of Wisconsin-Madison","Continuing Grant","Sankar Basu","01/31/2023","$290,278.00","","janeli@seas.upenn.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7798","1045, 7945, 9102","$0.00","Large-scale graph analytics, the class of big data analytics that essentially explores the relationship among a vast collection of interconnected entities (e.g., ""friends"" in a social network), is becoming increasingly important due to its broad applicability, from machine learning to web search, precision medicine, and social sciences. However, the performance of graph processing systems is severely limited by the irregular data access patterns in graph computations. The existing solutions that have been developed for mainstream parallel computing are generally ineffective for massive, sparse real-world graphs due to the conventional computer architecture (i.e., von Neumann architecture) itself. In this project, new, fundamental methods will be explored in both theoretical and practical implementations to address this problem. It uniquely advances multiple fundamental cross-disciplinary areas in device, circuit, computer-aided design, and computer architecture and can be applied to address some of the most challenging ""big data"" problems ranging from fundamental research to everyday life. The research framework will be extended into an educational platform, providing a user-friendly framework for a laboratory-based curriculum and will serve the educational objectives for K-12 students, undergraduate and graduate students.<br/><br/>In this research, a new computing paradigm will be developed to fundamentally address the challenge in processing large-scale graphs and to achieve ultra-high computing efficiency, orders of magnitude higher in performance per watt than state-of-art mainstream computer. To this end, a holistic co-design and optimization of algorithm, software and hardware will be developed to leverage the great potential of emerging nonvolatile memory technology. A new computing model will be proposed and theoretically proven to be more efficient in runtime/area/energy than traditional von Neumann architecture in performing graph computation. Detailed micro-architectures and circuits will be designed and evaluated to best implement the proposed computing model for concept proof."
"1808381","Collaborative Research: Data Driven Control of Switched Systems with Applications to Human Behavioral Modification","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2018","08/27/2018","Mario Sznaier","MA","Northeastern University","Standard Grant","Radhakisan Baheti","08/31/2021","$250,000.00","Octavia Camps","msznaier@coe.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","ENG","7607","092E","$0.00","Dramatically increasing health care costs threaten the nation's economy. Over 80% of those costs are due to chronic illnesses which can be prevented or mitigated through lifestyle change. Physical activity is also a key behavioral component of ideal cardiovascular health. This suggests that promoting physical activity through the personalized virtual health advisors can lead to substantial health improvements across a broad spectrum of the population. Motivated by these observations, this proposal seeks to develop a tractable, practical framework for designing personalized behavior monitoring systems, aimed at maintaining optimal levels of physical activity. This is accomplished by embedding the problem into a more general, systems-theoretic one:  design of controllers with provable performance for systems characterized by a collection of models where neither the number of models nor their parameters are a priori known and must be obtained from experimental data, collected from multiple sensors with large variations in quality. Education is proactively integrated into this project, starting with STEM summer camps projects for urban middle school students on data driven modeling and continuing at the college level with a multi-disciplinary program that uses personalized medicine to link a full range of distinct subjects ranging from machine learning to systems theory and optimization. At the graduate level, these activities are complemented by recruitment efforts that leverage the resources of Penn State's McNair Scholars Program and Northeastern University's Program in Multicultural Engineering to broaden the participation of underrepresented groups in research. <br/><br/>Motivated by the problem of designing effective behavioral interventions, this proposal seeks to develop a comprehensive, computationally tractable framework for synthesizing data driven control laws for a class of systems described by switched difference inclusions. These models arise in a broad class of domains, ranging from resilient infrastructures to health care, characterized by large amounts of uncertainty and abruptly changing dynamics. The research addresses both the identification and control design problems in a unified framework based on polynomial optimization and its connections to the problem of moments. Contributions to the field of identification include the development of a tractable framework for robust identification of uncertain switched systems that exploits the underlying structure of the problem to substantially reduce the computational complexity and can handle both worst case and risk-adjusted descriptions. Contributions to control include a new framework for chance constrained control of uncertain switched systems that maximizes the probability of achieving a desired final state, while, at the same time, minimizing the probability of entering bad sets. As a proof-of-principle, the resulting framework is applied to the problem of designing smartphone based virtual health advisors capable of providing individualized optimal physical activity strategies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751143","CAREER: Reliable and Efficient Data Encoding for Extreme-Scale Simulation and Analysis","OAC","CAREER: FACULTY EARLY CAR DEV","04/15/2018","04/09/2018","Seung Woo Son","MA","University of Massachusetts Lowell","Continuing Grant","Alan Sussman","03/31/2023","$334,234.00","","SeungWoo_Son@uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","1045","026Z, 062Z, 1045","$0.00","Transformative research in science and engineering to address challenges of our time, such as designing new combustion systems, depends on progressively sophisticated computational models and simulations that operate on high performance computing systems.  These simulations and analyses are increasingly constrained by the massive volumes of data that they must use, generate, and analyze.  To manage this enormous amount of data, this project explores innovative mechanisms to optimize the performance of these simulations by reducing data movement and maximizing the use of computing power, while minimizing errors and information loss.  Such performance improvements support NSF's mission to advance emerging, data-intensive science discovery and contribute to solving the world's most pressing and complex contemporary science and engineering problems.  This project implements comprehensive outreach and education to train the next-generation of professional workers and researchers in the latest computing architectures and programming methodologies, and provides rich opportunities for student engagement, research, and employment.  It leverages multiple campus and national resources and implements proven, research-based interventions to attract, retain, and educate female and underrepresented minority populations in computer engineering, which furthers the US national goal of increased participation in engineering. <br/><br/>The research goal of this project is to adapt techniques and formats for compressing video data to the investigation of novel data encoding and decoding schemes to optimize data movement and computation in data-intensive simulation and analyses.  Innovative new mechanisms have the potential to efficiently reduce the volume of data generated and transferred while also enabling rapid execution of various analysis kernels using compressed data, and permitting seamless scaling of their performance on current and future extreme-scale platforms.  The research objectives are to investigate data encoding/decoding of scientific datasets and harness encoded data, employ and scale encoded datasets seamlessly within current extreme-scale scientific workflows, and optimize machine learning and data mining algorithms with the goal of maximizing the use of computing power while minimizing errors.  These new mechanisms are applied to an evaluation framework and validated on multiple extreme-scale data-driven scientific applications, including climate, multiphysics, and fluid dynamics.  This approach is expected to transform data representation and encoding while incurring minimal disturbance to existing applications, responding to the trends in hardware architecture and dataset characteristics.  It is anticipated to improve the overall performance of computational scientists' workloads by reducing defensive and productive I/O costs, respectively, up to 100x and 200x data reduction spatially and temporally, potentially resulting in up to an overall 50x I/O cost improvement.  The project leverages multiple collaborations in order to establish the governing principles for system co-design and scalable system software layers for better data encoding within world-class computational infrastructures.  This project strengthens the University of Massachusetts Lowell computer engineering curriculum, broadens participation in computer engineering, and creates a collaborative, interdisciplinary research program geared toward exploiting ever-evolving computing paradigms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757641","REU Site: REU Program at University of Texas at Arlington (UTA): Assistive Technologies for People with Disabilities","IIS","RSCH EXPER FOR UNDERGRAD SITES","03/15/2018","07/31/2019","Ishfaq Ahmad","TX","University of Texas at Arlington","Standard Grant","Wendy Nilsen","08/31/2021","$325,778.00","Sarah Rose","iahmad@cse.uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","1139","9250","$0.00","In this Research Experience for Undergraduates (REU) site, the Department of Computer Science and Engineering (CSE), in partnership with several other departments at the University of Texas at Arlington (UTA), proposes a REU program focusing on assistive technologies (AT) for people with disabilities (PWD). Undergraduate students are quite capable of performing tangible research, given the right motivation, facilities, mentoring, and guidance. Moreover, participation in active research can greatly enhance their educational experience and can be effective in motivating them to graduate studies. The proposed theme encompasses innovative and novel topics that can help PWD meet barriers of different kinds - attitudinal, architectural, and technological - in accessing employment, navigating public spaces, freely communicating, and living independently. Our REU program is a summer-long research experience for 10 carefully selected participants. The program includes close mentoring that will provide participants with a balance between technology research focus and grounding in pragmatic aspects that shape PWD's lives and assistive applications. The program also encompasses five weeks of intensive training in scientific research techniques and the technical areas, followed by five weeks of instruction in broader aspects of computer coding, technical writing, presentation skills, and aspects of a graduate career. The proposed innovative theme will excite students intellectually, as well as supporting their passion and creativity.<br/><br/>People who have disabilities make up one of the largest minorities in the world. The World Health Organization estimates as many as 975 million of those 15 years of age and older live with disabilities. For PWD, AT can create a positive influence by removing or reducing many barriers. AT include devices, systems, equipment, and software that make inclusive environments possible and thereby allow PWD to participate fully in society. Through close and frequent interactions between faculty and participants, the proposed theme of AT will expose participants to innovative and novel inter-disciplinary projects through research and prototyping. In the proposed REU program of CSE at UTA, participants will work on computer programming, algorithms, sensors, data analytics, machine learning, and as well as other aspects of basic and applied computer science and engineering. The prototype projects undertaken by REU participants, even though small in nature, will contribute towards inventive methods, techniques, systems, and apps by enhancing the fundamental knowledge base in related fields, while creating new avenues for cross-disciplinary research. By incorporating an important theme of AT in the diversity-rich UTA environment, we expect to increase the pool of diverse and talents students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844451","I-Corps, Humo Base: Ankle Complex Wearable for Kinematic and Kinetic Movement Data Capture and Assessment","CNS","I-Corps","10/01/2018","09/20/2018","Reuben Burch","MS","Mississippi State University","Standard Grant","Pamela McCauley","03/31/2019","$50,000.00","","burch@ise.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","CSE","8023","9150","$0.00","The broader impact/commercial potential of this I-Corps project will be to improve the health and performance of sports, industrial, or military athletes on a court, in a warehouse, or serving our country, through a wearable liquid metal sensor solution.  One of the largest over-use injuries that is prevalent in non-contact injuries, occurs at the foot and ankle, resulting in millions of dollars lost each year through missed time off and rehabilitation costs. Therefore, this wearable solution is designed to assess current movement patterns and function as a pre-rehabilitation device, which provides an assessment to warn wearers and practitioners of potential over-use movement patterns.  In addition, the wearable can be used as a training device to ensure proper rehabilitation techniques and ""back to work or play"" range of motion assessments, ensuring effective decision making about when the wearer can and should return to activity.   Based on wearable liquid metal technology, this device can be applied to all other joints in the human body, creating a wide range of uses which broaden our potential customer base beyond sports pre-rehabilitation of the ankle.  <br/><br/>This I-Corps project takes the precision of research equipment out of the laboratory and into the environment where training actually occurs.  The wearable device designed for this project is comprised of soft liquid metal sensors and a machine-learning computational platform that is both unrestrictive and non-obtrusive around the wearer's feet and ankles. Through pilot testing utilizing the gold standard of an optical motion capture system, specific sensor positions have been identified that provide linear relationships to the angular changes in single and tri-planar movement(s) of the ankle complex.  Paired with wireless communication capabilities, the wearable device will allow managers, coaches, and other practitioners of human performance the opportunity to detect asymmetrical leg movement patterns that cause muscular imbalances and often lead to non-contact injuries.  Through user experience (UX) testing protocols, software development will enable customizable user interfaces and reports that answers the ""voice of the customer"", looking for data from the ground up.  Based on the results from laboratory testing, the next step is to field test the wearable device and synchronize data visualizations with appropriate mobile devices in order to gain deeper insight on customer wants and needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840435","Planning Grant: Engineering Research Center for Data for Socio-Physical Extreme Event Resilience (Data-SPEER)","EEC","ERC-Eng Research Centers","09/01/2018","08/30/2018","Jack Baker","CA","Stanford University","Standard Grant","Dana L. Denick","08/31/2019","$100,000.00","Jenny Suckale, Anne Kiremidjian, Daniel Aldrich","bakerjw@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","ENG","1480","126E, 1480","$0.00","The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program.  Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.<br/><br/>Urban resilience against extreme events is widely recognized as a critical societal challenge for all societal groups, as indicated by major US Government programs and philanthropic efforts. Our vision is to create communities that are resilient to extreme natural events and national security threats though civic minded engineering. This planning grant will support three workshops to sharpen our research objectives. Across the workshops, we will deepen our team formation and refine our leadership structure by incorporating economists, computer scientists, researchers focused on disasters outside the proposing team's expertise, as well as researchers of resilience in minority communities. Moreover, we will establish a committed stakeholder community by convening federal and local resilience policy makers, emergency managers, and data providers (like technology companies) that can support and benefit from the planned center. Through these activities, we aim to help build a stronger society that is better able to withstand and bounce back from disasters.<br/><br/>This project aims to unite engineers, social scientists, and community leaders to model and design for the dynamic interplay of physical structures and social systems using modern high-resolution data. With this effort, the impacts of preparedness and mitigation investments have the potential to be modeled at individual, household, and community-scales. We aim to constrain these models using crowdsourced data, high resolution imagery, and machine learning techniques to quantify the physical and social impacts of disaster events, planning, and recovery. Additionally, we aim to merge engineering and social science resilience models to produce more holistic benefit-cost quantifications of possible community investments and trajectories. Translating resilience data and modeling into societally beneficial tools and insights requires the convergence of technical expertise in modeling physical disaster impacts and social science expertise in modeling community resilience. The project will drive future convergent disaster resilience research and prepare a new generation of scientists conversant in engineering and social science. Such training will include the ability to engage with civic leaders to jointly work towards reducing economic loss and casualties, while increasing community well-being in the wake of adversity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842974","I-Corps: Geospatial Analytics","IIP","I-Corps","09/15/2018","08/16/2018","Vipin Kumar","MN","University of Minnesota-Twin Cities","Standard Grant","Andre Marshall","02/28/2021","$50,000.00","","kumar@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will be across multiple industries that can benefit from regular monitoring of changes on the Earth's surface. Changes such as droughts, floods, deforestation, urbanization, and grain supply (acreage and yields) fluctuations have considerable impact on businesses as well as the environment. For example, it is estimated that US lost approximately 23 billion dollars due to drought in 2012. This I-Corps project will make use of vast amounts of satellite data to provide accurate, timely and actionable insights about these changes to customers in various industries such as commodity trading, crop insurance, and precision agriculture, pasture management. Our ability to track these changes at a global scale will also improve our understanding of the impact of climate and human induced changes on various ecosystems. <br/><br/>This I-Corps project uses machine learning algorithms that analyze vast amounts of satellite imagery datasets to provide near real time as well as historical information about various changes on the Earth's surface. These algorithms were developed through various federal research grants which includes three NSF grants. Data products created using these algorithms have been validated to improve the state-of-the-art in several earth science applications such as forest fire mapping, surface water dynamics, palm oil plantation mapping, and crop mapping. The analytics technology is more generally applicable to other domains with similar underlying data characteristics. The commercial-use prototype will be developed with commodity traders providing expert feedback on their most meaningful use-cases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831224","SBIR Phase II:  Field-deployable, hand-held spectrophotometer sensor platform for citrus growers to rapidly screen for HLB disease","IIP","STTR Phase II, SBIR Phase II","09/15/2018","05/07/2020","Perry Edwards","PA","Atoptix, Inc.","Standard Grant","Muralidharan Nair","03/31/2021","$875,423.00","","perry@atoptix.com","200 Innovation Blvd","State College","PA","168036602","8148087056","ENG","1591, 5373","169E, 4080, 5373, 8034, 8035","$0.00","The broader impact/commercial potential of this project is to increase overall crop health and production with the proposed smartphone compatible crop health sensor. More specifically, crop disease remains a significant threat to global food security, and with the ability to perform pre-visual screening, the proposed platform enables cost efficient, quantitative detection of crop disease, empowering farmers to take action to reduce disease impact. Initially the sensor platform provides a timely solution for cost effective, high throughput, and pre-visual screening of citrus Huanglongbing disease (HLB), enabling citrus growers to effectively manage HLB in their groves and remain profitable. The pre-visual, cost efficient and quantitative detection capability also translates to detection of diseases in other crops. In addition, the sensing technology developed can be used for quantitative assessment of crop nutrient and water stress, enabling farmers to optimally manage crop health, providing a means to optimize profits, increase crop yields, and reduce environmental impacts. This is critical to ensuring national and global food security, and protecting national water supplies by reducing eutrophication of water bodies due to misdiagnosis and mistreatment of crop stressors.<br/><br/>This Small Business Innovation Research (SBIR) Phase 2 project is built upon Atoptix?s patented compact self-referenced spectrophotometer design, which reduces the size and cost of an optical spectrophotometer to enable field use and integration with smartphone technology. For each spectral measurement, the sensor simultaneously records a self-referenced spectrum, retaining the sensitivity and reliability generally reserved for costlier and bulkier spectrophotometer designs, but also enabling a non-technical user to collect data in the field at the push of a button. Distinct from surface reflection methods, the proposed sensor enables pre-visual detection of a pathogen, as it only captures light that has penetrated inside of a leaf and interacted with internal structures. By lowering the cost of the optical sensor through patented designs, increasing ease of use via a smartphone, and joining the precision of optical spectroscopy with machine learning based analytics, the proposed sensor can enable widespread adoption by growers in disease prone regions, where community wide screening is key for protecting grower assets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816611","CSR: Small: Cost-Aware Cloud Profiling, Prediction, and Provisioning as a Service","CNS","CSR-Computer Systems Research","10/01/2018","08/30/2018","Kyle Chard","IL","University of Chicago","Standard Grant","Marilyn McClure","09/30/2021","$500,000.00","Ian Foster","chard@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7354","7923","$0.00","Cloud computing has the potential to transform computational practice by enabling immediate, on-demand access to large-scale computing resources. But large-scale cloud computing can easily be costly. The Scalable Cost-Aware Cloud Infrastructure Management and Provisioning (SCRIMP) project aims to develop new cloud access methods that will reduce the complexity and cost and improve the efficiency of using cloud resources. The project will innovate in three areas: profiling, prediction, and provisioning. Its new machine learning-based profiling techniques aim to predict application performance, at different levels of accuracy, across a diverse set of cloud resources, based upon derivation of comparable and related instance classes, explorative profiling techniques, and analysis of historical usage. Its ensemble-based market prediction models will allow the many existing cloud market prediction models to be easily compared and then combined so that their collective strengths can be used to predict costs with the aim of minimizing cost, price risk, and likelihood of instance revocation. Finally, its overarching provisioning model will combine application profiles and market prediction models to enable automated, cost-efficient, policy-based cloud provisioning as well as efficient placement and migration of workload within the resulting dynamically provisioned environment.<br/><br/>SCRIMP will advance the use of computation across the sciences, particularly within smaller institutions, by simplifying access to on-demand cloud computing and improving the efficiency with which researchers make use of cloud infrastructure. By lowering scientific computing costs and complexity for many users, SCRIMP will enable more efficient use of cloud credits (whether from cloud providers or funding agencies), democratize access to cloud computing by researchers without dedicated computing infrastructure or expertise, and allow researchers and students to conduct increasingly complex analytics, on larger datasets, and at higher resolution. SCRIMP will also be directly relevant in education, allowing educators to provide access to large resource pools at low cost with guaranteed performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821242","Collaborative Research: Design and Analysis of Data-Enabled High-Order Accurate Multiscale Schemes and Parallel Simulation Toolkit for Studying Electromagnetohydrodynamic Flow","DMS","CDS&E-MSS","09/01/2018","08/07/2018","Zhiliang Xu","IN","University of Notre Dame","Standard Grant","Christopher Stark","08/31/2020","$100,000.00","Dinshaw Balsara","zxu2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","8069","9263","$0.00","Various magnetohydrodynamics (MHD) approximations have served scientists and engineers well for studying problems in astrophysics, space physics and engineering such as tokamaks, plasma propulsion, and plasma instability in engineering devices. Even so, the limitations in this approximation have now become evident. Especially when dealing with dilute plasmas, charge separation cannot be accommodated in MHD. To match the full range of observational data and experiments, it is imperative to provide the plasma physics community with a capability that goes beyond the MHD approximation. The work aims at developing high-order accurate, efficient and easy-to-use numerical methods for simulation-driven discoveries related to multiscale electromagnetohydrodynamic problems on complex geometry. Indeed, the methods developed will actually offer high-order accuracy and extremely robust performance for any conservation law beyond electromagnetics or elasticity applications. Therefore, several other fields of great importance in science and engineering, and indeed of great importance to the NSF mission, will be directly benefited by the methods developed here. Training a new generation of computational scientists capable of conducting interdisciplinary research is one of the central activities of the work. Courses relevant to the research such as numerical partial differential equations, advanced scientific computing, uncertainty quantification and machine learning have been introduced by the investigators and will be renovated by incorporating outcomes from the project into course materials. <br/> <br/>The specific objectives of this project are to develop, analyze and evaluate data-enabled high-order accurate and robust computational modeling tools for simulating multiscale high energy density plasma flows containing both continuum and rarefied regimes in complex geometry. Both new high-order divergence-constraint-preserving central discontinuous Galerkin (DG) scheme on overlapping unstructured grid cells for simulating continuum plasma coupled with Maxwell's equations, and asymptotic preserving central DG scheme for solving Vlasov-Maxwell-Boltzmann (VMB) equations to model the dilute plasma flow will be developed. An innovative data-enabled stochastic concurrent coupling algorithm combining these schemes will be also devised for multiscale simulations. In this coupling algorithm, a novel data-enabled stochastic heterogeneous domain decomposition method to exchange statistical distribution at the interface of continuum and rarefied regimes will be developed. This will be the first attempt to stochastically couple continuum and kinetic plasma flow models. There is no current capability that integrates these unique advances, and the investigators will be the first group to deliver such a forward-looking capability to the plasma physics community. All numerical simulations will be validated by advanced data-enabled uncertainty quantification method developed in this project. A large-scale parallel code with these capabilities will be developed and released to the plasma physics community. It will not only enable the plasma physics community to carry out transformational simulations for new discoveries related to the multiscale electromagnetohydrodynamic physics for the first time but also lower the threshold for new computational scientists to use the new cutting-edge numerical methods for other applications such as nonlinear optics. Simulations will be used to explain new observations such as enhanced electron transport, which are difficult to study experimentally in a harsh plasma environment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840295","I-Corps: Remote monitoring of commercial pollination honey bee colonies","IIP","I-Corps","07/01/2018","06/18/2018","Tom Schryver","NY","Cornell University","Standard Grant","Andre Marshall","10/31/2019","$50,000.00","","tps1@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will supply commercial beekeepers with a way to remotely monitor and identify honey bee colonies in need of management in real time, regardless of location, allowing for a reduction in annual colony loss. Currently, commercial beekeepers must rely on time intensive manual inspections to determine which colonies need assistance in the field. This practice is especially resource intensive for commercial migratory beekeepers who frequently relocate their honey bees to fulfill pollination services for US farmers. In addition, this proposed approach will allow for measuring impact of various agricultural landscapes on honey bee colony health, enabling farmers to engage in quantitative based discussions with farmers about pollinator friendly best practices. Ultimately this I-Crops project focuses on leveraging the economic importance of honey bees to create financial incentives for pollinator friendly farming practices across agricultural landscapes. <br/><br/>This I-Corps project leverages traditional large dataset analysis and machine learning to decipher and detect adverse conditions within the honey bee colony, and how the landscapes in which colonies reside contribute to overall colony health. This is possible through continuous monitoring of the internal colony environment, which is meticulously regulated and stable when the colony is healthy, and utilization of long range low power communications infrastructure. When a colony is unhealthy or suffers an adverse event, like the loss of a queen, the colonies internal environment is disrupted. Honey bee populations have experienced large annual losses recently and although substantial research has been conducted on honey bees residing in backyard or university research apiaries, no work (apart from limited popular press interviews) have attempted to document the problems, economic and biological, that commercial migratory beekeepers and their bees face on their annual pollination cycle. Remote monitoring of these commercial colonies will provide much needed insight into the inner lives of this cohort of economically critical insects, in addition to providing optimization and logistical support to an essential component of US agricultural production.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833447","Symposium on Mathematical Statistics and Applications: From Time Series and Stochastics, to Semi- and Non-Parametrics, to High-Dimensional Models","DMS","STATISTICS","07/01/2018","05/17/2018","Frederi Viens","MI","Michigan State University","Standard Grant","Gabor Szekely","06/30/2019","$25,000.00","Vincent Melfi, Tapabrata Maiti, Weixing Song","viens@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","1269","7556","$0.00","This award supports participation in the conference ""Symposium on Mathematical Statistics and Applications: From Time Series and Stochastics, to Semi- and Non-Parametrics, to High-Dimensional Models"" held September 14 - 16, 2018 at Michigan State University. This conference is designed to benefit graduate students and early-career researchers in mathematical statistics. As the data revolution of the 21st century takes the US to new frontiers in data-based decision-making and predictive analytics in business and government, the role of the mathematically-trained and analytically-critical statistician remains more important than ever, to develop and analyze sound, innovative ways of understanding data science algorithms, methodologies, and procedures. The conference will bring together established and aspiring researchers from around the country and abroad to explore frontiers of mathematical statistics and a host of emerging applications. The exposure and networking opportunities provided by the conference to the dozens of graduate students and early-career researchers will be of great value, and potentially career-changing for some.<br/><br/>Foundational work to further develop the analysis and use of data in all its forms is underway in several areas, particularly in semi- and non-parametric statistics. Such mathematical development is needed across a wide spectrum of the statistical sciences, including high-dimensional statistical inference, non-parametric time series, functional data analysis, empirical processes, and machine learning. The conference will expose the participants to these topics as well as other directions, including asymptotic theory of adaptive and efficient estimation, time series in econometrics, and survival analysis. With ample time built into the schedule for discussions, the conference will give participants opportunities to engage in fruitful cross-group collaborations. A particularly impactful aspect of the conference is the inclusion of several presentations by local non-statisticians who work in areas typically underserved by statisticians but with very significant data analysis needs, all interested in interacting with statisticians from across the country with whom there would ordinarily be no chance to meet. Application areas covered will include quantitative finance, insurance mathematics and risk analytics, nuclear physics, agricultural economics, neuro-imaging, soil science, hospital management, hydrology, and others. The conference website is https://stt.msu.edu/MSUStatSymposium2018/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750474","CAREER: Visual Analytics by Demonstration for Interactive Data Analysis","IIS","Info Integration & Informatics","05/01/2018","05/08/2019","Alexander Endert","GA","Georgia Tech Research Corporation","Continuing Grant","Maria Zemankova","04/30/2023","$385,097.00","","endert@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364","1045, 7364","$0.00","In today's data-driven era, more people analyze data as part of their daily lives. Visual analytic technologies help people in a variety of these domains including business, health, education, national security, and many more. These visual analytic techniques are effective since they combine powerful machine learning with interactive data visualization. Currently, the way that people use such systems is through user interfaces with control panels to adjust parameters of the analytic models and visualization properties directly. For example, controls may ask users to select which analytic model to use, which parameter to adjust, and by how much. However, when people do not have the necessary expertise or training in data science or information visualization, they may not be able to properly use these tools and miss out on important insights. Instead, this project will explore, design, develop, and evaluate techniques that allow people to demonstrate their analytic goals, tasks, and operations. Developing this ""visual analytics by demonstration"" method of user interaction has the potential to impact numerous data-driven domains, and society more broadly. This project will also provide educational experience and research training for graduate and undergraduate students to guide them towards careers in computing.<br/><br/>The proposed research will create Visual Analytics by Demonstration prototypes, generalizable toolkits, and demonstration primitives to foster exploration and discovery in visual analytics. Instead of control panels that require users to directly parameterize analytic models and visualizations, people provide demonstrations, from which the system selects the appropriate visual representation, analytic model, and parameters. To realize the benefits of such technology, many research challenges exist and must be addressed. For instance, what are the basic demonstration primitives that people use to communicate their intent to a system? How can systems interpret these demonstrations and perform the correct analytic and visualization operations? Finally, how can these systems guard against potential user bias in exploring data using by-demonstration? Project objectives include the design, implementation, and evaluation of by-demonstration visual analytic prototypes. The research will perform formative studies to develop demonstration primitives categorized by user tasks. The project will design and develop applications for specific data domains as well as general, open-sourced toolkits for other researchers to use and extend. User studies will identify if and how specific tasks and operations can be performed by demonstrations, and how these compare in performance to current control panel interfaces. The proposed work will also create instructional material to integrate visual analytics by demonstration into courses that teach visual analytics, or data science more generally. The project website (http://va.gatech.edu/projects/visual-analytics-by-demonstration/) will include project information, links to resulting publications, videos, and open-source software produced.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757916","REU Site: Computational Methods for Discovery Driven by Big Data","IIS","RSCH EXPER FOR UNDERGRAD SITES","04/01/2018","02/20/2020","George Karypis","MN","University of Minnesota-Twin Cities","Standard Grant","Wendy Nilsen","03/31/2021","$370,390.00","Evan Rosenberg, Amy Larson, Kathryn Jensen","karypis@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1139","9250","$0.00","The objective of this project is to continue the University of Minnesota (UMN) Research Experiences for Undergraduates (REU) Site in which students engage in research that develops computational methods for scientific discovery across disciplines that are driven by big data. Closely mentored by Computer Science and Engineering (CS&E) faculty, each student will contribute to active research that addresses open questions in computational complexity, machine learning, parallel and distributed computing, mobile and cloud computing, or graphics and visualization. A UMN REU participant might use observation data to simulate crowd behavior, analyze genome sequence data to better understand microbial communities, develop tools to analyze chemical-genetic interaction networks, improve spatial perception in a virtual environment, develop visualization techniques to better understand massive data sets, enhance parallel distributed processing through algorithm development or by harnessing the computational power of a network of mobile devices, or use graph-based approaches to better understand climate change. The diverse research of CS&E faculty represents collaboration across the University with faculty in genetics, chemistry, climate science, neuroscience, architecture, medicine, and biomedical engineering to propel all of these disciplines and computer science towards previously unattainable insights and discoveries. In this 10-week summer program, in addition to immersion in research, students will receive technical training and professional development that encourages and prepares them for a sustained career in the sciences. This includes Big Data Colloquia, Communicating Science workshops, career mentoring, and public dissemination of research findings. Towards an objective of increased participation and broader impacts, this program will bring together nationally recruited students and those from UMN and local institutions to establish a cohort with diverse academic and cultural backgrounds. http://reubigdata.cs.umn.edu/<br/><br/>The objectives of the University of Minnesota (UMN) REU Site program are to (i) intellectually engage and excite participants to motivate their commitment to and pursuit of a career in the sciences, specifically to foster academic persistence, (ii) increase participation in and contribution to the sciences by women and underrepresented minorities in computer science, (iii) train students for sustained contribution to the sciences, particularly in computational methods for big data transdisciplinary research, and (iv) professionally prepare and mentor participants for a career in the sciences, i.e., to teach participants to be effective communicators, be career savvy, and versed in the ethics of science. Towards these objectives, in a 10-week summer program students are immersed daily in research addressing open questions in computational methods for big data. Throughout the summer, each student is closely mentored by a faculty member and graduate student. Program activities help students quickly acclimatize to research and independent work, and most importantly, motivate and prepare students for academic persistence and a career in the sciences. Activities include research tutorials, a Big Data Colloquium series, a Communicating Science workshop series, career mentoring, and a poster presentation at a campus-wide research symposium. The program combines a non-resident and resident program to create a cohort of up to 25 students: 2 from local institutions, 8 from a national recruiting effort funded by this grant, and 15 students through other funding mechanisms. This combined program increases diversity, improves program and impact sustainability, and capitalizes on economic efficiencies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1947919","TRIPODS+X:RES: Collaborative Research:Privacy-Preserving Genomic Data Analysis","DMS","TRIPODS Transdisciplinary Rese","10/01/2018","09/09/2019","Vishesh Karwa","PA","Temple University","Standard Grant","Tracy Kimbrel","09/30/2021","$72,564.00","","vishesh.karwa@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","041Y","047Z, 062Z","$0.00","Much of modern day medicine is driven by genomic data, with the size and complexity of genomic datasets increasing at a rapid pace. Naturally, any use of human genomic data raises grave privacy concerns. This is because the power to query multiple genomic databases with seemingly innocuous questions such as ""Do you contain any genome that has mutation X?"" is enough to determine whether an individual's genome is present in the databases. Such re-identification attacks have raised a germane question: can one implement privacy protection for genomic data so that meaningful data analysis remains possible, but attacks such as these become impossible? The main idea of this project is to achieve this goal by making and exploiting statistical assumptions about the data, such that if the assumptions are false, data analysis will suffer but privacy will not. The project will also generate curricular material for a graduate class at the intersection of data privacy, machine learning, and genomics.<br/><br/>The project considers three major research questions on preserving privacy in the context of genomic data. The notion of privacy used is differential privacy, which provably protects against re-identification attacks, and has found large-scale adoption in both academia and industry. The first research question is the estimation of allele frequencies, and of linkage disequilibrium, while preserving individual privacy. Given a set of human genomes, the objective of allele frequency estimation is to estimate the frequency of the different mutations across various locations in the chromosome. Linkage disequilibrium is the deviation from independence for pairs of alleles. The second question is haplotype sampling. Haplotypes correspond to sets of genetic variations (typically extending over multiple genes), that tend to be inherited together.  In haplotype sampling, the objective is to generate synthetic haplotypes given a data set of human genomes, while respecting biology behind these genetic variations. Finally, the project aims to estimate pathogenic variants of breast cancer genes. Variants of the BRCA 1 and 2 genes are known to be pathogenic for breast cancer. However, a lot of the variants are still not classified as pathogenic / non-pathogenic and are VUSs - Variants of Unknown Significance. The objective is to develop a privacy-preserving system to gather statistics about the VUSs from individually sequenced genes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836921","ICE-T: RC: Multi-Domain Multi-Broker Elastic Optical Networks with Cognitive Functionalities","CNS","Special Projects - CNS","10/01/2018","09/09/2018","S.J.Ben Yoo","CA","University of California-Davis","Standard Grant","Deepankar Medhi","09/30/2021","$300,000.00","Roberto Proietti","yoo@ece.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","1714","","$0.00","The proposed project seeks an innovative approach to the design, control and management of next generation Internet core networks. A new inter-domain networking paradigm is considered, where broker agents compete freely with each other to provide attractive inter-networking services to Autonomous Systems (ASes), while ASes also choose broker services suitable for their inter-networking needs.<br/><br/>The project develops a market-driven multi-broker-assisted multi-domain framework that can coordinate -- in a distributed, incentivized manner - while providing complete autonomy and freedom to both brokers and ASes. Through market-driven incentives and competitions, the brokers and the ASes will pursue mutually beneficial services, which will in turn continuously spur innovative solutions and services.  The new inter-domain networking architecture with multiple brokers will facilitate automated, Quality-of-Service aware, and impairment-responsive services. The project leverages cognitive functionalities based on machine learning to optimize end-to-end service provisioning. <br/><br/>The project seeks to architect, simulate, prototype, and experiment to improve system (a) scalability, (b) end-to-end performance, (c) robustness and adaptability, and (d) manageability and coordination.  The project will leverage novel and emerging cognitive techniques for dynamic traffic prediction and adaptive provisioning, quality of transmission estimation and fault detection across multiple domains. This project extends a research collaboration between the University of California, David, the Universitat Politecnica de Cataluna (UPC), Spain, and Consorzio Nazionale and Interuniversitario per le Telecomunicazioni (CNIT), Italy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818858","NeTS: JUNO2: Resilience in Next-Generation Intelligent Optical Networks","CNS","Special Projects - CNS, Networking Technology and Syst","09/01/2018","12/10/2019","Suresh Subramaniam","DC","George Washington University","Continuing Grant","Ann Von Lehmen","08/31/2021","$258,362.00","","suresh@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","1714, 7363","7071, 9251","$0.00","The world's telecommunication infrastructure is dominated by fiber optics because of its tremendous bandwidth. Huge investments have been and continue to be made in that infrastructure in order to support the Internet, which is at the center of business and daily life. As demand for bandwidth grows, however, the network becomes more complex and harder to manage and protect against natural and other disasters. Network designers are therefore increasingly turning toward optical nodes composed of simple building blocks of essential functions. This project aims to equip the network infrastructure with resilience against failures, both small-scale (due to component or system degradation) and large-scale (due to disasters, for example). Resilience schemes at multiple levels, ranging from component-level to network-level to service-level, will be developed. This is a collaborative project with researchers from Nagoya University and Kagawa University in Japan. The collaboration will leverage the extensive laboratory equipment for prototyping and testing the developed technology and the close interaction with industry that the project's Japanese collaborators have. The results of the project will increase the trustworthiness of optical networks, which form the backbone of our information and communications infrastructure. <br/><br/>The research addresses challenging problems in designing trustworthy optical networks at multiple granularities ranging from component level to network level and to service level. Cost-efficient architectures and practical and intelligent algorithms for surviving multiple failures and for providing service-level availability guarantees will be developed by leveraging multiple techniques such as machine learning and Markov decision processes. Our research objectives are to:<br/><br/>(a) Design highly reliable optical nodes that consist of unreliable components by introducing redundancy in a cost-effective manner.<br/>(b) Design intelligent algorithms for surviving multiple failures at the network level.<br/>(c) Design trustworthy service resource management algorithms.<br/>(d) Develop a small-scale node prototype and evaluate its performance in a system testbed. This will be achieved using the laboratory facilities at the project's Japanese partner institutions. <br/><br/>This project will enable new theory and algorithms to be incorporated into advanced graduate classes and new experimental projects to be developed for undergraduate students. The project will recruit students from underrepresented minorities, and includes extensive K-12 outreach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1805307","Collaborative Research: Understanding and manipulating the solvent microenvironment for selective, catalytic amination of renewable oxygenates","CBET","Catalysis","09/01/2018","08/13/2019","Andreas Heyden","SC","University of South Carolina at Columbia","Continuing Grant","Robert McCabe","08/31/2021","$152,000.00","","heyden@engr.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","ENG","1401","9150","$0.00","The initial steps of biomass refining involve breakdown of the raw material to a biocrude oil containing a mixture of building block chemicals.  The building block chemicals can be further refined to higher value products, often in the liquid phase, with the aid of a solvent and a solid catalyst.  This project will investigate the transformation of one of those building block chemicals, 3-hydroxybutryolactone (3HBA), to several higher-value chemicals.  Theoretical analysis and experimental methods will be combined to understand how the solvent influences the performance of the catalyst in promoting conversion of 3HBA to the desired products.  Results of the study can be applied more generally to other bio-based chemicals to support a growing bio-refining industry relevant for the transition to renewable chemical production.  The project will contribute to a highly trained workforce of experts in biomass processing, while also adding to U.S. technical prominence in biomanufacturing of chemicals.  <br/><br/>A major goal of heterogeneous catalysis research is to identify active sites and to understand how they interact with reactants, products, and the bulk environment to facilitate chemical transformations.  While most catalyst studies focus on catalyst discovery, it is often the bulk reaction environment that benefits most from redesign. The focus on solvation effects in heterogeneous catalysis has recently expanded with the trend toward liquid-phase, catalytic processing of biomass. Motivated by this shift, the project focuses on developing the scientific foundations needed for the rational design of solvent systems for catalytically processing renewable oxygenates. Specifically, the proposed research aims at understanding how the nature of the solvent microenvironment impacts activity and selectivity of ruthenium (Ru) catalysts during reductive amination of 3-HBA to form 2-amino-3-hydroxytetrahydrofuran and 3-aminotetrahydrofuran. The proposed combination of computational and experimental research is structured around (1) state-of-the-art density functional theory calculations, (2) machine learning tools for accelerating complex reaction network investigation, (3) microkinetic reactor modeling under various experimental reaction conditions, (4) vapor phase catalyst evaluation and kinetic isotope effect studies, (5) catalyst evaluations in condensed phases of water, ethanol, 1,4-dioxane, and cyclohexane, and (6) systematic correlation of experimental data with computational models through Bayesian statistical analysis. An iterative research loop is proposed, with experimental observations leading to hypotheses that motivate new computations, while computational models will rationalize experimental findings and guide new investigations. The research program includes undergraduate outreach, and research results will be integrated into undergraduate and graduate electives and the core chemical engineering curriculum at both Syracuse University and the University of South Carolina.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824354","Social Mobility and the Origins of American Science","SMA","Economics, SciSIP-Sci of Sci Innov Policy","08/01/2018","07/26/2018","Petra Moser","MA","National Bureau of Economic Research Inc","Standard Grant","Mark Fiegener","07/31/2021","$281,520.00","","pmoser@stern.nyu.edu","1050 Massachusetts Avenue","Cambridge","MA","021385398","6178683900","SBE","1320, 7626","1320, 7626","$0.00","Advances in science and innovation are the number one source of sustained improvements in individual well-being and economic growth. This project examines the origins of American leadership in science and innovation in the 20th century, starting with three major questions: 1) How has socioeconomic inequality influenced participation in American science? 2) What are the benefits of public investments in education, reaching from pre-K to the university level? 3) How has social mobility influenced science and innovation in the United States? <br/><br/>A major challenge for empirical analyses of science lies in the absence of systematic long-rung data on successful scientists, including their socioeconomic background, education, work histories, inventions, and publications. This research constructs such data for more than 100,000 American scientists who were born between 1817 and 1933. Machine-learning algorithms link each scientist with their census records, patents, and publications. Empirical analyses examine the effects of ethnicity-based restrictions on immigration in the 1920s to investigate the effects of such policies on US science and innovation. Tests for inequality examine the changing influence of a person's socioeconomic background on their success as a scientist. Analyses of education investigate the effects of expansions in public education on participation in STEM by people who were disadvantaged based on their socioeconomic status, as well as by women and minorities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829239","Collaborative Research: Understanding drivers of innovation in the use of science in Federal NEPA decision-making","SMA","SciSIP-Sci of Sci Innov Policy","09/01/2018","07/18/2018","Gwen Arnold","CA","University of California-Davis","Standard Grant","Mark Fiegener","08/31/2021","$157,013.00","Tyler Scott","gbarnold@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","SBE","7626","7626","$0.00","Effective public decision-making requires an evidence-based approach, and leading scientists frequently call for producing policy-relevant science, yet little is known about how government officials use science in their day-to-day work. This project will fill this gap by conducting a systematic analysis of use of science in thousands of decisions about environmental policy made at multiple levels of the US federal government. We focus on the contents of Environmental Impact Statements (EISs) prepared by the US Forest Service (USFS) under the National Environmental Policy Act. Providing a more detailed picture of how scientific information is used in government decision-making will enable government agencies to improve the processes through which they incorporate scientific information. At the same time, providing details of what types of scientific information are utilized in public decision-making processes will help scientists, universities, and funding agencies prioritize science that is policy-relevant. To forward these goals, the project will publish an index that will assist scientists in comparing the policy impact of different publications and will conduct outreach activities with environmental planners to help them understand the implications of the research.<br/><br/>This project will use web scraping and computational text analysis to collect and analyze the use of science in all USFS EISs produced between 2006 and 2018. These EISs record the scientific basis for at least 2,000 decisions made about public land management in a wide variety of contexts. The project has two objectives: (1) Understand what drives the inclusion of innovative scientific information (data, facts, evidence) in USFS EISs; and (2) Understand what drives the adoption of innovative scientific practices (operating procedures, protocols, norms) by USFS decision-makers preparing EISs. Using computational text analysis and machine learning tools, we will measure various aspects of the scientific content of each document such as citations, key concepts, and analytical practices. We will combine this information with publicly available data about the scientific information in each document, such as place of publication, employer of the scientists producing the document, extent of public participation in the document?s preparation, and the government office using the scientific information. This analysis will help in understanding how characteristics of scientific information and characteristics of government offices influence how science is used in public decision-making processes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823821","Social Context and Revenue Measures","SES","Sociology","08/15/2018","08/17/2019","Isaac Martin","CA","University of California-San Diego","Continuing Grant","Joseph Whitmeyer","01/31/2021","$404,999.00","","iwmartin@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","SBE","1331","019Z, 9179","$0.00","In this project, new computational methods are applied to the texts of more than 3,000 local revenue measures that were subject to voter referendums in more and less diverse places.  These data are used to investigate how tax policy characteristics may moderate the known effects of diversity on voters' willingness to pay for public spending.  The results will yield new insights into what makes for a sustainable revenue source.  This is crucial for enabling American society to meet the fiscal challenges of the future, and thus for the health, prosperity and welfare of American society as well as for national security. <br/><br/>The project involves the assembly and digitization of revenue measures presented to local voters since 1986. Supervised and unsupervised machine learning techniques then are applied to classify these measures and to identify features of the text that mediate the observed associations between diversity and reported election outcomes. The most general goal of the project is to identify institutional mechanisms that moderate associations between social context and public spending. A more specific goal is to identify the causes of the well-known negative association between diversity and public spending on health, welfare, and other social transfers and services. The project thereby advances theoretical understanding of the dimensions of policy that affect voting behavior. It also analyzes institutional mechanisms that yield one of the most important observed patterns in fiscal sociology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1929568","Randomized Algorithms for Matrix Computations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2018","03/28/2019","Per-Gunnar Martinsson","TX","University of Texas at Austin","Standard Grant","Leland Jameson","08/31/2020","$120,749.00","","pgm@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","MPS","1271","9263","$0.00","This project will develop mathematical techniques for accelerating computational tasks such as simulating electromagnetic scattering,  medical imaging, extracting useful information from large datasets, machine learning, and many others. In all these computations, the step that tends to be the most time-consuming, and which therefore limits how large problems can be solved, concerns the manipulation of large square or rectangular arrays of numbers, called ""matrices"". Many of the matrices that arise in practical applications have redundancies, and can be compressed to enable them to be stored using less space. Using the compressed format, computations involving the matrix can also be greatly accelerated. The problems that will be addressed are deterministic in nature, but the algorithms that will be developed are probabilistic. It turns out that by exploiting certain mathematical properties of large ensembles of independent random numbers, one can build algorithms for compressing matrices that are much faster than traditional deterministic techniques. The new randomized algorithms can in theory fail, but the likelihood of failure can be shown to  be lower than 1 time out of 10,000,000,000 runs in typical applications. Randomized algorithms of this type have recently attracted much interest due to the fact that they perform  particularly well on emerging computing platforms such as mobile computing (where conserving energy is the key priority), computing using graphical processor units (where the vast numbers of computational cores create challenges), and distributed memory parallel computers. The methods also perform very well when applied  to massively large datasets that must be stored on hard drives, or on large server farms. The project will train one doctoral student, and will lead to the release of a publicly available software package that implements the methods that will be developed. <br/><br/>From a technical point of view, the objective of the project is to develop efficient algorithms for factorizing matrices and for solving large linear systems of algebraic equations. The algorithms will be based on randomized sampling, and will exploit remarkable mathematical properties of random matrices and random orthogonal projections. Such randomized algorithms require less communication  than traditional methods, which makes them particularly attractive for modern applications involving multicore processors, distributed computing, out-of-core computing, etc. Specifically, the project will address the following problems: (1) Computing full matrix factorizations (e.g. the so called ""column pivoted QR factorization"") which are core building blocks in scientific computing. Preliminary numerical experiments demonstrate speed-ups of close to an order of magnitude compared to state-of-the-art software packages. (2) Solving linear systems involving many unknowns and many equations. We expect to achieve substantial practical acceleration, and are cautiously optimistic about the possibility to develop solvers with substantially better asymptotic complexity than the cubic complexity achieved by standard techniques. (3) Developing randomized methods for accelerating computational simulations of phenomena such as electro-statics, composite materials, biochemical processes, slow fluid flows, Gaussian processes in 2 and 3 dimensions, etc. Technically, this will be achieved by developing randomized methods for compressing so called ""data-sparse"" or ""rank-structured"" matrices."
"1838901","SCH: INT: Individualizing Care in Pregnancy and Childbirth through Digital Phenotyping","IIS","S&CC: Smart & Connected Commun, Smart and Connected Health","10/01/2018","09/12/2018","Kelly Gaither","TX","University of Texas at Austin","Standard Grant","Sylvia Spengler","09/30/2022","$1,200,000.00","Maytal Dahan, Karl Schulz, Radek Bukowski","kelly@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","033Y, 8018","8018, 8062, 9102","$0.00","Bringing together researchers and physicians from the Department of Women's Health, the Texas Advanced Computing Center, and the Institute for Computational Engineering & Sciences at The University of Texas at Austin, the goal of this project is to develop a digital phenotype of pregnancy to better understand factors influencing pregnancy outcomes. Women's health represents one of the most pressing health-policy issues impacting our nation. In no medical specialty are the deficiencies of medical evidence more pronounced than in women's health, especially in obstetrics. Over the course of the human life span, birth is one of the most dangerous health episodes for both mother and baby. Worldwide, between 2.6 and 4 million pregnancies result in stillbirth annually. Unlike other leading causes of mortality, birth-related deaths are largely preventable. Today, however, most adverse pregnancy outcomes are not predictable, and cannot be prevented. In this project, the research team will passively monitor a cohort of one thousand pregnant women from their first prenatal visit to six weeks post-partum. To accomplish this, participants will download the HealthyPregnancy smartphone app developed in this project to collect in situ social and behavioral data. The application passively captures participant's interactions with people and places via sensors and software throughout pregnancy. Analysis of this large collection of digital data, in combination with traditional medical monitoring data collected via participant's medical records will result in a digital phenotype of pregnancy. The digital phenotype allows for a more complete understanding of pregnancy at the macro scale and for more detailed understanding of outcomes as a continuum rather than isolated discrete events. <br/><br/>It is widely understood that activity, social support, sleep, and cognitive function are important markers of health, particularly during pregnancy. Maternal obesity is associated with a number of complications in pregnancy including gestational diabetes, pre-eclampsia, macrosomia, caesarean delivery and stillbirth. Lack of social support and social interaction is also an important risk factor and has been shown to have adverse effects on pregnancy outcomes. Sleep disturbances are associated with poor health outcomes, particularly cardiovascular disease and inflammatory responses. Additionally, short sleep duration is associated with an increased incidence in diabetes and obesity and has been associated with an increase in mortality. Research suggests that women who experience pre-eclampsia more frequently report daily cognitive failures and increased emotional dysfunction years later. With the ubiquitous use of smartphones, it is now possible to collect lived experiences or data reflecting markers of pregnancy in the wild. Collecting accelerometer and GPS over time provide an indication of physical mobility and gross motor activity. Call and text message logs detail communication events and contribute to a view of social interaction and social contacts. Additionally, power state, screen time and touch events can be used to understand potential sleep disruption. Of greater significance is the analysis of this data in aggregate over the course of pregnancy. This longitudinal view and analysis of pregnancy in the wild using machine learning and mathematical models provides both an individual's digital phenotype of pregnancy and an aggregate digital phenotype of pregnancy. By gathering and analyzing these two products, they can be used to better understand outcomes and the continuum of events leading up to pregnancy outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839323","TRIPODS+X: RES: Collaborative Research: Scaling Up Descriptive Epidemiology and Metabolic Network Models via Faster Sampling","DMS","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","09/10/2018","Santosh Vempala","GA","Georgia Tech Research Corporation","Standard Grant","A. Funda Ergun","09/30/2021","$120,000.00","","vempala@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","041Y, 1253","047Z, 062Z","$0.00","Sampling from a given distribution from a space with many attributes is a fundamental problem in computer science. Over the past two decades, practical applications of sampling have proliferated in areas such as statistics, networking, biology, differential privacy, and, most notably, machine learning. Sampling is used to evaluate models, as a subroutine for optimization, and more generally for exploring large complex spaces. In these practical settings, the time complexity of sampling is a severe limitation; known methods often require either restricting sampling to very small instances or resorting to unproven heuristics or overly restrictive assumptions. This project will develop a toolkit for sampling and evaluate it on real data sets --- a large-scale, high-dimensional toolkit for sampling smooth and non-smooth distributions, and a suite of functions that can be computed or estimated using access to samples. It will be developed working together with domain experts in health metrics and systems biology.<br/><br/>The overall goal of the project is to produce a general-purpose, open-source, and publicly accessible software for sampling non-smooth log-concave distributions with millions of variables. Achieving these goals requires overcoming complex challenges in both theory and implementation. The new algorithms will be inspired by the investigators' expertise in convex optimization, high dimensional geometry, and randomized linear algebra, especially their breakthroughs in linear programming and volume computation. In both target application domains, health metrics and systems biology, the investigators have worked with experts to develop the current state-of-the-art software tools and deployed them. Drawing from this experience, they are poised to both develop general tools and make data-driven discoveries in these domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839116","TRIPODS+X: RES: Collaborative Research: Scaling Up Descriptive Epidemiology and Metabolic Network Models via Faster Sampling","DMS","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","09/10/2018","Yin Tat Lee","WA","University of Washington","Standard Grant","A. Funda Ergun","09/30/2021","$479,397.00","Abraham Flaxman","yintat@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","041Y, 1253","047Z, 062Z","$0.00","Sampling from a given distribution from a space with many attributes is a fundamental problem in computer science. Over the past two decades, practical applications of sampling have proliferated in areas such as statistics, networking, biology, differential privacy, and, most notably, machine learning. Sampling is used to evaluate models, as a subroutine for optimization, and more generally for exploring large complex spaces. In these practical settings, the time complexity of sampling is a severe limitation; known methods often require either restricting sampling to very small instances or resorting to unproven heuristics or overly restrictive assumptions. This project will develop a toolkit for sampling and evaluate it on real data sets --- a large-scale, high-dimensional toolkit for sampling smooth and non-smooth distributions, and a suite of functions that can be computed or estimated using access to samples. It will be developed working together with domain experts in health metrics and systems biology.<br/><br/>The overall goal of the project is to produce a general-purpose, open-source, and publicly accessible software for sampling non-smooth log-concave distributions with millions of variables. Achieving these goals requires overcoming complex challenges in both theory and implementation. The new algorithms will be inspired by the investigators' expertise in convex optimization, high dimensional geometry, and randomized linear algebra, especially their breakthroughs in linear programming and volume computation. In both target application domains, health metrics and systems biology, the investigators have worked with experts to develop the current state-of-the-art software tools and deployed them. Drawing from this experience, they are poised to both develop general tools and make data-driven discoveries in these domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822598","CRCNS Research Proposal: Collaborative Research: New dimensions of visual cortical organization","IIS","CRCNS-Computation Neuroscience","10/01/2018","09/07/2018","Michael Stryker","CA","University of California-San Francisco","Standard Grant","Kenneth Whang","09/30/2022","$775,776.00","","stryker@phy.ucsf.edu","1855 Folsom St Ste 425","San Francisco","CA","941034249","4154762977","CSE","7327","7327, 8089, 8091","$0.00","The visual system of the mouse is now widely studied as a model for developmental neurobiology, as well as for the understanding of human disease, because it can be studied with the most powerful modern genetic and optical tools.  This project aims to discover how neurons in the visual cortex of the mouse allow it to see well by measuring how the cortex represents ecologically-relevant properties of the visual world.  Quantitative studies of neurons in the mouse's primary visual cortex to date reveal only very poor vision, but their behavior indicates that mice can see much better than that -- they avoid predators and catch crickets in the wild. To understand mouse vision, the investigators will study responses to novel, mathematically tractable stimuli resembling the flow of images across the retina as the mouse moves through a field of grass.  Studies based on these new stimuli indicate that most V1 neurons respond reliably to fine details of the visual scene.  A mathematical understanding of how the brain takes in the visual world should have real implications for how we see, and should have great benefits for artificial vision by computers and robots.  Bringing these ideas into the classroom will provide the foundation for new technologies, and will expose students to both real and artificial vision systems.<br/><br/>Analyses of the brain's visual function are limited by the stimuli used to probe them. Conventional quantitative approaches to understanding biological vision have been based on models with linear kernels in which only the output might be subject to a nonlinearity, all derived from responses of neurons in the brain to gratings of a range of spatial frequencies.  This analysis fails to capture relevant features of natural images, which can not be constrained to linearity. The goal of this project is to probe the mouse visual system beyond the linear range but below the barrier posed by the complexity of arbitrary natural images. The investigators have identified an intermediate stimulus class--visual flow patterns--that formally approximate important features of natural visual scenes, resembling what an animal would see when running through grass. Flow patterns have a rich geometry that is mathematically tractable.  This project will develop such stimuli and test them on awake-behaving mice, while recording the resultant neural activity in the visual cortex.  Studying the mouse opens up the possibility of applying the entire range of powerful modern neuroscience tools-- genetic, optical, and electrophysiological. Visual responses will be analyzed using a novel variety of machine learning algorithms, which will allow the investigators to model the possible neural circuits and then test predictions from those model circuits.  Such an understanding of the brain will inform both primate vision and the next generation of artificially-intelligent algorithms which, as a result, should benefit from being more ""brain-like.""<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816591","SaTC: CORE: Small: Online Malicious Intent Inference for Safe CPS Operations under Cyber-attacks","CNS","Secure &Trustworthy Cyberspace","09/01/2018","07/03/2018","Nicola Bezzo","VA","University of Virginia Main Campus","Standard Grant","Phillip Regalia","08/31/2021","$290,642.00","","nb6be@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8060","025Z, 7434, 7923","$0.00","Modern autonomous vehicles are not built with security in mind. The increased sensing, computation, control capabilities, and task complexity have introduced security concerns beyond traditional cyber-attacks. By injecting malformed data, by spoofing sensors, by tampering with controllers, and even by manipulating the environment, an attacker can compromise the integrity and even take control over the functionality of such cyber-physical systems. Examples of attacks have recently been demonstrated on a variety of systems which include the rerouting of drones, the hijacking of vessels by Global Positioning System (GPS) spoofing, and the use of wireless connectivity to take over the steering and brakes of automobiles. Several solutions have been pursued in recent years to solve this problem, yet the bulk of cyber-physical system security literature is focused on the detection and estimation of malicious attacks without considering the context, risk or consequences, much less the intent of the attack. Predicting the intention, by contrast, may yield more information about the attack and thus offer defense mechanisms. This research focuses on the development of techniques to identify, predict, and mitigate malicious intentions of autonomous vehicles, seeking to develop fundamental methods for estimating risk and consequences of malicious attacks, identifying malicious intent, and defending, controlling, and reconfiguring the compromised system.<br/><br/>This project will provide fundamental approaches to increase resiliency in autonomous vehicles. Specifically, the proposed research includes: 1) new techniques to estimate risk and consequences of attacks, leveraging knowledge about the system model and reachability-based analysis; 2) machine learning-based and control-level intent inference methods; and 3) the development of policies for resilient planning and control to ensure continuous operation of the system with closed-loop performance guarantees. To better develop and assess the security techniques proposed in this work, realistic case studies will be implemented using state-of-the-art unmanned aerial and ground vehicles with different sensing, computation, and communication capabilities to facilitate their transition into practice. The proposed research is also applicable to cyber-physical systems broadly and will contribute directly to the development of safe autonomous systems. Additionally, as part of this project, a major emphasis will be given to education and outreach including the development of novel curriculum activities centered on the topic of resiliency in robotics, involvement of undergraduate and graduate students in research, and collaborations with industry to train the next generation workforce on cyber-physical system security problems and mitigation schemes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1819553","SBIR Phase I:  IMPROVING BEHAVIORAL ASSESSMENT OF CHILDREN DIAGNOSED WITH AUTISM SPECTRUM DISORDER THROUGH ENHANCED DATA ACQUISITION","IIP","SBIR Phase I","06/15/2018","06/20/2018","Gary Higginson","CT","SPINRISE TECHNOLOGIES LLC.","Standard Grant","Peter Atherton","11/30/2019","$223,565.00","","gary@spinrisetechnologies.com","14 Kachina Way","Madison","CT","064431962","8605753931","ENG","5371","5371, 8030","$0.00","This SBIR Phase I project intends to improve the outcomes of children diagnosed with autism spectrum disorder (ASD) by providing critical data from non-clinical settings that are currently unavailable to autism treatment providers. The project also focuses on accelerating the analysis of that data. It is estimated that the costs associated with ASD will be more than $2 million over a patient's life. However, studies have shown that the costs of lifetime care for a child diagnosed with ASD can be reduced by 2/3 with early intervention. Designing an appropriate treatment program, and then monitoring its efficacy, requires the collection of a range of data to avoid dependence on inference and anecdotes. Treatment providers often collect detailed data while working with an individual diagnosed with ASD, but they count on parents and caregivers to provide them with data regarding the child's progress outside the clinical setting. The data submitted are usually incomplete and inadequate (if submitted at all). This SBIR Phase I project will result in a prototype that will rapidly provide autism treatment providers with critical data and analysis, reducing the amount of time it takes to design an optimal treatment program, and thereby reducing the long-term costs associated with a diagnosis of ASD.<br/><br/><br/>This Phase I project will result in a prototype that will make the process of logging data almost frictionless for parents. That narrative data will be combined with video data, biometric data, and additional data collected passively from external sources. All the data will be merged, and then analyzed via machine learning to locate patterns and provide a complete picture to the child's behavior analyst. Developing a camera that will be unobtrusive enough for a child with ASD to tolerate will present a challenge, as will developing the associated software to tag items within the camera's view and save only relevant clips (such as the times immediately before, during and after a meltdown). It will also be challenging to develop a wearable device that a child with ASD will tolerate, along with the associated sensors and software to track metrics associated with arousal and stress. The most significant technical challenge, however, will be to develop the software that combines the multiple data streams and outputs a standardized ABC report (Antecedent, Behavior, Consequence) that would be judged as high quality by a Board Certified Behavior Analyst.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810932","Collaborative Research: Automated observations of phytoplankton communities from open water moorings","OCE","OCEAN TECH & INTERDISC COORDIN","01/15/2018","12/15/2017","Uwe Send","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Kandace Binkley","12/31/2020","$230,106.00","Andrew Barton","usend@ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","GEO","1680","7916","$0.00","Phytoplankton are a diverse group of microscopic organism living near the ocean surface. They play essential role in marine food webs and global biogeochemical cycles. The Imaging FlowCytobot(IFCB) has been used for a number of years but because of the high power requirements deployment has been limited to these types of nearshore sites with power and internet connection. This EAGER proposal will test deploy the Imaging FlowCytobot on an open ocean mooring.<br/><br/>The ability to determine the composition and temporal dynamics of phytoplankton communities is important but traditional methods for collecting samples is time consuming. The Imaging FlowCytobot (IFCB) was developed at WHOI as a submersible flow cytometer that can resolve particles in the 10-150 micrometer size range, including critical phytoplankton groups such as diatoms, dinoflagellates, and coccolithophores, and can capture up to 30,000 high resolution images per hour. To date IFCB?s have been deployed at coastal fixed structures (piers, tower) with power, internet, and regular maintenance. This EAGER proposal will expand the operational capabilities of IFCB by testing it on an open ocean mooring. If successful, a new generation of autonomous plankton samplers could be deployed in geographically diverse locations throughout the global ocean in ecologically and biogeochemically significant locations far from the coastlines. This project will fund a graduate student from SIO who will be trained to operate and maintain the IFCB, as well as analyze data using machine learning tools and image analysis software."
"1836552","QLC: EAGER: Harnessing molecular conformational dynamics for electromechanical qubits","CHE","Macromolec/Supramolec/Nano","09/01/2018","07/11/2019","Daniel Lambrecht","PA","University of Pittsburgh","Standard Grant","Lin He","08/31/2019","$156,194.00","Geoffrey Hutchison","dlambrecht@fgcu.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","MPS","6885","057Z, 7237, 7916","$0.00","The basic unit of information in a conventional computer is the bit.  It can exist in one of two states, which are often called 'on' and 'off' states, or ones and zeros.  Quantum computers are different. The basic unit of information in a quantum computer is the qubit. However, unlike the bit in conventional computers, qubits can exist in many different states, which gives rise to the possibility of building powerful computers that can revolutionize science and technology.  However, creating qubits is challenging and oftentimes they persist only at very low temperatures.  Thus, realizing the transformative potential of quantum computing requires qubits that are simultaneously stable at room temperature and can be precisely manipulated and measured. With support from the Macromolecular, Supramolecular, and Nanochemistry program in the Division of Chemistry, Professors Daniel Lambrecht and Geoffrey Hutchison at the University of Pittsburgh are studying a new type of molecular qubit that could operate at much warmer temperatures and with much longer lifetimes when compared to current systems. The project's discoveries could facilitate a 'quantum leap' toward room temperature quantum computing that could have broad implications for materials design, drug discovery, machine learning, secure communications, and more. This work creates research and training opportunities for students at multiple levels, including high school, undergraduate, and graduate. Importantly, this work trains students at the interdisciplinary intersection of chemistry and quantum information science, as is crucially needed for a STEM workforce that can fully utilize the power of quantum computing.<br/><br/>This project employs theory and computation in tandem with experimental validation to study the electric-field gated inversion of bowl-shaped molecules such as ""buckybowl"" corannulenes and sumanenes as realizations of molecular electromechanical qubits. These ""nanobowl qubits"" have the potential to overcome limitations of current generation electromechanical qubits, specifically the requirement for ultra-low (mK) cryogenic operation and decoherence due to internal defects or qubit-bath interactions. Specifically, they: (i) offer greater than 100x increased temperatures for the quantum-to-classical transition, as compared e.g. to current generation suspended carbon nanotubes, (ii) reduce qubit-environment dipolar interactions and thereby enhance coherence lifetimes by more than 10x compared to current types of electromechanical qubits, and (iii) can be operated at electric field strengths realistically achievable in nanoelectronics. These characteristics address the main drawbacks of current electromechanical qubits. Research questions include: 1) Does the high zero-point energy of the nanobowl inversion mode facilitate significantly higher temperature operation? 2) How strong is the predicted entanglement between nanobowl qubits and what is their decoherence lifetime? Addressing these questions is giving rise to a proof of principle exploration of nanobowls as electromechanical qubits that are potentially orders of magnitude better than the current generation. A theory/experiment feedback loop is used to validate computational predictions and refine computational approaches, if necessary.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833538","SCH: INT: Collaborative Research: S.E.P.S.I.S.: Sepsis Early Prediction Support Implementation System","IIS","Smart and Connected Health","03/01/2018","06/25/2019","Muge Capan","PA","Drexel University","Standard Grant","Sylvia Spengler","09/30/2019","$111,054.00","","mc3922@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8018","8018, 8062, 9102, 9150, 9251","$0.00","Sepsis, infection plus systemic manifestations of infection, is the leading cause of in-hospital mortality. About 700,000 people die annually in US hospitals and 16% of them were diagnosed with sepsis (including a high prevalence of severe sepsis with major  complication). In addition to being deadly, sepsis is the most expensive condition associated with in-hospital stay, resulting in a 75% longer stay than any other condition.  The total burden of sepsis to the US healthcare system is estimated to be $20.3 billion, most of which is paid by Medicare and Medicaid. In fact, in June 2015 the Centers for Medicare & Medicaid Services (CMS) reported that sepsis accounted for over $7 billion in Medicare payments (second only to major joint replacement), a close to 10% increase from the previous year.  This pervasive drain on health care resources is due, in part, to difficulties in diagnosis and delayed treatment. For example, every one hour delay in treatment of severe sepsis/shock with antibiotics decreases a patient's survival probability by 10%. Many of these deaths could have been averted or postponed if a better system of care was in place. The goal of this research is to overcome these barriers by integrating electronic health records (EHR) and clinical expertise to provide an evidence-based framework to diagnose and accurately risk-stratify patients within the sepsis spectrum, and develop and validate intervention policies that inform sepsis treatment decisions.  The  project to bring together health care providers, researchers, educators, and students to add value to patient care by integrating machine learning, decision analytical models, human factors analysis, as well as system and process modeling to advance scientific knowledge, predict sepsis, and prevent sepsis-related health deterioration. In addition to the societal impact that clinical translation of these findings may bring, the project will provide engineering and computer science students and health services researchers with cross-disciplinary educational experience.<br/><br/>The proposed research will apply engineering and computer science methodologies to analyze patient level EHR across two large scale health care facilities, Mayo Clinic Rochester and Christiana Care Health System and to inform clinical decision making for sepsis. The multi-institutional, interdisciplinary collaboration will enable the development of health care solutions for sepsis by describing and accurately risk-stratifying hospitalized patients, and developing decision analytical models to personalize and inform diagnostic and treatment decisions considering patient outcomes and response implications. The Sepsis Early Prediction Support Implementation System (S.E.P.S.I.S.) project aims will be to: 1) Develop data-driven models to classify patients according to their clinical progression to diagnose sepsis and predict risk of deterioration, thus informing therapeutic actions. 2) Develop personalized intervention policies for patients within the sepsis spectrum.  3) Develop decision support systems (DSS) for personalized interventions focusing on resource implications and usability within a real hospital setting. The team will 1) identify important factors that uncover patient profiles based on Bayesian exponential family principal components analysis; 2) develop hidden Markov models (HMMs) and input-output HMMs to identify clusters of patients with similar progression patterns within the sepsis spectrum; 3) provide an analytical framework to support sepsis staging in clinical practice using bilevel optimization. They will 1) predict short- and long-term individual patient outcomes using multivariate statistical models and simulation; 2) develop semi-Markov decision process and partially observable semi-Markov decision process models to identify timing of therapeutic actions and diagnostic tests. Furthermore, the team will 1) predict demand for resources and develop and validate a hybrid mixed integer programming and queueing model to optimize system level allocations; 2) utilize human factors analysis and usability testing to assess the implementation of the DSS."
"1822035","Planning IUCRC at Georgia Institute of Technology:  Center for [Digital Composite Joining and Repair]","IIP","IUCRC-Indust-Univ Coop Res Ctr","09/01/2018","08/29/2018","Chun (Chuck) Zhang","GA","Georgia Tech Research Corporation","Standard Grant","Prakash Balan","08/31/2019","$15,000.00","Donggang Yao, Massimo Ruzzene","czhang343@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","5761","5761","$0.00","This award supports a planning workshop for a new Industry-University Cooperative Research Center (IUCRC) aimed at improving composite material joining and repair (CJAR) technologies. Composite materials are vital to the health and competitiveness of several important industry sectors, including automotive, manufacturing, energy, and aerospace. As composites involve different compositions and element concentration/orientation and tolerances that vary widely, current CJAR practices are highly specialized, labor-intensive, and require experienced technicians to complete the critical inspection, maintenance and repair jobs. Partner institutions including Georgia Institute of Technology, Oakland University, and the University of Tennessee-Knoxville will work collaboratively with various industrial companies, in the automotive, aerospace, energy, and bio-medical sectors of the US economy. The Georgia Tech Site of D-CJAR will primarily work with firms in materials (Chomarat, Henkel, Owens Corning), aerostructures manufacturing (Boeing, Fokker Aerostructures, Lockheed Martin), system operators/maintenance providers (AFSC-Robins AFB, Delta), and equipment vendors (Optomec, Siemens). Faculty and student teams at the partner universities will work with industry members to conduct precompetitive research to develop and disseminate basic and applied technologies/knowledge to facilitate rapid, reliable, and cost-effective composite joining and repair, with an overall goal of significantly reducing costs, cycle time, and variation of CJAR operations within ten (10) years.<br/><br/>With the goal of transforming the current labor-intensive and specialized processes into science-based, automated, digital CJAR processes, the Georgia Tech Site of D-CJAR will primarily focus on developing CJAR technologies for maintenance, repair, and overhaul of aerostructures, automobiles, and infrastructure. We expect advances in several fields and knowledge domains around CJAR, including (1) design and analysis, (2) materials and process engineering, (3) testing and NDE, and (4) data analytics. We will apply diverse advanced digital techniques, including advanced computational modeling, sensing, materials characterization, and machine learning to practical CJAR cases that will also advance the education and workforce preparedness of students working on projects at the partner universities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822212","Planning IUCRC University of Southern California for Networked Embedded Smart and Trusted Things (NESTT)","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/01/2018","07/29/2018","Bhaskar Krishnamachari","CA","University of Southern California","Standard Grant","Behrooz Shirazi","07/31/2019","$15,000.00","","bkrishna@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","5761","5761","$0.00","The University of Southern California plans to work together with Arizona State University, University of Arizona, Southern Illinois University and University of Connecticut to form a new NSF Industry University Cooperative Research Center (IUCRC) for Networked Embedded, Smart and Trusted Things (NESTT). The vision of the NESTT center is to contribute to the development of an equitable, safe and secure connected world, focusing on creating holistic Internet of things (IoT) solutions by integrating technology disciplines with expertise in other areas such as law, business, and humanities. <br/><br/>This planning grant's objective is to organize workshops and a planning meeting with industry partners to form the research agenda for NESTT and to explore industry commitment to the proposed center.  At these workshops and the planning meeting, USC faculty will present and discuss their research pertaining to IoT, spanning a range of technology horizontals, including networking, distributed edge and cloud computing, blockchain, formal methods, machine learning, data analytics and security, as well as application verticals, including transportation, energy, smart buildings, and smart cities, and interdisciplinary collaborations with non-engineering experts from business, policy, and education. <br/><br/>The proposed NESTT center aims to undertake cutting edge fundamental and applied research  that will lower the barriers to the adoption of IoT technologies through holistic multidisciplinary design and innovation in diverse domains such as smart cities, transportation, and energy. These research activities would lead to broader economic, societal and environmental benefits and contribute to new scientific discoveries. The planning effort will also seek effective ways for the proposed center to broaden the participation of underrepresented students in STEM disciplines.<br/><br/>Information about the NESTT IUCRC planning meeting and associated workshops will be posted online at USC under the auspices of the USC Viterbi Center for Cyber-Physical Systems and the Internet of Things (website: http://cci.usc.edu/).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752728","CAREER: Phase Transitions in Randomized Combinatorial Search and Optimization Problems","DMS","PROBABILITY, Division Co-Funding: CAREER","09/01/2018","01/26/2018","Nike Sun","CA","University of California-Berkeley","Continuing Grant","Pawel Hitczenko","09/30/2019","$85,192.00","","nsun@mit.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1263, 8048","1045","$0.00","This project centers on constraint satisfaction problems (csps) - archetypal examples of combinatorial search/optimization problems - with a principal aim of building mathematical theory for these problems. A further objective is to promote connections to statistical physics and computer science, where random csps are fundamental models. Indeed, recent progress on random csps has crucially relied on the exchange of ideas among several disciplines: probability, statistical physics, combinatorics, and computer science. The proposed research will endeavor to advance this dialogue, which has potential to open new research avenues in those disciplines. The proposed research is interdisciplinary in nature: its primary focus is in the development of probability theory, but it is expected that the research approach will be much influenced by developments in statistical physics and computer science. The educational component of the proposal seeks to further promote this interdisciplinary aspect, in classroom education as well as in mentorship of graduate students. With the proliferation of statistical inference problems on large datasets, the development of fast algorithms for combinatorial problems becomes an increasingly urgent problem. At the same time it becomes more important to quantify fundamental barriers in these problems, in terms of information-theoretic and algorithmic limits. This study is complemented by proposing to develop more robust techniques to handle a wider range of problems, including some concrete problems of interest for network theory and machine learning. The educational component involves curriculum development at undergraduate and graduate levels, graduate special topic course development, and mentoring graduate students and postdocs.<br/><br/><br/>Combinatorial search/optimization problems are prominent in a wide range of scientific contexts. Broadly, the defining feature of these problems is that the a priori solution requires exhaustive search over a combinatorial state space such as {0,1}^n. A major challenge is that many such problems are expected to be computationally intractable in worst-case instances (np-hard). In response to this, significant attention has been directed towards random problem instances - they represent natural average-case scenarios, and serve as a useful practical benchmark. A deeper understanding of obstacles in the random setting has potential to inspire algorithmic advances. Practical considerations aside, random problem instances are of deep theoretical interest, and full of rich connections among diverse fields of research. In probability theory, random csps have contributed numerous long-standing open problems - especially ones concerning various phase transitions, conjectured either from numerical simulations or from physical heuristics. A notable problem of this type is the location of sharp satisfiability thresholds. It is a widely held belief that for many random csps, the solution space has a complicated geometric structure; and that this is precisely what obstructs standard probabilistic approaches for analyzing phase transitions. Moreover, it is conjectured that essential features of the solution space geometry are universal to a large class of random csps. Some components of this conjectural picture have been validated by recent progress in the theory of random csps, including results on satisfiability thresholds and partition function asymptotics. However, many key aspects of this picture - particularly ones relevant to algorithmic challenges - remain at the level of conjecture. One of the main goals of the current project is to shed light on these questions: to this end, specific research problems are posed regarding asymptotic properties of the solution space (in the search context) and energy landscape (in the optimization context).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1805724","Globally convergent optimization for data-dependent systems enabled through a novel data-driven branch-and-bound framework","CBET","Proc Sys, Reac Eng & Mol Therm","09/01/2018","08/13/2018","Fani Boukouvala","GA","Georgia Tech Research Corporation","Standard Grant","Raymond Adomaitis","08/31/2020","$300,284.00","","fani.boukouvala@chbe.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1403","","$0.00","Decision-making for complex engineering systems depends on the development of algorithms for data-driven optimization based on data generated either by high fidelity simulations and/or experiments. Despite the high potential of data-driven optimization, there is currently a lack of efficient and scalable methods that can provide high quality solutions for a general class of data-dependent problems. The proposed work is motivated by the increasing number of applications that can benefit from data-driven optimization and control, including chemical process synthesis, enhanced oil recovery, carbon dioxide sequestration, energy efficiency of buildings, and many more.<br/><br/>The proposed research is focused on the integration of traditional process systems engineering with machine learning and uncertainty quantification concepts to overcome key challenges of data-dependent optimization which currently hinder their efficiency and scalability in applications with a high number of dimensions and constraints. The objectives of the proposed research are (a) the identification of efficient space and variable decomposition strategies for creating tractable optimization sub-problems, (b) the formulation of theoretically overestimating and underestimating approximating functions for data-dependent correlations by leveraging data and model uncertainty, and (c) the study of convergence rates and optimality bounds of data-driven branch-and bound optimization for a large set of challenging benchmark problems, as well as challenging case studies for oil-field operations, enhanced oil recovery and building design and efficiency.  The central idea of this work is the formulation of novel under/over-estimating approximations, which will be incorporated within a novel customized branch & bound search to systematically identify optimal solutions with a tractable number of samples and improved convergence rates.  Scalable data-driven optimization tools and a benchmarking library will be created and made publicly available with examples drawn from prominent fields, such as mechanical and structural design, chemical flowsheet design, oilfield control, parameter estimation, and protein folding. There is also a plan to incorporate data-science concepts into the chemical engineering education in the form of teaching modules that will be made available to the academic community at large. A Vertically Integrated Projects (VIP) program is also proposed aimed at attracting undergraduate students from science and engineering disciplines to work as members of interdisciplinary teams towards solving challenging data-driven optimization problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750198","CAREER: Graph-Based Security Analytics: New Algorithms, Robustness under Adversarial Settings, and Robustness Enhancements","CNS","Secure &Trustworthy Cyberspace","03/15/2018","03/11/2019","Neil Gong","IA","Iowa State University","Continuing Grant","Wei-Shinn Ku","08/31/2019","$164,057.00","","neil.gong@duke.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","8060","025Z, 1045, 7434","$0.00","The goal of this project is to make graph-based security analytics practical and robust.  General-purpose graph algorithms and graph-based machine learning methods have had some success when applied to a number of security problems ranging from detecting malicious websites and compromised devices in computer networks to detecting compromised or inauthentic accounts in social networks.  However, because the existing methods are designed for generic contexts rather than for specific security problems, there is room to improve their performance in detecting bad actors in networks.  Further, in security contexts, there is often a determined adversary trying to evade detection that general-purpose algorithms are not designed to consider, which makes them vulnerable to attack.  This project will develop novel graph inference algorithms that consider unique characteristics of security problems, analyze the spectrum of possible attacks on such algorithms, define measures of their robustness against attack, and develop methods to improve their robustness.  The project team will create and share datasets related to graph-based security analytics along with software that implements their algorithms and robustness measures with both industrial practitioners and other researchers.  They will also mentor undergraduate and graduate students in the research, using the problems and data to support new college courses and Science, Technology, Engineering, and Mathematics (STEM) outreach activities for K-12 students.<br/><br/>The work focuses on collective classification algorithms that simultaneously label all nodes in a network as malicious or benign.  The first main research thrust involves advancing analytic techniques that combine random walk and loopy belief propagation-based algorithms through local rules that model the joint probabilities of a given node and its neighbors being malicious.  To do this, the team will develop versions of the algorithms that relax assumptions that neighboring nodes have strong homophily, developing characterizations of neighboring nodes' relationships and creating novel Markov Random Field formulations that leverage these characterizations.  The second research thrust will model the attack surface of collective classification algorithms, characterizing the goals and capabilities of attackers, the cost of evasive moves such as creating nodes or edges and generating network activity, and the effect of different goals, capabilities, and levels of evasion on the algorithms' performance.  The third thrust will be to develop methods to identify such evasion by developing attacker-resistant link prediction algorithms and similarity metrics, then mitigate evasion efforts through developing local rule-based techniques that add noise to graphs in ways that confound attacks.  The team will evaluate the metrics and algorithms on datasets from a number of domains, including malicious users in social networks, malicious URLs in the web graph, malicious domains embedded in domain name service redirects, and malicious orders in an e-commerce marketplace.  These problems, and the associated datasets, will be integrated into an existing course on data-driven security and a new graduate seminar course on collective classification. Results from all activities will be used as cases and materials in both existing and new courses, as well as a K-12 summer program and cybersecurity competition organized around detecting malicious actors in networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833276","NSF Workshop on Internet-of-Things (IoT) Hardware Systems","CCF","Special Projects - CNS, Special Projects - CCF, Software & Hardware Foundation, CPS-Cyber-Physical Systems","08/01/2018","05/22/2018","Marilyn Wolf","GA","Georgia Tech Research Corporation","Standard Grant","Sankar Basu","07/31/2019","$30,000.00","","mwolf@unl.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1714, 2878, 7798, 7918","073Z, 7556, 7945, 9102","$0.00","This project supports a workshop to identify research directions and build a research community for Internet-of-Things (IoT) hardware systems. IoT hardware systems combine wireless, low-power sensor and actuator nodes in networks along with fog and cloud computing.  They provide rich data sets that can be used for real-time monitoring, control, and decision-making. IoT systems are widely used for industrial, transportation, health care, and other applications. As IoT systems increase in scope and complexity, new research challenges arise that require new results and novel interdisciplinary collaborations. The workshop will involve researchers with expertise in a wide range of topics relevant to IoT. The attendees will collaborate to develop a report identifying key challenges and opportunities in IoT hardware systems as well as important resources needed to carry out this research.  <br/><br/>The workshop convens researchers from related disciplines: sensor networks, VLSI, communication, signal processing, etc. The workshop will help to build a community of IoT researchers. The workshop will include graduate student attendees to broaden educational impact. The workshop organizers will strive to build an inclusive set of workshop attendees. The workshop will include a session on diversity issues in IoT research. Talks from the workshop will be broadcast for wider dissemination. The workshop report will be broadly distributed. Workshop attendees will be selected on the basis of white papers submitted to an open call. Workshop presentations will be broadcast on the Internet using video-conferencing tools. Topics of potential interest to the workshop include: VLSI IoT device architectures; ultra-low power design for IoT devices; IoT communication and networking; emerging technologies for IoT devices and systems; distributed algorithms under power and bandwidth constraints; IoT applications in manufacturing, transportation, health care, smart communities, etc.; safe and secure IoT systems; data science challenges in IoT systems; machine learning and AI using IoT.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831560","Dimensions US-BIOTA-Sao Paulo: Collaborative Proposal: Traits as predictors of adaptive diversification along the Brazilian Dry Diagonal","DEB","Dimensions of Biodiversity","09/01/2018","08/10/2018","Scott Edwards","MA","Harvard University","Standard Grant","Douglas Levey","08/31/2023","$419,334.00","","sedwards@fas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","BIO","7968","7968","$0.00","The Brazilian Dry Diagonal (BDD) is a broad region of diverse and unique habitats sandwiched in between the wetter Amazon basin to the west and the moist tropical and subtropical Atlantic Forest to the east. Although the BDD can appear superficially to be wasteland with little variety, in fact it harbors many unique species, comprise the most endangered set of habitats in Brazil, and is being rapidly being lost to agriculture and other human encroachment. This research will be the first to synthesize information from species' traits, communities, and genetics to understand how the unique traits and communities of organisms living in the BDD evolved. Using a diverse set of approaches to study in detail a variety of animals, plants and fungi that today inhabit this broad swath of dry habitats in central Brazil, the researchers will evaluate: 1) what sorts of traits - such as body size, shape or specific behaviors - allow a group of organisms to succeed in a novel environment? 2) how the novel environment determines the set of organisms - the community - that we see today? 3) how organisms adapt genetically and morphologically to that novel environment? and 4) how novel traits, communities, and genetic changes interact to produce the variety of organisms that are seen in different environments today? Answers to these questions will improve scientists' ability to predict the impacts of a changing world on biodiversity. The project will foster new international research collaborations by engaging a large team of experts from the United States and Brazil and will also provide interdisciplinary research training opportunities for undergraduate students, graduate students and post-doctoral researchers.   <br/><br/>To accomplish the four aims listed above, the project will first use a machine learning approach applied to hundreds of traits and lineages in the BDD to determine which traits predict evolutionary success - the tendency for xeric-adapted species to undergo evolutionary diversification. The project will then employ community phylogenetics approaches to understand how functional trait variation is distributed in the BDD, and will determine the evolutionary patterns and over- or under-dispersion of trait values observed in particular communities inhabiting the BDD today, as well as how traits are filtered across habitat gradients. Finally, the project uses a variety of genomic technologies, including whole genome sequencing and transcriptomics, to understand how organisms adapt to the more xeric habitats of the BDD as compared to close relatives living in adjacent mesic biomes. By linking evolutionary patterns in traits, communities and genes, the project will synthesize the functional, phylogenetic and genetic dimensions of biodiversity of the BDD and present a comprehensive portrait of its origins and evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831319","Dimensions US-BIOTA-Sao Paulo: Traits as predictors of adaptive  diversification along the Brazilian Dry Diagonal.","DEB","Dimensions of Biodiversity","09/01/2018","08/10/2018","Bryan Carstens","OH","Ohio State University","Standard Grant","Douglas Levey","08/31/2023","$383,071.00","","carstens.12@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","BIO","7968","7968","$0.00","The Brazilian Dry Diagonal (BDD) is a broad region of diverse and unique habitats sandwiched in between the wetter Amazon basin to the west and the moist tropical and subtropical Atlantic Forest to the east. Although the BDD can appear superficially to be wasteland with little variety, in fact it harbors many unique species, comprise the most endangered set of habitats in Brazil, and is being rapidly being lost to agriculture and other human encroachment. This research will be the first to synthesize information from species' traits, communities, and genetics to understand how the unique traits and communities of organisms living in the BDD evolved. Using a diverse set of approaches to study in detail a variety of animals, plants and fungi that today inhabit this broad swath of dry habitats in central Brazil, the researchers will evaluate: 1) what sorts of traits - such as body size, shape or specific behaviors - allow a group of organisms to succeed in a novel environment? 2) how the novel environment determines the set of organisms - the community - that we see today? 3) how organisms adapt genetically and morphologically to that novel environment? and 4) how novel traits, communities, and genetic changes interact to produce the variety of organisms that are seen in different environments today? Answers to these questions will improve scientists' ability to predict the impacts of a changing world on biodiversity. The project will foster new international research collaborations by engaging a large team of experts from the United States and Brazil and will also provide interdisciplinary research training opportunities for undergraduate students, graduate students and post-doctoral researchers.   <br/><br/>To accomplish the four aims listed above, the project will first use a machine learning approach applied to hundreds of traits and lineages in the BDD to determine which traits predict evolutionary success - the tendency for xeric-adapted species to undergo evolutionary diversification. The project will then employ community phylogenetics approaches to understand how functional trait variation is distributed in the BDD, and will determine the evolutionary patterns and over- or under-dispersion of trait values observed in particular communities inhabiting the BDD today, as well as how traits are filtered across habitat gradients. Finally, the project uses a variety of genomic technologies, including whole genome sequencing and transcriptomics, to understand how organisms adapt to the more xeric habitats of the BDD as compared to close relatives living in adjacent mesic biomes. By linking evolutionary patterns in traits, communities and genes, the project will synthesize the functional, phylogenetic and genetic dimensions of biodiversity of the BDD and present a comprehensive portrait of its origins and evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831241","Dimensions USBIOTA- Sao Paulo: Collaborative Proposal: Traits as predictors of adaptive diversification along the Brazilian Dry Diagonal","DEB","Dimensions of Biodiversity","09/01/2018","02/13/2019","Frank Burbrink","NY","American Museum Natural History","Standard Grant","Douglas Levey","08/31/2023","$425,835.00","","fburbrink@amnh.org","Central Park West at 79th St","New York","NY","100240000","2127695975","BIO","7968","7968","$0.00","The Brazilian Dry Diagonal (BDD) is a broad region of diverse and unique habitats sandwiched in between the wetter Amazon basin to the west and the moist tropical and subtropical Atlantic Forest to the east. Although the BDD can appear superficially to be wasteland with little variety, in fact it harbors many unique species, comprise the most endangered set of habitats in Brazil, and is being rapidly being lost to agriculture and other human encroachment. This research will be the first to synthesize information from species' traits, communities, and genetics to understand how the unique traits and communities of organisms living in the BDD evolved. Using a diverse set of approaches to study in detail a variety of animals, plants and fungi that today inhabit this broad swath of dry habitats in central Brazil, the researchers will evaluate: 1) what sorts of traits - such as body size, shape or specific behaviors - allow a group of organisms to succeed in a novel environment? 2) how the novel environment determines the set of organisms - the community - that we see today? 3) how organisms adapt genetically and morphologically to that novel environment? and 4) how novel traits, communities, and genetic changes interact to produce the variety of organisms that are seen in different environments today? Answers to these questions will improve scientists' ability to predict the impacts of a changing world on biodiversity. The project will foster new international research collaborations by engaging a large team of experts from the United States and Brazil and will also provide interdisciplinary research training opportunities for undergraduate students, graduate students and post-doctoral researchers.   <br/><br/>To accomplish the four aims listed above, the project will first use a machine learning approach applied to hundreds of traits and lineages in the BDD to determine which traits predict evolutionary success - the tendency for xeric-adapted species to undergo evolutionary diversification. The project will then employ community phylogenetics approaches to understand how functional trait variation is distributed in the BDD, and will determine the evolutionary patterns and over- or under-dispersion of trait values observed in particular communities inhabiting the BDD today, as well as how traits are filtered across habitat gradients. Finally, the project uses a variety of genomic technologies, including whole genome sequencing and transcriptomics, to understand how organisms adapt to the more xeric habitats of the BDD as compared to close relatives living in adjacent mesic biomes. By linking evolutionary patterns in traits, communities and genes, the project will synthesize the functional, phylogenetic and genetic dimensions of biodiversity of the BDD and present a comprehensive portrait of its origins and evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829470","Collaborative Research: RUI: Isolating neural mechanisms of perceptual awareness from post-perceptual processes","BCS","Cognitive Neuroscience","09/01/2018","08/22/2018","Michael Cohen","MA","Amherst College","Standard Grant","Kurt Thoroughman","08/31/2021","$350,764.00","","mcohen@amherst.edu","Controller's Office","Amherst","MA","010025000","4135422804","SBE","1699","1699, 9229","$0.00","Understanding the neural mechanisms that give rise to perceptual awareness is a long-standing and fundamental endeavor in human cognitive neuroscience. For decades, cognitive neuroscientists have tried distinguishing between neural activity patterns associated with conscious and unconscious processing. Answering this question can shed light on whether the fundamental conscious experience is linked to activity in lower-level sensory systems, higher-level brain systems in frontal cortex that developed later (in both phylogeny and ontogeny), or interactions between the two.  This investigation is also relevant for understanding various disorders of consciousness such as persistent vegetative state or minimally conscious state, in which individuals are non-responsive, and where such findings could be a first step towards construction of computer algorithms that could indicate the presence of conscious states. Finally, with the adoption of machine learning and neural networks, understanding the neural processes that lead to conscious experience in the human brain could be useful in developing new network architectures designed to more closely mimic human intelligence. <br/><br/>The goal of the proposed research is to identify neural correlates of perceptual awareness using a variety of methods such as electroencephalography and functional magnetic resonance imaging. To examine this issue, the researchers will compare conscious versus unconscious processing in the brains of awake human observers. Specifically, they will investigate the differences in neural activity when observers consciously perceive visual items compared to when those same items go unnoticed.  A critical problem in comparing conscious versus unconscious processing is that it is difficult to separate the neural events associated with conscious perception from the neural events associated with task performance such as memory, decision-making, or verbal and manual responses. Therefore, the researchers will use newly developed ""no-report paradigms"" that include conditions in which stimuli are sometimes seen and sometimes unseen, but observers are not required to report their perceptual experience. By comparing differential neural activity for seen versus unseen stimuli when observers report their experience versus when they do not, the researchers will have a unique opportunity to test several leading theories of consciousness and to identify minimally sufficient neural correlates of perceptual awareness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810945","Conference on Predictive Inference and Its Applications","DMS","STATISTICS","05/01/2018","03/29/2018","Daniel Nettleton","IA","Iowa State University","Standard Grant","Nandini Kannan","04/30/2019","$10,000.00","","dnett@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","MPS","1269","7556","$0.00","The Department of Statistics at Iowa State University will host the Conference on Predictive Inference and Its Applications on May 7 and 8, 2018, in Ames, Iowa.  The conference program includes plenary presentations from 16 distinguished speakers and a poster session featuring presentations from students, postdoctoral scholars, and early-career researchers.  Goals of the conference include raising awareness about the importance of prediction, showcasing research of current and emerging leaders in the field, motivating the development of more accurate prediction methods, and encouraging interactions and collaborations among a diverse collection of scientists with complementary skills and abilities.  In addition to attracting PhD statisticians and graduate students majoring in statistics, the conference has the potential to draw participation from the broader data science community that includes researchers from bioinformatics, business analytics, computer science, electrical and computer engineering, economics, and social sciences among other areas.  The Conference on Predictive Inference and Its Applications will provide a valuable venue for developing new methodologies that enable accurate prediction and assessment of uncertainty in our data-rich world.  Such methodologies provide valuable insights and lead to better decision making in science and industry.<br/><br/>Problems involving the prediction of unobserved but eventually observable quantities are ubiquitous in the modern world.  Historically, the discipline of statistics has placed greater emphasis on hypothesis testing and parameter estimation than it has on prediction.  Other research communities connected to but not equivalent to statistics, including computer science, machine learning, analytics, big data, data mining, and data science have more fully embraced prediction problems and developed a variety of useful prediction tools.  Nonetheless, statistical thinking and innovation have an important role to play in addressing prediction problems now and in the future.  In some cases, statistical thinking can be used to generate appropriate measures of uncertainty to accompany point predictions provided by existing tools.  In other cases, statistical thinking can lead to methods for predictive inference that strike a better balance between bias and variance or make more efficient use of the data than existing approaches.  The formal presentations and ideas that arise from discussions at the conference are intended to motivate scientific progress and result in publications that will appear in refereed journal articles.  This research will enhance the state of the art in prediction methodology, which in turn will lead to advances in many fields where accurate predictions and accurate assessments of prediction error are essential.  The conference website is available at PredictiveInference.github.io.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814778","Collaborative Research: Photometric redshifts via Bayesian functional data analysis","AST","EXTRAGALACTIC ASTRON & COSMOLO","07/01/2018","06/13/2018","Tamas Budavari","MD","Johns Hopkins University","Standard Grant","Nigel Sharp","06/30/2021","$99,505.00","","budavari@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1217","1206","$0.00","Many projects mapping the sky require precise estimates of the speed at which galaxies are receding.  A wide-spread technique derives this number, the redshift, from carefully measuring galaxy colors, using an analysis called ""photo-zs"".  Unfortunately, current estimation methods are not precise enough to achieve major survey science goals.  The present team of astronomers and statisticians will improve both the precision and the reliability of photo-zs.  This work will provide key enabling technology for large surveys in progress and in development, which represent a considerable investment in astronomy.  This project will increase the return on that investment.  The research requires innovation in both astronomy and statistics and the development of new statistical methods likely to have significant additional impact.  The research will help to train a diverse population of students and postdocs in advanced statistics via summer schools and other special sessions, and historically, many such trainees have gone on to pursue careers in data science.<br/><br/>Current and forthcoming automated digital sky surveys aim to push further into ""precision cosmology"" territory by meticulously mapping the distribution and properties of hundreds of millions of galaxies, and measuring the details of thousands of supernovae.  This requires accurate and precise estimation of the redshifts of galaxies using broad-band photometric data, by the technique known as photometric redshifts, or photo-zs for short.  Unfortunately, current estimation methods do not enable the most complete science return from these surveys.  The present project unites astronomers and statisticians to improve the precision as well as the reliability of photo-zs, by modeling spectral energy distributions of galaxies, using Bayesian functional data analysis (FDA), an approach that emphasizes predictive modeling and thorough propagation of information and uncertainty across hierarchical, multi-stage discovery chains.  The project will use a modular, hierarchical modeling framework and account for similarity and diversity, with both conventional parameterizations, and new data-driven parameterizations.  Because this framework will produce probabilistic photo-z estimates, with possibly complex uncertainties, the team will also study how optimally to provide such estimates and to use them for cosmological science.  Open-source implementations of their algorithms will be accelerated by graphics processing units where appropriate.  The research will provide valuable inter-disciplinary training to a graduate student, while developing new statistical methods by innovatively combining FDA, machine learning, and high-performance computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1746871","SBIR Phase I:  Biometric IoT system for First Responders","IIP","SBIR Phase I","01/01/2018","12/19/2017","Zachary Braun","GA","FireHUD Inc.","Standard Grant","Rick Schwerdtfeger","12/31/2018","$224,143.00","","zack@firehud.co","1701 Oakbrook Dr. Suite K.","Norcross","GA","300931800","6787495878","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to improve the safety of firefighters through the research and development of a real-time, wearable sensor system comprising a biometric heads-up display and accompanying analysis tools. This device collects each firefighter?s vital signs in real-time, displays the information via a heads-up display, and alerts them if they are in danger. Simultaneously, it will send the data to authorized officials for real-time strategic decision-making. By receiving access to life-critical information, the commander can make informed decisions on the allocation of key resources during the hectic scene of an emergency. Firefighting is chaotic; every year over one million firefighters risk their lives to protect others. Over 50% of the deaths in firefighting are caused by overexertion and stress, which can induce heart attacks as well as other serious medical issues. Furthermore, around 70,000 firefighting injuries occur each year. The proposed system aims to reduce the amount of firefighting injuries and subsequent costs, which totaled $7.8 billion in 2004, but the design is not limited to firefighting. The proposed system can be easily adapted to serve similar occupations such as military personnel and industrial workers.<br/><br/>The proposed project may be the first to monitor the effects of the extreme nature of fire incidents with physiological stressors and provide this data in real-time. The proposed technology could significantly improve the occupational safety of firefighters and further enhance the scientific knowledge created by studying their physiological states. The proposed project includes three main technical objectives: 1) The research and development of a rugged wearable system that will monitor the physiology of firefighters in real-time. 2) The research and development of a machine learning algorithm to identify key markers that will indicate the exertion and stamina levels of first responders in chaotic environments. 3) The development of a long-range radio system capable of transmission within large urban structures comprising various possible interferences. All three objectives will consist of two pilot studies with an intermittent development phase in between, where feedback from the first pilot study will be incorporated into both the hardware and software. It is expected that the outcomes of this project will demonstrate a significant reduction in firefighter injuries, paving the way for a clear return on investment for the partnering fire departments."
"1756069","Neurobiology of Cognition: 2018 Gordon Research Seminar","BCS","Cognitive Neuroscience","01/15/2018","01/12/2018","Jennifer Groh","RI","Gordon Research Conferences","Standard Grant","Betty Tuller","12/31/2018","$25,200.00","David Freedman","jmgroh@duke.edu","512 Liberty Lane","West Kingston","RI","028921502","4017834011","SBE","1699","7556, 9150","$0.00","The Neurobiology of Cognition conference will enhance communication and collaboration among scientists from several disciplines, theoreticians, and technical development specialists.  It emphasizes both the most recent findings in traditional ""core"" areas, such as memory and decision making, as well as several new themes, including: 1) the evolutionary processes that have led to the human brain's cognitive abilities; 2) implications of machine learning for cognitive neuroscience; and 3) cognition in natural contexts. The program is highly interdisciplinary, bringing together scientists who use behavioral, neuroimaging and electrophysiological techniques in humans and non-human animals with investigators who use computational approaches to construct realistic models of cognitive operations to guide future experimentation.<br/><br/>This award will provide registration, or registration and travel, for up to 40 doctoral and postdoctoral trainees and early-career scientists to attend the Gordon Research Seminar (GRS), which prepares attendees for more in-depth participation in the Gordon Research Conference (GRC) on the Neurobiology of Cognition that immediately follows. The format promotes intensive interactions among investigators and trainees from different disciplines and, in particular, between experimentalists and theorists.  The program is about 40% female, which approximately matches the base rate representation of women in this sub-discipline of neuroscience.  The GRS and GRC will expose early-career scientists to novel, state-of-the-art experimental and theoretical approaches that promise to elucidate fundamental principles of cognition."
"1819802","SBIR Phase I: Enabling Musical Creativity for People with Special Needs","IIP","SMALL BUSINESS PHASE I","06/15/2018","06/13/2018","Jacob Zax","CO","Edify Technologies, Inc.","Standard Grant","Rajesh Mehta","11/30/2018","$225,000.00","","jacob@edify.co","1232 Detroit St.","Denver","CO","802063330","3039029093","ENG","5371","5371, 8031, 8032","$0.00","This SBIR Phase I project is dedicated to producing software that makes it easy for people with special needs, especially those on the autism spectrum, to compose their own music. A wide base of evidence shows that music can be a powerful emotional and creative outlet for people with disabilities. Multiple studies show that music can help children on the autism spectrum improve their skills in social interaction, verbal communication,initiating behavior, and social-emotional reciprocity. However, public access to music education has declined dramatically in the United States over the last 40 years. The 6.6 million students in the US with special needs face further barriers to music education because of the significant financial resources and demanding degree of physical dexterity required by instrument lessons. Within this context, private sector innovation is more necessary than ever to increase the accessibility of music education. By offering affordable and intuitive software this project has the potential to make a major impact on the economy by helping to reduce education and health care costs for parents of children with special needs, school special education programs, and adults with disabilities. Just as importantly, democratizing the joy of musical creativity will make a powerful contribution to unlocking the passion, talents, and productivity of people with special needs.<br/><br/><br/>To make it easy for people with special needs to make their own music, this project will employ gestural composition, responsive design, and data science to produce a software interface that adapts to a users' accessibility preferences based on their previous interactions with the program. A second focus of the project is to produce interactive music lessons focused on self-expression and socio-emotional intelligence. Lessons will utilize and expand upon the creative interface to guide users to write songs that express their mood. Robust user testing and intelligent application of machine learning will be required to ensure that the interface smoothly evolves to enable user creativity. Considerable experimentation will also be required to build lessons which strike a balance between supporting users through linear instruction and empowering users to make expressive decisions and take ownership of their emotions. To mitigate the project's technical risk special needs experts and health professionals will be consulted at every stage of user testing and development. In sum, these innovations address the significant and urgent challenge of opening music up as a medium of creativity, communication and self-expression for people with special needs, especially those on the autism spectrum.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
