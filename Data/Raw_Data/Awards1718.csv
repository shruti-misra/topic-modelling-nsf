"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1742656","Image-Data-Driven Deep Learning in Geosystems","CMMI","Geotechnical Engineering and M","09/01/2017","07/30/2017","Zhen Liu","MI","Michigan Technological University","Standard Grant","Joy Pauschke","08/31/2021","$227,367.00","Shiyan Hu","zhenl@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","ENG","1636","036E, 037E, 038E, 1057, CVIS","$0.00","Breakthroughs in deep learning in 2006 triggered numerous cutting-edge innovations in text processing, speech recognition, driverless cars, disease diagnosis, and so on.  This project will utilize the core concepts underlying the recent computer vision innovations to address a rarely-discussed, yet urgent issue in engineering: how to analyze the explosively increasing image data including images and videos, which are difficult to analyze with traditional methods. These concepts will be employed to explore the possibility of accurately assessing the safety of retaining walls with image data.  This effort aims at setting up a paradigm for connecting engineering disciplines to artificial intelligence and enhancing the safety of geosystems as an essential infrastructure component by enabling their analysis with image-data-driven deep learning.  The project will help revitalize traditional artificial intelligence sub-areas in geotechnical and other engineering areas, just as deep learning rekindled the interest in artificial neural networks and machine learning, and turned them into leading players in STEM research and innovations.  The project may also change engineers' opinions regarding how to create knowledge with a revolutionary way attributed to deep learning, i.e., learning directly from data instead of indirectly from models established based on the data.  Innovative education and outreach effort will be made by means of developing a mobile app to disseminate the idea and products of the project.  The project will contribute to education by outreaching to K-12 students, underrepresented groups, and geotechnical engineering researchers and practitioners with the project products including the app at various events at the PIs' institution and professional conferences. <br/><br/>The goal of this study is to understand the image-data-driven deep learning in geosystems with an exploratory investigation into the stability analysis of retaining walls.  To achieve the goal, the recent breakthroughs in computer vision, which were later used as one of the core techniques in the development of Google's AlphaGo, will be studied for its capacity in assessing the stability of a typical geosystem, i.e., retaining walls.  The core concept enabling machines to surpass humans in visual classification capacity, i.e., convolutional neural nets (CNN), will be used to process the big data in geotechnical engineering, which primarily consist of still and live images (videos), that cannot be readily analyzed using traditional geotechnical engineering methods.  Conventional neural nets will be used to analyze images for retaining walls to tell whether a wall is safe or failed.  For quantitative analysis, 2D and 3D images for retaining walls will be generated using stochastic methods and analyzed using traditional limit analysis and numerical methods for labeling.  These labeled image data will be used as input to train convolutional neural nets for supervised learning. The trained nets will be tested against another independent set of data generated in the same way as the training data. Three research tasks will be conducted in this project: 1) understanding the data science for image-data-driven geotechnical engineering research, 2) investigating the connections between those image patterns in deep learning and the physical mechanisms, and 3) revealing the robustness and extrapolation capacity of the deep learning approach in the stability analysis of retaining walls."
"1740151","SI2-SSE: Deep Forge: a Machine Learning Gateway for Scientific Workflow Design","OAC","Software Institutes","09/01/2017","10/17/2019","Akos Ledeczi","TN","Vanderbilt University","Standard Grant","Stefan Robila","08/31/2021","$400,000.00","Peter Volgyesi, Brian Broll","akos.ledeczi@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","8004","7433, 8004, 8005","$0.00","Recent advances in machine learning have already had a transformative impact on our lives. However, astonishing successes in diverse domains, such as image classification, speech recognition, self-driving cars and natural language processing, have mostly been driven by commercial forces, and these techniques have not yet been widely transitioned into various science domains. The field is ripe for innovation since many science fields have readily available large-scale datasets, as well as access to public or private compute infrastructure capable of executing computationally expensive artificial neural network (ANN) training workflows. The main roadblocks seem to be the steep learning curve of the ANN tools, the accidental complexities of setting up and executing machine learning workflows, and the fact that finding the right deep neural network architecture requires significant experience and lots of experimentation. DeepForge overcomes these obstacles by providing an intuitive visual interface, a large library of reusable components and architectures as well as automatic software generation enabling domain scientist to experiment with ANNs in their own field. There is unmet high demand of talent in machine learning, exactly because it has so much potential in a wide variety of application areas. Therefore, any tool that helps scientists apply machine learning in their own domains will have a broad impact. The promise of DeepForge is to flatten the learning curve, hide low level unimportant details and provide components that are reusable within and across disciplines. Therefore, DeepForge will have transformative impact on a number of fields.<br/><br/>DeepForge, a web- and cloud-based software infrastructure raises the abstraction of creating ANN workflows via an intuitive visual interface and by managing training artifacts. Hence, it enables domain scientists to leverage recent advances in machine learning. DeepForge will also integrate with existing cyberinfrastructure, including private and commercial compute clusters, cloud services (e.g. Amazon EC2), public supercomputing resources, and online repositories of scientific datasets. The DeepForge visual language for designing ANN architectures and workflows is powerful enough to capture the concepts related to common deep learning tasks, yet it provides a high level of abstraction that shields the users from the underlying complexity at the same time. DeepForge will provide a facility that allows for sharing design artifacts across a wide interdisciplinary user community. Curating a rich library of reusable components, integrating with a wide variety of existing cyberinfrastructure resources from data sources to compute platform and providing data provenance in a seamless manner are other advantages of the project. DeepForge will promote ""data as product,"" ""model as product,"" and ""service as product"" concepts through integration with the Digital Object Identifier (DOI) infrastructure. DeepForge will enable scientist to assign DOIs to their shared assets providing data provenance enabling citing and publicly reproducing research results by executing the referenced ANN workflows with the linked data artifacts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1741431","BIGDATA: IA: Distributed Semi-Supervised Training of Deep Models and Its Applications in Video Understanding","IIS","Big Data Science &Engineering","09/01/2017","07/07/2018","Boqing Gong","FL","The University of Central Florida Board of Trustees","Standard Grant","Maria Zemankova","08/31/2020","$662,431.00","Liqiang Wang, Mubarak Shah","bgong@icsi.berkeley.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","8083","7433, 8083","$0.00","This project investigates semi-supervised training of deep neural network models using large-scale labeled and unlabeled data in a distributed fashion. Deep neural networks have recently been widely deployed in artificial intelligence and related scientific fields, largely attributing to well-labeled big datasets and improved computing capabilities. However, the unlabeled data, which is often bigger, is inherently ruled out by the prevailing supervised training of the deep models. It is indeed highly challenging to model the unlabeled parts of many recent and emerging datasets, which are often unstructured and distributed over different nodes of a network (e.g., the videos captured by a camera network). This project aims to explore how to effectively use the unlabeled and distributed data to complement the discriminative cues of the labeled data, to jointly learn accurate and robust deep models. The research seamlessly unifies machine learning, computer vision, and parallel computing, and fosters unique interdisciplinary research and education programs for the graduate and undergraduate students.<br/><br/>Despite the progress on semi-supervised learning and deep learning, the confluence of these two is mostly studied on a small scale in single-machine environment. However, many new datasets easily grow beyond the computation or even storage capacity of a single machine. Hence, it becomes a pressing need to investigate the semi-supervised learning of deep models on parallel computing platforms. To better account for this scenario, this project develops improved network architectures to facilitate the parallel training, and the training procedure developed adaptively switches between synchronized and asynchronized modes for optimal efficiency. The main idea is to incorporate a parametric distribution to the neural network and use covariate matching to coordinate the network behaviors across different machines. The researchers also explore a novel application, extreme-scale spatial-temporal action annotation of video sequences, to benchmark the algorithms and frameworks in this project."
"1733834","AitF: Collaborative Research: A Framework of Simultaneous Acceleration and Storage Reduction on Deep Neural Networks Using Structured Matrices","CCF","Algorithms in the Field","09/15/2017","09/13/2017","Bo Yuan","NY","CUNY City College","Standard Grant","Rahul Shah","11/30/2018","$448,086.00","Victor Pan","bo.yuan@soe.rutgers.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","CSE","7239","","$0.00","      Deep neural networks (DNNs) have emerged as a class of powerful techniques for learning solutions in a number of challenging problem domains, including computer vision, natural language processing and bioinformatics.  These solutions have been enabled mainly because we now have computational accelerators able to sift through the myriad of data required to train a neural network.   As the size of DNN models continues to grow, computational and memory resource requirements for training will also grow, limiting deployment of deep learning in many practical applications. <br/><br/>      Leveraging the theory of structured matrices, this project will develop a general framework for efficient DNN training and inference, providing a significant reduction in algorithmic complexity measures in terms of both computation and storage.  <br/>The project, if successful, should fundamentally impact a broad class of deep learning applications.  It will explore accelerating this new structure for deep learning algorithms targeting emerging accelerator architectures, and will evaluate the benefits of these advances across a number of application domains, including big data analytics, cognitive systems, unmanned vehicles and aerial systems, and wearable devices.  The interdisciplinary nature of this project bridges the areas of matrix theory, machine learning, and computer architecture, and will affect education at both Northeastern and CCNY, including the involvement of underrepresented and undergraduate students in the rich array of research tasks.<br/><br/>     The project will: (1) for the first time, develop a general theoretical framework for structured matrix-based DNN models and perform detailed analysis and investigation of error bounds, convergence, fast training algorithms, etc.; (2) develop low-space-cost and high-speed inference and training schemes for the fully connected layers of DNNs; (3) impose a weight tensor with structure and enable low computational and space cost convolutional layers; (4) develop high-performance and energy-efficient implementations of deep learning systems on high-performance parallel platforms, low-power embedded platforms, as well as emerging computing paradigms and devices; (5) perform a comprehensive evaluation of the proposed approaches on different performance metrics in a variety of platforms.  The project will deliver tuned implementations targeting a range of computational platforms, including ASICs, FPGAs, GPUs and cloud servers. The hardware optimizations will focus on producing high-speed and low-cost implementations of deep learning systems."
"1733701","AitF: Collaborative Research: A Framework of Simultaneous Acceleration and Storage Reduction on Deep Neural Networks Using Structured Matrices","CCF","Algorithms in the Field","09/15/2017","09/13/2017","Xue Lin","MA","Northeastern University","Standard Grant","A. Funda Ergun","08/31/2021","$348,037.00","David Kaeli","xuelin@coe.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7239","9102","$0.00","      Deep neural networks (DNNs) have emerged as a class of powerful techniques for learning solutions in a number of challenging problem domains, including computer vision, natural language processing and bioinformatics.  These solutions have been enabled mainly because we now have computational accelerators able to sift though the myriad of data required to train a neural network.   As the size of DNN models continues to grow, computational and memory resource requirements for training will also grow, limiting deployment of deep learning in many practical applications. <br/><br/>      Leveraging the theory of structured matrices, this project will develop a general framework for efficient DNN training and inference, providing a significant reduction in algorithmic complexity measures in terms of both computation and storage.  <br/>The project, if successful, should fundamentally impact a broad class of deep learning applications.  It will explore accelerating this new structure for deep learning algorithms targeting emerging accelerator architectures, and will evaluate the benefits of these advances across a number of application domains, including big data analytics, cognitive systems, unmanned vehicles and aerial systems, and wearable devices.  The interdisciplinary nature of this project bridges the areas of matrix theory, machine learning, and computer architecture, and will affect education at both Northeastern and CCNY, including the involvement of underrepresented and undergraduate students in the rich array of research tasks.<br/><br/>     The project will: (1) for the first time, develop a general theoretical framework for structured matrix-based DNN models and perform detailed analysis and investigation of error bounds, convergence, fast training algorithms, etc.; (2) develop low-space-cost and high-speed inference and training schemes for the fully connected layers of DNNs; (3) impose a weight tensor with structure and enable low computational and space cost convolutional layers; (4) develop high-performance and energy-efficient implementations of deep learning systems on high-performance parallel platforms, low-power embedded platforms, as well as emerging computing paradigms and devices; (5) perform a comprehensive evaluation of the proposed approaches on different performance metrics in a variety of platforms.  The project will deliver tuned implementations targeting a range of computational platforms, including ASICs, FPGAs, GPUs and cloud servers. The hardware optimizations will focus on producing high-speed and low-cost implementations of deep learning systems."
"1721749","SBIR Phase I:  Artificial Intelligence, Scientific Reasoning, and Formative Feedback: Structuring Success for STEM Students","IIP","SMALL BUSINESS PHASE I","07/01/2017","07/25/2018","Norbert Elliot","FL","My Reviewers, LLC","Standard Grant","Rajesh Mehta","06/30/2018","$156,165.00","","nelliot3@usf.edu","6324 South Queensway Dr","Temple Terrace","FL","336172437","8134049734","ENG","5371","110E, 5371, 8031, 8032","$0.00","This SBIR Phase I project uses artificial intelligence techniques to identify the ways that undergraduate students in scientific courses explicate problems, describe procedures, make claims, provide evidence, offer qualifications, and draw conclusions. With emphasis on forms of scientific reasoning, this new use of artificial intelligence, to identify language patterns associated with scientific reasoning, will allow students to improve their written laboratory reports before they are submitted, therefore freeing instructors to devote precious instructional time to preparing students for roles as practicing scientists. As the U.S. continues to experience rapid diversity growth, this focus on helping students through innovative uses of technology holds the potential to expand science education by cultivating student ability through autonomous writing and revision. Because artificial intelligence techniques are intended to expand capabilities, the techniques being used, available 24/4 on the web, will have the direct impact of growing our technical and scientific workforce, thus expanding the many dynamic pathways to STEM occupations. As the NSF observed in 2015 in its report Revisiting the STEM Workforce, these jobs are extensive and critical to innovation and competitiveness and are essential to the mutually reinforcing goals of individual and national prosperity and competitiveness. An investment in such a technology is thus an investment in national competitiveness, education policy, innovation, and workforce diversity.<br/><br/>NSF SBIR support will be used to design and launch artificial intelligence techniques based on Deep Artificial Neural Network (DANN) as driven by Natural Language Processing (NLP), Latent Semantic Analysis (LSA), and the latest advances in AI algorithms. Because NLP and LSA techniques are presently used solely to identify grammatical and organizational patterns, the application of DANN is high risk in making a leap from identifying patterns of language use to capturing patterns of scientific reasoning. Trained on a proprietary corpus of 100,000 lab reports scored and annotated by instructors and students using a single rubric, the AI application will identify logic structures of scientific reasoning in student laboratory reports. Once methodically identified, categorized according to ability level, and validated by STEM instructors, digital instruction will be used to help students improve their scientific reasoning processes. With the singular goal of structuring student success through asynchronous machine learning, this innovation holds the promise to figure meaningfully in discussions of national competitiveness, education policy, innovation, and diversity as related to STEM education."
"1734938","NCS-FO: Neuroimaging to Advance Computer Vision, NLP, and AI","IIS","GVF - Global Venture Fund, ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","08/15/2017","07/12/2019","Jeffrey Siskind","IN","Purdue University","Standard Grant","Kenneth Whang","07/31/2021","$1,000,000.00","Ronnie Wilbur, Evguenia Malaia","qobi@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","054Y, 7980, 8624","5946, 5980, 6869, 8089, 8091, 8551, 8817","$0.00","It is often said that a picture is worth a thousand words. Frequently, to search for what is needed, whether images or objects in those images, words are needed instead. Getting accurate labels for efficient searches is a longstanding goal of computer vision, but progress has been slow. This project employs new methods to significantly change how picture-word labeling is accomplished by taking advantage of the best picture recognizer available, the human brain. Through functional magnetic resonance imaging and electroencephalography, brain activity of humans looking at pictures/videos is recorded and then used to improve performance on artificial intelligence (AI) tasks involving computer vision and natural language processing. Current systems use machine learning to train computers to recognize objects (nouns) and activities (verbs) in images/video, which are then used to describe events. Reasoning tasks (e.g., solving math problems) can then be done. These systems are trained on specially prepared datasets with samples of nouns for objects, verbs for activities, sentences describing events, and exam questions and answers. A novel paradigm using humans to perform the same tasks while their brains are scanned allows determination of neural patterns associated with those tasks. The brain activity patterns, in turn, are used to train better computer systems.<br/><br/>The central hypothesis is that understanding human processing of grounded language involving predication and its use during reasoning will materially improve engineered computer vision, natural language processing, and AI systems that perform image/video captioning, visual question answering, and problem solving.  Scientific and engineering goals include developing models of human language grounding and reasoning consistent with neuroimaging, to improve engineered systems integrating language and vision that support automated reasoning.  The main scientific question is to understand mechanisms by which predicates and arguments are identified, linked, and used for reasoning by the human brain.  The hypothesis, that predicate-argument linking in visual and linguistic representations are accomplished similarly, and that this then supports reasoning and problem solving, will be tested using multiple neuroimaging modalities, and machine learning algorithms to decode ""who did what to whom"" from brain scans of subjects processing linguistic and visual stimuli.  The iterative approach will involve understanding information integration at the neural level, to improve machine learning performance on AI tasks by training computers to perform increasingly complex tasks with neuroimaging data from stimuli derived from large-scale natural tasks.  Using identical datasets for human and machine performance will support translation of scientific advances to engineering practice involving integration of computer vision and natural language processing.<br/><br/>This award is cofunded by the Office of International Science and Engineering."
"1651843","CAREER: Deep Robotic Learning with Large Datasets: Toward Simple and Reliable Lifelong Learning Frameworks","IIS","Robust Intelligence","07/01/2017","07/23/2020","Sergey Levine","CA","University of California-Berkeley","Continuing Grant","Erion Plaku","06/30/2022","$549,998.00","","sergey.levine@gmail.com","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","1045, 7495","$0.00","Learning robot behaviors from large data sets is an important way to make robots more capable and reliable.  This project will develop algorithms for autonomous robotic skill learning that can easily be used by novice hobbyists with low-cost robots. If deployed widely, such an approach could be used to gather a large number of robotic motions, which can be combined to improve the robot's skills.  Availability of large datasets has proven critical in machine learning application areas, from computer vision to speech recognition, and the ability to collect a large amount of robotic interaction data would substantially increase the capabilities of learning-based robotic systems. Since the approach will be designed for untrained users, it also doubles as an effective tool for robotics education.<br/><br/>Deep learning has emerged as a powerful technique for taming the complexity of the real world. The success of deep learning depends on the availability of large datasets, which traditionally have been difficult to obtain for robotic learning. This project will focus on deep learning algorithms that can be used for effective and reliable robotic skill learning, generating intelligent actions directly from raw sensory input, with an eye towards enabling widespread deployment for large-scale data collection. To that end, the proposed research will aim to: (1) devise reliable and robust real-world robotic learning algorithms that can collect experience without human oversight or intervention; (2) build algorithms centered around transfer learning, whereby experience from prior tasks can be used to inform dramatically faster learning of new skills with potentially different robotic platforms; and (3) devise algorithms that can effectively control heterogeneous, low-cost, imprecise robots, so as to facilitate widespread deployment and the project's educational mission."
"1725729","MRI: Development of an Instrument for Deep Learning Research","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","10/01/2017","09/14/2017","William Gropp","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","09/30/2021","$2,721,983.00","Roy Campbell, Volodymyr Kindratenko, Jian Peng","wgropp@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1189, 7231","1189","$0.00","This project will develop and deploy a novel instrument for accelerating deep learning research at the University of Illinois (UI). The instrument will integrate the latest computing, storage, and interconnect technologies in a purpose-built shared-use system. This Instrument will deliver unprecedented performance levels for extreme data intensive emerging fields of research with far-reaching impacts in many areas, such as computer vision, natural language processing, artificial intelligence, healthcare and education. The instrument development will be driven by the UI deep learning (DL) community needs and will be carried out in collaboration with IBM and Nvidia. The instrument will serve as a focal point for the rapidly growing DL research community at UI, enable expansion of several research programs at UI, and contribute to STEM education and training.<br/><br/>Specifically, the proposed instrument is a far-reaching cyberinfrastructure development for the research community and industry engaged with deep learning. The work will result in an advanced high-performing scalable instrument with capabilities far beyond those currently deployed in academia or industry to tackle large-scale deep learning projects. This instrument will serve as a focal point for a community-driven effort to advance the field of DL, integrating the work of computer scientists, systems engineers, and software developers. This project is transformative both in the systems architecture and domain science fields it will imbue, with new knowledge to be developed via new interactions and synergies that will emerge as part of this effort.<br/><br/>The proposed development of this well-integrated instrument will improve the quality and expand the scope of research and training, provide inter- and intra-organizational use amongst many disciplines, and engage private sector partners. The work will have deep and long-lasting effects on future computer architectures for compute- and data-intensive applications by making the blueprints of the novel system architecture of the instrument publically available. Access to the high-performance software developed through this project will aid numerous science domains that utilize DL frameworks. The unprecedented computational capabilities of many applications will make it possible to tackle complex science, engineering and societal problems in many important fields ranging from education, to healthcare, to artificial intelligence (AI). This project will strive to include participants from under-represented minority and female students, to make new discoveries, train, and educate a new generation of users fluent with DL tools and methodologies, contributing to the development of a highly educated, and diverse workforce with specialized skillsets. Finally, the work will enable new industry-academic collaborations benefiting both the scientific community and industry nationwide."
"1734082","D3SC and EAGER: Using Deep Learning to Find Algorithms for Optimizing Chemical Reactions","CHE","Chem Struct,Dynmcs&Mechansms B","09/01/2017","06/09/2017","Richard Zare","CA","Stanford University","Standard Grant","Tingyu Li","08/31/2020","$209,734.00","","zare@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","9102","7433, 7916, 8084","$0.00","With support from the Chemical Structure, Dynamics and Mechanisms - B Program in the Division of Chemistry and in response to the Data-Driven Discovery Science in Chemistry (D3SC) Dear Colleague Letter, Professor Richard N. Zare at Stanford University is working on optimizing chemical reactions in microdroplets with deep reinforcement learning.  Unoptimized reactions are expensive because they waste time and reagents.  A common way for chemists to explore reaction optimization is to change one variable at a time while all other variables remain fixed. This method, however, might not find the best conditions, that is the global optimum.  Another way is to search across all combinations of reaction conditions by using batch chemistry. This approach gives a better chance to find the global optimal condition, but it is time-consuming and expensive. Deep reinforcement learning is believed to be a superior approach in which the computer analyzes a large data set and recognizes the pattern of features that lead to best reaction outcomes.  It is like training a dog: suppose we want the dog to pick up a ball.  If the dog does what we want, we say ""Good dog!""; if it does not, we say ""Bad dog!"". Similarly, Professor Zare uses a machine learning method to give the system a positive reward if the reaction reaches a better result than previous ones, or a negative reward if it does not.  A repeated process will eventually result in a set of best reaction conditions for certain reactions.  Professor Zare and his group apply this approach to microdroplet chemistry, where many reactions can be carried out in small droplets and be accelerated by factors of one thousand to one million compared with the same reaction happening in bulk solution. Combining the efficient deep reinforcement learning method with accelerated microdroplet reactions, Professor Zare and his group are seeking to find optimal reaction conditions in a fast way.  This combined approach can represent a significant step for enabling artificial intelligence to be used to optimize chemical reactions, which should have benefits in chemical production, drug screening, and materials discovery.  The students in the Zare group enjoy the unique opportunity to experience micro-droplet chemical synthesis, fast chemical characterization, and deep learning-based complex data analysis.<br/><br/>A reaction can be thought of as a system having multiple inputs (parameters) and providing one or more outputs. Example inputs include: temperature; solvent composition; pH; catalyst; droplet size; and time. Example outputs include: product yield; selectivity; purity; and cost. The goal of reaction optimization described here is to select the best inputs to achieve a given output, which can be formulated as a reinforcement learning system. In order to find the optimal reaction conditions, Professor Zare is searching for critical reaction condition to try at the next step based on previous reaction conditions and product yields. A recurrent neural network is used to model the policy for reaction optimization. The reinforcement learning system is trained on mock reactions (random functions) and then real reactions for better performance.  The approach, if successful, could help better understanding of fundamental features of reactivity and enable important industrial applications."
"1715475","RI: Small: Integrating Flexible Normalization Models of Visual Cortex into Deep Neural Networks","IIS","Robust Intelligence","09/01/2017","08/17/2017","Odelia Schwartz","FL","University of Miami","Standard Grant","Kenneth Whang","08/31/2021","$349,996.00","","odelia@cs.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","7495","7495, 7923, 8089","$0.00","Recent advances in artificial intelligence models of deep neural networks have led to tremendous progress in artificial systems that recognize objects in scenes, and in a host of other applications such as speech recognition, and robotics. Although deep neural networks often incorporate computations inspired by the brain, these have typically been applied in a fairly simple and restrictive manner, rather than based on more principled models of neural processing in the brain. Using vision as a paradigmatic example, this project proposes that artificial systems can benefit from integrating approaches that have been developed in biological models of neural processing of scenes. The biological models make use of contextual flexibility, whereby neurons are influenced in a rich way by the image structure that spatially surrounds a given object or feature. This flexibility is expected to improve task performance in deep neural networks, and to impact development of artificial systems that are more compatible with human cognition. The resulting framework, with its deep architecture spanning multiple layers of processing, will, in turn, make predictions about neural processing in the brain, which will impact the neuroscience and cognitive science communities. <br/><br/>This project focuses specifically on normalization, a nonlinear computation that is ubiquitous in the brain, and that has been shown to benefit task performance in deep neural networks. The project will develop more principled strategies for determining normalization in deep convolutional neural networks. The main focus will be on learning a form of flexible normalization based on scene statistics models of visual cortex. In this framework, normalization is recruited only to the degree that a visual input is inferred to contain statistical dependencies across space. Performance will be tested for classification and segmentation on large-scale image databases, and will also target tasks more suited to mid-level vision such as figure/ground judgment. This will result in better understanding of normalization nonlinearities in deep convolutional networks, and the implications of flexible normalization for task performance and generalization compared to other forms of normalization. Biologically, normalization is poorly understood beyond primary visual cortex. The models developed will help shed light on the equivalence of this inference for middle cortical areas, and make predictions about what image structure leads to recruitment of normalization. This project will also include launching of an interdisciplinary Deep Learning Discussion Group."
"1718550","RI: Small: CompCog: Leveraging Deep Neural Networks for Understanding Human Cognition","IIS","Perception, Action & Cognition, Robust Intelligence","08/15/2017","08/04/2017","Thomas Griffiths","CA","University of California-Berkeley","Standard Grant","Kenneth Whang","07/31/2019","$448,284.00","","tomg@princeton.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7252, 7495","7495, 7923, 8089","$0.00","The last few years have seen significant breakthroughs in artificial intelligence and machine learning, resulting in systems that approach or even exceed human performance in interpreting pictures and words. This project explores the implications of these breakthroughs for understanding how the human mind works. Focusing on artificial neural networks, a key technology behind many recent breakthroughs that is capable of discovering novel representations for complex stimuli, the project has two goals. First, assessing the degree of correspondence between human and machine learning by examining whether the pictures or words that are similar in the representations discovered by neural network models are also judged to be similar by people. Second, developing methods for increasing this correspondence, with the goal of being able to use neural network representations to generate good predictions about how people learn and form categories using real images or text.<br/><br/>This research project will answer basic scientific questions about how the representations discovered by contemporary neural networks relate to human cognition. It will then explore what architectures and training regimes produce representations with these properties. In addition, the project will address the methodological question of how one can modify these representations to produce better alignment with human cognition. Answering this question will lead to powerful new tools for making models of human behavior in naturalistic contexts, leveraging the latest results in machine learning to broaden the scope of experimental research in cognitive science. By building stronger links between human and machine learning, this project will have implications for both fields. Even if current neural network systems turn out to differ significantly from human learning, they provide state-of-the-art representations for images and text that can be used as a starting point for developing better accounts of human representations. By discovering the ways in which the representations learned by artificial neural networks differ from those of humans, one can identify new algorithms and training methods that will result in a closer alignment. Since human beings remain the best examples available of systems that can solve certain problems, such an alignment offers a path toward expanding the capacities of current artificial intelligence systems and making them more interpretable by people, which is critical in settings that require human-machine interaction."
"1704938","RI: Medium: Collaborative Research: Incorporating Biologically-Motivated Circuit Motifs into Large-Scale Deep Neural Network Models of the Brain","IIS","Robust Intelligence","10/01/2017","08/16/2017","Kenneth Miller","NY","Columbia University","Standard Grant","Kenneth Whang","09/30/2020","$525,000.00","","kdm2103@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7495","7495, 7924, 8089","$0.00","This project studies the effects of incorporating, into deep neural networks for visual processing, several heretofore unincorporated features of biological visual cortical circuits. Deep neural networks are artificial circuits loosely inspired by the brain's cerebral cortex. Their abilities to solve complex problems, such as recognizing objects in visual scenes, have revolutionized artificial intelligence and machine learning in recent years. The hierarchy of layers in a deep network trained for visual object recognition also provides the best existing models of the hierarchy of areas in the visual cortex implicated in object recognition (the ""ventral stream""). This project seeks to understand whether and how incorporating additional features of brain circuits may (1) improve machine learning performance, particularly on tasks that are more challenging than those typically studied; and (2) yield improved models of visual cortex. Improving the performance of deep networks would yield great benefits across wide swaths of society and industry that are impacted by advances in artificial intelligence. Improved models of visual cortex will advance understanding of cortical function, which may lead to significant further benefits for understanding normal mental functioning and perception and their potential enhancement, as well as mental illness and perceptual and cognitive deficits. <br/><br/>Deep networks currently achieve their success using almost purely feedforward processing. Yet the visual cortical ventral stream that helped inspire deep networks also uses massive recurrent processing within each area as well as feedback connections from higher areas to lower areas and ""bypass"" connections from lower areas to areas multiple steps higher in the hierarchy. Deep networks also use ""neurons"" that can either excite or inhibit different neurons that they project to, whereas biological neurons are exclusively excitatory or inhibitory. This project will incorporate feedback and bypass connections into deep networks, as well as local recurrent processing in networks of separate excitatory and inhibitory neurons. Recent work by the investigators has shown how local recurrent processing explains a number of nonlinear visual cortical operations often summarized as ""normalization."" Simple forms of normalization currently used in deep networks maintain activities in an appropriate dynamic range, but the biological forms of normalization involve interactions between different stimulus features and locations in determining neural responses, which may have important computational roles e.g. in parsing visual scenes. The performance of deep networks incorporating these features will be assayed on a variety of visual tasks and as models of ventral stream neural data and human psychophysical data, and compared to performance of existing deep net models."
"1703161","RI: Medium: Collaborative Research: Incorporating Biological-Motivated Circuit Motifs into Large-Scale Deep Neural Network Models of the Brain","IIS","Robust Intelligence","10/01/2017","08/16/2017","Daniel Yamins","CA","Stanford University","Standard Grant","Kenneth Whang","09/30/2020","$524,779.00","","yamins@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","7495, 7924, 8089","$0.00","This project studies the effects of incorporating, into deep neural networks for visual processing, several heretofore unincorporated features of biological visual cortical circuits. Deep neural networks are artificial circuits loosely inspired by the brain's cerebral cortex. Their abilities to solve complex problems, such as recognizing objects in visual scenes, have revolutionized artificial intelligence and machine learning in recent years. The hierarchy of layers in a deep network trained for visual object recognition also provides the best existing models of the hierarchy of areas in the visual cortex implicated in object recognition (the ""ventral stream""). This project seeks to understand whether and how incorporating additional features of brain circuits may (1) improve machine learning performance, particularly on tasks that are more challenging than those typically studied; and (2) yield improved models of visual cortex. Improving the performance of deep networks would yield great benefits across wide swaths of society and industry that are impacted by advances in artificial intelligence. Improved models of visual cortex will advance understanding of cortical function, which may lead to significant further benefits for understanding normal mental functioning and perception and their potential enhancement, as well as mental illness and perceptual and cognitive deficits. <br/><br/>Deep networks currently achieve their success using almost purely feedforward processing. Yet the visual cortical ventral stream that helped inspire deep networks also uses massive recurrent processing within each area as well as feedback connections from higher areas to lower areas and ""bypass"" connections from lower areas to areas multiple steps higher in the hierarchy. Deep networks also use ""neurons"" that can either excite or inhibit different neurons that they project to, whereas biological neurons are exclusively excitatory or inhibitory. This project will incorporate feedback and bypass connections into deep networks, as well as local recurrent processing in networks of separate excitatory and inhibitory neurons. Recent work by the investigators has shown how local recurrent processing explains a number of nonlinear visual cortical operations often summarized as ""normalization."" Simple forms of normalization currently used in deep networks maintain activities in an appropriate dynamic range, but the biological forms of normalization involve interactions between different stimulus features and locations in determining neural responses, which may have important computational roles e.g. in parsing visual scenes. The performance of deep networks incorporating these features will be assayed on a variety of visual tasks and as models of ventral stream neural data and human psychophysical data, and compared to performance of existing deep net models."
"1717896","AF: Small: Efficiently Learning Neural Network Architectures with Applications","CCF","Algorithmic Foundations","09/01/2017","06/26/2017","Adam Klivans","TX","University of Texas at Austin","Standard Grant","A. Funda Ergun","08/31/2020","$449,920.00","","klivans@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7796","7923, 7926","$0.00","In the last few years there have been several breakthroughs in machine learning and artificial intelligence due to the success of tools for learning ""deep neural networks"" including the best computer program for playing Go, the best programs for automatically playing Atari games, and the best tools for several fundamental object-recognition tasks.  These are considered some of the most exciting new results in all of computer science.<br/><br/>From a theoretical perspective, however, the mathematics underlying these neural networks is not as satisfying.  We have few rigorous results that explain how and why heuristics for learning deep neural networks perform so well in practice. The primary research goal of this proposal is to develop provably efficient algorithms for learning neural networks that have rigorous performance guarantees and give applications to related problems from machine learning.  Given the ubiquity of machine learning algorithms, this research will have direct impact on data science problems from a diverse set of fields including biology (protein interaction networks) and security (differential privacy).  The PI is also developing a new data mining course at UT-Austin that will incorporate the latest research from these areas.<br/><br/>A central technical question of this work is that of the most expressive class of neural networks that can be provably learned in polynomial time.  Furthermore, the algorithm should be robust to noisy data.  A neural network can be thought of as a type of directed circuit where the internal nodes compute some activation function of a linear combination of the inputs.  The classical example of an activation function is a sigmoid, but the ReLU (rectified linear unit) has become very popular.  In a recent work, the PI showed that a neural network consisting of a sum of one layer of sigmoids is learnable in fully-polynomial time, even in the presence of noise.  This is the most expressive class known to be efficiently learnable.  Can this result be extended to more sophisticated networks?  This question has interesting tie-ins to kernel methods and kernel approximations.<br/><br/>For the ReLU activiation, the PI has shown that this problem is most likely computationally intractable in the worst case.  The intriguing question then becomes that of the minimal assumptions needed to show that these networks are computationally tractable.  In a recent work, the PI has shown that there are distributional assumptions that imply fully-polynomial-time algorithms for learning sophisticated networks of ReLUs.  Can these assumptions be weakened?  This work has to do with proving that certain algorithms do not overfit by using compression schemes.  Another type of assumption that the weights of the unknown network are chosen in some random way (as opposed to succeeding in the worst-case).  This corresponds to the notion of random initialization from machine learning.  Can we prove a type of smoothed analysis for learning neural networks, where we can give fully-polynomial-time learning algorithms for almost all networks?<br/><br/>Finally, in this proposal we will explore what other tasks can be reduced to various types of simple neural network learning.  For example, the problem of one-bit compressed sensing can be viewed as learning a threshold activation using as few samples as possible.  Still, we lack a one-bit compressed sensing algorithm that has optimal tolerance for noise.  Another canonical example is matrix or tensor completion, where it is possible to reduce these challenges to learning with respect to polynomial activations.  Finding the proper regularization to ensure low sample complexity is an exciting area of research."
"1657193","CRII: RI: Learning Structured Prediction Models with Auxiliary Supervision","IIS","CRII CISE Research Initiation","03/01/2017","02/23/2017","Kai-Wei Chang","VA","University of Virginia Main Campus","Standard Grant","Weng-keen Wong","11/30/2017","$174,252.00","","kw@kwchang.net","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","026Y","7495, 8228","$0.00","Many machine learning problems involve making joint predictions over a set or a sequence of mutually dependent outputs. As an example, consider recognizing a handwritten word, where each character must be recognized in order for the word to be understood. It is important to consider the correlations between the predictions of adjacent characters to aid the individual predictions of characters. Structured prediction models are proposed to solve problems of this type. They have been shown to be successful in many real-world applications, including speech recognition, natural language understanding, and object detection in images. Despite its success, training a structured prediction model requires an extensive collection of training data. However, obtaining human annotations with complex structures is costly. For example, it takes a professionally trained linguist several minutes to label a syntactic parse tree for a single sentence, making it expensive to obtain high-quality annotations. The objective of this research is to develop methods that utilize learning signals that do not directly aim to achieve the target tasks. The outcome of this project will create a fundamental shift in the applicability of structured prediction models, enabling applications in which complex decisions are required and annotated data are expensive to acquire. This will bring new collaboration opportunities with other areas, including education, healthcare, and social and behavioral sciences.<br/><br/>The goals of this project are to study algorithms for training structured prediction models from heterogeneous learning signals, design automatic algorithms for mining useful information from massive columns of structured and unstructured data, and apply the proposed techniques in real-world applications. The proposed algorithms will be evaluated on a broad range of natural language processing applications, including the algebra word problem, co-reference resolution, and grammatical error correction. The results of the project will be disseminated by publishing papers, releasing open-source software and data sets, organizing workshops and tutorials, and creating new courses on natural language processing and machine learning."
"1760523","CRII: RI: Learning Structured Prediction Models with Auxiliary Supervision","IIS","CRII CISE Research Initiation","07/01/2017","10/17/2017","Kai-Wei Chang","CA","University of California-Los Angeles","Standard Grant","Weng-keen Wong","02/29/2020","$170,865.00","","kw@kwchang.net","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","026Y","7495, 8228","$0.00","Many machine learning problems involve making joint predictions over a set or a sequence of mutually dependent outputs. As an example, consider recognizing a handwritten word, where each character must be recognized in order for the word to be understood. It is important to consider the correlations between the predictions of adjacent characters to aid the individual predictions of characters. Structured prediction models are proposed to solve problems of this type. They have been shown to be successful in many real-world applications, including speech recognition, natural language understanding, and object detection in images. Despite its success, training a structured prediction model requires an extensive collection of training data. However, obtaining human annotations with complex structures is costly. For example, it takes a professionally trained linguist several minutes to label a syntactic parse tree for a single sentence, making it expensive to obtain high-quality annotations. The objective of this research is to develop methods that utilize learning signals that do not directly aim to achieve the target tasks. The outcome of this project will create a fundamental shift in the applicability of structured prediction models, enabling applications in which complex decisions are required and annotated data are expensive to acquire. This will bring new collaboration opportunities with other areas, including education, healthcare, and social and behavioral sciences.<br/><br/>The goals of this project are to study algorithms for training structured prediction models from heterogeneous learning signals, design automatic algorithms for mining useful information from massive columns of structured and unstructured data, and apply the proposed techniques in real-world applications. The proposed algorithms will be evaluated on a broad range of natural language processing applications, including the algebra word problem, co-reference resolution, and grammatical error correction. The results of the project will be disseminated by publishing papers, releasing open-source software and data sets, organizing workshops and tutorials, and creating new courses on natural language processing and machine learning."
"1709641","BRAIN: Brain-Inspired Memristive Nanofiber Neural Networks","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/01/2017","06/26/2017","Juan Nino","FL","University of Florida","Standard Grant","Anthony Kuh","07/31/2020","$323,660.00","","jnino@mse.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","ENG","7607","155E","$0.00","The human brain is currently the most powerful information processor known to man. Recent advances in neural networks and network science indicate that in order to match the power and efficiency of the brain, there is a need for a brand-new type of neuromorphic hardware that is able to connect physically independent neurons with dedicated, modifiable synapses. The PI and coworkers have developed a brain-inspired concept where, preliminary theoretical and experimental results show that a mat of memristive nanofibers is anticipated to yield neural networks with enhanced connectivity, functionality, and overall performance. This project aims at assessing the potential of this concept and it is guided by the overarching fundamental question can these brain-inspired memristive nanofiber neural network (MN3) architectures be effectively used for advanced neuromorphic computing.<br/><br/>The intellectual merit of the project stems from its goal to investigate the potential of MN3 architectures as the basis for novel neural network architectures that emulate the brain's computational abilities. This BRAIN project has three main objectives: 1) Manufacture MN3 architectures based on connective matrices of conductive-core, memristive-shell nanofibers and electrically characterize the networks in order to compare their characteristics to theoretical simulations; 2) Develop a simulation framework for modeling the proposed MN3 architectures in order to investigate and predict the signal behavior and computational properties of the networks; and 3) Investigate methods for implementing and training the networks as artificial neural networks, and evaluate the resulting networks on a set of benchmark machine learning tasks to determine performance characteristics. <br/><br/>The broader impacts of the project can be summarized in four main areas: a) investigation of a new brain-inspired design paradigm for fabricating neural networks that, if successful, can potentially transform the broad fields of neuromorphic hardware and machine learning; b) advancement of the discovery and understanding of neural network architectures while training undergraduate and graduate students in STEM fields; c) dissemination of the gained scientific and technological understanding of memristive networks through professional conferences, peer-review publications, and online hubs; and d) dissemination of tutorials and workshops on Artificial Intelligence targeting the general population in collaboration with the Cade Museum."
"1713691","Nonlinear Partial Differential Equations, Monotone Numerical Schemes, and Scaling Limits for Semi-Supervised Learning on Graphs","DMS","APPLIED MATHEMATICS","08/15/2017","08/08/2017","Jeffrey Calder","MN","University of Minnesota-Twin Cities","Standard Grant","Victor Roytburd","07/31/2020","$163,136.00","","jwcalder@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1266","","$0.00","Machine learning aims to harness the power of data to train algorithms to make predictions and perform complex tasks in science, engineering, medicine, and everyday life.  Despite the wide success of machine learning, the growing size of typical data sets is creating serious computational challenges, partially due to our incomplete understanding of how algorithms work in the big data regime.  The first goal of this project is to address these issues within a branch of machine learning called graph-based semi-supervised learning, which has found applications in problems such as protein sequencing, website classification, and speech recognition.  The investigator analyzes some recently-proposed algorithms for semi-supervised learning.  A deep mathematical understanding of these algorithms explains why they work and, more importantly, when they will fail.  The investigator uses these insights to develop new, fast, and more efficient algorithms for semi-supervised learning.  The second goal of this project is to develop and study new algorithms for curvature motions of curves and surfaces that are stable, efficient, and convergent.  Curvature motion has found many applications in science and engineering, including materials science, computer vision, image processing, and recently in data science and machine learning.  Curvature motion describes, for example, the motion of soap bubbles as well as the evolution of polycrystalline materials (such as metals and ceramic) during the manufacturing process.  Because both machine learning and curvature motions have very broad applications across science and engineering, any improvement in algorithms and in our understanding of them could have broad societal impacts.<br/><br/>The first objective of this project is to study rigorously a recently-proposed algorithm for graph-based semi-supervised learning based on Lp-Laplacian regularization.  In particular, the investigator proves that the learning algorithms have continuum limits that correspond to solving a weighted p-Laplace equation in the viscosity sense.  In the limit of small labeled data and infinite unlabeled data, it has recently been conjectured that Lp-Laplacian regularization is ill-posed when p is smaller than the intrinsic dimension of the data.  The project aims to settle this conjecture rigorously.  The investigator uses these continuum limits to study the relationship between the fraction of labeled data and the performance of the algorithms, and to develop new and more efficient computational algorithms based on these insights.  The second objective of the project is to develop and analyze a new class of monotone finite difference schemes for curvature-driven motions of curves and surfaces, for which rigorous convergence proofs are available.  The investigator has recently discovered a general technique for constructing monotone finite difference schemes for a wide class of curvature motions of curves and surfaces.  He implements the new schemes for a variety of motions, experimentally tests convergence rates, and rigorously proves convergence to the viscosity solution using the Barles-Souganidis framework.  Monotonicity of the new schemes allows for the development of fully implicit and unconditionally monotone time-stepping schemes, and guarantees the approximations are capturing the correct continuum dynamics."
"1651142","Conference:   Perceptrons and Syntactic Structures at 60: Computational Modeling of Language","BCS","Linguistics, Robust Intelligence","08/01/2017","07/19/2017","Joseph Pater","MA","University of Massachusetts Amherst","Standard Grant","William Badecker","01/31/2019","$24,184.00","Brendan O'Connor","pater@linguist.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","SBE","1311, 7495","1311, 7495, 7556","$0.00","This workshop will bring together leading researchers in cognitive science and artificial intelligence who specialize in the integration of linguistic theory with statistical approaches, especially neural networks. Neural networks have been important in many of the recent advances in language technologies (in what's called ""deep learning""), and the greater integration of linguistic structure into these models promises to lead to further breakthroughs. <br/><br/>The main focus of this meeting will be on how integration of models of linguistic structure with probabilistic learning theories may lead to a deeper understanding of the way that humans process and represent language. This sort of integration has been difficult to achieve in the past in part because of the separation of researchers in each tradition into different disciplines interacting in different conferences. In-depth analysis of the structure of human languages is conducted in mostly in Linguistics, while neural network modeling and other statistical learning research is conducted mostly in Psychology and Computer Science. The workshop will be held as part of the inaugural meeting of the Society for Computation in Linguistics, taking place concurrently with the meeting of the Linguistic Society of America. It will thus bring researchers from other disciplines into contact with linguists, and will stimulate productive intellectual exchange."
"1704860","AF: Large: Collaborative Research: Nonconvex Methods and Models for Learning: Toward Algorithms with Provable and Interpretable Guarantees","CCF","Special Projects - CCF, Algorithmic Foundations","06/01/2017","06/25/2020","Sanjeev Arora","NJ","Princeton University","Continuing Grant","A. Funda Ergun","05/31/2022","$1,363,968.00","Elad Hazan, Yoram Singer","arora@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","2878, 7796","7796, 7925, 7926","$0.00","Artificial Intelligence along with Machine Learning are perhaps the most dominant research themes of our times - with far reaching implications for society and our current life style. While the possibilities are many, there are also doubts about how far these methods will go - and what new theoretical foundations may be required to take them to the next level overcoming possible hurdles. Recently, machine learning has undergone a paradigm shift with increasing reliance on  stochastic optimization to train highly non-convex models -- including but not limited to deep nets. Theoretical understanding has lagged behind, primarily because most problems in question are provably intractable on worst-case instances. Furthermore, traditional machine learning theory is mostly concerned with classification, whereas much practical success is driven by unsupervised learning and representation learning. Most past theory of representation learning was focused on simple models such as k-means clustering and PCA, whereas  practical work uses vastly more complicated models like autoencoders, restricted Boltzmann machines and deep generative models. The proposal presents an ambitious agenda for extending theory to embrace and support these practical trends, with hope of influencing practice. Theoretical foundations will be provided for the next generation of machine learning methods and optimization algorithms. <br/><br/>The project may end up having significant impact on  practical machine learning, and even cause a cultural change in the field -- theory as well as practice -- with long-term ramifications. Given the ubiquity as well as  economic and scientific implications of machine learning today, such impact will extend into other disciplines, especially in (ongoing) collaborations with researchers in neuroscience. The project will train a new generation of machine learning researchers, through an active teaching and mentoring plan at all levels, from undergrad to postdoc. This new generation will be at ease combining cutting edge theory and applications. There is a pressing need for such people today, and the senior PIs played a role in training/mentoring several existing ones.<br/> <br/>Technical contributions will include new theoretical models of knowledge representation and semantics, and also frameworks for proving convergence of nonconvex optimization routines. Theory will be developed to explain and exploit the interplay between representation learning and supervised learning that has proved so empirically successful in deep learning, and seems to underlie new learning paradigms such as domain adaptation, transfer learning, and interactive learning. Attempts will be made to replace neural models with models with more ""interpretable""  attributes and performance curves.  All PIs have a track record of combining theory with practice. They  are also devoted to a heterodox research approach, borrowing from all the past phases of machine learning: interpretable representations from the earlier phases (which relied on logical representations, or probabilistic models), provable guarantees from the middle phase (convex optimization, kernels etc.), and an embrace of nonconvex methods from the latest deep net phase. Such eclecticism is uncommon in machine learning, and may give rise to new paradigms and new kinds of science."
"1704656","AF: Large: Collaborative Research: Nonconvex Methods and Models for Learning: Towards Algorithms with Provable and Interpretable Guarantees","CCF","Algorithmic Foundations","06/01/2017","06/25/2020","Rong Ge","NC","Duke University","Continuing Grant","A. Funda Ergun","05/31/2022","$402,819.00","","rongge@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","7925, 7926, 9251","$0.00","Artificial Intelligence along with Machine Learning are perhaps the most dominant research themes of our times - with far reaching implications for society and our current life style. While the possibilities are many, there are also doubts about how far these methods will go - and what new theoretical foundations may be required to take them to the next level overcoming possible hurdles. Recently, machine learning has undergone a paradigm shift with increasing reliance on  stochastic optimization to train highly non-convex models -- including but not limited to deep nets. Theoretical understanding has lagged behind, primarily because most problems in question are provably intractable on worst-case instances. Furthermore, traditional machine learning theory is mostly concerned with classification, whereas much practical success is driven by unsupervised learning and representation learning. Most past theory of representation learning was focused on simple models such as k-means clustering and PCA, whereas  practical work uses vastly more complicated models like autoencoders, restricted Boltzmann machines and deep generative models. The proposal presents an ambitious agenda for extending theory to embrace and support these practical trends, with hope of influencing practice. Theoretical foundations will be provided for the next generation of machine learning methods and optimization algorithms. <br/><br/>The project may end up having significant impact on  practical machine learning, and even cause a cultural change in the field -- theory as well as practice -- with long-term ramifications. Given the ubiquity as well as  economic and scientific implications of machine learning today, such impact will extend into other disciplines, especially in (ongoing) collaborations with researchers in neuroscience. The project will train a new generation of machine learning researchers, through an active teaching and mentoring plan at all levels, from undergrad to postdoc. This new generation will be at ease combining cutting edge theory and applications. There is a pressing need for such people today, and the senior PIs played a role in training/mentoring several existing ones.<br/> <br/>Technical contributions will include new theoretical models of knowledge representation and semantics, and also frameworks for proving convergence of nonconvex optimization routines. Theory will be developed to explain and exploit the interplay between representation learning and supervised learning that has proved so empirically successful in deep learning, and seems to underlie new learning paradigms such as domain adaptation, transfer learning, and interactive learning. Attempts will be made to replace neural models with models with more ""interpretable""  attributes and performance curves.  All PIs have a track record of combining theory with practice. They  are also devoted to a heterodox research approach, borrowing from all the past phases of machine learning: interpretable representations from the earlier phases (which relied on logical representations, or probabilistic models), provable guarantees from the middle phase (convex optimization, kernels etc.), and an embrace of nonconvex methods from the latest deep net phase. Such eclecticism is uncommon in machine learning, and may give rise to new paradigms and new kinds of science."
"1745410","EAGER: Deep Learning-Based Self-Organizing Network for B5G Communications with Massive IoT Devices","ECCS","CCSS-Comms Circuits & Sens Sys","09/01/2017","07/20/2017","Meryem Simsek","CA","International Computer Science Institute","Standard Grant","Lawrence Goldberg","08/31/2019","$149,217.00","","simsek@ICSI.Berkeley.EDU","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","ENG","7564","153E, 7916","$0.00","Future wireless networks are expected to provide a platform for highly reliable and ultra-low latency information exchange that will revolutionize the way machines communicate and how people interact. A vast number of devices, particularly sensors and actors, is anticipated to be able to connect to a single wireless access point, empowering the deployment of massive Internet of Things devices that generate an enormous amount of information, namely Big Data. The management of both, the wireless connections of the devices and the Big Data they generate, requires substantial changes to legacy wireless networks. The goal of this project is to conduct exploratory research on such changes, which, if successful, will deliver a key corner stone to the future deployment of massive Internet of Things. This deployment will constitute an essential component in various aspects of life, such as improvements in consumer products, e.g. home automation and better consumer oriented entertainment; in health, through smart body sensors and remote, low-cost diagnosis; in more cost-efficient manufacturing; in environmental monitoring, e. g., for enhancing environmental conditions like reduced air pollution; or in the realization of smart cities.<br/><br/>Following the recent great success of artificial intelligence, machine learning, and specifically deep learning in many different applications, the goal of this project is to explore and to perform research on novel deep-learning-assisted autonomous decision-making approaches for the self-management of future wireless communications networks. The problem to be solved is to establish an efficient and effective wireless network self-management architecture that is flexible enough to facilitate the deployment and operation of diverse massive Internet of Things devices. This architecture is expected to be able to exploit hidden information in Big Data generated by the Internet of Things devices through deep learning techniques. The research into this architecture comprises the exploration of novel machine learning algorithms for the intelligent, predictive, and autonomous allocation of radio and network resources at different layers of the wireless network. Ultimately, a novel multi-layer self-organizing network architecture will be introduced that fully exploits the flexibility and capability of future wireless networks for servicing massive Internet of Things. The developed approaches will lay the foundations of substantially enhancing legacy self-organizing wireless networks and will impact the design of future wireless networks, its efficiency, and the realization of various emerging use cases generating Big Data likewise. The results will also provide novel insights into the role of Big Data for the communications industry, potentially fostering the development of new business models and strengthening the industry's national and international competitiveness."
"1723344","AitF: Collaborative Research: Algorithms for Probabilistic Inference in the Real World","CCF","Algorithms in the Field","01/01/2017","12/28/2016","David Sontag","MA","Massachusetts Institute of Technology","Standard Grant","A. Funda Ergun","08/31/2021","$399,999.00","","dsontag@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7239","","$0.00","Statistical models provide a powerful means of quantifying uncertainty, modeling prior beliefs, and describing complex dependencies in data.  The process of using a model to answer specific questions, such as inferring the state of several random variables given evidence observed about others, is called probabilistic inference.  Probabilistic graphical models, a type of statistical model, are often used in diverse applications such as medical diagnosis, understanding protein and gene regulatory networks, computer vision, and language understanding.  On account of the central role played by probabilistic graphical models in a wide range of automated reasoning applications, designing efficient algorithms for probabilistic inference is a fundamental problem in artificial intelligence and machine learning.<br/> <br/>Probabilistic inference in many of these applications corresponds to a complex combinatorial optimization problem that at first glance appears to be extremely difficult to solve.  However, practitioners have made significant strides in designing heuristic algorithms to perform real-world inference accurately and efficiently.  This project focuses on bridging the gap between theory and practice for probabilistic inference problems in large-scale machine learning systems.  The PIs will identify structural properties and methods of analysis that differentiate real-world instances from worst-case instances used to show NP-hardness, and will design efficient algorithms with provable guarantees that would apply to most real-world instances.  The project will also study why heuristics like linear programming and other convex relaxations are so successful on real-world instances.  The efficient algorithms for probabilistic inference developed as part of this project have the potential to be transformative in machine learning, statistics, and more applied areas like computer vision, social networks and computational biology.  To help disseminate the research and foster new collaborations, a series of workshops will be organized bringing together the theoretical computer science and machine learning communities.  Additionally, undergraduate curricula will be developed that use machine learning to introduce students to concepts in theoretical computer science."
"1659585","REU Site: Language, Cognition and Computation","SMA","","06/15/2017","06/13/2017","Michael Frank","CA","Stanford University","Standard Grant","Josie S. Welkom","05/31/2020","$283,231.00","Christopher Potts","mcfrank@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","SBE","Q170","7736, 9250","$0.00","This site is supported by the Department of Defense in partnership with the NSF's Research Experiences for Undergraduates (REU) Sites program. The REU program has both scientific and societal benefits integrating research and education. Recent developments in cognitive science have led to breakthrough new scientific results and are providing the basis for exciting new applications in areas like social computing and assistive technologies. These developments present a challenge for education, however. Even at top research universities, students are hard-pressed to receive the appropriate training; the situation is even more difficult at institutions that do not provide extensive research training. This REU addresses this challenge. Based at Stanford's Center for the Study of Language and Information (CSLI), a top institution for interdisciplinary cognitive science, the program provides talented undergraduates from diverse backgrounds with both an opportunity to do mentored research in a top laboratory and a supportive program framework that includes technical training, professional development, and academic discussion.<br/><br/>The scientific and technological innovations motivating this REU derive from a convergence within the core disciplines of cognitive science -- psychology, linguistics, and computer science -- around themes of uncertainty, approximation, and learning. As psychology and linguistics are becoming more computational, computation is returning to its cognitive roots. Artificial intelligence techniques developed in psychology are undergoing a resurgence in machine learning, and natural language processing models of syntactic structure are becoming the standard cognitive modeling frameworks in psycholinguistics. The prerequisites for research in this new intellectual environment include an understanding of how the mind works, familiarity with the nature of human language and communication, proficiency in statistical analysis, and advanced programming skills. Yet a classic psychology or linguistics degree provides almost no programming or technical experience, and a standard computer science education doesn't include any content on how the mind works. This REU fills such gaps in the training of undergraduates and helps to foster a new, more diverse generation of researchers entering cognitive science."
"1650587","University of Oregon Planning Proposal: I/UCRC for Big Learning","CNS","INDUSTRY/UNIV COOP RES CENTERS","02/15/2017","02/21/2017","Dejing Dou","OR","University of Oregon Eugene","Standard Grant","Dmitri Perkins","01/31/2018","$15,000.00","Hai Phan, Daniel Lowd, Allen Malony, Stephen Fickas","dou@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","5761","5761","$0.00","The proposed NSF I/UCRC Center for Big Learning (CBL) consists of multi-disciplinary experts at the four founding universities that are geographically distributed across the country: University of Oregon (UO, West), Carnegie Mellon University (CMU, East), University of Missouri at Kansas City (UMKC, Central), and University of Florida (UF, South).  The mission of this center is to explore research frontiers in the design of novel algorithms and developing efficient systems for deep learning research and its applications in the era of big data and big systems. Through a multi-site and multi-disciplinary consortium, the CBL center at the UO will focus on key applications of large-scale deep learning involving multi-modal media (i.e., text, image, and Q&A) in various domains (i.e., health, life science, IoT/mobile, and business) relying on strong support from the industry partners. The proposed multidisciplinary center will offer important opportunities for training new scientists and graduate students, and provide an environment for cross-disciplinary engagement.<br/><br/>The research team at the UO includes experts in data science, artificial intelligence, machine learning, high performance computing, IoT, health informatics, and bioinformatics. The CBL at the UO seeks to catalyze the fusion of expertise from academia, government, and industry stakeholders related to the rapid innovation in algorithms, systems, applications as well as education, and technology transfer into cutting-edge products and services with real-world relevance and significance. The UO site will explore several research projects related to health behavior modeling, activity recommendation, social network analysis, and privacy preserving by deploying various deep learning models. The planning activities will lead to a successful proposal for the establishment of the CBL center with a solid consortium across multiple campuses and a large number of industry partners. Our proposed meetings, forums, conferences, and planned training sessions will greatly promote and broaden the research and materialization of large-scale deep learning."
"1714855","ACL 2017 Student Research Workshop","IIS","Robust Intelligence","02/01/2017","01/18/2017","Marine Carpuat","MD","University of Maryland College Park","Standard Grant","Tatiana Korelsky","01/31/2019","$15,000.00","","marine@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7556","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization for computational linguistics and natural language processing.  It also is one of the primary application areas for researchers in machine learning and artificial intelligence.  The proceedings of its annual meeting provide the foundation of the field; it is the most cited and most respected publication in computational linguistics.  Thus, it is also the most important gathering of researchers in computational linguistics and natural language processing. This project is to subsidize travel, conference and housing expenses of students selected to participate in the ACL Student Research Workshop, which will take place during the main ACL conference from July 31 - August 2, 2017 in Vancouver, Canada.  The Student Research Workshop helps create a new generation of researchers with a more thorough understanding of their field, with connections and collaborations across institutions, and with innovative and exciting research programs.  This contributes to America's pool of researchers with the needed scientific and engineering knowledge and skills. The workshop encourages a spirit of collaborative research and builds a supportive environment for a new generation of computational linguists.<br/><br/>The Student Research Workshop solicits two categories of submissions: research papers and research proposals. The research proposal can have only one author who must be a student. The research papers can have multiple authors, with the first author being a student (at either the graduate or undergraduate level).  The workshop is a venue for students to receive constructive critical feedback on their work from experts outside of their institution, and to connect with other students and senior researchers in their field.  The students gain exposure by presenting their work earlier than they would otherwise (i.e., in a form not yet ready for the main conference). This is particularly valuable for students from smaller institutions and undergraduate students. In addition, the workshop is organized and run by students. The student organizers gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference."
"1745535","Student Travel Support for 2017 Workshop for Women and Underrepresented Minorities in NLP","IIS","ROBUST INTELLIGENCE","07/01/2017","06/21/2017","Marine Carpuat","MD","University of Maryland College Park","Standard Grant","Tatiana Korelsky","06/30/2018","$20,000.00","","marine@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7556","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization for computational linguistics and natural language processing.  It also is one of the primary application areas for researchers in  machine learning and artificial intelligence.  The proceedings of its annual meeting provide the foundation of the field; it is the most cited and most respected publication in computational linguistics. Yet demographic representation within the authors and leaders in the ACL community is notably lop-sided. This project is to subsidize travel to the workshop for women and underrepresented minorities in NLP (WiNLP), which will be a part of the 2017 meeting of the ACL held in Vancouver, Canada  on July 30, 2017.  The workshop aims to highlight and foster the work by researchers of diverse backgrounds within the ACL community. This contributes to America's pool of researchers with the variety of backgrounds, experiences, interests and areas of expertise required for science and engineering to be successful and benefit broad segments of society.<br/> <br/>The WiNLP workshop provides a venue for women and underrepresented minorities to present their work, and to get feedback and advice from mentors who are senior researchers in the field. The workshop solicits submissions in the form of a 2-page extended abstract focusing on all areas of NLP. Accepted abstracts are presented either as a poster of a talk. The workshop will also include a mentoring sessions and invited talks by established researchers in the field. This enables women and underrepresented minorities to get feedback from the broad ACL community, to participate in that community, and for that community to become more aware of their work."
"1659788","REU Site: Machine Learning in Natural Language Processing and Computer Vision","IIS","RSCH EXPER FOR UNDERGRAD SITES","06/01/2017","03/14/2017","Jugal Kalita","CO","University of Colorado at Colorado Springs","Standard Grant","Wendy Nilsen","05/31/2020","$379,853.00","Jonathan Ventura","jkalita@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","1139","9250","$0.00","The proposal seeks to increase the number of American citizens and permanent resident undergraduates who are attracted to careers in research and advanced studies in Computer Science. Training in theoretical and empirical machine learning will enable participants to contribute to ubiquitous software-based technologies at the highest levels in innovative ways. The proposal will also focus almost exclusively on training future computer scientists from institutions with limited research opportunities, women and under-represented minorities. The research experience will encourage these students to be productive researchers in academic and non-academic environments during their future careers. Computer vision, through applications such as face recognition and autonomous vehicles, has profound implications for society in terms of future of national security and transportation. Natural language processing, through applications such as efficient large volume semantic analysis and summarization of textual documents from disparate sources, also has implications for national security in addition to numerous other possibilities that can be exploited by industry.  <br/><br/>The objective of this proposal is to expose bright and motivated undergraduates who want to pursue advanced careers in Computer Science to active research experience early in their careers. This proposal seeks to develop an REU site to broaden the intellectual horizon of participants through exposure to opportunities available in university research.  Students will be involved in research projects in machine learning techniques and in emerging applications that exploit machine learning.  The applications of interest are in the fields of natural language processing and computer vision.  Students will have the opportunity to utilize a combination of theoretical reading, analysis, and research within laboratory environments.  The students will be introduced to the topics and helped to obtain in-depth understanding of selected topics through introductory presentations. Then, they will perform hands-on research on novel problems, conduct experiments, and communicate their results through written papers and presentations. The REU students will be involved in research in machine learning and its applications in diverse and emergent areas alongside faculty mentors and graduate students in a university environment. The research will involve undergraduate students in cutting-edge research where they will write software to solve interesting and timely problems and write papers for publication."
"1704662","NeTS: Medium: Collaborative Research: Big Data Enabled Wireless Networking: A Deep Learning Approach","CNS","Networking Technology and Syst","08/15/2017","07/03/2019","Jian Tang","NY","Syracuse University","Continuing Grant","Alexander Sprintson","07/31/2021","$700,000.00","Yanzhi Wang","jtang02@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7363","7924","$0.00","Wireless networks are becoming larger and more complicated, generating a huge amount of runtime statistics data (such as traffic load, resource usages, etc.) every second. Instead of treating big data in wireless networks as an unwanted burden, we aim to leverage them as a great opportunity for better understanding user demands and system capabilities such that we can optimize resource allocation to better serve mobile users. In addition, Cloud Radio Access Networks (C-RANs) have become a key enabling technology for the next generation wireless communication systems. Their centralized architecture makes it easy to collect and analyze various runtime system data. This project aims to exploit how the powerful new machine learning techniques, including Deep Learning (DL) and Deep Reinforcement Learning (DRL), can be leveraged to grasp the exciting opportunity provided by big data to enable future wireless networks to better serve their users. The proposed research is expected to significantly improve resource utilization of wireless networks and reduce their operational costs (such as power consumption), which can substantially benefit wireless network carriers and mobile users, and more importantly, is good for global environment. Beyond wireless networking, the proposed DL models and algorithms may find its applications in a large variety of domains, including video content analysis, user behavior study, etc. Moreover, the proposed project is expected to advance public understanding of the emerging 5G wireless communications, DL and DRL via publications, seminars and workshops, and international and industrial collaborations.<br/> <br/>The objective of this project is to develop a novel deep learning approach to enable efficient design and operations of future wireless networks with big data. Specifically, we will propose DL models and algorithms for spatiotemporal analysis and prediction of key system parameters, which can provide accurate and useful input information for existing resource allocation algorithms to better operate a wireless network. Moreover, we will develop a novel DRL-based control framework for a wireless network to efficiently allocate its resources by jointly learning the system environment and making decisions under the guidance of a powerful deep neural network. To achieve the above object, the project is organized into three cohesive thrusts: Thrust 1 Deep Learning based Modeling and Prediction; Thrust 2 Deep Reinforcement Learning based Dynamic Resource Allocation; and Thrust 3 Validation and Performance Evaluation."
"1704092","NeTS: Medium: Collaborative Research: Big Data Enabled Wireless Networking: A Deep Learning Approach","CNS","Networking Technology and Syst","08/15/2017","08/02/2018","Guoliang Xue","AZ","Arizona State University","Continuing Grant","Alexander Sprintson","07/31/2021","$500,000.00","","xue@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7363","7924","$0.00","Wireless networks are becoming larger and more complicated, generating a huge amount of runtime statistics data (such as traffic load, resource usages, etc.) every second. Instead of treating big data in wireless networks as an unwanted burden, we aim to leverage them as a great opportunity for better understanding user demands and system capabilities such that we can optimize resource allocation to better serve mobile users. In addition, Cloud Radio Access Networks (C-RANs) have become a key enabling technology for the next generation wireless communication systems. Their centralized architecture makes it easy to collect and analyze various runtime system data. This project aims to exploit how the powerful new machine learning techniques, including Deep Learning (DL) and Deep Reinforcement Learning (DRL), can be leveraged to grasp the exciting opportunity provided by big data to enable future wireless networks to better serve their users. The proposed research is expected to significantly improve resource utilization of wireless networks and reduce their operational costs (such as power consumption), which can substantially benefit wireless network carriers and mobile users, and more importantly, is good for global environment. Beyond wireless networking, the proposed DL models and algorithms may find its applications in a large variety of domains, including video content analysis, user behavior study, etc. Moreover, the proposed project is expected to advance public understanding of the emerging 5G wireless communications, DL and DRL via publications, seminars and workshops, and international and industrial collaborations.<br/> <br/>The objective of this project is to develop a novel deep learning approach to enable efficient design and operations of future wireless networks with big data. Specifically, we will propose DL models and algorithms for spatiotemporal analysis and prediction of key system parameters, which can provide accurate and useful input information for existing resource allocation algorithms to better operate a wireless network. Moreover, we will develop a novel DRL-based control framework for a wireless network to efficiently allocate its resources by jointly learning the system environment and making decisions under the guidance of a powerful deep neural network. To achieve the above object, the project is organized into three cohesive thrusts: Thrust 1 Deep Learning based Modeling and Prediction; Thrust 2 Deep Reinforcement Learning based Dynamic Resource Allocation; and Thrust 3 Validation and Performance Evaluation."
"1659774","REU Site: Carnegie Mellon University Robotics Institute REU Site","IIS","RSCH EXPER FOR UNDERGRAD SITES","06/01/2017","03/27/2017","John Dolan","PA","Carnegie-Mellon University","Standard Grant","Wendy Nilsen","05/31/2021","$359,938.00","","jmd@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1139","9250","$0.00","This Research Experiences for Undergraduates (REU) Site will advance knowledge by training students in the key technological area of robotics and preparing them for graduate study and technical leadership.  The sciences that make up the discipline of robotics provide a unique opportunity to immerse students in research with real-world applications. Rapid research advances place robotics at the forefront of the nation's interests. Furthermore, robotics plays a vital role in science, technology, engineering, and mathematics education due to its multi-disciplinary nature. Robotics-related technologies are becoming ubiquitous, for example sensors and wearable devices, and they are dominating national headlines and discussions on such innovations as driverless cars, big data and data mining, or medical robotics.  An emphasis on increasing the participation of under-represented groups in this site has the potential to extend both the range of research projects within the robotics field and the range of societal concerns and problems addressed by these researchers as they enter upon and conduct their careers. <br/><br/>This site will provide high-quality guided research experiences for undergraduate students with leading faculty in computer vision, field and space robotics, artificial intelligence, manipulation, and machine learning. Additional mentors, including researchers and graduate students, will provide unique perspectives and insights into science and engineering education careers. Mentors will meet weekly with scholars to facilitate their research experience and positive learning outcomes. The strategic goal of this site is to provide research experiences and mentorship to U.S. citizens and permanent residents from under-represented groups and those from higher education institutions with fewer research opportunities. Student recruitment and selection will be conducted accordingly and will draw on broad past experience in attracting under-served populations.  The leadership team and participating faculty share this commitment and bring expertise in mentoring students from diverse backgrounds to communicate their research results to coming generations of students, middle school and high school teachers, and the general public."
"1721622","SBIR Phase I:  Using Machine Learning and NLP tools to expedite the review and analysis of legal contracts","IIP","SMALL BUSINESS PHASE I","07/01/2017","06/28/2017","Ghaith Hammouri","NY","XR.AI Inc.","Standard Grant","Peter Atherton","06/30/2018","$225,000.00","","g@xr.ai","114 Forrest St Apt 2B","Brooklyn","NY","112064712","2038873492","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to provide a new approach to analyzing and understanding concisely structured documents such as legal contracts. The success of this project would provide a technological disruption in the way legal documents are processed. This will result in reduced processing time and financial savings in the review of legal contracts. Moreover, achieving this level of disruption within the legal market has the potential to open the door to automate processing, improve quality, and reduce the cost of many other legal services impacting the entire economy. With a market size of $437 Billion within the US alone, the legal industry is ready for a technological disruption that would bring this market up to par with other industries that have been significantly improved with the adoption of cutting edge Machine Learning and Natural Language Processing (NLP) technologies. This underlines the massive commercial potential for the technology under development.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project pursues a new innovative approach to understanding and analyzing contracts with accuracy comparable to what can be achieved by humans. Regardless of the technology, accuracy is the most important metric for lawyers to trust and adopt this innovation. Standard NLP techniques cannot generate the desired level of accuracy without a new approach to the problem.  The approach proposed for this project creates innovative custom-defined rules that operate using standard NLP technologies to break a contract into a data structure capturing different aspects of the contract meaning and allowing a higher level of understanding for the contract. In order to achieve the desired accuracy, a new innovative recurrent neural network (RNN) is designed to learn over the extracted contract-meaning-data-structure which significantly improves the accuracy of the entire system. These two steps form somewhat orthogonal learning processes and when coupled with human-supervised-learning can produce the desired accuracy. This project will allow a full assessment of the underlying technology by training the core engine on a sufficiently large corpus of contracts to test the hypothesis on a larger scale."
"1743262","Group Travel Award for 2017 Workshop on Learning Perception and Control for Autonomous Flight: Safety, Memory, and Efficiency","IIS","Robust Intelligence","04/15/2017","06/05/2017","Konstantinos Karydis","CA","University of California-Riverside","Standard Grant","Rebecca Hwa","03/31/2019","$12,000.00","","kkarydis@ece.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7495","7495, 7556","$0.00","Aerial robots, commonly referred to as drones, offer promise in several research, educational, defense and commercial applications.  Some examples include precise agriculture, aerial photography, agile inspection and monitoring, and package delivery.  In most of those applications that aerial robots have started venturing outside the research lab and into the real world, robot operation is often semi-autonomous.  Semi-autonomous operation typically assumes availability of GPS signal for localization, and at least some prior information about the working environment. Sensory-based, fully autonomous operation in unknown environments remains mostly at the research stage.  Yet, endowing full autonomy to aerial robots can enhance their impact on the nation's education, economy, and defense.  To this end, it is important to seamlessly merge perception, planning, and control for autonomous robotic flight in unknown environments.  This can be achieved by integrating machine learning tools into aerial robot perception and control.  Deep learning has recently emerged as a promising way to extract semantic meaning for autonomy.  Learning perception and control for autonomous flight can be approached by replacing hand-engineered map representations with raw sensor observations, and learning appropriate responses.  However, this is not a straightforward task, and several challenges remain.  This workshop critically addresses how to i) best incorporate memory and ii) derive safety guarantees for the learning-based system.  These two aspects are necessary to improve the capacity of aerial robots to operate autonomously in unknown environments, and to push forward the current state-of-the-art in robotic flight.  In addition to the domain of robotic flight, the outcomes of this workshop are relevant to endowing autonomy in general robotic systems that are able to learn, thus helping make autonomous robots ubiquitous.<br/><br/>The objective of this workshop is to address the theoretical and technical challenges faced in order to endow learning-based systems with the capacity to operate autonomously in unknown environments.  A critical step in this effort is to understand how memory-augmented autonomous learners can operate with provable safety guarantees.  The workshop thus examines two highly-relevant questions.  i) How to theoretically analyze the data and structure of learning-based systems to provide guarantees on safety and task success?  ii) What is the effect of long-term memory and, in particular, can recurrent connections or dynamic external memory replace global map information?  The workshop seeks answers to these questions by bringing together experts from robot planning and control, reinforcement learning and deep learning, and formal methods. The workshop also solicits participation of contributed authors working in relevant areas.  These include but are not limited to applying deep reinforcement learning for vision-based control of underactuated robots; learning visuomotor policies and deriving formal guarantees for learning based on neural networks; and developing neural network architectures that involve temporal recurrence and memory.  The above questions are asked here in the context of high-speed aerial robot autonomous navigation.  However, their scope can be generalized to other areas of robotics that learning perception and control for autonomous operation in unknown environments is desirable; examples include manipulation and legged locomotion."
"1659250","REU Site: Computational Methods for Understanding Music, Media, and Minds","IIS","RSCH EXPER FOR UNDERGRAD SITES","03/01/2017","05/03/2018","Ajay Anand","NY","University of Rochester","Standard Grant","Wendy Nilsen","02/29/2020","$323,950.00","Ajay Anand","ajay.anand@rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","1139","9250","$0.00","How can a computer learn to read an ancient musical score? What can methods from signal processing and natural language analysis tell us about the history of popular music? Can a computer system teach a person to better use prosody (the musical pattern of speech) in order to become a more effective public speaker?  These are some of the questions that students will investigate in the University of Rochester's Research Experience for Undergraduates (REU) site. Students will explore an exciting, interdisciplinary research area that combines computer science, electrical engineering, cognitive science, and music. Each student will be mentored by two or more faculty members from the University's schools of engineering and music.  Other activities of the REU site include workshops on career development; scholarship community colloquiums; Python programming for machine learning; and music-focused activities.  The goals of this REU are to increase the diversity and broaden the horizons of students engaged in computer science research. The themes of music, digital media, and cognitive science will attract many students from groups under-represented in computer science. Students who are already majoring in computer science will discover that the research in the field is not limited to traditional engineering applications, but can address questions of art, culture, and human psychology. Students with experience in combining computer science with humanistic research are already in great demand in industry and academia, and will help define what it means to be a computer scientist in the 21st century.<br/><br/>Students in the University's REU will engage in interdisciplinary research that combines machine learning, audio engineering, music theory, and cognitive science. These disciplines are united by their use of a common set of formal representations and computational methods; in particular, probabilistic models and machine learning.  In the research activity, REU students will work on topics such as using machine learning and wide-spectrum imaging to recover lost ancient musical scores; working with cognitive scientists to understand how prosody makes a person a convincing public speaker; and developing algorithms for synchronizing hundreds of audio and video streams of an event to reconstruct the experience of live music performances.  The University of Rochester's Department of Computer Science has a long history of contributions in machine learning, natural language processing, and computer vision, and the Department of Brain and Cognitive Science is in the top three nationally. The University's recently-founded audio engineering program is growing rapidly, and the Eastman School of Music is the nation's premiere music conservatory.  Although the major objective of the REU is to encourage students to enter STEM graduate programs, many of the projects can be expected to lead to novel and publishable research in machine learning and audio processing."
"1717884","CIF: Small: Collaborative Research: Error Correction with Natural Redundancy","CCF","Comm & Information Foundations","08/01/2017","07/17/2017","Jehoshua Bruck","CA","California Institute of Technology","Standard Grant","Phillip Regalia","07/31/2021","$166,666.00","","bruck@paradise.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7797","7923, 7935","$0.00","Part 1: Nontechnical description of the project<br/><br/>This project studies the fundamental problem of removing errors from data by using internal structures of data. It shows that the vast amount of data stored in current data-storage systems possess very rich structures; therefore, by fully exploiting them for error correction, the reliability of data-storage systems can be improved significantly. The project studies several fundamental aspects of this technology, including how to discover and characterize the highly complex structures of various types of data, how to use them to correct errors in data efficiently to improve the reliability of data-storage systems, how to combine the technology with existing error-correction techniques that are based on adding external redundancy to data, and how to implement the technology in practical data-storage systems. <br/><br/>This project addresses a critical issue of the modern society: how to ensure that data can be stored reliably at large scale and over a long time. The new technology has the potential to substantially improve the dependability of information infrastructure, which accesses vast amounts of data frequently for scientific and industrial computing. The project is interdisciplinary in nature: it combines multiple scientific fields including information theory, machine learning, big data analysis and algorithm design, and aims to educate students and contribute to workforce development for next-generation storage systems. The project conjugates rigorous theoretical analysis and significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts.<br/><br/>Part 2: Technical description of the project<br/><br/>This project studies how to use the inherent redundancy in big data for error correction. Examples of big data include languages, images, databases, and others. The inherent redundancy is integrated with error-correcting codes (ECC) for effective error correction. The objective is to elevate data reliability in storage systems to the next level. To achieve this goal, new techniques will be developed to discover various types of inherent redundancy in both compressed and uncompressed data. New approaches will be explored to combine inherent-redundancy decoders and ECC decoders for effective error correction. Fundamental limits of both capacity and computational complexity will be studied for error correction using inherent redundancy.<br/><br/>This project combines error correction with machine learning and is interdisciplinary in nature. It will expand the current knowledge on error correction in multiple ways. First, it uses techniques in natural language processing and deep learning to discover new types of redundancy in big data that are suitable for error correction, and which extend beyond current knowledge in joint source-channel coding. This includes redundancy discovery techniques for data already compressed by various compression algorithms. Second, it explores decoding algorithms for ECCs with not only regular ECC-imposed redundancy, but also irregular inherent redundancy. It extends existing error correction schemes to cast the fundamental limits of inherent redundancy for error correction, in terms of both capacity and computational complexity. Third, by integrating a theoretical study with practical systems, a foundation can be laid for next-generation systems that store and transmit big data.<br/><br/>Modern society relies increasingly heavily on digital data. With the explosive amount of data generated each day, it is essential to make advances in error correction that can catch the speed of data explosion. This project aims at improving data reliability significantly to the next level, and improvements in this direction can be highly beneficial to the daily work and life of the modern society. This project, being interdisciplinary between coding theory and machine learning, can foster collaboration between the information theory and computer science communities. The project combines rigorous theoretical analysis with significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts. The proposed research will be integrated with engineering education by developing new courses for graduate and undergraduate students, and involving under-represented, domestic and international students in advanced research. The results will be actively publicized in national/international conferences and journals."
"1718886","CIF: Small: Collaborative Research: Error Correction with Natural Redundancy","CCF","Comm & Information Foundations","08/01/2017","07/17/2017","Anxiao Jiang","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","07/31/2021","$299,999.00","Krishna Narayanan","ajiang@cs.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7797","7923, 7935","$0.00","Part 1: Nontechnical description of the project<br/><br/>This project studies the fundamental problem of removing errors from data by using internal structures of data. It shows that the vast amount of data stored in current data-storage systems possess very rich structures; therefore, by fully exploiting them for error correction, the reliability of data-storage systems can be improved significantly. The project studies several fundamental aspects of this technology, including how to discover and characterize the highly complex structures of various types of data, how to use them to correct errors in data efficiently to improve the reliability of data-storage systems, how to combine the technology with existing error-correction techniques that are based on adding external redundancy to data, and how to implement the technology in practical data-storage systems. <br/><br/>This project addresses a critical issue of the modern society: how to ensure that data can be stored reliably at large scale and over a long time. The new technology has the potential to substantially improve the dependability of information infrastructure, which accesses vast amounts of data frequently for scientific and industrial computing. The project is interdisciplinary in nature: it combines multiple scientific fields including information theory, machine learning, big data analysis and algorithm design, and aims to educate students and contribute to workforce development for next-generation storage systems. The project conjugates rigorous theoretical analysis and significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts.<br/><br/>Part 2: Technical description of the project<br/><br/>This project studies how to use the inherent redundancy in big data for error correction. Examples of big data include languages, images, databases, and others. The inherent redundancy is integrated with error-correcting codes (ECC) for effective error correction. The objective is to elevate data reliability in storage systems to the next level. To achieve this goal, new techniques will be developed to discover various types of inherent redundancy in both compressed and uncompressed data. New approaches will be explored to combine inherent-redundancy decoders and ECC decoders for effective error correction. Fundamental limits of both capacity and computational complexity will be studied for error correction using inherent redundancy.<br/><br/>This project combines error correction with machine learning and is interdisciplinary in nature. It will expand the current knowledge on error correction in multiple ways. First, it uses techniques in natural language processing and deep learning to discover new types of redundancy in big data that are suitable for error correction, and which extend beyond current knowledge in joint source-channel coding. This includes redundancy discovery techniques for data already compressed by various compression algorithms. Second, it explores decoding algorithms for ECCs with not only regular ECC-imposed redundancy, but also irregular inherent redundancy. It extends existing error correction schemes to cast the fundamental limits of inherent redundancy for error correction, in terms of both capacity and computational complexity. Third, by integrating a theoretical study with practical systems, a foundation can be laid for next-generation systems that store and transmit big data.<br/><br/>Modern society relies increasingly heavily on digital data. With the explosive amount of data generated each day, it is essential to make advances in error correction that can catch the speed of data explosion. This project aims at improving data reliability significantly to the next level, and improvements in this direction can be highly beneficial to the daily work and life of the modern society. This project, being interdisciplinary between coding theory and machine learning, can foster collaboration between the information theory and computer science communities. The project combines rigorous theoretical analysis with significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts. The proposed research will be integrated with engineering education by developing new courses for graduate and undergraduate students, and involving under-represented, domestic and international students in advanced research. The results will be actively publicized in national/international conferences and journals."
"1652750","CAREER: Advancing Open-Ended Crowdsourcing: The Next Frontier in Crowdsourced Data Management","IIS","Info Integration & Informatics","04/15/2017","06/18/2019","Aditya Parameswaran","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Wei-Shinn Ku","10/31/2019","$302,530.00","","adityagp@berkeley.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7364","1045, 7364","$0.00","Machine learning on big data is finally having an impact on our daily lives, from small triumphs like Siri and Google Translate to much tougher emerging applications like driverless cars and computer-assisted medical image diagnosis.  From mundane online fraud detection to the most sophisticated uses of computer vision, these applications share an insatiable appetite for massive labeled training data. The primary source of high-quality labels is crowdsourcing, and research to date on crowdsourcing has focused on the key problem of how to maximize the production of high-quality crowdsourced labels per dollar spent, for problems where workers must choose between just a few predefined labels.  However, more open-ended labeling problems have grown to constitute almost half of crowdsourced tasks today, and open-ended tasks raise an entirely new set of research challenges for crowdsourced data management.<br/><br/>This activity addresses the key new research challenges in managing and optimizing open-ended crowdsourcing. Since open-ended crowdsourcing employs tasks with a large number of alternatives, humans struggle to select error-free ones. Additional challenges emerge in determining the open-ended task types appropriate for a specific problem, developing schemes to ascertain the right answer given open-ended worker responses, and inferring the hidden perspectives behind worker answers. The activity targets open-ended crowdsourcing problems that span nearly 90% of those used in practice today, with wide applicability in computer vision, natural language processing, and machine learning in general. The technical outcomes of the activity include the first foundational principles for open-ended crowdsourced data management, which in turn will expand the reach of machine learning into new and more challenging domains and more effective solutions in existing applications that impact our everyday lives. The pedagogical outcomes of the activity include a course on human-in-the-loop data analytics, crowdsourcing education modules for school teachers, as well as a quantification and dissemination of how crowdsourcing is performed in practice, along with a benchmark to accelerate crowdsourcing research in the future."
"1661329","Collaborative Research: ABI Innovation: Dark Ecology: Deep Learning and Massive Gaussian Processes to Uncover Biological Signals in Weather Radar","DBI","ADVANCES IN BIO INFORMATICS","05/15/2017","05/08/2017","Steven Kelling","NY","Cornell University","Standard Grant","Peter McCartney","06/30/2020","$309,306.00","Frank LaSorte","stk2@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","BIO","1165","1165","$0.00","Every spring and fall billions of birds migrate across the US, largely under the cover of darkness. Data collected by the US network of weather radars and new analysis methods let us track these migrations. The Dark Ecology Project will develop new resources allowing us to estimate the densities of migrating birds as they have changed in the last 20 years. One outcome will be our better ability to monitor bird populations and their migration systems, and the impacts of various environmental factors. The US network of weather radars has recorded a comprehensive 25-year archive of images of the atmosphere, which provides the baseline information about bird movements. Extracting biological information from the images is not automated currently, making it very slow and inefficient. A team of ecologists and computer scientists will conduct novel research combining methods in computer vision and machine learning to unlock detailed information about bird migration from the entire US archive of weather radar data. The resulting dataset will be freely available, providing an information resource for researchers to estimate the number of birds migrating on any given night, measure the patterns and trends of bird populations, and do hypothesis driven science. The research will advance big data analysis and visualization techniques for large-scale science questions, and will engage scientists, conservation planners, students, and the general public with data, visualizations, and educational material about bird migration.<br/><br/>Dark Ecology will leverage large-scale cloud computing and develop novel computer vision, machine learning, and radar analysis methods to measure the densities and velocities of migrating birds across the US. Deep convolutional networks will be trained to discriminate migrating birds from precipitation and other clutter in the radar data. New techniques for domain transfer and weakly supervised training will enable the training of convolutional networks with only modest-sized training sets. Gaussian process (GP) models will be developed to create smooth national maps of migration density and velocity. Novel GP methods and cloud-computing workflows will allow us to scale to massive radar data sets and analyze the more then 200 million archived radar scans. The resulting data and tools will be curated with open access policies, and used by the research team to conduct ecological research about patterns and drivers of continent-scale migration. Project information can be found at http://darkecology.cs.umass.edu."
"1661259","Collaborative Research: ABI Innovation: Dark Ecology: Deep Learning and Massive Gaussian Processes to Uncover Biological Signals in Weather Radar","DBI","ADVANCES IN BIO INFORMATICS","05/15/2017","05/08/2017","Daniel Sheldon","MA","University of Massachusetts Amherst","Standard Grant","Peter McCartney","04/30/2021","$903,339.00","Subhransu Maji","sheldon@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","BIO","1165","1165","$0.00","Every spring and fall billions of birds migrate across the US, largely under the cover of darkness. Data collected by the US network of weather radars and new analysis methods let us track these migrations. The Dark Ecology Project will develop new resources allowing us to estimate the densities of migrating birds as they have changed in the last 25 years. One outcome will be our better ability to monitor bird populations and their migration systems, and the impacts of various environmental factors. The US network of weather radars has recorded a comprehensive 20-year archive of images of the atmosphere, which provides the baseline information about bird movements. Extracting biological information from the images is not automated currently, making it very slow and inefficient. A team of ecologists and computer scientists will conduct novel research combining methods in computer vision and machine learning to unlock detailed information about bird migration from the entire US archive of weather radar data. The resulting dataset will be freely available, providing an information resource for researchers to estimate the number of birds migrating on any given night, measure the patterns and trends of bird populations, and do hypothesis driven science. The research will advance big data analysis and visualization techniques for large-scale science questions, and will engage scientists, conservation planners, students, and the general public with data, visualizations, and educational material about bird migration.<br/><br/>Dark Ecology will leverage large-scale cloud computing and develop novel computer vision, machine learning, and radar analysis methods to measure the densities and velocities of migrating birds across the US. Deep convolutional networks will be trained to discriminate migrating birds from precipitation and other clutter in the radar data. New techniques for domain transfer and weakly supervised training will enable the training of convolutional networks with only modest-sized training sets. Gaussian process (GP) models will be developed to create smooth national maps of migration density and velocity. Novel GP methods and cloud-computing workflows will allow us to scale to massive radar data sets and analyze the more then 200 million archived radar scans. The resulting data and tools will be curated with open access policies, and used by the research team to conduct ecological research about patterns and drivers of continent-scale migration. Project information can be found at http://darkecology.cs.umass.edu."
"1652530","CAREER: Adversarial Machine Learning for Structured Prediction","IIS","Robust Intelligence","09/01/2017","06/17/2019","Brian Ziebart","IL","University of Illinois at Chicago","Continuing Grant","Rebecca Hwa","08/31/2022","$295,244.00","","bziebart@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","7495","1045, 7495","$0.00","Many important inductive reasoning problems, ranging from understanding text and images to enabling desirable robotic behavior, are structured prediction tasks. These tasks require the joint prediction of many related variables rather than independent predictions for individual variables. For example, an autonomous vehicle's lane change decisions may depend on its position and velocity estimates for nearby vehicles, its assessment of road conditions, its localization and identification of other potential obstacles on the roadway, and so on. The goal of this NSF CAREER award project is to develop safer and more beneficial structured prediction methods. Anticipated improvements have the potential for broader impact in application areas with critical performance measures, such as healthcare and autonomous vehicle safety. This project fosters these potentials by creating multidisciplinary curriculum in data science and releasing general purpose adversarial structured prediction tools that will expose machine learning techniques to a wider audience. Additionally, the project seeks to involve undergraduates in research activities at the University of Illinois at Chicago, which is an urban institution serving a diverse student population.<br/><br/>The approach pursued in this project is to perform structured prediction by making worst-case assumptions when reasoning about uncertainty. The main technical objectives of this project within the proposed adversarial structured prediction formulation are to:<br/>(1) Provide stronger theoretical guarantees (e.g., Fisher consistency, tighter generalization bounds) than existing performance measure approximation methods;<br/>(2) Develop scalable algorithms for solving large adversarial structured prediction problems for a range of structures and performance measures;<br/>(3) Enable safer structured prediction when learning from training data that is generated from a different distribution than the testing data distribution; and <br/>(4) Demonstrate the developed methods on a diverse range of tasks from natural language processing, inverse optimal control, and computer vision."
"1724421","CRCNS US-France-Israel-Research Proposal: Processing of Complex Sounds: Cortical Network Mechanisms and Computations","IIS","GVF - Global Venture Fund, CRCNS-Computation Neuroscience, Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","10/01/2017","08/16/2017","Tatyana Sharpee","CA","The Salk Institute for Biological Studies","Continuing Grant","Kenneth Whang","09/30/2021","$723,470.00","","sharpee@salk.edu","10010 N TORREY PINES RD","LA JOLLA","CA","920371002","8584534100","CSE","054Y, 7327, 7495, 8624","014Z, 5905, 5918, 5976, 5980, 7327, 8089, 8091","$0.00","What are the relationships between the complex and constantly changing soundscapes that surround us and the electrical activity that represents them in the brain? This project brings together three groups, one experimental and two computational, to address this question at two different levels. First, mechanistic models will be developed, based on known properties of neural networks, to describe how different types of neurons cooperate to represent sounds. Second, the function of these neurons will be characterized by describing the features of sounds that they represent. These two goals will be achieved by combining experimental studies of neural responses to sounds with computational analyses to test candidate mechanisms for how sounds are represented in large cortical circuits. In addition to deeper understanding of auditory perception, this research will provide insights into general principles of cooperation between neurons within a single neural network. As such, the research has implications for understanding representation of signals in other sensory modalities as well as the general principles of neural coding in the brain. The research has a number of potential practical applications, including the design of advanced hearing aids and artificial speech recognition systems. Further, given that the altered balance between excitatory and inhibitory neurons has been implicated in a number of attention deficits and psychiatric disorders, including autism and schizophrenia, the project has potential medical relevance. The outreach component of the project will involve demonstration involving music and speech perception for K-12 students and exhibitions.<br/><br/>Technically, the experimental group will produce recordings of neural responses from the auditory thalamus and cortex in response to pure tones and complex sounds known as tone clouds. The tone clouds have sharp transitions like natural sounds, but with well-controlled spectral and temporal power distributions. Computational components of this project will aim to reproduce neural recordings through analytical modeling and simulations of large scale neural circuits composed of multiple cell types. The experimental and computational results will be matched not only in statistical terms, such as average dynamics of neural responses across the population, but also in terms of specific features of sounds that are encoded by different types of neurons in the network.<br/><br/>This award is cofunded by the Office of International Science and Engineering. Companion projects are being funded by the French National Research Agency (ANR) and the US-Israel Binational Science Foundation (BSF)."
"1650499","I/UCRC: Center for Visual and Decision Informatics (CVDI) Site at SUNY Stony Brook","CNS","IUCRC-Indust-Univ Coop Res Ctr","03/01/2017","03/20/2020","Arie Kaufman","NY","SUNY at Stony Brook","Continuing Grant","Ann Von Lehmen","02/28/2022","$400,000.00","Dimitrios Samaras, Erez Zadok, Hansen Schwartz, Klaus Mueller","ari@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","5761","5761","$0.00","This award seeks to add SUNY Stony Brook (SBU) to the Center for Visual and Decision Informatics (CVDI) as a Phase II university site. The industry-centric national-scale research program at CVDI seeks to conduct multi-disciplinary, cross-institutional, pre-competitive research and develop the next generation technologies in data science, big data, analytics, data acquisition and management, and data visualization, all of which have large potential for transformative impact on how information is analyzed and interpreted. Researchers will produce practical solutions for a range of market sectors including healthcare, energy, business intelligence, computational law, banking, information technology, cyber and homeland security, transportation, manufacturing, and internet of things. CVDI?s research along with its associated activities will (1) generate societal benefits in areas of national priority such as health and wellness, cyber and homeland security, and energy; (2) impact regional and national economy in terms of delivering value to both industry and government through innovation in big data and data science by making them globally competitive; (3) advance education and student training to create future workforce, specifically the MS Specialization in Data Science & Engineering (DSE) at SBU, and (4) engage women and underrepresented minorities through training, education and outreach.<br/><br/>Key research challenges will be addressed by SUNY at Stony Brook using visual analytics, immersive analytics, natural language processing, machine learning, computer vision, social media analytics, geometric/topological analytics, visualization, human-computer interaction, virtual/augmented reality, gamification, knowledge management, system and storage optimization, wireless and mobile computing, data streaming, information retrieval, high performance computing, and innovative algorithms."
"1660190","SBIR Phase II:  Decoding Obfuscated Text to Find Trafficking Victims","IIP","SBIR Phase II","04/01/2017","08/27/2019","Andreas Olligschlaeger","PA","Marinus Analytics LLC","Standard Grant","Peter Atherton","09/30/2021","$1,425,894.00","","olli@marinusanalytics.com","4620 Henry Street","Pittsburgh","PA","152133715","8669452803","ENG","5373","165E, 169E, 5373, 8032, 8240, 9251","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is to combat modern day slavery in the United States and Canada. In this project the company will go beyond the public sector, selling its capabilities to the hospitality industry, which shares a role in tackling this problem. Banks also play a part in detecting financial transactions which stem from criminal revenue streams. These new markets expand the revenue opportunity and social impact of the technology. The proposed Phase II research and development will solve huge challenges voiced by the company's law enforcement users. The deployment of these research products through a platform that enables evidence management and collaboration will accelerate the impact by increasing communication among the fragmented law enforcement jurisdictions in the United States. This will enable agencies to conduct more effective investigations, and empower them to take on larger cases involving organized crime across state lines. Finally, human expertise will be developed within the company and through its partnership with Carnegie Mellon University to commercialize advanced computing research for real-world, social impact. These innovations will empower more victim rescues and exploiter prosecutions. The project will create a culture within the company to nurture engineers in social entrepreneurship.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project will expand on machine learning technology created in Phase I to deobfuscate escort ads and implement end-to-end innovations for investigations. Each day, there are thousands of online data points related to prostitution. Hidden behind this content are victims of sex trafficking, those forced or coerced into sex work, including juveniles who have not reached the age of consent. Big Data presents the opportunity to seize this information to disrupt traffickers and organized groups who drive the cycle of exploitation. The company's research objectives include maximizing evidence recall using sophisticated crawlers and deobfuscation methods, as well as generating leads using natural language processing and multi-modal machine learning. The project will further develop computer vision capabilities to interpret features of an image and enable visual search for missing victims. It will formalize methods for collecting ground truth, preventing false positives, and diagnosing algorithmic performance relevant to users' needs. Finally, the company will deploy this research into accessible software products that provide real-time, digestible, and actionable information to law enforcement, resulting in the rescue of hundreds, or potentially thousands, of sex trafficking victims."
"1734380","NRI: INT: COLLAB: Development, Deployment and Evaluation of Personalized Learning Companion Robots for Early Literacy and Language Learning","DRL","ITEST-Inov Tech Exp Stu & Teac, NRI-National Robotics Initiati","09/01/2017","04/21/2020","Abeer Alwan","CA","University of California-Los Angeles","Standard Grant","Wu He","08/31/2021","$623,646.00","Alison Bailey","alwan@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","EHR","7227, 8013","8086, 9251","$0.00","This National Robotics Initiative project will develop, deploy and evaluate personalized companion robots to assist kindergarten-age children in learning language and vocabulary skills. The aim is to accelerate the impacts of social robots for early education in schools and at home. The four-year project will advance knowledge in three key areas: (1) automatic speech recognition models for young children; (2) multi-modal student assessment algorithms for early language and literacy skills; and (3) personalization of activities, content, and dialogic question generation to boost learning outcomes. The project will generate new insights for how to develop expressive, socially responsive robots that provide more effective, engaging, and empathetic educational experiences for young children. To evaluate the impacts of long-term interactions on educational outcomes, the project team will conduct a 4-month study with Kindergarten classrooms, as well as a 3-month at-home study. The project will engage teachers and parents to develop key guidelines for best practices for use of social robots in classroom and home settings, and participating undergraduate and graduate students will be trained in the multidisciplinary aspects of social robotics, speech recognition and understanding, human participation studies, interactive machine learning for automatic assessment and personalization tools, and early education research. <br/><br/>This research and development project will be implemented in two phases: An initial phase consisting of short pilot deployments to train and continually iterate development of project technologies and systems, followed by longer term deployment of the robot to examine autonomous interactions with social robots in school and home educational settings. During the development stage of individual components (automatic reading and language assessment tools, automatic question-generation algorithm, automatic speech recognition and spoken language understanding system models, and activities with the autonomous social robot learning companion) the project team will collect and analyze data with practical and performance measures, and refine and iterate each component of the system being developed. After development of the individual components, the autonomous social robot storytelling companion will be developed through repeated iterations with children. In the final year of the project, two 4-month studies will be conducted in six Kindergarten classrooms with 15 to 20 students each. This project is expected to result in five key contributions: (1) Development of Automatic Speech Recognition and Spoken Language Understanding systems for young children's speech, (2) Multi-modal automatic assessment algorithms for Kindergarten age children's spoken language and early reading skills; (3) Automatic personalization algorithms for story content customization and dialogic question generation in the context of young children's verbal storytelling; (4) Development of a fully autonomous, collaborative, peer-like social robot system with effective educational activities; and (5) Long-term studies with deployed social robots in schools and homes spanning several months and demonstrating sustained engagement and positive learning outcomes."
"1734443","NRI: INT: COLLAB: Development, Deployment and Evaluation of Personalized Learning Companion Robots for Early Literacy and Language Learning","DRL","ITEST-Inov Tech Exp Stu & Teac, NRI-National Robotics Initiati","09/01/2017","01/29/2020","Cynthia Breazeal","MA","Massachusetts Institute of Technology","Standard Grant","Wu He","08/31/2021","$882,935.00","Hae Won Park, Regina Barzilay","cynthiab@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","EHR","7227, 8013","8086","$0.00","This National Robotics Initiative project will develop, deploy and evaluate personalized companion robots to assist kindergarten-age children in learning language and vocabulary skills. The aim is to accelerate the impacts of social robots for early education in schools and at home.  The four-year project will advance knowledge in three key areas: (1) automatic speech recognition models for young children; (2) multi-modal student assessment algorithms for early language and literacy skills; and (3) personalization of activities, content, and dialogic question generation to boost learning outcomes. The project will generate new insights for how to develop expressive, socially responsive robots that provide more effective, engaging, and empathetic educational experiences for young children. To evaluate the impacts of long-term interactions on educational outcomes, the project team will conduct a 4-month study with Kindergarten classrooms, as well as a 3-month at-home study.  The project will engage teachers and parents to develop key guidelines for best practices for use of social robots in classroom and home settings, and participating undergraduate and graduate students will be trained in the multidisciplinary aspects of social robotics, speech recognition and understanding, human participation studies, interactive machine learning for automatic assessment and personalization tools, and early education research.<br/><br/>This research and development project will be implemented in two phases:  An initial phase consisting of short pilot deployments to train and continually iterate development of project technologies and systems, followed by longer term deployment of the robot to examine autonomous interactions with social robots in school and home educational settings.  During the development stage of individual components (automatic reading and language assessment tools, automatic question-generation algorithm, automatic speech recognition and spoken language understanding system models, and activities with the autonomous social robot learning companion) the project team will collect and analyze data with practical and performance measures, and refine and iterate each component of the system being developed.  After development of the individual components, the autonomous social robot storytelling companion will be developed through repeated iterations with children.  In the final year of the project, two 4-month studies will be conducted in six Kindergarten classrooms with 15 to 20 students each.  This project is expected to result in five key contributions: (1) Development of Automatic Speech Recognition and Spoken Language Understanding systems for young children's speech, (2) Multi-modal automatic assessment algorithms for Kindergarten age children's spoken language and early reading skills; (3) Automatic personalization algorithms for story content customization and dialogic question generation in the context of young children's verbal storytelling; (4) Development of a fully autonomous, collaborative, peer-like social robot system with effective educational activities; and (5) Long-term studies with deployed social robots in schools and homes spanning several months and demonstrating sustained engagement and positive learning outcomes."
"1744159","I-Corps: Approximate Dynamic Programming and Artificial Neural Network Control for Microgrids","IIP","I-Corps","07/01/2017","12/03/2019","Shuhui Li","AL","University of Alabama Tuscaloosa","Standard Grant","Pamela McCauley","12/31/2020","$50,000.00","","sli@eng.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","ENG","8023","9150","$0.00","The broader impact/commercial potential of this I-Corps project is to act as a catalyst in the growth of distributed generation and microgrid industries. This artificial intelligence based control system will potentially provide an electrical network that is reliable by reducing outages and restoration costs with incredibly fast bidirectional power flow, secured with real time diagnostics, self-healing and adaptive capabilities, and more economical by reducing equipment failures and minimizing power losses. The product potentially three broad markets, including utilities, distributed generation and consumer. The solution will enhance energy generation from renewables, improve microgrid efficiency, reliability, stability and power quality, and add intelligent control to conventional power systems. Inverter capabilities are presently a significant challenge for integrating distributed generation sources. The proposed innovation would potentially provide an appropriate solution to address this challenge.<br/><br/>This I-Corps project develops a neural network control technology for microgrid control and management. Microgrids are one path for integrating renewable and distributed generation sources into the grid and can generally support a future smart electricity grid.  A key challenge in microgrid adoption is adequate control of power inverters. Problems include high oscillations when connecting or disconnecting an energy source, fluctuating voltage and frequency, malfunctions and reliability, competing control between inverters, and high harmonic distortions. The proposed innovation uses adaptive dynamic programming and artificial neural networks to implement microgrid control. It integrates into one controller the advantages of conventional control methods, including optimal control, proportional integral control, predictive control, and sliding mode control. The proposed innovation has the potential to overcome the limitations of the conventional control technologies and better meet customer demands and requirements."
"1737846","Establishing a ground truth for focus placement in naturally-occurring speech","IIS","Linguistics, Robust Intelligence","07/01/2017","06/27/2018","Jonathan Howell","NJ","Montclair State University","Standard Grant","D.  Langendoen","06/30/2020","$121,894.00","","howellj@mail.montclair.edu","1 Normal Avenue","Montclair","NJ","070431624","9736556923","CSE","1311, 7495","1311, 7495, 7916, 9251","$0.00","By emphasizing words acoustically, people can convey the information about which concepts they wish to contrast.  This feature of speech, known as focus, is pervasive in English, yet is inadequately modeled in state-of-the-art speech technologies.  The challenge, which this Early Grant for Exploratory Research addresses, is that it is often difficult to identify phonetic emphasis  independently of semantic contrast:  words whose meanings are focused are usually realized with increased acoustic prominence, but not all cases of increased acoustic prominence are due to focus.  The project is innovative in its use both of speech that has been recorded in a laboratory under controlled conditions, and also of speech that occurs naturally, such as in podcasts and videos.  Judgments of focus location in laboratory speech and in naturally-occurring speech are collected from ordinary, non-expert listeners using online crowd-sourcing.  Using the comparative construction (for example, ""He liked it better than I did"" or ""I like it better now than I did"") in which focus can be independently verified, computational procedures are developed to mimic the judgment of subjects who read but do not listen to the utterance being investigated.  The findings will inform research in speech synthesis and in automatic speech recognition.  Commercial applications may include aids for the deaf and hearing impaired, robot assistants for the elderly, language instruction and speech therapy.<br/><br/>In a previous proof-of-concept study, the researcher collected utterances of ""than I did"" in laboratory experiments and from transcribed podcasts available on the web.  Machine learning classifiers (using linear discriminant analysis and support vector machines) were trained to detect focus from acoustic features alone, including measures of fundamental frequency, duration and intensity.  Location of focus can be determined independently from prosody in the comparative construction by observing the presence or absence of co-reference between subjects in the main and comparative clauses.  This research generalizes that study to variations of the comparative with different pronouns and auxiliaries and also introduces updated methods of acoustic extraction and classification.  Then, a verification dataset is created in order to reject annotations from participants who annotate non-focal prominence or who mark focus location incorrectly.  Finally, classifiers are trained to detect focus on pronouns and auxiliaries in contexts other than the comparative, using the crowd-sourced annotation data to infer correct location of focus independently from prosody."
"1701565","Exploiting Metal-Insulator-Transition in Strongly Correlated Oxides as Neuron Device for Neuro-Inspired Computing","ECCS","EPMD-ElectrnPhoton&MagnDevices","08/01/2017","05/01/2017","Shimeng Yu","AZ","Arizona State University","Standard Grant","Dimitris Pavlidis","12/31/2018","$360,000.00","","shimeng.yu@ece.gatech.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","1517","100E","$0.00","A radical shift in computing paradigm towards the neuro-inspired computing is attractive for performing data-intensive applications such as image/speech recognitions. The neuro-inspired architecture leverages the distributed computation in the neuron nodes and the localized storage in the synaptic elements. The neuron node today is generally implemented by tens of silicon transistors. Compared to the crossbar array of synaptic elements, the silicon neuron is power-hungry and area-inefficient, thereby reducing the parallelism of computing system. In such context, how to design a single device that can efficiently emulate the neuronal behavior (e.g. integrate-and-fire) is critical to the neuromorphic hardware design. This project aims to exploit the metal-insulator-transition phenomenon in strongly correlated oxides as a compact neuron node that can self-oscillate, namely oxide neuron, to overcome the aforementioned limitations of silicon neuron. The proposed research will have a profound impact on the society that is embracing the artificial intelligence. For instance, a compact design of neuromorphic hardware may enable intelligent information processing on power-efficient mobile platforms, e.g. autonomous vehicle, personalized healthcare, wearable devices, and smart sensors. The objective of the research and education integration is to train undergraduate/graduate students and next-generation workforce with interdisciplinary skills. The cross-layer nature of this project ranging from materials engineering, semiconductor device, circuit-device interaction and artificial neural network provides an ideal platform for this educational goal. The project also plans to engage minority and unrepresentative students in research. Technology transfer will be performed through video or on-site seminars and student internships with industrial collaborators.<br/><br/><br/>The goal of this research is to advance the artificial neuron device design by exploiting the volatile and threshold switching behavior in strongly correlated oxides, with the purpose of significantly reducing the area and energy of the neuron node, and making it compatible for the integration with crossbar array of resistive synaptic elements. The scope of the project is to explore various material systems of the strongly correlated oxides, in particular, NbO2 and SmNiO3 to demonstrate the self-oscillation behavior in the artificial neuron node. When such oxide device is connected with a series synaptic element whose resistance is within the on/off dynamic range of the oxide device, the node voltage between the oxide device and the synaptic element will start self-oscillation, and the oscillation frequency represents the synaptic conductance. This project aims to explore such self-oscillation to emulate the integrate-and-fire neuronal behavior. To achieve the aforementioned research goal, device fabrication, physical and electrical characterization, device modeling, and circuit-device co-design will be performed to demonstrate the feasibility of the concept and further optimize the device performance. The intellectual significance of this project is two folded. From the fundamental science perspective, the physical switching mechanism of metal-insulator-transition in strongly correlated oxides will be investigated. From the applied engineering perspective, the oxide neuron device will be integrated with the resistive crossbar array for demonstration of a neural network for solving a practical problem, i.e. the image pattern classification."
"1701099","PFI:AIR - TT:  Memory Processing Unit: A Low Power Processor for Analytics Applications","IIP","Accelerating Innovation Rsrch","06/01/2017","06/12/2017","Karthikeyan Sankaralingam","WI","University of Wisconsin-Madison","Standard Grant","Jesus Soriano Molla","11/30/2019","$200,000.00","Martin Ganco","karu@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","8019","8019","$0.00","This PFI: AIR Technology Translation project focuses on translating 3D chip-stacking technology to develop a new microprocessor architecture called Memory Processing Unit (MPU). The new architecture offers the promise of faster, more energy efficient calculations than current solutions. This is important because there is a large body of applications such as deep-learning, big-data analytics, and data science that all require significant processing capability. While capability requirements continue to increase to meet the needs of these new applications, the rate of improvement of power efficiency of the microprocessors is decreasing (in other words, the trends predicted by Moore's Law are beginning to slow).  <br/><br/>This project will result in a prototype chip-design of the MPU with complete software API (Application Programming Interface) for end-to-end application demonstrations based on real-time speech recognition and big-data analytics for Internet search capabilities. The MPU includes the following unique features:  an energy-efficient simple core implementation (including exploration of mechanisms for pipeline organization, virtual-memory, and coherence), an implementation that connects 128 such cores together, cores connected through 3D links to memory, and end-to-end software implementation. Compared to state-of-art server chips in the market, the MPU architecture provides two-fold to 12-fold calculation speedup while reducing energy consumption by 10-fold.  <br/><br/>This project addresses the following two technology gaps as it translates from research discovery toward commercial application: a) the difficulty of developing end-to end software on this new highly concurrent architecture and the determination of the mechanisms required, and b) Extensive exploration of various application domains (initially demonstrating speech recognition and internet search capabilities) to determine quantitatively the benefits of this architecture over the state-of-the-art. <br/><br/>Personnel involved in this project, including graduate students and undergraduates, will receive innovation and entrepreneurship experiences through the technology commercialization activities, customer interviews, and business development. In addition the team will work with entrepreneurship programs like D2P at UW-Madison."
"1725447","SPX: Collaborative Research: Ula! - An Integrated Deep Neural Network (DNN) Acceleration Framework with Enhanced Unsupervised Learning Capability","CCF","SPX: Scalable Parallelism in t","09/01/2017","07/22/2017","Yuan Xie","CA","University of California-Santa Barbara","Standard Grant","Yuanyuan Yang","08/31/2021","$280,000.00","","yuanxie@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","042Y","026Z","$0.00","In light of very recent revolutions of unsupervised learning algorithms (e.g., generative adversarial networks and dual-learning) and the emergence of their applications, three PIs/co-PI from Duke and UCSB form a team to design Ula! - an integrated DNN acceleration framework with enhanced unsupervised learning capability. The project revolutionizes the DNN research by introducing an integrated unsupervised learning computation framework with three vertically-integrated components from the aspects of software (algorithm), hardware (computing), and application (realization). The project echoes the call from the BRAIN Initiative (2013) and the Nanotechnology-Inspired Grand Challenge for Future Computing (2015) from the White House. The research outcomes will benefit both Computational Intelligence (CI) and Computer Architecture (CA) industries at large by introducing a synergy between computing paradigm and artificial intelligence (AI). The corresponding education components  enhance existing curricula and pedagogy by introducing interdisciplinary modules on the software/hardware co-design for AI with creative teaching practices, and give special attentions to women and underrepresented minority groups.<br/><br/>The project performs three tasks: (1) At the software level, a generalized hierarchical decision-making (GHDM) system is designed to efficiently execute the state-of-the-art unsupervised learning and reinforcement learning processes with substantially reduced computation cost; (2) At the hardware level, a novel DNN computing paradigm is designed with enhanced unsupervised learning supports, based on the novelties in near data computing, GPU architecture, and FGPA + heterogeneous platforms; (3) At the application level, the usage of Ula! is exploited in scenarios that can greatly benefit from unsupervised learning and reinforcement learning. The developed techniques are also demonstrated and evaluated on three representative computing platforms: GPU, FPGA, and emerging nanoscale computing systems, respectively."
"1725456","SPX: Collaborative Research: Ula! - An Integrated Deep Neural Network (DNN) Acceleration Framework with Enhanced Unsupervised Learning Capability","CCF","SPX: Scalable Parallelism in t","09/01/2017","07/22/2017","Yiran Chen","NC","Duke University","Standard Grant","Yuanyuan Yang","08/31/2021","$520,000.00","Hai Li","yiran.chen@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","042Y","026Z","$0.00","In light of very recent revolutions of unsupervised learning algorithms (e.g., generative adversarial networks and dual-learning) and the emergence of their applications, three PIs/co-PI from Duke and UCSB form a team to design Ula! - an integrated DNN acceleration framework with enhanced unsupervised learning capability. The project revolutionizes the DNN research by introducing an integrated unsupervised learning computation framework with three vertically-integrated components from the aspects of software (algorithm), hardware (computing), and application (realization). The project echoes the call from the BRAIN Initiative (2013) and the Nanotechnology-Inspired Grand Challenge for Future Computing (2015) from the White House. The research outcomes will benefit both Computational Intelligence (CI) and Computer Architecture (CA) industries at large by introducing a synergy between computing paradigm and artificial intelligence (AI). The corresponding education components  enhance existing curricula and pedagogy by introducing interdisciplinary modules on the software/hardware co-design for AI with creative teaching practices, and give special attentions to women and underrepresented minority groups.<br/><br/>The project performs three tasks: (1) At the software level, a generalized hierarchical decision-making (GHDM) system is designed to efficiently execute the state-of-the-art unsupervised learning and reinforcement learning processes with substantially reduced computation cost; (2) At the hardware level, a novel DNN computing paradigm is designed with enhanced unsupervised learning supports, based on the novelties in near data computing, GPU architecture, and FGPA + heterogeneous platforms; (3) At the application level, the usage of Ula! is exploited in scenarios that can greatly benefit from unsupervised learning and reinforcement learning. The developed techniques are also demonstrated and evaluated on three representative computing platforms: GPU, FPGA, and emerging nanoscale computing systems, respectively."
"1740184","E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays","CCF","Energy Efficient Computing: fr","09/15/2017","08/26/2019","Jeehwan Kim","MA","Massachusetts Institute of Technology","Continuing Grant","Sankar Basu","08/31/2020","$365,436.00","","jeehwan@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","015Y","7945","$0.00","In recent years, deep learning and artificial neural networks have been very successful in large-scale recognition and classification tasks, some even surpassing human-level accuracy. However, state-of-the-art deep learning algorithms tend to present very large network models, which poses significant challenges for hardware, especially for memory. Emerging resistive devices have been proposed as an alternative solution for weight storage and parallel neural computing, but severe limitations still exist for applying resistive random access memories (RRAMs) for practical large-scale neural computing. This proposal targets on addressing limitations in resistive device based neural computing through novel device engineering, new bitcell designs, new neuron circuits, energy-aware architecture, and a new circuit-level benchmark simulator. A successful completion of this research is likely to have consequences to our society, enabling wide adoption of dense and energy-efficient intelligent hardware to power-/area-constrained local mobile/wearable devices. Furthermore, a self-learning chip that learns in near real-time and consumes very low-power can be integrated in smart biomedical devices, personalizing healthcare. This project will have a strong effort on integrating the research outcomes with education and outreach through summer outreach programs for high school students, undergraduate/graduate student training, and organization of tutorials and workshops at conferences for knowledge dissemination.<br/><br/>The proposal will perform innovative and interdisciplinary research to address many limitations in today?s resistive device based neural computing and make a leap progress towards energy-efficient intelligent computing. Severe limitations of applying resistive random access memories (RRAMs) for practical large-scale neural computing include: (1) device-level non-idealities, e.g., non-linearity, variability, selector, and endurance, (2) inefficiency in representing negative weights and neurons, and (3) limited demonstration on simpler networks, instead of cutting-edge convolutional and recurrent neural networks. To address these limitations, novel technologies from devices to architectures will be investigated. First, new bitcell circuits will be designed for today?s binary resistive devices, efficiently mapping XNOR functionality with (+1, -1) weights and neurons. Second, a novel epitaxial resistive device (EpiRAM) that exhibits many idealistic properties will be investigated, including linear programming for analog weights, suppressed variability, self-selectivity, and high endurance. Third, new neuron circuits will be explored for integration with new resistive devices for feedforward/feedback deep neural networks. Finally, new data-mapping techniques that efficiently map state-of-the-art deep neural networks onto the hardware framework with RRAM arrays will be developed, and the overall energy-efficiency will be verified with a new benchmark simulator ?NeuroSim?. With vertical innovations across material, device, circuit and architecture, tremendous potential and research needs will be pursued towards energy-efficient artificial intelligence in ubiquitous resource-constrained hardware systems."
"1740225","E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays","CCF","Energy Efficient Computing: fr","09/15/2017","08/21/2019","Jae-sun Seo","AZ","Arizona State University","Continuing Grant","Sankar Basu","08/31/2021","$579,066.00","Shimeng Yu","jaesun.seo@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","015Y","7945","$0.00","In recent years, deep learning and artificial neural networks have been very successful in large-scale recognition and classification tasks, some even surpassing human-level accuracy. However, state-of-the-art deep learning algorithms tend to present very large network models, which poses significant challenges for hardware, especially for memory. Emerging resistive devices have been proposed as an alternative solution for weight storage and parallel neural computing, but severe limitations still exist for applying resistive random access memories (RRAMs) for practical large-scale neural computing. This proposal targets on addressing limitations in resistive device based neural computing through novel device engineering, new bitcell designs, new neuron circuits, energy-aware architecture, and a new circuit-level benchmark simulator. A successful completion of this research is likely to have consequences to our society, enabling wide adoption of dense and energy-efficient intelligent hardware to power-/area-constrained local mobile/wearable devices. Furthermore, a self-learning chip that learns in near real-time and consumes very low-power can be integrated in smart biomedical devices, personalizing healthcare. This project will have a strong effort on integrating the research outcomes with education and outreach through summer outreach programs for high school students, undergraduate/graduate student training, and organization of tutorials and workshops at conferences for knowledge dissemination.<br/><br/>The proposal will perform innovative and interdisciplinary research to address many limitations in today?s resistive device based neural computing and make a leap progress towards energy-efficient intelligent computing. Severe limitations of applying resistive random access memories (RRAMs) for practical large-scale neural computing include: (1) device-level non-idealities, e.g., non-linearity, variability, selector, and endurance, (2) inefficiency in representing negative weights and neurons, and (3) limited demonstration on simpler networks, instead of cutting-edge convolutional and recurrent neural networks. To address these limitations, novel technologies from devices to architectures will be investigated. First, new bitcell circuits will be designed for today's binary resistive devices, efficiently mapping XNOR functionality with (+1, -1) weights and neurons. Second, a novel epitaxial resistive device (EpiRAM) that exhibits many idealistic properties will be investigated, including linear programming for analog weights, suppressed variability, self-selectivity, and high endurance. Third, new neuron circuits will be explored for integration with new resistive devices for feedforward/feedback deep neural networks. Finally, new data-mapping techniques that efficiently map state-of-the-art deep neural networks onto the hardware framework with RRAM arrays will be developed, and the overall energy-efficiency will be verified with a new benchmark simulator ?NeuroSim?. With vertical innovations across material, device, circuit and architecture, tremendous potential and research needs will be pursued towards energy-efficient artificial intelligence in ubiquitous resource-constrained hardware systems."
"1657379","CRII: CHS: Mining Intentions on Social Media to Enhance Situational Awareness of Crisis Response Organizations","IIS","CRII CISE Research Initiation","09/01/2017","03/01/2017","Hemant Purohit","VA","George Mason University","Standard Grant","Andruid Kerne","05/31/2020","$174,965.00","","hpurohit@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","026Y","7367, 8228","$0.00","In large-scale emergencies, people post a lot of information about their status, needs, and abilities to help on social media. In principle, these posts might help emergency management teams get a better picture of the situation and find useful resources, but the number and questionable accuracy of these posts make them less useful than they could be. This project is about developing tools that identify people's intentions related to the emergency, sorting tweets into categories such as requests for help or information, offers of help, announcements of their safety or location, and so on. This problem of intent inference is a key scientific problem in natural language processing and artificial intelligence, with practical uses in a number of areas beyond emergency management, including web search and providing location-aware services. The researchers will attack the intent inference problem by narrowing it to the emergency response domain. First, they will work closely with emergency response teams to identify meaningful categories of intent that align with emergency response needs, in order to guide the collection and labeling of social media posts. Then, they will develop strategies drawn from existing image and natural language processing techniques and informed by the emergency response context to do the categorization work. Finally, they will build and evaluate a tool that uses the categorization algorithms to highlight the social media posts that are most likely to be useful to emergency responders. The work will be used to help develop courses around data science at the lead researcher's school, and the tools will be made publicly available through an open source code and advertised to communities of interest. <br/><br/>To build the set of crisis-specific intent categories, the research team will first analyze existing operational manuals for emergency response including the Incident-Command-System models to extract key processes and initial categories, then refine that set working with experts from the Fairfax Fire and Rescue Department, an advisory committee of social media working group for emergency services at Department of Homeland Security that has members across the country, and members of the project's advisory board.  Intent extraction will be modeled as a multilabel classification problem on two dimensions: type of intent, and topical category; this formulation maps well to characteristics of posts (which might contain multiple intents and topics) and scopes the complexity of general intent inference. Datasets will be gathered from prior crisis events and labeled by crowd workers interested in humanitarian work according to the categories identified from the first phase. Features of posts will be constructed from posts? metadata using natural language processing techniques on textual content, image processing techniques on multimedia content and author profiling techniques. Features will include extracting syntactic-semantic patterns that represent declarative and psycholinguistic knowledge as well as ideas from discourse analysis, while features of authors will be drawn from their provided profile information as well as aggregate inferences from their posts. The team will use a multi-task learning framework as the underlying algorithm to leverage relationships between the different categories to be classified.  Finally, the developed interface will support faceted browsing by intent, topic, location, and response management process, and be evaluated through training exercises with the research team's practitioner partners."
"1740197","E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays","CCF","Energy Efficient Computing: fr","09/15/2017","08/20/2019","Saibal Mukhopadhyay","GA","Georgia Tech Research Corporation","Continuing Grant","Sankar Basu","08/31/2021","$243,618.00","","saibal@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","015Y","7945, 8089","$0.00","In recent years, deep learning and artificial neural networks have been very successful in large-scale recognition and classification tasks, some even surpassing human-level accuracy. However, state-of-the-art deep learning algorithms tend to present very large network models, which poses significant challenges for hardware, especially for memory. Emerging resistive devices have been proposed as an alternative solution for weight storage and parallel neural computing, but severe limitations still exist for applying resistive random access memories (RRAMs) for practical large-scale neural computing. This proposal targets on addressing limitations in resistive device based neural computing through novel device engineering, new bitcell designs, new neuron circuits, energy-aware architecture, and a new circuit-level benchmark simulator. A successful completion of this research is likely to have consequences to our society, enabling wide adoption of dense and energy-efficient intelligent hardware to power-/area-constrained local mobile/wearable devices. Furthermore, a self-learning chip that learns in near real-time and consumes very low-power can be integrated in smart biomedical devices, personalizing healthcare. This project will have a strong effort on integrating the research outcomes with education and outreach through summer outreach programs for high school students, undergraduate/graduate student training, and organization of tutorials and workshops at conferences for knowledge dissemination.<br/><br/>The proposal will perform  interdisciplinary research to address many limitations in today's resistive device based neural computing and make a leap progress towards energy-efficient intelligent computing. Severe limitations of applying resistive random access memories (RRAMs) for practical large-scale neural computing include: (1) device-level non-idealities, e.g., non-linearity, variability, selector, and endurance, (2) inefficiency in representing negative weights and neurons, and (3) limited demonstration on simpler networks, instead of cutting-edge convolutional and recurrent neural networks. To address these limitations, novel technologies from devices to architectures will be investigated. First, new bitcell circuits will be designed for today's binary resistive devices, efficiently mapping XNOR functionality with (+1, -1) weights and neurons. Second, a novel epitaxial resistive device (EpiRAM) that exhibits many idealistic properties will be investigated, including linear programming for analog weights, suppressed variability, self-selectivity, and high endurance. Third, new neuron circuits will be explored for integration with new resistive devices for feedforward/feedback deep neural networks. Finally, new data-mapping techniques that efficiently map state-of-the-art deep neural networks onto the hardware framework with RRAM arrays will be developed, and the overall energy-efficiency will be verified with a new benchmark simulator ""NeuroSim"". With innovations across material, device, circuit and architecture,  research needs will be pursued towards energy-efficient processing in ubiquitous resource-constrained hardware systems."
"1747818","EAGER:   Harnessing the Power of Graph Data Analytics","IIS","Info Integration & Informatics","09/01/2017","08/04/2017","Weili Wu","TX","University of Texas at Dallas","Standard Grant","Wei Ding","08/31/2020","$150,000.00","","weiliwu@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7364","7364, 7916","$0.00","The rapid growth of user generated content in online social networks forms an abundant source of big graph data. This project will develop efficient techniques for extracting information from such graph data. Specifically, this EAGER project will develop efficient algorithms for the following tasks: (1) machine learning and sentiment analysis, such as large vocabulary conversational speech recognition, multi-label learning, and multi-polarity sentiment analysis, and (2) effector detection and related issues, such as business expansion and rumor blocking. The advanced optimization techniques developed in this project will significantly improve the performance of the existing data mining algorithms and the theoretical study of efficient approximation techniques. In addition, this project will provide an excellent platform for educating students with hands-on experience. <br/><br/>To develop the efficient algorithms described, the project will take advantage of recent development in nonlinear combinatorial optimization, and will formulate new methodologies using discrete convex analysis for submodular functions and duality theory for the discrete DC (different of convex) programming, as well as reverse sampling techniques for probabilistic objective functions. These new methodologies will enrich the throughput of machine learning and sentiment analysis. Since nonlinear combinatorial optimization is still in its early stages, exploratory and innovative research on efficient approximation algorithm will be performed based on fresh fundamental theoretical developments. A detailed evaluation will be conducted based on several real world data sets."
"1657613","CRII: RI: Inference for Probabilistic Programs: A Symbolic Approach","IIS","CRII CISE Research Initiation","03/01/2017","02/21/2017","Guy Van den Broeck","CA","University of California-Los Angeles","Standard Grant","Rebecca Hwa","02/28/2019","$174,639.00","","guyvdb@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","026Y","7495, 8228","$0.00","Probabilistic machine learning and artificial intelligence have revolutionized the world and are present in most aspects of our life. However, the tools used to develop probabilistic machine learning solutions are limited in what they can express. Moreover, they require significant expert knowledge, and are not accessible to scientists in each discipline, let alone everybody else. Probabilistic programming aims to make probabilistic machine learning accessible to all, and as easy to program as a phone application. To make this dream a reality, probabilistic program execution, making probabilistic predictions from observations, has to become as highly efficient and robust as our current non-probabilistic software tools. This project develops general-purpose algorithms to execute probabilistic programs efficiently, using advanced symbolic reasoning techniques from artificial intelligence. Moreover, it does so for probabilistic programs that are significantly more complex than the ones in use today, involving a wide range of programming language features that are both discrete and continuous. This increase in scalability and expressive power will foster novel, increasingly advanced machine learning applications. <br/><br/>More specifically, probabilistic programs subsume classical probabilistic graphical models and are additionally able to capture complex probabilistic dependencies that include arbitrary pieces of executable code. While many expressive probabilistic programming languages have been proposed in recent years, the current bottleneck and barrier to success is the lack of general-purpose reasoning algorithms to perform inference with probabilistic programs efficiently. This research tackles two key problems in probabilistic program inference. First, current sampling-based algorithms have problems reasoning about dependencies between large numbers of discrete random variables and explaining low-probability observations. In one thrust, this project develops new inference algorithms based on knowledge compilation. This technique compiles the program into a symbolic structure that is efficient for probability computation. The algorithm does not compile the entire program, which is generally intractable, but uses importance sampling on partially compiled programs to sample efficient subprograms. This combines the best of approximate program evaluation by sampling with highly efficient compilation techniques for exact inference. Second, symbolic approaches to inference are fundamentally discrete and have problems dealing with continuous and integer variables, which frequently appear in real code. Conversely, algorithms for continuous distributions cannot efficiently handle discrete program structure. In another thrust, this project studies symbolic approaches to probabilistic reasoning in programs with both types of structure, using recent breakthroughs based on satisfiability modulo theories and hashing-based sampling. This project provides a scientific leap at a fundamental level. It also provides a context for training undergraduate and graduate students in subjects spanning machine learning, artificial intelligence, statistics, and programming languages, and targets the integration of probabilistic programming into computer science curricula."
"1821828","Collaborative Research: Productivity Prediction of Microbial Cell Factories using Machine Learning and Knowledge Engineering","MCB","Systems and Synthetic Biology","10/05/2017","08/06/2018","Forrest Sheng Bao","IA","Iowa State University","Standard Grant","Anthony Garza","07/31/2021","$230,672.00","","fsb@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","BIO","8011","144E, 1757, 7465","$0.00","Over the past decade, systems and synthetic biology approaches provided novel mechanism to enhance the production of diverse chemicals and biofuels from renewable resources in laboratory settings. However, it is still rare for synthetically modified strains to meet the production requirement for commercialization. Strain development falls into the tedious and costly design-build-test-learn cycle because existing modeling approaches failed to capture the complicated metabolic responses in such engineered cells. This proposal will explore an alternate, data-driven approach that has the potential to predict the productivity of synthetic organisms by leveraging the vast array of microbial cell factory publications. Using Artificial Intelligence approaches such as Machine Learning and Knowledge Representation, one can abstract ""previous lessons'' hidden in published data to facilitate a priori estimations of the metabolic output by engineered hosts given a set of specific genetic instructions and fermentation growth conditions. The resulting platform can assist current constraint-based models to design the most effective strategies for producing value-added chemicals. On the educational front, this proposal will offer educational and research training opportunities in synthetic biology, computer programming, and artificial intelligence for graduate students to provide them with a non-conventional career pathway. <br/><br/>Synthetic biology relies on extensive genetic modification and pathway engineering, which often result in unexpected physiological changes or metabolic shifts that reduce the productivity and stability of the hosts. The investigators conceived of a creative, multidisciplinary approach that relies on artificial intelligence-inspired methods for predicting the performance of two distinct unicellular cell factories (Escherichia coli and Saccharomyces cerevisiae). These platforms can be used to quantify the factors that govern microbial productivity (yield, titer, and growth rate), including the type and availability of metabolic precursors; the elements that constitute a biosynthetic pathway; fermentation conditions; and the specific genetic modification to optimize the system. By extracting and classifying information derived from referenced publications within the last 20 years, one can construct a ''knowledge base'' containing sufficient samples of bio-production assemblies. This information will then inform the building of cellular factories using supervised machine learning and non-monotonic logic programming to estimate the productivity of hosts. The data-driven platform will also be integrated into genome scale models to project physiological changes of specific mutant strains. This novel approach will reduce the need for costly design-build-test bench work. Key outcomes from this project include: (1) a database to standardize synthetic biology studies, (2) machine learning models to recognize lessons and patterns hidden in published data, and (3) integration of machine learning with flux balance models, leading to the design of strains with high chances of success in industry settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1806332","SCH: EXP: Intelligent Clinical Decision Support with Probabilistic and Temporal EHR Modeling","IIS","Smart and Connected Health","08/15/2017","11/17/2017","Sriraam Natarajan","TX","University of Texas at Dallas","Standard Grant","Sylvia Spengler","12/31/2018","$127,346.00","","sriraam.natarajan@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","8018","8018, 8061","$0.00","Clinical decision support has the potential to reduce healthcare costs and improve patient outcomes, while shedding light into policy questions surrounding healthcare costs and practices in the US.  This project aims to develop intelligent clinical decision support techniques for recommending optimal action plans - including both diagnostic tests and medical interventions - for treating chronic disease, performing multi-step and adaptive treatments, and modifying long-term health habits. In an effort to integrate evidence-driven decision-making with established clinical practices, the research will develop disease-agnostic artificial intelligence techniques that combine data from large electronic health records (EHRs) with recommendations from human experts. A prototype decision support system will be tested on three clinical settings - cardiology, clinical depression, and emergency room readmission - using existing EHR datasets and consultation with domain experts from clinical partners. Outcomes-driven and cost-driven optimized decisions will be compared to current clinical practice. This exploratory research will provide the groundwork for follow-up projects in decision support information presentation, integration with clinical workflow and IT systems, and making the transition from retrospective studies to clinical trials.  Other broader impacts include workshops for healthcare applications of AI, and women and minority students will be recruited and mentored in graduate and undergraduate computer science research.<br/><br/>The technical approach of this research builds on state-of-the-art machine learning and artificial intelligence methods to automatically learn, simulate, and reason about patient-specific treatment plans.  Such methods must be simultaneously probabilistic and temporal.  Probabilistic techniques are needed to handle significant uncertainties in clinical diagnoses and outcomes, much like a human clinician would.  Temporal techniques are needed to consider sequences of future decisions over the course of treatment, rather than decisions at single time points.  More specifically, this project will consider the use of statistical relational learning (SRL) techniques to mine for probabilistic, temporal patterns in large electronic health records, and these patterns will be used in partially-observable Markov decision processes (POMDPs) that exhaustively search for optimal treatment sequences. Recent results indicate that SRL achieves superior performance to other machine learning methods in predicting cardiac arrest from demographic and lifestyle observations, and POMDP treatment plans outperform existing fee-for-service practices by reducing costs by 50% and improving outcomes by 40% on a clinical depression dataset.  By combining SRL and POMDPs, specifically, using SRL to learn a disease progression model used by the POMDP, this project aims to achieve further improvements in recommendation quality and computational scalability for complex treatments.  Furthermore, because EHRs may suffer from limited or missing data, clinical decision support tools should follow established practices and expert knowledge when necessary.  To do so, new workflows for integrating expert knowledge into SRL and POMDPs will be explored.  Evaluation will be performed on a variety of disease scenarios in conjunction with clinical partners at Marshfield Clinic, Centerstone, Wake Forest School of Medicine, and South Bend Memorial Hospital."
"1718846","RI: Small: Linguistic Structure in Neural Sequence Models","IIS","Robust Intelligence","08/01/2017","07/27/2017","Jason Eisner","MD","Johns Hopkins University","Standard Grant","Tatiana Korelsky","07/31/2021","$395,002.00","","jason@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7495","7495, 7923","$0.00","Over the past 25 years, the field of artificial intelligence has made great strides in the ability to automatically analyze and generate sequential data.  Much of this progress has come by building probabilistic models.  For example, mathematical descriptions of how words are typically used in context are based on a scientific understanding of the relationships among letters, sounds, words, and phrases, thanks to the field of linguistics.  Probabilistic models based on this understanding have allowed us to develop computational, data-driven methods for reasoning about the likely structure and meaning of sentences.  In the same way, probabilistic models of sequences of events have led to computational methods for predicting the unfolding of future events and reconstructing the ordering of past ones.  This project starts with sophisticated probabilistic models of linguistic structure and event sequences, and aims to make them more powerful, by using ""deep learning"" (neural networks) to increase their sensitivity to contextual effects.  Deep learning has already recently had a revolutionary impact on artificial intelligence.  This research will focus on using deep learning to enhance probabilistic models in settings where the model must discover structure that is not provided in its training data, such as the compositional units of language or the causal relations among events.<br/><br/>The planned model design will not focus on hand-engineered features, but rather on broad representational choices.  The overall architectures are motivated by certain basic notions that linguists and other modelers have found indispensable in their analyses of empirical data as follows: (1) stick-breaking processes that respect duality of patterning, the linguistic notion that a word's internal form is not necessarily related to its external usage but is governed by separate rules or by chance; (2) finite-state transducers that can capture local editing that transforms an input sequence into an output sequence; (3) context-free grammars that can model hierarchical structure to help explain word sequences; and (4) temporal point processes that can capture process intensity, where different events are competing to occur next, and combinations of earlier events combine to elevate or suppress the rates of later events.  The project will infuse these probabilistic techniques with recurrent neural networks, in particular, long short-term memory (LSTM) networks.  In some cases, exact inference in the resulting models will not be tractable, necessitating the design of Monte Carlo or variational approximations."
"1657600","CRII: RI: Reasoning Geometric Commonsense for 3D Image/Video Parsing","IIS","CRII CISE Research Initiation","05/01/2017","05/03/2017","Xiaobai Liu","CA","San Diego State University Foundation","Standard Grant","Jie Yang","04/30/2020","$115,062.00","","xiaobai.liu@mail.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","CSE","026Y","7495, 8228","$0.00","Commonsense reasoning studies the consensus reality, knowledge, causality, and rationales available to the overwhelming majority of people, and can be used to enhance all aspects of Artificial Intelligence (AI). This project develops representations of geometric commonsense as well as computing principles of commonsense reasoning for computer vision applications. The project systemically studies commonsense knowledge over geometric dimensions of scene entities, e.g., the length of a sedan is shorter than that of a bus; or that window edges on the same façade are parallel to each other and are orthogonal to the edges on the ground. These first-order and second-order knowledges, once extracted, are fairly stable across different types of scenes, and are informative enough for enhancing the understanding of images or videos in both 2D and 3D. The project integrates research with education by supporting graduate students, and outreaches to computer vision and AI research communities by organizing workshops in the relevant conferences.<br/><br/>This research studies geometric commonsense reasoning for 3D scene parsing in images or videos, and contributes a unified probabilistic approach that is capable of reconstructing a wide variety of scene categories (e.g., suburb, urban, campus) from a single input image or a monocular video sequence.  The project approaches the problem from two aspects. First, a new attributed grammar model is developed to represent both images and the associated geometric commonsense knowledge using a hierarchical graphical structure. With this grammar model, the segmentation of semantic regions, the reconstruction of scene entities, and the reasoning of geometric commonsense can be all solved through creating a valid parse graph from images or videos. Second, a new computing framework is introduced so that the inference of image parsing can be conducted in the joint space of discrete semantic labels and continuous geometric labels, and the learning of grammar models can be conducted over training images with weak supervision. The developed techniques enable a state-of-the-art computer vision system that can robustly estimate semantic and geometric scene structures from images or videos."
"1741034","1st US-Japan Workshop Enabling Global Collaborations in Big Data Research; June, 2017, Atlanta, GA","CNS","S&CC: Smart & Connected Commun","05/15/2017","05/10/2017","Calton Pu","GA","Georgia Tech Research Corporation","Standard Grant","Meghan Houghton","04/30/2018","$25,000.00","","calton@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","033Y","7556","$0.00","The 1st US-Japan Workshop Enabling Global Collaborations in Big Data Research brings together researchers from the United States (U.S.) and Japan to discuss experiences, challenges, and opportunities in international research collaborations. The workshop provides opportunities for participants from both countries to identify mutual research interests that leverage resources and expertise to accelerate advancements in smart and connected communities, cyber-physical systems, artificial intelligence, and machine learning. The workshop includes two tracks to support a broad range of participation: track 1, for participants with prior collaboration experience and significant potential to advance the field; and track 2, for those without prior collaboration experience who may benefit significantly from the diverse training environments afforded by international collaborations. The outcomes of the workshop will be disseminated through a report describing the main strategic areas, key research opportunities, and collaboration scenarios discussed during the workshop, thereby benefiting a broader research community. The workshop is co-located with the 2017 IEEE International Conference on distributed Computing Systems (ICDCS). This engagement builds upon prior National Science Foundation (NSF)-Japan Science and Technology Agency (JST) collaborations.<br/><br/>The areas of smart and connected communities, cyber-physical systems, artificial intelligence, and machine learning present intellectual challenges of their own, as well as challenges and opportunities at their intersections. Specifically, the challenges and opportunities created by growing data, sensors, cloud and edge computing, and networking at a global scale call for international research collaborations. For example, advances in machine learning and artificial intelligence, paired with the development and implementation of large sensor networks (e.g., Array of Things in Chicago and Fujisawa Sustainable Smart Town in Japan) can enable city-scale data to improve efficiency, economic prosperity, and security in our cities and communities. The workshop will focus on research challenges that are beyond the individual reach of each participant, but that become feasible goals with effective collaboration between the two countries. This can be achieved when both sides share similar interests, but with complementary expertise and skills, (e.g., from different but related areas such as the examples mentioned above)."
"1717062","SaTC: CORE: Small: Scalable and Meaningful Threat Intelligence Generation","CNS","Secure &Trustworthy Cyberspace","08/15/2017","08/07/2017","Damon McCoy","NY","New York University","Standard Grant","Sara Kiesler","07/31/2021","$492,064.00","","dm181@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","8060","025Z, 7434, 7923, 9102","$0.00","Threat intelligence is used by organizations to protect systems and end-users by detecting and blocking communications with known attackers' systems. Quality threat intelligence can also provide methods of detecting attackers' tools, which are often less ephemeral than their attack infrastructure. Unfortunately, producing quality threat intelligence is often a highly manual and inefficient process. This has resulted in limited amounts of useful threat intelligence which is available only to those companies that can afford it. This research develops new data-analytics methods to identify an attacker's infrastructure and attack tools. Our methods leverage the ability to efficiently collect large amounts of raw attacker data, process it, and build artificial intelligence techniques to discover attack patterns. This project improves the efficiency of generating high quality threat intelligence data, and makes it more affordable to a large range of companies.<br/> <br/>Achieving this goal of improving the efficiency of generating useful threat intelligence requires progress on several key challenges. The project (i) investigates supervised machine learning based methods for efficiently collected large-scale amounts of data from attackers, (ii) improves methods for storing this data and other freely available raw threat intelligence data such that it can be easily joined, (iii) identifies robust features that can be extracted from this raw data which can be used for training supervised machine learning detection techniques, and (iv) enables high performance and efficient generation of large-scale useful threat intelligence data. Consequently, this research has the potential to transform the way in which threat intelligence data is produced and improve the security of organizations by making threat intelligence more accessible. This work also creates many educational opportunities for undergraduate and graduate students to gain experience using data-analytics techniques to efficiently detect emerging threats and improve the security of organizations."
"1652052","CAREER: Active and Action-Centric Visual Understanding","IIS","Robust Intelligence","07/01/2017","07/17/2020","Ali Farhadi","WA","University of Washington","Continuing Grant","Jie Yang","06/30/2023","$549,999.00","","afarhad2@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","1045, 7495","$0.00","This project develops technologies for visual semantic planning; the problem of producing ordered sequences of actions that change the current world state from what is depicted in a given image or video to the state defined by a query task. The project bridges the gap between current levels of image understanding and what is needed to actively understand the visual world to the extent that an agent can plan and perform tasks. The project develops the technology for a crucial next step in recognition: active and action-centric image understanding by semantic understanding of actions, their preconditions and effects, and visual planning.  Doing so empowers several applications in healthcare, prospective memory failure care, visually impaired care, elderly care, robotics, entertainment, and education.<br/><br/>This research addresses the visual planning problem that entails knowing what actions are, how they change the world state, and which sequences of actions change the current state to a desired one. Successful active understanding of images requires addressing several fundamental and challenging problems at the intersection of computer vision and artificial intelligence. The research is focused on the development of a framework for active visual understanding, new scalable algorithms for joint detection of actions and their arguments, new datasets and representations for actions' preconditions and effects, new algorithms for predicting the consequences of actions with intuitive laws of physics, and visual semantic planning. The developed framework is designed for active and action-centric image understanding by large-scale, semantic action recognition, modeling actions' preconditions and effects, predicting consequences of actions, and visual planning. These resources not only enable new research directions in computer vision, robotics, and AI, but also bring together some of the independent efforts across these disciplines."
"1738065","EAGER: SC2: Intelligent spectrum collaboration via a dynamically reconfigurable radio architecture","CNS","Networking Technology and Syst","04/01/2017","03/24/2017","Tan Wong","FL","University of Florida","Standard Grant","Monisha Ghosh","03/31/2019","$99,363.00","John Shea","twong@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7363","7916","$0.00","Current engineering practices and regulatory approaches on the use of the radio frequency (RF) spectrum are too antiquated to meet the ever<br/>surging demand on the RF spectrum. A promising new solution to tackle this spectrum scarcity problem is to equip radio networks with artificial intelligence so that they can learn and predict the RF environment, as well as be social by interacting with other radio networks, leading to more collaborative use of the RF spectrum. This project will develop a software-defined radio system that can intelligently sense and adapt to others' use of the radio spectrum and collaborate with other radio networks in sharing the common RF spectrum. The developed system will be characterized by its flexibility to quickly and agile adaptability to changes in how others are using the RF spectrum. It will be also be characterized by how it uses machine-learning techniques to both extract the most relevant information about how the RF spectrum is being used and to adapt the communication strategies based on this information.<br/><br/>A dynamically reconfigurable system architecture will be developed in this project to make most efficient use of all the available computational resources in order to support all radio and ML functionalities. This highly flexible software-defined structure takes advantage of the learned knowledge about the RF environment by adapting the physical and medium access control layers use of spectrum and coordinating this utilization through carefully designed network protocols. A machine learning system is developed to identify the key information about the evolution of the communication scenario, and autonomously learn the state of the model.  Reinforcement learning will be used to generate appropriate adaptive communication strategies based on the system state."
"1709351","CDS&E: D3SC: The Dark Reaction Project:  A machine-learning approach to exploring structural diversity in solid state synthesis","DMR","CONDENSED MATTER & MAT THEORY, Chem Thry, Mdls & Cmptnl Mthds, Data Cyberinfrastructure","09/01/2017","07/26/2017","Joshua Schrier","PA","Haverford College","Standard Grant","Daryl Hess","05/31/2019","$645,288.00","Alexander Norquist, Sorelle Friedler","jschrier@fordham.edu","370 Lancaster Avenue","Haverford","PA","190411336","6108961000","MPS","1765, 6881, 7726","054Z, 7433, 8084, 9177","$0.00","NONTECHNICAL SUMMARY<br/>This award receives funds from the Division of Materials Research, the Chemistry Division and the Office of Advanced Cyberinfrastructure. This award supports research and education that uses data-centric methods to enable the prediction of metal oxide compounds with desired properties. Organically-templated metal oxides have a tremendous degree of structural diversity and compositional flexibility. This allows chemists to tune the structures, properties, and symmetries of these compounds to optimize their performance in specific applications that include catalysis, molecular sieving, gas adsorption, and nonlinear optics.  However, new compounds are typically created by a trial-and-error procedure, and creating novel compounds with specific structures is a grand challenge in solid state chemistry.  This project will develop artificial intelligence techniques for computers called machine learning techniques that can be used to predict the conditions for chemical reactions that will increase structural diversity and lead to specific structural features.  This project will also develop machine learning techniques that generate human-readable explanations about the formation mechanism, which will be tested in the laboratory.<br/> <br/>The primary impact of this project will be to decrease the amount of time and to lower the cost of discovering new materials with specific structural features, which in turn help bring new materials for applications to market more quickly.  This project is an example of a collaboration among synthetic chemists, computational chemists, and computer scientists and as a model it may be directly transferred to a wide range of disciplines and avenues of investigation. Undergraduate student research opportunities and curricular developments will be involved throughout the project, thus contributing to the scientific workforce.<br/> <br/>TECHNICAL SUMMARY <br/>This award receives funds from the Division of Materials Research, the Chemistry Division and the Office of Advanced Cyberinfrastructure. This award supports research and education that uses data-centric methods to enable the prediction of metal oxide compounds with desired properties. Hydrothermal synthesis is widely used to create new metal oxide materials with a wide range of functional properties and applications.  This project will advance the field by developing software infrastructure for associating the results of X-ray diffraction experiments with individual reactions, extracting structural outcome descriptors from this data, and then determining the extent to which these structural outcomes can be predicted from reaction description data.  This will be achieved by developing structural outcome descriptors for geometric properties, non-covalent interaction properties, and electron-density properties, then building machine learning models that correlate these outcomes to reaction conditions, and finally testing the quality of these predictions experimentally.  Active learning and auditable and interpretable models will be incorporated into the workflows to help synthetic chemists select better (more insightful/novel) reactions in an interactive fashion."
"1704908","RI: Medium: Collaborative Research: Causal Inference: Identification, Learning, and Decision-Making","IIS","Robust Intelligence","08/01/2017","07/27/2017","Elias Bareinboim","IN","Purdue University","Standard Grant","Rebecca Hwa","01/31/2020","$536,513.00","","eb@cs.columbia.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7495","7495, 7924","$0.00","Understanding the causal mechanisms underlying an observed phenomenon is one of the primary goals of science. The realization that statistical associations in themselves are insufficient for elucidating those mechanisms has led researchers to enrich traditional statistical analysis with techniques based on ""causal inference"". Most of the recent advances in the field, however, operate under overly optimistic assumptions, which are often not met in practical, large-scale situations. This project seeks to develop a sound and general causal inference theory to cover those situations. The goal is to design a framework for decision-making of intelligent systems, including (1) learning a causal representation of the data-generating environment (learning), (2) performing efficient inference leveraging the learned model (planning/inference), and (3) using the new inferred representation, based on (1) and (2), to decide how to act next (decision-making). The new finding will benefit investigators in every area of the empirical sciences, including artificial intelligence, machine learning, statistics, economics, and the health and social sciences. The research is expected to fundamentally change the practice of data science in areas where the standard causal assumptions are violated (i.e., missing data, selection bias, and confounding bias). The work on decision-making is expected to pave the way toward the design of an ""automated scientist"", i.e., a program that combines both observational and experimental data, conducts its own experiments, and decides on the best choices of actions and policies. The project also helps to disseminate the principles of causal inference throughout the sciences by (1) engaging in the establishment of new ""data science"" curriculum where causal inference plays a central role, and (2) developing new educational materials for students and the general public explaining the practice of causal inference (e.g., book). Furthermore, the project supports the causal inference community by fostering a number of educational initiatives such as forums, workshops, and the creation of new incentives for the development of educational material (e.g., a ""Causality Education Award"").<br/><br/>Making claims about the existence of causal connections (structural learning), the magnitude of causal effects (identification), and designing optimal interventions (decision-making) are some of the most important tasks found throughout data-driven fields. This project studies identification, learning, and decision-making settings where (1) data are missing not at random, (2) non-parametric estimation is not feasible, and (3) aggregated behavior does not translate into guidance for individual-level decision-making. Specifically, the project considers the problem when measurements are systematically distorted (missing data), which has received an enormous amount of attention in the statistical literature, but has not essentially been investigated in the context of causal inference when data are missing not at random. The project further aims to leverage the special properties of linear models, the most common first approximation to non-parametric causal inference, to elucidate causal relationships in data, and to facilitate sensitivity analysis in such models.  Finally, the project considers the fundamental problem on how causal and counterfactual knowledge can speed-up experimentation and support principled decision-making. The goal is to develop a complete algorithmic theory to determine when a particular causal effect can be learned from data and how to incorporate causal knowledge learned (possibly by experimentation) so that it can be amortized over new environmental conditions."
"1704932","RI: Medium: Collaborative Research: Causal Inference: Identification, Learning, and Decision-Making","IIS","Robust Intelligence","08/01/2017","07/27/2017","Judea Pearl","CA","University of California-Los Angeles","Standard Grant","Rebecca Hwa","07/31/2020","$265,000.00","","judea@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7495, 7924","$0.00","Understanding the causal mechanisms underlying an observed phenomenon is one of the primary goals of science. The realization that statistical associations in themselves are insufficient for elucidating those mechanisms has led researchers to enrich traditional statistical analysis with techniques based on ""causal inference"". Most of the recent advances in the field, however, operate under overly optimistic assumptions, which are often not met in practical, large-scale situations. This project seeks to develop a sound and general causal inference theory to cover those situations. The goal is to design a framework for decision-making of intelligent systems, including (1) learning a causal representation of the data-generating environment (learning), (2) performing efficient inference leveraging the learned model (planning/inference), and (3) using the new inferred representation, based on (1) and (2), to decide how to act next (decision-making). The new finding will benefit investigators in every area of the empirical sciences, including artificial intelligence, machine learning, statistics, economics, and the health and social sciences. The research is expected to fundamentally change the practice of data science in areas where the standard causal assumptions are violated (i.e., missing data, selection bias, and confounding bias). The work on decision-making is expected to pave the way toward the design of an ""automated scientist"", i.e., a program that combines both observational and experimental data, conducts its own experiments, and decides on the best choices of actions and policies. The project also helps to disseminate the principles of causal inference throughout the sciences by (1) engaging in the establishment of new ""data science"" curriculum where causal inference plays a central role, and (2) developing new educational materials for students and the general public explaining the practice of causal inference (e.g., book). Furthermore, the project supports the causal inference community by fostering a number of educational initiatives such as forums, workshops, and the creation of new incentives for the development of educational material (e.g., a ""Causality Education Award"").<br/><br/>Making claims about the existence of causal connections (structural learning), the magnitude of causal effects (identification), and designing optimal interventions (decision-making) are some of the most important tasks found throughout data-driven fields. This project studies identification, learning, and decision-making settings where (1) data are missing not at random, (2) non-parametric estimation is not feasible, and (3) aggregated behavior does not translate into guidance for individual-level decision-making. Specifically, the project considers the problem when measurements are systematically distorted (missing data), which has received an enormous amount of attention in the statistical literature, but has not essentially been investigated in the context of causal inference when data are missing not at random. The project further aims to leverage the special properties of linear models, the most common first approximation to non-parametric causal inference, to elucidate causal relationships in data, and to facilitate sensitivity analysis in such models.  Finally, the project considers the fundamental problem on how causal and counterfactual knowledge can speed-up experimentation and support principled decision-making. The goal is to develop a complete algorithmic theory to determine when a particular causal effect can be learned from data and how to incorporate causal knowledge learned (possibly by experimentation) so that it can be amortized over new environmental conditions."
"1657598","CRII: AF: Characterization and Complexity of Information Elicitation","CCF","CRII CISE Research Initiation, Algorithmic Foundations","06/01/2017","05/29/2018","Rafael Frongillo","CO","University of Colorado at Boulder","Standard Grant","Tracy Kimbrel","05/31/2020","$182,300.00","","raf@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","026Y, 7796","7796, 7932, 8228, 9251","$0.00","The way one judges the accuracy of predictions can greatly impact what predictions people or computers make.  For example, Glenn Brier argued in 1950 that the way meteorologists were evaluated would actually give them an incentive to distort the true probability of rain.  Brier's study inspired a growing body of work in statistics, economics, and now computer science, which studies evaluation metrics that incentivize accurate reports from people or machines.  These evaluation metrics are also used in machine learning, a branch of artificial intelligence, where a designer implicitly tells the computer what statistic to predict by providing only the evaluation metric itself.  This project seeks to mathematically characterize this link between statistics and evaluation metrics, and moreover, to understand the computational and statistical difficulty of evaluating different statistics.  A precise understanding of this link would provide new evaluation metrics with the potential to increase predictive power across a vast array of applications such as climate simulations and smart cities.  In particular, metrics for statistics that quantify uncertainty or risk could improve decision making in many fields, including healthcare, engineering, and finance.<br/><br/>A dominant algorithmic paradigm in machine learning, encompassing most regression techniques and classification algorithms, is that of empirical risk minimization (ERM): choosing a model from some class that best fits the data, according to some evaluation metric called a loss function.  A thread of research in theoretical machine learning called property elicitation gives a mathematical formalism to describe the link between loss functions and their corresponding statistics.  In these terms, this project seeks to characterize the statistics which have calibrated loss functions, and determine how many regression parameters or data points are required for the calibration to hold.  These questions are particularly relevant to machine learning when restricting attention to certain classes of loss functions which can be easily optimized or which have desirable statistical learning guarantees.  The class of statistics from mathematical finance known as risk measures, which are used to regulate banks, form an important focus of the project."
"1658406","Collaborative Research: Conference on Cognitive Computational Neuroscience (CCN)","BCS","COGNEURO, CRCNS","04/01/2017","03/30/2017","Thomas Naselaris","SC","Medical University of South Carolina","Standard Grant","Uri Hasson","03/31/2018","$15,000.00","","tnaselar@musc.edu","171 ASHLEY AVE","CHARLESTON","SC","294258908","8437923838","SBE","1699, 7327","1699, 7327, 7556, 8089, 8091, 9150","$0.00","Cognitive Computational Neuroscience (CCN) is an annual scientific meeting for neuroscientists characterizing the neural computations that underlie complex behavior. The goal is to develop computationally defined models of brain information processing that explain rich measurements of brain activity and behavior. Such models will ultimately have to perform feats of intelligence such as perception, internal modelling and memory of the environment, decision-making, planning, action, and motor control under naturalistic conditions. Historically, different disciplines have met subsets of these goals. Cognitive science has developed computational models at the cognitive level to explain aspects of complex behavior. Computational neuroscience has developed neurobiologically plausible computational models to explain neuronal responses to sensory stimuli and certain low-dimensional decision, memory, and control processes. Cognitive neuroscience has mapped a broad range of cognitive processes onto brain regions. Artificial intelligence has developed models that perform feats of intelligence. The community must now put the pieces of the puzzle together, and CCN is unique in its focus on the intersection between these fields. CCN is envisioned not only as an engine for advancing research, but as a vehicle for making broader impacts on education and society. As evidenced by the recent trend of major corporate acquisitions of AI startups founded by neuroscientists, biological inspiration for electronics and software development is a growing trend with significant economic implications. In its early stages, the broader impact focus of CCN will be on increasing the visibility of women and scientists from underrepresented populations via speaking opportunities and travel awards.  In addition, representation on women on the female fractions on the steering and advisory committees exceed those typical in relevant fields, without compromise in qualifications. Conferences will include hands-on tutorials, and materials from these will propagate to various university curricula.<br/><br/>A central goal of neuroscience is to understand how vast populations of neurons give rise to complex behavior. Today, advances in various domains offer tangible possibilities to make fundamental conceptual breakthroughs. From an experimental point of view, neural recording technologies, such as high-resolution fMRI, dense recording arrays, magnetoencephalography (MEG), and calcium imaging, now provide opportunities to observe neural activity at unprecedented resolution and scale. At the same time, research in cognitive science has become increasingly sophisticated in identifying computational principles that may serve as the basis for human cognition, and machine learning and artificial intelligence have made great strides in building models to autonomously solve complex cognitive tasks. However, interactions among these distinct disciplines remain rare. This new conference may stimulate unifying frameworks that fully realize the cross-disciplinary potential of these individual advances. In more concrete terms, the goal of CCN is to create and foster a community that will develop models of brain information processing with several key features. These models should (1) be fully computationally defined and implemented in computer simulations; (2) be neurobiologically plausible; (3) explain measurements of brain activity (and continue to do so as spatiotemporal resolution and scale improve); (4) explain behavior for naturalistic stimuli and tasks; and (5) perform feats of intelligence such as recognition, internal modelling and representation of the environment, decision-making, planning, action, and motor control. Such models currently do not exist and are unlikely to emerge without greatly improved cross-disciplinary engagement."
"1704352","RI: Medium: Collaborative Research: Causal Inference: Identification, Learning, and Decision-Making","IIS","Robust Intelligence","08/01/2017","07/27/2017","Jin Tian","IA","Iowa State University","Standard Grant","Rebecca Hwa","07/31/2020","$260,169.00","","jtian@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7495","7495, 7924","$0.00","Understanding the causal mechanisms underlying an observed phenomenon is one of the primary goals of science. The realization that statistical associations in themselves are insufficient for elucidating those mechanisms has led researchers to enrich traditional statistical analysis with techniques based on ""causal inference"". Most of the recent advances in the field, however, operate under overly optimistic assumptions, which are often not met in practical, large-scale situations. This project seeks to develop a sound and general causal inference theory to cover those situations. The goal is to design a framework for decision-making of intelligent systems, including (1) learning a causal representation of the data-generating environment (learning), (2) performing efficient inference leveraging the learned model (planning/inference), and (3) using the new inferred representation, based on (1) and (2), to decide how to act next (decision-making). The new finding will benefit investigators in every area of the empirical sciences, including artificial intelligence, machine learning, statistics, economics, and the health and social sciences. The research is expected to fundamentally change the practice of data science in areas where the standard causal assumptions are violated (i.e., missing data, selection bias, and confounding bias). The work on decision-making is expected to pave the way toward the design of an ""automated scientist"", i.e., a program that combines both observational and experimental data, conducts its own experiments, and decides on the best choices of actions and policies. The project will also help to disseminate the principles of causal inference throughout the sciences by (1) engaging in the establishment of new ""data science"" curriculum where causal inference plays a central role, and (2) developing new educational materials for students and the general public explaining the practice of causal inference (e.g., book). Furthermore, the project supports the causal inference community by fostering a number of educational initiatives such as forums, workshops, and the creation of new incentives for the development of educational material (e.g., a ""Causality Education Award"").<br/><br/>Making claims about the existence of causal connections (structural learning), the magnitude of causal effects (identification), and designing optimal interventions (decision-making) are some of the most important tasks found throughout data-driven fields. This project will study identification, learning, and decision-making settings where (1) data are missing not at random, (2) non-parametric estimation is not feasible, and (3) aggregated behavior does not translate into guidance for individual-level decision-making. Specifically, the project will consider the problem when measurements are systematically distorted (missing data), which has received an enormous amount of attention in the statistical literature, but has not essentially been investigated in the context of causal inference when data are missing not at random. The project will further aim to leverage the special properties of linear models, the most common first approximation to non-parametric causal inference, to elucidate causal relationships in data, and to facilitate sensitivity analysis in such models.  Finally, the project will consider the fundamental problem on how causal and counterfactual knowledge can speed-up experimentation and support principled decision-making. The goal is to develop a complete algorithmic theory to determine when a particular causal effect can be learned from data and how to incorporate causal knowledge learned (possibly by experimentation) so that it can be amortized over new environmental conditions."
"1658493","Collaborative Research: Conference on Cognitive Computational Neuroscience (CCN)","BCS","COGNEURO, CRCNS","04/01/2017","03/30/2017","Alyson Fletcher","CA","University of California-Los Angeles","Standard Grant","Uri Hasson","03/31/2018","$16,560.00","","akfletcher@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","1699, 7327","1699, 7327, 7556, 8089, 8091, 9150","$0.00","Cognitive Computational Neuroscience (CCN) is an annual scientific meeting for neuroscientists characterizing the neural computations that underlie complex behavior. The goal is to develop computationally defined models of brain information processing that explain rich measurements of brain activity and behavior. Such models will ultimately have to perform feats of intelligence such as perception, internal modelling and memory of the environment, decision-making, planning, action, and motor control under naturalistic conditions. Historically, different disciplines have met subsets of these goals. Cognitive science has developed computational models at the cognitive level to explain aspects of complex behavior. Computational neuroscience has developed neurobiologically plausible computational models to explain neuronal responses to sensory stimuli and certain low-dimensional decision, memory, and control processes. Cognitive neuroscience has mapped a broad range of cognitive processes onto brain regions. Artificial intelligence has developed models that perform feats of intelligence. The community must now put the pieces of the puzzle together, and CCN is unique in its focus on the intersection between these fields. CCN is envisioned not only as an engine for advancing research, but as a vehicle for making broader impacts on education and society. As evidenced by the recent trend of major corporate acquisitions of AI startups founded by neuroscientists, biological inspiration for electronics and software development is a growing trend with significant economic implications. In its early stages, the broader impact focus of CCN will be on increasing the visibility of women and scientists from underrepresented populations via speaking opportunities and travel awards.  In addition, representation on women on the female fractions on the steering and advisory committees exceed those typical in relevant fields, without compromise in qualifications. Conferences will include hands-on tutorials, and materials from these will propagate to various university curricula.<br/><br/>A central goal of neuroscience is to understand how vast populations of neurons give rise to complex behavior. Today, advances in various domains offer tangible possibilities to make fundamental conceptual breakthroughs. From an experimental point of view, neural recording technologies, such as high-resolution fMRI, dense recording arrays, magnetoencephalography (MEG), and calcium imaging, now provide opportunities to observe neural activity at unprecedented resolution and scale. At the same time, research in cognitive science has become increasingly sophisticated in identifying computational principles that may serve as the basis for human cognition, and machine learning and artificial intelligence have made great strides in building models to autonomously solve complex cognitive tasks. However, interactions among these distinct disciplines remain rare. This new conference may stimulate unifying frameworks that fully realize the cross-disciplinary potential of these individual advances. In more concrete terms, the goal of CCN is to create and foster a community that will develop models of brain information processing with several key features. These models should (1) be fully computationally defined and implemented in computer simulations; (2) be neurobiologically plausible; (3) explain measurements of brain activity (and continue to do so as spatiotemporal resolution and scale improve); (4) explain behavior for naturalistic stimuli and tasks; and (5) perform feats of intelligence such as recognition, internal modelling and representation of the environment, decision-making, planning, action, and motor control. Such models currently do not exist and are unlikely to emerge without greatly improved cross-disciplinary engagement."
"1717705","CHS:Small: A Kinder, Gentler Technology: Enhancing Human-Machine Symbiosis Using Adaptive, Personalized Affect-Aware Systems","IIS","HCC-Human-Centered Computing","08/15/2017","08/04/2017","Domen Novak","WY","University of Wyoming","Standard Grant","Balakrishnan Prabhakaran","07/31/2020","$447,889.00","Sean McCrea","dnovak1@uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","CSE","7367","7367, 7923, 9150","$0.00","A longstanding goal in artificial intelligence is to develop smart systems that interact well with humans.  Advances in sensing and machine learning are increasingly allowing computers to infer mental states, raising questions about how agents might use those inferences to adapt to human partners.  This project will systematically address how to design and evaluate ""affect-aware"" systems that adapt their behavior based on estimates of their users' emotional experiences.  The team will first look at the effectiveness of current strategies that vary the difficulty of educational tasks and games based on inferred affect.  They will then develop new strategies that take into account both individual personality and dynamic characteristics of the physical environment.  Finally, they will evaluate these strategies, paying particular attention to what happens when systems act on incorrect inferences about affect.  These studies will help pave the way toward self-driving cars, conversational assistants, and virtual reality characters that consider affect when interacting with people, ideally leading to better experiences and outcomes.  The team will also develop new interdisciplinary courses in human factors and human-computer interaction, connecting with industrial partners to help train students in both the practice and research of these kinds of adaptive systems.  Further, they will do public outreach about these systems and use them to provide summer research experiences for K-12 and community college students, focusing on those from groups traditionally underrepresented in computing.<br/><br/>The project will be structured as a series of lab studies, using spatial cognition games and robot-assisted motor rehabilitation tasks as testbeds that allow the team to directly manipulate task difficulty and measure enjoyment/engagement and performance/learning outcomes.  The team will first collect training data with people using the testbeds at randomly selected difficulty levels and reporting the perceived level of difficulty as too easy (bored), too hard (frustrated), or about right, while capturing heart rate signals, skin conductance and temperature, electroencephalogram (EEG) data, and environmental factors including light, time of day, and room temperature.  These will be used to train affect recognizers using a variety of machine learning methods: linear discriminant analysis (including a Kalman adaptive version), support vector machines, neural and Bayesian networks, and random forests.  Using a common adaptation strategy that adjusts difficulty up or down one step, the team will measure the enjoyment and performance outcomes that affect-aware recognizers achieve both with and without considering environmental factors, comparing those to a baseline strategy that adapts difficulty based only on task performance.  During these experiments, the team will also collect data about users' personality characteristics and use those to develop individualized recognition models and adaptation strategies for different personality types. These individualized models and strategies will be evaluated by comparing them to the baseline data from the first experiment.  Finally, they will compare the outcomes of these systems with those from a ""best-case"" system controlled by humans and a ""worst-case"" error-prone system that chooses adaptation strategies randomly, looking at those induced error rates along with the natural error rates captured during the other experiments to determine the effect of recognition and adaptation error on satisfaction and task outcomes."
"1763224","Workshop on Intelligent Cognitive Assistants (ICA). To Be Held November 14-15, 2017 in San Jose, California.","ECCS","CCSS-Comms Circuits & Sens Sys, EPCN-Energy-Power-Ctrl-Netwrks, ENG NNI Special Studies","10/15/2017","10/23/2017","David Wallach","NC","Semiconductor Research Corporation","Standard Grant","Shubhra Gangopadhyay","06/30/2018","$49,500.00","","david.wallach@src.org","4819 Emperor Blvd.","Durham","NC","277035420","9199419400","ENG","7564, 7607, 7681","1653, 7556","$0.00","The PI requests that the CCSS program, within the ECCS Division of the Engineering Directorate at the NSF, to support a workshop titled workshop titled ""Workshop on Intelligent Cognitive Assistants (ICA)"" to be held at IBM Almaden Research Center, San Jose, CA on November 14-15,2017. The goal of the workshop titled ""Workshop on Intelligent Cognitive Assistants (ICA)"" is to identify gaps in science, engineering and technology for developing intelligent, energy-efficient, brain-inspired perception and corresponding devices and systems to support human daily activities and address emerging applications.<br/><br/>The proposed ICA workshop will gather experts from fields spanning psychology, sociology, artificial intelligence and machine learning, robotics, computer science and engineering, to identify the highest priority research needed to address the scientific and engineering challenges of creating the most effective and beneficial Intelligent Assistant-Human collaboration to address applications such as: elder care, autonomous vehicles, smarter communities, and workplace activities."
"1649972","CAREER: Adversarial Artificial Intelligence for Social Good","IIS","Robust Intelligence","03/01/2017","02/14/2018","Yevgeniy Vorobeychik","TN","Vanderbilt University","Continuing grant","Reid Simmons","12/31/2018","$198,977.00","","yvorobeychik@wustl.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","7495","1045, 7495","$0.00","The success of AI technologies has resulted in their widespread deployment, with algorithms for reasoning under uncertainty, such as machine learning, having a particularly high impact.  A challenge that is often ignored, however, is the adversarial nature of many domains, in which social, economic, and political interests may try to manipulate intelligent systems into making costly mistakes.  While AI has a long history in playing adversarial games, such as chess and poker, the approaches have not been appropriate for many real-world situations.  The goal of the proposed research is to develop a general framework for adversarial AI that is far broader in scope and applicability, building on insights from game theory, AI planning, and cybersecurity.<br/><br/>A key modeling insight of the proposed research is that attacks across a broad array of settings can be modeled as planning problems, so that robust algorithms can be fundamentally viewed as interdicting attack plans.  Our research will develop new foundational techniques for scalable plan interdiction under uncertainty, building off of the framework of Stackelberg games. Proposed techniques will leverage a combination of abstraction, factored representation of state, and value function approximation.  In addition, novel scalable algorithms will be developed for multi-stage interdiction problems, modeled as sequential stochastic games, considering both perfect and imperfect information. Moreover, the research will make novel modeling and algorithmic contributions in multi-defender and multi-attacker interdiction games.  Finally, in the more applied arena, the research will make significant intellectual contributions in applying advances in adversarial AI to model problems exhibiting important adversarial aspects, such as privacy-preserving data sharing, access control and audit policies, and vaccine design.<br/>"
"1652735","CAREER: Urban Transport Network Design with Privacy-Aware Agent Learning","CMMI","CAREER: FACULTY EARLY CAR DEV, CIS-Civil Infrastructure Syst","03/15/2017","03/14/2017","Joseph Chow","NY","New York University","Standard Grant","Yueyue Fan","02/28/2022","$500,000.00","","joseph.chow@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","ENG","1045, 1631","029E, 036E, 039E, 1045","$0.00","Technological advances for transportation systems are quickly evolving from their roots in highway materials and traffic control 70 years ago to technologies that support ""smart cities"": autonomous vehicles, on-demand mobility services, and traffic control with machine learning, among others. In 2016, the U.S. Department of Transportation proposed spending $4 billion on autonomous vehicles, and pledged $40 million in tackling smart cities as a grand challenge. However, successful operation of these technologies in a large scale, highly congested city remains prone to operational pitfalls and obstacles. For example, how should a service operator best deploy their vehicles or inform their travelers in real time to optimize service and learning potential simultaneously, while acknowledging their privacy? Some high profile failures include the on-demand transit service Kutsuplus in Helsinki and the Car2Go car share service in San Diego. These systems are highly dynamic, but methods in machine learning and dynamic optimization are not designed for the unique intricacies of urban transport networks. This Faculty Early Career Development (CAREER) Program project is for integrated research and advanced education to create new technologies for these dynamic systems, train students and professionals in these technologies, and engage with private operators and incubators in New York City to operationalize them. The project will use real data from industry partners in ridesharing and autonomous vehicle systems operations, and drive innovation and entrepreneurship by defining new functional roles that mix transportation, computer science, and economics. This will culminate in a test bed in New York City that is expected to shape a next-generation national interdisciplinary research center on ""smart transit"" over the next decade. The PI will recruit local high school students to work with his PhD students each summer; the high school students will be identified through the university's ARISE (Applied Research Innovations in Science and Engineering) program which creates STEM education experiences for women, minorities, and students from low-income backgrounds.  <br/><br/>The research marries three theories together in order to address these new mobility problems in smart cities: dynamic resource allocation under uncertainty, agent-based machine learning, and privacy optimization in a network context. All three are necessary because transportation systems need to be optimized holistically, but data is typically obtained from multiple travelers or vehicles in operation. As such, agent learning and minimization of privacy concerns in the system optimization is needed. The methodology expands the science of inverse optimization by integrating it with dynamic network optimization with privacy control as constraints on the estimated parameters. The research benefits all types of dynamic mobility systems: it will make shared autonomous vehicle fleet operations viable and on-demand service fleets more sustainable and resilient. Data privacy and security in a transport system design context will also be advanced, allowing data from travelers to be more accessible. This research will benefit smart cities, sensor deployment, artificial intelligence, collective behavior, differential privacy, service systems, public policy, and network economics."
"1730158","CI-New: Cognitive Hardware and Software Ecosystem Community Infrastructure (CHASE-CI)","CNS","Special Projects - CNS, CSR-Computer Systems Research, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2017","12/02/2019","Larry Smarr","CA","University of California-San Diego","Standard Grant","Marilyn McClure","09/30/2021","$1,215,998.00","Kenneth Kreutz-Delgado, Tajana Rosing, Ilkay Altintas, Thomas DeFanti","lsmarr@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1714, 7354, 7359","7354, 7359, 9251","$0.00","This project, called the Cognitive Hardware And Software Ecosystem Community Infrastructure (CHASE-CI), will build a cloud of hundreds of affordable Graphics Processing Units (GPUs), networked together with a variety of neural network machines to facilitate development of next generation cognitive computing. This cloud will be accessible by 30 researchers assembled from 10 universities via the NSF-funded Pacific Research Platform. These researchers will investigate a range of problems from image and video recognition, computer vision, contextual robotics to cognitive neurosciences using the cloud to be purpose-built in this project. <br/><br/>Training of neural network with large data-sets is best performed on GPUs. Lack of availability of affordable GPUs and lack of easy access to the new generation of Non-von Neumann (NvN) machines with embedded neural networks impede research in cognitive computing. The purpose-built cloud will be available over the network to address this bottleneck. PIs will study various Deep Neural Network, Recurrent Neural Network, and Reinforcement Learning Algorithms on this platform."
"1753761","I-Corps: Automated Spatiotemporal Intelligence Operations for Asset Integrity Management","IIP","I-Corps","11/01/2017","11/01/2017","Christopher Lippitt","NM","University of New Mexico","Standard Grant","Pamela McCauley","12/31/2018","$50,000.00","","clippitt@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","ENG","8023","9150","$0.00","The broader impact/commercial potential of this I-Corps project includes the enabling of wide scale exploitation of the growing volume of airborne image data from unmanned aerial systems (UAS) to monitor the environment in near real-time. Persistent and tactical surveillance of infrastructure assets (e.g., critical infrastructure, roads, pipelines), military bases, agricultural fields, boarders, or other extensive assets requiring routine monitoring for tactical decision making is becoming cost feasible with the introduction of UAS, but the volume of data collected cannot be exploited using traditional, largely manual, methods. Automated processing and interpretation of large volumes of airborne imagery in near real-time will enable improved decision making and, subsequent, cost reductions and improved performance for a range of industries and agencies. The operations of infrastructure management, disaster response, security, intelligence, and agriculture are amongst the expected beneficiaries.<br/><br/>This I-Corps project explores the commercialization potential of a platform from near real-time analysis and exploitation of airborne image data. Research exploring the development of an airborne system for monitoring critical infrastructure during the response phase on natural disasters resulted in the development of an analytical model for repeat station imaging and refinement of a conceptual model for the design of time-sensitive remote sensing systems that collectively permit the design and implementation of automated change detection and monitoring systems from airborne imaging. This spatial analytics platform automates 3D scene reconstruction, and uses artificial intelligence and machine learning techniques to convert digital photos into a cataloged and indexed spatial intelligence database of changes over time.  3D/4D object classifiers are developed to extract complex features that 2D imagery is unable to represent. This method trains neural networks on volumetric data and multi-temporal spatial data to facilitate the extraction and identification of features and how they have changed. Object identification and characterization will provide the capability to semantically describe changes 3D and 4D space."
"1661516","Collaborative Research: ABI Innovation: Enabling machine-actionable semantics for comparative analyses of trait evolution","DBI","ADVANCES IN BIO INFORMATICS","09/01/2017","08/30/2017","Josef Uyeda","VA","Virginia Polytechnic Institute and State University","Standard Grant","Peter McCartney","08/31/2020","$172,356.00","","juyeda@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","BIO","1165","9150","$0.00","The millions of species that inhabit the planet all have distinct biological traits that enable them to successfully compete in or adapt to their ecological niches. Determining accurately how these traits evolved is thus fundamental to understanding earth's biodiversity, and to predicting how it might change in the future in response to changes in ecosystems. Although sophisticated analytical methods and tools exist for analyzing traits comparatively, applying their full power to the myriad of trait observations recorded in the form of natural language descriptions has been hindered by the difficulty of allowing these tools to understand even the most basic facts implied by an unstructured free-text statement made by a human observer. The technological arsenal needed to overcome this challenge is now in principle available, thanks to a number of recent breakthroughs in the areas of knowledge representation and machine reasoning, but these technologies are challenging enough to deploy, orchestrate, and use that the barriers to effectively exploit them remains far too high for most tools. This project will create infrastructure that will dramatically reduce this barrier, with the goal of providing comparative trait analysis tools easy access to algorithms powered by machines reasoning with and making inferences from the meaning of trait descriptions. Similar to how Google, IBM Watson, and others have enabled developers of smartphone apps to incorporate, with only a few lines of code, complex machine-learning and artificial intelligence capabilities such as sentiment analysis, this project will demonstrate how easy access to knowledge computing opens up new opportunities for analysis, tools, and research. It will do this by addressing three long-standing limitations in comparative studies of trait evolution: recombining trait data, modeling trait evolution, and generating testable hypotheses for the drivers of trait adaptation.<br/><br/>The treasure trove of morphological data published in the literature holds one of the keys to understanding the biodiversity of phenotypes, but exploiting the data in full through modern computational data science analytics remains severely hampered by the steep barriers to connecting the data with the accumulated body of morphological knowledge in a form that machines can readily act on. This project aims to address this barrier by creating a centralized computational infrastructure that affords comparative analysis tools the ability to compute with morphological knowledge through scalable online application programming interfaces (APIs), enabling developers of comparative analysis tools, and therefore their users, to tap into machine reasoning-powered capabilities and data with machine-actionable semantics. By shifting all the heavy-lifting to this infrastructure, tools can programmatically obtain answers to knowledge-based questions that would otherwise require careful study by a human export, such as objectively and reproducibly assessing the relatedness, independence, and distinctness of characters and character states, with only a few lines of code. To accomplish this, the project will adapt key products and know-how developed by the Phenoscape project, including an integrative knowledgebase of ontology-linked phenotype data, metrics for quantifying the semantic similarity of phenotype descriptions, and algorithms for synthesizing morphological data from published trait descriptions. To drive development of the computational infrastructure and to demonstrate its enabling value, the project's objectives focus on addressing three concrete long-standing needs for which the difficulty of computing with domain knowledge is the major impediment: (1) computationally synthesizing, calibrating, and assessing morphological trait matrices from across studies; (2) objectively and reproducibly incorporating morphological domain knowledge provided by ontologies into evolutionary models of trait evolution; and (3) generating testable hypotheses for adaptive diversification by incorporating semantic phenotypes into ancestral state reconstruction and identifying domain ontology concepts linked to evolutionary changes in a branch or clade more frequently than expected by chance. In addition, to better prepare evolutionary biologist users and developers of comparative analysis tools for adopting these new capabilities, a domain-tailored short-course on requisite knowledge representation and computational inference technologies will be developed and taught. More information on this project can be found at http://cate.phenoscape.org/."
"1661356","Collaborative Research: ABI Innovation: Enabling machine-actionable semantics for comparative analyses of trait evolution","DBI","ADVANCES IN BIO INFORMATICS","09/01/2017","08/30/2017","Todd Vision","NC","University of North Carolina at Chapel Hill","Standard Grant","Peter McCartney","08/31/2020","$880,522.00","","tjv@bio.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","BIO","1165","9150","$0.00","The millions of species that inhabit the planet all have distinct biological traits that enable them to successfully compete in or adapt to their ecological niches. Determining accurately how these traits evolved is thus fundamental to understanding earth's biodiversity, and to predicting how it might change in the future in response to changes in ecosystems. Although sophisticated analytical methods and tools exist for analyzing traits comparatively, applying their full power to the myriad of trait observations recorded in the form of natural language descriptions has been hindered by the difficulty of allowing these tools to understand even the most basic facts implied by an unstructured free-text statement made by a human observer. The technological arsenal needed to overcome this challenge is now in principle available, thanks to a number of recent breakthroughs in the areas of knowledge representation and machine reasoning, but these technologies are challenging enough to deploy, orchestrate, and use that the barriers to effectively exploit them remains far too high for most tools. This project will create infrastructure that will dramatically reduce this barrier, with the goal of providing comparative trait analysis tools easy access to algorithms powered by machines reasoning with and making inferences from the meaning of trait descriptions. Similar to how Google, IBM Watson, and others have enabled developers of smartphone apps to incorporate, with only a few lines of code, complex machine-learning and artificial intelligence capabilities such as sentiment analysis, this project will demonstrate how easy access to knowledge computing opens up new opportunities for analysis, tools, and research. It will do this by addressing three long-standing limitations in comparative studies of trait evolution: recombining trait data, modeling trait evolution, and generating testable hypotheses for the drivers of trait adaptation.<br/><br/>The treasure trove of morphological data published in the literature holds one of the keys to understanding the biodiversity of phenotypes, but exploiting the data in full through modern computational data science analytics remains severely hampered by the steep barriers to connecting the data with the accumulated body of morphological knowledge in a form that machines can readily act on. This project aims to address this barrier by creating a centralized computational infrastructure that affords comparative analysis tools the ability to compute with morphological knowledge through scalable online application programming interfaces (APIs), enabling developers of comparative analysis tools, and therefore their users, to tap into machine reasoning-powered capabilities and data with machine-actionable semantics. By shifting all the heavy-lifting to this infrastructure, tools can programmatically obtain answers to knowledge-based questions that would otherwise require careful study by a human export, such as objectively and reproducibly assessing the relatedness, independence, and distinctness of characters and character states, with only a few lines of code. To accomplish this, the project will adapt key products and know-how developed by the Phenoscape project, including an integrative knowledgebase of ontology-linked phenotype data, metrics for quantifying the semantic similarity of phenotype descriptions, and algorithms for synthesizing morphological data from published trait descriptions. To drive development of the computational infrastructure and to demonstrate its enabling value, the project's objectives focus on addressing three concrete long-standing needs for which the difficulty of computing with domain knowledge is the major impediment: (1) computationally synthesizing, calibrating, and assessing morphological trait matrices from across studies; (2) objectively and reproducibly incorporating morphological domain knowledge provided by ontologies into evolutionary models of trait evolution; and (3) generating testable hypotheses for adaptive diversification by incorporating semantic phenotypes into ancestral state reconstruction and identifying domain ontology concepts linked to evolutionary changes in a branch or clade more frequently than expected by chance. In addition, to better prepare evolutionary biologist users and developers of comparative analysis tools for adopting these new capabilities, a domain-tailored short-course on requisite knowledge representation and computational inference technologies will be developed and taught. More information on this project can be found at http://cate.phenoscape.org/."
"1713952","EAPSI: Learning Semantic Decomposition in Support of Commonsense Reasoning","OISE","EAPSI","06/01/2017","05/10/2017","William Hancock","GA","Hancock                 William","Fellowship Award","Anne Emig","05/31/2018","$5,400.00","","","","Atlanta","GA","303187959","","O/D","7316","5921, 5978, 7316","$0.00","Commonsense reasoning is the ability for humans to use everyday facts about the world to support planning. To create a story about a bank robbery, one must have knowledge of what a bank is, why one would wish to rob it, etc. Artificial intelligence (AI) agents can learn a lot about the world by reading existing text sources, such as Wikipedia or narrative. However, the information learned so far from these sources tends to be explicit, such as, person A was born at location B. This project will investigate methods that can be used to learn implicit knowledge from text such as spatial or temporal properties of concepts. This knowledge can then support many different current challenges in AI, such as narrative generation. This project will be conducted at Toyota Technological University under the mentorship of Dr. Makoto Miwa. The collaboration provides access to unique data that will enable new insights into how humans process everyday concepts.<br/><br/>Current work on information extraction tends to learn propositions that are explicitly encoded in text. Implicit information is equally or more important in giving cognitive agents the ability to reason about the world. Under the supervision of Dr. Miwa, I will apply machine learning and analogical techniques to the task of associative concept learning. I will specifically look at low level qualia related attributes including temporality, space, sentiment, aesthetics, etc. I will use various corpora in support of this task. Dr. Miwa, along with his research group at Toyota Technological University, are leaders in information extraction techniques. Subsequently, Dr. Miwa's insight and expertise will be an invaluable aid in developing techniques for machine learning implicit knowledge.<br/><br/>This award, under the East Asia and Pacific Summer Institutes program, supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science."
"1661456","Collaborative Research: ABI Innovation: Enabling machine-actionable semantics for comparative analyses of trait evolution","DBI","ADVANCES IN BIO INFORMATICS","09/01/2017","08/30/2017","Hilmar Lapp","NC","Duke University","Standard Grant","Peter McCartney","08/31/2021","$569,764.00","","hilmar.lapp@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","BIO","1165","9150","$0.00","The millions of species that inhabit the planet all have distinct biological traits that enable them to successfully compete in or adapt to their ecological niches. Determining accurately how these traits evolved is thus fundamental to understanding earth's biodiversity, and to predicting how it might change in the future in response to changes in ecosystems. Although sophisticated analytical methods and tools exist for analyzing traits comparatively, applying their full power to the myriad of trait observations recorded in the form of natural language descriptions has been hindered by the difficulty of allowing these tools to understand even the most basic facts implied by an unstructured free-text statement made by a human observer. The technological arsenal needed to overcome this challenge is now in principle available, thanks to a number of recent breakthroughs in the areas of knowledge representation and machine reasoning, but these technologies are challenging enough to deploy, orchestrate, and use that the barriers to effectively exploit them remains far too high for most tools. This project will create infrastructure that will dramatically reduce this barrier, with the goal of providing comparative trait analysis tools easy access to algorithms powered by machines reasoning with and making inferences from the meaning of trait descriptions. Similar to how Google, IBM Watson, and others have enabled developers of smartphone apps to incorporate, with only a few lines of code, complex machine-learning and artificial intelligence capabilities such as sentiment analysis, this project will demonstrate how easy access to knowledge computing opens up new opportunities for analysis, tools, and research. It will do this by addressing three long-standing limitations in comparative studies of trait evolution: recombining trait data, modeling trait evolution, and generating testable hypotheses for the drivers of trait adaptation.<br/><br/>The treasure trove of morphological data published in the literature holds one of the keys to understanding the biodiversity of phenotypes, but exploiting the data in full through modern computational data science analytics remains severely hampered by the steep barriers to connecting the data with the accumulated body of morphological knowledge in a form that machines can readily act on. This project aims to address this barrier by creating a centralized computational infrastructure that affords comparative analysis tools the ability to compute with morphological knowledge through scalable online application programming interfaces (APIs), enabling developers of comparative analysis tools, and therefore their users, to tap into machine reasoning-powered capabilities and data with machine-actionable semantics. By shifting all the heavy-lifting to this infrastructure, tools can programmatically obtain answers to knowledge-based questions that would otherwise require careful study by a human export, such as objectively and reproducibly assessing the relatedness, independence, and distinctness of characters and character states, with only a few lines of code. To accomplish this, the project will adapt key products and know-how developed by the Phenoscape project, including an integrative knowledgebase of ontology-linked phenotype data, metrics for quantifying the semantic similarity of phenotype descriptions, and algorithms for synthesizing morphological data from published trait descriptions. To drive development of the computational infrastructure and to demonstrate its enabling value, the project's objectives focus on addressing three concrete long-standing needs for which the difficulty of computing with domain knowledge is the major impediment: (1) computationally synthesizing, calibrating, and assessing morphological trait matrices from across studies; (2) objectively and reproducibly incorporating morphological domain knowledge provided by ontologies into evolutionary models of trait evolution; and (3) generating testable hypotheses for adaptive diversification by incorporating semantic phenotypes into ancestral state reconstruction and identifying domain ontology concepts linked to evolutionary changes in a branch or clade more frequently than expected by chance. In addition, to better prepare evolutionary biologist users and developers of comparative analysis tools for adopting these new capabilities, a domain-tailored short-course on requisite knowledge representation and computational inference technologies will be developed and taught. More information on this project can be found at http://cate.phenoscape.org/."
"1661529","Collaborative Research: ABI Innovation: Enabling machine-actionable semantics for comparative analyses of trait evolution","DBI","ADVANCES IN BIO INFORMATICS","09/01/2017","01/26/2018","Wasila Dahdul","SD","University of South Dakota Main Campus","Standard Grant","Peter McCartney","08/31/2020","$336,493.00","Paula Mabee, Wasila Dahdul","wdahdul@uci.edu","414 E CLARK ST","vermillion","SD","570692307","6056775370","BIO","1165","9150","$0.00","The millions of species that inhabit the planet all have distinct biological traits that enable them to successfully compete in or adapt to their ecological niches. Determining accurately how these traits evolved is thus fundamental to understanding earth's biodiversity, and to predicting how it might change in the future in response to changes in ecosystems. Although sophisticated analytical methods and tools exist for analyzing traits comparatively, applying their full power to the myriad of trait observations recorded in the form of natural language descriptions has been hindered by the difficulty of allowing these tools to understand even the most basic facts implied by an unstructured free-text statement made by a human observer. The technological arsenal needed to overcome this challenge is now in principle available, thanks to a number of recent breakthroughs in the areas of knowledge representation and machine reasoning, but these technologies are challenging enough to deploy, orchestrate, and use that the barriers to effectively exploit them remains far too high for most tools. This project will create infrastructure that will dramatically reduce this barrier, with the goal of providing comparative trait analysis tools easy access to algorithms powered by machines reasoning with and making inferences from the meaning of trait descriptions. Similar to how Google, IBM Watson, and others have enabled developers of smartphone apps to incorporate, with only a few lines of code, complex machine-learning and artificial intelligence capabilities such as sentiment analysis, this project will demonstrate how easy access to knowledge computing opens up new opportunities for analysis, tools, and research. It will do this by addressing three long-standing limitations in comparative studies of trait evolution: recombining trait data, modeling trait evolution, and generating testable hypotheses for the drivers of trait adaptation.<br/><br/>The treasure trove of morphological data published in the literature holds one of the keys to understanding the biodiversity of phenotypes, but exploiting the data in full through modern computational data science analytics remains severely hampered by the steep barriers to connecting the data with the accumulated body of morphological knowledge in a form that machines can readily act on. This project aims to address this barrier by creating a centralized computational infrastructure that affords comparative analysis tools the ability to compute with morphological knowledge through scalable online application programming interfaces (APIs), enabling developers of comparative analysis tools, and therefore their users, to tap into machine reasoning-powered capabilities and data with machine-actionable semantics. By shifting all the heavy-lifting to this infrastructure, tools can programmatically obtain answers to knowledge-based questions that would otherwise require careful study by a human export, such as objectively and reproducibly assessing the relatedness, independence, and distinctness of characters and character states, with only a few lines of code. To accomplish this, the project will adapt key products and know-how developed by the Phenoscape project, including an integrative knowledgebase of ontology-linked phenotype data, metrics for quantifying the semantic similarity of phenotype descriptions, and algorithms for synthesizing morphological data from published trait descriptions. To drive development of the computational infrastructure and to demonstrate its enabling value, the project's objectives focus on addressing three concrete long-standing needs for which the difficulty of computing with domain knowledge is the major impediment: (1) computationally synthesizing, calibrating, and assessing morphological trait matrices from across studies; (2) objectively and reproducibly incorporating morphological domain knowledge provided by ontologies into evolutionary models of trait evolution; and (3) generating testable hypotheses for adaptive diversification by incorporating semantic phenotypes into ancestral state reconstruction and identifying domain ontology concepts linked to evolutionary changes in a branch or clade more frequently than expected by chance. In addition, to better prepare evolutionary biologist users and developers of comparative analysis tools for adopting these new capabilities, a domain-tailored short-course on requisite knowledge representation and computational inference technologies will be developed and taught. More information on this project can be found at http://cate.phenoscape.org/."
"1714779","AF: Small: Lower Bounds for Computational Models, and Relations to Other Topics in Computational Complexity","CCF","Algorithmic Foundations","09/01/2017","07/10/2017","Ran Raz","NJ","Princeton University","Standard Grant","Joseph Maurice Rojas","08/31/2021","$450,000.00","","ranr@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796","7923, 7927","$0.00","Computational complexity theory is a mathematical field that studies the limits of computers and the resources needed to perform computational tasks. A mathematical theory of computation is crucial in our information age, where computers are involved in essentially every part of our life. Computational complexity is also essential in designing efficient communication protocols, secure cryptographic protocols and in understanding human and machine learning. Studying the limits of computational models is among the most exciting, most challenging, and most important topics in theoretical computer science and is essential for understanding the power of computation and for the development of a theory of computation.<br/><br/>The project will study lower bounds for the resources required by different computational models, as well as related topics in computational complexity. The project will focus on three main research directions:<br/><br/>Time-Space lower bounds for learning: In a sequence of recent works, the PI and his coauthors proved that some of the most extensively studied learning problems require either a super-linear memory size or a super-polynomial number of samples. The project will further study memory/samples lower bounds for learning and their relations to other topics in complexity theory. Lower bounds for learning under memory constraints demonstrate the importance of memory in learning and cognitive processes. They may be relevant to understanding human learning and may have impact on machine learning, artificial intelligence and optimization. They also have applications in cryptography.<br/><br/>Lower bounds for arithmetic circuits: The project will study lower bounds for arithmetic circuits and formulas, as well as for subclasses of arithmetic circuits and formulas. Lower bounds for arithmetic circuits may have a broader impact within theoretical computer science, because of the centrality of polynomials in theoretical computer science.<br/><br/>Lower bounds for communication complexity: In a sequence of recent works, the PI and his coauthors proved the first gaps between information complexity and communication complexity. These results show that compression of interactive communication protocols to their information content is not possible, and hence show that interactive analogs to Shannon's source coding theorem and Huffman coding are not possible. Separation results of communication complexity and information complexity may be relevant to electrical engineering and in particular to the design of efficient communication protocols. The project will further study these topics, and more generally, lower bounds for communication complexity and their relations to other topics in computational complexity."
"1721381","SBIR Phase I:  Fast Creation of Photorealistic 3D Models using Consumer Hardware","IIP","SMALL BUSINESS PHASE I","07/01/2017","06/28/2017","Jeevan Kalanithi","CA","Openspace","Standard Grant","Peter Atherton","06/30/2018","$225,000.00","","zoinks@gmail.com","3802 23rd St","San Francisco","CA","941143321","4159947035","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be large: a successful project would transform the construction industry, making it far more efficient by reducing legal conflicts, schedule slips and poor decision making; the project has the potential to make real estate sales and marketing more efficient by allowing buyers and sellers to accurately represent properties online, reducing the need for on-site visits. The proposed work will enable the fast and easy creation of 100% complete visual documentation of a physical space; this documentation can be generated many times throughout the course of construction. In so doing, the proposed project will allow professionals in the construction industry to track progress and communicate with their teams far more efficiently than ever before. A second exciting effect of the proposal will be the creation of vast, detailed, never before seen datasets of construction projects and real estate, allowing technical innovations in artificial intelligence and computer vision to impact one of the largest industries in the nation and the world. For example, systems could be trained to automatically spot safety concerns, augmenting the efforts of safety managers and keeping workers safer than ever before.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will develop a fast, easy to use and cheap method to create photorealistic 3D models using off the shelf consumer hardware. Technical hurdles include validating the quality and efficacy of models generated with consumer hardware, near instantaneous creation of 3D models on device, and automatic creation of routes through the 3D space without human annotation. With these hurdles cleared, advanced work might include automated analytics between and among 3D models of the same site captured over time. Because of the system's ease of use, it will enable the collection of large, totally novel datasets. The goal of the research is to produce a prototype that a layperson can use to create a 3D model of a physical site in order to document it. The plan to reach these goals includes iterative software development against the hurdles listed above, as well as continuous user feedback to guide and refine development."
"1718538","CSR: Small: Evolution of Computer Vision for Low Power Devices, Breaking its Power Wall and Computational Complexity","CNS","CSR-Computer Systems Research, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2017","08/16/2017","Avesta Sasan","VA","George Mason University","Standard Grant","Matt Mutka","09/30/2021","$499,792.00","Houman Homayoun","asasan@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7354, 7359","7923","$0.00","The accuracy of computer vision for object recognition and classification has surpassed human capabilities. Adoption of brain-inspired Convolutional Neural Network (CNN) models and the ability to train and execute these complex networks by modern graphical processing units (GPUs) are the backbone of this progress. However, in terms of computational requirement, memory usage, and power consumption, the CNN solutions are extremely demanding. Meanwhile, many interesting applications of computer vision - such as small robotics, a wide range of Cyber-Physical Systems, and many smart devices on the Internet of Things - are resource constrained. This project aims to substantially lower the computational complexity, the average-case classification power and the latency of CNN-based vision, enabling its deployment to a much wider range of platforms. From a societal viewpoint, this study enhances the research, education, and diversity at George Mason University (GMU) by involving graduate, undergraduate, minority and female students, and enriches several courses that are offered at GMU.<br/><br/>The goals of this research project are as follows: (1) Reformulating the CNN-based learning model into an Iterative Convolutional Neural Network (ICNN) learning model that allows early classification and permits early termination via various thresholding mechanisms and developing a framework to use the contextual knowledge that could be extracted from earlier iterations to guide and reduce the computation of future iterations. (2) Developing an approximate ICNN coprocessor that supports approximation in memory and logic by exploring new approximation opportunities created by ICNN, and enhancing the ICNN to adjust and learn the approximate hardware behavior in addition to its intended functionality."
"1723175","Computational Methods for Hierarchical Manifold Learning","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, CDS&E-MSS","08/01/2017","06/13/2017","Timothy Sauer","VA","George Mason University","Standard Grant","Christopher Stark","07/31/2021","$329,954.00","","tsauer@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1266, 1271, 8069","8083, 9263","$0.00","The enormous practical success of deep learning warrants a clear and concise mathematical explanation. Although the components of a neural network, and the rules for propagation of information through the layers, are extremely simple, there is to date a lack of a corresponding deep understanding of the roles of the various mechanisms involved. Second, there is a lack of transparency: While a given set of network weights may fit scientific or engineering data in-sample and even generalize well out-of-sample, using a neural net to explain the data or the system producing the data is usually very difficult or impossible.  In this project, the PI will leverage recent progress in mathematical methods from applied and computational harmonic analysis to develop a hierarchical algorithm, based on manifold learning, to replicate the strikingly successful properties of deep learning while adding improved accuracy, adaptability to data, smoothness priors, and transparency.<br/><br/>A sequence of computational projects is planned to develop a hierarchical algorithm for deep learning, based on representing manifolds by the Laplace-Beltrami operator. Proposed work supports the construction of a complete algorithm that uses layers of manifold learning kernel methods to represent data in a deep manifold learning infrastructure.  The development of the learning algorithm consists of three parts: (1) the construction of a hierarchical manifold learning architecture, using eigenfunctions of the Laplace-Beltrami operator to represent data, and replicating the sharing and pooling features of neural networks, (2) building innovative algorithms for the purpose of optimizing the solution to identification problems, and (3) development of resampling methods to handle large data sets."
"1651538","CAREER: Evaluating the Process-Structure-Property Relationships of Carbon Nanotube Forests with In-Situ Synthesis Observation and Dynamic Simulations","CMMI","CAREER: FACULTY EARLY CAR DEV, Materials Eng. & Processing","07/15/2017","03/10/2017","Matthew Maschmann","MO","University of Missouri-Columbia","Standard Grant","Thomas F. Kuech","06/30/2022","$500,000.00","","MaschmannM@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","ENG","1045, 8092","1045, 8021, 8025, 9150","$0.00","This Faculty Early Career Development (CAREER) Program research project investigates how carbon nanotubes interact with one another during collective growth by using direct observation and complimentary numerical simulation. Nanoscale materials such as carbon nanotubes offer superior mechanical, electrical, and thermal properties relative to many other conventional engineering materials. When vast arrays of carbon nanotubes are synthesized together they form interconnected and self-organized populations known as carbon nanotube forests. The interactions between growing nanotubes lead to the structures bending and kinking which detract from their mechanical properties.  Numerical simulations employing artificial intelligence and machine learning will facilitate the rapid exploration of high-dimensional processing space associated with nanotube formation and will guide the experimental aspects of this project. The results of this work and the understanding of the synthesis process will help control the mechanisms of carbon nanotube interactions during their synthesis, leading to enhanced and engineered carbon nanotube forest properties. Middle school students will be engaged with the research through a one-week summer camp in which hands-on nanoscale materials engineering will be merged with artistic expression through collaboration with the University of Missouri Museum of Art and Archeology. The results of these investigations will enable the use of the art and observation at the nanoscale to engage and inspire grade school children in STEM-based learning in cooperation with the university art museum.<br/><br/>The forces and mechanisms that drive carbon nanotube forest self-assembly are currently poorly understood, in part because in-situ diagnostic techniques with sufficient resolution to interrogate the evolving mechanical interactions are lagging. This project will implement direct visualization of carbon nanotube forest growth and assembly using in-situ synthesis methods within scanning and transmission electron microscopes. A variety of synthesis conditions will be investigated to link the role of important processing factors to the observed behaviors. Transmission electron microscope techniques will facilitate not only time-resolved visual inspection of individual nanotubes and their host catalyst nanoparticles but also compositional analyses at or near atomic resolution. Scanning electron microscope techniques will examine coordinated assembly of carbon nanotubes within the larger population at length scale ranging from tens of nanometers of tens of microns. Experimental observations will be input into a time-resolved finite element simulation to interrogate the forces generated during nanotube forest assembly. After synthesis, the mechanical properties of carbon nanotube forests will be measured using in-situ scanning electron microscope compression and simulated with the finite element model for validation. The validated model will serve as a vehicle for rapid assessment of process-structure-property relationships using an artificial intelligence algorithm to autonomously search for appropriate synthesis conditions that satisfy user-defined forest property sets. A carbon nanotube forest synthesis simulation tool will be made publically available at nanohub.org to facilitate broader impact of the simulations developed in this project."
"1744192","I-Corps: Dexterous Robotic Prosthetic Control Using Deep Learning Pattern Prediction from Ultrasound Signal","IIP","I-Corps","07/01/2017","07/12/2017","Gil Weinberg","GA","Georgia Tech Research Corporation","Standard Grant","Anita La Salle","07/31/2018","$50,000.00","","gil.weinberg@coa.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project lies in the development a novel system that would allow people with transradial and partial hand amputations to gain unparalleled precise individuation of prosthetic digit motion including continuous and simultaneous movement for individual digits without requiring long and complicated training process. To allow for such functionality a novel set of deep learning algorithms are designed to model muscle movement patterns from ultrasound images. The network is pre-trained with a large amount of data in an effort to minimize later individual training. In addition to power prosthetics, the proposed technology can provide broad impacts in other markets where easy-to-use and accurate gestural control of robotics and/or digital environment are required. These include tele-robotics, exoskeleton operation, virtual reality, gaming, glove boxes as well as work related Personal Protective Equipment, and Performance Augmentation and Amplification Devices.<br/><br/>This I-Corps project will develop and utilize a novel ultrasound sensor and novel deep learning algorithms to recognize continuous muscle activity patterns that can predict accurate and dexterous finger motion. Current myoelectric powered prostheses use discrete classifiers that can only predict a limited number of discrete gestures from noisy electromyography (EMG) signal. Feeding deep learning architectures, such as Convolutional Neural Networks, with rich and detailed ultrasound signal promises to allow for the modeling and prediction of detailed continuous and simultaneous muscle movements patterns, which can be mapped to control continuous and simultaneous movements of individual prosthetic fingers. An additional intellectual of this project merit is the pre-training of these deep neural network with a large amount of data, which would allow for short fine tuning training for individual users, allowing for wide and easy adoption of the technology. The proposed project could therefore allow amputees and people with upper body disabilities to perform finger-by-finger movement activities such as fine object manipulation, typing or playing a musical instrument."
"1723440","SaTC: EDU: Learning Moving Target Defense Concepts: Teaching and Training Curricula Development Based on Software Defined Networking and Network Function Virtualization","DGE","Secure &Trustworthy Cyberspace","09/01/2017","08/11/2017","Dijiang Huang","AZ","Arizona State University","Standard Grant","Victor Piotrowski","08/31/2021","$299,756.00","","dijiang@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","EHR","8060","7254, 7434, 9178, 9179, SMET","$0.00","Moving Target Defense (MTD) is a new security concept to increase uncertainty and complexity for attackers, reduce their window of opportunity and increase the costs of their attack efforts. MTD solutions involve a wide-range of advanced technical expertise, which current education models lack. The project from Arizona State University proposes to develop an MTD courseware for both senior undergraduate level and graduate level in computer science focusing on network-based MTD technologies. Additionally, a cloud-based hands-on laboratory will be established to support MTD labs, which will increase access to students and educators in lab environments with limited computer networking and system security capabilities. The proposed MTD curricula will be published as a textbook associating with a MTD lab repository allowing instructors to build a new MTD course or pick part of the teaching materials to incorporate them into their current curricula. The project team will also work with industry partners to develop the course content and provide a Teacher Training workshop to disseminate the course materials. All the created course content and labs will be freely downloadable by instructors and students.<br/><br/>MTD solutions involve technologies in machine learning, artificial intelligence, and big data analysis models, and current education resources in this realm should address these fields. The proposed project will address the research challenge on how to build-out a much-needed hands-on learning module that can be easily deployed on existing cloud service platforms to learn and experiment with new computer networking and security technologies such as Software Defined Networking (SDN), Network Function Virtualization (NFV), and MTD. This project will focus on four interdependent tasks: (a) building the MTD education capability by creating software components to support SDN, NFV, and MTD, and establishing MTD lab repository and APIs to enhance MTD learning outcomes; (b) developing both long-term semester-based course curriculum, and short-term training oriented MTD teaching contents; (c) conducting a comprehensive evaluation plan by involving both internal and external independent evaluators to use the developed course content and labs; and (d) working with industry partners and hosting a Teacher Training workshop to maximally disseminate developed MTD learning contents and hands-on exercises."
"1722432","SBIR Phase I:  Compact, Low-cost, Automated 3D Ultrasound System for Regular and Accessible Breast Imaging","IIP","SBIR Phase I","07/01/2017","08/27/2018","Maryam Ziaei-Moayyed","CA","iSono Health, Inc.","Standard Grant","Henry Ahn","03/31/2019","$225,000.00","","maryam@isonohealth.com","177 Townsend St.","San Francisco","CA","941075910","5105411320","ENG","5371","5371, 8038","$0.00","This SBIR Phase I project introduces a new paradigm for early monitoring and detection of breast cancer: the Quantified Self Exam. In the United States over 300,000 women are diagnosed and 40,000 women die from breast cancer every year. Breast cancer has a 99% survival rate if detected early, but limitations in cost, sensitivity, accessibility, and convenience of existing screening technologies result in one third of breasts cancers getting missed at early stages. Since treatment for early stage cancer is an order of magnitude less costly than treatment for stage 3 and 4 cancers, there is a clear economic and societal benefit for the development of better breast cancer monitoring and screening tools. To address this challenge, the technology proposed in this project leverages the proven benefits of ultrasound imaging and the newfound power of cloud-based artificial intelligence to provide a regular and accessible self-monitoring tool that can quantify and track suspicious changes in breast tissue. The device portability, low cost, 2 minute scan time, and automated analysis of breast image data will greatly increase the accessibility of breast cancer monitoring for women, which in turn stands to decrease the cost burden of this disease for the US healthcare system. <br/><br/>This SBIR Phase I project proposes to develop a new tool for early detection and monitoring of breast cancer: the Quantified Self Exam (QSE) that combines a low-cost, compact 3D ultrasound device and positioning accessory with artificial intelligence to empower women and their physicians with appropriate and actionable data.  The QSE technology proposed in this project operates independent of user skill and captures 3D volumetric images of the whole breast in 2 minutes.  The system architecture allows for simplified and low-cost ultrasound hardware that connects wirelessly to a smart phone/tablet and transfers captured data to secure cloud for advanced image processing and storage. The ultrasound scanner attaches to a positioning accessory for repeatable imaging that enables longitudinal 3D monitoring of abnormal growth using machine learning-based image analysis. The proposed Phase I R&D efforts focus on four objectives: (i) optimize electrical hardware and low-level imaging software for spatial resolution and image quality; (ii) build a QSE scanner that maximizes field of view and volumetric integrity; (iii) build a positioning accessory for positioning of the QSE scanner; (iv) demonstrate the longitudinal repeatability of QSE imaging by validating the alignment of 3D ultrasound volumes on a breast phantom."
"1759592","I-Corps: Embedded Machine Listening for Smart Acoustic Monitoring","IIP","I-Corps","11/01/2017","11/01/2017","Juan Bello","NY","New York University","Standard Grant","Andre Marshall","12/31/2019","$50,000.00","","jpbello@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is the use of embedded machine listening as a low-cost, turnkey solution for early detection of machinery malfunction and improve predictive maintenance. In manufacturing, sound-based condition monitoring coupled with data-driven maintenance can help significantly reduce unscheduled work stoppages, faulty products and waste of raw materials. Building management systems can be augmented by integrating real-time condition updates for critical machinery such as HVAC units, elevators, boilers and pumps, minimizing disruption for managers and users of those services. This technology is flexible, accurate and data-driven, potentially providing a low barrier to adoption for prospective customers and adaptability to various markets. Beyond predictive maintenance, applications include noise level monitoring for ensuring compliance in workplaces and airports, home and building security, early alert for traffic accidents, bio-acoustic monitoring of animal species, and outdoor noise monitoring at scale for improved enforcement in smart cities.<br/><br/>This I-Corps project further develops research at the intersection of artificial intelligence and the internet of things. The technology consists of a calibrated and highly accurate acoustic sensor with embedded sound recognition AI based on deep learning.  Sound conveys critical information about the environment that often cannot be measured by other means. In manufacturing, early stage machinery malfunction can be indicated by abnormal acoustic emissions. In smart homes and buildings, sound can be monitored for signs of alarm, distress or compliance.  Sound sensing is omnidirectional and robust to occlusion and contextual variables such as lightning conditions at different times of the day. Many existing solutions cannot identify different types of sounds or complex acoustic patterns, making them unsuited for these applications. This solution is both low cost and capable of identifying events and sources at the network edge, thus eliminating the need for sensitive audio information to be transmitted."
"1717084","III: Small: Learning Latent Representations of Heterogeneous Information Networks","IIS","Info Integration & Informatics","08/01/2017","07/27/2017","Wang-Chien Lee","PA","Pennsylvania State Univ University Park","Standard Grant","Maria Zemankova","07/31/2021","$499,635.00","Zhen Lei","wlee@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7364","7364, 7923","$0.00","Feature engineering is an important pre-processing step in applying machine learning algorithms for knowledge discovery in all fields of scientific research and business applications. In these applications it is crucial to obtain appropriate features that best describe the observed phenomena. Traditionally, researchers often manually decide features of interest based on the knowledge and experiences of domain experts, which is costly and labor-intensive. Recently, a new line of research, called representation learning, has used neural networks to automatically learn features that may be used in various scientific research projects and business applications. The PI plans new representation learning methods to capture rich, meaningful and discriminative features in heterogeneous information networks (HINs), which have been used to model heterogeneous types of network entities and their relationships in support of network data analysis and mining. The work planned in this project includes information about model design, scalability, sample data extraction, network variety and data heterogeneity issues in the implementation of the learning frameworks. This research will be integrated into graduate and undergraduate courses of data mining and machine learning, enabling students to develop analytics and big data skills.<br/><br/>The specific research objectives of this project are three-fold:  1) The PI aims to leverage information in HINs to learn representations of latent features for nodes and relationships specified by meta-paths in the network. Novel techniques will be developed to address the scalability issues in learning. 2) The PI seeks to address model design and learning issues arising in HINs growing with time, e.g., citation networks. New neural network architectures and new sample data extraction schemes will be devised. 3) The PI plans to integrate both content and network structures in representation learning of HINs. New neural network architectures will be devised. To evaluate research prototypes, the PI will develop a testbed consisting of new neural network frameworks for representation learning on HINs. Techniques and software will be made available as research resources to the communities of data mining and representation learning."
"1664172","SI2-SSI: LIMPID: Large-Scale IMage Processing Infrastructure Development","OAC","DMR SHORT TERM SUPPORT, CYBERINFRASTRUCTURE, Software Institutes","10/01/2017","09/08/2019","Bangalore Manjunath","CA","University of California-Santa Barbara","Standard Grant","Robert Beverly","09/30/2022","$3,436,422.00","Tresa Pollock, Amit Roy Chowdhury, Nirav Merchant, Robert Miller","manj@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","1712, 7231, 8004","026Z, 054Z, 075Z, 7433, 8004, 8009, 9216, 9251","$0.00","Scientific imaging is ubiquitous: From materials science, biology, neuroscience and brain connectomics, marine science and remote sensing, to medicine, much of the big data science is image centric. Currently, interpretation of images is usually performed within isolated research groups either manually or as workflows over narrowly defined conditions with specific datasets. This LIMPID (Large-scale IMage Processing Infrastructure Development) project will have a transformative impact on such discipline-centric workflows through the creation of an extensive and unique resource for the curation, distribution and sharing of scientific image analysis methods. The project will create an image processing marketplace for use by a diverse community of researchers, enabling them to discover, test, verify and refine image analysis methods within a shared infrastructure. As a freely available, cloud-based resource, LIMPID will facilitate participation of underrepresented groups and minority-serving institutions, as well as international scientists, allowing them to address questions that would otherwise require expensive software. The potential impacts of the project are significant: from wide dissemination of novel processing methods, to development of automatic methods that can leverage data and human feedback from large datasets for software training and validation. For the broader scientific community, this immediately provides a resource for joint data and methods publication, with provenance control and security. This in turn will facilitate faster development and deployment of tools and foster new collaborations between computer scientists developing methods and scientific users. The project will prepare a diverse cadre of students and researchers, including women and members of under-represented groups, to tackle complex problems in an interdisciplinary environment. Through workshops, participation at scientific meetings, and summer undergraduate research internships, a broad community of users will be engaged to actively contribute to all aspects of research, development, and training during the course of this project. <br/>    <br/><br/>The primary goal is to create a large scale distributed image processing infrastructure, the LIMPID, though a broad,  interdisciplinary collaboration of researchers in databases, image analysis, and sciences.  In order to create a resource of broad appeal, the focus will be on three types of image processing: simple detection and labelling of objects based on detection of significant features and leveraging recent advances in deep learning, semi-custom pipelines and workflows based on popular image processing tools, and finally fully customizable analysis routines.  Popular image processing pipeline tools will be leveraged to allow users to create or customize existing pipeline workflows and easily test these on large-scale cloud infrastructure from their desktop or mobile devices. In addition, a core cloud-based platform will be created where custom image processing can be created, shared, modified, and executed on large-scale datasets and apply novel methods to minimize data movement. Usage test cases will be created for three specific user communities: materials science, marine science and neuroscience. An industry supported consortium will be established at the beginning of the project towards achieving long-term sustainability of the LIMPID infrastructure.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate for Mathematical and Physical Sciences."
"1652655","CAREER:  A Dynamic Program Monitoring Framework Using Neural Network Hardware","CCF","Software & Hardware Foundation","04/15/2017","04/03/2019","Abdullah Muzahid","TX","University of Texas at San Antonio","Continuing Grant","Yuanyuan Yang","09/30/2019","$264,680.00","","abdullah.muzahid@tamu.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","7798","1045, 7941","$0.00","Software bugs and security attacks cripple US economy by costing more than $150 billion a year. However, there has been no major innovation in this context. This research project aims to change that fact with the help of neural network based hardware. If the project is successful, it will significantly affect current industry <br/>practices and spur a new trend. It will encourage companies to invest in new techniques for debugging and security attack analysis using neural network hardware and make a compelling use case for the hardware implementation, thereby influencing continuous investment in neural network hardware. In addition, the project will contribute to the research and educational activities of a minority serving institution. Students will be tightly integrated into the project through dissertation, thesis work, and undergraduate research work. The PI will incorporate emerging architecture design and its programming in undergraduate and graduate coursework. Moreover, the PI will involve local high school students in computer science related projects through summer internships.<br/><br/>Neural network is a machine learning technique that mimics human brain. Therefore, neural network hardware provides some unique capabilities that can be utilized in many different ways. This project proposes to utilize neural network hardware for ""program monitoring"". Program execution monitoring is often used to detect software bugs, performance issues, security attacks etc. Neural network hardware will learn the normal ""behavior"" of the program. Then it will detect any deviation of such behavior. Such deviation can be attributed to software bugs, performance issues or security attacks. The proposed approach provides a general framework for handling these issues. Due to online learning and testing capability of neural network hardware, the framework will be adaptive to any change in program inputs, code, and platforms."
"1733686","AitF: Collaborative Research: Efficient High-Dimensional Integration using Error-Correcting Codes","CCF","Algorithms in the Field","09/01/2017","08/11/2017","Stefano Ermon","CA","Stanford University","Standard Grant","Tracy Kimbrel","08/31/2021","$360,000.00","","ermon@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7239","","$0.00","Efficiently estimating integrals of high-dimensional functions is a fundamental and largely unsolved computational problem, manifesting in scientific areas from biology and physics to economics. In particular, in Artificial Intelligence and Machine Learning, a wide array of methods are computationally limited precisely because they require the computation of high-dimensional integrals. While computing such integrals exactly is highly intractable, approximations suffice for many applications. Currently, approximation is attempted using two main classes of algorithms: Markov Chain Monte Carlo (MCMC) sampling methods and variational inference techniques. The former are asymptotically accurate, but their computational budget is inflexible and often prohibitive. The latter have manageable computational budget, but typically come with no accuracy guarantees. This project will investigate a new family of computationally efficient approximation methods which reduce the task of integration to the much better studied task of optimization, thus leveraging decades of research and engineering in combinatorial optimization methods and technology. A key goal of the project is to develop an open-source software library of efficient tools for high-dimensional integration.<br/><br/>The reduction of integration to optimization builds on the probabilistic reduction of decision problems to uniqueness promise problems developed in the mid-80s. Specifically, the idea is to use systems of random parity equations in order to specify random subsets of the function's domain, and relate integration to the task of optimization over these subsets. In general, the capacity for efficient optimization fundamentally stems from the capacity to summarily dispense large parts of the domain as uninteresting. The key question to be addressed by the project is whether it is possible to define random subsets over which optimization is both tractable and informative for integration. To that end, the project will employ random systems of linear equations corresponding to Low Density Parity Check (LDPC) matrices for error-correcting codes. The energy landscape, i.e., the number of violated equations, of such systems is far smoother than that of the generic (dense) random systems of linear equations that underlie the original mid-80s technique, thus being far more amenable to optimization. The project will also build upon the deep understanding gained in the last two decades for LDPC codes in the field of communications, with the goal of integrating a priori knowledge about the energy landscape in the optimization strategy. This will provide a fundamentally new use for error-correcting codes, creating a bridge between the areas of optimization and information theory."
"1744294","I-Corps: Data Analytics and Automated Candidate Assessment","IIP","I-Corps","07/01/2017","07/12/2017","Saman Aliari Zonouz","NJ","Rutgers University New Brunswick","Standard Grant","Andre Marshall","12/31/2019","$50,000.00","","saman.zonouz@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","ENG","8023","","$0.00","The broader impact and commercial potential of this I-Corps project will improve the transition quality and efficiency of university students to companies. This platform will potentially provide several customer segments such as students, companies and universities with solutions that remove many barriers that currently make job hunting and hiring a time-consuming, costly, stressful, and often biased endeavor. The impact of the product goes beyond a specific academic major, and has the potential to cover all scientific subject matters for which there is a robust job market. Most importantly, the solution will remove the current potential biases against underrepresented minorities by automating the skill assessment process and minimizing the human involvement in the process.<br/><br/>This I-Corps project will develop effective, unbiased, and automated platforms and algorithms for skill assessment. The target customers will be the companies, and university graduates that would like to enter the job market with no prior experience. This research will propose novel techniques and working tools using artificial intelligence and machine learning methods to provide interaction during the interview process between the assessment engine and the interviewee. The project further develops novel formal methods and programming language analysis techniques to analyze the answers submitted by the candidate during the interview and adaptively select the next sequence of questions for each specific candidate. A realistic test-bed infrastructures on which the candidates will be asked to perform experiments is used to assess the expertise level of the candidates.  These novel techniques for automated interviews will reduce the cost and time for both companies and students."
"1662731","Collaborative Research: Computational Design of Metal-Organic Framework Materials","CMMI","DEMS-Design Eng Material Syst","06/15/2017","06/13/2017","Matthew Campbell","OR","Oregon State University","Standard Grant","Kathryn Jablokow","05/31/2021","$274,135.00","","matt.campbell@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","ENG","8086","024E, 067E, 073E","$0.00","This award supports research in computational methods for the automatic exploration of the search space of metal-organic frameworks to obtain desired combinations of mechanical, thermal and chemical properties. A metal-organic framework is a repeating three-dimensional crystal lattice with a large open space-frame structure composed of organic linking molecules bonded to inorganic nodal units. The interconnected pore-spaces in these materials endow them with a rich variety of unusual properties that can be exploited for gas storage, gas separation, and catalysis. Given the constraints of chemical bonds and bond angles, one cannot easily customize the lattice for a particular size and shape. Thankfully there is an astronomically large number of permutations for combining various atomic elements together. However, human designers are confounded on how to find one for their particular problem domain. What is needed is a computational process to search the space for a best solution. By formalizing the molecular design as a decision tree, the developed computer algorithms will invent new materials whose functional behavior is defined by its chemical makeup and the resulting geometry and movement of the lattice structure. As part of testing the design approach, the project will address two technologically important problems:  designing new metal organic frameworks optimized for gas storage, and materials for separating isomers in industrially important chemical feed stocks. STEM outreach activities to high school students and teachers will also be performed.<br/><br/>Specifically, the project seeks materials that exhibit highly chemically selective adsorption or permeability of gases through mechanisms that arise from chemical, steric, and vibrational behavior of the frameworks. The computational search for this incorporates a unique graph transformation approach that mimics correct stoichiometric reactions and leads to a large search tree that is amenable to recent advances in artificial intelligence planning algorithms. Furthermore, machine learning methods will establish a link between the structure and function of organic frameworks by leveraging data from complex molecular simulations. This will lead to more efficient search of the decision tree so that meaningful results can be obtained. Through detailed simulations of the resulting metal organic frameworks, the researchers will publish how their new materials can be used to tackle challenging problems in energy storage, high-tech manufacturing, and the creation of new sensitive sensor equipment."
"1748582","NRI: Collaborative Research: Learning Deep Sensorimotor Policies for Shared Autonomy","IIS","NRI-National Robotics Initiati","06/01/2017","09/15/2017","Siddhartha Srinivasa","WA","University of Washington","Standard Grant","Roger Mailler","08/31/2020","$453,379.00","","siddhartha.srinivasa@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","7495, 8086","$0.00","Assistive robots have the potential to transform the lives of persons with upper extremity disabilities, by helping them perform basic daily activities, such as manipulating objects and feeding. However, human control of assistive robots presents substantial challenges. The high dimensionality of robotic arms means that joystick-like interfaces are unnatural hard to use intuitively, and motions resulting from direct teleoperation are often slow, imprecise, and severely limited in their dexterity. This research address these challenges by developing learning algorithms for shared autonomy, where the robot anticipates the user's intent and provides a degree of assistive autonomy to ensure fluid and successful motions. This research will also pave the way for future research that can bootstrap from teleoperation and build towards full robot autonomy. <br/><br/>The research proposes a hierarchical and multi-phased approach to shared autonomy, using techniques from deep learning and reinforcement learning. The system begins by using deep inverse reinforcement learning to quickly ascertain the user's high-level goal, such as whether the user wants to grasp a particular object or operate an appliance, from raw sensory inputs. This goal inference layer supplies objectives to the lower control layer, which consists of deep neural network control policies that can directly process raw sensory input about the environment and the user to make decisions. These policies choose low-level controls to satisfy the high-level objective while minimizing disagreement with the user's commands. The algorithms will be deployed and tested on a wheelchair-mounted robot arm with the potential to assist users with upper extremity disabilities to perform activities of daily living."
"1725743","SPX: CISIT: Computing In Situ and In Memory for Hierarchical Numerical Algorithms","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/11/2017","George Biros","TX","University of Texas at Austin","Standard Grant","Marilyn McClure","09/30/2020","$800,000.00","Lizy John, Andreas Gerstlauer","gbiros@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","042Y","026Z, 9229","$0.00","High performance computing holds an enormous promise for revolutionizing science, technology, and everyday life through modeling and simulation, statistical inference, and artificial intelligence.  Despite the numerous successes in software and hardware technologies, energy efficiency barriers have become a major hurdle towards more powerful computers -- from mobile devices all the way to supercomputers. The originally natural separation between the memory subsystem and the central processing unit (CPU) of a computer has emerged as one the main reasons for energy inefficiency. Data movement between the memory and the CPU requires orders of magnitude more energy than the computations themselves. To address these challenges, this project will consider novel architectural design paradigms and algorithms that are aimed at blurring these traditional boundaries between separated memory and computation subsystems and, by distributing computations to be performed directly in the memory or as part of the memory data transfers, achieve order of magnitude gains inenergy efficiency and performance. This project will investigate such novel approaches in the context of a class of methods in computational mathematics, which appear at the core of many problems in computational science, large-scale data analytics, and machine learning.<br/><br/>Specifically, this project will focus on data-driven rather than compute-driven co-design of algorithms and architectures for the construction, approximation, and factorization of hierarchical matrices. The end-goal of the project is the design of a novel architecture, CISIT (for ``Computing In Situ and In Transit''), that specifically aims to address acceleration of both computation and data movement in the context of hierarchical matrices. CISIT will uniquely combine traditional general-purpose CPU and GPU cores with: (1) acceleration of core algorithmic primitives using custom hardware; (2) in-situ computing capabilities that will comprise both processing in or near main memory as well as computing within on-chip caches and memory close to the cores; (3) novel in-transit compute capabilities that will enable cutting down on and in many cases completely eliminating unnecessary roundtrip data transfers by processing of data transparently as it is transferred between main memory and local compute cores across the cache hierarchies. Upon success, CISIT will influence future architectural implementations.  Along with the research activities, an educational and dissemination program will be designed to communicate the results of this work to both students and researchers, as well as a more general audience of computational and application scientists."
"1733884","AitF: Collaborative Research: Efficient High-Dimensional Integration using Error-Correcting Codes","CCF","Algorithms in the Field","09/01/2017","09/19/2019","Jose Garcia-Luna-Aceves","CA","University of California-Santa Cruz","Standard Grant","Tracy Kimbrel","08/31/2021","$438,177.00","","jj@cse.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7239","","$0.00","Efficiently estimating integrals of high-dimensional functions is a fundamental and largely unsolved computational problem, manifesting in scientific areas from biology and physics to economics. In particular, in Artificial Intelligence and Machine Learning, a wide array of methods are computationally limited precisely because they require the computation of high-dimensional integrals. While computing such integrals exactly is highly intractable, approximations suffice for many applications. Currently, approximation is attempted using two main classes of algorithms: Markov Chain Monte Carlo (MCMC) sampling methods and variational inference techniques. The former are asymptotically accurate, but their computational budget is inflexible and often prohibitive. The latter have manageable computational budget, but typically come with no accuracy guarantees. This project will investigate a new family of computationally efficient approximation methods which reduce the task of integration to the much better studied task of optimization, thus leveraging decades of research and engineering in combinatorial optimization methods and technology. A key goal of the project is to develop an open-source software library of efficient tools for high-dimensional integration.<br/><br/>The reduction of integration to optimization builds on the probabilistic reduction of decision problems to uniqueness promise problems developed in the mid-80s. Specifically, the idea is to use systems of random parity equations in order to specify random subsets of the function's domain, and relate integration to the task of optimization over these subsets. In general, the capacity for efficient optimization fundamentally stems from the capacity to summarily dispense large parts of the domain as uninteresting. The key question to be addressed by the project is whether it is possible to define random subsets over which optimization is both tractable and informative for integration. To that end, the project will employ random systems of linear equations corresponding to Low Density Parity Check (LDPC) matrices for error-correcting codes. The energy landscape, i.e., the number of violated equations, of such systems is far smoother than that of the generic (dense) random systems of linear equations that underlie the original mid-80s technique, thus being far more amenable to optimization. The project will also build upon the deep understanding gained in the last two decades for LDPC codes in the field of communications, with the goal of integrating a priori knowledge about the energy landscape in the optimization strategy. This will provide a fundamentally new use for error-correcting codes, creating a bridge between the areas of optimization and information theory."
"1663360","Collaborative Research: Computational Design of Metal-Organic Framework Materials","CMMI","DEMS-Design Eng Material Syst","06/15/2017","06/13/2017","Peter Greaney","CA","University of California-Riverside","Standard Grant","Kathryn Jablokow","05/31/2021","$269,245.00","","agreaney@engr.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","ENG","8086","024E, 067E, 073E","$0.00","This award supports research in computational methods for the automatic exploration of the search space of metal-organic frameworks to obtain desired combinations of mechanical, thermal and chemical properties. A metal-organic framework is a repeating three-dimensional crystal lattice with a large open space-frame structure composed of organic linking molecules bonded to inorganic nodal units. The interconnected pore-spaces in these materials endow them with a rich variety of unusual properties that can be exploited for gas storage, gas separation, and catalysis. Given the constraints of chemical bonds and bond angles, one cannot easily customize the lattice for a particular size and shape. Thankfully there is an astronomically large number of permutations for combining various atomic elements together. However, human designers are confounded on how to find one for their particular problem domain. What is needed is a computational process to search the space for a best solution. By formalizing the molecular design as a decision tree, the developed computer algorithms will invent new materials whose functional behavior is defined by its chemical makeup and the resulting geometry and movement of the lattice structure. As part of testing the design approach, the project will address two technologically important problems:  designing new metal organic frameworks optimized for gas storage, and materials for separating isomers in industrially important chemical feed stocks. STEM outreach activities to high school students and teachers will also be performed.<br/><br/>Specifically, the project seeks materials that exhibit highly chemically selective adsorption or permeability of gases through mechanisms that arise from chemical, steric, and vibrational behavior of the frameworks. The computational search for this incorporates a unique graph transformation approach that mimics correct stoichiometric reactions and leads to a large search tree that is amenable to recent advances in artificial intelligence planning algorithms. Furthermore, machine learning methods will establish a link between the structure and function of organic frameworks by leveraging data from complex molecular simulations. This will lead to more efficient search of the decision tree so that meaningful results can be obtained. Through detailed simulations of the resulting metal organic frameworks, the researchers will publish how their new materials can be used to tackle challenging problems in energy storage, high-tech manufacturing, and the creation of new sensitive sensor equipment."
"1721739","SBIR Phase I:  IoT Smart Water Management System","IIP","SBIR Phase I","06/01/2017","06/09/2017","Matthew Cusack","NY","Mobius Labs, Inc","Standard Grant","Rick Schwerdtfeger","05/31/2018","$224,949.00","","cusackmatthewj@gmail.com","1659 Central Avenue","Albany","NY","122050000","5189925408","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to provide a simple to use ""smoke alarm"" for water appliances coupled with a level of insight into water consumption and failures that has never existed before. What was once a reliance on water meter data for the sum of all appliances is now a detailed understanding for each individual water fixture. This unprecedented access to water usage data at an individual appliance level will provide the opportunity for increased data analytics research, predictive analytics for failure modes and maintenance scheduling, and better understanding of human consumption habits to identify and solve problems that allow organizations to meet aggressive sustainability goals.  Only recently has there been an ability for hospitals to aggregate their medical information to aid research efforts to treat and cure chronic diseases and illnesses.  Mobius is looking to do the same for any building owner for water, as the growing concern to continued, reasonable costing fresh water becomes even greater.<br/><br/>The proposed project will provide the opportunity for the development of an IoT Smart Water Management System (SWMS). It will use artificial intelligence and machine learning to both identify leaks so they can be corrected before wasting precious water and energy, and provide predictive analytics for actionable insights. This will significantly improve water appliance maintenance and prevent costly water damage to properties. Further, at a macro level, it provides municipality level insight for maintenance and crisis management. The vision is to easily install these SWMS devices into any existing water fixture. It will take less than 60 seconds to install and connect to the Internet. The design goal is to be cost-effective from the start. The SWMS provides an affordable and easy way to adopt data driven decision making into current operations. The critical objectives set forth in this proposal are centered on achieving a simple ""smoke alarm"" like warning of a fault coupled with a robust analytics platform.  Finally, these will be designed as highly durable IoT devices that require little or no maintenance."
"1818643","XPS: FULL: Collaborative Research: Parallel and Distributed Circuit Programming for Structured Prediction","CCF","Exploiting Parallel&Scalabilty","10/01/2017","06/18/2018","Vivek Sarkar","GA","Georgia Tech Research Corporation","Standard Grant","Anindya Banerjee","07/31/2019","$88,265.00","","vsarkar@rice.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8283","","$0.00","This project develops a system for ""circuit programming,"" which allows a programmer to focus on the high-level solution to a problem rather than on the details of how the computation is organized. Circuit programming consists of writing rules that describe how data items depend on one another. The intellectual merits lie in the design of a new programming language for specifying these rules, along with the algorithms whereby the computer automatically finds efficient strategies for managing the necessary computations on available parallel hardware.  The project's broader significance and importance lie in its potential to streamline work in areas such as artificial intelligence and machine learning.  With the growing complexity of systems in these areas and their need to process big data in depth, research and teaching typically get bogged down in programming details, especially for parallel platforms; this project aims to delegate those details to automatic methods.<br/><br/>The research develops a programming system for Dyna, a circuit programming language that enables concise specification of large function graphs that may be cyclic and/or infinite. Dyna employs (1) a pattern-matching notation that augments pure Prolog with evaluation and aggregation and (2) an object-like mechanism for dynamically defining new sub-circuits as modifications of old ones.  This project is building an adaptive system that can mix forward and backward chaining to seek a fixpoint of the circuit and to update this fixpoint as the inputs change.  The system will perform compile-time and runtime analysis of the Dyna program and will map it to Habanero, a system for scheduling parallel computations on multicore processors, with extensions for task priorities, task cancellation, GPU execution, and distributed execution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740126","E2CDA: Type II: Non-Volatile In-Memory Processing Unit: Memory, In-Memory Logic and Deep Neural Network","CCF","Energy Efficient Computing: fr","09/15/2017","07/29/2019","Deliang Fan","FL","The University of Central Florida Board of Trustees","Continuing Grant","Sankar Basu","02/29/2020","$184,470.00","","dfan@asu.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","015Y","7945","$0.00","The objective of this project is to explore leveraging emerging nanoscale spin-orbit torque magnetic random access memory (SOT-MRAM) to develop a non-volatile in-memory processing unit that could simultaneously work as non-volatile memory and a co-processor for next-generation energy efficient and high performance computing system. Such energy efficient in-memory computing system integrates logic and memory units by exploring innovations from emerging spintronic device technology to non-Von Neumann architecture, which is targeting to tackle power wall and memory wall bottlenecks in traditional computing system. It will be crucial for industry and academia to identify next-generation energy efficient and high performance computing platform design. The project also has education and outreach components including new curriculum in post-CMOS devices and circuits for undergraduate/graduate students, engineering outreach to diverse population and other underrepresented groups at the University of central Florida. The project will also directly involve minority and female graduate/ undergraduate students.<br/><br/>The proposed research requires synergistic exploration spanning from device technology to architecture innovation. Specifically, it consists of three research thrusts: (i) exploring novel SOT-MRAM memory array that could implement in-memory logic (AND/OR/XOR) without add-on logic circuits; (ii) investigating non-volatile in-memory processing unit (MPU) architecture that could simultaneously work as nonvolatile memory and co-processor to pre-process raw data within memory to accelerate data/computing intensive applications without sacrificing memory capacity; (iii) exploring MPU to implement in-memory convolution to greatly reduce data communication and accelerate state-of-the-art deep learning convolutional neural networks."
"1719175","SaTC: CORE: Small: Towards Robust and Scalable Search of Binary Code and Data","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/15/2017","05/29/2020","Heng Yin","CA","University of California-Riverside","Standard Grant","Sol Greenspan","08/31/2021","$508,756.00","","heng@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","1714, 8060","025Z, 7434, 7923, 9178, 9251","$0.00","The problem of binary code and data search concerns how to glean valuable information from binary code and binary data in an accurate, scalable and robust fashion. This concern is central to  many security problems, including vulnerability scanning, code plagiarism detection, software lineage, malware classification, memory forensics, virtual machine introspection, malicious document detection, etc. Although this problem is not new and a great deal of solutions have been proposed, no solutions can achieve the requirements of accuracy, scalability and robustness simultaneously. There are bottlenecks for binary code and data search due to the search schemes: pair-wise comparison for binary code search does not scale, and rule-based binary data search is too rigid and thus not robust against changes caused by different platform versions and malicious manipulations.<br/><br/>The proposed work takes a novel approach to the problem of binary code and data search, one that mimics how the human brain recognizes interesting objects from an enormous amount of visual information. There are two research thrusts: 1) scalable cross-platform binary code search, which aims to quickly identify semantically equivalent or similar code from a large binary code base in different architectures, by automatically learning high-level features from binary code via clustering and deep learning; and 2) adaptive, efficient and robust binary data analysis, which aims to accurately identify objects from binary data such as memory dumps and documents, by constructing deep neural network models. Because binary code and data search are foundational for many security applications, advances to these foundations can push the boundary for all the security applications built on top. Moreover, successful application of deep learning onto binary code and data search will revolutionize how we solve many security problems in general and stimulate more research in the direction of security by deep learning."
"1648023","SBIR Phase I:  Efficient Custom Platforms for Smart Computer Vision in the Internet of Things","IIP","SMALL BUSINESS PHASE I","02/01/2017","02/03/2017","Kyle Rupnow","IL","Inspirit IoT, Inc.","Standard Grant","Rick Schwerdtfeger","01/31/2018","$225,000.00","","kjrupnow@gmail.com","2510 Hallbeck Dr.","Champaign","IL","618226879","2177786116","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will result in a significant improvement in the performance, power, and cost of deploying smart computer vision applications. This improvement will simplify the deployment of smart vision applications in automotive, sports and entertainment, consumer, robotics and machine vision, medical, and security/surveillance domains. Through a unique and energy-efficient hardware computation platform, automated hardware design for machine learning applications, and efficient implementation libraries, this project will improve both the feature set and efficiency of smart vision applications across a wide range of end-use cases. With the rapid growth in smart vision applications, this project will be a key enabling technology to support high-performance, energy efficient and scalable solutions. Internet of Things (IoT) applications promise to produce billions in revenue and trillions in global economic impact through improved efficiency, safety, energy, and labor costs. Wide deployment of customized computing in IoT applications will lead to substantial energy savings, and a corresponding reduction in carbon emissions, and a more sustainable growth model for deployment of intelligent sensor systems with thousands or millions of sensor nodes analyzing large volumes of input data.<br/>The proposed project focuses on the design of high performance, energy-efficient IoT computer vision platforms, design tools, and implementation libraries. Field-programmable gate arrays (FPGAs) are an attractive design and implementation platform to meet performance and energy goals; however, there are two main challenges to their adoption (1) small FPGAs are cost-effective but insufficient to replace the efficient, low-cost ASICs for computation-demanding applications; large FPGAs can fulfill all computation demands, but are too expensive to meet IoT price points, and (2) design and development for FPGAs is challenging and require hardware design expertise. This project?s innovation targets these challenges. First, our proposed platform will combine a media ASIC for efficient video processing with a small cost-effective FPGA for custom machine learning. Second, our proposed domain specific high level synthesis will generate efficient machine learning accelerators for standard machine learning infrastructures quickly, limiting required hardware design expertise while out-performing general purpose design techniques. This project will leverage background expertise in hardware design, design tools, and machine learning to develop and demonstrate the advantages of hybrid computation platforms for smart vision applications in terms of performance, energy consumption, cost and physical size."
"1650485","Carnegie Mellon University Planning Grant: I/UCRC for Big Learning","CNS","IUCRC-Indust-Univ Coop Res Ctr","02/15/2017","02/15/2017","Ruslan Salakhutdinov","PA","Carnegie-Mellon University","Standard Grant","Dmitri Perkins","01/31/2018","$14,999.00","Eric Xing","rsalakhu@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","5761","5761","$0.00","This project will study the feasibility of establishing the Center for Big Learning (CBL), as an NSF IUCRC. The mission of CBL is to develop novel large-scale deep learning algorithms, systems, and applications through unified and coordinated efforts in the CBL consortium. The vision of CBL is to develop intelligence algorithms towards intelligence-driven society. With the explosion of big data generated from natural systems, scientific experiments, engineered systems, and human activities, we need to develop intelligent algorithms and systems to facilitate our decision making with distilled insights automatically at scale. The proposed CBL center is a timely initiative as we move towards intelligence-enabled world of opportunities. The CBL consortium is expected to become the magnet of deep learning research and applications and attract leading researchers, entrepreneurs, IT and industry giants working together on accomplishing our mission and vision. This planning grant will lead to a successful Phase I proposal for the establishment of the Center for Big Learning at CMU with a solid consortium across multiple campuses and a large number of industry partners.<br/><br/>CBL has the following broader impacts. (1) Making significant contributions and impacts to the deep learning community on pioneering research and applications to address a broad spectrum of real-world challenges. (2) Making significant contributions and impacts to promote products and services of industry in general and our members in particular. (3) Making significant contributions and impacts to the urgently-needed education of our next-generation talents with real-world settings and world-class mentors from both academia and industry. (4) Our meetings, forums, conferences, and planned training sessions will greatly promote and broaden the research and materialization of Deep Learning.<br/><br/><br/>Recent dramatic breakthroughs in deep learning (DL) and multi-model learning (e.g., image, video, speech, and text), hold great promise for making a big impact on many research areas, including computational biology, neuroscience, medical diagnosis, computer vision, data mining, and robotics. The key mission of CBL at CMU is to pioneer in large-scale deep learning (DL)  algorithms, systems, and applications through unified and coordinated efforts in the CBL consortium via fusion of broad expertise from our large number of faculty members, students, and industry partners. The vision of CBL at CMU is to develop intelligent algorithm towards intelligence-driven society. CBL possesses the pioneering intellectual merit in the following key research themes:<br/><br/>(1) Novel algorithms. This theme focuses on novel DL algorithms and architectures, such as deep neural networks, complex recurrent neural networks, brain-inspired components, optimization, deep reinforcement learning, and unsupervised learning.<br/>(2) Novel systems. We propose to develop novel architectures, resource management, and software frameworks for enabling large-scale DL platforms and applications on desktops, mobiles, clusters, and clouds.<br/>(3) Novel applications in health, mobile/IoT, and surveillance. During the planning phase, we will establish a solid center strategic plan, marketing plan, and the CBL consortium that consists of four academic sites and a large number of industrial members."
"1651832","CAREER: Inverse Origami: Generalized Pose-Normalization for Large-Scale Fine-Grained Recognition","IIS","Robust Intelligence","06/01/2017","05/08/2019","Ryan Farrell","UT","Brigham Young University","Continuing Grant","Jie Yang","05/31/2022","$520,578.00","","farrell@cs.byu.edu","A-285 ASB","Provo","UT","846021231","8014223360","CSE","7495","1045, 7495, 9251","$0.00","This project seeks to create a computational platform capable of performing fine-grained visual categorization (FGVC) with accuracy comparable to a highly-trained human domain expert.  This ability to visually classify an object amongst thousands of subtly-differing subordinate categories has numerous real-world applications in biology, forestry and agriculture, homeland security and medical pathology.  While the research conducted in this project is applicable to many domains and applications, it targets insects as the next frontier in fine-grained recognition with specific applications of assessing and conserving biodiversity, and protecting forests and agricultural crops from known invasive pests.  The project's impact extends beyond the advancements in computer vision research, tightly integrating biological fieldwork and citizen science activities such as a field-deployable autonomous recognition system, a mobile recognition app, and an interactive museum exhibit to inspire future scientists.<br/><br/>The primary research goal of this project is to develop a recognition system that can learn to recognize new fine-grained subcategories using as few as one image.  The developed pose-normalization framework explicitly factorizes an object's representation into a domain-level geometric model and a subcategory-specific appearance model.  The geometric domain model is learned automatically by observation of live subjects.  A unified pose-estimation model determines the location and orientation of the one-, two- and three-dimensional parts of the geometric model, leveraging an innovative form of convolutional neural network.  The subcategory labels and complex part annotations required to train the model are acquired efficiently using physical museum specimens and a multi-view annotation framework.  A large 2500-category dataset of insect images is being constructed for the project and for release to the computer vision research community."
"1736364","Collaborative Research:  AMPS:  Multi-Fidelity Modeling via Machine Learning for Real-time Prediction of Power System Behavior","DMS","AMPS-Algorithms for Modern Pow, ","08/15/2017","06/27/2018","Guang Lin","IN","Purdue University","Continuing Grant","Leland Jameson","07/31/2021","$143,898.00","Steven Pekarek","lin491@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","045Y, R189","","$0.00","The operation of current power systems depend on deterministic and static models, which are not suitable for analyzing smart power grids due to the increasing large-volume of data collected by the grids and sensors and the need to integrate intermittent renewable resources and dynamic load compositions. Large uncertainty in the model prediction is problematic as it does now allow careful planning, and failure to identify large fluctuations and possible instabilities could endanger the reliable operation of the power grid. Hence, it is crucial to incorporate new monitoring capabilities realized by new tools such as machine learning and predictive multi-rate modeling in modeling the smart grid. Classical methods that deal with uncertainty lead to inefficient solutions as they are too slow to converge to a solution and hence they cannot be used effectively for real-time control of power grids. This difficulty stems from the requirement of sampling the very complex power grid thousands of times in order to arrive to a reasonably accurate solution. The goal of this project is to establish significant advances in research and education in the development of machine learning and real-time predictive modeling of power systems, with particular focus on the smart grid.<br/><br/>Machine learning and real-time predictive modeling have received increasing attention in recent years. Extensive research effort has been devoted to these topics, and novel numerical methods have been developed to efficiently deal with sensor data and complex engineering systems. Both machine learning and real-time predictive modeling enable us to better extract the useful information from available sensor data and make critical decision in real time with the presence of uncertainties. For example, solar and wind energy will depend on the weather condition. Machine learning and real-time predictive modeling are thus critical to many important practical problems such as power system stability analysis and social cyber-network prediction, etc. For large-scale power systems, deterministic simulations can be very time-consuming, and conducting predictive simulations further increases the simulation cost and can be prohibitively expensive. One of the biggest challenges in machine learning and real-time predictive modeling is how to develop hierarchical reduced-order models and how to fuse information from such hierarchical reduced-order models. This project aims to address these critical challenges. A novel set of deep-learning based multi-fidelity algorithms (deep Gaussian processes) will be developed for real-time prediction of power systems. The approach under development in this research project is based on scalable algorithms for building deep-learning based reduced-order models for efficient power system dimension reduction. The new algorithms will be based on building multi-fidelity models via deep learning for power systems, and they will significantly advance the current state of the art of deep learning and real-time predictive modeling. The project will also integrate educational opportunities and will expand the population of modelers who use machine learning and predictive modeling tools to solve network problems. The project will expose a diverse group of undergraduates and minority students to machine learning and predictive modeling."
"1736088","Collaborative Research:  AMPS:  Multi-Fidelity Modeling via Machine Learning for Real-time Prediction of Power System Behavior","DMS","AMPS-Algorithms for Modern Pow, ","08/15/2017","06/27/2018","George Karniadakis","RI","Brown University","Continuing Grant","Leland Jameson","07/31/2021","$130,000.00","","George_Karniadakis@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","045Y, R189","9150","$0.00","The operation of current power systems depend on deterministic and static models, which are not suitable for analyzing smart power grids due to the increasing large-volume of data collected by the grids and sensors and the need to integrate intermittent renewable resources and dynamic load compositions. Large uncertainty in the model prediction is problematic as it does now allow careful planning, and failure to identify large fluctuations and possible instabilities could endanger the reliable operation of the power grid. Hence, it is crucial to incorporate new monitoring capabilities realized by new tools such as machine learning and predictive multi-rate modeling in modeling the smart grid. Classical methods that deal with uncertainty lead to inefficient solutions as they are too slow to converge to a solution and hence they cannot be used effectively for real-time control of power grids. This difficulty stems from the requirement of sampling the very complex power grid thousands of times in order to arrive to a reasonably accurate solution. The goal of this project is to establish significant advances in research and education in the development of machine learning and real-time predictive modeling of power systems, with particular focus on the smart grid.<br/><br/>Machine learning and real-time predictive modeling have received increasing attention in recent years. Extensive research effort has been devoted to these topics, and novel numerical methods have been developed to efficiently deal with sensor data and complex engineering systems. Both machine learning and real-time predictive modeling enable us to better extract the useful information from available sensor data and make critical decision in real time with the presence of uncertainties. For example, solar and wind energy will depend on the weather condition. Machine learning and real-time predictive modeling are thus critical to many important practical problems such as power system stability analysis and social cyber-network prediction, etc. For large-scale power systems, deterministic simulations can be very time-consuming, and conducting predictive simulations further increases the simulation cost and can be prohibitively expensive. One of the biggest challenges in machine learning and real-time predictive modeling is how to develop hierarchical reduced-order models and how to fuse information from such hierarchical reduced-order models. This project aims to address these critical challenges. A novel set of deep-learning based multi-fidelity algorithms (deep Gaussian processes) will be developed for real-time prediction of power systems. The approach under development in this research project is based on scalable algorithms for building deep-learning based reduced-order models for efficient power system dimension reduction. The new algorithms will be based on building multi-fidelity models via deep learning for power systems, and they will significantly advance the current state of the art of deep learning and real-time predictive modeling. The project will also integrate educational opportunities and will expand the population of modelers who use machine learning and predictive modeling tools to solve network problems. The project will expose a diverse group of undergraduates and minority students to machine learning and predictive modeling."
"1703166","III: Medium: Learning Multimodal Knowledge about Entities and Events","IIS","Info Integration & Informatics","08/01/2017","09/15/2018","Hanna Hajishirzi","WA","University of Washington","Standard Grant","Wei Ding","07/31/2021","$700,000.00","Yejin Choi, Ali Farhadi, Hanna Hajishirzi","hannaneh@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7364","7364, 7924","$0.00","Everyday knowledge about the world is a necessary condition for intelligent information processing and reasoning. People can read between the lines in text and see beyond what are visible in images because of everyday functional knowledge about how the world works. The primary goal of this research is to develop learning algorithms that can automatically acquire such knowledge, centered around entities and events, from large-scale multimodal web data. Entity knowledge includes a broad range of physical and conceptual knowledge about objects and people, including their attributes, their relative differences, and logical relations among them. Event knowledge focuses on structural knowledge about everyday events in people's lives organized through hierarchical and temporal relations among sub-events and the event participants. Together, the resulting knowledge will be a critical step forward to enable robust AI systems at the intersection between natural language processing and computer vision that can understand and reason about unstructured multimodal information. The potential impact of this research includes interactive assistive systems for the visually-impaired and multimodal educational interfaces. <br/><br/>This project investigates multimodal knowledge extraction as a new research paradigm drawing connections between relevant methods in natural language processing such as information extraction, textual entailments, and frame semantics with recent advances in computer vision. One of the critical challenges in commonsense knowledge acquisition is to overcome reporting bias, i.e., people do not state the obvious. Therefore, this project develops new learning algorithms based on a graph-based collective inference that can reason about unspoken knowledge that systematically influences the way people describe the world in language, images, and videos. In addition, this project develops new models for visual semantic parsing and event recognition, which generalize existing studies on activity recognition by specifying various structural components of events such as actors, objects, locations, tools, intents, and goals. The learned knowledge and representation will be validated through several applications including multimodal question answering and grounded language understanding."
"1648753","STTR Phase I:  Development of a Safety System for Individuals with Alzheimer's Disease and Related Dementias","IIP","STTR Phase I","01/01/2017","12/16/2016","George Netscher","CA","SafelyYou Inc.","Standard Grant","Jesus Soriano Molla","10/31/2017","$225,000.00","Alexandre Bayen","gnetscher@gmail.com","2935 MLK Jr Way, Unit C","Berkeley","CA","947032166","7138226924","ENG","1505","1505, 8018, 8032, 8042, 8089","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is a safety system for improving the quality and reducing the cost of dementia care. Alzheimer's disease affects 5.4M in the US, including 1 in 9 over 65 and 1 in 3 over 85, and represents two thirds of all those affected by dementia. Despite that Alzheimer's disease is the single most expensive disease in the US and falls are the leading cause of hospitalization in Alzheimer's care, current tools offer little support. Although 3/4 of elderly fallers will experience a repeat fall, solutions like bed alarms and wearable fall detection systems offer no way to see how falls occur. Care staff have no way of learning from the first fall to reduce the likelihood of the second and must implement painful and expensive policies such as sending every unwitnessed fall to the emergency room in case a hit to the head occurred. <br/><br/>The proposed project addresses this critical gap in Alzheimer's care by detecting falls based on camera video where falls can be reviewed by a human assistant in real-time and after the fact. Real-time review allows for instant notification if a hit to the head occurred, and review after the fact allows for determining the cause of the fall to see if changes in room layout and/or policy could be made. The primary aim of this project is to collect video data of real falls 1) to apply and extend state-of-the-art deep learning methods to perform high accuracy detection and 2) to validate that affected individuals, family, and care staff are accepting of a camera-based solution. Fall detection will be performed by extending the Region-Based Convolutional Neural Network (RCNN) algorithm using domain adaptation techniques developed to robustly handle night-vision camera operation, occlusion, and non-standard human pose. Technical success will be measured by <1% missed detection and <50% false positive rate from this feasibility study. This first accuracy threshold will define a lower bound where, as has been demonstrated repeatedly in the deep-learning paradigm, accuracy will continue to improve as more data is collected."
"1743050","EAGER: Parallel Semi-supervised Machine Learning for Volumetric Datasets","IIS","Info Integration & Informatics","06/01/2017","05/10/2017","Anand Rangarajan","FL","University of Florida","Standard Grant","Wei Ding","11/30/2019","$99,998.00","Sanjay Ranka, S. Balachandar","anand@cise.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7364","7364, 7916","$0.00","Machine learning and parallel computing have come of age. Rapid advances have been made in this decade in the automated recognition of objects and faces in images with ""deep learning"" almost becoming a household term at the present time. However, a clear limitation in most of the present work is the restriction to two dimensional datasets such as images and the like. When the focus shifts to large three dimensional datasets such as 3D medical imaging, fluid dynamics simulations, remote sensing and electron microscopy, the problems become more difficult by several orders of magnitude. The automatic labeling of three dimensional structures in large datasets requires a comprehensive integration of machine learning and parallel computing with a ""from the ground up redesign"" to be efficient, accurate and capable of scaling to ever larger volumes. The benefits to the engineering communities and society at large are clear. Successful execution of this project will enable experts in a variety of disciplines in which three dimensional data are generated to efficiently perform large scale automated labeling of structures of interest like the hippocampus in brain images or vortices in fluid dynamics simulations. Students trained in this nexus of machine learning and parallel computing will be capable of making their own contributions ranging from academic research to commercialization. Finally, the software suites generated by this project should play a role in the formation of vertically integrated enterprises.<br/><br/>Volumetric applications require the development of novel, efficient and scalable machine learning algorithms as existing approaches are computationally intensive and are limited to small size images/video. Volumetric data require that approaches classify homogeneous regions into single categories while maintaining clear-cut region boundaries between classes (urban versus forest for example in terrain classification). To this end, new methods are developed for extracting supervoxels from volumetric datasets, using three dimensional filters, nonlinear dimensionality reduction and Hamilton-Jacobi or Schrodinger geodesic solvers. Next, deep learning principles which have resulted in automated feature extraction and discriminative convolutional filters must be adapted to work on volumetric data. Consequently, the integration of supervoxels and deep learning is central to the proposed work. Very limited expert interaction is permitted since the volume of the datasets is too large, therefore calling for semi-supervised learning approaches. The integrated machine learning and parallel processing software suite created by this project will be disseminated using software management repositories and open source licensing. In summary, the intellectual merit lies in the careful integration of semi-supervised learning, volumetric supervoxel driven segmentation, deep learning and parallel processing."
"1741571","Doctoral Dissertation Research: The Grammar of Space and Social Distance in Cushillococha Ticuna, an Endangered Language","BCS","DEL DDRIG Document Endangered, Linguistics","08/01/2017","06/21/2017","Lev Michael","CA","University of California-Berkeley","Standard Grant","Joan Maling","03/31/2020","$17,036.00","Amalia Skilton","levmichael@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","SBE","036Y, 1311","1311, 7719, 9179, SMET","$0.00","All languages have two categories of words: words that can be understood without context, like 'frog' and 'sand,' and words that can only be understood in context, like 'this,' 'that,' 'here,' and 'you.' There are many differences between these context-dependent words, known in linguistics as indexicals, and context-independent words. People use different cognitive processes to understand context-dependent words.  Context-dependent words differ more between languages. Gestures, such as pointing, are also more important for understanding context-dependent language. Moreover, because many linguistic theories focus on context-independent aspects of language, the structure of context-dependent language is poorly understood. Against this background, this study examines context-dependent language in an endangered language that is an isolate, meaning it has no linguistic relatives. Using perspectives from linguistics, psychology, and anthropology, the researchers will study the meaning and use of this isolate's words equivalent to 'this,' 'that,' 'here,' 'there,' 'he,' and 'she.' This work will advance scientific understanding of context-dependent language and its relationship to gesture, a topic with applications in natural language processing, computer vision, and other areas of computational linguistics. The documentation will form the core material analyzed in a doctoral dissertation produced by the CoPI. Broader impacts include a publicly available deposit of the recordings and transcriptions, as well as collaborative efforts with community leaders to improve the literacy curriculum used in local indigenous schools, promoting educational equity and economic development in the region.<br/><br/><br/>The CoPI, a doctoral student at the University of California, Berkeley, will analyze the documentation of Cushillococha Ticuna, an endangered indigenous language isolate spoken only in Peru. This project will specifically focus on the grammar of two types of context-dependent expressions in Ticuna: demonstratives -- words like this, that, and there -- and third-person pronouns. Ticuna's demonstratives have more complex meanings than those of other languages. While English 'this' and 'that' are usually believed to convey an object's location in space, the equivalent words in Ticuna convey both the location of the object and the tense of the entire sentence. Similarly, third person pronouns are richer in meaning in Ticuna than in English and other well-known languages. They encode both the gender of the referent, like English 'he' and 'she,' and the speaker's level of respect for the referent, as in the French pronouns 'tu' (you, informal) and 'vous' (you, formal). The researcher will study these two systems of context-dependent expressions using perspectives from three disciplines: logical semantics, neo-Gricean pragmatics, and Conversation Analysis. The interdisciplinary research methods will incorporate psycholinguistic experiments, interviews with native speakers, and recording of everyday conversations. In the demonstratives component of the project, the researcher will test recent attention-oriented theories of demonstratives by (a) applying them to typologically novel data and (b) probing their claims about the role of lexically specific heuristics in construal of demonstratives. In the honorifics component of the project, the research will examine everyday conversation and interview data to analyze both (a) the social meaning of honorifics and (b) their formal semantic properties."
"1657338","CRII: III: Real-World Machine Learning: Adaptation Methods for Addressing Temporal, Geographic, and Demographic Confounds in User-Generated Content","IIS","CRII CISE Research Initiation","09/01/2017","02/28/2017","Michael Paul","CO","University of Colorado at Boulder","Standard Grant","Sylvia Spengler","08/31/2020","$174,117.00","","mpaul@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","026Y","7364, 8228","$0.00","There is a rapidly growing body of research that uses user-generated content from the web, e.g., social media messages, to draw conclusions about the world.  Using machine learning and natural language processing methods, it is possible to estimate public opinion, consumer sentiment, and population health based on what people are publicly sharing about their thoughts and actions online.  For example, if someone writes that they have a fever, we might infer that they have the flu; if we aggregate all messages like this, we can track the prevalence and spread of the flu at a population level.  However, a challenge with applying machine learning to user-generated content is that the characteristics of the content are highly dependent on the Who, When, and Where of the users.  Online discussions evolve rapidly; a system built in one year might not work well in the next, and a system built for one community of users might not work for another.  The proposed project seeks to create machine learning methods that are robust to variations in time, geography, and demographics of content and content creators.  Related to domain adaptation techniques in machine learning, the PI proposes methods that learn to generalize across these various content attributes.  The general goal is to create robust, open source tools that can be easily adopted by other researchers.  One particular outcome of the project will be to improve the machine learning classifiers used in prior work on social media-based disease surveillance.  The output of the PI's health analysis systems will be integrated into HealthTweets.org, a publicly accessible website that shares daily estimates of disease prevalence for other researchers and health officials. <br/><br/><br/>The project will create hierarchical Bayesian models for training classifiers that can be adapted across different content attributes.  The specific attributes of interest include time, geography, and demographic group of the author, but the proposed models do not depend on the specific attributes, and can be broadly applied to other machine learning settings.  As a starting point, a predictive model (classification or regression) will be constructed that can be adapted across one attribute at a time.  The PI will then create novel extensions to the model that can adapt across conjunctions of multiple attributes, such as time AND location.  These extensions are related to the PI's prior work on building structured topic models that learn relationships between different features of content.  Finally, in addition to creating predictive models, the PI will also build models of content that can be used to infer missing attributes (e.g., the location of a user if it is unknown), which can be combined with the predictive models to jointly perform inference and classification. Classification performance in new settings on a variety of datasets and exploration of the effects of, and sensitivity to, different parameters will be tested.  Specific deliverables include the improvement a classifier for detecting influenza infection on Twitter, and integrating the classifier into the website, HealthTweets.org."
"1718787","SaTC: CORE: Small: Attack-Agnostic Defenses against Adversarial Inputs in Learning Systems","CNS","Secure &Trustworthy Cyberspace","08/15/2017","08/08/2017","Ting Wang","PA","Lehigh University","Standard Grant","Wei-Shinn Ku","11/30/2019","$498,315.00","","tbw5359@psu.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","8060","025Z, 7434, 7923","$0.00","Deep learning technologies hold great promise to revolutionize the way people live and work. However, deep learning systems are inherently vulnerable to adversarial inputs, which are maliciously crafted samples to trigger deep neural networks to misbehave, leading to disastrous consequences in security-critical applications. The fundamental challenges of defending against such attacks stem from their adaptive and variable nature: adversarial inputs are tailored to target deep neural networks, while crafting strategies vary greatly with concrete attacks. This project develops EagleEye, a universal, attack-agnostic defense framework that (i) works effectively against unseen attack variants, (ii) preserves predictive power of deep neural networks, (iii) complements existing defense mechanisms, and (iv) provides comprehensive diagnosis about potential risks in deep learning outputs.<br/><br/>In particular, EagleEye leverages a set of invariant properties underlying most attacks, including the ""minimality principle"": to maximize attack evasiveness, an adversarial input is generated by applying the minimum possible distortion to a legitimate input. By exploiting such properties in a principled manner, EagleEye effectively discriminates adversarial inputs (integrity checking) and even uncovers their correct outputs (truth recovery). The specific research tasks include: (i) identifying inherently distinct properties (differentiators) of legitimate and adversarial inputs, (ii) developing attack-agnostic adversarial input detection methods based on these differentiators, and (iii) analyzing possible countermeasures by adversaries to evade such defenses. This research not only facilitates the adoption of deep learning-powered systems and services, but also enlightens designing and implementing robust machine learning systems in general. New theories and systems developed in this project are integrated into undergraduate and graduate education and used to raise public awareness of the importance of machine learning security. More information about this project can be found at the project homepage: http://x-machine.github.io/project/eagleeye"
"1650549","University of Missouri-Kansas City Planning Grant: I/UCRC for Big Learning","CNS","IUCRC-Indust-Univ Coop Res Ctr","02/15/2017","02/13/2017","Zhu Li","MO","University of Missouri-Kansas City","Standard Grant","Dmitri Perkins","01/31/2018","$15,000.00","Yugyung Lee","lizhu@umkc.edu","5100 Rockhill Road","Kansas City","MO","641102499","8162355839","CSE","5761","5761, 9150","$0.00","The mission of the proposed NSF I/UCRC Center for Big Learning (CBL) is to explore research frontiers in emerging large-scale deep learning (DL) to realize effective and efficient computational intelligence, design novel learning algorithms and system mechanisms for intelligence research and applications in the era of big data and big systems. Through the big learning consortium of multiple academic sites (in collaboration with Florida, CMU, and Oregon) and a large number of industry partners, the center seeks to catalyze the fusion of wisdom from academia, government, industry stakeholders, the rapid innovation in algorithms, systems, and education, and technology transfer into cutting-edge products and services with real-world relevance and significance.<br/>    <br/>Broader Impacts of the proposed center: with the explosive growth of data generated from natural systems, engineered systems, and human/life activities, we need intelligent software and hardware to facilitate our decision making with distilled insights automatically at scale. The proposed I/UCRC Center for Big Learning is a timely initiative as our society moves towards intelligence-enabled world of opportunities. The Big Learning consortium is expected to become the magnet of deep learning research and applications and attract leading researchers, enthusiastic entrepreneurs, IT and industry giants working together on accomplishing the promising missions and visions of CBL. In particular, CBL has the following broader impacts. (1) Making significant contributions and impacts to the deep learning community on pioneering research and applications to address a broad spectrum of real-world challenges. (2) Making significant contributions and impacts to promote products and services of industry in general and our members in particular. (3) Making significant contributions and impacts to the urgently needed education of our next-generation talents with real-world settings and world-class mentors from both academia and industry. (4) Our meetings, forums, conferences, and planned training sessions will greatly promote and broaden the research and materialization of DL.<br/><br/><br/>With dramatic breakthroughs in signal compression, classification and identification in multiple modalities of challenges (e.g., image, video, speech, text, and life, health & science data), the renaissance of computational intelligence is looming. The mission of the CBL is to pioneer in this emerging trend through united and coordinated efforts and deep integration and fusion of broad expertise from our large number of faculty members, students, and industry partners. The vision of CBL is to create intelligence enablers towards intelligence-driven society. CBL possesses the pioneering intellectual merit in the following key research themes. (1) Novel algorithms. This theme focuses on novel DL algorithms and architectures, such as deep architectures, complex deep neural networks, brain-inspired components, optimization and acceleration of the deep learning, neural machines, and adaptation of conventional machine learning algorithms. (2) Novel systems. We propose novel resource management strategies, heterogeneous architectures, and software tool kits for embedded devices, mobiles, desktops, clusters, and clouds. (3) Novel applications in business, health, imaging, and smart things, including deep residual networks in new image/video modeling and compression, RNN for large scale context models in entropy coding, large scale visual object re-identification, and targeted drug delivery with imaging. During the planning phase, we will establish a solid center strategic plan, marketing plan, and the consortium of big learning that consists of five academic sites and several dozens of industrial members."
"1837827","Collaborative Research: NCS-FO: Learning Efficient Visual Representations From Realistic Environments Across Time Scales","IIS","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","09/01/2017","07/19/2018","Per Sederberg","VA","University of Virginia Main Campus","Standard Grant","Roger Mailler","08/31/2020","$459,047.00","","pbs5u@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7980, 8624","8089, 8091, 8551","$0.00","Computer vision algorithms examine images and make sense of what these images depict. Current computer vision algorithms  are able to interpret images at the level of a typical middle school student for many image interpretation tasks.  Recent advances in computer vision have led to rapid technological advances which are still unfolding but affect not only the technology industry, but education,  national security and health care. However, these new algorithms are as yet poorly understood and do not describe how natural learners such as a typical middle school student learn to understand the visual world.  This proposal draws together a team of cognitive psychologists, neuroscientists, and computer scientists to develop a new class of algorithms for computer vision inspired by the way people learn.  <br/><br/>The key insight of this proposal is that human learners, unlike many leading computer vision techniques, make extensive use of the temporal structure of visual experience to extract structure.  In the real world the image  on the human retina is almost never static.   Changes in eye position and movements of the head and body create a rich and complex temporal structure over a range of scales from hundreds of milliseconds up to days and weeks. This proposal a) develops databases of realistic and dynamically changing images in the real world and in immersive virtual reality environments, b) develops computational models for learning visual representations from temporally structured experiences  and, c) examines the brain structures supporting representations integrating time and space across scales using fMRI. The algorithms pursued in this project are inspired by recent theoretical work in the neuroscience of scale-invariant memory.  However, because the databases will be made publicly available, other researchers will be able to develop other algorithms that exploit temporal and spatial correlations.  Taken together, these efforts are intended to catalyze a new generation of techniques for human-like machine learning algorithms with applications in computer vision."
"1652159","CAREER: Scalable Neuromorphic Learning Machines","CCF","Special Projects - CCF, Software & Hardware Foundation, IntgStrat Undst Neurl&Cogn Sys","02/15/2017","09/09/2019","Emre Neftci","CA","University of California-Irvine","Continuing Grant","Sankar Basu","01/31/2022","$563,663.00","","eneftci@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","2878, 7798, 8624","1045, 7798, 7945, 8089, 8091","$0.00","Machine learning algorithms based on artificial neural networks significantly advanced our ability to solve human-centric cognitive tasks at or above human proficiency. Neuromorphic hardware that emulate the biological processes of the brain on a physical, electronic substrate are emerging as ultra low-power alternatives for performing such tasks where adaptability and autonomy are critical. However, neuromorphic hardware lacks general and efficient inference and learning models of the type that empower artificial neural networks, while being compatible with the spatial and temporal constraints of the brain. This research will bridge isolated fields of machine learning and neuromorphic engineering, and address the energetic and performance merits of computing under physical constraints on communication, precision, retention and failures. The solutions sought to meet these challenges will outline the principles for designing continuously learning hardware, resistant to soft errors and failures of future and emerging computing and memory technologies.This interdisciplinary effort will bring a multifaceted skill set to students and researchers alike, and impact many domains of embedded computing, such as brain implants for detecting and alleviating neurological conditions, implantable prosthetics, assistive robots capable of learning and performing human-level cognitive tasks, as well as defense and surveillance related workloads. To encourage young generations to this approach, the PI will 1) organize hands-on workshops, 2) initiate a student-driven project for developing educational tools targeted for teaching K-12 students the building blocks of spike-based deep learning, and 3) offer public video and lab-based courses on neuromorphic intelligence, including hands-on experiments with cutting edge neuromorphic hardware for students.<br/><br/>The proposed approach will study the stochastic nature of biological neurons and synapses to provide a blueprint for inference and learning machines compatible with the digital and mixed-signal neuromorphic hardware. The goals of this vertically-integrated project will be achieved by devising: 1) Spike-based algorithms guided by statistical machine learning theory that operate on information that is locally available to the underlying physical and neural processes that achieve or surpass the performance of equivalent learning algorithms in deep artificial neural networks; 2) Dedicated scalable neuromorphic hardware architectures for ultra low-power, continuously learning, which are key to adaptive behavior in embedded real-time behaving systems; 3) Rules governing the organization of attention and working memory in the brain using insights obtained from neural networks models equipped with dynamic feedback loops. In tandem with the breakthroughs in deep recurrent neural networks, this project aims to create unprecedented transfer of knowledge, sparking the foundations for novel computers that proactively interpret and learn from real-world data, solve novel problems using what they learned, and operate with the efficiency and proficiency of the human brain."
"1652491","CAREER: Beyond Worst-Case Analysis: New Approaches in Approximation Algorithms and Machine Learning","CCF","Algorithmic Foundations","03/15/2017","03/12/2020","Aravindan Vijayaraghavan","IL","Northwestern University","Continuing Grant","A. Funda Ergun","02/28/2022","$368,532.00","","aravindv@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7796","1045, 7926","$0.00","Combinatorial optimization problems such as clustering, and unsupervised learning of probabilistic models are important computational problems that arise in diverse areas including machine learning, computer vision, operations research and data analysis. However, there is a large disconnect between our theoretical and practical understanding of these problems -- while theory tells us that many interesting computational problems in combinatorial optimization and machine learning are intractable in the worst case, practitioners in areas such as machine learning and computer vision have made significant progress in solving such theoretically hard problems. This project focuses on bridging the fundamental gap between theory and practice by developing paradigms and machinery that will allow us to reason about the performance of algorithms on real-world instances. This research has the potential to have broad impact on both theory and practice of computational problems across different areas of computer science, machine learning and statistics. The project will involve students at all levels of research, will integrate aspects of average-case analysis in both graduate and undergraduate courses, and will include outreach activities in high schools in Evanston and the broader Chicago area.<br/><br/>The PI will study several problems in machine learning and combinatorial optimization by using realistic average-case models and smoothed analysis. Broad goals include designing new model-independent algorithms with provable guarantees for realistic average-case models of graph partitioning and clustering and challenging average-case settings where there is no unique or planted solution. These algorithms will also lead to new algorithmic techniques for learning probabilistic models such as mixtures of Gaussians and stochastic block models that are robust to various kinds of modeling errors and noise. Another focus of the project is on developing new efficient algorithms for learning latent variable models and for reasoning about the performance of algorithms using smoothed analysis."
"1710009","SNNnow: Probabilistic Learning for Deep Spiking Neural Networks: Foundations and Hardware Co-Optimization","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/01/2017","03/20/2020","Durgamadhab Misra","NJ","New Jersey Institute of Technology","Standard Grant","Anthony Kuh","07/31/2020","$380,000.00","Alexander Haimovich, Osvaldo Simeone","dmisra@njit.edu","University Heights","Newark","NJ","071021982","9735965275","ENG","7607","1653","$0.00","Overview: Deep neural networks (DNN) have become the de-facto standard tool to carry out complex learning<br/>tasks. DNNs belong to the second generation of artificial neural networks (ANNs), which rely on neurons<br/>that implement memory-less non-linear transformations of the synaptic inputs. Motivated by the biological<br/>analogy with the behavior of neurons in the brain, the third generation of neural networks, also referred to<br/>as Spiking Neural Networks (SNNs), was introduced in the nineties. In SNNs, synaptic input and neuronal<br/>output signals are spike trains. This proposal argues that the time for the use of SNNs as machine learning<br/>tools has come, and sets forth a systematic approach for the design and implementation of SNNs as learning<br/>and inference machines.<br/><br/>Intellectual merit: SNNs have a number of unique advantages as compared to ANNs: (i) They are event-based<br/>systems with natural sparsity properties, which have the potential to make deep learning machines feasible for<br/>energy-limited devices; (ii) They are uniquely capable to natively process data that comes in the form of timeencoded<br/>processes, for example, from bio-inspired sensors. The main goal of this project is the establishment<br/>of a theoretical framework to enable the design of flexible spike-domain learning algorithms that are tailored<br/>to the solution of supervised and unsupervised cognitive tasks, as well as their co-optimization on nanoscale<br/>hardware architectures. To this end, this project puts forth a principled probabilistic framework based on the<br/>graphical formalism of Directed Information Graphs.<br/><br/>Broader impact: The outcome of this research is expected to have a profound impact on the increasing number<br/>of practical applications that are based on the processing of time-encoded signals, including biological sensors<br/>and next-generation communication systems, and/or that require the adoption of computing solutions with a<br/>significantly smaller power budget as compared to conventional DNNs. The research methodology is based<br/>on a multi-disciplinary approach that integrates machine learning, information theory, probabilistic graphical<br/>models, neuromorphic computing and device/system architecture at the nanoscale. The educational plan at<br/>the home institution targets both undergraduate and graduate students via hands-on learning and experimentation<br/>activities."
"1724197","S&AS: INT: RoboBees 2.0 Towards Autonomous Micro Air Vehicles","IIS","S&AS - Smart & Autonomous Syst","09/01/2017","02/28/2020","Gu-Yeon Wei","MA","Harvard University","Standard Grant","David Corman","02/28/2021","$400,000.00","Robert Wood, David Brooks, Vijay Janapa Reddi, Scott Kuindersma, Ryan Adams","guyeon@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","039Y","046Z","$0.00","In 2009, a group of researchers from Harvard led an NSF Expeditions in Computing project to build a colony of flapping-wing robots, called RoboBees, motivated by the multidisciplinary challenges associated with building and controlling effective robotic insects. The research has been exciting and it has tickled the imagination of many ""young and old"" through numerous museum exhibits and outreach activities. The severe inherent constraints associated with building at-scale flying robotic insects required many innovations and new technologies at each step. For example, a new manufacturing process called pop-up MEMS was developed to enable mass production of small-scale, foldable devices. New electronics were developed to flap artificial insect-scale wings. A new small-scale computer chip (called the BrainSoC), connected to various sensors, was created to control the robot. The culmination of this work has been exciting demonstrations of RoboBees hovering and maneuvering about within carefully controlled environments. The next phase of this work is to imbue these robots with machine intelligence and autonomy: RoboBee 2.0. The main objective of this proposal will be to teach the RoboBees to fly autonomously.<br/><br/>Over the past 10 years, while roboticists have been busily building small-scale robots, there has been a surge of activity in machine learning that has led to rapid advances in machine perception and control. For example, the recent success of deep learning can be attributed to the virtuous cycle of (i) more and higher quality data; (ii) faster parallel computation; and (iii) more efficient learning algorithms. The time is ripe to combine these threads of research to develop machine learning-enabled flight control and perception for RoboBees. This project brings together a multidisciplinary team of experts from different engineering backgrounds to build the next generation of RoboBees. The project seeks to push the envelope by targeting the RoboBees platform, which introduces flight dynamics and sensitivity requirements beyond the bleeding edge of what is possible using off-the-shelf components. This effort builds on the existing experimental RoboBee platform at Harvard built with special onboard electronics, which will be used to record large volumes of flight data. This data can then feed exploration of machine learning flight control algorithms, which begins with simple hovering before tackling more challenging maneuvers such as obstacle avoidance and object tracking. Since hand tuning conventional control algorithms is overly cumbersome, focus will be on modern computing paradigms that can be taught rather than programmed. Development and demonstration of autonomous flight control based on deep learning for insect-scale flapping-wing robots will broadly impact the fields of microrobotics, machine learning, energy-efficient computing, and a broad array of autonomous systems, further extending capabilities of autonomy, to a broad range of robotic platforms, from regular vehicles to tiny robots of diverse configurations and applications."
"1713973","EAPSI: A Machine Learning Approach to Lunar Spacecraft Trajectory  Optimization","OISE","EAPSI","06/01/2017","05/08/2017","Christopher Sprague","NY","Sprague                 Christopher    I","Fellowship Award","Anne Emig","05/31/2018","$5,400.00","","","","Troy","NY","121803599","","O/D","7316","5921, 5978, 7316","$0.00","This research will investigate innovative spacecraft trajectory optimization methods at the Japanese Aerospace Exploration Agency (JAXA) in collaboration with Dr. Yasuhiro Kawakatsu for the upcoming lunar small-spacecraft mission, EQUULEUS, which will be launched aboard NASA's Space Launch System (SLS) rocket at the end of 2018. The findings may enable space missions that were once thought to be impossible, leading to more exotic and exciting opportunities for science collection. This mission will also further scientists understanding of the radiation environment surrounding Earth by imaging its plasmasphere and measuring its distribution, which may provide important insight for protecting both humans and electronics from radiation damage during long space journeys. This research explores transformative concepts, combining machine learning and trajectory optimization, two subjects which, in combination, have been largely unexplored. Collaboration with JAXA is a unique opportunity to further this research, as it is a recognized trajectory design leader and has extensive mission experience with low-thrust and low-energy spacecraft. <br/><br/>The spacecraft will insert itself into a stable orbit about the L2 Lagrange point of the Earth-Moon system through a cislunar trajectory, exploiting the topological stability of the Earth-Moon system's effective potential through low-energy pathways. A large data set of optimal control trajectories will be generated through conventional trajectory optimization methods (i.e. direct methods and indirect methods). Once the data set of state-control pairs is generated, an artificial neural network (ANN) will be trained on the data set. Through training, the ANN develops a spatial control policy that can be implemented in real-time. The spacecraft, at any moment in time, will perceive its environment and take actions (i.e. throttle its thrusters) accordingly. This control method is analogous to how organisms behave in nature. Just as a simple house fly is able to navigate to its food source, making decisions in real-time, a spacecraft should be able to do the same when trying to achieve its objective.<br/><br/>This award, under the East Asia and Pacific Summer Institutes program, supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science."
"1657477","CRII: AF: Algorithms for Noise-Tolerant Function Testing with Applications to Deep Learning","CCF","CRII CISE Research Initiation","03/01/2017","02/23/2017","Grigory Yaroslavtsev","IN","Indiana University","Standard Grant","Tracy Kimbrel","02/28/2021","$174,553.00","","gyarosla@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","026Y","7796, 7926, 8228","$0.00","Machine learning has emerged as an important area of computer science, which has a potential significantly to change our lives and society. In deep learning, one needs to rely on being able to quickly test the properties of objective functions. The goal of this project is to develop algorithms for testing analytic properties of high-dimensional functions. Better understanding of properties of optimization objectives used in deep learning will enable researchers in the field to make more educated decisions regarding the choice of optimization methods. It will simplify and introduce rigor in the art of parameter tuning that plays key role in achieving high performance in training deep neural nets. The framework for approximate algorithmic functional analysis (Lp-testing) developed by the PI that forms the starting point for this research has been taught in courses on learning theory and algorithms for big data at the University of Pennsylvania and University of Buenos Aires. Together with the outcomes of the research in this proposal it will be included into M.S./Ph.D. classes on foundations of data science and algorithms for big data at Indiana University taught by the PI.<br/><br/>The PI will develop ultra-efficient algorithms for assisting humans in their understanding of analytic properties of high-dimensional functions and objectives used in deep learning. Three main goals and related challenges in the design of such tools are:(1) Performing algorithmic analysis of local properties of deep learning objectives in the absence of clear global structure (2) Enabling rigorous analysis of analytic properties of functions based on noisy data (3) Introducing tolerance to sampling errors in function evaluations arising in deep learning applications for performance reasons. The project will involve development of new mathematical methods for understanding how global properties of noisy functions such as monotonicity, convexity and Lipschitzness are affected by projections onto random low-dimensional linear subspaces. It will suggest choices of distributions for generation of such subspaces in order to best preserve the desired properties. A rigorous study of fundamental advantages of data-dependent methods will be conducted as a separate part of the project."
"1652950","CAREER: Blending Deep Reinforcement Learning and Probabilistic Programming","IIS","Robust Intelligence","03/01/2017","06/29/2020","David Wingate","UT","Brigham Young University","Continuing Grant","Rebecca Hwa","02/28/2022","$421,009.00","","wingated@cs.byu.edu","A-285 ASB","Provo","UT","846021231","8014223360","CSE","7495","1045, 7495, 9251","$0.00","In reinforcement learning (RL), autonomous agents (such as disaster recovery robots, self-driving cars or unmanned aerial vehicles) must simultaneously learn about an unknown environment while acting in that environment.  Deep reinforcement learning is a variant of RL that leverages the power of deep neural networks (DNNs) to learn both to extract information from sensors and how to transform that information into optimal actions.  The combination of RL and deep learning has generated impressive advances, but it is expensive, both in terms of data and in terms of computation: typical agents only learn after tens or hundreds of millions of interactions with an environment, making most algorithms unusable in anything but a fast simulator.  This stands in stark contrast to humans attempting the same tasks, who can perform well after only a few minutes of practice (or even merely watching another human practice for a few minutes).<br/> <br/>By its nature, this type of machine learning research builds tools for others to use. To connect these tools with disciplines outside of theoretical computer science and to improve outreach and education, this work integrates a new student exchange program, intended to both export results and to import technical challenges from other fields. The plan is rounded out with a mix of undergraduate research opportunities, competitions, and project-oriented classes focused on real-world systems and data -- all intended to spark excitement and communicate a hope for a better world through improved technology.<br/><br/>This work seeks to improve deep RL by addressing two fundamental issues: first, how to reduce the amount of data and computation needed by deep RL, and second, how to improve deep RL's ability to solve complex tasks by incorporating model-based prior knowledge.  The technical strategy builds on ideas from cognitive science, mimicing three key human cognitive capabilities lacking in current deep RL algorithms: (1) humans' native ability to build models of the world, which allows them to (2) transfer knowledge from previous experience via abstraction, and (3) reason explicitly about their own uncertainty.<br/> <br/>To accomplish this, this work combines the strengths of two frameworks: deep neural networks and Bayesian models.  The DNNs provide low-level signal processing, flexible and learnable model components, and powerful building blocks for discriminative inference, while the Bayesian models provide high-level reasoning about objects, causality, and theory of mind in a coherent probabilistic framework that can deal explicitly with uncertainty.  These capabilities are delivered by improved probabilistic programming frameworks that enable both the necessary models and algorithms: probabilistic programming permits the construction complex probabilistic models, and provides natural opportunities for integration with DNNs through automated inference compilers."
"1715017","III: Small: Integrating and Interpreting Heterogeneous Genomic Data Through Deep Learning","IIS","Info Integration & Informatics","09/01/2017","09/01/2017","Xiaohui Xie","CA","University of California-Irvine","Continuing Grant","Sylvia Spengler","08/31/2021","$470,796.00","","xhx@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7364","7364, 7923, 7924","$0.00","Comprehensive identification of all functional elements encoded in genomes is a fundamental need in both basic and applied biological research. Although the coding regions of genomes are well understood, the noncoding regions, representing over 98% of mammalian genomes, are far less studied, but hold the key to understanding gene regulation, evolution, genetic basis of complex phenotypes, etc. The goal of this project is to develop computational methods to infer the function of noncoding sequences by leveraging the plethora of data from publicly available genomic data and state-of-the-art algorithms from machine learning.  These algorithms can greatly expand the utility of existing genomic data, improving the accuracy of annotating pathogenicity of noncoding variants, and offering a new way of studying grammars of gene regulation encoded by noncoding sequence. The project will additionally create opportunities to facilitate interactions between biologists and computer scientists, and offer interdisciplinary training for both undergraduate and graduate students, especially those from traditionally underrepresented groups.<br/><br/>The goal of this project is to develop a new computational framework based on deep learning to understand noncoding sequences. Over the past few years, researchers have generated thousands of genome-scale datasets on chromatin accessibility, histone modifications, DNA methylation, protein-binding, and others, spanning a broad range of tissue and cell types. This project will integrate these heterogeneous datasets to derive a comprehensive characterization of noncoding sequence through innovative machine learning algorithms based on convolutional and recurrent neural nets, and deep generative models. The PI will develop deep learning algorithms to map the relationship between noncoding sequences and the diverse genomic measurements, learn chromatin states and discover novel functional elements from these measurements, and predict effects of noncoding genetic variants. Training a flexible and scalable learning model with large amounts of data provides a way of characterizing noncoding sequences in an unbiased and robust fashion, and offers a better chance of extracting complex regulatory rules encoded within noncoding sequences than conventional methods. This project will provide the genomics community with a versatile, modular, open-source toolbox of software packages, with the goal of greatly improving the accuracy of current genome analyses."
"1716609","RI: Small: Efficient Learning and Inference with Perturbations","IIS","Robust Intelligence","09/01/2017","08/17/2017","Jean Honorio","IN","Purdue University","Standard Grant","Rebecca Hwa","08/31/2020","$272,979.00","","jhonorio@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7495","7495, 7923","$0.00","Learning and inference drives much of the research in many diverse domains, such as natural language processing, computer vision, speech processing and computational biology. In these fields, complex models are required in order to better represent real-world objects (e.g., sentences, images, speech, proteins). As such, one aims to obtain more representational power by expressing objects as the interaction of a large number of constituent elements. While producing more realistic models, this also increases the computational cost of inferring such objects, as well as of learning such inference models from data. The situation worsens as real-world objects become large scale, which opens the opportunity to investigate the use of randomized algorithms to make learning and inference more computationally efficient. This project will also provide education and outreach opportunities through a Hands-on Learning Theory course, undergraduate involvement in research and workshops at major conferences on the topic of learning and inference.<br/><br/>The goal of this project is to develop novel randomized polynomial-time algorithms for learning and inference in large-scale structured prediction problems. The project aims to analyze maximum margin models, maximum a-posteriori perturbation models, latent variable models, and the relationship between regularization and different notions of perturbation. The project makes use of theoretical methods for creating new algorithms with practical advantages over current methods. The project aims to produce algorithms that work in polynomial-time, use a small sufficient and necessary number of training samples, and have a guarantee of small generalization error. All the software produced in this project will be open-sourced, and made available for download."
"1715122","AstroML: Machine Learning for Astrophysics","AST","EXTRAGALACTIC ASTRON & COSMOLO, CDS&E-MSS","08/15/2017","08/07/2017","Andrew Connolly","WA","University of Washington","Standard Grant","Nigel Sharp","07/31/2021","$398,888.00","Zeljko Ivezic, Jacob Vanderplas","ajc@astro.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1217, 8069","1206, 7433, 8084, 9263","$0.00","Astronomy has entered an era of massive data streams, with catalogs containing hundreds of millions of stars and galaxies measured at thousands of time-steps with hundreds of attributes to be analyzed.  To extract knowledge from these large and complex data sets we must account for noise and gaps, and understand if and when we may have detected a fundamentally new physical phenomenon.  The problem is not solely the size of the data, but a basic question of how to discover, represent, visualize and interact with the knowledge that these data contain.  Astronomical data provide a popular testbed for developing methods applicable throughout the physical and life sciences.<br/><br/>astroML is an open source machine-learning library that addresses all of the challenges, providing a publicly available repository for fast python implementations of statistical routines for astronomy, as well as examples of astrophysical data analyses using techniques from statistics and machine learning.  In the three years since its release, astroML has been installed over 21,000 times.  The current project will further develop astroML into a general machine learning toolkit for the next generation of astrophysical surveys, adding code examples and tutorials, exploiting multicore and multiprocessing hardware, and supporting the second edition of the text ""Statistics, Data Mining, and Machine Learning in Astronomy: A Practical Python Guide for the Analysis of Survey Data"".  Algorithms to be developed include approximate Bayesian computation, hierarchical Bayes, an interface to deep learning algorithms, and modifying the regression and regularization code to account for uncertainties within the data.<br/><br/>All developed algorithms will be publicly available, and astroML has already been used in cancer research and analysis of the securities market, and to teach data science in astronomy.  The refactored code can be used to teach both the statistics and software engineering techniques needed for large scale machine learning."
"1652131","CAREER: Hashing and Sketching Algorithms for Resource-Frugal Machine Learning","IIS","Info Integration & Informatics","05/01/2017","05/12/2020","Anshumali Shrivastava","TX","William Marsh Rice University","Continuing Grant","Wei Ding","04/30/2022","$393,161.00","","as143@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7364","1045, 7364","$0.00","Modern applications are constantly dealing with datasets at terabyte scale, and the anticipation is that very soon it will reach petabyte levels.  The size and dimensionality of current datasets have made machine learning (ML) models significantly large and complex, which adds to the existing problems. Classical approaches to learning and inference fail to address new concerns of computational resources, storage limitations, network communication constraints, energy efficiency, real-time latency, etc.  This project focuses on basic design and implementation of (exponentially) resource-frugal and scalable machine learning algorithms which are ideally suited for current big-data constraints.<br/><br/>This project leverages probabilistic hashing techniques for advancing the state-of-the-art machine learning algorithms. The focus is on redesigning existing machine learning pipelines to make them amenable to the hashing speedup. Apart from being exponentially cheap, the designed algorithms are also massively parallelizable. The three primary objectives are:  1) Computationally Efficient Deep-Learning and Kernel-Based Learning via Hashing, 2) Sketching Algorithms for (Exponentially) Compressing Machine Learning Models, and 3) Improving Efficiency of Hash Functions. This project capitalizes on several recent ideas, including asymmetric hashing, hash-based kernels, densified hashing schemes, sub-linear adaptive sampling, and adaptive sketching, to push learning algorithms to the extreme-scale.  By creating a unique bridge between probabilistic hashing and machine learning, this project further enhances the current understanding of tradeoffs involving computations, space, and accuracy."
"1704834","SHF: Medium: A Cloudless Universal Translator","CCF","Software & Hardware Foundation","06/01/2017","07/25/2017","David Brooks","MA","Harvard University","Continuing Grant","Yuanyuan Yang","05/31/2022","$1,000,000.00","Gu-Yeon Wei, Alexander Rush, Paul Whatmough","dbrooks@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7798","7924, 7941","$0.00","This project explores the research foundations necessary to build a universal language translator on a portable computing device for secure private use without the need for reliance on cloud servers. Transformative developments in both machine learning and computer hardware design have made this exciting challenge feasible. The project will nurture a true bidirectional co-design process between researchers in both fields. The broader impacts of the project include: 1) the practical applications of widely available language translation technology, and 2) the training of graduate engineers who have specialization in machine learning as well as hardware and circuit design, skills in broad demand in US industry.<br/><br/>The problem of developing hardware to fit deep learning models is not simply one of fitting current machine learning models on current circuit technology, as the models are much too large, too slow, and too energy-hungry. This project will need to develop novel machine learning techniques that take these factors into account. Machine learning researchers mostly optimize for accuracy; however, the project goal will require considering trade-offs on model size, speed, and computation. Conversely, the hardware design will have to consider and exploit the unique properties of the neural models, such as high-tolerance to certain types of noise, repeated computational structure, and non-linear interactions. The research approach includes three major areas for interaction: model compression, approximation in architecture, and training for unreliable hardware. Succeeding in these goals will be necessary to build a successful on-device system."
"1652866","CAREER: Designing Ultra-Energy-Efficient Intelligent Hardware with On-Chip Learning, Attention, and Inference","CCF","Software & Hardware Foundation","03/15/2017","03/26/2020","Jae-sun Seo","AZ","Arizona State University","Continuing Grant","Sankar Basu","02/28/2022","$376,515.00","","jaesun.seo@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7798","1045, 7945, 8089","$0.00","Building intelligent computers that can perform cognitive tasks (e.g., learning, recognition) as well as humans do has been a long-standing goal of computing research. State-of-the-art deep learning and neuromorphic algorithms have recently advanced the software performance for cognitive applications. However, such algorithms are computation-memory-communication intensive, which makes the hardware design challenging to perform low-power real-time training and classification on portable platforms. Furthermore, to optimize system-level power, efficient power delivery and supply voltage regulation of such large-scale hardware systems also becomes a critical concern. This project will address these challenges across multiple disciplines of hardware and software design, towards the overarching goal of building brain-inspired intelligent computing systems that are ultra-energy-efficient for various cognitive tasks in computer vision, speech, robotics and biomedical applications. The success of this research is likely to impact many user-centric computing systems in  society and industry, including wearable, mobile, and edge computing. This project also entails integrative education and outreach plans through a new interdisciplinary coursework development, undergraduate/graduate student training, and a summer outreach program for high school students.<br/><br/>In this project, energy-efficient circuits, architectures and algorithms will be designed to incorporate learning, attention and inference computations in area-/power-constrained mobile/wearable hardware platforms. The particular technologies that will be developed to achieve large improvement in energy-efficiency include: (1) computation redundancy minimization of state-of-the-art deep learning algorithms with bio-inspired attention models, (2) novel memory compression schemes that apply to both software and hardware implementation, (3) real-time on-chip learning methods that consume low power on mobile/wearable devices, (4) efficient on-chip voltage regulators that can adapt to abrupt changes in cognitive workloads, and (5) cross-layer optimization of circuit, architecture and algorithm. The outcomes of this research will feature new very-large-scale integration (VLSI) systems that can learn and perform cognitive tasks in real-time with superior power efficiency, opening up possibilities for ubiquitous intelligence in small-form-factor devices."
"1652944","CAREER:Integrated Research and Education on Delta-Sigma Based Digital Signal Processing Circuits for Low-Power Intelligent Sensors","ECCS","CCSS-Comms Circuits & Sens Sys","02/15/2017","02/09/2017","Wei Tang","NM","New Mexico State University","Standard Grant","Shubhra Gangopadhyay","01/31/2022","$500,000.00","","wtang@nmsu.edu","Corner of Espina St. & Stewart","Las Cruces","NM","880038002","5756461590","ENG","7564","096E, 1045, 9150","$0.00","Applications of low-power integrated intelligent sensors have been prolific in recent years, including, for instance, in environmental observation, security surveillance, infrastructure monitoring and communication, and biomedical health care monitoring. Although such sensors usually have wireless data communication capability, transmitting raw sensed data is usually not an option because of limited battery power, since in a wireless sensor the radio communication power is usually much higher than the signal processing power. Therefore, intelligent sensors need to be capable of providing preprocessing of raw data based on signal processing algorithms and sending only the processed results. Another inevitable challenge in biomedical applications is oversampling. This is because the targeted biomedical signals usually have a wide fractional bandwidth in the frequency spectrum, and require time-frequency analysis. Therefore, in these applications, the Analog to Digital Convertor (ADC) have to apply oversampling in order to avoid the signal distortion introduced by anti-aliasing filters due to the trade-off between fast roll-off and flat group delay. The requirements of both oversampling and the higher resolution also exacerbate the power problem. One promising solution to the above challenges is Delta-Sigma technology. The research objective of this CAREER proposal is to apply Delta-Sigma based Integrated Circuit (IC) design in Digital Signal Processing (DSP) to solve circuit power and area problems for the next-generation of low-power intelligent sensors. The proposed research will have a broad impact on next-generation pervasive computing and ubiquitous sensing applications. The educational objectives of this CAREER proposal are promoting active, inquiry-based learning, introducing students to interdisciplinary study and research, and preparing them to meet the future expectations of both academia and industry by providing them a complete skill set including system design, assembly, verification, and optimization, and making cutting-edge research projects more accessible to minority and underrepresented groups of students.<br/><br/>The proposed integrated linear signal processing circuits are based on Delta-Sigma re-modulation using digital Delta-Sigma modulators. Delta Sigma linear processing circuits such as adders, coefficient multipliers, and filters will be designed, fabricated, characterized, and compared to conventional signal processing systems. In particular, based on the preliminary results, the research will target a biomedical signal processor applying the Cross-Frequency-Coupling (CFC) algorithm, which includes designing of FIR and IIR Band-pass filters, Hilbert filters, and Coordinate Rotation Digital Computer (CORDIC) circuits. The proposed system with reconfigurable resolution will be investigated in order to evaluate the benefit of introducing adaptive resolution sensors to save sensing and communication power. The proposed research significantly advances the state-of-the-art in integrated intelligent sensors by employing ideas from Delta-Sigma Modulation (DSM), Digital Signal Processing, and Integrated Circuits design in a cross-disciplinary fashion.  Specifically, the proposed research will lead to a highly power-efficient circuit and system architecture by addressing the following objectives: 1) studying design and properties of Delta-Sigma based adders and coefficient multipliers for a theoretical understanding of how they can be applied to various circuit architectures, 2) investigating Delta-Sigma based digital filters and neural network systems and exploring the feasibility of applying the proposed circuits in machine learning, and 3) developing an integrated circuit of Cross-Frequency-Coupling algorithm for electroencephalograph (EEG) signal processing using Delta-Sigma digital signal processing circuits, and evaluating the system performance compared with conventional architecture."
"1744019","Workshop Proposal:  Machine Learning and Discovery Science","OAC","BD Spokes -Big Data Regional I, GVF - Global Venture Fund","08/01/2017","08/04/2017","Aram Galstyan","CA","University of Southern California","Standard Grant","Beth Plale","01/31/2018","$50,000.00","Naira Hovakimyan","galstyan@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","024Y, 054Y","5939, 5980, 7556, 8083","$0.00","The emergence of big data has been transformational in many areas in science and engineering - biology, health sciences, material science, physics, and so on. At the heart of this transformation is statistical machine learning, a subfield of computer science that aims at studying and developing algorithms that can analyze large volumes of data. The goal of this international workshop is to bring together researchers, both from USA and Armenia, who work on machine learning (ML) and other scientific disciplines that are poised to benefit from the recent advances in ML.  <br/><br/>The workshop will include participation from leading experts in a number of disciplines. In machine learning, the workshop will cover topics such as deep learning, tensor methods, unsupervised learning with high dimensional data, and so on. In computational social sciences, the topics will include social network analysis, behavioral modeling, modeling of socio-technical systems. And in computational biology, the topics will cover gene expression analysis, computational neuroscience, predictive diagnostics. The workshop will serve as a bridge to get these communities talking to one another and explore collaborative research.  The workshop will provide a forum for the participating researchers to formulate a research agenda that will help to utilize recent advances in ML in data-intensive disciplines. Second, the workshop will support participation of senior graduate students and early career scientists. Finally, the workshop will promote scientific cooperation between the American and Armenian researchers.<br/><br/>This award is cofunded by the Office of International Science and Engineering."
"1717775","CSR:  Small:  Collaborative Research:  EUReCa:  Enabling Untethered VR/AR System via Human-centric Graphic Computing and Distributed Data Processing","CNS","CSR-Computer Systems Research","09/01/2017","08/03/2017","Xiang Chen","VA","George Mason University","Standard Grant","Erik Brunvand","08/31/2020","$249,799.00","","xchen26@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7354","7923","$0.00","Virtual Reality (VR) and Augmented Reality (AR) devices, especially their mobile versions are newly emergent technologies. However, a major challenge that VR/AR technologies faces is the gap between the increasing needs for graphic and data processing and the limited computing capability of the mobile hardware. Two researchers from GMU and Duke form a team to develop an innovative VR/AR system, namely, ""EUReCa"", which tackles the challenge of human-centric graphic processing and distributed data processing. This research studies VR/AR system design using standard workloads to understand the computation source utilization that leads to development of usage model, and reduces the computation loads via task allocation, thereby enhancing the efficiency and scalability of computation. The research outcomes will benefit both research and industry at large by integrating the innovations of human interaction and advanced data processing technologies. The education plan enhances existing curricula and pedagogy by integrating interdisciplinary modules on computer graphic, embedded systems, and machine learning with newly developed teaching practices, and gives special attention to women and underrepresented minority groups.<br/> <br/>The project performs three tasks. Task 1 models the computation resource utilization of VR/AR systems by considering the system configuration and dynamics of user operations. Task 2 explores efficient human-centric graphic rendering framework for reducing computation loads of VR/AR systems. Task 3 exploits novel schemes to enhance computation efficiency via balancing the computation loads and data allocations in graphic rendering and deep neural network (DNN) applications. The techniques will be evaluated on mobile devices."
"1717657","CSR:   Small:   Collaborative Research:  EUReCa:   Enabling Untethered VR/AR System via Human-centric Graphic Computing and Distributed Data Processing","CNS","CSR-Computer Systems Research","09/01/2017","08/03/2017","Yiran Chen","NC","Duke University","Standard Grant","Erik Brunvand","08/31/2020","$250,000.00","","yiran.chen@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7354","7923","$0.00","Virtual Reality (VR) and Augmented Reality (AR) devices, especially their mobile versions are newly emergent technologies. However, a major challenge that VR/AR technologies faces is the gap between the increasing needs for graphic and data processing and the limited computing capability of the mobile hardware. Two researchers from GMU and Duke form a team to develop an innovative VR/AR system, namely, ""EUReCa"", which tackles the challenge of human-centric graphic processing and distributed data processing. This research studies VR/AR system design using standard workloads to understand the computation source utilization that leads to development of usage model, and reduces the computation loads via task allocation, thereby enhancing the efficiency and scalability of computation. The research outcomes will benefit both research and industry at large by integrating the innovations of human interaction and advanced data processing technologies. The education plan enhances existing curricula and pedagogy by integrating interdisciplinary modules on computer graphic, embedded systems, and machine learning with newly developed teaching practices, and gives special attention to women and underrepresented minority groups.<br/> <br/>The project performs three tasks. Task 1 models the computation resource utilization of VR/AR systems by considering the system configuration and dynamics of user operations. Task 2 explores efficient human-centric graphic rendering framework for reducing computation loads of VR/AR systems. Task 3 exploits novel schemes to enhance computation efficiency via balancing the computation loads and data allocations in graphic rendering and deep neural network (DNN) applications. The techniques will be evaluated on mobile devices."
"1663256","GOALI: Machine Learning Approaches for Supply Chain Decision-Making","CMMI","OE Operations Engineering, GOALI-Grnt Opp Acad Lia wIndus","07/01/2017","06/08/2017","Martin Takac","PA","Lehigh University","Standard Grant","Georgia-Ann Klutke","06/30/2021","$328,846.00","Lawrence Snyder, Ioannis Akrotirianakis","mat614@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","ENG","006Y, 1504","071E, 073E, 078E, 1504, 8023","$0.00","Supply chain refers to the system that moves goods from where they are produced to where they are consumed. This system includes manufacturing, assembly, warehousing, transportation, and retailing processes. This project will study how to improve efficiency and robustness of the supply chain by using novel machine learning techniques to control the supply chain automatically. The project specifically focuses on the development of decision-making strategies to deal with uncertainties and correct disruptions in the supply chain system. In collaboration with an industrial partner, Siemens, the project will focus on a real supply chain application related to the production and distribution of radiopharmaceuticals (an important component in health diagnostics). The advancements in supply chain and machine learning resulting from this project have potential to benefit a wide range of industries.   <br/><br/>This project studies a new approach for using machine learning (ML) as a tool for optimizing, analyzing and controlling supply chains. Current approach to supply chain operations makes assumptions about statistical distributions of uncertain factors in supply chain and uses predictive techniques or forecasts to estimate the few parameters required to characterize these distributions. These estimated distributions are then used in the analysis and control of the supply chain. This project has a novel approach of combining the data-analysis and supply-chain optimization stages into a single ML algorithm.  The project focuses on two main classes of supply chain problems: production and distribution of radiopharmaceuticals (a core problem for the industrial partner, Siemens), and early warnings and corrective actions for stochastic supply chains. In addition, the project will make methodological contributions to the theory and implementation of ML algorithms, including new loss functions, techniques for using deep learning as a preprocessing step for reinforcement learning, adaptive strategies to handle non-stationary data, and improved initialization methods for ML. The tools and concepts to be develop will be generalizable more broadly, both within and beyond supply chain."
"1718380","AF: Small: Integrated Knowledge Discovery and Analysis Using Sum-of-Squares Proofs","CCF","Algorithmic Foundations","09/01/2017","06/02/2020","Brendan Juba","MO","Washington University","Standard Grant","A. Funda Ergun","08/31/2021","$445,997.00","","bjuba@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7796","7923, 7926, 7927, 9150, 9251","$0.00","Developing approaches to subjects as diverse as advertising, bioinformatics, counterterrorism, fraud detection, politics, sociology, and so on are based on data analysis. This project will develop new algorithms for data analysis with strong guarantees of their correctness and efficiency. Tools based on these algorithms will be usable as-is, without expert knowledge, in innovative data-driven applications far removed from academia. The project will also involve the training of students in advanced techniques for data analysis.<br/><br/>The algorithms considered in this project involve automated reasoning with an algebraic logic known as ""sum-of-squares."" Automated reasoning requires a delicate trade-off between expressiveness and simplicity, to facilitate reasoning that is both fast and effective. Sum-of-squares is capable of expressing much statistical reasoning, and yet is sufficiently simple to allow the design of tractable algorithms for reasoning. This project will consider how these algorithms can be used to reason about an overall distribution or population from a sample of data drawn from it. The main aim of the project is to develop efficient algorithms that guarantee that all of the relevant statistical facts are discovered during data analysis. The project will further develop these algorithms to solve problems in domains such as Computer Vision and Natural Language Processing.<br/><br/>In addition to the development of domain-specific algorithms, the project will consider sum-of-squares reasoning with high-degree polynomials. Although such reasoning in the standard sense is intractable, the project aims to simulate reasoning with such high-degree expressions with the aid of the sample of data from the distribution to be reasoned about. The project will also investigate the expressive power of sum-of-squares with such high-degree expressions. Specifically, the project will investigate whether or not such reasoning can simulate other logics such as resolution (or vice-versa), and will further investigate the extent of its ability to basic capture statistical notions."
"1717431","RI:Small: Unsupervised Discriminatively-Generative Learning:","IIS","Robust Intelligence","08/01/2017","07/27/2017","Zhuowen Tu","CA","University of California-San Diego","Standard Grant","Jie Yang","07/31/2021","$450,000.00","","zhuowen.tu@gmail.com","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7495, 7923","$0.00","Great success has been achieved in obtaining powerful discriminative classifiers via supervised training where humans provide manual annotations to the training data. Unsupervised learning, in which the input data is not accompanied with task-specific annotations, is of great importance since a large number of tasks have no to little supervision. However, it still remains to be one of the most difficult problems in machine learning. A typical unsupervised learning task learns effective generative representations for highly structured data such as images, videos, speech, and text. Existing generative models for unsupervised learning are often constrained by their simplified assumptions, while existing discriminative models for supervised learning are of limited generation capabilities. This project develops a new introspective machine learning framework that greatly enhances and expands the power of both generation and discrimination for a single model. The outcome of the project, introspective generative/discriminative learning, significantly improves the learning capabilities of the existing algorithms by building stronger computational models for a wide range of fields including computer vision, machine learning, cognitive science, computational linguistics, and data mining. <br/><br/>This research investigates a new machine learning framework, introspective generative/discriminative learning (IGDL), which attains a single generator/discriminator capable of performing both generation and classification.  The IGDL generator is itself a discriminator, capable of introspection --- being able to self-evaluate the difference between its generated samples and the given training data. When followed by iterative discriminative learning, desirable properties of modern discriminative classifiers such as convolutional neural networks (CNN) can be directly inherited by the IGDL generator. Moreover, the discriminator aspect of IGDL also produces competitive results in fully supervised classification by using self-generated new data (called pseudo-negatives) to enhance the classification performance against adversarial samples. The training process of IGDL is carried out using a two-step synthesis-by-classification algorithm via efficient backpropagation. Effective stochastic gradient descent Monte Carlo sampling processes for IGDL training are studied. Across three key areas in machine learning including unsupervised, semi-supervised, and fully-supervised learning, IGDL produces competitive results in a wide range of applications including texture synthesis, object modeling, and image classification."
"1737586","SCC-Planning: Pedestrian Safe and Secure Communities with Ambient Machine Vision","CNS","S&CC: Smart & Connected Commun","09/01/2017","08/24/2017","Hamed Tabkhi","NC","University of North Carolina at Charlotte","Standard Grant","Ralph Wachter","08/31/2019","$99,156.00","Shannon Reid, Tao Han, Arun Ravindran, Srinivas Pulugurtha","htabkhiv@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","033Y","042Z","$0.00","This project with the University of North Carolina at Charlotte in cooperation with the Charlotte-Mecklenburg counties addresses community challenges of pedestrian safety and community policing, building on advances in cyber-physical systems (CPS).  As communities adopt technologies such as vision-based traffic cameras and smart traffic signs at intersections, the data from these technologies possess traces of the activity within a community of which a few might need a response because of risk to individual and public safety or suggest a local police response.  Such technologies may provide a more accurate community-wide operational picture.  With this data communities can have a better understanding of itself and within established law and custom will better serve and protect individuals and the public at large.  This planning grant will enable community planners, local government, and businesses along with technologists, urban planners and traffic engineers to explore the potential of these emerging technologies for improving the quality of life of a community.<br/><br/>This planning grant will leverage research in CPS, big data, and urban transportation planning to provide new capabilities for community engagement. It will draw upon technologies from computer vision, machine learning, edge computing, and generally CPS and the Internet of Things. This will set the stage for designing edge computing systems for ambient vision processing at city street intersections with cooperative processing over the entire edge network in a city. The project will advance knowledge of pedestrian and driver behaviors and models, specifically in urban transportation settings. It will enable the study and characterization of driver behaviors for driver-in-the-loop traffic control system. The planned extensive community engagement will facilitate ascertaining community goals and concerns, especially regarding privacy and transportation mobility planning in future community deployment of these proposed technologies."
"1722049","SBIR Phase I:  An Automated Design Flow to Build Energy Efficient Vision Processing and Machine Learning Chips for the Internet of Things","IIP","SMALL BUSINESS PHASE I","06/01/2017","06/09/2017","Dylan Hand","CA","Reduced Energy Microsystems, Inc","Standard Grant","Rick Schwerdtfeger","05/31/2018","$224,576.00","","dhand@remicro.com","264 Dore St","San Francisco","CA","941034308","9739791020","ENG","5371","5371, 8034, 8035","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to bring data-driven decision making to new areas of human interaction with technology. The Internet of Things (IoT) embodies the hardware, software, and systems that enable monitoring and managing objects in the physical world electronically. IoT devices will enable performance optimization of systems and processes, time savings for people and businesses, and quality of life improvements. The total economic impact of the IoT is estimated to exceed $11 trillion by 2025. However, reaching these lofty estimates requires advances in hardware technology, particularly for energy-constrained IoT devices that must gather data, make decisions based on the data gathered, and communicate to a larger system under a limited power budget. The company will develop low power embedded computer vision systems and machine learning algorithms for use in for virtual/augmented reality, drones, surveillance cameras and other applications.  <br/><br/>The proposed project advances the commercialization of new ?timing-resilient? chip technology, which promises unparalleled power efficiency by bringing dynamic voltage scaling to IoT devices with minimal impact on traditional design flows. Changing the operating voltage of a device can lead to significant energy efficiency improvements, yet many circuit designers do not take advantage of this technique due to increased design time and complexity. This proposal focuses on the development of a comprehensive computer-aided design (CAD) flow that transforms existing synchronous designs into more efficient asynchronous timing-resilient designs that support a wide range of voltages. The proposed project addresses three aspects of the automated flow: design for manufacturability and test; analysis of logic cell libraries at lower voltages; and interfacing the new timing-resilient circuits with synchronous circuits. The flow combines simulation, analysis, synthesis, place-and-route, and test with similar efficiency as standard commercial flows. This research aims to limit the additional testing overhead of the converted design and the performance impact of interfacing with traditional circuits to within 10% of synchronous counterparts. The development of this new flow will enable chip designs that are fundamentally more energy efficient and help bring the full power of machine learning to smaller form factors."
"1734868","NCS-FO:Collaborative Research:Decoding and Reconstructing the Neural Basis of Real World Social Perception","SMA","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","08/01/2017","08/07/2017","Max G'Sell","PA","Carnegie-Mellon University","Standard Grant","Kurt Thoroughman","07/31/2021","$490,074.00","Louis-Philippe Morency","mgsell@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","7980, 8624","8089, 8091, 8551","$0.00","Social and affective perception is the critical input that governs how we interact with others during everyday life. Consequently, having a model of the neurobiological basis of social and affective perception is critical for understanding the neural basis of human behavior. The overwhelming majority of our understanding of the neural basis of social and affective perception comes from studies done in artificial lab settings, which cannot capture the richness, complexity, and salience of real-world social interactions. This project aims to fill this gap in knowledge. To accomplish this goal, the researchers will record electrical brain activity from patients undergoing neurosurgical treatment for epilepsy. To determine the region of the brain responsible for their seizures, these patients are implanted with electrodes in various parts of their brain and then they spend 1-2 weeks in the hospital during which they interact with doctors, nurses, friend and family visitors, etc. This award will support research into using the recordings from their brains to understand how these patients perceive and understand the actions, emotions, and communication during these interactions on a moment-to-moment basis. The results of these studies have the potential to transform our understanding of social and affective perception by illuminating the neural basis of these processes during real life, meaningful interactions. The lack of models of the neural basis of natural, real world social and affective perception is a critical impediment to understanding these processes and ultimately a developing treatments for debilitating neurological and psychiatric disorders of social and affective perception, such as autism, post traumatic stress disorder, etc. In addition, through education, mentoring, and teaching, this award will provide an avenue for new researchers to take advantage of the rare and valuable opportunity for basic neuroscientific research provided by direct recordings from the human brain. This research is supported by the EHR Core Research Program, providing funding for fundamental research in STEM learning and learning environments, broadening participation in STEM, and STEM workforce development. <br/><br/>Models of social visual perception developed using unnatural stimuli often assume that neurons have unchanging response sensitivity and are organized into bottom-up hierarchies. While some recent models acknowledge the role of feedback, they remain simplistic with a relatively limited number of core systems and often neglect of the role of social context and dynamic prior knowledge. These models are unlikely to fully generalize to natural social vision where the system can rapidly and actively adapt its response to optimize processing of rich and complex natural visual input. The PI and colleagues will combine intracranial EEG (iEEG) recordings captured during long stretches of natural visual behavior with cutting-edge computer vision, machine learning, and statistical analyses to understand the neural basis of natural, real-world visual perception. The goal of their program of research is to develop the first fully ecologically validated models of social perception. The researchers will use recent advances in iEEG in combination with cutting-edge gaze tracking technology, video analysis tools, and big data statistical and machine learning tools to understand the rapid, complex neural information processing that occurs during real-world social vision. The project will involve decoding the spatiotemporal patterns of neural activity and reconstruct the expressive features of people they see at these different levels on a moment-to-moment basis. The multidisciplinary nature of this project provides an excellent environment for students and postdocs to be trained in computational methods, statistics, and neuroscience. Given the rapid advance of high-level computational and statistical methods in neuroscience, this multidisciplinary training is critical for modern neuroscientists. Enhanced understanding of the mechanisms involved in social cognition has implications for teaching and learning. For example, knowing more about how people form impressions of one another can inform teachers' abilities to recognize and respond to students and other stakeholders in educational settings.<br/><br/>This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE)."
"1737842","EAGER:   SC2: PHY-Layer-Integrated Collaborative Learning in Spectrum Coordination","CNS","Special Projects - CNS","03/15/2017","03/20/2017","Sebastian Pokutta","GA","Georgia Tech Research Corporation","Standard Grant","Monisha Ghosh","02/28/2019","$99,877.00","Matthieu Bloch","sebastian.pokutta@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1714","7363, 7916","$0.00","With the explosion of wireless devices, spectrum is becoming a scarce resource that wireless systems fiercely compete for. To ensure that future civilian and military systems, ranging from connected Internet of Things (IoT) devices to battlefield ad-hoc networks, continue to support services with growing quality, systems must evolve beyond the traditional spectrum licensing model and towards an intelligent spectrum sharing paradigm, in which networks nodes collaborate to efficiently share the spectrum. This radical paradigm shift requires the integration of the latest machine learning advances with the more recent progress in software defined radio, in order to endow wireless devices with the intelligence and agility necessary to realize the vision of efficient unsupervised spectrum sharing. The proposed research aims at addressing this fundamental issue, and will offer ample opportunities to provide interdisciplinary training of students at the intersection of machine learning and communications engineering.<br/><br/>The project envisions a paradigm shift in radio design, which will intertwine agile communications engineering techniques with advanced machine learning algorithms to fuse the traditional physical-layer and link layers into a ""collaboration layer"". Specifically, the approach comprises three key elements: (1) a multi-carrier modulation format at the physical layer that provides the required agility to react to interfering signals; (2) a high-performing modulation recognition software that exploits the latest advances in deep learning and convolutional neural networks to accurately classify the radio frequency signals in the environment; and (3) a decision module exploiting the latest advances in regret minimization online algorithms to achieve high exploration versus exploitation performance in the wireless environment."
"1734907","NCS-FO:Collaborative Research:Decoding and Reconstructing the Neural Basis of Real World Social Perception","SMA","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","08/01/2017","08/07/2017","Avniel Ghuman","PA","University of Pittsburgh","Standard Grant","Kurt Thoroughman","07/31/2020","$460,043.00","","ghumana@upmc.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","SBE","7980, 8624","8089, 8091, 8551","$0.00","Social and affective perception is the critical input that governs how we interact with others during everyday life. Consequently, having a model of the neurobiological basis of social and affective perception is critical for understanding the neural basis of human behavior. The overwhelming majority of our understanding of the neural basis of social and affective perception comes from studies done in artificial lab settings, which cannot capture the richness, complexity, and salience of real-world social interactions. This project aims to fill this gap in knowledge. To accomplish this goal, the researchers will record electrical brain activity from patients undergoing neurosurgical treatment for epilepsy. To determine the region of the brain responsible for their seizures, these patients are implanted with electrodes in various parts of their brain and then they spend 1-2 weeks in the hospital during which they interact with doctors, nurses, friend and family visitors, etc. This award will support research into using the recordings from their brains to understand how these patients perceive and understand the actions, emotions, and communication during these interactions on a moment-to-moment basis. The results of these studies have the potential to transform our understanding of social and affective perception by illuminating the neural basis of these processes during real life, meaningful interactions. The lack of models of the neural basis of natural, real world social and affective perception is a critical impediment to understanding these processes and ultimately a developing treatments for debilitating neurological and psychiatric disorders of social and affective perception, such as autism, post traumatic stress disorder, etc. In addition, through education, mentoring, and teaching, this award will provide an avenue for new researchers to take advantage of the rare and valuable opportunity for basic neuroscientific research provided by direct recordings from the human brain. This research is supported by the EHR Core Research Program, providing funding for fundamental research in STEM learning and learning environments, broadening participation in STEM, and STEM workforce development. <br/><br/>Models of social visual perception developed using unnatural stimuli often assume that neurons have unchanging response sensitivity and are organized into bottom-up hierarchies. While some recent models acknowledge the role of feedback, they remain simplistic with a relatively limited number of core systems and often neglect of the role of social context and dynamic prior knowledge. These models are unlikely to fully generalize to natural social vision where the system can rapidly and actively adapt its response to optimize processing of rich and complex natural visual input. The PI and colleagues will combine intracranial EEG (iEEG) recordings captured during long stretches of natural visual behavior with cutting-edge computer vision, machine learning, and statistical analyses to understand the neural basis of natural, real-world visual perception. The goal of their program of research is to develop the first fully ecologically validated models of social perception. The researchers will use recent advances in iEEG in combination with cutting-edge gaze tracking technology, video analysis tools, and big data statistical and machine learning tools to understand the rapid, complex neural information processing that occurs during real-world social vision. The project will involve decoding the spatiotemporal patterns of neural activity and reconstruct the expressive features of people they see at these different levels on a moment-to-moment basis. The multidisciplinary nature of this project provides an excellent environment for students and postdocs to be trained in computational methods, statistics, and neuroscience. Given the rapid advance of high-level computational and statistical methods in neuroscience, this multidisciplinary training is critical for modern neuroscientists. Enhanced understanding of the mechanisms involved in social cognition has implications for teaching and learning. For example, knowing more about how people form impressions of one another can inform teachers' abilities to recognize and respond to students and other stakeholders in educational settings.<br/><br/>This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE)."
"1738003","Harmonic Analysis and Machine Learning for Emergency Response","DMS","ATD-Algorithms for Threat Dete, ","09/01/2017","09/05/2018","Wojciech Czaja","MD","University of Maryland College Park","Continuing Grant","Leland Jameson","08/31/2020","$249,999.00","John Benedetto","wojtek@math.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","MPS","046Y, Q218","6877","$0.00","The research presented in this project resides in threat detection and disaster management. Significant mathematical contributions to this field are still needed, due in part to the ongoing revolution with complex data problems, known as the Big Data paradigm. This project aims to bridge the big data gap, providing added value to the field while simultaneously expanding the impact of modern mathematics. With this point of view, this project studies potentially predictive disaster scenarios, from radioactive leaks, to modern battlefield issues, to natural disasters. However, the notion of threat in this project is not limited to an earthquake, flood, or nuclear explosion, but rather what impedes the people affected by disasters.  The project research intends to provide efficient ways of disaster aftermath management.<br/><br/>Algorithmic Threat Detection (ATD) is a predictive concept that must be quantified and effectively designed to address major defense problems.  After analysis of recent disaster scenarios, a suite of technologies have been formulated that use mathematics to help mitigate disaster impacts. Machine learning and deep learning are major techniques in this suite, and involve data processing, spectral graph analysis, Schroedinger eigenmap technology with customized non-linear potentials, transport models, and recent innovations, ideas, and results dealing with Fourier scattering transforms, pooling operators, and convolutional neural networks. Essentially, this work develops a toolkit from modern applied harmonic analysis and machine learning. The mathematical rigor in this work will enable us to construct fast and efficient disaster mitigation implementations.  The ideas and results proposed here are new and innovative, and the applications to threat detection are timely and relevant. These methods may also impact other areas of science."
"1723712","I-Corps: Optimized Compiler Applications","IIP","I-Corps","02/01/2017","01/26/2017","Qing Yi","CO","University of Colorado at Colorado Springs","Standard Grant","Steven Konsek","07/31/2017","$50,000.00","Terrance Boult","qyi@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is expected to enhance the performance, productivity, and correctness assurance of computer software development, particularly for computation or data intensive software in areas such as scientific computing, gaming, graph/map processing, data mining, signal processing, and machine learning.  The project has potential to speed and scale these types of applications on a variety of hardware platforms. The correctness assurance of these applications is enhances by having a variety of implementation variants for different platforms automatically generated, while allowing a single easy-to-maintain version of the software being used as the source for the different implementation variants.  These ""automatically improved"" applications can, in turn, enable better knowledge discovery through larger-scale scientific simulations and machine learning, and potentially improve the quality of people's daily lives through better GPS routing and natural language processing implementations.<br/><br/>This I-Corps project will explore the commercial viability of a system that automatically produces efficient implementation variants of computer software on a variety of different hardware platforms.  A graphical user interface is used to permit users to interactively specify the desired application features, provide knowledge about the application source code, and select the appropriate hardware platforms.  The project is unique in its interface for supporting interactive communications among software developers, compiler optimizations, and performance tuning of optimizations for varying platforms. While the existing state-of-the-art largely treats compilers as black boxes that optimize a given software application for a single desired hardware platform, this project packages compiler technologies as a collection of optimization tool sets to be used interactively by developers so that optimizations can be applied at a much finer-grained level, specially customized to suit the needs of each application, and better tuned to exploit the varying capabilities of hardware platforms."
"1736274","Collaborative Research: RUI: Uncovering the Neural Dynamics of Scene Categorization through Electroencephalography, Machine Learning, and Neuromodulation","BCS","Cognitive Neuroscience","08/01/2017","06/29/2017","Michelle Greene","ME","Bates College","Standard Grant","Kurt Thoroughman","07/31/2021","$304,266.00","","mgreene2@bates.edu","2 Andrews Road","Lewiston","ME","042406028","2077868375","SBE","1699","1699, 9150, 9229","$0.00","A long-standing problem in cognitive neuroscience is understanding how we can categorize a novel scene in about the same amount of time that it takes to blink one's eyes. Categorization aids both identifying objects and locating them in cluttered scenes, and thus allows for intelligent action in the world. How do we derive semantically meaningful categories from the raw image pixels? Currently, there is experimental support for multiple mechanisms supporting scene categorization, such as through recognizing the scene's objects or other visual features such as spatial layout, color, or texture. Crucially, substantial correlations exist between all of these proposed features. This make it difficult to disentangle their relative contributions to categorization. For example, if two scenes share an object, they will often also share the texture features associated with that object. In this work, the PI (Dr. Bruce C Hansen, Colgate University) and co-PI (Dr. Michelle R Greene, Bates College) seek to disentangle the contribution of such features, and also to determine when these features become available for use, and how they combine to support scene categorization. By understanding the temporal dynamics of the brain activity related to scene categorization, it will be possible to obtain critical insights into how people rapidly but flexibly extract information from the environment. This work forms a bridge across several disciplines including psychology, cognitive neuroscience, computer vision, and machine learning. As such, the project will engage undergraduate students in truly interdisciplinary training that is at the cutting edge of multiple fields.<br/><br/>This project will make use of high-density EEG combined with machine learning, computational modeling behavioral measures, and advanced neuromodulation to determine how and when the behaviorally relevant features support scene categorization. First, the work will link the encoding of these features to visual event related potentials (vERPs) and also to category information using multivariate classification techniques from machine learning. Taken together, these techniques will allow the PIs to determine the unique contributions of each feature to category-related brain activity over time. A hallmark of intelligent action is flexibility. Therefore, the project will also investigate the flexibility of feature use by manipulating the diagnosticity of information available to observers. These studies will provide insights regarding feature space usage as a function of task demands, as well as the impact of such demands on the time course of feature space availability as indexed by vERPs. Lastly, the project will test for a potential causal role of vERPs to categorization through the use of advanced neuromodulation techniques."
"1736394","Collaborative Research: RUI: Uncovering the Neural Dynamics of Scene Categorization through Electroencephalography, Machine Learning, and Neuromodulation","BCS","Cognitive Neuroscience, Perception, Action & Cognition","08/01/2017","06/29/2017","Bruce Hansen","NY","Colgate University","Standard Grant","Kurt Thoroughman","07/31/2021","$186,708.00","","bchansen@colgate.edu","13 Oak Drive","Hamilton","NY","133461398","3152287457","SBE","1699, 7252","1699, 7252, 9229","$0.00","A long-standing problem in cognitive neuroscience is understanding how we can categorize a novel scene in about the same amount of time that it takes to blink one's eyes. Categorization aids both identifying objects and locating them in cluttered scenes, and thus allows for intelligent action in the world. How do we derive semantically meaningful categories from the raw image pixels? Currently, there is experimental support for multiple mechanisms supporting scene categorization, such as through recognizing the scene's objects or other visual features such as spatial layout, color, or texture. Crucially, substantial correlations exist between all of these proposed features. This make it difficult to disentangle their relative contributions to categorization. For example, if two scenes share an object, they will often also share the texture features associated with that object. In this work, the PI (Dr. Bruce C Hansen, Colgate University) and co-PI (Dr. Michelle R Greene, Bates College) seek to disentangle the contribution of such features, and also to determine when these features become available for use, and how they combine to support scene categorization. By understanding the temporal dynamics of the brain activity related to scene categorization, it will be possible to obtain critical insights into how people rapidly but flexibly extract information from the environment. This work forms a bridge across several disciplines including psychology, cognitive neuroscience, computer vision, and machine learning. As such, the project will engage undergraduate students in truly interdisciplinary training that is at the cutting edge of multiple fields.<br/><br/>This project will make use of high-density EEG combined with machine learning, computational modeling behavioral measures, and advanced neuromodulation to determine how and when the behaviorally relevant features support scene categorization. First, the work will link the encoding of these features to visual event related potentials (vERPs) and also to category information using multivariate classification techniques from machine learning. Taken together, these techniques will allow the PIs to determine the unique contributions of each feature to category-related brain activity over time. A hallmark of intelligent action is flexibility. Therefore, the project will also investigate the flexibility of feature use by manipulating the diagnosticity of information available to observers. These studies will provide insights regarding feature space usage as a function of task demands, as well as the impact of such demands on the time course of feature space availability as indexed by vERPs. Lastly, the project will test for a potential causal role of vERPs to categorization through the use of advanced neuromodulation techniques."
"1721878","SBIR Phase I:  Automatic Editing Through Semantic Alignment with Deep Learning","IIP","SMALL BUSINESS PHASE I","07/01/2017","06/28/2017","Jonathan Herr","VA","BlackBoiler, LLC","Standard Grant","Peter Atherton","06/30/2018","$225,000.00","","jonathan@BlackBoiler.com","1537 N. Ivanhoe St","Arlington","VA","222052742","2025706316","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to render the tedious, time-consuming, and expensive manual process of contract review and negotiation as archaic.  The outcome from the proposed research will accurately review and negotiate in-bound contracts based on a user's history of reviewing and negotiating just a handful of similar contracts and will result in (a) a 50-90% reduction in companies' contract review and negotiation time and (b) standardized risk across all contracts within an organization.  Furthermore, the proposed research will provide small and medium-sized businesses with the ability to afford and obtain the same quality of legal review of in-bound contracts as the largest and most sophisticated companies in the world. <br/><br/>This Small Business Innovation Research Phase I Project will develop the first system to automate edits in contracts through semantic sentence alignment with deep learning techniques.  The proposed research will expand upon state-of-the-art methodologies for unsupervised learning of distributed representation of various length text segments and then launch the first and only machine learning platform that automates contract review and negotiation with semantic editing capabilities."
"1738097","EAGER:   SC2: SpeCOlab Spectrum Collaboration","CNS","Special Projects - CNS","04/01/2017","03/27/2017","Dirk Grunwald","CO","University of Colorado at Boulder","Standard Grant","Alexander Sprintson","03/31/2020","$99,985.00","Peter Mathys, Youjian Liu, Lijun Chen, Sangtae Ha","grunwald@cs.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1714","7363, 7916","$0.00","The radio spectrum that is usable for wireless communications is a reusable but finite resource. Exponentially growing demand by phones, computers, and the internet of things drives a need for significantly improved dynamic spectrum sharing and allocation. Spectrum allocation is slowly moving towards automated management, with improved efficiency and new wireless services enabled at each step. New spectrum systems still require controlled management of the spectrum. By comparison, this project investigates ways to completely automate the management of spectrum, leaving it to individual devices to learn how to collaboratively share spectrum.<br/><br/>The project team is an interdisciplinary research group that brings together the disciplines of ""Big Data"", machine learning (ML), telecommunications, and signal processing. Efficient opportunistic radio spectrum allocation requires intelligent dynamic agents that can make real-time decisions at the time scale of tens of milliseconds, and can only be achieved using sophisticated machine learning algorithms. A key ingredient for making informed spectrum allocation decisions is the accurate identification of radio signals such as radio and TV broadcasting, local and wide area data networks, cell phones, analog and digital voice, radar, etc., which have been contaminated by noise, fading effects, timing errors, and transmission channel distortion.  The project focus is on the development and implementation of ML strategies for classification, strategies for wireless network configuration and multi-network collaboration. Building upon successful strategies that have been used for image and voice recognition, this project adapts deep learning (DL) algorithms such as convolutional neural networks, deep neural networks, and reinforcement learning networks for the classification of communication signals."
"1619028","III: Small: Collaborative Research: Global Event and Trend Archive Research (GETAR)","IIS","Info Integration & Informatics","01/01/2017","10/24/2016","Edward Fox","VA","Virginia Polytechnic Institute and State University","Standard Grant","James French","12/31/2019","$446,000.00","Donald Shoemaker, Andrea Kavanaugh, Chandan Reddy, Alla Rozovskaya","fox@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7364","7364, 7923","$0.00","This project will devise interactive, integrated, digital library/archive systems coupled with linked and expert-curated webpage/tweet collections, covering key parts of the 1997-2020 timeframe, supporting research on urgent global challenge events and initiatives. It will allow diverse stakeholder communities to interactively collect, organize, browse, visualize, study, analyze, summarize, and explore content and sources related to biodiversity, climate change, crises, disasters, elections, energy policy, environmental policy/planning, geospatial information, green engineering, human rights, inequality, migrations, nuclear power, population growth, resiliency, shootings, sustainability, violence, etc. Studying and addressing important global issues, by scholars, the public, and K-12 students, will be enhanced through tailored interfaces coupled with important collections that will be primary resources for understanding the modern world and its challenges, as well as initiatives, trends, and solutions.<br/><br/>Research will extend work on modeling trends, events, and sources, to guide focused crawling, information extraction, tagging, and collaboration. Domain experts will leverage rich event models, exploiting the generality of the 5S framework (Societies, Scenarios, Spaces, Structure, Streams), extending from word, n-gram, topic, concept, and language models. This research will enable efficient assembly of knowledge bases, rapid prototyping of interfaces, gathering/curation of collections with high precision and recall, and flexible discovery in support of research and learning. Interdisciplinary research advances will address digital humanities, web archiving, information retrieval, natural language processing, machine learning, and the construction of valuable interactive/collaborative interfaces. For further information see the project web site at eventsarchive.org."
"1718771","CIF:Small:Model-Based Blind Demixing for Signal Processing and Machine Learning","CCF","Comm & Information Foundations","09/01/2017","08/28/2018","Justin Romberg","GA","Georgia Tech Research Corporation","Standard Grant","Phillip Regalia","08/31/2021","$499,703.00","Kiryung Lee","jrom@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","7923, 7936","$0.00","The research centers on novel numerical methods and supporting theory for the multichannel convolutive blind demixing (MCBD) problem, where the responses for a set of inter-related time-invariant systems are estimated by observing only their outputs. The MCBD problem arises in many well-known applications in signal processing and communications; one of our goals for this project is to provide a unified framework for solving these problems that has a firm algorithmic and theoretical foundation. The goals are to provide a fundamental analysis of the information theoretic limits of MCBD, along with scalable algorithms that operate with provable performance guarantee at or near these limits. New applications of the MCBD problem will also be explored in the area of machine learning. In particular, the investigators will study how solutions to the MCBD problem can be used as an efficient method for both for the initialization in training deep convolutional neural networks, and for solving inverse problems associated with generative models.<br/><br/>The work will combine classical statistical approaches and modern optimization-based techniques for constrained inverse problems. Of particular interest is the role that structure plays on making the problem identifiable, and on the stability of the solutions when the observations are corrupted by noise. Scenarios where this structure comes from domain-specific knowledge will be considered, along with scenarios where the model is data-driven. The algorithms developed in the project will be validated on applications in astronomical imaging, neuroimaging, medical imaging, seismic imaging, underwater acoustics, and deep learning. The proposed research has direct relevance to next-generation array processing for massive MIMO communications, device-to-device communication for the Internet-of-Things, and new integrated circuit RF transmitters. The work also may open a new direction in parallel MRI. The research activities will be complemented by new graduate courses focusing on modern mathematical methods for the next generation of data scientists."
"1619371","III: Small: Collaborative Research: Global Event and Trend Archive Research (GETAR)","IIS","Info Integration & Informatics","01/01/2017","07/28/2016","Jefferson Bailey","CA","Internet Archive","Standard Grant","James French","12/31/2019","$54,000.00","","jefferson@archive.org","300 Funston Avenue","San Francisco","CA","941182116","4155616767","CSE","7364","7364, 7923","$0.00","This project will devise interactive, integrated, digital library/archive systems coupled with linked and expert-curated webpage/tweet collections, covering key parts of the 1997-2020 timeframe, supporting research on urgent global challenge events and initiatives. It will allow diverse stakeholder communities to interactively collect, organize, browse, visualize, study, analyze, summarize, and explore content and sources related to biodiversity, climate change, crises, disasters, elections, energy policy, environmental policy/planning, geospatial information, green engineering, human rights, inequality, migrations, nuclear power, population growth, resiliency, shootings, sustainability, violence, etc. Studying and addressing important global issues, by scholars, the public, and K-12 students, will be enhanced through tailored interfaces coupled with important collections that will be primary resources for understanding the modern world and its challenges, as well as initiatives, trends, and solutions.<br/><br/>Research will extend work on modeling trends, events, and sources, to guide focused crawling, information extraction, tagging, and collaboration. Domain experts will leverage rich event models, exploiting the generality of the 5S framework (Societies, Scenarios, Spaces, Structure, Streams), extending from word, n-gram, topic, concept, and language models. This research will enable efficient assembly of knowledge bases, rapid prototyping of interfaces, gathering/curation of collections with high precision and recall, and flexible discovery in support of research and learning. Interdisciplinary research advances will address digital humanities, web archiving, information retrieval, natural language processing, machine learning, and the construction of valuable interactive/collaborative interfaces. For further information see the project web site at eventsarchive.org."
"1738317","SBIR Phase II:  A New Paradigm for Physical Security Information:  A Platform Integrating Social Media and Online News with Information Sharing Across Trusted Networks","IIP","SBIR Phase II","09/15/2017","09/20/2019","Gregory Adams","WA","Stabilitas Intelligence Communications, Inc.","Standard Grant","Peter Atherton","02/28/2021","$1,399,944.00","","greg@stabilitas.io","6701 Fox Ave S","Seattle","WA","981083417","7706053035","ENG","5373","165E, 169E, 5373, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is as follows. Commercially, the technology described herein has the capability to provide faster, granular, dynamic information about safety and security, globally, to firms, universities, governments, and NGOs. This project uses novel methods to improve Natural Language Processing and Machine Learning. As a result, better geo-parsing of digital sources of news about security will result in risk content that can be aggregated, displayed, and analyzed in original ways. This enables security managers at these organizations to better understand risk and protect their staff, providing a higher quality of care. For non-governmental organizations (including firms), this enables improved decision-making about operations, travel, and investment. For governments, this enables improved physical security resource allocation. Socially, this project has the potential to improve transparency and accountability regarding trends about safety and security, by improving the aggregation and visualization of data. As an example, groups of firms and governments in emerging markets can collectively identify previously unnoticed patterns of insecurity, in support of public accountability. <br/> <br/>This Small Business Innovation Research (SBIR) Phase II project is an innovation over the state of the art in the following ways. First, this project builds on current geo-parsing extraction methodologies by adding methodologies unique to the safety and security space. Second, this project uses external data sources for cross correlations to improve the ""aboutness"" and granularity of extracted reports. Third, this project exploits contributions from users at the organizational level - as well as individuals. That is, this project supports the growth of an ecosystem in which human users of information also contribute to the quality, volume, and timeliness of that information. This contribution is also intended to improve the geo-parsing methodologies via machine learning. The opportunity is the improvement of geo-parsing extraction mechanisms. The research objectives are to test the hypotheses that NLP algorithms can exploit patterns unique to the safety and security space; that external sources of news can be exploited for improved granularity and ""aboutness"" scores; and that user-generated content can serve to support an ecosystem of information sharing. The anticipated results are that the above innovations will result in usability scoring sufficient for the safety and security use case."
"1719727","Computational Tools for Polycrystalline Materials","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/07/2017","Selim Esedoglu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Leland Jameson","06/30/2021","$201,890.00","","esedoglu@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","1271","8037, 9251, 9263","$0.00","This project will develop new algorithms for computer simulation of how the internal structure (called microstructure) of many technologically essential materials, such as most metals and ceramics, change during common manufacturing processes such as heat treatment. The microstructure of such materials is known to have implications for the physical properties, such as conductivity or yield strength, of the material. Although models describing how the microstructure evolves, for example during heat treatment, have been available, their efficient numerical simulation at scales large enough to be of practical interest to materials scientists have remained a challenge. The project will address this challenge. Understanding the evolution of microstructure holds the promise of materials with more desirable characteristics. The project will also develop machine learning and computer vision algorithms for automatically extracting microstructure information from experimental measurements (microscopy images) of such materials, so that predictions of the models to be simulated can be more readily checked against experiments. One Ph.D. student's training and thesis work will be an integral part of the research.<br/><br/>Continuum models of interfacial motion in polycrystalline materials often take the form of a system of nonlinear partial differential equations describing the geometric flow of a network of surfaces. For certain important interfacial phenomena, such as grain boundary motion, the evolution is given by second order differential equations describing motion by mean curvature of the network. For that setting, a surprisingly simple, elegant, and efficient class of algorithms known as threshold dynamics have been developed that makes large scale simulation particularly feasible. Other phenomena, such as the motion of the free surface of a thin polycrystalline film, or that of pores in a sintered metal, are described by higher order geometric evolutions, such as motion by surface diffusion, and are more challenging to simulate. This project will develop efficient algorithms for such high order multi-phase geometric evolutions. In particular, it will explore whether threshold dynamics or a combination of it with phase field methods can be devised to simulate the motion of networks of surfaces in which some of the interfaces evolve via motion by mean curvature, and others via motion by surface diffusion, coupled along free boundaries known as junctions along which appropriate boundary conditions are satisfied. Some of these models describing the multi-phase geometric motion of networks of interfaces also arise almost verbatim in the context of computer vision and machine learning. By leveraging this mathematical connection, the project will also develop new algorithms for automatically extracting grain boundaries in microscopy images of real polycrystalline materials."
"1704105","SaTC: CORE: Medium: Collaborative: A Linguistically-Informed Approach for Measuring and Circumventing Internet Censorship","CNS","Secure &Trustworthy Cyberspace","08/15/2017","08/14/2019","Prateek Mittal","NJ","Princeton University","Continuing Grant","James Joshi","07/31/2021","$540,001.00","Mung Chiang","pmittal@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8060","025Z, 7434, 7924","$0.00","Internet censorship consists of restrictions on what information can be publicized or viewed on the Internet. According to Freedom House's annual Freedom on the Net report, more than half the world's Internet users now live in a place where the Internet is censored or restricted. However, members of the Internet Freedom community lack comprehensive real-time awareness of where and how censorship is being imposed. The challenges to achieving such a solution include but are not limited to coverage, scalability, adoption, and safety. The project explores a linguistically-informed approach for measuring and circumventing Internet censorship.<br/><br/>The research takes a new perspective on the problem by investigating a hybrid method for censorship detection and evasion from the lens of linguistic analysis. The team develops new models to measure Internet censorship, investigates mechanisms to circumvent censorship using linguistic techniques, conducts communication and social network measurements of censored content. Active Sensing and natural language processing techniques, in conjunction with machine learning and optimization, invigorates new research directions in Internet Freedom and produces new high quality data and tools available for public use. This new allogamy between computer science, information security, network analysis and linguistics provides the foundation for evolution of anti-censorship technologies. The research contributes to a number of fields including Internet censorship, privacy and online information retrieval, as well as computational social science by modeling and analyzing the phenomenon of censorship using the signal available in language. The broader contribution includes wide dissemination of the research results via peer-reviewed publications, special topic courses and workshops. Additional benefits include providing graduate and undergraduate researchers with significant experience of highly practical work on a difficult interdisciplinary problem. Significant gains are obtained in recruitment of minority students through research training in computer science and linguistics."
"1724282","S&AS: INT: Inference, Reasoning, and Learning for Robust Autonomous Driving","IIS","S&AS - Smart & Autonomous Syst","09/01/2017","05/05/2020","Mark Campbell","NY","Cornell University","Standard Grant","David Corman","08/31/2021","$1,398,587.00","Kilian Weinberger, Bharath Hariharan","mc288@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","039Y","046Z","$0.00","While research in autonomous driving has made great strides in recent years, fully autonomous cars are still a distant goal, primarily because of a lack of robustness. Current autonomous cars cannot drive on new roads, or roads that have changed substantially (such as after an earthquake), or when there is a GPS or data outage such as in parking garages, urban cities and tunnels. Importantly, humans are good at all of this: Humans can drive without detailed maps or high precision GPS/IMU sensors, and typically require only a small amount of sparse information for guidance, and their performance typically gets better over time through learning. Using the ""intelligent"" human driver as a guide, the planned research will develop algorithms that can perceive and make predictions about a scene in real time with measurable confidence, particularly as the scene is closer to the car. New robustness characteristics will be achieved through the ability to detect and overcome mistakes, both in the near term (real time) and long term (learning). The planned algorithms will be designed and validated in a way to enable an inherent robustness not currently available in autonomous driving, and fast adoption by the community. This project is aligned with NSF's Intelligent Physical Systems (IPS) because the algorithms will require cognizant and reflective capabilities in a knowledge-rich environment. Additionally, outputs of this project will impact robotics, machine learning and cyber-physical systems. Educationally, data logs will be disseminated to enable open ended student projects in the community, and undergrad and high school students will collaborate with the research team to integrate sensors, perform experiments and data collection, and disseminate data logs to the community. <br/><br/>Led by researchers in Mechanical and Aerospace Engineering, and Computer Science at Cornell University, the goal of this research is to develop, integrate and validate theory and algorithms to enable robust and persistent autonomous driving. This project is aligned with NSF's Intelligent Physical Systems (IPS) because the algorithms will require cognizant and reflective capabilities in a knowledge-rich environment. The technical approach will develop a robust perceptual pipeline for detection, scene estimation, prediction, and anomaly/mistake detection and learning; integrate the algorithms into Cornell's autonomous car software framework and validate the components and system in a series of experimental scenarios to enable their faster adoption by the community. Key component level algorithms to be developed include anytime deep learning detectors with quantifiable performance; multiple hypothesis reasoning with memory attributes; generalized probabilistic anticipation algorithms to mimic a human's mental model of a dynamic scene; and anomaly/mistake detection coupled with online learning. Outcomes will include open source algorithms and data logs; publications, conferences, workshops; data logs for open ended projects in courses and across the community; and undergrad and high school education and diversity programs in the interdisciplinary area of autonomous driving."
"1704113","SaTC: CORE: Medium: Collaborative: A Linguistically-Informed Approach for Measuring and Circumventing Internet Censorship","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","08/15/2017","07/18/2019","Chris Leberknight","NJ","Montclair State University","Continuing Grant","James Joshi","07/31/2021","$500,849.00","Anna Feldman","leberknightc@mail.montclair.edu","1 Normal Avenue","Montclair","NJ","070431624","9736556923","CSE","1714, 8060","025Z, 7434, 7924","$0.00","Internet censorship consists of restrictions on what information can be publicized or viewed on the Internet. According to Freedom House's annual Freedom on the Net report, more than half the world's Internet users now live in a place where the Internet is censored or restricted. However, members of the Internet Freedom community lack comprehensive real-time awareness of where and how censorship is being imposed. The challenges to achieving such a solution include but are not limited to coverage, scalability, adoption, and safety. The project explores a linguistically-informed approach for measuring and circumventing Internet censorship.<br/><br/>The research takes a new perspective on the problem by investigating a hybrid method for censorship detection and evasion from the lens of linguistic analysis. The team develops new models to measure Internet censorship, investigates mechanisms to circumvent censorship using linguistic techniques, conducts communication and social network measurements of censored content. Active Sensing and natural language processing techniques, in conjunction with machine learning and optimization, invigorates new research directions in Internet Freedom and produces new high quality data and tools available for public use. This new allogamy between computer science, information security, network analysis and linguistics provides the foundation for evolution of anti-censorship technologies. The research contributes to a number of fields including Internet censorship, privacy and online information retrieval, as well as computational social science by modeling and analyzing the phenomenon of censorship using the signal available in language. The broader contribution includes wide dissemination of the research results via peer-reviewed publications, special topic courses and workshops. Additional benefits include providing graduate and undergraduate researchers with significant experience of highly practical work on a difficult interdisciplinary problem. Significant gains are obtained in recruitment of minority students through research training in computer science and linguistics."
"1745673","EAGER:  Supporting GUI-Based Text Analytics on Social Media Data by Non-Technical Users","IIS","Info Integration & Informatics","08/01/2017","11/06/2018","Chen Li","CA","University of California-Irvine","Standard Grant","Maria Zemankova","07/31/2019","$216,000.00","","chenli@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7364","7364, 7916, 9251","$0.00","A wealth of information is being created at an increasingly fast rate from social media sources such as Twitter. Many researchers need text analytics on social media data to obtain domain-specific insights, by doing various computational tasks such as keyword search, regular expression, natural language processing, and sentiment analysis. A main challenge is their lack of IT background, making it hard for them to conduct research efficiently. In addition, very often they need to use machine learning models to do deep analysis, and the data-preparation process for generating labeled instances for training models can be time consuming and labor intensive. <br/><br/>This project studies how to support text analytics on social media data by users who do not have a strong IT background. It develops an open-source system with the following goals: (1) modularizing common text computation as basic operators; (2) providing a graphic user interface (GUI) for users to form a workflow plan declaratively without writing code; (3) supporting text analytics as a Web-based service; and (4) supporting indexing whenever possible to improve performance. The system has been under development on Github for more than 15 months with more than 30 committers and 21 thousand lines of high-quality source code. An initial prototype is setup and used by Public Health researchers at UC Irvine to analyze Twitter data. The techniques are general-purpose, systems-oriented, and can benefit many other domains as well."
"1748067","EAGER:  Data-Driven Contact Modeling","IIS","NRI-National Robotics Initiati","08/15/2017","08/14/2017","C. Karen Liu","GA","Georgia Tech Research Corporation","Standard Grant","Erion Plaku","10/31/2019","$200,000.00","","karenliu@cs.stanford.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","7916","$0.00","Accurate physics simulation has become an essential component for developing robots that physically interact with the world.  A particularly important aspect is simulating contacts between the robots and objects in the environment, which can be useful for both planning and machine learning.  However, robots that learn in simulation often perform poorly in the real world due to inaccurate parameters, idealized dynamic and contact models, or other unmodeled factors.  This project therefore tackles an important challenge in physics simulation: accurate modeling of contacts. The results will significantly improve contact modeling in physics simulation, which offers a safe space to learn difficult and highly risky motor skills such that the robots can operate more efficiently and robustly even in unseen scenarios in the real world. This capability will have potential impact on robotics in healthcare, search-and-rescue, and space exploration.<br/><br/>This proposal introduces a technique that effectively utilizes real-world data to model the complex, poorly understood contact phenomena. Specifically, the new data-driven technique accurately computes contact states (sticking, sliding, or breaking) and contact forces such that the simulated results match the real-world phenomena. Instead of taking the conventional approach of system identification, this proposal leverages empirical evidence and deep learning techniques to enhance the existing contact model, namely, an implicit time-stepping, velocity-based Linear-Complementarity Program (LCP).  The key insight is that the contact problem can be broken down into two steps: predicting the next state of each contact point and calculating contact forces based on the prediction and current dynamic state. The first step is solved by learning a classifier from real-world data. By doing so, the second step can be simplified from an LCP to a Linear Program (LP), thus making the calculation of contacts much more efficient.<br/>"
"1734266","NRI: FND: Scene Understanding and Predictive Monitoring for Safe Human-Robot Collaboration in Unstructured and Dynamic Construction Environments","IIS","NRI-National Robotics Initiati","09/01/2017","03/15/2019","SangHyun Lee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Jie Yang","08/31/2020","$750,000.00","Vineet Kamat, Jia Deng","shdpm@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8013","8086","$0.00","The construction industry has the highest number of fatalities and injuries due to hazardous working conditions. The introduction of robots on construction sites has the potential to relieve human workers from dangerous and repetitive tasks by making machines intelligent and autonomous. However, robotic solutions for construction face significant challenges. This project will develop technologies of automated monitoring and intervention through computer vision to provide a means to dramatically improve the perception of construction safety in the presence of co-robots. The new methods developed in this project will impact computer vision, machine learning, and effective human-robot collaboration in unstructured environments, while significantly contributing to safety. Further, the developed methodologies can be broadly applicable in situations where robots are deployed in human-centered environments (hospitals, airports, shipyards, etc.) and have other priorities such as productivity and efficiency as their objective. This project will engage a diverse group of individuals by training graduate and undergraduate students (including women and underrepresented minorities), reaching out to K-12 students, and interacting with industry professionals for broad dissemination of the research results.<br/><br/>This research will investigate new computer vision based methods that can be coupled with other sensing modalities for holistic understanding and predictive analysis of jobsite safety on co-robotic construction sites. The project will consist of two main research thrusts. First, holistic scene understanding will be pursued on construction sites using graphical models to enable joint reasoning of various scene components. This holistic understanding in turn will help evaluate compliance with established safety rules expressed as formal statements. Second, predictive analysis will be investigated by exploiting the fact that, for safety intervention, the complex dynamics of a construction scene make it necessary to simulate what will happen next. In particular, Recurrent Neural Networks will be leveraged to predict future events and prevent impending accidents. Finally, an integrated demonstration system will be built and tested on real construction sites."
"1659488","REU Site: Data Science of Risk and Human Activity","CCF","RSCH EXPER FOR UNDERGRAD SITES","06/01/2017","03/10/2017","George Mohler","IN","Indiana University","Standard Grant","Joseph Maurice Rojas","05/31/2020","$287,377.00","","georgemohler@gmail.com","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","1139","9250","$0.00","This Research Experiences for Undergraduates (REU) program at Indiana University Purdue University Indianapolis (IUPUI) will provide eight undergraduate students from across the United States with the opportunity to conduct research on the data science of human activity.  The students will spend ten weeks during the summer working with IUPUI faculty on projects related to predictive modeling and estimation of risk with applications to crime, conflict, political instability and daily routine activity.  The students will also attend a data science bootcamp as part of the program that will provide training in the foundations of data science (statistics, machine learning, and software development).  Data science is a rapidly growing field due to increases in the volume and variety of data being generated.  Graduates with skill sets at the intersection of mathematics, statistics, computing, data analysis, and data modeling are in high demand, both in industry and academia.  The REU site will increase undergraduate student awareness, preparation, and interest in pursuing graduate degrees in STEM fields where data science is becoming a larger focus.  Students choosing to pursue a career in industry upon graduation will also be better prepared to meet the data challenges of the increasing number of companies where data science is a high priority. <br/>  <br/>The REU projects address new challenges in the data science of risk and human activity.  Students working in problem area 1 will focus on learning to rank problems for space-time crime prediction.  Machine learning models for ranking crime hotspots according to risk will be developed that will be tailored to ranking based loss functions that arise in criminology.  The second project will focus on classifying types of activity from mobile sensor time series.  The students will compare existing techniques to deep learning algorithms employing architectures developed specifically for 3d data from the accelerometer and gyroscope.  The third area of research will focus on point process models of heterogeneous grievance data.  In particular, students will develop spatio-temporal models for conflict data in Sub-Saharan Africa coupled with geolocated, relevant Tweets from the same time period."
"1739095","CAREER: Creation, Visualization, and Mining of Domain Textual Graphs: Integrating Domain Knowledge and Human Intelligence","IIS","Info Integration & Informatics, EPSCoR Co-Funding","07/01/2017","02/08/2018","Wei Jin","TX","University of North Texas","Continuing Grant","Sylvia Spengler","01/31/2021","$390,205.00","","wei.jin@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","7364, 9150","1045, 7364, 9150","$0.00","It is understood that textual information is growing at an astounding pace, creating an enormous challenge for analysts trying to discover valuable information that is buried within. For example, new non-trivial trends, patterns, and associations among entities of interest, such as associations between genes, proteins and diseases, and the connections between different places or the commonalities of people, are such forms of underlying knowledge. The goal of this research is to explore automated solutions for sifting through these extensive document collections to detect interesting links and hidden information that connect facts, propositions or hypotheses. In addition, a more comprehensive view of discovered knowledge will be provided by generating an in-depth and concise cross-document summary explaining the underlying meaning of each connection, along with relevant links and explanations acquired from the Wikipedia knowledge base, which serves as the primary means of complementing or enhancing existing information in text collections. The project will impact many areas, such as homeland security, aviation safety, biomedical and healthcare applications. The techniques will have the potential to expose new information available in large document collections and to provide a multi-view perspective of discovered hypotheses by integrating domain knowledge and relevant information acquired from Wikipedia. Research-based education and training opportunities will be offered by this project to prepare students at all levels in information analysis and discovery. Specific attention will also be paid to promoting the participation of underrepresented groups in the research efforts.<br/><br/>This project focuses on the exploration of a novel textual knowledge representation, integration, and mining framework that will cover the following areas: (i) automatic construction of graphical frameworks for entity relationship discovery, a new representation conducive to fine-grained information search and discovery; (ii) effective integration of information from multiple sources, including knowledge contained in representative data collections, domain-specific knowledge (e.g., domain ontologies), and world knowledge (e.g., lexical resources such as WordNet and large-scale knowledge repositories such as Wikipedia); (iii) new discovery algorithms and tools that identify hidden connections among entities; (iv) enhancement of domain modeling through enabling automatic ontology-driven scenario detection and topic-level modeling; and (v) interactive visualization tools for the graphical framework and discovered hypotheses. This research proposes that next-generation search tools require the capability of integrating information from multiple interrelated units and combining various evidence sources, which will make fundamental advances in the current state of the art for information search and discovery. A combination of techniques in Natural Language Processing (NLP), Information Extraction (IE), Information Retrieval (IR), Data Mining, Machine Learning, and Semantic Web will be explored to attack critical information discovery problems. For further information see the project web site: http://www.cs.ndsu.nodak.edu/~wjin/WSD-RelMiner."
"1704458","III: Medium: Non-Convex Methods for Discovering High-Dimensional Structures in Big and Corrupted Data","IIS","Info Integration & Informatics","08/01/2017","10/24/2019","Rene Vidal","MD","Johns Hopkins University","Standard Grant","Sylvia Spengler","07/31/2021","$1,150,000.00","Daniel Robinson","rvidal@cis.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7364","7364, 7924","$0.00","Discovering structure in high-dimensional data, such as images, videos and 3D point clouds, has become an essential part of scientific discovery in many disciplines, including machine learning, computer vision, pattern recognition, and signal processing. This has motivated extraordinary advances in the past decade, including various sparse and low-rank modeling methods based on convex optimization with provable theoretical guarantees of correct recovery. However, existing theory and algorithms rely on the assumption that high-dimensional data can be well approximated by low-dimensional structures. While this assumption is adequate for some datasets, e.g., images of faces under varying illumination, it may be violated in many emerging datasets, e.g., 3D point clouds. The goal of this project is to develop a mathematical modeling framework and associated non-convex optimization tools for discovering high-dimensional structures in big and corrupted data.<br/><br/>This project will develop provably correct and scalable optimization algorithms for learning a union of high-dimensional subspaces from big and corrupted data. The proposed algorithms will be based on a novel framework called Dual Principal Component Pursuit that, instead of learning a basis for each subspace, seeks to learn a basis for their orthogonal complements. In sharp contrast with existing sparse and low-rank methods, which require both the dimensions of the subspaces and the percentage of outliers to be sufficiently small, the proposed framework will lead to results where even subspaces of the highest possible dimension (i.e., hyperplanes) can be correctly recovered from highly corrupted data. This will be achieved by solving a family of non-convex sparse representation problems whose analysis will require the development of novel theoretical results to guarantee the correct recovery of the subspaces from corrupted data. The project will also develop scalable algorithms for solving these non-convex optimization problems and study conditions for their convergence to the global optimum. These algorithms will be evaluated in two major applications in computer vision: segmentation of point clouds and clustering of image categorization datasets."
"1833137","RI: Medium: Collaborative Research: Learning to Su","IIS","Robust Intelligence","11/08/2017","08/19/2018","Fei Sha","CA","University of Southern California","Continuing Grant","Jie Yang","08/31/2021","$501,505.00","","feisha@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","7495, 7924","$0.00","Today there is far more video being captured - by consumers, scientists, defense analysts, and others - than can ever be watched.  With this explosion of video data comes a pressing need to develop automatic video summarization algorithms.  Video summarization takes a long video as input and produces a short video as output, while preserving its information content as much as possible.   As such, summarization techniques have great potential to make large video collections substantially more efficient to browse, search, disseminate, and facilitate communication.  Such increased efficiency will play a vital role in many important application areas.  For example, with reliable summarization systems, a primatologist gathering long videos of her animal subjects could quickly browse a week's worth of their activity before deciding where to inspect the data most closely.  A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict.  An intelligence agent could rapidly sift through reams of aerial video, reducing the resources required to analyze surveillance data to identify suspicious activities.<br/><br/>This project develops new machine learning and computer vision algorithms for video summarization.  Unsupervised methods, which are the cornerstone of nearly all existing approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.  By instead posing video summarization as a supervised learning problem, this project investigates a markedly different formulation of the task.  The research team is investigating four key new ideas: powerful probabilistic models for learning to select the optimal subset of video frames for summarization, semi-supervised learning models and co-summarization algorithms for leveraging the abundance of multiple related videos, algorithms for exploiting photos on the Web to improve summarization, and evaluation protocols that assess summaries in a way that aligns well with human comprehension.  The broader impact of the proposed research includes practical tools for video summarization, scientific advances that appeal broadly to several communities, publicly disseminated research results, inter-disciplinarily trained graduate students, and outreach activities to engage young students in STEM education and career paths."
"1741615","CAREER: Common Links in Algorithms and Complexity","CCF","Algorithmic Foundations","01/20/2017","02/11/2020","Ryan Williams","MA","Massachusetts Institute of Technology","Continuing Grant","Joseph Maurice Rojas","11/30/2020","$503,312.00","","rrw@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","1045, 7926, 7927","$0.00","The field of algorithm design builds clever programs that can quickly solve computational problems of interest. The field of complexity theory mathematically proves ""lower bounds,"" showing that no such clever program exists for (other) core problems. Intuitively, it appears that these two fields work on polar-opposite tasks. The major goal of this project is to discover counter-intuitive new connections between algorithm design and complexity theory, and to study the scientific consequences of the bridges built by these connections. It is hard to overestimate the potential impact---societal, scientific, and otherwise---of a theoretical framework which would lead to a fine-grained understanding of what computers can and cannot do. This project is focused on exploring concrete steps towards a better understanding, via studying links between the seemingly opposite tasks of algorithms and lower bounds. Another goal of the project is to bring complexity research closer to real-world computing, and to introduce practitioners to aspects of complexity that will impact their work. A final goal is educational outreach, through online forums dedicated to learning computer science, teaching summer school courses, and collaboration with the media on communicating theoretical computer science (including links between algorithms and lower bounds) to the public.<br/><br/>The PI seeks common links between algorithms and complexity: counter-intuitive similarities and bridges which will lead to greater insight into both areas. A central question in computer science is the famous P versus NP open problem, which is about the difficulty of combinatorial problems which admit short solutions. Such problems can always be solved via ?brute force?, trying all possible solutions. Can brute force always be replaced with a cleverer search method? This question is a major one; no satisfactory answers are known, and concrete answers seem far away. The conventional wisdom is that in general, brute force cannot be entirely avoided, but it is still mathematically possible that most natural search problems can be solved extremely rapidly, without any brute force. <br/><br/>Computational lower bounds are among the great scientific mysteries of our time: there are many conjectures and beliefs about them, but concrete results are few. Moreover, the theory is hampered by ?complexity barriers? which show that most known proof methods are incapable of proving strong lower bounds. The PI's long-term objective is to help discover and develop new ways of thinking that will demystify lower bounds, and elucidate the limits of possibilities of computing. The PI hypothesizes that an algorithmic perspective on lower bounds is the key: for example, earlier work of the PI shows that algorithms for the circuit satisfiability problem (which slightly beat brute force search) imply circuit complexity lower bounds. The PI has developed several new links within the past few years, and has proposed many more to be investigated. Among the various angles explored in this project, the potential scientific applications are vast, ranging from logical circuit design, to network algorithms, to improved hardware and software testing, to better nearest-neighbor search (with its own applications in computer vision, DNA sequencing, and machine learning), and to cryptography and security."
"1716400","AF:Small: Novel Geometric Techniques for Several Biomedical Problems","CCF","Algorithmic Foundations","09/15/2017","09/07/2017","Jinhui Xu","NY","SUNY at Buffalo","Standard Grant","Tracy Kimbrel","08/31/2021","$451,802.00","","jinhui@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7796","7923, 7929","$0.00","Recent progress in biomedicine has relied heavily on computer science technology. As the territory of biomedicine is rapidly enlarging, more powerful computational techniques are needed to foster its continuous growth. This project develops efficient computer algorithms for three fundamental geometric problems arising in several biomedical applications: (1) Truth Discovery, (2) Abnormal Clusters Detection, and (3) Resource Allocation Voronoi Diagram. Problem (1) develops quality-guaranteed polynomial time solutions to a key problem in data crowdsourcing, finding trustworthy information from multiple data sources, which is also motivated by the biomedical problem of learning critical information for improving treatment planning of endovascular intervention. Problem (2) detects extremely small-sized abnormal clusters from large datasets, which is motivated by detecting genomic structure variants from large populations. Problem (3) investigates new generalizations of the classical Voronoi diagram, which are motivated by a segmentation problem of biological images. This project will provide educational and research opportunities to undergraduate and graduate students (including those from under-represented groups), and develop a teaching evaluation tool for improving the quality of education. <br/> <br/>This project uses computational geometry techniques to develop novel algorithms for the proposed problems. It will introduce several general algorithmic techniques to the area of computational geometry, enriching and prodding its further development. These algorithmic techniques are also likely to be used in other areas, such as machine learning, computer vision, data mining, and pattern recognition, and bring new ideas to these areas. This project could lead to several long term impacts. It could potentially improve the quality of endovascular intervention, help identifying potential genomic structural variants in some genetic disorders, and provide more accurate quantitative information for medical image analysis."
"1651118","Deceit and Interactional Synchrony In Different Social  Constellations","BCS","Social Psychology","05/01/2017","04/27/2017","Mark Frank","NY","SUNY at Buffalo","Standard Grant","Steven J. Breckler","04/30/2021","$318,526.00","Ifeoma Nwogu","mfrank83@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","SBE","1332","1332","$0.00","Humans are a social species, but the nature of the connection between any two persons varies. Typically, two friends have a stronger relationship than do two strangers. The nature of the interaction also varies depending on the messages conveyed, such as when a truth versus a lie is told. A measure that captures the fundamental social dynamics between people is called interactional synchrony, and it reflects the extent to which the behavior of two or more individuals correlates within a short time window. Past research shows that interactional synchrony strengthens a relationship. However, a cohesive synchrony measure that captures multiple facets of behavior has yet to be tested. This project explores the role of deception in interactional synchrony in three different social constellations: individuals who have built rapport with a stranger, individuals who are friends, and individuals who belong to the same social organization. The synchrony between an interviewer and an interviewee, of varying levels of social relationship, will be captured when the interviewee lies and tells the truth about an action. The research uses computer vision and machine learning techniques to analyze and uncover synchrony in the faces, vocal tones, bodies, postures, and sub-visible physiological responses of interviewers and interviewees. The results will inform the need to consider social ties in research on deceptive behavior and when assessing demeanor during real world interactions, such as interviews with criminal suspects or potential terrorists.<br/><br/>This project explores the role of deception in interactional synchrony in three different social constellations: individuals who have built rapport with a stranger, individuals who are friends, and individuals who belong to the same social organization. Interactional synchrony is expected to be higher for interactions that involve high rapport, closer friendship, and shared group membership. The deception scenarios involve both sanctioned and unsanctioned lying. The variation in interactional synchrony as a result of lying compared to truth telling within participants and as a function of truth/lie interactions with rapport, friendship, and shared group membership will be evaluated. The analysis of synchrony using computer vision and machine learning algorithms applied to different channels, ranging from more controllable behaviors, such as posture, to those less controllable, such as electrodermal responses, will enable a global interactional synchrony measure and also aid in the understanding of interactional synchrony as a concept, separating intentional mimicry from synchrony that is more automatic and unconscious. This refinement in the measurement technology of interactional synchrony will be of special value in detecting deceit, thereby contributing to the nation's security priorities."
"1802284","BIGDATA: IA: Collaborative Research: Domain Adaptation Approaches for Classifying Crisis Related Data on Social Media","IIS","Big Data Science &Engineering","09/20/2017","12/01/2017","Cornelia Caragea","KS","Kansas State University","Standard Grant","Aidong Zhang","01/31/2019","$400,000.00","","cornelia@uic.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","8083","7433, 8083","$0.00","The project investigates the use of big-data analysis techniques for classifying crisis-related data in social media with respect to situational awareness categories, such as caution, advice, fatality, injury, and support, with the goal of helping emergency response teams identify useful information. A major challenge is the scale of the data, where millions of short messages are continuously posted during a disaster, and need to be analyzed. The use of current technologies based on automated machine learning is limited due to the lack of labeled data for an emergent target disaster, and the fact that every event is unique in terms of geography, culture, infrastructure, technology, and the people involved. To tackle the above challenges, domain adaptation techniques that make use of existing labeled data from prior disasters and unlabeled data from a current disaster are designed. The resulting models are continuously updated and improved based on feedback from crowdsourcing volunteers. The research will provide real, usable solutions to emergency response organizations and will enable these organizations to improve the speed, quality and efficiency of their response. <br/><br/>The research provides novel solutions based on domain adaptation and deep neural networks to tackle the unique challenges in applying machine learning for crisis-related data analysis, specifically the volume and velocity challenges of big crisis data. Domain adaptation approaches enable the transfer of information from prior source disasters to an emergenet target disaster. Deep learning approaches make it possible to employ large amounts of labeled source data and unlabeled target data, and to incrementally update the models as more labeled target data becomes available. Large-scale analysis across combinations of source and target crises will help identify patterns of transferable situational awareness knowledge. The resulting technical and social solutions will be blended together for use in data management and emergency response."
"1657262","CRII: SCH: Using Digital Images to Connect Eating Environment with Dietary Quality","IIS","CRII CISE Research Initiation","04/15/2017","04/10/2017","Fengqing Zhu","IN","Purdue University","Standard Grant","Wendy Nilsen","03/31/2020","$174,792.00","","zhu0@ecn.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","026Y","8018, 8228","$0.00","Chronic disease such as heart disease, diabetes, and obesity are known to be strongly linked with diet and may be rooted in the environmental context where they are prevalent. This proposal aims to develop imaging-based techniques to investigate the link between eating environment and dietary quality and satisfaction which are not known. The project will use images from the food environment to address the fundamental question of where, how and when food should be consumed to maximize health and prevent disease. Monitoring the personal dietary environment and determination of environmental patterns related to dietary intake can empower both health care providers and patients to optimize evidence-based decisions. This information can help individuals recognize less healthful behaviors that may be occurring in their lives. Health professionals will also have better information to advise behavioral strategies within the context of the patient's environment. The results may also be used to help guide the development of programs to reduce the prevalence of obesity and diet-related chronic diseases in the US population and advise US dietary policy.<br/><br/>This highly interdisciplinary investigation explores image processing and computer vision techniques to extract and quantify dietary environmental factors and study their connections with dietary intake. The project plans to build informative models of behavioral health profiles that can take advantage of a large set of observed data, including food images and contextual information that the PI has access to. The team will develop computational methods that leverage the use of contextual information for image-based dietary data which is highly individualized, temporal, and contextualized. The benefits of including contextual information are twofold: it provides a more complete composite of a person's health influencers of dietary behavior; and can improve the accuracy of food recognition and nutrient intake estimation using computer vision techniques. The proposed work will develop 1) new image analysis techniques that leverage contextual cues such as eating time, location type, co-occurrence patterns of objects, personalized learning models from image-based dietary record; 2) novel machine learning and statistical analysis tools for dietary pattern discovery and prediction by exploring relationships among the environmental factors and their association with dietary quality; 3) experimental validation of the proposed methods using existing image-based dietary data."
"1738479","SBIR Phase II:  Human agent prediction for autonomous vehicles","IIP","SBIR Phase II","09/15/2017","09/19/2017","Samuel Anthony","MA","Perceptive Automata, Inc.","Standard Grant","Peter Atherton","01/31/2020","$749,955.00","","santhony@perceptiveautomata.com","1 Broadway 5th Fl","Cambridge","MA","021421190","6172991296","ENG","5373","5373, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project results from the fact that it will unlock the potential of autonomous vehicles in dense urban environments in such a way that these vehicles will be safe, effective, and able to operate in environments with a wide range of often-vulnerable road users.  By providing a system for autonomous vehicles to understand the goals and behaviors of humans on the road, the technology will allow autonomous vehicles to react to humans safely and effectively. Without the innovations commercialized with the help of this award, autonomous vehicles will be at best uselessly timid and dangerous additions to urban roads, and at worst a deadly obstacle to the goal of safe, livable cities.  Every autonomous vehicle that is capable enough to be on the market will need to solve the problem that this project is helping to solve.<br/> <br/>This Small Business Innovation Research (SBIR) Phase II project will help to address one of the thorniest problems in autonomous vehicles. The question of how a computer system can gain an understanding of human mental states has occupied researchers and laypeople with an interest in machine intelligence since the coining of the term.  By building an approach based on the leveraging of careful human measurement and state-of-the-art learning algorithms, the innovations developed in this project will help pave a new path towards the computational modeling of cognitive facilities that are central to human intelligence but historically intractable to model using conventional machine learning or computer vision techniques.  In addition to helping solve the central problem of human understanding for autonomous vehicles, the research published from this project will open new avenues for understanding a broad class of problems where the question of ""ground truth"" about the world is difficult or impossible to answer."
"1723445","CAPA: Collaborative Research: ARION: Taming Heterogeneity with DSLs, Approximation, and Synthesis","CCF","Software & Hardware Foundation","10/01/2017","09/20/2019","Jonathan Ragan-Kelley","CA","University of California-Berkeley","Continuing Grant","Nina Amla","09/30/2021","$200,000.00","","jrk@mit.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7798","021Z, 026Z, 7798, 8206, 8585","$0.00","Specialization and the arrival of new technologies are key forces motivating heterogeneous systems. Heterogeneity is already in use widely, with public clouds offering instances that are heterogeneous in both compute capabilities and storage. This project identifies the following forces that will make systems heterogeneous beyond just compute and storage, complicating programming and compilation beyond the challenges that we face today. This project develops Arion, a system for compiling programs onto heterogeneous platforms based on several unifying ideas. The Arion system will be evaluated on practically relevant workloads ranging from computer vision and virtual reality, to graph computations, machine learning and stream processing. The investigators will work with partners in industry to transfer research results to products, and the tools and software developed by this project will be released as open source.<br/><br/>The research in this project relies on four unifying ideas. The first thrust explores schedules and type systems separate a program's specification from its implementation strategy, enabling performance portability because one can select, without changing the program, its parallelism, locality, and hardware mapping. The second thrust uses domain-specific languages to describe not only programs but also artifacts used during compilation, such as schedules, resource-, and memory consistency models. This allows automatic synthesis of these artifacts. The third thrust uses resource models to bring scheduling and synthesis to large programs because the target program need not be scheduled or synthesized all at once. Instead, the compiler makes high-level decisions by estimating performance using a model before committing to low-level decisions. Finally, the investigators will use formal methods to lift programs into, and verify and synthesize programs in our DSLs, providing a high degree of automation. The verifiers and synthesizers are automatically generated from descriptions of DSLs."
"1656905","CRII: AF: Novel Geometric Algorithms for Certain Data Analysis Problems","CCF","CRII CISE Research Initiation","05/15/2017","05/29/2018","Hu Ding","MI","Michigan State University","Standard Grant","Rahul Shah","04/30/2019","$174,328.00","Hu Ding","huding@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","026Y","7796, 7929, 8228","$0.00","We can often see trends or clusters in data by graphing or plotting-- giving geometric form to data.   As data increases in volume and complexity, giving it geometric form and then developing computational geometry algorithms is still a fruitful way to approach data analysis. For example, activity data from a smartphone or fitness tracker can be viewed as a point in thousands of dimensions whose coordinates include all positions, heart rates, etc. from an entire sequence of measurements.  For better privacy, we can share summaries (rough position, duration, etc.) as points in tens of dimensions.  Points from many people can be clustered to identify similar patterns, and patterns matched (with unreliable data identified and discarded) to recognize actions that a digital assistant could take to improve quality of life or health outcomes.   <br/><br/>This project aims to develop a set of advanced data structures and novel geometric algorithms for three fundamental data analysis problems: (1) constrained clustering in high dimensions, (2) geometric matching under certain transformations, and (3) extracting trustworthy information from unreliable data.  The first two problems are both naturally studied by computational geometry, and the third has a novel formulation as a geometric optimization problem in high dimensions.  The goal is to achieve highly efficient and quality guaranteed solutions for each of these problems. The new geometric insights, advanced data structures, and efficient algorithmic techniques introduced by this project will enrich further development in computational geometry and bring fresh ideas to other areas, including machine learning, computer vision, data mining, and bioinformatics.  <br/><br/>This project provides research and educational opportunities in data analysis to both graduate and undergraduate students (including women, minorities, and other underrepresented groups) at Michigan State University.  It also undertakes outreach activities for students in K-12 outreach activities and prepares online materials to benefit more students and teachers. In particular, student evaluations of teacher performance will be one of the data sets used in problem (3), extracting trustworthy information from unreliable data."
"1718432","AF:Small:Extreme Streaming Problems","CCF","Algorithmic Foundations","07/15/2017","07/14/2017","Shanmugavelayu Muthukrishnan","NJ","Rutgers University New Brunswick","Standard Grant","A. Funda Ergun","06/30/2020","$499,088.00","","muthu@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7796","7923, 7926","$0.00","The need for modern streaming systems to collect and analyze human activities is enormous, in businesses, government, academia, and society. Businesses can improve their operations and production; governments can improve the participation and satisfaction of citizens; society at large can be more sustainable or safe; etc. However, systems that collect and analyze such streams have enormous challenges of scale. At the highest level, this proposal combines a research, education and outreach plan to address these challenges. <br/><br/>This research focusses on developing new algorithmic methods and theoretical understanding of modern streaming problems.  There is an extensive theory of streaming algorithms for single streams, or to a limited extent to distributed streams, for one (or few) high cardinality dimensional data and simple frequency based analyses. But there are large gaps in creating streaming algorithms for ""extreme"" needs in modern data streaming applications, where the dimension of data that is collected is large, multiple streams of collected in distributed vantage points, there is a need to find anomalies in high dimensional spaces, and analyses that are needed are sophisticated including machine learning and other real time decision making tasks.  This research develops the algorithmic theory of these extreme streaming problems. In particular, the project develops the algorithmic foundations for using large scale distributed streaming systems and tradeoff quality, certainty, CPU, memory and communication needed to do extreme, streaming, sophisticated analyses. Since modern streaming systems power businesses and deal with behavioral data on users, this work has broad societal impact. The project significantly improves the state of the art in algorithms for modern streaming systems. By providing new, rich algorithmic approaches, the project inspires practitioners in academia and industry to conceive more impactful applications, which are infeasible given the current algorithmic tools.<br/><br/>The research program both enables and benefits from an education and outreach program that enhances curriculum, fosters training women and underrepresented minorities. To enable technology transfer the project involves practitioners in streaming systems, for field-testing the methods whenever possible. All publishable results are disseminated in respected academic journals, conferences, and workshops. All code and data sets developed in this work are made openly available to the community via the MassDAL site that already has code that is used by the community for classical streaming problems.<br/><br/>Classical streaming algorithms use space sublinear, typically polylogarithmic in the input parameters. When extended to distributed systems, often the focus is on sublinear communication. The research program, here, builds on this algorithmic theory of past 15 years and addresses the modern, extreme needs of streaming applications. The project extends the theory to: many dimensions with large attributes, using far fewer resources in memory, computing and communication; emerging, pipeline models of streaming; more sophisticated analyses from local privacy to deep learning type vector embeddings; etc.  The research program addresses fundamental problems. In more detail:<br/><br/>(a)  Extant streaming algorithms work for one or few dimensions of data of high cardinality. Modern streaming systems collect logs of human activity and have 100s of dimensions, 10s or more of them have very high cardinality. Can one identify the key problems for these domains and develop an algorithmic theory? The PI has identified an effective approach based on graphical modeling of the relationship between the dimensions, and believes this nugget can yield an effective theory.<br/>(b) Extant streaming algorithms use polylogarithmic words of memory per analysis when they are considered successful (and lower bounds point to problems for which sublinear space is not sufficient). Modern streaming systems run several orders of magnitude of such analyses, for example, one analysis for each of the millions of users. The project has identified an approach of ""frugal"" streaming, where algorithms use a small constant number of words, and develops a theory of frugal streaming algorithms, and their limitations.<br/>(c) Modern data stream systems allow pipelining, with the stream (modifiable or not) passing through stages, either at individual sites, or across the sites. The project abstracts and develops algorithmic theory of streaming problems with pipelined streaming systems. Preliminary results indicate that this allows algorithms that use time sublinear in the sublinear space used by the solutions, and there is a rich class of path problems that can be solved in these models which are impossible in classical streaming.<br/>(d) Modern systems need sophisticated streaming analyses. For example, streaming systems that collect usage data from users need private methods, and combining local differential privacy with streaming methods is an exciting direction; as another example, systems that analyze usage data might rely on embedding data into vectors with semantics, like word2vec and related deep learning methods. These methods need to work with polylogarithmic space for streaming. As another example, rich classes of graph problems are solvable in property testing framework with sublinear samples, can such classes be solved in streaming models too? The project highlights specific research challenges involved in developing streaming algorithms, and develops algorithms with provable performance guarantees on the tradeoff of resources used, approximation ratio, and probability of success. The project empirically evaluates them when possible.<br/><br/>One cannot take known statements of problems and hope to solve them using streaming algorithms. One needs to modify the problems a bit to be amenable to streaming. In classical streaming, ""heavy hitters"" and ""few terms"" properties helped achieve that. In similar vein, the project identifies certain natural phenomena which helps formulate versions of problems for which extreme streaming solutions can be developed. Contours of this are already seen in using graphical models that limit interactions between dimensions to circumvent high dimensional high cardinality cases, or reusing counters in frugal streaming or sampling the sketch data structure in privacy problems and pipelined streaming, or using the randomness in stream order. This may eventually lead to algorithmic and empirical insights. Overall vision of the project is to provide a principled perspective for design and analysis of streaming algorithms with extreme needs."
"1723352","CAPA: Collaborative Research: ARION: Taming Heterogeneity with DSLs, Approximation, and Synthesis","CCF","Information Technology Researc","10/01/2017","09/07/2017","Rastislav Bodik","WA","University of Washington","Standard Grant","Nina Amla","06/30/2021","$850,000.00","Luis Ceze, Emina Torlak, Alvin Cheung","bodik@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1640","021Z, 026Z, 7798, 8206, 8585","$0.00","Specialization and the arrival of new technologies are key forces motivating heterogeneous systems. Heterogeneity is already in use widely, with public clouds offering instances that are heterogeneous in both compute capabilities and storage. This project identifies the following forces that will make systems heterogeneous beyond just compute and storage, complicating programming and compilation beyond the challenges that we face today. This project develops Arion, a system for compiling programs onto heterogeneous platforms based on several unifying ideas. The Arion system will be evaluated on practically relevant workloads ranging from computer vision and virtual reality, to graph computations, machine learning and stream processing. The investigators will work with partners in industry to transfer research results to products, and the tools and software developed by this project will be released as open source.<br/><br/>The research in this project relies on four unifying ideas. The first thrust explores schedules and type systems separate a program's specification from its implementation strategy, enabling performance portability because one can select, without changing the program, its parallelism, locality, and hardware mapping. The second thrust uses domain-specific languages to describe not only programs but also artifacts used during compilation, such as schedules, resource-, and memory consistency models. This allows automatic synthesis of these artifacts. The third thrust uses resource models to bring scheduling and synthesis to large programs because the target program need not be scheduled or synthesized all at once. Instead, the compiler makes high-level decisions by estimating performance using a model before committing to low-level decisions. Finally, the investigators will use formal methods to lift programs into, and verify and synthesize programs in our DSLs, providing a high degree of automation. The verifiers and synthesizers are automatically generated from descriptions of DSLs."
"1656951","CRII: CIF: Universal Analysis of Optimization Algorithms","CCF","CRII CISE Research Initiation","02/15/2017","02/10/2017","Laurent Lessard","WI","University of Wisconsin-Madison","Standard Grant","Phillip Regalia","01/31/2020","$175,000.00","","laurent.lessard@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","026Y","7797, 7936, 8228","$0.00","Iterative optimization algorithms lie at the heart of modern data-intensive applications such as machine learning, computer vision, and data science. The choice of algorithm, or even the choice of tuning parameters for a particular algorithm, has a dramatic effect on performance and viability. Even simple variants of well-known algorithms can be troublesome to analyze and must be considered on a case-by-case basis with the help of either deep insights by experts or extensive numerical simulations. Then, theoretical analyses of algorithms may fail to provide accurate or faithful performance guarantees because they do not account for sensitivity to parameter choice, robustness to noise either inherent to the algorithm or built in to how the iterations are computed, or other sources of uncertainty.<br/><br/>The starting point for this research is to view iterative algorithms as dynamical systems with feedback. In gradient-based descent methods, for example, gradients are evaluated at each step and used to compute subsequent iterates. This research leverages tools from robust control (specifically, integral quadratic constraints and semidefinite programming) to develop a versatile, scalable, and modular framework capable of analyzing a variety of algorithms under different assumptions in an efficient and systematic manner. Of particular interest will be large-scale algorithms such as stochastic gradient descent and its variants, as well as distributed or asynchronous implementations."
"1657197","CRII: RI: Towards a Comprehensive Dynamic Subset Selection Framework","IIS","CRII CISE Research Initiation","09/01/2017","02/22/2017","Ehsan Elhamifar","MA","Northeastern University","Standard Grant","Jie Yang","08/31/2021","$174,863.00","","eelhami@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","026Y","7495, 8228","$0.00","Subset selection is the task of finding a small subset of informative items from a large ground set. Sequential data, including time-series and ordered data, form an important large part of modern datasets, requiring effective subset selection techniques. This project develops a unified framework for sequential subset selection that incorporates dynamic models and relationships among items, addresses both unsupervised and supervised problems and handles multi-modal data. The project develops practical tools for video and multi-media summarization, human information selection models that benefit human-in-the-loop systems, as well as providing interdisciplinary research training to students and disseminates research results. The research results from this project can impact several communities, such as machine learning, computer vision, signal processing, visualization, and health-informatics. <br/><br/>This research develops a comprehensive framework for subset selection in sequential datasets by addressing three important problems: i)it develops novel algorithms for sequential subset selection that incorporate dynamic models and relationships among sequential data and develops efficient optimization techniques to solve the problem; ii) it addresses the problem of unsupervised and supervised sequential subset selection by generalizing metric learning for subset selection and by tackling the challenge of the lack of pairs of positive and negative items; and iii) it proposes a multi-modal sequential subset selection framework to effectively take advantage of all modalities in datasets. The project brings together tools from sparse and low-rank recovery, convex optimization, message passing, metric learning and dynamical systems to tackle the problem. The project also designs objective evaluation methods to measure the performance of the methods."
"1707401","NeuroNex Technology Hub: Live Imaging of the C.elegans Connectome","DBI","Cross-BIO Activities","09/01/2017","07/14/2019","Oliver Hobert","NY","Columbia University","Continuing Grant","Edda Thiels","08/31/2020","$2,097,912.00","Hang Lu","or38@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","BIO","7275","1228, 8091, 9178, 9179","$0.00","The human brain is composed of billions of interconnected neurons that form highly complex neuronal circuits that process information and encode behavior. Many questions about these interconnected networks are unanswered: How variable are they from individual to individual, how do they change throughout life, how does the environment impact on them, and what are the genetic blueprints that generate these networks. Disruptions of the genetic blueprints that build neuronal networks are the likely cause of many human neurological diseases. In order to study neuronal networks in the brain, it is of paramount interest to easily visualize the patterns of connectivity of neurons, ideally in the context of live organisms. The cellular complexity of brains prevents such types of studies in complex organisms, and this project therefore uses a simple invertebrate model system, the nematode C.elegans, to visualize all the major neuronal connections of its simple nervous system. Previous studies have amply demonstrated that mechanisms of brain patterning discovered in C.elegans are conserved in other animals as well.  The investigators develop and use cutting-edge fluorescent reporter technology, combined with microscopical and computer vision technology to achieve this goal. The project's construction of animals in which most neuronal connections are fluorescently labeled provides a major resource. This resource is made available to the large field of C.elegans researchers who with that resource can study the many questions that relate to circuits in the brain, including the decoding of the nervous system's genetic blueprint. In addition, the project includes cutting-edge, interdisciplinary training opportunities for undergraduate and graduate students from diverse backgrounds, as well as postdoctoral fellows.<br/><br/>The project entails the development and dissemination of tools that empower the C.elegans neuroscience community to study the connectome of the nematode C.elegans. In the first phase, the technology hub develops two sets of tools: One group uses fluorescent-based reporter technology (GRASP and iBlinc as potential alternative) to generate a large number of transgenic C.elegans strains in which the main ""edges"" of the entire wiring diagram (i.e., pairwise combinations of neurons) are visualized. As part of this project, this resource is distributed throughout the C.elegans community to enable labs with long-standing interest in various aspects of neuronal development and function and with a focus on specific neuronal circuits and behaviors to use these synaptic labels to examine variability, development, and plasticity of these connections. In parallel, the other group develops microfluidic-based and automated image analysis technologies to precisely quantify the structure of the connectome and to enable high-throughput screening of worm population for defects in synaptic wiring. Computer vision and machine learning is used to score automatically disruptions of synaptic wiring to detect subtle changes in wiring. This NeuroTechnology Hub award is part of the BRAIN Initiative and NSF's Understanding the Brain activities."
"1747688","I-Corps: Platform for Scaled Autonomous Vehicle Technology","IIP","I-Corps","07/01/2017","07/13/2017","Evangelos Theodorou","GA","Georgia Tech Research Corporation","Standard Grant","Anita La Salle","05/31/2018","$50,000.00","","evangelos.theodorou@ae.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to help bring safe self-driving vehicles to market more quickly. Self-driving vehicles could eliminate over 90% of the 35,000 annual traffic fatalities that occur in the United States alone, convert commuting time into increased productivity or leisure time, and completely overhaul society's relationship with transportation. Billions of dollars are actively being invested by established automotive manufacturers, technology companies, startups, and researchers. The self-driving vehicle market is projected to be $27 billion by 2025, and grow to $77 billion by 2035. Despite all of the activity, there is still a gap in the ability to test self-driving vehicles in between the two current methods of computer simulations and full sized vehicles. Scaled vehicle technology provides an intermediate method to enable faster prototyping and testing of new algorithms and electronics in the real world without the risks and costs associated with full sized vehicles. This would provide immense value to society both in terms of accelerating a large new industry and reducing human casualties and injuries of the transportation system.<br/><br/>This I-Corps project will investigate the commercial potential of a self-driving vehicle industry testbed.  It involves a combination of new theory and algorithms with hardware systems that interact in the real world. The technology platform is a scaled vehicle testbed, 1/5 the size of a car and weighing 45 lbs.  It includes a sensor suite, onboard computer, and is ready for self-driving testing out of the box. Using these as autonomous vehicle testbeds, self-driving vehicle technologies can be tested in the real world without endangering people or expensive equipment. This technology is based on a fleet of prototype autonomous vehicle testbeds that have been used for controls, machine learning, and computer vision research successfully applied to the self-driving domain."
"1700383","Bridging the Gap in Automated and Connected Vehicle Technology Education","DUE","Advanced Tech Education Prog","04/15/2017","04/21/2017","Justin Morgan","OH","Sinclair Community College","Standard Grant","Virginia Carter","03/31/2021","$752,980.00","James Truxal, Thomas Freels","justin.morgan8747@sinclair.edu","444 West Third Street","Dayton","OH","454021460","9375124573","EHR","7412","1032, 9178, SMET","$0.00","Profound developments in research and resources are feeding technological advancements that will soon make self-driving vehicles commonplace. Today's leading-edge, semi-autonomous connected vehicles utilize technology that includes radars, cameras, lidar, multi-domain controllers, wireless vehicle-to-vehicle communications, and software applications, to name a few. The proposed postsecondary faculty professional development project, focused on automated and connected vehicle technology, will serve the NSF's mission of promoting the progress of science by creating industry-supported workshops and college-level educational resources to academically strengthen postsecondary automotive technician degree programs. Project activities such as summer institutes and professional development webinars for faculty will be designed to advance the field of vehicle technology education across the country by informing the educators of future technicians about the new generation of vehicles that are radically changing the automotive and transportation industries. In addition to professional development, the autonomous vehicles and other materials purchased through the grant will also be used to provide outreach activities in secondary schools, including creating a toolkit comprised of interactive demonstrations designed to orient secondary school students, teachers, and counselors to automotive technician educational pathways and careers. <br/><br/>Auto technician education must keep pace with the complexity of vehicles that incorporate aspects of robotics, machine learning, computer vision, and mechanical engineering. The goal of the proposed project is to increase the autonomous vehicle technology experience and knowledge of automotive technology educators throughout the nation and address the growing need for qualified, knowledgeable technicians with capacity to maintain and repair autonomous vehicles. The project will provide a week-long professional development session, webinars, and other learning events for 40 community college faculty annually. The project will also conduct outreach activities in secondary schools to impart information about the high wage, high-tech nature of the work of automotive technicians, important since many secondary school students skilled in science, math, and technology are not choosing automotive technician careers due in part to their misconceptions about what the work entails. The project will also monitor and document the skills and knowledge needed by the future automotive technician who will repair and maintain vehicles using a new set of skills not taught today. The project outcomes will be useful to other institutions with automotive technician education programs that need to update their curriculum to meet the changing needs in the industry or are struggling to diversify their student population interested in automotive technology careers."
"1733800","WORKSHOP:  Doctoral Consortium at the IEEE FG 2017 Conference","IIS","HCC-Human-Centered Computing, Robust Intelligence","03/01/2017","02/21/2017","Yan Tong","SC","University of South Carolina at Columbia","Standard Grant","Ephraim Glinert","02/28/2018","$12,500.00","","tongy@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","7367, 7495","7367, 7495, 7556, 9150","$0.00","This is funding to support a Doctoral Consortium (workshop) of approximately 8 graduate students from U.S. educational institutions, along with unpaid senior members of the research community as mentors, to be held in conjunction with the twelfth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2017), which will take place May 29-June 3, 2017, in Washington DC.   The IEEE FG conferences are the premier international forum for research in image- and video-based face, gesture, and body movement recognition.  Their broad scope includes new algorithms for computer vision, pattern recognition, and computer graphics, as well as machine learning techniques relevant to face, gesture and body motion for a variety of applications.  The conferences present research that advances the state of the art in these and related areas, leading to new capabilities in interactive systems, biometrics, surveillance, healthcare, and entertainment, and they play an important role in shaping related scientific, academic, and educational programs.  More information is available online at http://www.fg2017.org/.  The Doctoral Consortium will provide an opportunity for Ph.D. students whose dissertations are on topics related to automatic face and gesture recognition to present their proposed research, and receive constructive feedback from an invited committee of faculty and industry researchers, as well as from other students working in these areas.  The event will give students valuable exposure to outside perspectives of their work, and provide a comfortable forum in which to discuss and fine-tune their career objectives with members of the international research community, and identify areas that need further development. The workshop will also enable these young researchers to develop a network of contacts at a critical stage of their careers, and will foster a supportive community of scholars and spirit of collaborative research, which will have broad impact because graduate students who are conducting creative and groundbreaking work are the foundation and future of the community.  The organizers will make a particular effort to recruit and include students from underrepresented groups (women and underrepresented minorities) and from smaller schools. <br/><br/>The Doctoral Consortium will be a half-day event during the conference.  There will be five distinct aspects to the event.  Each student participant will be assigned a mentor based upon similarity of research interest and experience, who will discuss the work one-on-one with the student and provide constructive comments and guidance.  There will be an oral session in which all participants present their research to the group.  And there will also be a Doctoral Consortium poster session during the main conference, which will afford the students an opportunity to present their research to the rest of the community.  Extended abstracts (up to 4 pages long) of the students' research will be published online on the FG 2017 website.  Finally, students and mentors will all share a working lunch, in which mentors will briefly describe their career paths and student participants will ask the mentors questions about the different career paths.  The combination of these five activities will provide an excellent and structured way for students to communicate with other students as well as with established members of their research community."
"1648144","SBIR Phase I: Automated Census of Street Trees from Public Imagery","IIP","SMALL BUSINESS PHASE I","02/01/2017","02/05/2017","David Hall","CA","AEye Labs, Inc.","Standard Grant","Peter Atherton","01/31/2018","$225,000.00","","dhall@caltech.edu","166 S Parkwood Ave","Pasadena","CA","911074712","6266771806","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is that it will lead to more numerous and healthier city trees throughout the US. It will impact climate, energy, air and water quality and livability. This will be achieved through more effective and efficient management of the urban forest by municipalities, and through more and better organized citizen participation. Trees are a valuable asset for a community. Their benefits include: a reduction in energy use; improvement in air and water quality; increased carbon capture and storage; increased property values; and an improvement in individual and community well-being. To ensure maximum benefit and minimum cost, street trees need to be managed efficiently. In order to do this, municipalities need up-to-date inventories of trees. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project intends to automatically generate inventories of street trees from aerial and street-view imagery using cutting-edge computer vision and machine learning techniques. The inventory contains each tree's GPS location, its species, trunk diameter and an estimate of its health. The inventory is updated every time a new aerial or street-view image becomes available. Both tree detection and tree species classification are unprecedented applications based on combining the learning capabilities of deep convolutional networks, 3D geometry, and large annotated datasets that are collected by a combination of experts and crowdsourcing. This innovation makes it possible to maintain an always up-to-date, accurate, complete, US-wide street tree inventory at a fraction of the current cost."
"1659845","REU Site: Parallel and Distributed Computing","OAC","RSCH EXPER FOR UNDERGRAD SITES, Integrative Activities in Phys, EPSCoR Co-Funding","05/15/2017","03/27/2017","Sanjeev Baskiyar","AL","Auburn University","Standard Grant","Alan Sussman","04/30/2021","$291,590.00","Alvin Lim","baskisa@auburn.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","1139, 9134, 9150","026Z, 9150, 9250, SMET","$0.00","This project establishes a Research Experiences for Undergraduates (REU) Site to promote early engagement of undergraduate students in research.  A multi-disciplinary team of faculty from the departments of Computer Science and Software Engineering, Electrical and Computer Engineering, and Physics at Auburn University collaborate to provide participating students with research experiences in computational aspects of multiple disciplines.  It will expose students to high-performance computing and other cyberinfrastructure resources, provide hands-on experience in a collaborative research environment, and inspire them towards advanced STEM education and research careers.  Thus, the project will promote participation of students in graduate studies in computer science, electrical engineering and physics and will help maintain US leadership in computing education and research.  The society will benefit from the trained workforce in critical areas of national need of cyberinfrastructure, parallel and distributed computing and neuroimaging informatics.  Students from underrepresented groups will be encouraged to participate in the REU site.  Faculty will mentor students in carefully planned research projects which pose a range of scientific and technological challenges. The program fosters long-term mentoring relationships between students and faculty through collaboration.  The research outcomes can lower energy costs and carbon footprint in operating data centers and secure smart utility networks in US power grids.  The research aims for better understanding of enhancing urban traffic control, homeland security and location information to vehicles in a GPS degraded environment, plasma physics and better diagnosis of mental health diseases.<br/><br/>The objective of this project is to offer research opportunities to undergraduate students around a coherent theme of parallel and distributed computing.  The students will use cyberinfrastructure to solve problems in computer science, electrical and computer engineering and physics.  As energy consumption in computing is of current importance, students will solve problems with a common focus on energy reduction, reinforcing the cohort experience.  The multidisciplinary interaction will provide experience in crosscutting research.  The students will participate in training activities at the beginning of summer.  Next, they will conduct research under the supervision of mentors and write reports and deliver oral presentations.  The research project aims to contribute to the design of new thermal conscious computer systems to improve energy-efficiency and thereby longer component lifespan.  It will investigate novel data dissemination algorithms in ad-hoc cyberinfrastructure to correct GPS information using minimal bandwidth and time.   It explores innovative real-time distributed analytics on mobile cyberinfrastructure to support multi-user coordinated actions, which balance risk and reward.   It can contribute to the understanding of ion velocity ring instabilities in plasmas, novel deep learning algorithm using channel state finger-printing to reduce power for indoor location detection and machine learning algorithms in neuro-informatics to advance brain science."
"1722445","STTR Phase I:  A Clinical Decision Support Tool for Brain Magnetic Resonance Imaging (MRI) in Children","IIP","STTR PHASE I","07/01/2017","06/27/2017","Sinchai Tsao","CA","Advanced Medical Systems LLC","Standard Grant","Henry Ahn","07/31/2018","$225,000.00","","mail@sinchaitsao.com","3200 S Sepulveda Blvd","LOS ANGELES","CA","900344299","2134531833","ENG","1505","108E, 1505, 8018, 8042, 8089, 8091","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is to enable doctors to maximize the diagnostic information extracted from costly Pediatric Magnetic Resonance Imaging (MRI) scans of the brain. Annually, approximately 3.5 million brain MRI scans are performed with average prices with interpretation from $500 - $2,000. Even though the scans are expensive to acquire and interpret, doctors generally only use visual inspection of the images to diagnose abnormalities. The proposed software will be relatively cheap compared to the overall cost of an MRI and is expected to let doctors make accurate measurements of key structures in the brain. This is important because developmental and other diseases can cause small volume and surface area changes that cannot be easily seen by the human eye. By detecting these changes early, doctors should be able to treat the disease at early stages, potentially leading to better quality of life and financial savings. The proposed tool is also expected to enable doctors to better track treatment outcomes, by measuring the effect a drug or treatment has on particular structures in the brain. Ultimately, it is expected that the proposed technology will improve quality of care as well as reduce healthcare costs.<br/><br/>The proposed project leverages current advances in computational technology such as machine learning and computer vision to automatically measure volume, surface area and shapes in critical parts of the brain in children. These measurements are then compared to a large database of children over the ages of 0-12 years to determine if they have deviated from normal. Although MRIs have been increasing adopted as the diagnostic tool of choice for childhood brain disorders due to the lack of radiation, there has yet to be a tool that allows doctors to accurately measure changes in a child's brain from MRI scans. Because, in children, the brain is rapidly changing as she/he grows, it is difficult to determine visually whether the changes are due to normal development or disease. By measuring the brain accurately and comparing it to a large database already collected by the proposing team, it is expected that a pediatric doctor will be able to determine whether a child's brain has deviated from normal. It could also allow physicians to better select treatments and monitor the patient response to a specific therapy."
"1661280","Collaborative Research: ABI Innovation: Towards Computational Exploration of Large-Scale Neuro-Morphological Datasets","DBI","ADVANCES IN BIO INFORMATICS","07/15/2017","03/12/2020","Shaoting Zhang","NC","University of North Carolina at Charlotte","Standard Grant","Peter McCartney","06/30/2021","$387,057.00","","szhang16@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","BIO","1165","","$0.00","Analyzing single neuron's property is a fundamental task to understand the nervous system and brain working mechanism. Investigating neuron morphology is an effective way to analyze neurons, since it plays a major role in determining neurons' properties. Recently, the ever-increasing neuron databases have greatly facilitated the research of neuron morphology. However, the sheer volume and complexity of these data pose significant challenges for computational analysis, preventing the realization of the full potential of such data. This interdisciplinary project will seek for new avenue to assemble the massive neuron morphologies and provide a unified framework for neuroscientists to explore and analyze different types of neurons. The research is able to tackle many challenges in neuroscience which are hard to solve with previous methods, including fine-grained neuron identification, latent pattern discovery and exploration, etc. The large-scale methods being developed will be particularly beneficial in the future of neuroscience, since more and more neurons are reconstructed and added to the databases. The computational methods and tools developed are very likely to be applicable for solving other bioinformatics problems, especially those dealing with large-scale datasets. The broader impact of this project not only includes educational support for undergraduate researchers and high school students, particularly women and those underrepresented groups, but also contributes to the research of neuroscience and other STEM fields.<br/><br/>The long-term goal of this project is to develop effective computational methods and tools for neuroscientists to interactively explore large-scale neuron databases with ultra-fine-grained accuracy, in real-time. This research has a strong multidisciplinary component that involves a nexus ideas from machine learning, information retrieval, and neuroinformatics. Particularly, novel ideas will be implemented in three inter-related components through the whole framework. The first one addresses the accurate and efficient neuron reconstruction and tracing based on deep learning models. The second addresses the efficient discovery of relevant instances among large-size neuron databases via multi-modal and online binary coding methods. The third part addresses intelligent visualization and interaction schemes for knowledge discovery and mining, equipped with interactive coding that can incorporate domain experts' feedback to enhance the query algorithms for fine-tuned results. Compared with previous methods and systems, this project will open a new avenue to assist neuroscientists analyzing and exploring large-scale neuron databases with high efficiency, accuracy, and robustness. The performance of proposed methods will be validated using public neuro-morphological databases (e.g., NeuroMorpho, BigNeuron) and compared with several benchmarks. The effectiveness of the tools to be developed will be evaluated by neuroscientists on domain-specific hypothesis-driven applications. The outcome of the project will be made available at the following websites: http://webpages.uncc.edu/~szhang16/  and https://github.com/divelab/."
"1718738","RI: Small: Multi-View Latent Class Discovery and Prediction with a Streamlined Analytics Platform","IIS","Robust Intelligence","08/01/2017","07/27/2017","Jinbo Bi","CT","University of Connecticut","Standard Grant","Rebecca Hwa","07/31/2021","$450,000.00","Song Han","jinbo.bi@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7495","7495, 7923","$0.00","Discovering latent subgroups in a sample is an important problem in many scientific disciplines. Social scientists identify subgroups within a population based on behavioral patterns to examine differential effects of social status. Engineers recognize malfunctions of a manufacturing system based on performance measures to detect design defects. Physicians define subtypes of a disorder on the basis of clinical symptoms to identify associated genetic risk factors. This kind of problem involves two sets of variables: a set of descriptors describing the issue (e.g., behavioral patterns, or symptoms) and a set of moderators or predictors (e.g., social status, or genetic factors). The ability to accurately predict the latent classes (e.g., disease subtypes) from predictors (e.g., genetic risk) in the absence of observed descriptors (e.g., before symptoms are developed) will advance many of these disciplines. This project aims to develop an effective and efficient platform of machine learning algorithms to solve this problem. The team will effectively integrate research and teaching to engage students into the proposed study. Validated methods and software will be broadly disseminated through the project web repository and scientific presentations.<br/><br/>This project addresses the latent class discovery and prediction problem by deriving novel and efficient approaches, including multi-view co-clustering, multi-view subspace clustering, multi-objective optimization of co-training, and multi-modal deep learning methods. Parallel and distributed algorithms will be developed to implement and scale up these methods. A streamlined analytics platform will be constructed to maximize the utility of the proposed approaches in real-world applications. The proposed solutions will be evaluated in the analysis of large-scale sensory and behavioral data. By collaborating with domain experts, the project will (1) identify risk factors for problematic human behaviors such as binge drinking; and (2) locate the sensory features most discriminative of gait abnormalities due to neurological disorders such as Parkinson's disease or stroke."
"1740385","I-Corps: Lymphedema Intervention Exercise for Breast Cancer Survivors","IIP","I-Corps","03/15/2017","03/24/2017","Yao Wang","NY","New York University","Standard Grant","Anita La Salle","08/31/2018","$50,000.00","Mei Fu","yw523@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is its benefit for millions of individuals who are at risk for or have developed chronic lymphedema, including patients who have been treated for breast cancer and other cancers (head and neck, prostate, melanoma, gynaecological cancer). Lymphedema is a progressive and chronic swelling with many distressful symptoms after cancer treatment. Each year, 1.38 million women worldwide and more than 230,000 women in the US are diagnosed with breast cancer. At least 20-40% of breast cancer survivors have developed lymphedema and the rest of them are at lifelong risk for lymphedema. This project further develops a lymphatic therapy effective in reducing the risk of lymphedema and relieving lymphedema symptoms for breast cancer survivors. The approach may potentially be applied more broadly to individuals with chronic heart failure, end-stage renal disease and hypertension with similar abnormal fluid accumulation symptoms.<br/><br/>This I-Corps project explores the commercial potential of a lymphatic therapy.  The core technology consists of a physical therapy system that can automatically and accurately track a patient's movement and evaluate it against the desired movement. The system provides instant feedback on whether a patient is performing an exercise correctly and how should the patient make adjustments in their movement.  Furthermore, the system will be designed to motivate the patient to perform the therapy correctly and regularly. The system will leverage the novel computer vision and machine learning algorithms being developed in an on-going research project in order to overcome the sensor constraints in detecting joint positions and the significant variability of the normal range of the desired movement, which are the major technical obstacles faced by current sensor-based physical therapy systems."
"1720452","Multiscale Algorithms for the Geometric Analysis of Hyperspectral Data","DMS","COMPUTATIONAL MATHEMATICS, MSPA-INTERDISCIPLINARY, EnvS-Environmtl Sustainability","10/01/2017","06/01/2017","Demetrio Labate","TX","University of Houston","Standard Grant","Leland Jameson","09/30/2021","$270,284.00","Bernhard Bodmann, Saurabh Prasad","dlabate@math.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","MPS","1271, 7454, 7643","8007, 9263","$0.00","Hyperspectral imaging is a sensing technique that collects hundreds of narrowband images from across the electromagnetic spectrum. By both going beyond the visible spectrum and accurately discriminating wavelengths within the visible range, this technology can be remarkably powerful for distinguishing different materials when standard imagery is ineffective. As a result, hyperspectral remote sensing offers unique capabilities for tasks that include monitoring the development and health of crops, mapping oil spills and invasive species, and detecting objects that may be camouflaged. With modern remote sensing applications not being constrained to satellite images, however, the image acquisition in many scenarios is no longer under controlled conditions, because illumination, physical parameters, and viewing angles may change over time and objects of interest may be partially occluded. This investigation introduces a new generation of mathematical and algorithmic tools that are designed to provide robust classification of hyperspectral data under such realistic conditions. The project aims to develop a new class of analysis and classification algorithms for hyperspectral data that are robust with respect to changes of illumination, viewpoint, and physical conditions.  The results are intended to have direct application to the monitoring of environmental conditions in coastal wetlands and to other observations of societal, economic, and national security interest.<br/><br/>While hyperspectral imaging and image processing have been well developed within the remote-sensing community, image acquisition in remote sensing may occur in conditions where illumination, physical parameters, and viewing angle change over time. This research program combines ideas from sparse representations, multilayer convolutional networks, and machine learning to address the challenges to imaging posed by such changing conditions. A novelty of the approach is the adaptation of methods from sparse representations and shearlets, an anisotropic multiscale system that is particularly effective at capturing the directional content of multidimensional data. This approach provides the basis for constructing a deep learning neural convolutional network tailored to hyperspectral data and designed to generate stable and robust feature vectors. This investigation aims to develop an efficient multiscale representation that is customized to the specifics of hyperspectral data. The scattering transform will be adapted in combination with shearlets by exploiting the covariance properties of shearlets under affine transformations to build stable and viewpoint-invariant features for hyperspectral data. A novel hierarchical scheme for classification optimized for the specific structure of hyperspectral data and sparsity-based inpainting methods to restore hyperspectral data corrupted by occlusions will be developed. These new algorithms will be used for the analysis of hyperspectral data to monitor environmental conditions of coastal wetlands, a challenging case study of great social and economic importance."
"1661289","Collaborative Research: ABI Innovation: Towards Computational Exploration of Large-Scale Neuro-Morphological Datasets","DBI","ADVANCES IN BIO INFORMATICS","07/15/2017","07/06/2017","Shuiwang Ji","WA","Washington State University","Standard Grant","Peter McCartney","05/31/2020","$296,001.00","","sji@tamu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","BIO","1165","","$0.00","Analyzing single neuron's property is a fundamental task to understand the nervous system and brain working mechanism. Investigating neuron morphology is an effective way to analyze neurons, since it plays a major role in determining neurons' properties. Recently, the ever-increasing neuron databases have greatly facilitated the research of neuron morphology. However, the sheer volume and complexity of these data pose significant challenges for computational analysis, preventing the realization of the full potential of such data. This interdisciplinary project will seek for new avenue to assemble the massive neuron morphologies and provide a unified framework for neuroscientists to explore and analyze different types of neurons. The research is able to tackle many challenges in neuroscience which are hard to solve with previous methods, including fine-grained neuron identification, latent pattern discovery and exploration, etc. The large-scale methods being developed will be particularly beneficial in the future of neuroscience, since more and more neurons are reconstructed and added to the databases. The computational methods and tools developed are very likely to be applicable for solving other bioinformatics problems, especially those dealing with large-scale datasets. The broader impact of this project not only includes educational support for undergraduate researchers and high school students, particularly women and those underrepresented groups, but also contributes to the research of neuroscience and other STEM fields.<br/><br/>The long-term goal of this project is to develop effective computational methods and tools for neuroscientists to interactively explore large-scale neuron databases with ultra-fine-grained accuracy, in real-time. This research has a strong multidisciplinary component that involves a nexus ideas from machine learning, information retrieval, and neuroinformatics. Particularly, novel ideas will be implemented in three inter-related components through the whole framework. The first one addresses the accurate and efficient neuron reconstruction and tracing based on deep learning models. The second addresses the efficient discovery of relevant instances among large-size neuron databases via multi-modal and online binary coding methods. The third part addresses intelligent visualization and interaction schemes for knowledge discovery and mining, equipped with interactive coding that can incorporate domain experts' feedback to enhance the query algorithms for fine-tuned results. Compared with previous methods and systems, this project will open a new avenue to assist neuroscientists analyzing and exploring large-scale neuron databases with high efficiency, accuracy, and robustness. The performance of proposed methods will be validated using public neuro-morphological databases (e.g., NeuroMorpho, BigNeuron) and compared with several benchmarks. The effectiveness of the tools to be developed will be evaluated by neuroscientists on domain-specific hypothesis-driven applications. The outcome of the project will be made available at the following websites: http://webpages.uncc.edu/~szhang16/  and https://github.com/divelab/."
"1711936","NSF Postdoctoral Fellowship in Biology FY 2017: Leveraging digital herbaria and crowd-sourced photos to understand climate-driven disruption of community flowering phenology","DBI","Collections Postdocs","11/01/2017","06/16/2017","Ian Breckheimer","WA","Breckheimer             Ian            K","Fellowship","Amanda Simcox","11/30/2019","$138,000.00","","","","Seattle","WA","981074248","","BIO","001Y","","$0.00","This is an NSF Postdoctoral Research Fellowship in Biology, under the program Research Using Biological Collections. The fellow, Ian Breckheimer, is conducting research and receiving training that utilizes biological collections in innovative ways, and is being mentored by Andrew Richardson at Harvard University. Specifically, the fellow will use digital records from museum collections and crowd-sourced photos uploaded by the public to measure where and when plants in different environments are exposed to risky climatic events like drought and growing season freezing during vulnerable periods in their seasonal cycle.  Climate plays an important role in determining which organisms occur in which habitats, but exactly how this happens is still unclear, and understanding the mechanisms will allow us to better forecast the economic and ecological impacts of changes to the environment. Recent work suggests that, for non-woody plants like wildflowers, extreme events such as freezing or drought during vulnerable periods such as flowering strongly influence which plants can survive in which environments. The fellow will test this hypothesis for a large group of plants that inhabit mountain meadows in the Western USA. These are economically important ecosystems that attract millions of visitors each summer, and also experience extreme variations in climate. This project will advance our understanding of how climate affects the distribution of organisms, identify environments and species at risk of climatic disruption, and provide an important proof-of-principle for using crowd-sourced images to track spatial and temporal patterns in ecosystems.<br/><br/>At approximately a dozen heavily-visited natural areas in the western USA, the fellow will use species occurrence information from digital herbarium collections along with image classifications from volunteer citizen-scientists and cutting-edge computer vision algorithms to identify common subalpine and alpine wildflowers in public geo-located photos hosted on social media platforms. By accounting for variation in the observation process, new statistical tools will allow the fellow to use these unstructured and imperfect observations to reliably measure the seasonal timing of flowering and fruiting.  The fellow will then combine these observations with environmental information from satellites and weather stations to understand how that reproductive timing is affected by climate, and how geographic distributions and species responses affect the risk of disruptive climatic events during reproduction at each site. In addition to developing new knowledge about climatic disruption of ecosystems, this fellowship will support training in cutting-edge modeling and machine learning techniques, and advance new methods for combining citizen observations and museum collections.  Results from these studies will be published in peer-reviewed journals and presented at scientific meetings."
"1650888","Prediction during Processing of Repairs in Spoken Language","BCS","Linguistics","06/15/2017","06/14/2017","Fernanda Ferreira","CA","University of California-Davis","Standard Grant","Tyler Kendall","11/30/2020","$246,276.00","","fferreira@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","SBE","1311","1311, 9178, 9179, 9251","$0.00","Everyone is familiar with the experience of being disfluent. Despite their best efforts, on average speakers will produce a filler such as ""uh"" or an ""um"" every twenty words, and they may also make a speech error which will need to be repaired. Previous research has established some of the causes of disfluency and has revealed that disfluencies of different types characterize different types of speakers; for example, individuals with ADHD are more likely to produce speech errors than to use fillers, and those without ADHD show the opposite pattern. Far less is known, however, about how listeners are able to understand speakers despite the presence of this noise in the linguistic signal. Early proposals tested the hypothesis that listeners must somehow ignore disfluencies, but more recent studies show that disfluencies are only partially suppressed, indicating that disfluencies affect how listeners interpret the sentence they hear and even how they evaluate the speaker. In addition, these newer experiments show that when listeners hear a word spoken in error, they use the error to predict what the speaker is likely to say instead. This prediction mechanism is helpful for two reasons: first, because it allows the listener to get a head start on processing the speaker's intended meaning; and second, because it helps the listener come up with a more sensible interpretation of the utterance should the speaker fail to detect and correct his or her error. Understanding how these prediction mechanisms operate is especially relevant for our understanding of language and aging; speakers are known to become more disfluent as they age, making their speech harder to understand. This is a pressing concern given the aging population of the United States. This work will also help enhance speech recognition devices that must be robust to disfluency if they are to operate on natural, spontaneous speech. Devices that respond to voice commands are now in millions of Americans' homes and pockets, and as they become more common, users will increasingly come to expect them to work smoothly and reliably. <br/><br/>The experiments that will be conducted for this project use two complementary methods for assessing people's comprehension of speech on a millisecond-by-millisecond basis: recording of brain electrophysiological activity (EEG), and recording of eye movements to visual displays presented during listening tasks. The experiments are designed to answer three core questions about prediction during processing of disfluencies: (1) When do listeners begin to predict? (2) What precisely is the content of the prediction (a specific word, a general category)? (3) What is the fate of an incorrect prediction? That is, given that listeners' expectations will not always match the speaker's output, how do listeners reconcile their prediction with any discrepant content? This project will involve students who will be trained in experimental psycholinguistics, statistics, and computational methods, allowing them to gain experience in designing and interpreting data, as well as in preparing scientific reports for presentation and publication."
"1734245","SBE-RCUK: CompCog: Modeling the Development of Phonetic Representations","BCS","Linguistics, DS -Developmental Sciences, Perception, Action & Cognition","09/01/2017","06/22/2017","Naomi Feldman","MD","University of Maryland College Park","Standard Grant","Betty Tuller","08/31/2021","$520,058.00","","nhf@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","SBE","1311, 1698, 7252","003Z","$0.00","Listeners' processing of speech is tuned to their native language. For example, Japanese listeners categorizing English [l] and [r] do not rely on the same aspects of the speech signal that native English listeners do. This project uses computational models to investigate how children develop language-specific perceptual strategies. A better understanding of this perceptual learning process could lead to better diagnosis and treatment of developmental language impairments that have a perceptual basis and can provide insight into the difficulties that listeners face when learning a second language in adulthood. Building computational models of how children learn their native language from the speech around them can also lead to improved speech technology for low-resource languages (languages that are not spoken by many people in the world or that lack digital resources such as large-scale, annotated databases), ultimately leading to systems that learn more effectively using little or no transcribed audio. Such systems could become important tools for documenting and analyzing endangered and minority languages and could help make speech technology more universally available.<br/><br/>A series of simulations tests the hypothesis that children's processing of speech can become specialized for their native language through a process of dimension learning that does not rely on knowledge of sound categories. Two models that use dimension learning are proposed, drawing on representation learning methods that have performed well in low-resource automatic speech recognition, where extensive labeled training data are not available. The first model relies on temporal information as a proxy for sound category knowledge, while the second model relies on top-down information from similar words, which infants have been shown to use. Each model is trained on speech recordings from a particular language and is evaluated on its ability to predict how adults and infants with that language background discriminate sounds. The research will yield new methods for training and testing cognitive models of language with naturalistic speech recordings and has the potential to significantly impact theories of how and when children learn about the sounds of their native language."
"1650791","Doctoral Dissertation Research: Categorization and Segmentation Inside and Outside Language","BCS","DDRI Linguistics","03/01/2017","03/02/2017","William Idsardi","MD","University of Maryland College Park","Standard Grant","William Badecker","08/31/2018","$17,617.00","Rochelle Newman, Christopher Heffner","idsardi@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","SBE","8374","1311, 9178, 9179, 9251","$0.00","Humans hear the speech of others almost every day.  Understanding that speech is often quite difficult, as can be seen when interacting with automated speech recognition technologies.  Doing so requires the use of complex yet surprisingly effective cognitive abilities.  But are the mental tools that humans use to understand speech used for speech only, or are there ones that are applied to multiple purposes?  This project seeks to link language learning and perception to other tasks to determine the extent to which speech perception shares an underlying basis with other cognitive processes.  This project will enrich the understanding of cognition.  Furthermore, it could open up new avenues for designing technologies to better improve speech processing as well as lead to new methodologies to train people learning a second language.<br/><br/>To study the domain-specificity of speech perception, this project will center on two particular aspects of speech: category learning and segmentation.  Accurate comprehension of spoken language demands the segmentation of continuous speech into discrete words, just as the perception of actions demands the segmentation of perceived activity into discrete events.  And listeners must learn to deal with the variability in speech sounds in order to treat some sounds as belonging to the same category, just as they must group, say, disparate dog sounds as belonging to a single ""barking"" category.  One experiment will investigate the extent to which rate information can affect the segmentation of events, while another will assess the extent to which biases that seem to be present in phonetic category learning can also be found in non-speech category learning.  A third experiment will use magnetoencephalography (MEG) to probe the acquisition of certain types of speech sound categories.  All told, the research will illuminate whether and which processes in language and in other domains parallel each other, which relates to the notion of modularity, the idea that the brain houses separate components that have evolved to perform individual functions in the world."
"1730479","Speech Across Dialects of English (SPADE): large-scale digital analysis of a spoken language across space and time","SMA","Digging into Data Challenge","03/15/2017","07/23/2019","Jeff Mielke","NC","North Carolina State University","Standard Grant","Joan Maling","08/31/2021","$199,791.00","Erik Thomas, Robin Dodsworth, Tyler Kendall, Paul Fyfe","jimielke@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","SBE","8677","1311, 9179, SMET","$0.00","This project focuses on new and fast ways to analyze speech across dialects--here, dialects of English, but the process can eventually be used for any language. The researchers take methods from computer science and put them to work with tools and methods from speech science, linguistics and digital humanities to find out objectively how much the sounds of English dialects across the Atlantic vary now and in the past. Scholars across the humanities and social sciences already routinely analyze huge amounts of written English quickly and easily.  In fact, the electronic tools for searching texts and, within seconds, obtaining summaries that one can see are available to anyone. However, speech research is only now entering its own ""big data"" revolution. Past linguistic research tended to carry out detailed analyses of a few aspects of speech from one or a few languages or dialects. The current scale of speech research studies has shaped our understanding of spoken language and the kinds of questions that we ask. Today, massive digital collections of transcribed speech are available from many languages, gathered for numerous purposes: from oral histories to large datasets for training speech recognition systems to legal and political interactions. Sophisticated speech processing tools exist to analyze these data, but they require substantial technical skill. Combining these data and tools allows linguists to answer fundamental questions about the nature and development of spoken language. <br/><br/>This collaborative project seeks to establish the key tools to enable large-scale speech research to become as powerful and pervasive as large-scale text mining. This ability to quickly and easily analyze speech in many English dialects should be applicable in computational, forensic, and clinical approaches to speech, and forensic and clinical speech applications, and useful to literary scholars, sociologists, anthropologists, historians, political scientists. What is learned from this project will be shared with the public through an interactive sound mapping website.<br/><br/>This award was made as part of Round 4 of the Digging Into Data Challenge, an international funding opportunity designed to foster research collaboration across countries and to encourage innovative approaches to analyzing large data sets in the social sciences and humanities. The U.S based researchers will collaborate with scholars in Canada and the U.K. to achieve the goals of this project."
"1651108","Second Dialect Acquisition and Stylistic Variation in Mobile Speakers","BCS","Linguistics","09/01/2017","09/06/2017","Jennifer Nycz","DC","Georgetown University","Standard Grant","Joan Maling","02/28/2021","$217,477.00","","jn621@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","SBE","1311","1311, 9179, 9251, SMET","$0.00","How do people change their accents after moving to new regions? Mobile people typically acquire some, but not all, of a new region's dialect features; this project will investigate the developmental, social, and linguistic factors determining what kinds of changes occur, and the ways in which mobile speakers vary their use of accent features to achieve social goals in conversation. This research will help us understand how words and speech sounds are represented in the mind as well as the flexibility of language over the lifespan.<br/><br/>The results of this project, the largest study of second dialect acquisition to date, will have several practical applications. Understanding how accents vary and change over a lifetime is crucial for improving language- and dialect-teaching pedagogy as well as speech-recognition and speech-generating technologies. Knowing how mobile speakers are likely to change their accents can aid forensic and intelligence investigations involving speaker profiling or judging authenticity in cases of possible voice disguise. Finally, improved awareness of how speech varies and how this variation is used by everyone to communicate is key to stemming language-based prejudice and discrimination.<br/><br/>The researcher will interview eighty native speakers of English: forty natives of Toronto, Canada, who relocated as adults to New York City, and forty natives of New York City who relocated as adults to Toronto. Both speaker samples will be stratified by gender, age of arrival in their new region, and number of years living in the new region. Each mobile speaker will participate in activities along with a friend who is a lifelong resident of the migrant's current city, including a conversational interview and a series of reading and judgment tasks. Activities will be audio-recorded, enabling the researcher to compare spontaneous and read speech as well as speech associated with different topics and expressed attitudes. Four vowel variables which distinguish the two cities will be analyzed using appropriate phonetic and statistical methods. The results of this analysis will be used to evaluate existing claims and generate new hypotheses about mobility-induced dialect change and to determine how patterns of variation reflect the social and attitudinal content of speech."
"1748058","EAGER: Collaborative Research: Interactive Dialog Agents for Social Language Development and Listening Comprehension","IIS","Cyberlearn & Future Learn Tech","10/01/2017","02/21/2019","Michael Neff","CA","University of California-Davis","Standard Grant","Tatiana Korelsky","06/30/2020","$104,818.00","Emily Solari","mpneff@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8020","7916, 8045, 8089, 9251","$0.00","Practicing oral language skills supports children's later ability to process written text. Low socioeconomic status children receive lower levels of quality home language input, which negatively affects language development. Many children already suffer from a significant deficit in reading comprehension by the time they begin formal education, a deficit they may never overcome. This EArly Grant for Exploratory Research (EAGER) project will investigate improving children's home language environment with pedagogical learning agents, animated story characters and a child-like narrator who carry on a dialog with a child while telling a story. Story characters will talk directly to the child, tell the story from different perspectives, ask the child questions, phrase those questions in different ways using different vocabulary, and thus engage the child in understanding the story and improving their social language and listening comprehension skills.  Pedagogical learning agents are a cheap and easily deployable technology that could readily be provided to children in any socioeconomic group.<br/><br/>This project will develop technology for expressive interactive storytelling agents that model human conversational storytelling with a high level of social engagement. This will require methods to effectively manage a spoken dialog with a child, using speech recognition and text-to-speech output. The project will scaffold from a deep computational representation of narrative to generate different dialogs around the same story. Novel algorithms for natural language generation will support asking different types of questions at different story points and generation of story events with first person and direct speech to allow the characters to talk.  The humanoid child narrator will model high immediacy nonverbal behaviors shown to increase engagement and positive affect. The project's prototype will be informed by regular evaluation by children at the UC Davis Reading and Academic Development Center, as the project assesses the promise of this exploratory technology to improve social language and comprehension skills."
"1748056","EAGER: Collaborative Research: Interactive Dialog Agents for Social Language Development and Listening Comprehension","IIS","Cyberlearn & Future Learn Tech","10/01/2017","02/26/2019","Marilyn Walker","CA","University of California-Santa Cruz","Standard Grant","Tatiana Korelsky","12/31/2020","$235,182.00","","mawalker@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","8020","7916, 8045, 8089, 9251","$0.00","Practicing oral language skills supports children's later ability to process written text. Low socioeconomic status children receive lower levels of quality home language input, which negatively affects language development. Many children already suffer from a significant deficit in reading comprehension by the time they begin formal education, a deficit they may never overcome. This EArly Grant for Exploratory Research (EAGER) project will investigate improving children's home language environment with pedagogical learning agents, animated story characters and a child-like narrator who carry on a dialog with a child while telling a story. Story characters will talk directly to the child, tell the story from different perspectives, ask the child questions, phrase those questions in different ways using different vocabulary, and thus engage the child in understanding the story and improving their social language and listening comprehension skills.  Pedagogical learning agents are a cheap and easily deployable technology that could readily be provided to children in any socioeconomic group.<br/><br/>This project will develop technology for expressive interactive storytelling agents that model human conversational storytelling with a high level of social engagement. This will require methods to effectively manage a spoken dialog with a child, using speech recognition and text-to-speech output. The project will scaffold from a deep computational representation of narrative to generate different dialogs around the same story. Novel algorithms for natural language generation will support asking different types of questions at different story points and generation of story events with first person and direct speech to allow the characters to talk.  The humanoid child narrator will model high immediacy nonverbal behaviors shown to increase engagement and positive affect. The project's prototype will be informed by regular evaluation by children at the UC Davis Reading and Academic Development Center, as the project assesses the promise of this exploratory technology to improve social language and comprehension skills."
"1748642","RI:  EAGER:  Collaborative Research:  Adaptive Heads-up Displays for Simultaneous Interpretation","IIS","Robust Intelligence","09/01/2017","08/13/2017","Graham Neubig","PA","Carnegie-Mellon University","Standard Grant","D.  Langendoen","10/31/2019","$150,000.00","","gneubig@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7916","$0.00","Interpretation, the task of translating speech from one language to another, is an important tool in facilitating communication in multi-lingual settings such as international meetings, travel, or diplomacy. However, simultaneous interpretation, during which the results must be produced as the speaker is speaking, is an extremely difficult task requiring a high level of experience and training. In particular, simultaneous interpreters often find certain content such as technical terms, names of people and organizations, and numbers particularly hard to translate correctly. This Early Grant for Exploratory Research project aims to create automatic interpretation assistants that will help interpreters with this difficult-to-translate content by recognizing this content in the original language, and displaying translations on a heads-up display (similar to teleprompter) for interpreters to use if they wish. This will make simultaneous interpretation more effective and accessible, making conversations across languages and cultures more natural, more common, and more effective and joining communities and cultures across the world in trade, cooperation, and friendship.<br/><br/>Creating these systems is a technically challenging problem and has not previously been attempted. One challenge is that simultaneous interpretation is already a cognitively taxing task, and any interface must not unduly increase the cognitive load on the interpreter by being too intrusive. Reducing this cognitive load requires an interface that can decide when to provide translation suggestions and when to refrain from doing so. To achieve this goal, this project will develop methods that are robust to speech recognition errors, and learn what to display by observing the interpreters' interpretation results. The utility of the proposed framework will be evaluated with respect to how much it improves the ability of interpreters to produce fluent, accurate interpretation results, as well as the cognitive load the additional interface imposes on them."
"1748663","RI:  EAGER:  Collaborative Research:  Adaptive Heads-up Displays for Simultaneous Interpretation","IIS","Robust Intelligence","09/01/2017","08/14/2017","Hal Daume","MD","University of Maryland College Park","Standard Grant","D.  Langendoen","12/31/2019","$150,000.00","Jordan Boyd-Graber, Leah Findlater","hal@umiacs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7916","$0.00","Interpretation, the task of translating speech from one language to another, is an important tool in facilitating communication in multi-lingual settings such as international meetings, travel, or diplomacy. However, simultaneous interpretation, during which the results must be produced as the speaker is speaking, is an extremely difficult task requiring a high level of experience and training. In particular, simultaneous interpreters often find certain content such as technical terms, names of people and organizations, and numbers particularly hard to translate correctly. This Early Grant for Exploratory Research project aims to create automatic interpretation assistants that will help interpreters with this difficult-to-translate content by recognizing this content in the original language, and displaying translations on a heads-up display (similar to teleprompter) for interpreters to use if they wish. This will make simultaneous interpretation more effective and accessible, making conversations across languages and cultures more natural, more common, and more effective and joining communities and cultures across the world in trade, cooperation, and friendship.<br/><br/>Creating these systems is a technically challenging problem and has not previously been attempted. One challenge is that simultaneous interpretation is already a cognitively taxing task, and any interface must not unduly increase the cognitive load on the interpreter by being too intrusive. Reducing this cognitive load requires an interface that can decide when to provide translation suggestions and when to refrain from doing so. To achieve this goal, this project will develop methods that are robust to speech recognition errors, and learn what to display by observing the interpreters' interpretation results. The utility of the proposed framework will be evaluated with respect to how much it improves the ability of interpreters to produce fluent, accurate interpretation results, as well as the cognitive load the additional interface imposes on them."
"1717324","RI: Small: General Intelligence through Algorithm Invention and Selection","IIS","Robust Intelligence","09/01/2017","07/25/2017","Julian Togelius","NY","New York University","Standard Grant","Roger Mailler","08/31/2020","$427,000.00","Andrew Nealen","julian.togelius@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7495","7495, 7923","$0.00","Creating better artificial intelligence has plenty of applications in different areas of society, from self-driving cars and aircraft to production planning, control of machines and music composition. Most current artificial intelligence research focuses on creating algorithms that can only do a single thing, or solve a single problem.  To achieve artificial general intelligence we must learn how to create algorithms that can solve many different problems, without a human having to adjust the algorithm for every problem. The research in this project aims to understand how such artificial general intelligence can be created. The basic idea is to build algorithms that can create their own more specific algorithms, and learn to automatically select the right algorithm for the right problem. In order to develop these algorithms, we need a large set of good problems to test them on. Games are widely used to test AI algorithms, because they model real-world problems but are fast and easy to execute. The general algorithms developed in this project will be tested on a set of classic games, and a real-time strategy game.<br/><br/>This research project aims to investigate how we can create more general artificial intelligence through online stochastic search for algorithms, combined with and informed by online selection among discovered algorithms. In other words, the project will investigate the combination of genetic programming in the space of tree search algorithms with algorithm selection, also called hyper-heuristics, for creating more general problem-solving abilities. These capabilities will be evaluated through a sequence of experiments on two different test beds.  Successful completion of the research will clarify the potential of search in algorithm space as a method for creating more general artificial intelligence, and produce a number of algorithms. This includes both the algorithms that will be designed for searching for algorithms and searching among algorithms, as well as the new algorithms that will be discovered by the search in algorithm space. The methods produced are expected to ultimately be generally applicable to a large number of problems."
"1747455","Artificial Intelligence in Interactive Digital Entertainment 2017:   Travel Support for the Doctoral Mentoring Program and the Playable Experiences Track","IIS","HCC-Human-Centered Computing","07/01/2017","07/05/2017","Brian Magerko","GA","Georgia Tech Research Corporation","Standard Grant","Ephraim Glinert","01/31/2018","$19,984.00","","magerko@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","7367, 7556","$0.00","AAAI's Artificial Intelligence in Interactive Digital Entertainment (AIIDE) is a yearly conference that brings together representatives from both academia and industry to present novel research and discuss interesting problems in the area of human-centered artificial intelligence and interactive media.  AIIDE 2017, the 13th conference in the series, will take place October 5-9 at the Snowbird Resort in Utah.  This is funding to provide scholarships to support participant travel for two tracks at the conference that both have the goal of strengthening and diversifying this growing research community: (1) the AIIDE doctoral mentoring program and (2) the AIIDE playable experiences track.  The organizers expect to have 10 students participating in the doctoral mentoring program, and 5 playable experiences showcased in the playable experiences track.  Artificial intelligence in digital entertainment is an inherently interdisciplinary research area, including artificial intelligence, human-computer interaction, psychology, digital media, and modeling/simulation, as well as humanities and the arts.  AIIDE serves an important role as an interface between AI and society, through its focus on digital entertainment and related applications.  The conference investigates not just how to make intelligent machines, but how to produce transformative technology that impacts engineers, designers, and authors on potentially large scale projects with broad cultural visibility.  The conference also endeavors to grow relationships between academic research and industrial practice.  More information about the conference is available online at https://sites.google.com/view/aiide2017/.  The AIIDE doctoral mentoring program and playable experiences tracks are crucial to the health and growth of the AIIDE community.  Supporting participants to attend enriches the conference experience for all attendees, not just those who receive travel scholarships.  By providing underrepresented students with mentoring opportunities early in their career, the doctoral mentoring program will serve to broaden the diversity of the whole AIIDE community (and the other communities it interacts with).  The playable experiences track also offers an opportunity to broaden participation in AIIDE.  By offering travel support to independent scholars, practitioners, and artists in communities that have greater diversity than core computer science/AI, it is possible to promote diversity by easing financial barriers to full participation in the community.  The organizers are committed to improving diversity; to these ends, they will be specifically be doing outreach to underrepresented groups, and they will also be limiting the number of accepted students to at most two per institution,<br/><br/>The AIIDE doctoral mentoring program will invite early-stage (ideally just before thesis proposal) students to receive feedback on their proposed research program. The purpose of the program is to foster connections between student peers at different institutions, establish faculty mentoring relationships that can lead to participation on committees (including thesis proposal committees), and provide other contacts that can help guide students early enough to have a large impact on the trajectory of their research.  The mentoring program begins with lunch on the first day, asking mentors and mentees to get to know each other before their presentations and give the mentees the opportunity to deepen their professional network during the conference.  During the doctoral consortium, students will each give brief presentations of their proposed thesis topic followed by conversation and questions with the mentors and conference attendees.  These presentations will serve as encouragement for interested attendees and mentors to provide 1-on-1 feedback at the poster session.  The committee will select students who are early in their career so that mentors have an opportunity to help shape the future of their research and so students may begin to build their professional network.  Mentors for the doctoral consortium will be invited based on the research areas of the accepted students, and will be selected from senior faculty and respected industry leaders in the field.  To facilitate external feedback, mentors will be selected from outside the student's thesis committee and institution.  After students are accepted to the cohort, DC co-chairs will ask them to provide more information about their professional goals in order to best match them with academic and/or industry mentors.  During the doctoral consortium, all mentors will be invited to the presentations and poster session to encourage discussion across research areas.<br/><br/>The AIIDE playable experiences track aims to integrate research and practice through showcasing innovative, AI-based games and interactive media.   This track aims to bring independent game developers, students, industry practitioners, and researchers together.  The track fosters discussion of applications of the AI research shown in the main conference towards designing new kinds of playable experiences, and encourages interdisciplinary collaboration between conference attendees and the broader interactive digital entertainment community; it consists of published short papers about the work, a panel with representation from each playable experience to discuss the role of AI in design, and a conference session where attendees are able to play the games and discuss the work 1-on-1 with participants.  The playable experiences track is especially valuable to students in the doctoral mentoring program who are interested in pursuing careers in industry, since designers and technologists who attend the playable experiences track can be excellent mentors.  For all students, especially in the early stages of their research, it is useful to see high-quality examples of how their research might be used in polished, complete games.  To ensure the health of this track, it is important that independent developers, artists, and practitioners outside of the traditional academic AI research community have the financial means to attend and show their work; often, those who are most qualified to submit work to the playable experiences track do not come from traditional computer science backgrounds, or do not have access to travel funds of their own."
"1712602","Representation Frames and Applications","DMS","APPLIED MATHEMATICS","09/01/2017","06/29/2017","Deguang Han","FL","The University of Central Florida Board of Trustees","Standard Grant","Victor Roytburd","08/31/2021","$199,000.00","","deguang.han@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","MPS","1266","","$0.00","In many engineering applications signals pass through linear systems, but in this process the recorded phase information can be lost or distorted. Examples of this problem occur in speech recognition, quantum state tomography, x-ray crystallography, and electron microscopy.  The frame based phase retrieval problem is to recover a signal from the absolute values of its frame measurement coefficients. The central theme of this project lies in the investigation of the theory of phase-retrievable representation frames and its state-of-the-art applications in signal processing and information theory. The main objective is to establish the theoretical foundations for phase-retrievable representation frames and develop new representation frame based recovering algorithms that will address the computational cost problems in the existed recovering algorithms.<br/> <br/>The project will focus on three main problems in frame based phase retrieval. The first is representation frames and phase-retrieval. Due to their nice algebraic and/or geometric features, well-structured frames are excellent candidates for applications. This part of the project will focus on establishing the theory for finite group projective representation frames that admit phase-retrievable frame vectors. The goal is to completely characterize all such representations, and obtain simple and easily verifiable criterion for all the phase-retrievable frame generators so that they can be easily constructed for applications. The investigation on the representation phase-retrievable frames for ordered product of semi-groups is in line within the scope of representation frames and is directly targeting at applications such as dynamic sampling. The second project will focus on phase-retrieval for signals in the union of lower dimensional spaces. Many applications often face great computational challenges when recovering lower dimensional signals sitting in a very large dimensional Hilbert space. Building the theoretical foundation for the existence of phase-retrievable representation frames with much smaller length for such signals will greatly reduce the encoding frame length and obtain good characterizations for all such frames. The third project is on representation frames in erasure-corrupted signal recovering. Data transmission often causes erasures and other type distortions. In many cases the locations of erased frame coefficients are unknown, and/or the received partial data might be disordered. This project will investigate several practical problems with applications of representation frames in signal recovering from disordered partial frame coefficients and in the frame design problem for encoder-decoder protections."
"1718498","SaTC: CORE: Small: Practical and Robust Hidden Voice Commands","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/01/2017","07/26/2017","Micah Sherr","DC","Georgetown University","Standard Grant","Indrajit Ray","08/31/2021","$506,313.00","Wenchao Zhou, Thomas Shields","msherr@cs.georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","1714, 8060","025Z, 065Z, 7434, 7923, 9178, 9251","$0.00","Systems like Amazon's Alexa, Google Home and Apple's Siri allow users to issue voice commands and pose questions to personal digital assistants.  Since these systems often have access to sensitive data and can perform tasks with serious impact (e.g., spend money to make a purchase), attacks against them could have significant consequences.  Unfortunately, recent research has shown that attacks against such voice-based interfaces are feasible. This project is exploring methods of securing the voice interfaces to smartphones and other devices to ensure that commands are only accepted from the devices' owners.<br/> <br/>The researchers recently introduced the notion of hidden voice commands: audio that is constructed to be interpreted as voice commands by a computer speech recognition system, but is incomprehensible to human listeners.  In theory, attackers may use hidden voice commands to surreptitiously control victims' smartphones and other electronic devices.  Is this a realistic threat in practice? This project is studying both the practicality of this threat and approaches to protection.  First, the researchers are investigating whether an attacker could construct hidden voice commands efficiently and covertly and make the commands operate under realistic conditions to circumvent previously proposed defenses.  Second, they are developing scalable detection techniques and defenses that reliably and efficiently prevent such attacks."
"1738585","Planning IUCRC Duke University:   Center for Alternative Sustainable and Intelligent Computing","CNS","INDUSTRY/UNIV COOP RES CENTERS","08/01/2017","07/28/2017","Yiran Chen","NC","Duke University","Standard Grant","Dmitri Perkins","07/31/2018","$15,000.00","Krishnendu Chakrabarty, Arthur Calderbank, Xin Li, Hai Li","yiran.chen@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","5761","5761","$0.00","This project will help plan for a potential NSF I/UCRC in Alternative Sustainable and Intelligent Computing (ASIC), in order to explore research frontiers in emerging computing platforms for cognitive applications, including speech recognition, face detection and election insights, etc. The planning of the I/UCRC center will be carried out under a unified banner, with inputs from academia, industry, and government stakeholders. As a result, the center will serve as a national platform for interdisciplinary research and communication, charged with investigating advanced computing platforms relevant to various topics of national interests such as national security and smart living, with an eye toward commercialization and economic development for the nation. It will pave the way for designing new computing diagram to address the White House Grand Challenge of developing transformational computing capabilities.<br/><br/>Motivated by the fast growth of cognitive applications and the recent significant development progress of underlined technologies, the center will focus on designing alternative computing platforms for these applications, which is generally difficult to be efficiently executed on conventional von Neumann architecture. Industry is in great demand for a revolutionary computing platform designated to these applications and the planned academia-industry consortium is well positioned to meet such demanding needs with real-world applications. The establishment of ASIC, hence, will accelerate the development of the new computing diagram in industry sectors and expedite technology transfer from research discoveries to practical solutions. <br/><br/>Duke University (Duke) site leads the coordination of the center and focuses on applications of alternative sustainable and intelligent computing systems, especially embedded systems, healthcare and medical, etc. The research projects performed, if succeed, will benefit computer and IT industries by introducing alternative computing platforms that well complement the conventional systems built on von Neumann architecture in short term, and position alternative sustainability and intelligent computing equivalently important in long term. The obtained knowledge and expertise will effectively backup the predictable booming of cognitive applications and their urgent demands for hardware and software supports. The talents and expertise of center PIs and the industry partners will be effectively integrated to enhance the competence of the center members and accelerate technology transfer process. The key transformative aspect of the planned ASIC is that, the success of the project will make great advances in seamlessly integrating non von Neumann vs. von Neumann architectures for future computing. Upon completion, this planning project will prepare a solid full center proposal for Phase I with strong supports from industry and government members. Broader impacts of the project will be reached by wide dissemination of the project outcomes via publications, seminars, tutorials and workshops, as well as industrial and military courses, social media and technology transfer activities. Students will receive invaluable experience in industry relevant applied research and networking opportunities with industry representatives in order to capitalize these experiences for their eventual employment."
"1650469","Syracuse University Planning Grant: I/UCRC for Alternative Sustainable and Intelligent Computing","CNS","INDUSTRY/UNIV COOP RES CENTERS","03/01/2017","02/15/2017","Qinru Qiu","NY","Syracuse University","Standard Grant","Dmitri Perkins","02/28/2018","$15,000.00","Yanzhi Wang","qiqiu@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","5761","5761","$0.00","This project will help plan for a potential NSF I/UCRC in Alternative Sustainable and Intelligent Computing (ASIC), in order to explore research frontiers in emerging computing platforms for cognitive applications, including speech recognition, face detection and election insights, etc. The planning of the I/UCRC center will be carried out under a unified banner, with inputs from academia, industry, and government stakeholders. As a result, the center will serve as a national platform for interdisciplinary research and communication, charged with investigating advanced computing platforms relevant to various topics of national interests such as national security and smart living, with an eye toward commercialization and economic development for the nation. It will pave the way for designing new computing diagram to address the White House Grand Challenge of developing transformational computing capabilities.<br/><br/>Motivated by the fast growth of cognitive applications and the recent significant development progress of underlined technologies, the center will focus on designing alternative computing platforms for these applications, which is generally difficult to be efficiently executed on conventional von Neumann architecture. Industry is in great demand for a revolutionary computing platform designated to these applications and the planned academia-industry consortium is well positioned to meet such demanding needs with real-world applications. The establishment of ASIC, hence, will accelerate the development of the new computing diagram in industry sectors and expedite technology transfer from research discoveries to practical solutions.<br/><br/>The Syracuse University (SU) Site of the proposed Center will focus on hardware software co-design and algorithm optimization for unconventional architectures with a focus of applications in smart surveillance, social media mining, and semantic based information retrieval and recommendation. The research projects performed, if succeed, will benefit computer and IT industries by introducing alternative computing platforms that well complement the conventional systems built on von Neumann architecture in short term, and position alternative sustainability and intelligent computing equivalently important in long term. The obtained knowledge and expertise will effectively backup the predictable booming of cognitive applications and their urgent demands for hardware and software supports. The talents and expertise of center PIs and the industry partners will be effectively integrated to enhance the competence of the center members and accelerate technology transfer process. The key transformative aspect of the planned ASIC is that, the success of the project will make great advances in seamlessly integrating non von Neumann vs. von Neumann architectures for future computing. Upon completion, this planning project will prepare a solid full center proposal for Phase I with strong supports from industry and government members. Broader impacts of the project will be reached by wide dissemination of the project outcomes via publications, seminars, tutorials and workshops, as well as industrial and military courses, social media and technology transfer activities. Students will receive invaluable experience in industry relevant applied research and networking opportunities with industry representatives in order to capitalize these experiences for their eventual employment."
"1650473","University of Notre Dame Planning Grant: I/UCRC for  Alternative Sustainable and Intelligent Computing (ASIC)","CNS","INDUSTRY/UNIV COOP RES CENTERS","03/01/2017","02/14/2017","Yiyu Shi","IN","University of Notre Dame","Standard Grant","Dmitri Perkins","02/28/2018","$14,999.00","","yshi4@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","5761","5761","$0.00","This project will help plan for a potential NSF I/UCRC in Alternative Sustainable and Intelligent Computing (ASIC), in order to explore research frontiers in emerging computing platforms for cognitive applications, including speech recognition, face detection and election insights, etc. The planning of the I/UCRC center will be carried out under a unified banner, with inputs from academia, industry, and government stakeholders. As a result, the center will serve as a national platform for interdisciplinary research and communication, charged with investigating advanced computing platforms relevant to various topics of national interests such as national security and smart living, with an eye toward commercialization and economic development for the nation. It will pave the way for designing new computing diagram to address the White House Grand Challenge of developing transformational computing capabilities.<br/><br/>Motivated by the fast growth of cognitive applications and the recent significant development progress of underlined technologies, the center will focus on designing alternative computing platforms for these applications, which is generally difficult to be efficiently executed on conventional von Neumann architecture. Industry is in great demand for a revolutionary computing platform designated to these applications and the planned academia-industry consortium is well positioned to meet such demanding needs with real-world applications. The establishment of ASIC, hence, will accelerate the development of the new computing diagram in industry sectors and expedite technology transfer from research discoveries to practical solutions. <br/><br/>The University of Notre Dame Site of the proposed Center will focus on hardware innovations, especially infrastructure and implementation, based on its wide connections with the corresponding industry. The research projects performed, if succeed, will benefit computer and IT industries by introducing alternative computing platforms that well complement the conventional systems built on von Neumann architecture in short term, and position alternative sustainability and intelligent computing equivalently important in long term. The obtained knowledge and expertise will effectively backup the predictable booming of cognitive applications and their urgent demands for hardware and software supports. The talents and expertise of center PIs and the industry partners will be effectively integrated to enhance the competence of the center members and accelerate technology transfer process. The key transformative aspect of the planned ASIC is that, the success of the project will make great advances in seamlessly integrating non von Neumann vs. von Neumann architectures for future computing. Upon completion, this planning project will prepare a solid full center proposal for Phase I with strong supports from industry and government members. Broader impacts of the project will be reached by wide dissemination of the project outcomes via publications, seminars, tutorials and workshops, as well as industrial and military courses, social media and technology transfer activities. Students will receive invaluable experience in industry relevant applied research and networking opportunities with industry representatives in order to capitalize these experiences for their eventual employment."
"1733465","Association for the Advancement of Artificial Intelligence (AAAI)-2017 Doctoral Consortium","IIS","Robust Intelligence","03/15/2017","03/10/2017","David Aha","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","James Donlon","08/31/2017","$16,074.00","","david.aha@nrl.navy.mil","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7556","$0.00","Support for student travel for select students participating in the 22nd AAAI/SIGAI Doctoral Consortium (DC). AAAI (Association for the Advancement of Artificial Intelligence) is the major professional association for AI. This year it holds its yearly conference in San Francisco, CA, where this DC will also take place. At the Doctoral Consortium (DC), PhD students who are pursuing work on AI-related topics present their proposed research and receive feedback from a panel of established researchers, as well as from other student participants. This provides the students with invaluable exposure to outside perspectives on their work at a critical time in their research and also enables them to explore their career objectives.<br/><br/>The DC program includes interactive sessions for feedback on dissertation topics from authorities in the field, collaboration-building sessions, early career advice, and well-placed networking opportunities. Participation in a doctoral mentoring opportunity such as this broadens participation to those who might not have attended an AI conference out of lack of habit or resources. This is especially true for those at smaller institutions and those which have less developed AI programs. Engaging such participants has the potential to draw more talent into AI research, improve research ideas in their formative stage, and engender collaborations across the breadth of disciplines associated with intelligent systems."
"1724434","Doctoral Consortium at IJCAI 2017","IIS","Robust Intelligence","04/01/2017","03/29/2017","Maria Gini","MN","University of Minnesota-Twin Cities","Standard Grant","Reid Simmons","03/31/2019","$25,000.00","","gini@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495","7495, 7556","$0.00","This proposal will support US-based Ph.D. students working in artificial intelligence the opportunity to share their knowledge and interact with each other and more senior researchers, to learn about different sub-fields within AI, and to be mentored in research, publication, and career opportunities. This goal will be accomplished by partially supporting the travel costs for US-based Ph.D. students to attend the International Joint Conference on Artificial Intelligence (IJCAI), which is one of the premier international conferences on research in artificial intelligence. The conference, which in 2017 will be held in Melbourne, Australia attracts an international crowd that includes academics, industry workers, entrepreneurs, and funding agency leaders."
"1734304","CompCog: Computational, distributed accounts of human memory: improving cognitive models","BCS","Perception, Action & Cognition, Robust Intelligence","08/01/2017","11/01/2019","Matthew Kelly","PA","Pennsylvania State Univ University Park","Standard Grant","Michael Hout","07/31/2021","$499,969.00","","matthew.kelly@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","SBE","7252, 7495","7252, 7495","$0.00","Memory is among the most impressive aspects of human cognition, allowing us to learn new words or new ideas from just a few examples.  However, the scientific understanding of how this learning occurs is limited.  This research project focuses on how learning occurs in the context of memory for language. Within the human mind, there is something like a dictionary that tells people what words mean (semantics) and how words are combined to make grammatical sentences (syntax). How does the mind learn this dictionary from experience with a language? Computer simulations can help science better understand this learning process. This scientific understanding can, in turn, help teach languages in the classroom and aid in the early detection of language deficits, whether it be developmental deficits in children, or age-related deficits in adults. Furthermore, improving the ability of computers to simulate language learning processes can also lead to the development of better technology such as machine translation, web search, and virtual assistants.  This project considers how a better understanding of language learning can help us avoid common pitfalls of memory connected to the use of language.  For example, humans easily over-generalize and judge a ""book by its cover"", associating certain occupations or personality traits with a gender.  If we know how people come up with associations between words and concepts, we can also detect and prevent prejudices in language to help ensure that artificial intelligence applications, such as web search, do not produce prejudiced results.  The project supports an interdisciplinary and diverse team of researchers and students at Penn State, attracting college students to engage with research in cognitive science and artificial intelligence.<br/><br/>In this project, the researchers are designing a new model of human memory, the Hierarchical Holographic Model.   This computational model helps explain certain aspects of how words and languages are learned.  The model draws on the successes of artificial intelligence and deep neural networks, and applies these insights to psychology.   With this model, the researchers investigate the question of whether human memory has the ability to detect arbitrarily indirect associations between concepts.  The model uses a recursive learning process, building on previously learned knowledge to acquire new knowledge, which allows the model to learn arbitrarily indirect and abstract relationships between words. The researchers consider evidence that sensitivity to abstract relations between words improves the ability of the computer model to learn syntax, such as parts-of-speech, and to use words appropriately to construct grammatical sentences. This work will be assessed against human language data and competing computational models. The success of the computational model should provide evidence that (1) language acquisition depends on indirect associations, and (2) human memory must be able to form indirect associations to facilitate it."
"1724392","S&AS: FND: Long-Term Planning and Robust Plan Execution for Multi-Robot Systems","IIS","S&AS - Smart & Autonomous Syst","09/01/2017","07/27/2017","Sven Koenig","CA","University of Southern California","Standard Grant","James Donlon","08/31/2021","$599,999.00","Nora Ayanian","skoenig@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","039Y","046Z, 9102","$0.00","How can multi-robot teams maneuver in tight and cluttered environments when ""no plan survives contact"" with reality?  Traditional approaches plan for idealized situations and must patch up maneuvers when sensors or actuators are imprecise, making them neither robust nor safe.  This project, a collaboration of PIs from artificial intelligence and robotics, will investigate fundamental research to capture and use timing and uncertainty constraints in  large robot navigation and coordination problems.  The target applications are just-in-time manufacturing and automated warehousing, but the results will extend beyond to many applications of smart and autonomous systems that need reliable and safe planning.  <br/><br/>The project will study Multi-Agent Path Finding (MAPF), which is an NP-hard planning problem that belongs to a class of important planning problems, namely multi-agent navigation problems with temporal and spatial constraints.  The research will relax simplifying assumptions typically made by MAPF solvers, namely that plan execution is perfect and stops once all robots have reached their goal locations.  Many AI planning methods that have been developed are not used on robots, since planning/scheduling uses idealized models of the environment and plan execution is never perfect, and there is often insufficient time for re-planning if execution deviates from the plan. This project will develop well-founded planning and plan-execution methods, based on probabilistic and temporal reasoning, that fuse ideas from robotics and artificial intelligence.  In particular, the PIs will combine advances in planning algorithms from the AI community, namely Simple Temporal Networks (STN), and adapt them to the robotics domain by adding timely execution constraints, as well as sensor, actuator, and model uncertainties.   They will make project results (such as papers, videos and code) available on their web pages, present tutorials on their research results to the artificial intelligence and robotics research communities, develop teaching material for multi-robot planning, and integrate undergraduate students into their research activities."
"1748375","WORKSHOP:   Doctoral Consortium at HCOMP 2017","IIS","Information Technology Researc","08/01/2017","08/02/2017","Brent Hecht","IL","Northwestern University","Standard Grant","Ephraim Glinert","07/31/2018","$21,870.00","","bhecht@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","1640","7367, 7556","$0.00","This is funding to support participation of at least 8 promising doctoral students from U.S. educational institutions at a Doctoral Consortium (workshop)along with distinguished research faculty, to be held in conjunction with the 5th AAAI Conference on Human Computation & Crowdsourcing (HCOMP 2017), which will take place October 24-26 in Quebec City, Canada, and which is sponsored by the Association for the Advancement of Artificial Intelligence.  HCOMP is the premier venue for disseminating the latest research findings on crowdsourcing and human computation.  While artificial intelligence (AI) and human-computer interaction (HCI) represent traditional mainstays of this cross-disciplinary conference, HCOMP believes strongly in inviting, fostering, and promoting broad, interdisciplinary research.  The diverse disciplines the field draws upon, and contributes to, range from human-centered qualitative studies and HCI design, to computer science and artificial intelligence, economics and the social sciences, all the way to digital humanities, policy, and ethics.  More information about the conference is available online at http://www.humancomputation.com/2017/.  The Doctoral Consortium will be a research-focused full day meeting that immediately precedes the conference, on October 23.  It will enhance the scientific workforce in this emerging research area by nurturing a group of promising young investigators interested in human computation and crowdsourcing. The award will also allow these young researchers to attend the HCOMP 2017 conference, thereby allowing them: to interact with other researchers and conference events; to learn of potential career paths within academia and industry; to access an international network of researchers who can support their professional development; and to observe the interdisciplinary nature, diversity and interrelationships of research in human computation.  The Doctoral Consortium Chairs will select up to 6 additional distinguished researchers to serve as faculty mentors; this group also will serve as the review committee for student applications. Students will be accepted based on a paper giving an overview of the student's dissertation research, an explanation of why the student wants to participate in the Doctoral Consortium, a CV, and a letter of support from the student's advisor. The organizers will give preference to students who are most in need of mentoring and joining a peer group.  Moreover, the organizers will promote diversity among the selected students by selecting no more than one or two students from any one school, and by prioritizing the selection of women and underrepresented minorities.   In accordance with CISE policy, NSF funds will only be used to support students from U.S.-based educational institutions.<br/><br/>The full-day Doctoral Consortium will include activities to guide the research of these promising young researchers. The Consortium will allow participants to interact with established researchers and with other students, through presentations, question-answer sessions, panel discussions, and invited presentations. Each participant will give a short presentation on their research and will receive feedback from at least one faculty mentor and from fellow students. The feedback will be geared toward helping the student participants understand and articulate how their research is positioned relative to other work on human computation and crowdsourcing.  The feedback will also address whether the students' topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are being appropriately analyzed and presented.  Activities led by the faculty will include a panel discussion to give students more information about the process and lessons of research and life in academia and industry. To further integrate the Doctoral Consortium participants into the conference itself, students will have a chance to present their work as posters in an interactive poster session and their papers will be posted online on the workshop webpage. These activities will benefit the participants by offering each fresh perspectives and comments on their work from researchers outside their own institution, both from faculty and other students; providing a supportive setting for mutual feedback on participants' current research and guidance on future research directions; and enabling participants to form a cohort of new researchers."
"1741706","Support for Doctoral Students from U.S. Universities to Attend AIED 2017 and/or  EDM 2017","IIS","ECR-EHR Core Research, Cyberlearn & Future Learn Tech","06/01/2017","05/19/2017","Erin Walker","AZ","Arizona State University","Standard Grant","Amy Baylor","11/30/2018","$20,000.00","","eawalker@pitt.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7980, 8020","7556, 8045, 8083","$0.00","The United States has historically been the global leader in the field of artificial intelligence in education (AIED), or ways to use computerized artificial intelligence to enhance teaching and learning in contexts ranging from children learning math in school, to soldiers learning highly technical jobs in the US military. The preeminent conference in this field is the AIED conference; at this conference the latest research is presented and practitioners learn the state of the art techniques that allow creation of these important educational technologies. A related conference that is equally significant is the Educational Data Mining (EDM) conference, which focuses on research on big data and analytics for education.<br/><br/>This proposal would provide partial travel support for 20 Ph.D. students, selected through a competitive process, to attend either or both of the AIED or EDM conference, present their work, and receive additional mentoring outside of their dissertation committees as part of a doctoral consortium. The intellectual merit of the work rests on the studies the graduate students submit to be considered for participation in the early career track of the conference; this work is then enhanced by guidance from world-class mentors who meet with the students in a structured format to improve their research. The broader impact includes the career impact on the twenty selected students, especially since promising graduate students whose advisors may not have funding to send them to the conference can still be included, and their work can be showcased and improved. Possible long-term broader impacts include building the field of artificial intelligence in education and data analytics researchers and thus eventually, improving the quality of education."
"1738441","SBIR Phase II:  Pushing the Boundaries of Intelligent Assistants for Financial Services","IIP","SBIR Phase II","09/15/2017","09/18/2017","Michael Laurenzano","MI","Clinc, Inc","Standard Grant","Peter Atherton","08/31/2019","$750,000.00","","michael.laurenzano@gmail.com","1940 Hedgenettle Ct.","Ann Arbor","MI","481039689","8582053027","ENG","5373","5373, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is in providing state-of-the-art tools allowing anyone to build and deploy domain specific commercial intelligent virtual assistant (IVA) solutions. These tools allow others to understand how IVAs should be architected and integrate IVA technology into their offerings. IVAs have shown promise in numerous commercial domains including financial services, healthcare, education, law enforcement, and retail, to name a few, reducing the barrier to knowledge access within domains by providing a medium for people to converse naturally with sophisticated computer and information systems.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project will address the significant technological challenges involved when scaling the domains and capabilities of an Intelligent Virtual Assistant (IVA). This project will innovate in designing scalable artificial intelligence models capable of learning and identifying hundreds or thousands of learned concepts, and designing the accompanying system architecture to support the growing compute demand of sophisticated algorithms. Specifically, the project aims at achieving: (1) Scalable Intelligence: the ability to handle hundreds or thousands of competencies and extractable semantic concepts, allowing users to interact with the system with unbounded, unconstrained language; (2) Customizable Intelligence: the ability to allow customers to (semi-) automatically train and (re)train and customize the intelligence on demand (adding new competencies, identifying new slot-value pairs, modify responses); (3) Conversational Complexity: support multi-turn conversations, where the context from prior utterances is used to refine and understand what the end-user is trying to accomplish; and (4) Scalable System Infrastructure: enhance open source IVA software infrastructure to seamlessly scale up and down the computational resources allocated for each intelligence engine based on load."
"1719307","Support for Student Participation in the 2017 ACM Intelligent User Interfaces Conference","IIS","HCC-Human-Centered Computing","03/01/2017","12/23/2016","Wai-Tat Fu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Ephraim Glinert","02/28/2018","$25,020.00","","wfu@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7367","7367, 7556","$0.00","This is funding to provide financial support for approximately 20 graduate students (all from U. S. universities and working towards either their Master's degree or a Doctorate) to attend the 2017 International Conference on Intelligent User Interfaces (IUI 2017), to be held March 13-16 in Limassol, Cypress, about 10 of them as participants in a special Student Consortium (workshop), and the rest as presenters in the main conference and/or as attendees at the conference for general training purposes.  Sponsored by ACM, the annual IUI conferences represent the growing interest in next-generation intelligent interactive user interfaces.  Attracting 200-300 attendees, they are the premier forum where researchers from academia and industry worldwide who work at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) come together to exchange insights and to present outstanding research and applications whose goal is to make the computerized world a more amenable place.  Unlike traditional AI the focus is not so much on making the computer smart all by itself, but rather on making the interaction between computers and people smarter; unlike traditional HCI, there is a focus on solutions that involve large amounts of knowledge and emerging technologies such as natural language understanding, brain computer interfaces, and gesture recognition. To this end, IUI encourages contributions not only from computer science but also from related fields such as psychology, behavioral science, cognitive science, computer graphics, design, the arts, etc.  IUI 2017 will be the 22nd conference in the series; more information about the conference is available online at http://iui.acm.org/2017.  This funding will enable attendance at the IUI conference by students who might otherwise be unable to do so for financial reasons.  It will enhance the educational experience of funded participants, by bringing them into contact with leading researchers in the field and by exposing them to the lively discussion during the course of the conference that often leads to opportunities for career advancement.  The quality of the conference itself will be enhanced as well, thanks to a broadening of the base of institutions represented and increased diversity of participants.  The rich exchange of ideas at IUI has previously proven to be a valuable source of ideas for future research, as well as leading to collaborative efforts; this funding will extend the opportunities for collaboration and provide intellectual stimulus to programs that have previously sent few or no representatives to this conference.  The organizing committee has undertaken to proactively recruit student participants from schools that have not traditionally been well represented in the IUI community.  Women and students who are members of underrepresented groups will be particularly encouraged to participate.  To further assure diversity, no more than two students will be accepted from any given institution.<br/><br/>The IUI 2017 Student Consortium will build on the success of previous such events.  The heart of the Consortium will be a full-day workshop on March 13 in parallel with the conference workshops and the day before the start of the technical program.  Student trainees will be afforded exposure to their new research community by giving a 20-30 presentation on their work and receiving feedback from peers and a panel of senior researchers.  A group lunch and dinner will encourage social interaction among the student cohort and informal personal interaction with the mentors.  The students' work will also be featured during the main conference in a poster session, where they will gain additional experience explaining their work to others in the field."
"1738291","SBIR Phase II:  Independent Science Learning through Serious Games with Expert Avatars and Complementary Stories","IIP","SBIR Phase II","09/15/2017","05/10/2018","Peter Solomon","CT","THEBEAMER LLC","Standard Grant","Rajesh Mehta","02/29/2020","$759,990.00","Kenneth Thompson","prsolomon@comcast.net","87 Church St","East Hartford","CT","061083720","8602125071","ENG","5373","079E, 5373, 8031, 8032, 8039, 8240","$0.00","This SBIR Phase II project will provide an engaging independent-learning platform which weaves science into an exciting story for ages eight to thirteen. The platform combines a time-travel adventure game, a complementary book, and in-game avatars for important scientists (like Albert Einstein) that can answer students' questions.  It is aimed at the need for educating more science and engineering professionals by tapping into children's strong interest in games to augment current science curricula that students often find boring and uninteresting. The story is about STARDUST (atoms) and its formation and history in the universe.  It engages students by taking them back in time to identify trillions of atoms they personally inherited from Einstein and the last T-Rex.  Besides school use, the game's independent learning and engagement allows distribution directly to children for recreational use, adding another avenue for science learning, and a sustainable business model for the Company through school and commercial sales.  The technology can also support independent learning in underperforming schools. The development of the scientist avatar artificial intelligence technology will have wide application for many other educational and training requirements.  <br/><br/>The innovation is a unique platform for independent learning about the sweeping science saga of atoms during the history of the Universe and Earth, and the connection of each student to that history through the trillions of atoms they inherited from prior beings like Albert Einstein and the last T-Rex.  The platform combines video games, a complementary fictional story book (based on the science) that acts as game introduction and player guide, and in-game avatars (like Albert Einstein and Henrietta Leavitt) that can provide verbal answers to students' spoken questions.  The artificial intelligence backing the scientist avatars includes learning manager software that guides student learning and provides assessments of progress in achieving pre-established learning goals. Four players cooperate in time travel adventures, exploring the human body, the Earth, and the Universe to find out what STARDUST is (atoms), how and when it was created (in the Big Bang and supernovae), how it got into Einstein, and from him to others (e.g., the carbon cycle). Excellent test results for the Phase I prototype game suggests a successful start at creating good player engagement. The project will continue to combine learning and engagement in 10 additional episodes with new scientist avatars including female and minority scientists."
"1743637","Symposium on Combinatorial Search - 2017","IIS","Robust Intelligence","05/15/2017","07/28/2020","Nathan Sturtevant","CO","University of Denver","Standard Grant","James Donlon","04/30/2021","$10,000.00","","sturtevant@cs.du.edu","2199 S. University Blvd.","Denver","CO","802104711","3038712000","CSE","7495","7495, 7556","$0.00","This grant supports student travel for select students and post-doctoral researchers to participate in the Symposium on Combinatorial Search (SoCS-2017) June 16-17 in Pittsburgh, PA.  SoCS is an annual event that brings together researchers in heuristic search and combinatorial optimization drawing from diverse areas of artificial intelligence, planning, robotics, constraint programming, operations research, bioinformatics, and computer games. This consortium is oriented on research and career development for students who have identified their PhD topics and are just embarking on that independent research.  <br/><br/>Sponsoring student travel to SoCS fosters a community of researchers from otherwise diverse areas of computer science to both advance the state of the art in heuristic search and/or combinatorial optimization, and also use these tools in their research.  Students participating in this symposium are also more likely to take full advantage of the top-tier conference in the International Conference on Automated Planning and Scheduling (ICAPS) 2017 taking place immediately prior to this symposium, and co-located with it.  The entire event provides an opportunity for students to engage in discussion with scientists from around the world and to explore new research directions and topics."
"1718384","RI: Small: A New Approach to Integrating Graphical Models in Decision-Theoretic Planning","IIS","Robust Intelligence","08/15/2017","07/25/2017","Eric Hansen","MS","Mississippi State University","Standard Grant","Roger Mailler","07/31/2021","$427,000.00","","hansen@cse.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","CSE","7495","7495, 7923, 9150","$0.00","This project addresses one of the central problems of research in Artificial Intelligence: the problem of planning, or sequential decision making, under uncertainty and imperfect information. Planning algorithms are widely-used for control and decision-making problems in engineering and business, with many practical applications in robotics, process control, logistics, user-adaptive systems, resource management, and related problems where automation of decision making is useful. This project considers two widely-used decision-theoretic frameworks for planning under uncertainty and imperfect information, which are partially observable Markov decision processes and influence diagrams, and integrates these two frameworks in a novel way that leverages their complementary advantages. <br/><br/>The project integrates these two frameworks by showing how to generalize algorithms for solving influence diagrams, especially classic variable elimination algorithms, so that they use algorithmic techniques for solving partially observable Markov decision processes (POMDPs) to improve scalability, as well as to represent plans and strategies more compactly. The generalized variable elimination algorithms developed in this project can behave like traditional algorithms for solving influence diagrams, or like traditional algorithms for solving POMDPs, depending on the order in which variables are eliminated. From this perspective, algorithms for influence diagrams and POMDPs that once appeared dissimilar can be viewed as special cases of the same, more general algorithm. More importantly, this perspective allows these complementary algorithmic techniques to be combined in new ways, leading to planning algorithms with improved performance, wider applicability, and easier-to-interpret results.  The project focuses on several related research problems that will extend this approach and make it more useful in practice, including the development of new heuristics for variable elimination ordering, the development of approaches to improving planner performance by leveraging problem structure, including context-specific independence, and the development of an integrated approach to bounded-error approximation that will allow tradeoffs between plan quality and computation time. Although the project focuses on finite-horizon planning problems, the integrated approach may also be used in solving infinite-horizon planning problems with non-Markovian structure. In addition to the intellectual impact of this research, the project will contribute to education, student mentoring, and outreach."
"1716333","RI: Small: Algorithmic Mechanism Design for Multi-Type Resource Allocation","IIS","Robust Intelligence","08/15/2017","07/27/2017","Lirong Xia","NY","Rensselaer Polytechnic Institute","Standard Grant","Roger Mailler","07/31/2021","$373,536.00","","xial@cs.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","7495","7495, 7923","$0.00","Allocating indivisible items to multiple agents without monetary transfer is a pressing problem in our society. In many situations, items are categorized into multiple types and each agent must get at least one item per type. Such problems are called multi-type resource allocation (MTRA). For example, MTRA arises in allocating courses to students, allocating computational resources to users in cloud computing, allocating medical resources to patients, as well as in multi-type exchange market and centralized welfare programs that involve multiple sub-programs. Unfortunately, most previous research overlook information regarding resource type, and are thus hindered by three barriers: preference bottleneck, computational bottleneck, and threats of agents' strategic behavior. The project aims at establishing theoretical and algorithmic foundations of mechanism design for MTRA with the help of Artificial Intelligence. The newly designed mechanisms will improve the economic efficiency and computational efficiency of resource allocation in multiagent systems, socio-economics systems, and operations research.<br/><br/>In doing so, the researcher will design and evaluate novel graphical languages to address the preference bottleneck; design novel frameworks for discovering new mechanisms, including sequential allocation mechanisms and extensions of the top-trading-cycles mechanism, to address the computational bottleneck; use game theory to analyze and measure agents' strategic behavior; and use high computational complexity to prevent agents' strategic behavior. Outcomes of the research will be integrated into an open-source Online Preference Reporting and Aggregation (OPRA) system, which serves as a platform to bridge theory, practice, and education."
"1741472","BIGDATA: F: Audio-Visual Scene Understanding","IIS","Big Data Science &Engineering","09/01/2017","05/08/2019","Chenliang Xu","NY","University of Rochester","Standard Grant","Maria Zemankova","08/31/2021","$666,000.00","Zhiyao Duan","chenliang.xu@rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","8083","7433, 8083, 9251","$0.00","Understanding scenes around us, i.e., recognizing objects, human actions and events, and inferring their spatial, temporal, correlative and causal relations, is a fundamental capability in human intelligence. Similarly, designing computer algorithms that can understand scenes is a fundamental problem in artificial intelligence. Humans consciously or unconsciously use all five senses (vision, audition, taste, smell, and touch) to understand a scene, as different senses provide complimentary information. For example, watching a movie with the sound muted makes it very difficult to understand the movie; walking on a street with eyes closed without other guidance can be dangerous. Existing machine scene understanding algorithms, however, are designed to rely on just a single modality. Take the two most commonly used senses, vision and audition, as an example, there are scene understanding algorithms designed to deal with each single modality. However, no systematic investigations have been conducted to integrate these two modalities towards more comprehensive audio-visual scene understanding. Designing algorithms that jointly model audio and visual modalities towards a complete audio-visual scene understanding is important, not only because this is how humans understand scenes, but also because it will enable novel applications in many fields. These fields include multimedia (video indexing and scene editing), healthcare (assistive devices for visually and aurally impaired people), surveillance security (comprehensive monitoring of the suspicious activities), and virtual and augmented reality (generation and alternation of visuals and/or sound tracks). In addition, the investigators will involve graduate and undergraduate students in the research activities, integrate research results into the teaching curriculum, and conduct outreach activities to local schools and communities with an aim to broader participation in computer science. <br/><br/>This project aims to achieve human-like audio-visual scene understanding that overcomes the limitations of single-modality approaches through big data analysis of Internet videos. The core idea is to learn to parse a scene into elements and infer their relations, i.e., forming an audio-visual scene graph. Specifically, an element of the audio-visual scene can be a joint audio-visual component of an event when the event shows correlated audio and visual features. It can also be an audio component or a visual component if the event only appears in one modality. The relations between the elements include spatial and temporal relations at a lower level, as well as correlative and causal relations at a higher level. Through this scene graph, information across the two modalities can be extracted, exchanged and interpreted. The investigators propose three main research thrusts: (1) Learning joint audio-visual representations of scene elements; (2) Learning a scene graph to organize scene elements; and (3) Cross-modality scene completion. Each of the three research thrusts explores a dimension in the space of audio-visual scene understanding, yet they are also inter-connected. For example, the audio-visual scene elements are nodes in the scene graph, and the scene graph, in turn, guides the learning of relations among scene elements with structured information; the cross-modality scene completion generates missing data in the scene graph and is necessary for good audio-visual understanding of the scene. Expected outcomes of this proposal include: a software package for learning joint audio-visual representations of various scene elements; a web-deployed system for audio-visual scene understanding utilizing the learned scene elements and scene graphs, illustrated with text generation; a software package for cross-modality scene completion based on scene understanding; and a large-scale video dataset with annotations for audio-visual association, text generation and scene completion. Datasets, software and demos will be hosted on the project website."
"1717530","CIF: Small: Foundations of Belief Sharing in Human-Machine Systems","CCF","Comm & Information Foundations","07/01/2017","06/28/2017","Lav Varshney","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","06/30/2021","$405,455.00","","Varshney@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7936","$0.00","This work aims to develop mathematical laws and foundational principles for belief sharing in systems with human and machine intelligence working together to make robust decisions. Prior work in statistical signal processing and in psychology has only considered technological limitations or human limitations independently, but jointly considering informational limitations of both humans and machines is critical in engineering future sociotechnical systems, especially when people are overwhelmed by too much information. A theory for fundamental limits and optimal designs for such systems is lacking.<br/><br/>Rather than systems with agents sharing either raw data or local decisions, we develop intermediate designs based on sharing beliefs. Belief sharing increases modularity among networked units compared to central analysis of raw data, yet also strengthens coordination compared to decentralized local decisions. We build on our prior bounded rationality models of people and stochastic models of artificial intelligence to determine optimal mixed human-machine architectures. First, we find fundamental information-theoretic limits of belief-sharing under Bayes risk and discrete choice models, new kinds of CEO problems. As a key substep, this involves determining fundamental Ziv-Zakai bounds on Bayesian estimation under non-quadratic criteria. We then use quantization and decision theory to develop optimal architectures that have cognitively- and algorithmically-limited agents, including optimal categorization of beliefs and judgment pooling rules taking human behavior into account. Finally, we consider the language needed to communicate beliefs in complicated network structures to achieve collective intelligence, studying Nash equilibria balancing focal and shared concerns."
"1736056","EXP: Collaborative Research: Empowering Learners to Conduct Experiments","IIS","IUSE","09/01/2017","06/22/2020","Steven Sutherland","TX","University of Houston - Clear Lake","Standard Grant","William Bainbridge","08/31/2021","$70,000.00","","Sutherland@UHCL.edu","2700 Bay Area Boulevard","Houston","TX","770581002","2812833016","CSE","1998","8045, 8209, 8841, 9178","$0.00","This project seeks to transform current practices in the teaching of scientific research methods by shifting the fundamental dynamics and focusing in a scientific domain that is relatable to a broad audience: designing and conducting social and behavioral science experiments. Scientific inquiry is key to making societal progress and improving our understanding of the world. Social and behavioral science programs are largely designed to prepare future researchers, but have a minimum expectation that students become critical consumers of research. Understanding the scientific method and the experimental methods used by researchers is necessary for establishing an ability to effectively assess the research that students will encounter in both the media and scientific outlets. Student understanding of scientific inquiry is significantly enhanced when anchored in inquiry experiences; however, opportunities for scientific research experiences are limited, even in research methods courses, due to the challenges of teaching experimental design and problems regarding access to and recruitment of participants. Without these experiences, students in higher education struggle to fully understand scientific inquiry. To address common barriers to learning how to conduct research, this project is designing a flexible, computer-based platform to be collaborative, narrative-based, engaging, and inspired by constructionist theories to facilitate learning with the use of artificial intelligence (AI) support.<br/> <br/>The platform developed in this project will serve as a model of a new genre of constructionist research environments, that enable learners to leverage technologies to create, modify, and replicate experiments, recruit participants, and analyze their results to learn about the world. The design-based research approach will operate in two cycles; in each cycle, a revised module and set of tools will be deployed. This results in two major research contributions: (1) Using mixed-methods, the theoretical and educational contribution is to study the process by which students in higher education learn to conduct experimental research, and about the roles of AI assistance, collaboration, narrative, and activities motivated by curiosity, exploration, and reflection. (2) The technological contribution is an innovative, AI-assisted set of scenario creation tools that empower learners to create experiments and that allow us to understand how an intelligent, collaborative, engaging, narrative-based platform can support students in higher education in designing and conducting social and behavioral science experiments.  With this system, it will be easier to create, run, replicate, and build upon studies and to reach out to a broader audience than the pool of university students used in typical in-person laboratory experiments. As a result, the platform will make it possible to transform social science research practices and even has the potential to foster new scientific discoveries."
"1736065","EXP: Collaborative Research: Empowering Learners to Conduct Experiments","IIS","IUSE","09/01/2017","06/01/2020","Camillia Matuk","NY","New York University","Standard Grant","William Bainbridge","08/31/2021","$112,000.00","","cmatuk@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","1998","8045, 8209, 8841, 9178","$0.00","This project seeks to transform current practices in the teaching of scientific research methods by shifting the fundamental dynamics and focusing in a scientific domain that is relatable to a broad audience: designing and conducting social and behavioral science experiments. Scientific inquiry is key to making societal progress and improving our understanding of the world. Social and behavioral science programs are largely designed to prepare future researchers, but have a minimum expectation that students become critical consumers of research. Understanding the scientific method and the experimental methods used by researchers is necessary for establishing an ability to effectively assess the research that students will encounter in both the media and scientific outlets. Student understanding of scientific inquiry is significantly enhanced when anchored in inquiry experiences; however, opportunities for scientific research experiences are limited, even in research methods courses, due to the challenges of teaching experimental design and problems regarding access to and recruitment of participants. Without these experiences, students in higher education struggle to fully understand scientific inquiry. To address common barriers to learning how to conduct research, this project is designing a flexible, computer-based platform to be collaborative, narrative-based, engaging, and inspired by constructionist theories to facilitate learning with the use of artificial intelligence (AI) support.<br/> <br/>The platform developed in this project will serve as a model of a new genre of constructionist research environments, that enable learners to leverage technologies to create, modify, and replicate experiments, recruit participants, and analyze their results to learn about the world. The design-based research approach will operate in two cycles; in each cycle, a revised module and set of tools will be deployed. This results in two major research contributions: (1) Using mixed-methods, the theoretical and educational contribution is to study the process by which students in higher education learn to conduct experimental research, and about the roles of AI assistance, collaboration, narrative, and activities motivated by curiosity, exploration, and reflection. (2) The technological contribution is an innovative, AI-assisted set of scenario creation tools that empower learners to create experiments and that allow us to understand how an intelligent, collaborative, engaging, narrative-based platform can support students in higher education in designing and conducting social and behavioral science experiments.  With this system, it will be easier to create, run, replicate, and build upon studies and to reach out to a broader audience than the pool of university students used in typical in-person laboratory experiments. As a result, the platform will make it possible to transform social science research practices and even has the potential to foster new scientific discoveries."
"1736185","EXP: Collaborative Research: Empowering Learners to Conduct Experiments","IIS","IUSE, Cyberlearn & Future Learn Tech","09/01/2017","03/30/2018","Casper Harteveld","MA","Northeastern University","Standard Grant","William Bainbridge","08/31/2020","$367,662.00","Gillian Smith","c.harteveld@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1998, 8020","8045, 8209, 8841, 9178","$0.00","This project seeks to transform current practices in the teaching of scientific research methods by shifting the fundamental dynamics and focusing in a scientific domain that is relatable to a broad audience: designing and conducting social and behavioral science experiments. Scientific inquiry is key to making societal progress and improving our understanding of the world. Social and behavioral science programs are largely designed to prepare future researchers, but have a minimum expectation that students become critical consumers of research. Understanding the scientific method and the experimental methods used by researchers is necessary for establishing an ability to effectively assess the research that students will encounter in both the media and scientific outlets. Student understanding of scientific inquiry is significantly enhanced when anchored in inquiry experiences; however, opportunities for scientific research experiences are limited, even in research methods courses, due to the challenges of teaching experimental design and problems regarding access to and recruitment of participants. Without these experiences, students in higher education struggle to fully understand scientific inquiry. To address common barriers to learning how to conduct research, this project is designing a flexible, computer-based platform to be collaborative, narrative-based, engaging, and inspired by constructionist theories to facilitate learning with the use of artificial intelligence (AI) support.<br/> <br/>The platform developed in this project will serve as a model of a new genre of constructionist research environments, that enable learners to leverage technologies to create, modify, and replicate experiments, recruit participants, and analyze their results to learn about the world. The design-based research approach will operate in two cycles; in each cycle, a revised module and set of tools will be deployed. This results in two major research contributions: (1) Using mixed-methods, the theoretical and educational contribution is to study the process by which students in higher education learn to conduct experimental research, and about the roles of AI assistance, collaboration, narrative, and activities motivated by curiosity, exploration, and reflection. (2) The technological contribution is an innovative, AI-assisted set of scenario creation tools that empower learners to create experiments and that allow us to understand how an intelligent, collaborative, engaging, narrative-based platform can support students in higher education in designing and conducting social and behavioral science experiments.  With this system, it will be easier to create, run, replicate, and build upon studies and to reach out to a broader audience than the pool of university students used in typical in-person laboratory experiments. As a result, the platform will make it possible to transform social science research practices and even has the potential to foster new scientific discoveries."
"1718945","RI: SMALL: Efficient Implementations of Goal-Directed Solvers for Answer Set Programming","IIS","Robust Intelligence","09/01/2017","08/04/2017","Gopal Gupta","TX","University of Texas at Dallas","Standard Grant","Roger Mailler","08/31/2021","$420,000.00","","gupta@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","7495, 7923","$0.00","The goal of this project is to develop efficient implementation techniques for realizing automated reasoning systems that emulate human-style common sense reasoning. Automating common sense reasoning is important for developing advanced applications of artificial intelligence (AI), particularly, in areas where the thought process of an expert needs to be automated, e.g., reasoning performed by a medical doctor during diagnosis and prescribing a treatment. Human reasoning is difficult to emulate on a computer, as humans simplify reasoning by using default conclusions (e.g., if Tweety is a bird, it must fly) coupled with raising exceptions (if Tweety turns out to be a penguin later, retract the conclusion about Tweety's flying abilities). Because of this peculiar nature of human reasoning, approaches based on standard logic do not work very well: one has to resort to a non-monotonic logic, i.e., a logic in which conclusions reached now may be withdrawn later as new information becomes available. Research conducted in this project will result in efficient, query-driven implementations of these non-monotonic logics. Successful completion of this project will result in advanced applications such as an automated system that can advise a physician on how to treat a particular disease, or a self-driving car's decision-making system that can emulate a human's driving expertise.<br/><br/>The project will rely on the paradigm of answer set programming (ASP) to represent common sense knowledge. An answer set program consists of rules containing (possibly negated) predicates. Current ASP systems rely on first grounding the answer set program to obtain an equivalent propositional program, and then using a Boolean satisfiability (SAT) solver to find models of this propositional program that contain the answer that is sought by the user. The grounding requirement restricts the range of programs that can be executed. This project builds upon earlier research on directly executing predicate answer set programs, i.e., without grounding them first. It aims to realize faster implementations of such systems by designing a virtual machine to which an answer set programs will be compiled to and executed."
"1745800","IIS-RI: International Conference on Automated Planning and Scheduling (ICAPS) 2017 Doctoral Consortium Travel Awards","IIS","Robust Intelligence","07/01/2017","06/28/2017","Emma Brunskill","CA","Stanford University","Standard Grant","James Donlon","06/30/2018","$6,888.00","","ebrun@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","7495, 7556","$0.00","This grant supports student travel for select students to participate in the Doctoral Consortium (DC) at the International Conference on Automated Planning and Scheduling (ICAPS), to be held in June 2017 in Pittsburgh, PA.  ICAPS is a top-tier conference for researchers and practitioners to share and learn from each other in the field of planning and scheduling using artificial intelligence techniques. This consortium is oriented on research and career development for students who have identified their PhD topics and are just embarking on that independent research.  <br/><br/>Sponsoring student travel to ICAPS fosters a community of research in theoretical and algorithmic advances in planning and scheduling, as well as a range of applications of such automated planning, such as in manufacturing, health care, logistics, space systems, software engineering, robotics, and entertainment.  The entire event provides an opportunity for students to engage in discussion with scientists from around the world and to explore new research directions and topics."
"1733809","Summer School on Cognitive Robotics","IIS","ROBUST INTELLIGENCE","05/01/2017","05/02/2017","Brian Williams","MA","Massachusetts Institute of Technology","Standard Grant","Reid Simmons","04/30/2018","$34,943.00","","williams@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","7495, 7556","$0.00","This proposal will support a week-long summer school on Cognitive Robotics to be held in Boston, Ma in June 2017.  The summer school will be a combination of invited talks and tutorials, which are designed to introduce students to issues in planning and execution from perspectives of both Robotics and Cognitive Artificial Intelligence, and daily labs, which are designed to give students hands-on experience with robotic hardware and state-of-the-art software tools for developing robotic behaviors.  The summer school will help expose graduate students to cutting-edge ideas at the intersection of Robotics and Cognitive Systems and will help to form a new community of researchers in this interdisciplinary area."
"1727336","Doctoral Dissertation Research: The interaction of expectations and evidence in pragmatic inference and generalizations","BCS","DDRI Linguistics","08/01/2017","07/20/2017","Chigusa Kurumada","NY","University of Rochester","Standard Grant","William Badecker","01/31/2019","$11,813.00","Amanda Pogue","ckuruma2@ur.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","SBE","8374","1311, 9179","$0.00","Spoken language not only communicates information about a speaker's thoughts or desires; it also conveys information about the speaker's identity. By simply listening to speakers' voices, accents, and word choice, we can learn a great deal about them, in addition to what is being talked about. Previous studies of language processing, however, have almost exclusively focused on the linguistic signal abstracted from individual speakers, investigating what listeners think is true about the world based on what an individual speaker has said. The project aims to explore the mechanism by which listeners extract information about the speaker through processing the linguistic signal. It then addresses the question of whether, and if so how, the increased knowledge about the speaker facilitates language comprehension. This research, consequently allows researchers to build a foundation for exploring how young children may learn speaker differences, which can contribute to new pedagogical tools for helping children to better interact with, and learn from, diverse populations. Secondly, the work will likely have industry applications for artificial intelligence technology, allowing it to better adapt its functionality to an individual user's talking style. <br/><br/>This dissertation project employs two approaches to investigating what information listeners extract from spoken utterances. First, a large-scale online survey technique will be used to solicit responses from participants from a wider variety of linguistic and cultural backgrounds than those included in previous studies. Participants are exposed to utterances produced by two speakers and subsequently answer questions that probe their sensitivity to across-speaker differences. In the second set of experiments, a combination of an artificial language learning paradigm and an eye-tracking methodology will be used to study real-time language comprehension behaviors. Listeners' eye-gaze will be used to gain fine-grained information about the real-time development of their linguistic expectations. By combining these experimental approaches, the researchers elucidate how the human language comprehension system derives fine-grained expectations for future linguistic input and how the mechanism develops as a function of increased knowledge about linguistic communication."
"1651565","CAREER: Modeling and Inference for Large Scale Spatio-Temporal Data","IIS","Robust Intelligence","03/15/2017","01/28/2019","Stefano Ermon","CA","Stanford University","Continuing Grant","Kenneth Whang","02/28/2022","$540,000.00","","ermon@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","1045, 7495","$0.00","Key sustainability challenges, such as poverty mitigation, climate change, and food security, involve global phenomena that are unique in scale and complexity. Our global sensing capabilities - from remote sensing to crowdsourcing - are becoming increasingly economical and accurate. These recent technological developments are creating new spatio-temporal data streams that contain a wealth of information relevant to sustainable development goals. Actionable insights, however, cannot be easily extracted because the sheer size and unstructured nature of the data preclude traditional analysis techniques. This five-year career-development plan is an integrated research, education, and outreach program focused on developing new AI techniques to extract actionable insights from large-scale spatio-temporal data. These techniques have the potential to yield accurate, inexpensive, and highly scalable models to inform research and policy.<br/><br/>The research goal of this project is to develop new modeling and algorithmic frameworks to help address global sustainability challenges involving spatio-temporal data. This research will develop new predictive models of complex spatio-temporal phenomena integrating in unique ways ideas from graphical models and representation learning, improving their overall performance. New approaches to learn from unlabeled data exploiting various forms of prior domain knowledge, including spatio-temporal dependencies and relationships between different data modalities, will be developed. To learn models and make predictions at scale, this project will also develop new scalable probabilistic inference methods based on the use of random projections to reduce the dimensionality of probabilistic models while preserving their key properties. The techniques developed will be made available to both academia and industry through open-source software, and will enable computationally feasible approaches for analyzing large spatio-temporal datasets and for modeling global scale phenomena. Predictions and data products produced by this project will enable new analyses and advance sustainability disciplines. Results will be disseminated widely through scientific articles, research seminars, and conference presentations to maximize the benefits to the scientific community. Educational and outreach efforts will include the involvement of undergraduate students undertaking independent research projects, a website describing research bridging computation and, and a summer outreach program aimed at introducing under-represented high-school students to computer science and artificial intelligence."
"1738375","SBIR Phase II:  Mobile Manipulation Hospital Service Robots","IIP","SBIR Phase II","09/15/2017","07/09/2020","Andrea Thomaz","TX","Diligent Droids, LLC","Standard Grant","Alastair Monk","07/31/2021","$1,199,909.00","","athomaz@diligentdroids.com","2418 Spring Ln PO Box 5017","Austin","TX","787034480","6177847154","ENG","5373","165E, 169E, 5373, 8018, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project on hospital service robots is improving the quality of care in hospital systems that are under increased pressure to provide high-quality patient-centric care while functioning as profitable businesses. Hospitals face a shortage of qualified nurses and high rates of nurse turnover. Nurses play a critical role in communicating care plans, educating patients, and guarding against medical errors. The amount of time they spend in direct care activities is a key determinant of patient satisfaction, better patient outcomes, fewer errors, and shorter lengths of stay. In the face of nursing shortages across the U.S., it is increasingly important to have nurses performing at the 'top of their license'. Reducing the amount of time they spend on non-nursing tasks is crucial to this goal. Automation could address these challenges and labor shortage by allowing clinical staff to focus on providing skilled care. The proposed project aims to develop technology that is general-purpose enough to transfer to other markets, such as long term care facilities and, eventually, individual consumers. Robots that perform assistive tasks in homes could increase the feasibility of independent living for many older adults.<br/><br/><br/>The proposed project will establish the technical and commercial feasibility of developing hospital service robots that act as assistants on acute care units, enabling nurses to spend more time at the bedside with patients. This project will make technical advances along three dimensions: the ability of the proposed robot to autonomously navigate within nursing units and across the hospital (navigation capabilities); to easily adapt its manipulation skills to specific tasks and to physical characteristics of a particular hospital/unit (adaptive learning of manipulation skills); and to work alongside humans in a socially acceptable manner, including appropriate navigation in crowded hallways, speech, and eye gaze behaviors that communicate the robot's intentions (socially intelligent interoperability). The team intends to collaborate closely with a single partner hospital to iteratively improve the reliability and robustness of the artificial intelligence software suite developed with NSF funding and to deploy production-quality versions of the three core competencies. The final 6 months will involve a long-term deployment, with the robot autonomously working on an acute care unit of the partner hospital. The impact of the robot on unit staff and workflows will be documented, with the ultimate goal of developing a service robot that hospital staff view as a competent member of the care team."
"1655300","Discovering Hierarchical Representations for Action Understanding","BCS","GVF - Global Venture Fund, Perception, Action & Cognition","08/01/2017","04/05/2017","Hongjing Lu","CA","University of California-Los Angeles","Standard Grant","Betty Tuller","07/31/2021","$555,792.00","","hongjing@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","054Y, 7252","5946, 5980, 7252","$0.00","A major issue in the psychological sciences is understanding how people can infer the intentions of others. Humans are remarkably adept at predicting the actions of other people and making inferences about their intention and goals. The present investigation examines how humans make such inferences from the physical movements of others. The work is guided by a computational theory of biological motion understanding that quantifies what aspects of actions allow observers to make inferences about the meaning of actions and what might come next. The larger goal is to explain how perception and reasoning operate synergistically to infer hidden goals and intentions. These findings will guide development of the next generation of intelligent machine-vision systems, useful in forensic sciences as well as many other real-world applications. Such systems will need to perform challenging tasks that currently are difficult and time-consuming for humans (for example, automated interpretation of human actions recorded in low-resolution surveillance video). The project will also help to identify individual differences in action understanding, potentially revealing the nature of the impairments in action understanding observed in people with autism disorder. In addition, the project will provide a unique training opportunity for students who are interested in interdisciplinary research at the interface between cognitive science and artificial intelligence and will provide an in-depth international research experience for a graduate student and postdoctoral fellow.<br/><br/>The research will integrate advanced psychophysical methods with sophisticated computational approaches. A key aim is to develop a unified theory based on a hierarchical non-parametric Bayesian framework, specifying the fundamental computational mechanisms involved in perception of human actions and reasoning about them. More generally, the project will use human body movements as an underutilized approach to understanding general problems in learning: how to construct, use and transform hierarchical representations to support human perception and cognition. Three aims are particularly noteworthy. First, the project will integrate computational modeling approaches with behavioral experiments to investigate the critical connection between perceptual and cognitive systems. Second, the project uses action stimuli derived from motion capture data in the real world as the visual input (CCTV images collected in the UK and secured at the University of Glasgow). By avoiding the limitations of studies that use restricted examples and constrained environments, the investigators maximize the likelihood that the findings will generalize to real-world situations. Third, the project will develop significant extensions of Bayesian approaches in order to study complex visual processes by combining generative models with probabilistic constraints.  This award is co-funded by the Perception, Action, and Cognition Program and the Office of International Science and Engineering."
"1744356","Workshop: Understanding Emerging Technologies and the Future of Work","BCS","Sociology, Social Psychology, IUSE, STS-Sci, Tech & Society","09/01/2017","02/27/2020","Laurel Smith-Doerr","MA","University of Massachusetts Amherst","Standard Grant","Steven J. Breckler","08/31/2020","$119,392.00","Shlomo Zilberstein, Enobong Branch, Henry Renski, Shannon Roberts","profsmithdoerr@gmail.com","Research Administration Building","Hadley","MA","010359450","4135450698","SBE","1331, 1332, 1998, 7603","063Z, 7556, 9179","$0.00","Intelligent, interactive, and highly networked machines are a growing part of work and the workplace.  Automation is moving from the factory floor to knowledge and service occupations.  The potential benefits of technology include increased productivity and more job opportunities.  But technology connected to work can also carry substantial social costs.  The workshop supported by this award will promote the convergence of education, social and behavioral sciences, computational sciences, and engineering with stakeholders. This diverse group will define key research challenges that focus on the intersection of humans, technology, and work.  Convergence is the deep integration of knowledge, theories, methods, and data from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. Two workshops will address the future of work at the human-technology frontier. The workshops will focus on the challenges of shaping emergent technologies that are equitable. They will also consider how the technologies will engage a wider range of people in the workforce of the future.  The results of the workshops will include reports, communication materials, and the organization of interdisciplinary panels at professional scientific meetings.<br/><br/>The specific focus of this workshop effort is on understanding the social and technical dimensions of new technologies. The goal is to develop a research agenda that will help us understand the challenges of shaping emergent technologies in ways that result in good jobs for a wide range of U.S. workers. This includes a workshop that will bring together expert scientists to consider (1) how the changing organization of work and technology affects income inequality; (2) how decisions are made in developing artificial intelligence and processes for human-technology partnerships; (3) how to develop methods for assessing emerging technologies in terms of likely work satisfaction and inequality in employment outcomes; and (4) how workforce development and economic systems can help make high-paying stable jobs widely available. The second workshop will include stakeholders and will focus on how to use these ideas at the local level."
"1724101","S&AS: FND: Reliable Semi-Autonomy with Diminishing Reliance on Humans","IIS","S&AS - Smart & Autonomous Syst, Robust Intelligence","09/01/2017","05/29/2018","Shlomo Zilberstein","MA","University of Massachusetts Amherst","Standard Grant","Jie Yang","08/31/2021","$707,512.00","Joydeep Biswas","shlomo@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","039Y, 7495","046Z, 9251","$0.00","Building reliable autonomous systems that can construct and execute plans to achieve some assigned goals, without human intervention, has been the hallmark of artificial intelligence and robotics since their inception.  Reliable autonomy is becoming increasingly important as it enables innovative new applications in areas such as transportation, health, and sustainable living.  Despite substantial progress, there are still considerable barriers to the long-term, large scale deployment of fully autonomous systems such as self-driving cars or mobile service robots.  These barriers range from technological and economic constraints to ethical and legal issues.  This project offers a comprehensive approach to circumvent these barriers by building semi-autonomous systems that rely on rich forms of human assistance, ranging from advice to constant supervision of the system with the possibility of taking over control.  The project develops techniques to assure the safety of such systems when human assistance is delayed and to reduce their reliance on human assistance over time.  Additionally, the project contributes to training of undergraduate and graduate students in this interdisciplinary area, mentoring of students with special attention to underrepresented groups, outreach activities to local schools, and strengthening of industrial collaborations.<br/><br/>The project answers fundamental questions about the feasibility, efficiency, and scalability of planning and learning algorithms to support semi-autonomous systems.  The main thrusts of the project are (1) develop techniques that can delegate autonomy to a system with some restrictions, and provide strong guarantees that these restrictions will be respected and that the system will maintain a safe state even when human assistance is delayed; (2) develop planning and learning algorithms that are cognizant of the availability of rich forms of human assistance and can effectively factor such assistive actions into the overall plan; (3) handle the high computational complexity of optimizing the interaction with humans under uncertainty and partial observability by creating a hierarchical multi-objective decision model; and (4) leverage human assistance to enable robust and accurate mapping and navigation in new areas, while reducing the reliance on human supervision over time.  The project evaluates these capabilities in complex realistic settings involving a campus-scale robot deployment, a driving simulator, and autonomous vehicles in collaboration with Nissan."
"1744386","Convergence HTF: A Workshop Shaping Research on Human-Technology Partnerships to Enhance STEM Workforce Engagement","BCS","Social Psychology, INSPIRE","09/01/2017","08/23/2017","Keivan Stassun","TN","Vanderbilt University","Standard Grant","Steven J. Breckler","08/31/2020","$98,346.00","Maithilee Kunda, Zachary Warren, Frank Tong, Nilanjan Sarkar","keivan.stassun@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","SBE","1332, 8078","060Z, 063Z, 7556","$0.00","The landscape of jobs and work is changing rapidly, driven by the development of new technologies. Intelligent, automated machines and services are a growing part of jobs and the workplace. New technologies are enabling new forms of learning, skills assessments, and job training. The potential benefits of these technologies include increased productivity and satisfaction, and more job opportunities. The workshop supported by this award aims to harness these innovations to enhance the science, technology, engineering, and mathematics (STEM) job opportunities and workforce engagement of individuals with autism spectrum disorder (ASD), and related developmental disabilities. The workshop will promote the convergence of psychology, data science, computer science, engineering, learning science, special education, organizational behavior, and business to define key challenges and research imperatives at the nexus of humans, technology, and work. This convergence workshop will employ deep integration of knowledge, theories, methods, and data from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. The results of the workshop will include the identification and sharing of new research directions and tools to enhance STEM workforce engagement of individuals with ASD and related developmental disabilities. This convergence workshop addresses the future of work at the human-technology frontier.<br/><br/>The workshop will explore tools and approaches to enhance retention, engagement, and productivity in STEM jobs, and specifically to harness unique capabilities and accommodate for individual needs of individuals with ASD. The workshop will develop a convergence research agenda around four topics, including 1) human-technology partnerships to support success in K-12 STEM education, 2) tools for characterizing individual capabilities and affinities and mapping these to STEM workforce needs, 3) artificial-intelligence and visual-cognition tools for human interaction with data, and 4) technologies to accommodate for unique needs and capabilites in the workplace. These topics will integrate previously disparate disciplines and research approaches, with speakers encompassing a wide range of subject matter expertise; from engineers and technologists who are developing human-technology interfaces and devices, to psychologists who are harnessing human-technology partnerships to better understand unique human capabilities for STEM, to computer scientists who are studying and developing novel data-visualization approaches patterned on autistic visual thinking, to organizational scientists developing innovative employment models for the creation of STEM sector employment spaces and technologies that leverage and support autistic individuals in the workforce. The conclusions and recommendations from the workshop will be disseminated via a white paper, and will be used to design a research agenda to help leverage human-technology advances to maximize workforce opportunities and productivity."
"1724026","CRCNS Research Proposal: Collaborative Research: Studying Competitive Neural Network Dynamics Elicited By Attractive and Aversive Stimuli and their Mixtures","EF","Cross-BIO Activities, CRCNS-Computation Neuroscience","09/01/2017","08/01/2018","Dirk Albrecht","MA","Worcester Polytechnic Institute","Continuing Grant","Edda Thiels","08/31/2020","$280,497.00","","dalbrecht@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","BIO","7275, 7327","1228, 7327, 8089, 8091, 9178, 9179","$0.00","This award supports basic research regarding the question of how networks in the brain allow odors to be detected and perceived.  Such a question is of fundamental interest in neuroscience because responding to odors or scents is one of the most basic ecological abilities exhibited across different animal species.  Further, responses to odors are highly dependent on context.  For example, certain smells may create both attractive and repulsive reactions, depending on small differences in dilution or whether they are encountered alone or as components in a cocktail.  Thus, studying how the brain processes odors can provide important clues regarding how animals and humans sense and perceive in complex environments.  In seeking such understanding, this project uses a unique combination of methods from neuroscience, mathematics, and engineering.  Brain activity from two different animal species are recorded during experiments in which odors are presented in isolation and in mixtures.  Subsequently, data analysis and mathematical modeling is used to identify brain activity patterns that distinguish the reaction of the animals to the odors in question.  Hence, the project uncovers how particular brain networks transform and transmit odor information in a way that is central to the sense of smell.  To broaden the impact of these studies, the project includes the development of a summer internship in sensory neural engineering, intended to allow undergraduate and high school students to learn about and experience how different academic disciplines contribute to future brain science.<br/><br/>The extent to which sensory networks amplify or suppress perceived differences in odor valence remains a fundamental, unanswered question in sensory neuroscience.  The overarching hypothesis of this project is that indeed, there exists a well-defined set of transformations, governed by neuronal dynamics, which map sensory network activity to behavior.  Specifically, the project will determine: (a) How neural networks enable the formation of time-varying neural activation patterns, or, trajectories, in response to sensory stimuli, (b) The mapping from trajectories to behavioral outcome, and (c) The commonality of this mapping across species.  The research goals use an interdisciplinary approach combining sensory systems neuroscience in two species, locusts (Schistocerca americana) and round worms (C. elegans), with computational modeling and dynamical systems theory.  Neural and behavioral responses are recorded from animals receiving nominally attractive and aversive odors, and these data inform computational models of the sensory networks and ensuing behaviors.  The models generate predictions on how behavioral responses might be modulated by a change in selectivity, or background state.  The latter is tested through a paradigm wherein animals are systematically fed or starved, thus shifting their response dynamics on the aversive-attractive spectrum.  Subsequently, model-based sensitivity analyses is used to predict mixture response curves and paradoxical mixtures (e.g., two aversive stimuli that when mixed, elicit an attractive response).  These predictions are tested by delivering component stimuli in systematic ratios.  Thus, the overall methodology combines physiological experiments with new systems-level analysis in an integrated, multidisciplinary modeling-theory loop."
"1744082","SHF: Small: Cross-Platform Solutions for Pruning and Accelerating Neural Network Models","CCF","Software & Hardware Foundation","05/01/2017","05/25/2017","Hai Li","NC","Duke University","Standard Grant","Sankar Basu","06/30/2019","$424,432.00","","hai.li@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7798","7923, 7945, 8089, 8091","$0.00","Deep neural networks (DNNs) have achieved remarkable success in many applications because of their powerful capability for data processing. The objective of this project is to investigate a software-hardware co-design methodology for DNN acceleration that can be applied to both traditional von Neumann and emerging neuromorphic architectures. The project fits into the general area of ""brain-inspired"" energy efficient computing paradigms that has been of much recent interest. The investigators are also active in various outreach and educational activities that include curricular development, engagement of minority/underrepresented students in research. Undergraduate and graduate students involved in this research will also be trained for the next-generation computer engineering and semiconductor industry workforce.<br/><br/>From a more technical standpoint, a novel neural network sparsification process is to be explored to preserve the state-of-the-art accuracy, while establishing hardware-friendly models of neural network computations.  The result is expected to lead to a holistic methodology composed of neural network model sparsification, hardware acceleration, and an integrated software/hardware co-design.  The project also benefits big data research and industry at large by inspiring an interactive design philosophy between the design of learning algorithms and the corresponding computational platforms for system performance and scalability enhancement."
"1724218","CRCNS Research Proposal: Collaborative Research: Studying Competitive Neural Network Dynamics Elicited By Attractive and Aversive Stimuli and their Mixtures","EF","Cross-BIO Activities, CRCNS-Computation Neuroscience","09/01/2017","07/25/2018","ShiNung Ching","MO","Washington University","Continuing Grant","Edda Thiels","08/31/2021","$469,503.00","Baranidharan Raman","shinung@ese.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","BIO","7275, 7327","1228, 7327, 8089, 8091, 9150, 9178, 9179","$0.00","This award supports basic research regarding the question of how networks in the brain allow odors to be detected and perceived.  Such a question is of fundamental interest in neuroscience because responding to odors or scents is one of the most basic ecological abilities exhibited across different animal species.  Further, responses to odors are highly dependent on context.  For example, certain smells may create both attractive and repulsive reactions, depending on small differences in dilution or whether they are encountered alone or as components in a cocktail.  Thus, studying how the brain processes odors can provide important clues regarding how animals and humans sense and perceive in complex environments.  In seeking such understanding, this project uses a unique combination of methods from neuroscience, mathematics, and engineering.  Brain activity from two different animal species are recorded during experiments in which odors are presented in isolation and in mixtures.  Subsequently, data analysis and mathematical modeling is used to identify brain activity patterns that distinguish the reaction of the animals to the odors in question.  Hence, the project uncovers how particular brain networks transform and transmit odor information in a way that is central to the sense of smell.  To broaden the impact of these studies, the project includes the development of a summer internship in sensory neural engineering, intended to allow undergraduate and high school students to learn about and experience how different academic disciplines contribute to future brain science.<br/><br/>The extent to which sensory networks amplify or suppress perceived differences in odor valence remains a fundamental, unanswered question in sensory neuroscience.  The overarching hypothesis of this project is that indeed, there exists a well-defined set of transformations, governed by neuronal dynamics, which map sensory network activity to behavior.  Specifically, the project will determine: (a) How neural networks enable the formation of time-varying neural activation patterns, or, trajectories, in response to sensory stimuli, (b) The mapping from trajectories to behavioral outcome, and (c) The commonality of this mapping across species.  The research goals use an interdisciplinary approach combining sensory systems neuroscience in two species, locusts (Schistocerca americana) and round worms (C. elegans), with computational modeling and dynamical systems theory.  Neural and behavioral responses are recorded from animals receiving nominally attractive and aversive odors, and these data inform computational models of the sensory networks and ensuing behaviors.  The models generate predictions on how behavioral responses might be modulated by a change in selectivity, or background state.  The latter is tested through a paradigm wherein animals are systematically fed or starved, thus shifting their response dynamics on the aversive-attractive spectrum.  Subsequently, model-based sensitivity analyses is used to predict mixture response curves and paradoxical mixtures (e.g., two aversive stimuli that when mixed, elicit an attractive response).  These predictions are tested by delivering component stimuli in systematic ratios.  Thus, the overall methodology combines physiological experiments with new systems-level analysis in an integrated, multidisciplinary modeling-theory loop."
"1658560","Collaborative Research: Structural and functional architecture shaping neural tuning within the human posterior superior temporal sulcus","BCS","Cognitive Neuroscience","01/15/2017","01/17/2017","Emily Grossman","CA","University of California-Irvine","Standard Grant","Kurt Thoroughman","12/31/2020","$273,582.00","","grossman@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","SBE","1699","1699, 7298","$0.00","Humans are social creatures with extensive neural systems dedicated to the skills required to navigate interactions with others. This includes decoding the actions of others to infer goals and intentions, and planning our own actions that are appropriate for the current context. Brain regions that support these skills are anatomically dispersed in the four lobes of the brain, organized as a network with communication via long-range white matter connections. One key hub of this network is the posterior superior temporal sulcus (pSTS). The work is this proposal will address an important outstanding question: how the long-range connections supporting action understanding are organized, and the nature of the information that is integrated through these connections. This work will combine structural and functional brain imaging to identify anatomical pathways connecting systems supporting action recognition, with particular attention to pathways through the pSTS, and will use computational statistical analyses to characterize the neural information that is carried through those pathways. This problem is of urgent scientific and clinical relevance: Neuroscience increasingly recognizes that brain regions do not function in isolation, but instead reflect the integration of neural signaling from many cortical sources. The work in this proposal seeks to advance brain science by explicitly modeling these sources in a targeted cortical network. The action recognition network holds additional importance to the public, as some neurodevelopmental disorders (such as autism) are linked to atypical development of the pSTS and poor communication within this neural network. Therefore the outcomes from this work may be critical for developing new clinical tools for diagnosis and interventions for these disorders. Implementing the work in this grant will also support the full engagement and promotion of under-represented and first-generation of young scientists training in neuroscientific research. <br/><br/>The problem of how information is communicated and structured within the action recognition network is an important one. Many competing scientific models exist as to the functional specialization of the posterior superior temporal sulcus and connected brain regions within the action recognition network. New empirical data and analytical techniques are required to advance these theoretical models. A key to understanding information structure within the pSTS and the larger action recognition network is to evaluate the sources integrated within the neural signals, which reflect both sensory-driven perceptual analysis of social cues and the top-down goal-directed signals modulate influences. The work in this proposal will combine innovative experimental design with advanced multivariate statistical analyses to extract structure from the rich regional brain activation response, and will decompose the contribution of sensory-driven and top-down signals on neural tuning. At the same time, one must consider where top-down goal-directed signals originate and the structural pathways by which they are transmitted. The work in this proposal is innovative in that it will characterize the network architecture, both structurally and functionally, using a combination of tools rarely implemented despite their clear complementarity."
"1757520","SCH: INT: Collaborative Research: FITTLE+: Theory and Models for Smartphone Ecological Momentary Intervention","IIS","Smart and Connected Health","08/18/2017","09/20/2017","Peter Pirolli","FL","Florida Institute for Human and Machine Cognition, Inc.","Standard Grant","Sylvia Spengler","09/30/2018","$192,193.00","","ppirolli@ihmc.us","40 S. Alcaniz St.","Pensacola","FL","325026008","8502024473","CSE","8018","8018, 8062, 9251","$0.00","Many health conditions are caused by unhealthy lifestyles and can be improved by behavior change. Traditional behavior-change methods (e.g., weight-loss clinics; personal trainers) have bottlenecks in providing expert personalized day-to-day support to large populations for long periods. There is a pressing need to extend the reach and intensity of existing successful health behavior change approaches in areas such as diet and fitness. Smartphone platforms provide an excellent opportunity for projecting maximally effective interventions for behavior change into everyday life at great economies of scale. Smartphones also provide an excellent opportunity for collecting rich, fine-grained data necessary for understanding and predicting behavior-change dynamics in people going about their everyday lives. The challenge posed by these opportunities for detailed measurement and intervention is that current theory is not equally fine-grained and predictive. <br/><br/>This interdisciplinary project investigates theory and methods to support fine-grained behavior-change modeling and intervention integrated via smartphone into the daily lives of individuals and groups.  Fittle+ develops a new and transformative form of smartphone-delivered Ecological Momentary Intervention (EMI) for improving diet and physical activity. This approach will provide social support and autonomously planned and personalized coaching that builds on methods from mobile sensing, cognitive tutoring, and evidence-based social design. The foundation for this new approach will require new predictive computational theories of health behavior change. Current coarse-grained conceptual theories of individual health behavior change will be refined into fine-grained predictive computational models. These computational models will be capable of tracking moment-by-moment human context, activity, and social patterns based on mobile sensing and interaction data. Using these monitoring capabilities, Fittle+'s computational models will support assessment of, and predictions about, individual users and groups based on underlying motivational, cognitive, and social mechanisms. These predictive models will also be used to plan and optimize coaching actions including detailed diagnostics, individualized goals, and contextually and personally adapted interventions. <br/><br/>The collaborative team of researchers works with weight-loss interventionists at one of nation's largest health organization's facility in Hawaii. The team includes expertise in mobile sensing, artificial intelligence, computational cognition, social psychology, human computer interaction, computer tutoring, and measurement theory."
"1724537","WORKSHOP: The Pioneers Workshop at the 2017 ACM/IEEE International Conference on Human-Robot Interaction","IIS","HCC-Human-Centered Computing","03/01/2017","02/21/2017","Brian Scassellati","CT","Yale University","Standard Grant","Ephraim Glinert","02/28/2018","$40,950.00","","brian.scassellati@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7367","7367, 7556","$0.00","This is funding to support a Pioneers Workshop (doctoral consortium) of approximately 24 graduate students (12 of whom are from the United States and therefore eligible for funding), along with distinguished research faculty.  The event will take place as part of the first day of activities at the 12th International Conference on Human Robot Interaction (HRI 2017), to be held March 6-9 in Vienna, Austria, and which is jointly sponsored by ACM and IEEE.  HRI is the premier conference for showcasing the very best interdisciplinary and multidisciplinary research on human-robot interaction, with roots in diverse fields including robotics, artificial intelligence, social psychology, cognitive science, human-computer interaction, human factors, engineering, and many more.  It is a single-track, highly selective annual international conference that invites broad participation.  Building on the ""Smart City Wien"" initiative, the theme of HRI 2017 is ""Smart Interaction.""  The conference seeks contributions from a broad set of perspectives, including technical, design, methodological, behavioral, and theoretical, that advance fundamental and applied knowledge and methods in human-robot interaction, with the goal of enabling human-robot interaction through new technical advances, novel robot designs, new guidelines for design, and advanced methods for understanding and evaluating interaction.  More information about the conference is available online at http://humanrobotinteraction.org/2017.  The Pioneers Workshop will afford a unique opportunity for the best of the next generation of researchers in human-robot interaction to be exposed to and discuss current and relevant topics as they are being studied in several different research communities.  This is important for the field, because it has been recognized that transformative advances in research in this fledgling area can only come through the melding of cross-disciplinary knowledge and multinational perspectives.  Participants will be encouraged to create a social network both among themselves and with senior researchers at a critical stage in their professional development, to form collaborative relationships, and to generate new research questions to be addressed during the coming years.  Participants will also gain leadership and service experience, as the workshop is largely student organized and student led.  The PI has expressed his strong commitment to recruiting women and members from under-represented groups.  To further ensure diversity the event organizers will consider an applicant's potential to offer a fresh perspective and point of view with respect to HRI, will recruit students who are just beginning their graduate degree programs in addition to students who are further along in their degrees, and will strive to limit the number of participants accepted from a particular institution to at most two.  As a new feature this year, the organizers will also invite 3 undergraduate students (all eligible for funding) to help increase diversity in the pipeline of students entering this field.<br/> <br/>The Pioneers Workshop is designed to complement the conference, by providing a forum for students and recent graduates in the field of HRI to share their current research with their peers and a panel of senior researchers in a setting that is less formal and more interactive than the main conference.  During the workshop, participants will talk about the important upcoming research themes in the field, encouraging the formation of collaborative relationships across disciplines and geographic boundaries.  To these ends, the workshop format will encompass a variety of activities including three keynotes, a distinguished panel session, and breakout sessions.  To start the day, all workshop attendees will briefly introduce themselves and their interests.  Following the opening keynote, approximately half of the participants will present 3-minute overviews of their work, leading into an interactive poster session.  This will enable all participants to share their research and receive feedback from students and senior researchers in an informal setting.  The workshop organizers will facilitate the post-presentation discussion and will encourage participants to ask questions of their peers during the interactive break and poster session.  After lunch, the remaining workshop participants will give their 3-minute overviews, followed by presentation of their posters during a second interactive poster session.  Senior researchers (in addition to those on the panel) will be invited to attend the student presentations and poster sessions in order to provide feedback to participants, and workshop participants will be invited to present their posters during the main poster session of the HRI conference as well.  The conversations between the panel and participants will continue over lunch and during dinner."
"1727303","CI-EN: Enhancement of a Large-scale Multiagent Simulation Tool","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2017","07/11/2017","Sean Luke","VA","George Mason University","Standard Grant","Roger Mailler","08/31/2021","$896,303.00","Robert Simon, Andrew Crooks","sean@cs.gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7359","7359","$0.00","An agent-based simulation is a software simulation of many independent actors (people, robots, animals, companies, etc.) interacting in complex ways.  For example, one might build a simulation of a swarm of ants, a school of fish, a large group of robots collectively building a house, a city of people under siege by a large medieval army, or the spread of urban legends over a social network. These kinds of simulations are used for everything from building software for swarms of delivery robots, to understanding the spread of disease in third-world slums, to predicting the impact of climate change on human migration patterns.  Similarly, these simulations help researchers and policy makers in engineering and robotics, in artificial intelligence, and in the biological and social sciences.  Such simulations can get very large, with large numbers of actors.  This project involves building a software tool to assist in the development of large-scale agent-based simulations spread over potentially many separate computers, and to make it easy for them to place their agents in simulated locations on the Earth.  At the same time the tool will be easy to use for high school and undergraduate students.<br/><br/>The project will develop a distributed agent-based modeling tool for constructing large simulations of swarms and groups of agents.  The project enhances an existing, successful open-source Java multiagent simulation library called MASON.  MASON is designed to run on a single machine; but the enhanced version will also allow distribution over many machines.  The enhanced tool will also include facilities for embedding agents in geographical information systems (GIS), model optimization and automation, interfaces for alternative programming languages targeting the Java Virtual Machine, statistics facilities, internal testing and verification, and integration with software tools.  The enhancement will not only provide a distributed simulation facility for researchers in the social sciences, biology, and engineering, but will also improve on MASON's graphical interface and language facilities to make it easier to use as a teaching tool."
"1747486","Real-world language: Future directions in the science of communication and the communication of science","BCS","Perception, Action & Cognition","09/15/2017","07/17/2017","James Magnuson","CT","University of Connecticut","Standard Grant","Betty Tuller","02/29/2020","$20,943.00","","james.magnuson@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","SBE","7252","7252, 7556","$0.00","This award will support the organization of a two-day workshop on future challenges in language science, with integrated discussion of science communication (making language science research accessible to specialist peers, scientists in other fields, and the general public). The workshop will take place in Madison, WI, immediately following the 2018 Cognitive Science Society annual meeting. Language science is an interdisciplinary area drawing on theories and methods from linguistics, cognitive psychology, developmental psychology, and artificial intelligence, among other fields. The primary goal is basic scientific understanding of the human capacity for language and potential longer-term impact on technology, education, and health. <br/><br/>Invited speakers will 1) provide critical reviews of different theoretical perspectives, methodological approaches, and tools (such as eye tracking, electroencephalography, or functional magnetic resonance imaging); 2) focus on near- and long-term challenges facing language science; or 3) focus on science communication and education. In an effort to spark discussion and collaboration,  research interest groups will be formed that will hold videoconference meetings in Fall, 2018. Plans include strategies for promoting student participation and inclusion of women and members of under-represented groups."
"1713439","Applying Game Design Principles for Supporting Computational Literacy Experiences in Museum Exhibits","DRL","AISL","09/01/2017","08/16/2017","Matthew Berland","WI","University of Wisconsin-Madison","Standard Grant","Catherine L. Eberbach","08/31/2021","$951,474.00","Leilah Lyons, Matthew A. Cannady","mberland@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","EHR","7259","8244","$0.00","Becoming computationally literate is increasingly crucial to everyday life and to expanding workforce capacity. Research suggests that computational literacy--knowing what, when, how, and why to use the ideas of computer science, in combination with the capacity to view problems and potential solutions through the lens of computational structures and procedures--can be supported through digital game play. This project aims to develop a social and creative exhibit game that foregrounds aspects of computer science, specifically artificial intelligence (AI) and computer programming, in ways that enable youth to explore, construct, and share computational complex systems content with one another and other museum visitors. To play the game, pairs of youth visitors will use code cards to program the behavior of AI animals in a virtual forest. As they do so, youth will engage with computational literacy practices, such as basic computer programming, describing their computational ideas, and doing computational problem solving with their friends. Their activity will be projected on a large screen as a strategy for enabling youth to test, rehearse, and communicate their computational ideas and to also interest other visitors into computational problem solving.<br/><br/>Using multi-perspective and iterative design-based research, university learning scientists, museum practitioners, and game developers will pursue research questions around how science museums can better engage youth who are traditionally underrepresented in computer science in complex computational practices. Data sources will include interactive-log data, observations of visitor interactions with the game, visitor interviews, and visitor surveys. A multimodal and mixed methods approach that searches for convergences between qualitative analysis, quantitative analysis, and learning analytics will be used to generate research findings. Changes in computational literacy will be assessed by evaluating what problems visitors choose to solve with programming, how they frame those problems, and their selections from among possible solutions, what they program, how they program, and how they describe programming ideas. The results of this project will include: 1) a social, interactive gameplay experience that supports the development of computational literacy; 2) design principles for game-based exhibits that facilitate development of computational literacy; and 3) new knowledge of variations in design and gameplay across diverse gameplay users, including those from underrepresented groups in computer science. It is anticipated that 1,000 museum youth visitors will directly participate in the study.  <br/><br/>This project is funded by the Advancing Informal STEM Learning (AISL) program, which seeks to advance new approaches to, and evidence-based understanding of, the design and development of STEM learning in informal environments. This includes providing multiple pathways for broadening access to and engagement in STEM learning experiences, advancing innovative research on and assessment of STEM learning in informal environments, and developing understandings of deeper learning by participants."
"1711775","Bridging the Gap Between Education and Research through Pre-College Engineering Systems (PCES) Outreach Program","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/01/2017","07/18/2017","James Momoh","DC","Howard University","Standard Grant","Anil Pahwa","07/31/2021","$170,000.00","","jmomoh@howard.edu","2400 Sixth Street N W","Washington","DC","200599000","2028064759","ENG","7607","1340, 155E, 9177","$0.00","The purpose of the project is to develop a residential summer research and education Science Technology Engineering and Mathematics (STEM) outreach program targeted at 11th and 12th graders from across the country. The program will advance students' preparation for careers in engineering through integration of research and education. The students in the program will undertake lectures on topics such as advanced mathematics and physics, chemistry, preparatory scholastic aptitude testing, advanced placement courses, and will be exposed to fundamentals of engineering courses and special topics in Electrical and Computer Engineering such as artificial intelligence, communications, nanotechnology, photonics, energy systems and smart grid technologies. These courses will prepare and motivate students for research work in energy management, automation functions for secured critical infrastructures, sensor based systems and safe environment. The background achieved and exposure through hands-on activities will capacity for a future diverse workforce of underrepresented groups and women to pursue careers in engineering.<br/><br/>The research work in the pre-college engineering systems program will include various fields of engineering and science used to help students to appreciate the role of creativity, analytical and hands-on work in conducting engineering projects, processes and systems. The research work will involve needs assessment, constraints, problem formulation and design of algorithms, implementation, testing and validation under different scenarios. The major areas for project ideas include communication and signal processing, photonics/electronics, nanotechnology, materials and energy and power systems, with each area having its own set of specific projects. Under communication and signal processing, specific projects include the solar bag, smart phones, new integrated display board, and others. In photonics/electronics, the specific works include mobile diagnostics for power electronics and smart displays. Nanotechnology and materials projects include design of nano toothpaste, nano skating boots, and a nano tricycle for kids. In the area of energy and power systems, smart city design using renewable energy resources and electric vehicles with self driving properties are specific project areas. Furthermore, wireless sensors, a handmade wristwatch for health monitoring, and automatic fan control will be pursued within the computer engineering projects. Students will work in teams and final products will be presented at the end of the program each year to an audience of faculty, parents and a representative of the funding agency, the National Science Foundation (NSF). Mentoring during and after the program (during the school year) will be in place to track students' continued progress toward life-long pursuit of engineering as a carrier. Lessons learned will be shared with other engineering schools so that the country will benefit from the NSF investment in education and research activities for pre-college students at Howard University."
"1751765","AF: EAGER: Homomorphism Problems in Digraphs (Dichotomies)","CCF","Algorithmic Foundations","09/15/2017","09/05/2017","Arash Rafiey","IN","Indiana State University","Standard Grant","Tracy Kimbrel","03/31/2021","$141,056.00","Geoffrey Exoo, Jeffrey Kinne, Laszlo Egri","Arash.Rafiey@indstate.edu","200 N 7TH STREET","TERRE HAUTE","IN","478091902","8122373088","CSE","7796","7916, 7926, 7927","$0.00","Graph coloring is one of the most important problems in theoretical computer science.  Many combinatorial optimization problems can be viewed as graph coloring problems.  For a given graph G and integer k, the question is whether there exists a coloring of its vertices with k colors such that any two adjacent vertices receive different colors.  The Graph (or Directed Graph) Homomorphism Problem is a generalization of graph coloring.  In the Graph Homomorphism Problem, the goal is to find a mapping from an input graph (or digraph) to a fixed target graph (or digraph) H that preserves adjacency.<br/><br/>Homomorphism problems, and the equivalent formulation as so-called constraint satisfaction problems (CSPs), enjoy a wide variety of applications as optimization problems that must be solved in practice.  Such applications can be seen in scheduling, planning, databases, artificial intelligence, and many other areas.<br/><br/>The Digraph Homomorphism Problem and CSPs have been two very active research areas in Theoretical Computer Science over the last two decades.  Several  tools (mostly algebraic) have been developed for solving CSPs, and very recently a number of proposed solutions (including our solution) to the main conjecture in the area (known as the CSP Conjecture) have arisen.  The present project aims to verify in detail each approach to distill the most elegant proof and most efficient algorithms.  The approach is purely combinatorial, using techniques from graph theory.<br/><br/>The project will also tackle problems closely related to the newly proposed solutions to the CSP Conjecture.  For example, the PIs seek forbidden obstruction characterizations for the types of digraphs H that make homomorphism problems feasible. This would help to improve the running time of the current algorithm.<br/><br/>The project aims also to have a high educational impact, through training graduate students in theoretical computer science, producing <br/>freely available and high quality lecture notes and survey material on the field, seeking connections between the research and other important areas of research across computing, and utilizing novel teaching and dissemination methods."
"1724753","Inductive learning of nonlocal phonological interactions","BCS","Linguistics","08/01/2017","08/08/2017","Gillian Gallagher","NY","New York University","Standard Grant","Tyler Kendall","01/31/2021","$214,541.00","Maria Gouskova","geg4@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","1311","1311, 9179","$0.00","Language is a fundamental and universal aspect of human cognition. Linguistic research over the past five decades has established that language structure is governed by detailed rules. These rules constrain meanings, sentences, words, and sound patterns--the focus of the proposed research. For many years, sound structure was investigated primarily by theorizing. More recently, linguists have begun to test these theories using experiments and computational models. Computational models are valuable because they can only be created based on a complete and explicit understanding of the underlying rules. If the model succeeds in learning human-like rules when given the same data that is available to human learners, then it can shed some light on the rules that constitute the human knowledge of language and how humans learn these rules. In addition to helping scientists understand the human mind, computational models and the datasets they use are invaluable in developing applied computational tools for machine language translation, language identification, and artificial intelligence.<br/><br/>The rules that govern sound patterns differ in nuanced ways between languages, and they can be divided into two kinds. First, all languages have rules that restrict how sounds interact with sounds that immediately precede or follow them: for example, in English, words can begin in ""pr"" but not ""pn"", whereas in Greek, words can begin in either sequence. But some languages also have rules that restrict the interactions of sounds that are not adjacent (nonlocal). Languages such as Hungarian and Turkish have vowel harmony, which means that all the vowels in a word tend to share certain features of their pronunciation. Navajo (Southwestern United States) has consonant harmony--consonants have to match in certain features. In languages such as Quechua (spoken in South America) and Amharic (Africa), certain features of consonants have to mismatch. Linguists have known about these patterns for a long time, and there are many theories of how they are cognitively represented. But these nonlocal rules continue to stymie computational models, because in order to notice them, the computer has to consider many more possibilities than it would for rules on adjacent sounds. This is similar to how much more difficult it is for a computer to crack a password the longer it gets. The proposed research builds a computational model of nonlocal rules that identifies certain clues to their existence in a language. The project will compile corpora to test the model's ability to find nonlocal rules (Quechua, Shona, Hungarian, Russian, Aymara, Sundanese). The model's performance will be compared with experiments with native speakers of several languages. The model, the corpora, and the experimental data will be made freely available to the scientific community and the public. Workshops will disseminate the research in Bolivia. The project will provide training for students in computational analysis and corpus building."
"1657039","CRII: CSR: Rethinking the FTL in SSDs -- a file translation layer instead of a flash translation layer","CNS","CRII CISE Research Initiation","03/15/2017","03/20/2017","Hung-Wei Tseng","NC","North Carolina State University","Standard Grant","Marilyn McClure","08/31/2019","$174,998.00","","htseng@ucr.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","026Y","7354, 8228","$0.00","As data sets for artificial intelligence, network services, and cloud storage grows, so does the demand of quickly and efficiently serving data from the storage device. Using solid state drives (SSDs) based on non-volatile, flash memory technologies is an effective approach to improve the performance of storage devices. However, as the rest parts of the computer system leverage the entrenched interface to communicate with SSDs, the overhead of supporting these abstractions buries the real potential of SSDs. Specifically, the different addressing modes for different system layers result in multiple address translations when accessing a single file and require additional system resource to maintain the mapping. This address translation overhead takes time and limits the bandwidth of SSDs. This project is addressing this problem by proposing an innovative storage interface that minimizes the system overhead but better fits the behaviors of applications in accessing data. The proposed interface will simplify the design of operating systems. This interface will require no modifications to existing applications. <br/><br/>As most file accessing overhead coming from the operating system, simplifying operating systems with the proposed interface will significantly boost the latency and bandwidth when applications access SSDs. This project will implement the proposed design in real SSDs without changes to existing hardware, meaning that the result of this project can immediately facilitate existing computer systems hosting big data applications without additional hardware costs. This project will also encourage researchers to revisit existing hardware/software interfaces for achieving better performance on emerging peripheral devices as well as exploring other benefits, including enhanced security and reduced energy consumption of pursuing this research direction."
"1658380","Collaborative research: Combining models and observations to constrain the marine iron cycle","OCE","Chemical Oceanography","07/01/2017","03/03/2017","Jefferson Moore","CA","University of California-Irvine","Standard Grant","Simone Metz","06/30/2021","$470,704.00","Francois Primeau","jkmoore@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","GEO","1670","","$0.00","Tiny marine organisms called phytoplankton play a critical role in Earth's climate, by absorbing carbon dioxide from the atmosphere. In order to grow, these phytoplankton require nutrients that are dissolved in seawater. One of the rarest and most important of these nutrients is iron. Even though it is a critical life-sustaining nutrient, oceanographers still do not know much about how iron gets into the ocean, or how it is removed from seawater. In the past few years, scientists have made many thousands of measurements of the amount of dissolved iron in seawater, in environments ranging from the deep sea, to the Arctic, to the tropical oceans. They found that the amount of iron in seawater varies dramatically from place to place. Can this data tell us about how iron gets into the ocean, and how it is ultimately removed? Yes. In this project, scientists working on making measurements of iron in seawater will come together with scientists who are working on computer models of iron inputs and removal in the ocean. The goal is to work together to create a program that allows our computer models to ""learn"" from the data, much like an Artificial Intelligence program. This program will develop a ""best estimate"" of where and how much iron is coming into the ocean, how long it stays in the ocean, and ultimately how it gets removed. This will lead to a better understanding of how climate change will impact the delivery of iron to the ocean, and how phytoplankton will respond to climate change. With better climate models, society can make more informed decisions about how to respond to climate change. The study will also benefit a future generation of scientists, by training graduate students in a unique collaboration between scientists making seawater measurements, and those using computer models to interpret those measurements. Finally, the project aims to increase the participation of minority and low-income students in STEM (Science, Technology, Engineering, and Mathematics) research, through targeted outreach programs.<br/><br/><br/><br/>Iron (Fe) is an important micronutrient for marine phytoplankton that limits primary productivity over much of the ocean; however, the major fluxes in the marine Fe cycle remain poorly quantified. Ocean models that attempt to synthesize our understanding of Fe biogeochemistry predict widely different Fe inputs to the ocean, and are often unable to capture first-order features of the Fe distribution. The proposed work aims to resolve these problems using data assimilation (inverse) methods to ""teach"" the widely used Biogeochemical Elemental Cycling (BEC) model how to better represent Fe sources, sinks, and cycling processes. This will be achieved by implementing BEC in the efficient Ocean Circulation Inverse Model and expanding it to simulate the cycling of additional tracers that constrain unique aspects of the Fe cycle, including aluminum, thorium, helium and Fe isotopes. In this framework, the inverse model can rapidly explore alternative representations of Fe-cycling processes, guided by new high-quality observations made possible in large part by the GEOTRACES program. The work will be the most concerted effort to date to synthesize these rich datasets into a realistic and mechanistic model of the marine Fe cycle. In addition, it will lead to a stronger consensus on the magnitude of fluxes in the marine Fe budget, and their relative importance in controlling Fe limitation of marine ecosystems, which are areas of active debate. It will guide future observational efforts, by identifying factors that are still poorly constrained, or regions of the ocean where new data will dramatically reduce remaining uncertainties and allow new robust predictions of Fe cycling under future climate change scenarios to be made, ultimately improving climate change predictions. A broader impact of this work on the scientific community will be the development of a fast, portable, and flexible global model of trace element cycling, designed to allow non-modelers to test hypotheses and visualize the effects of different processes on trace metal distributions. The research will also support the training of graduate students, and outreach to low-income and minority students in local school districts."
"1710302","Magneto-optoelectronic response in 2D atomic-layered materials","ECCS","GOALI-Grnt Opp Acad Lia wIndus, EPMD-ElectrnPhoton&MagnDevices","08/01/2017","11/06/2018","Ramesh Mani","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Paul Lane","01/31/2021","$370,404.00","","rmani@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","ENG","1504, 1517","019Z, 094E, 100E","$0.00","Abstract:<br/>Non-technical Description:<br/>The United States has the highest Gross Domestic Product in the world because it has been the leader in technological breakthroughs. Relatively recent advances such as the personal computer, the World Wide Web, the cell phone, high definition television, the forth-coming autonomous car and artificial intelligence, all have roots in government funded research and development. The meteoric growth in these areas has been made possible by the rapid advances in semiconductor capability for both electronics and photonics. To access new areas for growth, there is now a need to develop flexible, faster, thinner, and more power efficient semiconductor materials with new capability. This aim has led to the so-called van der Waals bonded materials, which are materials that can be peeled, layer by layer, down to the thickness of a single 2-dimensional (2D) atomic layer. Such materials promise high speed, greater power efficiency, flexibility, and novel electro-optic properties not found in materials utilized thus far. Thus, this research aims to study their material properties with a view towards applications. <br/>The research is to be carried out in the Physics & Astronomy Department of Georgia State University [GSU], one of the most diverse universities in the nation. The undergraduate Science, Technology, Engineering and Mathematics (STEM) educational component of this proposal aims to translate the abilities of general university students from historically underrepresented groups and women in STEM fields, into the pursuit of a career path in a STEM field, by providing them early exposure to a supportive, confidence building, research experience through mini-science projects in the 2D materials area. Such education/training provided in a southern urban inner-city academic institution in downtown Atlanta, Georgia, will help to add underrepresented sections of society to the nation's science and technology skill base for the electronics, photonics, defense, and wireless communications industries. <br/>Technical Description:<br/>Single atomic layers of bulk van der Waals bonded crystals, and stacks built up by van der Waals epitaxy including a number of single atomic layers with differing electronic, optical, spin, and superconducting properties, offer the possibility of obtaining new physical properties not available in existing bulk materials' properties that can be utilized to address outstanding technological problems in low power and flexible electronics, sensing, and photonics. Thus, this research will experimentally examine the magneto-optoelectronic response under steady state photo-excitation of 2D atomic-layered materials including mono-layer and bilayer graphene, atomically thin hexagonal boron nitride (h-BN), mono- and bilayer-molybdenum disulfide (MoS2), and other transition metal-dichalcogenides. A research team consisting of graduate students and a postdoc, with help from undergraduates participating in mini science projects, will build up 2D atomic-layered crystals by van der Waals epitaxy; fabricate devices by electron beam lithography, plasma etch, and metallization; and examine the properties of electrically contacted and non-contacted devices in the presence of a magnetic field under microwave, mm-wave, and terahertz photo-excitation. Here, some specific problems of interest include the mm-wave magneto-response of graphene, the electric field effect on photoresponse in h-BN encapsulated graphene and MoS2, and the study of the spin properties in graphene across the neutrality point. Such studies are expected to provide insight into the electronic structure of 2D materials, their photo response, spin-g-factors, spin lifetimes, and the dependence of induced bandgaps on applied electric and magnetic fields - attributes that would identify the suitability of such systems for various desirable applications.  Potentially transformative results could include the observation of novel radiation-induced magnetoresistance oscillations in graphene, the realization and measurement of long spin lifetimes in h-BN encapsulated graphene, and the measurement of bandgaps in electric field biased bilayer h-BN encapsulated graphene or MoS2 or other transition metal dichalcogenides in the small bandgap limit."
"1727894","SNM: Manufacturing Autonomy for Directed Evolution of Materials (MADE-Materials) for Robust, Scalable Nanomanufacturing","CMMI","SNM - Scalable NanoManufacturi, NANOMANUFACTURING","09/01/2017","05/21/2018","David Hoelzle","OH","Ohio State University","Standard Grant","Khershed Cooper","08/31/2021","$1,502,521.00","Kira Barton, Max Shtein","hoelzle.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","ENG","025Y, 1788","081E, 083E, 084E, 116E, 9178, 9231, 9251","$0.00","The development and manufacturing of cutting edge materials typically involves time-consuming materials and process design phases, followed by extensive testing of samples to adjust process conditions as the manufacturing scales up from the lab, to pilot plan, to industrial process scale. These distinct steps drive up the cost barrier to introduction of new and improved materials into the industrial pipeline and increase the cost of domestically manufactured advanced nanomaterials. Fortunately, recent developments in numerical modeling, additive manufacturing, and rapid testing of materials suggest that a new approach to material development and nanomanufacturing, where the previously distinct and time-consuming phases could be carried out nearly instantaneously to arrive at optimal material structure as well as process conditions for its manufacture. The focus of this award is to revamp the traditional, open-loop synthesis of nanostructured materials by: 1) using a versatile 3-D printing approach to manufacture these nanomaterials and nanostructures, 2) incorporate material property characterization directly into the printing process, and 3) use an artificial intelligence (AI) algorithm to adjust on the fly process conditions to achieve desired material properties. These concepts and components will be integrated into technical coursework, hands-on research opportunities, and outreach workshops to a broad range of students and the public. The co-PIs plan to leverage existing outreach and educational activities through their group's collaboration with a local museum, as well as curricular and extracurricular activities.<br/> <br/>The approach and framework is an investigation of process modeling, materials synthesis and characterization, and system design to autonomously discover new material configurations and reduce manufacturing defects and uncertainty. This AI framework will ""understand"" process-structure-property relationships, manufacturing constraints, and, importantly, statistical variations in material properties and manufacturing quality. The intellectual merit of this study is the discovery of general nanomanufacturing tools and feedstocks, with supervisory genetic algorithms, that autonomously correct for defects and compensate for innate manufacturing inaccuracies by a search for alternative designs; this is in contrast to standard tools that minimize uncertainty (e.g. environmental controls) or rely on post-fabrication characterization with human intervention. The framework will be tested using nanoscale additive manufacturing (AM) as the fundamental manufacturing tool and nanostructured metamaterials as the application. The paradigm and nanoscale metamaterials made via this approach have far-reaching impacts on scalable nanomanufacturing for integrated systems. The paradigm of systems that autonomously evolve parameters to meet construct specifications is extensible to macroscale additive manufacturing and pharmaceuticals where the process parameter space and chemistries available is vast, and design is not intuitive. Additive nanomanufacturing has the potential to transform metamaterial design by enabling design in 3-dimensions (3D) with multiple materials, creating complex composite metastructures."
"1654187","CAREER: Instrumental divergence and goal-directed choice","BCS","Cognitive Neuroscience","02/15/2017","06/26/2019","Mimi Liljeholm","CA","University of California-Irvine","Continuing Grant","Kurt Thoroughman","01/31/2022","$519,821.00","","m.liljeholm@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","SBE","1699","1045, 1699","$0.00","Theories of instrumental behavior distinguish between goal-directed decisions, motivated by a deliberate consideration of the probability and current utility of their consequences, and habits, which are rigidly and automatically elicited by the stimulus environment based on reinforcement history.  In spite of the far-reaching implications of this distinction, ranging from the structuring of economic policies to the diagnosis and treatment of behavioral pathology, much is still unknown about what factors shape goal-directed decisions and what conditions prompt a transition from goal-directed to habitual action selection.  Generally, while computationally expensive, a goal-directed strategy offers greater levels of flexible instrumental control.  Since subjective utilities often change from one moment to the next, such flexibility is essential for reward maximization and thus may have intrinsic value, potentially serving to motivate and reinforce specific decisions, as well as to justify the general processing cost of goal-directed computations.  A critical requirement for flexible instrumental control, however, is that available action alternatives yield distinct outcome states.  With the support of this NSF Career award, Dr. Mimi Liljeholm is investigating the novel hypotheses that instrumental divergence? the difference between outcome probability distributions associated with alternative actions? can shape choice preferences, induce conditioned reinforcement, and arbitrate between goal-directed and habitual decision strategies.  The objective of this research is to address important gaps in current knowledge about the nature and limits of goal-directed behavior, using a combination of innovative experimental designs, computational modeling and functional magnetic resonance imaging (fMRI).  The educational component of the award provides hands-on training in neuroimaging methods, and in the computational and neural bases of learning and decision-making, at undergraduate and graduate levels.  <br/><br/>All studies use a simple gambling task in which alternative actions yield different colored tokens, each worth a particular amount of money, with various probabilities.  In studies assessing a preference for flexible instrumental control, the relevant choice is between pairs of actions with different levels of instrumental divergence.  Expected monetary pay-offs vary independently of instrumental divergence across options, dissociating the relative contribution of each factor to behavioral choice performance.  Studies investigating the capacity of high instrumental divergence to induce conditioned reinforcement measure changes in the affective valence of visual stimuli based on their association with high versus low instrumental divergence.  Finally, following extended exposure to high versus low instrumental divergence, the degree to which behavior is goal-directed or habitual is assessed using a standardly employed outcome devaluation procedure, in which the monetary amount associated with a particular token color is altered: Goal-directed, but not habitual, decisions are modulated by such changes in the utility of sensory-specific outcomes states.  Neuroimaging data is acquired by scanning participants with fMRI as they perform the task, and a reinforcement learning framework is used to model the intrinsic value of flexible instrumental control (by treating instrumental divergence as a surrogate reward) at behavioral and neural levels. Since many psychiatric disorders are characterized by an abnormal sense of agency, and addiction associated with a rapid transition from goal-directed to habitual action-selection, broader impacts of this project include the potential development of pre-clinical diagnostic assays for early detection of cognitive, affective and behavioral pathology.  The concepts advanced under this project may also help improve the performance of reinforcement learning algorithms, for example by using instrumental divergence to specify new optimization criteria, potentially benefiting medical, industrial and commercial applications of artificial intelligence."
"1721550","Collaborative Research:  Automatic Video Interpretation and Description","DMS","OFFICE OF MULTIDISCIPLINARY AC, CI REUSE, CDS&E-MSS","09/01/2017","08/20/2017","Wing Hung Wong","CA","Stanford University","Standard Grant","Christopher Stark","08/31/2020","$160,000.00","","whwong@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","1253, 6892, 8069","1253, 7433, 8004, 8083, 9263","$0.00","Digital information processing has become an essential part of modern life. It is nowadays often expressed in a form of multimedia, involving videos accompanied with images, captions, and audio. Given the explosive growth of such multimedia data, it is extremely critical that it is accurately summarized and organized for automatic processing in artificial intelligence. One important yet challenging problem is automatic interpretation and summarization of video content, having enormous applications in video advertisements, online video searching and browsing, movie recommendation based on personal preference, and essentially any electronic commerce platform. In this project, the research team plans to develop statistical tools to raise our capacity of processing digital information to respond to a rapid growth of video content in real-world applications. The primary objective is to create a learning system to decipher the meaning of visual expressions as perceived by the audience, with a focus on understanding semantic meaning conveyed by a video.<br/><br/>This project aims to develop methods of automatic video interpretation and description, which understands visual thoughts expressed by a video and generates semantic expressions of the content of a video. Particularly, it will utilize conditional dependence structures of entities as well as between entities and their pertinent actions, in a framework of multi-label and hierarchical classification. It will focus on three areas: 1) entity and action learning, 2) semantic learning for long videos and content-based segmentation, and 3) automatic video description generation, each of which develops techniques in novel ways. In each area, classification will be performed collaboratively based on pairwise conditional label dependencies and temporal dependencies of video frames, characterized by graphical and hidden Markov models. Special effort will be devoted to learning from multiple sources and extracting latent structures corresponding to scenes of a video. The PIs also plan to release the software developed as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the codebase of the software developed. This will allow a wider growth of the  project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1744077","XPS: DSD: Collaborative Research: NeoNexus: The Next-generation Information Processing System across Digital and Neuromorphic Computing Domains","CCF","Exploiting Parallel&Scalabilty","05/01/2017","05/23/2017","Hai Li","NC","Duke University","Standard Grant","Yuanyuan Yang","08/31/2018","$189,019.00","","hai.li@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8283","","$0.00","The explosion of ""big data"" applications imposes severe challenges of data processing speed and scalability on traditional computer systems. The performance of traditional Von Neumann machines is greatly hindered by the increasing performance gap between CPU and memory, motivating the active research on new or alternative computing architectures. By imitating brain's naturally massive parallel architecture with closely coupled memory and computing as well as the unique analog domain operations, neuromorphic computing systems are anticipated to deliver superior speed for applications in image recognition and natural language understanding.<br/><br/>The objective of this research is to establish the fundamental framework and design methodology for NeoNexus -- the next-generation information processing system inspired by human neocortex. It integrates neuromorphic computing accelerators with conventional computing resources by leveraging large scale inference-based data processing and computing acceleration technique atop memristor crossbar arrays. The computation and data exchange will be carefully coordinated and supported by the innovative interconnect architecture, i.e., a hierarchical network-on-chip (NoC). The software-hardware co-design platform will be developed to address the various design challenges. The project will help computer architecture and high-performance computing communities to overcome the ever-increasing technical challenges of traditional architectures and accelerate the fusion between conventional computing technology and cognitive computing model. It will also promote the applications of artificial intelligence technology advances in modern computer architectures and motivate the inventions at both software and hardware levels. Undergraduate and graduate students involved in this research will be trained for the next-generation semiconductor industry workforce."
"1725785","Enhancing Visualization Skills and Conceptual Understanding Using a Drawing-Recognition Tutoring System for Engineering Students","DUE","IUSE","09/01/2017","07/21/2017","Vimal Viswanathan","CA","San Jose State University Foundation","Standard Grant","Abby Ilumoka","08/31/2022","$75,155.00","","vimal.viswanathan@sjsu.edu","210 North Fourth Street","San Jose","CA","951125569","4089241400","EHR","1998","8209, 8238, 8244, 9178","$0.00","Visual and spatial skills are important for scientific and engineering innovation. The ability to represent real systems through accurate yet simplified diagrams is a crucial skill for engineers. A growing concern among engineering educators is that students are losing both the skill of sketching and the ability to produce the free-body diagrams (FBDs) of real systems. These diagrams form the basis for various types of engineering analyses. To address this concern, investigators will redesign and test a cutting-edge educational technology for engineering concepts of statics and mechanics. The sketch-based technology developed at Texas A&M University, called Mechanix, enabled students to hand-draw FBDs, trusses, and other objects using digital ink and provided helpful feedback. The upgraded Mechanix software will include enhanced artificial intelligence (AI) to understand the sketches and provide immediate feedback to the student for individualized tutoring. Instructors will also receive real-time detailed information from the system so they can clarify misconceptions and guide students through problem solutions during classes. This free-hand sketch-based system will focus learning on the fundamental engineering concepts and not on how to use a software tool. These engineering concepts directly relate to a wide variety of designs including bridges, buildings, and trusses that are vital to the infrastructure of the nation's cities. The project will help prepare engineers with improved abilities to develop these designs that are essential in society.<br/><br/>This project will aim to demonstrate the impact of the sketch-recognition based tutoring system on students' motivation and learning outcomes, both generally and among students of diverse backgrounds. The Mechanix system will be converted to an HTML5 format to work on all devices and expand its accessibility for institutions with various technological requirements. Additional AI algorithms will be developed to accommodate more types of statics problems, increased sketch-recognition accuracy and speed, and improved feedback mechanisms for instructors that merge performance information for the students in a class. The upgraded system will be studied in various engineering courses across five different universities, and introduced to over 2,500 students in engineering and related fields. The investigators will utilize controlled classroom experiments, digital data collection, pre/post concept testing, focus groups, and interviews to explore the external validity of Mechanix as a learning tool. Analysis of Covariance will be used to compare outcomes for students using Mechanix and students in control groups. Project outcomes and the Mechanix software will be shared through the project website, professional development workshops, and publications."
"1730146","CompCog:  Collaborative Research:  Learning Visuospatial Reasoning Skills from Experiences","BCS","Science of Learning","08/15/2017","08/16/2017","Linda Smith","IN","Indiana University","Standard Grant","Soo-Siang Lim","07/31/2019","$99,691.00","","smith4@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","SBE","004Y","059Z","$0.00","This project uses methods from artificial intelligence (AI) to better understand how people learn visuospatial reasoning skills like mental rotation, which are a critical ingredient in the development of strong math and science abilities.  In particular, this project proposes a new approach to quantify the learning value contained in different visual experiences, using wearable cameras combined with a new AI system that learns visuospatial reasoning skills from video examples.  Results from this project will not only advance the state of the art in AI but also will enable researchers to measure how valuable different real-world visual experiences are in helping people to learn visuospatial reasoning skills.  For example, certain types of object play activities might be particularly valuable for helping a child to learn certain visuospatial reasoning skills.  Ultimately, this new measurement approach could be used to identify early signs of visuospatial reasoning difficulties in children and could also help in the design of new visuospatial training interventions to boost children?s early math and science development.<br/><br/>The core scientific question that this project aims to answer is: How are visuospatial reasoning skills learned from first-person visual experiences?  This question will be answered through computational experiments with a new AI system---the Mental Imagery Engine (MIME)---that learns visuospatial reasoning skills, like mental rotation, from video examples.  Training data will include first-person, wearable-camera videos from two different settings that are both important for human learning:  unstructured object manipulation by infants and visuospatial training interventions designed for children.  Results from experiments with the MIME AI system will advance the state of the art in both AI and the science of human learning by helping to explain how visuospatial reasoning skills can be learned from visual experiences, and, in particular, how having different kinds of visual experiences can affect the quality of a person?s learning outcomes in different ways."
"1650961","Individuating and comparing objects and events","BCS","Linguistics","07/15/2017","07/13/2017","Alexis Wellwood","IL","Northwestern University","Standard Grant","William Badecker","04/30/2018","$462,064.00","","wellwood@usc.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","SBE","1311","1311, 9178, 9179, 9251","$0.00","The primary mode by which we communicate our ideas about the world and about each other is through language. However, languages aren't merely passive vehicles for the transmission of information. Rather, our sentences carry along with them evidence of the fundamental concepts and categories that we use to understand the world and each other. At one level, this fact about language might seem obvious: a person on one side of an argument might choose to use words that a person on a different side might not, and attending to these different choices tells us something about the people speaking. Such observations about language and language users are studied in fields like sociolinguistics. Yet, our language also reveals more basic truths about us which are not as easily accessible to consciousness, and which are more tied to elements of our common experience. For example, people talk as if there are objects that can be counted (""four spoons"") and substances that cannot (e.g., ""four muds"" is odd), even if arranged in discrete piles. Investigating language at this deeper level can thus reveal basic structures of thought, informing theories of cognition and its development, as well as applications in artificial intelligence.<br/><br/>This project studies parallels in the conceptualization of the basic categories 'object' and 'event' as they are encoded in language and understood by both adults and 4 year olds. Previous research in linguistics and the philosophy of language has uncovered striking formal parallels in the encoding of these categories across nominal and verbal language. The project links this research to what is known about object representation in cognitive science, and uses this link to extend what is known about event representation. Specifically, the project (i) tests whether the observed linguistic parallels correspond to parallels in how adults and children conceptualize minimally-different static and dynamic scenes, (ii) investigates the extent to which representational biases for simple dynamic scenes predicts how adults and children understand quantificational language involving words like ""more"", and (iii) probes the hypothetical universality of the language-cognition linkages by teaching English-speaking adults and children attested, but non-English patterns of event encoding. The results of this project will demonstrate the fruitfulness of connecting formal semantics, philosophy of language, and cognitive science to illuminate the interface between linguistic and non-linguistic perception and cognition."
"1721926","STTR Phase I:  Smart and Fast Atomic Force Microscope for Imaging and Characterization","IIP","STTR Phase I","07/01/2017","05/02/2019","Adam Kollin","MI","RHK Technology Inc","Standard Grant","Benaiah Schrag","05/31/2019","$225,000.00","Darrin Hanna","kollin@rhk-tech.com","1050 E. Maple Road","Troy","MI","480832813","2485775426","ENG","1505","1505, 8034","$0.00","This Small Business Technology Transfer Phase I project represents a change in concept and technical paradigm for Atomic Force Microscopy (AFM) technology, and as such shall significantly impact research and development in both industry and academia.  As discussed in the Technical Merits below, the proposed AFM is fast, smart, and more powerful in terms of imaging and probing local mechanics.  The company has a track record of commercializing AFM controllers that are compatible with most all types of scanners, commercial and home-constructed.  Current AFM users can purchase the new scanner and/or controller to attain the enhanced performance. In addition, new AFM users are also anticipated especially in the areas of nanomaterials, devices and sensors, and multidimensional devices and materials where both high spatial and temporal resolutions are paramount.  Further, the combined high spatial resolution in conjunction with fast speed shall result in immediate advances in the fields of material development, surface coating, nanomaterial and nanodevice inspection and quality control, nanolithography, and tissue engineering.  Based on current market trends, sales are anticipated to reach $25M within the first three years.  The amount is likely higher given the forecasted growth of the global microscopy market. <br/><br/>The intellectual merit of this project includes three cutting edge improvements to current AFM: (1) faster image acquisition speed; (2) automated and rapid feature finding and tracking; (3) 1-2 orders of magnitude of improvement in speed and efficiency in nanomechanical imaging.  The ultra-high speed will be achieved by implementing a novel reconfigurable processor optimized for AFM into a unique hybrid, low-noise controller architecture, which is highly versatile and compatible with various known configurations of AFM microscopes from many different vendors.  In addition, the automatic feature finding and tracking functions will be accomplished using artificial intelligence directly in hardware and a novel scan pattern, completely different from current ?trace-retrace? scanning trajectory in current AFM.  These new and ?smart? approaches further speed up scanning and tracking speed.  Finally, the AFM will be able to produce nanomechanical images with high speed and accuracy using multifrequency spectroscopy. This concept has been proposed, and individual aspects have been demonstrated in isolation through simulations or lab prototypes over the past five years or more.  The faster and more powerful electronic controller shall enable the test and implementation of multifrequency spectroscopy technology in its full potential."
"1736899","Cambridge to Cambridge Competition Support","DGE","GVF - Global Venture Fund, CYBERCORPS: SCHLAR FOR SER","07/01/2017","07/21/2017","Howard Shrobe","MA","Massachusetts Institute of Technology","Standard Grant","Victor P. Piotrowski","06/30/2018","$56,193.00","","hes@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","EHR","054Y, 1668","5946, 5980, 9178, 9179, SMET","$0.00","In 2015, as part of a series of cybersecurity initiatives made public by the U.K. Prime Minister and the U.S. President, the two nations announced that MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) and the University of Cambridge, would organize a special collaborative competition in cybersecurity dubbed Cambridge2Cambridge. The event took place in 2016 at MIT and the two universities agreed to a follow-on event, to be held in the summer of 2017. Cambridge University will host this event at their campus in the U.K. and the event will include several schools from the United States. Thirty-one U.S. students have been selected based on their performance in a preliminary contest including students from MIT, Carnegie-Mellon University, Columbia University, United States Air Force Academy, Worcester Polytechnic Institute, University of Tampa and University of Puerto Rico. All of the U.K. Centers of Cyber Excellence (sponsored by the Government Communications Headquarters, GCHQ) will participate and represent the U.K.<br/><br/>This project promotes international cooperation in cybersecurity, furthering relationships between countries and among students. Several private companies with advanced cybersecurity capabilities, including Boeing, BP, Raytheon, BAE Systems, State Farm Insurance and Akamai, will provide mentors for student participants. They will encourage students to consider careers in a field that is intellectually challenging and of critical importance to U.S. and international security. This award is co-funded by the Global Venture Fund from the NSF's Office of International Science and Engineering."
"1726306","Enhancing Visualization Skills and Conceptual Understanding Using a Drawing-Recognition Tutoring System for Engineering Students","DUE","IUSE","09/01/2017","06/18/2019","Tracy Hammond","TX","Texas A&M Engineering Experiment Station","Standard Grant","John Jackman","08/31/2022","$988,683.00","Kristi Shryock, Stephanie Valentine","hammond@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","EHR","1998","8209, 8244, 9178","$0.00","Visual and spatial skills are important for scientific and engineering innovation. The ability to represent real systems through accurate yet simplified diagrams is a crucial skill for engineers. A growing concern among engineering educators is that students are losing both the skill of sketching and the ability to produce the free-body diagrams (FBDs) of real systems. These diagrams form the basis for various types of engineering analyses. To address this concern, investigators will redesign and test a cutting-edge educational technology for engineering concepts of statics and mechanics. The sketch-based technology developed at Texas A&M University, called Mechanix, enabled students to hand-draw FBDs, trusses, and other objects using digital ink and provided helpful feedback. The upgraded Mechanix software will include enhanced artificial intelligence (AI) to understand the sketches and provide immediate feedback to the student for individualized tutoring. Instructors will also receive real-time detailed information from the system so they can clarify misconceptions and guide students through problem solutions during classes. This free-hand sketch-based system will focus learning on the fundamental engineering concepts and not on how to use a software tool. These engineering concepts directly relate to a wide variety of designs including bridges, buildings, and trusses that are vital to the infrastructure of the nation's cities. The project will help prepare engineers with improved abilities to develop these designs that are essential in society.<br/><br/>This project will aim to demonstrate the impact of the sketch-recognition based tutoring system on students' motivation and learning outcomes, both generally and among students of diverse backgrounds. The Mechanix system will be converted to an HTML5 format to work on all devices and expand its accessibility for institutions with various technological requirements. Additional AI algorithms will be developed to accommodate more types of statics problems, increased sketch-recognition accuracy and speed, and improved feedback mechanisms for instructors that merge performance information for the students in a class. The upgraded system will be studied in various engineering courses across five different universities, and introduced to over 2,500 students in engineering and related fields. The investigators will utilize controlled classroom experiments, digital data collection, pre/post concept testing, focus groups, and interviews to explore the external validity of Mechanix as a learning tool. Analysis of Covariance will be used to compare outcomes for students using Mechanix and students in control groups. Project outcomes and the Mechanix software will be shared through the project website, professional development workshops, and publications."
"1707316","NeuroNex Technology Hub: Multimodal Integrated Neural Technologies (MINT) - Connecting Physiology to Functional Mapping","DBI","Engineering of Biomed Systems, Cross-BIO Activities","09/01/2017","06/24/2020","Euisik Yoon","MI","Regents of the University of Michigan - Ann Arbor","Cooperative Agreement","Reed Beaman","08/31/2021","$6,149,997.00","Gyorgy Buzsaki, James Weiland, Cynthia Chestek, Viviana Gradinaru","esyoon@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","BIO","5345, 7275","8091","$0.00","In order to understand how neural signals propagate to conduct specific functions in behaving animals and how individual neurons are physically connected in the context of behavior, advanced tools should be available at the hands of neuroscientists. The Multimodal Integrated Neural Technologies (MINT) hub aims to develop and provide tools that are able to read from and modulate neurons at multiple sites independently at high spatial and temporal resolutions. The hub will disseminate tools and methods to correlate the recorded cell activity with the structural connection. In this way, the connectivity of active cells can be visualized, labeled, and traced for detailed functional mapping. The mission of the MINT hub is to provide a collection of tools, synergistically developed, integrated, and available to the neuroscience community, to address one theme: connecting neurophysiology and structural analysis with a greater scale and resolution. The synergistic integration of these neurotechnology tools at the MINT Hub would accelerate the rate of discovery in neuroscience. This in turn can be expected to pave the way to improved treatments for neurological disorders and to breakthroughs in artificial intelligence, especially neuromorphic computing. The MINT hub will provide annual training workshops for new users to be familiar with new technologies and able to use them effectively. To achieve sustainability, the hardware tools will be actively marketed to the community and those with sustainable volume will be transitioned to commercialization partners. Importantly, this program will cross-train neuroscience and technology personnel during the course of this program, resulting in preparation of a new generation of multi-disciplinary engineers and scientists.<br/><br/>This hub uniquely combines high-density electrodes, chemical sensing, optical stimulation, and cell labeling. Fiberless high-density optoelectrodes can allow optical stimulation of individual or few neurons with high specificity and selectivity using monolithically integrated micro-LEDs or optical waveguides on multi-shank silicon probes. Carbon microthreads will be used to create advanced arrays that will dramatically increase the ability to record from interconnected neurons and label those cells with high accuracy. Advanced metal alloys will also be used to greatly enhance the signal-to-noise ratio of miniaturized electrodes. The MINT hub will innovate viral vector delivery and tissue clearing in the nervous system and combine these with multispectral labeling for intact cell phenotyping. Furthermore, an open-source software will be developed to improve the accuracy and efficiency of anatomical reconstruction for creating connectivity maps. The MINT hub will validate the developed tools and methods in three in-vivo experiments to exemplify what can be accomplished when the proposed modalities and methods are synergistically integrated. This NeuroTechnology Hub award is co-funded by the Division of Emerging Frontiers within the Directorate for Biological Sciences, and the Division of Chemical, Bioengineering, Environmental & Transport Systems within the Directorate for Engineering as part of the BRAIN Initiative and NSF's Understanding the Brain activities."
"1658436","Collaborative research: Combining models and observations to constrain the marine iron cycle","OCE","Chemical Oceanography","07/01/2017","03/03/2017","Seth John","CA","University of Southern California","Standard Grant","Simone Metz","06/30/2021","$197,957.00","","sethjohn@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","GEO","1670","","$0.00","Tiny marine organisms called phytoplankton play a critical role in Earth's climate, by absorbing carbon dioxide from the atmosphere. In order to grow, these phytoplankton require nutrients that are dissolved in seawater. One of the rarest and most important of these nutrients is iron. Even though it is a critical life-sustaining nutrient, oceanographers still do not know much about how iron gets into the ocean, or how it is removed from seawater. In the past few years, scientists have made many thousands of measurements of the amount of dissolved iron in seawater, in environments ranging from the deep sea, to the Arctic, to the tropical oceans. They found that the amount of iron in seawater varies dramatically from place to place. Can this data tell us about how iron gets into the ocean, and how it is ultimately removed? Yes. In this project, scientists working on making measurements of iron in seawater will come together with scientists who are working on computer models of iron inputs and removal in the ocean. The goal is to work together to create a program that allows our computer models to ""learn"" from the data, much like an Artificial Intelligence program. This program will develop a ""best estimate"" of where and how much iron is coming into the ocean, how long it stays in the ocean, and ultimately how it gets removed. This will lead to a better understanding of how climate change will impact the delivery of iron to the ocean, and how phytoplankton will respond to climate change. With better climate models, society can make more informed decisions about how to respond to climate change. The study will also benefit a future generation of scientists, by training graduate students in a unique collaboration between scientists making seawater measurements, and those using computer models to interpret those measurements. Finally, the project aims to increase the participation of minority and low-income students in STEM (Science, Technology, Engineering, and Mathematics) research, through targeted outreach programs.<br/><br/><br/><br/>Iron (Fe) is an important micronutrient for marine phytoplankton that limits primary productivity over much of the ocean; however, the major fluxes in the marine Fe cycle remain poorly quantified. Ocean models that attempt to synthesize our understanding of Fe biogeochemistry predict widely different Fe inputs to the ocean, and are often unable to capture first-order features of the Fe distribution. The proposed work aims to resolve these problems using data assimilation (inverse) methods to ""teach"" the widely used Biogeochemical Elemental Cycling (BEC) model how to better represent Fe sources, sinks, and cycling processes. This will be achieved by implementing BEC in the efficient Ocean Circulation Inverse Model and expanding it to simulate the cycling of additional tracers that constrain unique aspects of the Fe cycle, including aluminum, thorium, helium and Fe isotopes. In this framework, the inverse model can rapidly explore alternative representations of Fe-cycling processes, guided by new high-quality observations made possible in large part by the GEOTRACES program. The work will be the most concerted effort to date to synthesize these rich datasets into a realistic and mechanistic model of the marine Fe cycle. In addition, it will lead to a stronger consensus on the magnitude of fluxes in the marine Fe budget, and their relative importance in controlling Fe limitation of marine ecosystems, which are areas of active debate. It will guide future observational efforts, by identifying factors that are still poorly constrained, or regions of the ocean where new data will dramatically reduce remaining uncertainties and allow new robust predictions of Fe cycling under future climate change scenarios to be made, ultimately improving climate change predictions. A broader impact of this work on the scientific community will be the development of a fast, portable, and flexible global model of trace element cycling, designed to allow non-modelers to test hypotheses and visualize the effects of different processes on trace metal distributions. The research will also support the training of graduate students, and outreach to low-income and minority students in local school districts."
"1658392","Collaborative research: Combining models and observations to constrain the marine iron cycle","OCE","Chemical Oceanography","07/01/2017","03/03/2017","Timothy DeVries","CA","University of California-Santa Barbara","Standard Grant","Simone Metz","06/30/2020","$274,355.00","","tdevries@geog.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","GEO","1670","","$0.00","Tiny marine organisms called phytoplankton play a critical role in Earth's climate, by absorbing carbon dioxide from the atmosphere. In order to grow, these phytoplankton require nutrients that are dissolved in seawater. One of the rarest and most important of these nutrients is iron. Even though it is a critical life-sustaining nutrient, oceanographers still do not know much about how iron gets into the ocean, or how it is removed from seawater. In the past few years, scientists have made many thousands of measurements of the amount of dissolved iron in seawater, in environments ranging from the deep sea, to the Arctic, to the tropical oceans. They found that the amount of iron in seawater varies dramatically from place to place. Can this data tell us about how iron gets into the ocean, and how it is ultimately removed? Yes. In this project, scientists working on making measurements of iron in seawater will come together with scientists who are working on computer models of iron inputs and removal in the ocean. The goal is to work together to create a program that allows our computer models to ""learn"" from the data, much like an Artificial Intelligence program. This program will develop a ""best estimate"" of where and how much iron is coming into the ocean, how long it stays in the ocean, and ultimately how it gets removed. This will lead to a better understanding of how climate change will impact the delivery of iron to the ocean, and how phytoplankton will respond to climate change. With better climate models, society can make more informed decisions about how to respond to climate change. The study will also benefit a future generation of scientists, by training graduate students in a unique collaboration between scientists making seawater measurements, and those using computer models to interpret those measurements. Finally, the project aims to increase the participation of minority and low-income students in STEM (Science, Technology, Engineering, and Mathematics) research, through targeted outreach programs.<br/><br/><br/><br/>Iron (Fe) is an important micronutrient for marine phytoplankton that limits primary productivity over much of the ocean; however, the major fluxes in the marine Fe cycle remain poorly quantified. Ocean models that attempt to synthesize our understanding of Fe biogeochemistry predict widely different Fe inputs to the ocean, and are often unable to capture first-order features of the Fe distribution. The proposed work aims to resolve these problems using data assimilation (inverse) methods to ""teach"" the widely used Biogeochemical Elemental Cycling (BEC) model how to better represent Fe sources, sinks, and cycling processes. This will be achieved by implementing BEC in the efficient Ocean Circulation Inverse Model and expanding it to simulate the cycling of additional tracers that constrain unique aspects of the Fe cycle, including aluminum, thorium, helium and Fe isotopes. In this framework, the inverse model can rapidly explore alternative representations of Fe-cycling processes, guided by new high-quality observations made possible in large part by the GEOTRACES program. The work will be the most concerted effort to date to synthesize these rich datasets into a realistic and mechanistic model of the marine Fe cycle. In addition, it will lead to a stronger consensus on the magnitude of fluxes in the marine Fe budget, and their relative importance in controlling Fe limitation of marine ecosystems, which are areas of active debate. It will guide future observational efforts, by identifying factors that are still poorly constrained, or regions of the ocean where new data will dramatically reduce remaining uncertainties and allow new robust predictions of Fe cycling under future climate change scenarios to be made, ultimately improving climate change predictions. A broader impact of this work on the scientific community will be the development of a fast, portable, and flexible global model of trace element cycling, designed to allow non-modelers to test hypotheses and visualize the effects of different processes on trace metal distributions. The research will also support the training of graduate students, and outreach to low-income and minority students in local school districts."
"1733545","Workshop: Innovation, Cities, and the Future of Work","SMA","S&CC: Smart & Connected Commun, SCIENCE OF SCIENCE POLICY","06/15/2017","06/07/2017","Iyad Rahwan","MA","Massachusetts Institute of Technology","Standard Grant","Cassidy R. Sugimoto","05/31/2018","$24,750.00","","irahwan@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","033Y, 7626","7556, 7626","$0.00","Artificial intelligence (AI) is expected to significantly change labor markets nationwide, with specific implications for job availability and productivity in urban areas. Urbanization, largely driven by job migration, produces cities which act as innovation centers and economic engines for society. Technological improvements increase efficiency of production, but the rate of changing labor demands resulting from technology may be too fast to maintain urbanization trends. As an example, existing work highlights the prominent role education can play in advancing or broadening career opportunities, but it is not obvious which occupations will remain or be redefined as a result of changing labor demands with the introduction of new technology. Understanding these dynamics will lead to informed policy that promotes preparedness for future labor demands.  Evidence based urban policy can maximize the efficiency gains from AI, while minimizing the detrimental effects on the well-being of the residents of cities, such as net employment loss and increased occupational polarization. <br/><br/>This workshop, which connects researchers specializing in urban innovation, labor economics, the impacts of automation, and urban policy aims to identify new policies targeted at mitigating the detrimental labor market effects from imminent automation technology and identify prudent unanswered questions as future work that may further inform urban policy.  Answering these questions requires understanding systemic trends as identified by the urban physics literature and linking to the causal mechanisms related to automation identified by labor economists. This understanding will inform urban policy that maximizes efficiency gains from new technology while minimizing detrimental labor effects and provide a new understanding of occupational removal or redefinition resulting from technological change. This workshop aims to inform policy aimed at retraining existing workers and preparing new workers for the labor demands in the age of AI."
"1749430","EAGER: Identifying network dynamics promoting memory consolidation during sleep","BCS","Science of Learning","09/01/2017","08/15/2017","Victoria Booth","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Soo-Siang Lim","08/31/2020","$300,000.00","Michal Zochowski, Sara Aton, Geoffrey Murphy","vbooth@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","SBE","004Y","059Z, 7916","$0.00","While the exact physiological function of sleep remains unknown, there is mounting evidence that it plays an important role in the consolidation of long-term memories. In particular, it appears that sleep promotes the consolidation of declarative memories that require a functionally intact hippocampus, including memories of place. In rodents, place-dependent fear memory is promoted by sleep and disrupted by sleep deprivation. Sleep deprivation also disrupts a number biochemical and neurophysiological processes that are thought to be involved in memory consolidation. These studies have led many to suggest that sleep promotes long-term memory consolidation by modulating neural network dynamics and synaptic plasticity within the hippocampus. In experimental studies, the co-PIs have recently identified changes in hippocampal neural network dynamics during sleep that are induced by place-dependent fear learning.  In computational modeling studies, the co-PIs have shown that acetylcholine, a modulatory chemical whose levels vary across sleep states, can change neural network dynamics in a similar way. The proposed projects take a multidisciplinary, multi-scale approach to bridge the gap between experimental and computational results, to identify how effects of acetylcholine on neurons lead to changes in neural network dynamics to promote learning, ultimately leading to learning and memory behavior. While the focus is on fear learning and memory consolidation, the fundamental knowledge of learning-related and sleep-related brain network dynamics gained by the proposed experiments and computations will provide valuable insights into mechanisms for all types of learning.<br/><br/>At present, it is unclear how sleep-related changes in hippocampal network dynamics might promote contextual fear memory  consolidation. The team's recent experimental studies have shown that contextual fear conditioning produces long-lasting, sleep-dependent increases in the stability of hippocampal network functional connectivity patterns. These results, coupled with the team's recent computational studies describing a role for acetylcholine in network-wide activity and synaptic plasticity patterning, have led to the hypothesis that sleep promotes memory consolidation, at least in part, by dynamically shifting patterns in hippocampal neural network activity during naturally-occurring rapid eye movement and slow wave sleep brain states. Sleep-dependent acetylcholine has a primary role in driving these shifts in network activity through its effects on cellular excitability properties.   The proposed projects use behavioral, physiological, and computational approaches to tackle the missing links that will show the hypothesized network mechanisms occur in brain hippocampal networks and participate in fear learning and memory.  Hippocampal acetylcholine levels will be manipulated across wake and sleep states while recording multi-unit activity in hippocampus to quantify changes in spike timing dynamics in the context of fear and subsequent sleep or sleep deprivation. The team has developed a suite of quantitative measures to identify learning-related changes in network dynamics.  In addition, acetylcholine-induced changes in cellular membrane properties that affect network dynamics will be measured in hippocampal pyramidal cell and inhibitory interneuron populations. The results will be used to inform details of biophysical neural network models to identify specific dynamical mechanisms by which acetylcholine-mediated changes in network activity dynamics promote network stability, structural changes and synaptic reorganization associated with learning and consolidation."
"1725659","Enhancing Visualization Skills and Conceptual Understanding Using a Drawing-Recognition Tutoring System for Engineering Students","DUE","IUSE","09/01/2017","07/21/2017","Benjamin Caldwell","TX","LeTourneau University","Standard Grant","John Jackman","08/31/2022","$137,420.00","","BenjaminCaldwell@letu.edu","P O BOX 7001","Longview","TX","756077001","9032333100","EHR","1998","8209, 8244, 9178","$0.00","Visual and spatial skills are important for scientific and engineering innovation. The ability to represent real systems through accurate yet simplified diagrams is a crucial skill for engineers. A growing concern among engineering educators is that students are losing both the skill of sketching and the ability to produce the free-body diagrams (FBDs) of real systems. These diagrams form the basis for various types of engineering analyses. To address this concern, investigators will redesign and test a cutting-edge educational technology for engineering concepts of statics and mechanics. The sketch-based technology developed at Texas A&M University, called Mechanix, enabled students to hand-draw FBDs, trusses, and other objects using digital ink and provided helpful feedback. The upgraded Mechanix software will include enhanced artificial intelligence (AI) to understand the sketches and provide immediate feedback to the student for individualized tutoring. Instructors will also receive real-time detailed information from the system so they can clarify misconceptions and guide students through problem solutions during classes. This free-hand sketch-based system will focus learning on the fundamental engineering concepts and not on how to use a software tool. These engineering concepts directly relate to a wide variety of designs including bridges, buildings, and trusses that are vital to the infrastructure of the nation's cities. The project will help prepare engineers with improved abilities to develop these designs that are essential in society.<br/><br/>This project will aim to demonstrate the impact of the sketch-recognition based tutoring system on students' motivation and learning outcomes, both generally and among students of diverse backgrounds. The Mechanix system will be converted to an HTML5 format to work on all devices and expand its accessibility for institutions with various technological requirements. Additional AI algorithms will be developed to accommodate more types of statics problems, increased sketch-recognition accuracy and speed, and improved feedback mechanisms for instructors that merge performance information for the students in a class. The upgraded system will be studied in various engineering courses across five different universities, and introduced to over 2,500 students in engineering and related fields. The investigators will utilize controlled classroom experiments, digital data collection, pre/post concept testing, focus groups, and interviews to explore the external validity of Mechanix as a learning tool. Analysis of Covariance will be used to compare outcomes for students using Mechanix and students in control groups. Project outcomes and the Mechanix software will be shared through the project website, professional development workshops, and publications."
"1725423","Enhancing Visualization Skills and Conceptual Understanding Using a Drawing-Recognition Tutoring System for Engineering Students","DUE","IUSE","09/01/2017","07/21/2017","Julie Linsey","GA","Georgia Tech Research Corporation","Standard Grant","John Jackman","08/31/2022","$429,108.00","","julie.linsey@me.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","EHR","1998","8209, 8244, 9178","$0.00","Visual and spatial skills are important for scientific and engineering innovation. The ability to represent real systems through accurate yet simplified diagrams is a crucial skill for engineers. A growing concern among engineering educators is that students are losing both the skill of sketching and the ability to produce the free-body diagrams (FBDs) of real systems. These diagrams form the basis for various types of engineering analyses. To address this concern, investigators will redesign and test a cutting-edge educational technology for engineering concepts of statics and mechanics. The sketch-based technology developed at Texas A&M University, called Mechanix, enabled students to hand-draw FBDs, trusses, and other objects using digital ink and provided helpful feedback. The upgraded Mechanix software will include enhanced artificial intelligence (AI) to understand the sketches and provide immediate feedback to the student for individualized tutoring. Instructors will also receive real-time detailed information from the system so they can clarify misconceptions and guide students through problem solutions during classes. This free-hand sketch-based system will focus learning on the fundamental engineering concepts and not on how to use a software tool. These engineering concepts directly relate to a wide variety of designs including bridges, buildings, and trusses that are vital to the infrastructure of the nation's cities. The project will help prepare engineers with improved abilities to develop these designs that are essential in society.<br/><br/>This project will aim to demonstrate the impact of the sketch-recognition based tutoring system on students' motivation and learning outcomes, both generally and among students of diverse backgrounds. The Mechanix system will be converted to an HTML5 format to work on all devices and expand its accessibility for institutions with various technological requirements. Additional AI algorithms will be developed to accommodate more types of statics problems, increased sketch-recognition accuracy and speed, and improved feedback mechanisms for instructors that merge performance information for the students in a class. The upgraded system will be studied in various engineering courses across five different universities, and introduced to over 2,500 students in engineering and related fields. The investigators will utilize controlled classroom experiments, digital data collection, pre/post concept testing, focus groups, and interviews to explore the external validity of Mechanix as a learning tool. Analysis of Covariance will be used to compare outcomes for students using Mechanix and students in control groups. Project outcomes and the Mechanix software will be shared through the project website, professional development workshops, and publications."
"1728370","SNM: Large-area Printing and Integration of Metal Nanowires and Organic Semiconductors for Stretchable Electronics and Sensors","CMMI","SNM - Scalable NanoManufacturi, GOALI-Grnt Opp Acad Lia wIndus","07/01/2017","07/06/2020","Yong Zhu","NC","North Carolina State University","Standard Grant","Khershed Cooper","06/30/2021","$1,341,733.00","Jingyan Dong, Brendan O'Connor","yong_zhu@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","ENG","025Y, 1504","019Z, 081E, 083E, 084E","$0.00","Stretchable electronics and sensors, such as electronic skin, have wide ranging transformative applications in autonomous artificial intelligence (e.g. robots), medical diagnostics, and prosthetic devices capable of providing at least the same level of sensory perception as the biological equivalent. For example, for electronic skin to meet these expectations, a large number of distributed tactile sensors that are able to stretch and conform to curvilinear objects are required. Moreover, electronics that is conformal to human skin and deform in response to human motion requires stretchability. The functional requirements of stretchable electronics and sensors can be met through nano-enabled technologies, but their successful production requires innovation in scalable manufacturing and integration of processing methods. This Scalable NanoManufacturing (SNM) research project aims to investigate a scalable nanomanufacturing approach to fabricate large-area, high-resolution, stretchable electronics and sensor arrays by heterogeneous integration of metal nanowires and organic semiconductors. The research conducted as part of this award is integrated into interdisciplinary education for the involved students, course curriculum, and K-12 outreach. The project strongly encourages underrepresented students to participate in all aspects of the research.<br/><br/>Advancing the scalable nanomanufacturing of stretchable electronic skin requires: (1) stretchable functional materials (conductors, semiconductors), (2) a manufacturing strategy to integrate heterogeneous nanomaterials into a stretchable platform, and (3) optimal device design and integrated operation capabilities in both electronic and mechanical functionality. To meet these needs, the research investigates metal nanowires and polymer semiconductor material platforms. Stretchable conductors are achieved through forming nanowire-elastomer composites. Printing metal nanowires in large scale with high resolution is challenging. Here nanowire processing focuses on the fundamental understanding of the nanowire ink properties, ink-substrate interactions, and exploration of methods such as gravure and electrohydrodynamic printing. Approaches to achieve high-performance intrinsically stretchable polymer semiconductors are investigated with a focus on blending conjugated polymers with a secondary polymer matrix. Polymer semiconductor processing focuses on a combination of solution casting and advanced transfer printing methods. Device architecture and integrated manufacturing strategies are optimized to achieve high performance electronic-skin."
"1729720","The Development of Relational Processing in Infancy","BCS","Science of Learning, DS -Developmental Sciences","08/15/2017","08/22/2017","Susan Hespos","IL","Northwestern University","Standard Grant","Soo-Siang Lim","07/31/2021","$596,080.00","Kenneth Forbus, Dedre Gentner","hespos@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","SBE","004Y, 1698","059Z, 1698","$0.00","Analogical ability is the ability to make relational comparisons between objects, events, or ideas, and to see common relational patterns across them.  It is a cornerstone of higher reasoning ability, and is essential for learning mathematics and science.  This project  investigates the nature of this ability and how it develops in infants, tracing its development over the first year of life.  Delineating the conditions that promote relational learning in young infants, will lead to insights into how best to promote relational learning in children and in adults who show lags in abstract learning. One result of the proposed studies will be a set of methods and tools that can be used by teachers and caregivers to support relational learning. For example, this research can serve as a springboard for developing targeted interventions for young children diagnosed with language delay, as well as those diagnosed with autistic spectrum disorders.  Another result will be a better understanding of how to build artificial intelligence systems that learn more like people, with far less data than today's systems require.<br/><br/>The starting point for this proposal is a recent demonstration that 7- and 9-month-old infants can form abstract same and different relations, and apply them to new objects.  The preliminary studies suggest that even 3-month-old infants are capable of relational learning; however, they are highly vulnerable to distraction by the interestingness of the objects in the pairs.  The new research will use four series of experiments to trace infants' ability to learn abstract relations.  The first will examine the processes that promote relational learning in 3-month-olds.  The second will investigate the conditions that support spontaneous comparison and learning in 7- and 9-month-olds.  The third series will test how language influences relational learning- specifically, whether naming relations can improve learning, and whether naming objects can impede relational learning.  The fourth series of experiments will investigate the generalizability of these effects by testing a variety of abstract relations.  Computational modeling of the learning patterns found in our studies will provide complementary insights on these processes. Taken together, these studies will reveal information critical to understanding analogical processes and the origin and evolution of higher-order cognition."
"1658042","Collaborative Research: Combining models and observations to constrain the marine iron cycle","OCE","Chemical Oceanography","07/01/2017","03/03/2017","Thomas Weber","NY","University of Rochester","Standard Grant","Simone Metz","06/30/2021","$221,390.00","","t.weber@rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","GEO","1670","","$0.00","Tiny marine organisms called phytoplankton play a critical role in Earth's climate, by absorbing carbon dioxide from the atmosphere. In order to grow, these phytoplankton require nutrients that are dissolved in seawater. One of the rarest and most important of these nutrients is iron. Even though it is a critical life-sustaining nutrient, oceanographers still do not know much about how iron gets into the ocean, or how it is removed from seawater. In the past few years, scientists have made many thousands of measurements of the amount of dissolved iron in seawater, in environments ranging from the deep sea, to the Arctic, to the tropical oceans. They found that the amount of iron in seawater varies dramatically from place to place. Can this data tell us about how iron gets into the ocean, and how it is ultimately removed? Yes. In this project, scientists working on making measurements of iron in seawater will come together with scientists who are working on computer models of iron inputs and removal in the ocean. The goal is to work together to create a program that allows our computer models to ""learn"" from the data, much like an Artificial Intelligence program. This program will develop a ""best estimate"" of where and how much iron is coming into the ocean, how long it stays in the ocean, and ultimately how it gets removed. This will lead to a better understanding of how climate change will impact the delivery of iron to the ocean, and how phytoplankton will respond to climate change. With better climate models, society can make more informed decisions about how to respond to climate change. The study will also benefit a future generation of scientists, by training graduate students in a unique collaboration between scientists making seawater measurements, and those using computer models to interpret those measurements. Finally, the project aims to increase the participation of minority and low-income students in STEM (Science, Technology, Engineering, and Mathematics) research, through targeted outreach programs.<br/><br/><br/><br/>Iron (Fe) is an important micronutrient for marine phytoplankton that limits primary productivity over much of the ocean; however, the major fluxes in the marine Fe cycle remain poorly quantified. Ocean models that attempt to synthesize our understanding of Fe biogeochemistry predict widely different Fe inputs to the ocean, and are often unable to capture first-order features of the Fe distribution. The proposed work aims to resolve these problems using data assimilation (inverse) methods to ""teach"" the widely used Biogeochemical Elemental Cycling (BEC) model how to better represent Fe sources, sinks, and cycling processes. This will be achieved by implementing BEC in the efficient Ocean Circulation Inverse Model and expanding it to simulate the cycling of additional tracers that constrain unique aspects of the Fe cycle, including aluminum, thorium, helium and Fe isotopes. In this framework, the inverse model can rapidly explore alternative representations of Fe-cycling processes, guided by new high-quality observations made possible in large part by the GEOTRACES program. The work will be the most concerted effort to date to synthesize these rich datasets into a realistic and mechanistic model of the marine Fe cycle. In addition, it will lead to a stronger consensus on the magnitude of fluxes in the marine Fe budget, and their relative importance in controlling Fe limitation of marine ecosystems, which are areas of active debate. It will guide future observational efforts, by identifying factors that are still poorly constrained, or regions of the ocean where new data will dramatically reduce remaining uncertainties and allow new robust predictions of Fe cycling under future climate change scenarios to be made, ultimately improving climate change predictions. A broader impact of this work on the scientific community will be the development of a fast, portable, and flexible global model of trace element cycling, designed to allow non-modelers to test hypotheses and visualize the effects of different processes on trace metal distributions. The research will also support the training of graduate students, and outreach to low-income and minority students in local school districts."
"1721445","Collaborative Research: Automatic Video Interpretation and Description","DMS","OFFICE OF MULTIDISCIPLINARY AC, CI REUSE, CDS&E-MSS","09/01/2017","08/20/2017","Yunzhang Zhu","OH","Ohio State University","Standard Grant","Christopher Stark","08/31/2021","$79,999.00","","Zhu.219@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1253, 6892, 8069","1253, 7433, 8004, 8083, 9263","$0.00","Digital information processing has become an essential part of modern life. It is nowadays often expressed in a form of multimedia, involving videos accompanied with images, captions, and audio. Given the explosive growth of such multimedia data, it is extremely critical that it is accurately summarized and organized for automatic processing in artificial intelligence. One important yet challenging problem is automatic interpretation and summarization of video content, having enormous applications in video advertisements, online video searching and browsing, movie recommendation based on personal preference, and essentially any electronic commerce platform. In this project, the research team plans to develop statistical tools to raise our capacity of processing digital information to respond to a rapid growth of video content in real-world applications. The primary objective is to create a learning system to decipher the meaning of visual expressions as perceived by the audience, with a focus on understanding semantic meaning conveyed by a video.<br/><br/>This project aims to develop methods of automatic video interpretation and description, which understands visual thoughts expressed by a video and generates semantic expressions of the content of a video. Particularly, it will utilize conditional dependence structures of entities as well as between entities and their pertinent actions, in a framework of multi-label and hierarchical classification. It will focus on three areas: 1) entity and action learning, 2) semantic learning for long videos and content-based segmentation, and 3) automatic video description generation, each of which develops techniques in novel ways. In each area, classification will be performed collaboratively based on pairwise conditional label dependencies and temporal dependencies of video frames, characterized by graphical and hidden Markov models. Special effort will be devoted to learning from multiple sources and extracting latent structures corresponding to scenes of a video. The PIs also plan to release the software developed as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the codebase of the software developed. This will allow a wider growth of the  project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1726377","MRI: Development of Monitors for Alaskan and Canadian Auroral Weather in Space (MACAWS)","AGS","Major Research Instrumentation, AERONOMY","09/15/2017","11/26/2018","Anthea Coster","MA","Northeast Radio Observatory Corp","Standard Grant","Carrie E. Black","08/31/2021","$798,287.00","","acoster@haystack.mit.edu","77 Massachusetts Ave","Cambridge","MA","021394307","6172531975","GEO","1189, 1521","1189, 1521, 4444","$0.00","This Major Research Instrumentation development award is for the creation of a network of ground-based receivers that can use satellite navigation signals to provide crucial information about the Earth's ionosphere.  This network will include 35 sites in Alaska and Canada, bringing additional measurements to a data-sparse region.  The development activities also include making the system dynamic, adaptable, and autonomous.  Improving the amount and quality of data will help to answer many questions about the basic ionospheric processes, which could lead to improvements in the robustness of satellite navigation systems.  Many of the receivers will also be placed at schools, providing an educational benefit to the students.<br/><br/>The goal of this development award is to provide a ground-based sensor web network that provides both real-time and historical Global Navigation Satellite Systems (GNSS) ionospheric data products for use in geospace science and space weather monitoring in currently unsampled or under-sampled auroral/polar regions in North America.  A sensor web is a dynamic, adaptable, and autonomous network of sensors that can use artificial intelligence to react in real time to information from its instruments.  Thirty-five GNSS receivers will be deployed in Alaska and Canada to retrieve Total Electron Content (TEC) and scintillation statistics.  The project will result in the creation of a unified North American TEC map, development and deployment of triggering algorithms for highly dynamic periods, and the distribution of real-time TEC data to users.  Four specific scientific topics would be addressed: 1) What mechanism is responsible for the formation of polar cap patches?  And how do polar cap patches exit the night-side polar cap?  What is the relationship of the tongue of ionization to polar cap patches? 2) What contribution does the lower atmosphere make to variability in the high-latitude ionosphere? 3) What causes the irregularities that form at the front of the tongue of ionization in the nightside polar ionosphere?  What causes the irregularities that form with the SED plume as observed by SuperDarn? 4) What are the specific auroral and sub-auroral mechanisms that produce GPS scintillations?"
"1659628","REU: From the intertidal to the deep ocean: Monterey Bay Regional Ocean Science REU Program","OCE","EDUCATION/HUMAN RESOURCES,OCE","05/01/2017","08/03/2018","Corey Garza","CA","University Corporation at Monterey Bay","Continuing Grant","Elizabeth Rom","08/31/2020","$514,985.00","","cogarza@csumb.edu","100 Campus Center","seaside","CA","939558001","8315823089","GEO","1690","9250","$0.00","A Research Experience for Undergraduates (REU) program at the California State University, Monterey Bay (CSUMB) campus will bring eleven undergraduates to CSUMB each summer for three years. This program includes a partnership with five other organizations, including the Naval Postgraduate School, Hopkins Marine Station of Stanford University, Moss Landing Marine Laboratory, Monterey Bay Aquarium Research Institute, and Elkhorn Slough National Estuarine Research Reserve, which are all within a short distance of CSUMB. Students will live at CSUMB, but they will conduct research and be mentored by CSUMB faculty and researchers at partner organizations. Research themes include Oceanography, Marine Biology and Ecology, Ocean Engineering, and Marine Geology, with a wide range of topics in each of these themes. Students receive a stipend, housing and travel expenses. Professional development activities include workshops on scientific ethics and Responsible Conduct in Research (RCR), scientific boating, Geographical Information Systems (GIS), graduate school admissions and fellowships, and scientific communication. Many students will be able to present their research results at a conference or a professional meeting following the program. This program will provide unique research and professional development opportunities to a diverse group of thirty-three students and thus supports the national goal of creating a well-trained scientific workforce.<br/><br/>Students who participate in the this program have the opportunity to work with researchers at a variety of marine science research institutions. Potential research topics include, but are not limited to: Trace Metal Analysis; Internal Wave Dynamics; Ocean Analysis and Prediction; Ocean Modeling; Nearshore Processes; Coastal Circulation; Marine Microbiology; Fish Ecology; Population Genetics; Invasion Ecology; Biomechanics; Remotely Operated Vehicles (ROVs); Autonomous Underwater Vehicles (AUVs); Artificial Intelligence; Coastal Erosion; Seafloor Mapping, and Biogeochemical Analysis."
"1740544","I-Corps: Accurate GPS-free Navigation and Localization","IIP","I-Corps","04/01/2017","04/03/2017","Suman Chakravorty","TX","Texas A&M Engineering Experiment Station","Standard Grant","Anita La Salle","03/31/2018","$50,000.00","","schakrav@aero.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to develop autonomous navigation technology that will enable systems to robustly operate in uncertain environments without a Global Positioning System (GPS). The project is a result of a confluence of astronomy, aerospace, computational science and artificial intelligence. Commercialization of this technology has the potential to revolutionize space exploration, self-driving cars, Unmanned Aerial Vehicles (UAVs) and other such systems which need accurate position estimation. A key advantage of this project's technology is enhanced cybersecurity as it does not rely on external signals for navigation. Further, this project will contribute open-source software to the scientific community. It is envisioned that development of a software toolbox that integrates with the popular ROS (Robot Operating System) library will allow researchers to simulate autonomous navigation without GPS.<br/><br/>This I-Corps project is a result of research into the problem of Simultaneous Localization and Mapping (SLAM). In SLAM, a robot is not given prior knowledge of its environment, it must use its sensory data and actions to simultaneously build a map of its environment and position itself within its uncertain map. Competing methods in this area exhibit positioning errors which may be unsuitable for long-term navigation. The work developed here shows that by fusing orientation sensing with short-range sensing, the system attain a simplification of the underlying optimization problem. This allows fast and globally optimal solutions. In this approach, a vehicle uses a camera to track celestial bodies in the sky which allows the vehicle to estimate its orientation in space, this information is fused with short-range sensors such as lasers and cameras which track features in vicinity of the vehicle.  Using the proposed approach, a system can achieve 100x improvement in position error over existing methods."
"1721216","Collaborative Research:  Automatic Video Interpretation and Description","DMS","OFFICE OF MULTIDISCIPLINARY AC, CI REUSE, CDS&E-MSS","09/01/2017","08/20/2017","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Standard Grant","Christopher Stark","08/31/2020","$160,000.00","","xshen@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1253, 6892, 8069","1253, 7433, 8004, 8083, 9263","$0.00","Digital information processing has become an essential part of modern life. It is nowadays often expressed in a form of multimedia, involving videos accompanied with images, captions, and audio. Given the explosive growth of such multimedia data, it is extremely critical that it is accurately summarized and organized for automatic processing in artificial intelligence. One important yet challenging problem is automatic interpretation and summarization of video content, having enormous applications in video advertisements, online video searching and browsing, movie recommendation based on personal preference, and essentially any electronic commerce platform. In this project, the research team plans to develop statistical tools to raise our capacity of processing digital information to respond to a rapid growth of video content in real-world applications. The primary objective is to create a learning system to decipher the meaning of visual expressions as perceived by the audience, with a focus on understanding semantic meaning conveyed by a video.<br/><br/>This project aims to develop methods of automatic video interpretation and description, which understands visual thoughts expressed by a video and generates semantic expressions of the content of a video. Particularly, it will utilize conditional dependence structures of entities as well as between entities and their pertinent actions, in a framework of multi-label and hierarchical classification. It will focus on three areas: 1) entity and action learning, 2) semantic learning for long videos and content-based segmentation, and 3) automatic video description generation, each of which develops techniques in novel ways. In each area, classification will be performed collaboratively based on pairwise conditional label dependencies and temporal dependencies of video frames, characterized by graphical and hidden Markov models. Special effort will be devoted to learning from multiple sources and extracting latent structures corresponding to scenes of a video. The PIs also plan to release the software developed as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the codebase of the software developed. This will allow a wider growth of the  project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1726047","Enhancing Visualization Skills and Conceptual Understanding Using a Drawing-Recognition Tutoring System for Engineering Students","DUE","IUSE","09/01/2017","07/21/2017","Kimberly Talley","TX","Texas State University - San Marcos","Standard Grant","Abby Ilumoka","08/31/2022","$184,034.00","","kgt5@txstate.edu","601 University Drive","San Marcos","TX","786664616","5122452314","EHR","1998","8209, 8238, 8244, 9178","$0.00","Visual and spatial skills are important for scientific and engineering innovation. The ability to represent real systems through accurate yet simplified diagrams is a crucial skill for engineers. A growing concern among engineering educators is that students are losing both the skill of sketching and the ability to produce the free-body diagrams (FBDs) of real systems. These diagrams form the basis for various types of engineering analyses. To address this concern, investigators will redesign and test a cutting-edge educational technology for engineering concepts of statics and mechanics. The sketch-based technology developed at Texas A&M University, called Mechanix, enabled students to hand-draw FBDs, trusses, and other objects using digital ink and provided helpful feedback. The upgraded Mechanix software will include enhanced artificial intelligence (AI) to understand the sketches and provide immediate feedback to the student for individualized tutoring. Instructors will also receive real-time detailed information from the system so they can clarify misconceptions and guide students through problem solutions during classes. This free-hand sketch-based system will focus learning on the fundamental engineering concepts and not on how to use a software tool. These engineering concepts directly relate to a wide variety of designs including bridges, buildings, and trusses that are vital to the infrastructure of the nation's cities. The project will help prepare engineers with improved abilities to develop these designs that are essential in society.<br/><br/>This project will aim to demonstrate the impact of the sketch-recognition based tutoring system on students' motivation and learning outcomes, both generally and among students of diverse backgrounds. The Mechanix system will be converted to an HTML5 format to work on all devices and expand its accessibility for institutions with various technological requirements. Additional AI algorithms will be developed to accommodate more types of statics problems, increased sketch-recognition accuracy and speed, and improved feedback mechanisms for instructors that merge performance information for the students in a class. The upgraded system will be studied in various engineering courses across five different universities, and introduced to over 2,500 students in engineering and related fields. The investigators will utilize controlled classroom experiments, digital data collection, pre/post concept testing, focus groups, and interviews to explore the external validity of Mechanix as a learning tool. Analysis of Covariance will be used to compare outcomes for students using Mechanix and students in control groups. Project outcomes and the Mechanix software will be shared through the project website, professional development workshops, and publications."
"1730044","CompCog:  Collaborative Research:  Learning Visuospatial Reasoning Skills from Experience","BCS","Science of Learning","08/15/2017","08/16/2017","Maithilee Kunda","TN","Vanderbilt University","Standard Grant","Soo-Siang Lim","07/31/2020","$200,000.00","Bethany Rittle-Johnson","mkunda@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","SBE","004Y","059Z","$0.00","This project uses methods from artificial intelligence (AI) to better understand how people learn visuospatial reasoning skills like mental rotation, which are a critical ingredient in the development of strong math and science abilities.  In particular, this project proposes a new approach to quantify the learning value contained in different visual experiences, using wearable cameras combined with a new AI system that learns visuospatial reasoning skills from video examples.  Results from this project will not only advance the state of the art in AI but also will enable researchers to measure how valuable different real-world visual experiences are in helping people to learn visuospatial reasoning skills.  For example, certain types of object play activities might be particularly valuable for helping a child to learn certain visuospatial reasoning skills.  Ultimately, this new measurement approach could be used to identify early signs of visuospatial reasoning difficulties in children and could also help in the design of new visuospatial training interventions to boost children's early math and science development.<br/><br/>The core scientific question that this project aims to answer is: How are visuospatial reasoning skills learned from first-person visual experiences?  This question will be answered through computational experiments with a new AI system---the Mental Imagery Engine (MIME)---that learns visuospatial reasoning skills, like mental rotation, from video examples.  Training data will include first-person, wearable-camera videos from two different settings that are both important for human learning:  unstructured object manipulation by infants and visuospatial training interventions designed for children.  Results from experiments with the MIME AI system will advance the state of the art in both AI and the science of human learning by helping to explain how visuospatial reasoning skills can be learned from visual experiences, and, in particular, how having different kinds of visual experiences can affect the quality of a person's learning outcomes in different ways."
"1721595","SBIR Phase I:  Wearable Technology to Prevent Decompression Sickness Underwater by Continuously Monitoring Bubble Presence in the Bloodstream and Tissues","IIP","SBIR Phase I","07/01/2017","07/10/2017","William Garcia Rodriguez","PR","SIL Technologies LLC","Standard Grant","Henry Ahn","02/28/2019","$225,000.00","","william.garcia@siltechnologies.company","2539 Calle 14","Rincon","PR","006772461","5182217114","ENG","5371","5345, 5371, 8018, 8042, 9150","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to enable the development of a unique wearable scuba-diving device that will eliminate the risk of Decompression Sickness (DCS), by monitoring in real time the formation of nitrogen bubbles in the body of the user while in the dive. The risk to develop DCS while conducting underwater activities occurs during the ascension phase, when the changing pressure may yield the formation of nitrogen bubbles in tissues and body. The presence of bubbles triggers a variety of serious injuries with long term consequences and even death. Any professional and licensed recreational scuba training makes trainees aware about the risks associated to DCS. Divers are instructed to follow ascent rates and safety stops according with criteria established from statistical considerations of empirical data. Nevertheless, DCS is suffered by rule-abiding divers during 3% of the immersions, requiring costly and distressing evacuation and treatment with hyperbaric chambers. The wearable device will alert the scuba-diver before the sickness develops, reducing the risk to suffer DCS, making underwater activities simpler and safer. <br/><br/>The proposed project plans to demonstrate an innovative ultrasonic resonant concept adapted from the acoustic chamber notion. Acoustic chambers are structures which are belted by piezoelectric arrangements which are set to deform expanding and compressing when subjected to oscillating electric voltages. The frequency of oscillations can be set to match the modes of vibration of the media entering in resonance. Under such conditions the setup is extremely sensitive to minor changes in the elastic properties of the system, such as those induced by the presence of compressible bubbles. This project will execute a plan to produce a wearable design of the piezoelectric array. Artificial intelligence as well as conventional methods will be employed to analyze the electrical disturbances, and relate them to the bubbles sizes in real time. The computing, data acquisition and power requirements will be determined. Ultimately, these results will determine the feasibility to integrate all the required components in an autonomous wearable DCS risk detection device by a scuba diver."
"1658278","Collaborative Research: Structural and functional architecture shaping neural tuning within the human posterior superior temporal sulcus","BCS","Cognitive Neuroscience","01/15/2017","01/17/2017","John Pyles","PA","Carnegie-Mellon University","Standard Grant","Kurt Thoroughman","12/31/2020","$224,562.00","Michael Tarr","jpyles@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","1699","1699, 7298","$0.00","Humans are social creatures with extensive neural systems dedicated to the skills required to navigate interactions with others. This includes decoding the actions of others to infer goals and intentions, and planning our own actions that are appropriate for the current context. Brain regions that support these skills are anatomically dispersed in the four lobes of the brain, organized as a network with communication via long-range white matter connections. One key hub of this network is the posterior superior temporal sulcus (pSTS). The work is this proposal will address an important outstanding question: how the long-range connections supporting action understanding are organized, and the nature of the information that is integrated through these connections. This work will combine structural and functional brain imaging to identify anatomical pathways connecting systems supporting action recognition, with particular attention to pathways through the pSTS, and will use computational statistical analyses to characterize the neural information that is carried through those pathways. This problem is of urgent scientific and clinical relevance: Neuroscience increasingly recognizes that brain regions do not function in isolation, but instead reflect the integration of neural signaling from many cortical sources. The work in this proposal seeks to advance brain science by explicitly modeling these sources in a targeted cortical network. The action recognition network holds additional importance to the public, as some neurodevelopmental disorders (such as autism) are linked to atypical development of the pSTS and poor communication within this neural network. Therefore the outcomes from this work may be critical for developing new clinical tools for diagnosis and interventions for these disorders. Implementing the work in this grant will also support the full engagement and promotion of under-represented and first-generation of young scientists training in neuroscientific research. <br/><br/>The problem of how information is communicated and structured within the action recognition network is an important one. Many competing scientific models exist as to the functional specialization of the posterior superior temporal sulcus and connected brain regions within the action recognition network. New empirical data and analytical techniques are required to advance these theoretical models. A key to understanding information structure within the pSTS and the larger action recognition network is to evaluate the sources integrated within the neural signals, which reflect both sensory-driven perceptual analysis of social cues and the top-down goal-directed signals modulate influences. The work in this proposal will combine innovative experimental design with advanced multivariate statistical analyses to extract structure from the rich regional brain activation response, and will decompose the contribution of sensory-driven and top-down signals on neural tuning. At the same time, one must consider where top-down goal-directed signals originate and the structural pathways by which they are transmitted. The work in this proposal is innovative in that it will characterize the network architecture, both structurally and functionally, using a combination of tools rarely implemented despite their clear complementarity."
"1652703","CAREER: DeepMatter: A Scalable and Programmable Embedded Deep Neural Network","CNS","CSR-Computer Systems Research","05/01/2017","06/24/2019","Tinoosh Mohsenin","MD","University of Maryland Baltimore County","Continuing Grant","Erik Brunvand","04/30/2022","$281,242.00","","tinoosh@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7354","1045","$0.00","Deep neural networks (DNNs), modeled loosely after the human brain, have shown tremendous success to accurately interpret sensory data and recognize patterns. However, they have not been explored for current and future low power multi-sensor applications, such as Internet of Things (IoT), wearable health and mobile smart devices. The fundamental problem with embedded exploration is that current DNN models are very complex, making them challenging to deploy in embedded systems with limited hardware resources and power budgets.  <br/><br/>The project investigates novel and transformative methodologies for DNN network modeling, sparsification, and approximation techniques in software, termed DeepMatter. The research develops new architectures to design a programmable domain-specific many-core platform that implements the optimized network and provides performance, scalability, programmability, and power efficiency requirements necessary for embedded DNN implementations. An application program interface (API) will be designed to allow designers to rapidly prototype and deploy the next generation of sophisticated and intelligent applications. For demonstration, five applications including multi-physiological processing for seizure and distress detection, multi-modal assistive device, air quality monitoring and vision-based situational awareness will be evaluated on DeepMatter. <br/><br/>The success of this research project will result in small and energy efficient wearable/mobile computing devices which can perform knowledge extraction and classification on raw data at the sensor without sending massive raw data to the cloud for processing. This can revolutionize several fields including healthcare, transportation, ecology, surveillance, public utilities. Software models, hardware and tools will be available for the research community to prototype and evaluate different applications. This research provides a multidisciplinary platform for educational objectives of developing embedded smart processors and involves middle and high school students and teachers as well as undergraduate and graduate students."
"1725734","SPX: Secure, Highly-Parallel Training of Deep Neural Networks in the Cloud Using General-Purpose  Shared-Memory Platforms","CCF","SPX: Scalable Parallelism in t","09/01/2017","08/30/2017","Josep Torrellas","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yuanyuan Yang","08/31/2020","$500,000.00","Christopher Fletcher","torrellas@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","042Y","026Z","$0.00","Society is beginning to witness an explosion in the use of Deep Neural Networks (DNNs), with major impacts on many facets of human life, including health, finances, family life, and entertainment. To train DNNs, practitioners have preferred to use GPUs and, recently, specialized hardware accelerators.  Despite constituting the bulk of a data center?s compute resources, general-purpose shared-memory multiprocessors have been regarded as unattractive platforms. In this project, the Principal Investigators (PIs) think that these platforms have high potential. Consequently, this project will develop new techniques to dramatically improve shared-memory multiprocessor performance in training DNNs.  Already, shared-memory servers are compelling for several reasons: they can support a high-degree of parallelism, are general-purpose and easy to program, and provide flexible, fine-grain inter-core communication.  However, efficiently using shared-memory servers to train DNNs imposes <br/>significant challenges. First, fine-grain synchronization is still expensive, and latencies are non-trivial. In addition, when DNN training moves to an environment with multiple users sharing the same physical shared-memory platform in the cloud, privacy and integrity become major concerns.<br/><br/>To overcome these challenges, this project will synergistically address architecture and security issues.  On the architecture side, it will augment a highly-parallel shared-memory server with support for synchronization, data movement, data sharing, and DNN sparsity structuring.  On the security side, it will investigate how shared-memory servers create novel privacy and integrity threats (for example, leaking the DNN?s sparse structure and forcing incorrect model generation), and how to defend against those threats.  The project?s broader impact is to help enable ?neural network training for everyone,? by making a ubiquitous and easy-to-program platform a viable and safe target for running these important, emerging workloads."
"1654982","Scholars Award: Explaining Cognition Mechanistically","SES","STS-Sci, Tech & Society","02/15/2017","01/23/2017","Gualtiero Piccinini","MO","University of Missouri-Saint Louis","Standard Grant","Frederick Kronz","01/31/2019","$166,757.00","","piccininig@umsl.edu","ONE UNIVERSITY BLVD","SAINT LOUIS","MO","631214400","3145165897","SBE","7603","1353, 9150","$0.00","General Audience Summary   <br/><br/>This award supports research to develop an account of how cognitive neuroscientists should develop an explanation of cognition. A central goal of the field is to provide such an explanation, but there is no adequate view as to how this should be done. The researcher proposes an interdisciplinary approach; he will provide a mechanistic explanation of cognition that integrates key elements from philosophy, neuroscience, and psychology. Specifically, he proposes to articulate a comprehensive framework of multilevel neurocognitive mechanisms that perform neural computations. The framework will enable scientists and philosophers to compare, evaluate, and integrate theories from different areas of cognitive science. He will make use of the results of his research in his courses in Philosophy of Mind and Philosophy of Cognitive Science, which are enrolled in by both undergraduate and graduate students in philosophy, psychology, and neuroscience. These courses will be redesigned to facilitate the training of students to think and explain cognitive phenomena in terms of mechanisms spanning multiple levels and drawing from different fields. The PI will describe the results of his research in an accessible way for the general public; he will write a series of blog posts explaining the results of his research in clear, accessible terms on philosophyofbrains.com, and he will also rewrite his public lecture on how cognitive neuroscience explains cognition to include the results of this project.  <br/><br/>Technical Summary     <br/><br/>Developing an account of multilevel neurocognitive mechanisms is an important step toward unifying the sciences of the mind. The core of the proposed research consists of three theses. First, levels of organization (organism, nervous system, brain, cortical area, neural network, neuron, etc.) are ontologically on a par, meaning that lower levels are not more fundamental than higher levels, nor vice versa. Second, it is necessary to include the ways in which offline processing increases the representational power of mental models; this thesis contrasts with traditional accounts of mental representation, which focus primarily on mental representations that correlate with the environment in real time. Third, phenomenal consciousness has a functional nature without being (wholly) computational. The account just characterized will build on work in philosophy of science regarding mechanistic explanation, neural computation, and neural representation. The proposed framework will allow researchers in cognitive neuroscience working across different levels of organization to understand how their findings contribute to a unified science of cognition. Understanding the relationship between the mind and the nervous system more deeply has the potential to transform the debate about the place of the mind in the physical world in both science and humanities disciplines."
"1716388","Approximate Message Passing Algorithms and Networks","CCF","Comm & Information Foundations","07/01/2017","06/28/2017","Philip Schniter","OH","Ohio State University","Standard Grant","Phillip Regalia","06/30/2021","$499,570.00","","schniter@ece.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7797","7923, 7936","$0.00","A problem of paramount importance in engineering, science, and medicine is that of recovering information signals from high-dimensional measurements.  This problem manifests in many forms, e.g., reconstructing a high-quality image from a few noisy Fourier projections, determining which features in patient data are most likely associated with a given disease, or classifying which objects are present within an image.  Until recently, the dominant approach to signal recovery was algorithmic.  But nowadays, algorithms are increasingly being replaced by deep neural networks (DNNs), which can learn optimal inference strategies directly from the data.  This project researches algorithmic as well as deep-neural-network (DNN) approaches to high-dimensional signal recovery, leveraging connections between them to make advances in both.<br/><br/>On the algorithmic front, this project investigates the vector approximate message passing (VAMP) algorithm.  Like the original AMP algorithm of Donoho, Maleki, and Montanari, the VAMP algorithm enjoys low complexity and a scalar state-evolution that rigorously and concisely characterizes its behavior.  However, VAMP is applicable to a much larger class of problems than AMP.  On the DNN front, this project investigates DNNs whose architecture is inspired by the processing steps within VAMP.  The resulting DNNs are highly interpretable and, for some simple applications, statistical optimal.  This project aims to develop this VAMP-based DNN design framework to work with more complex applications."
"1656838","Influences of ecological niche on mechanisms of visual pathway maturation","IOS","Cross-BIO Activities, Organization","08/15/2017","07/20/2019","Sarah Pallas","GA","Georgia State University Research Foundation, Inc.","Continuing Grant","Evan Balaban","07/31/2020","$1,000,000.00","","spallas@umass.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","BIO","7275, 7712","1096, 9179","$0.00","The connections between nerve cells in juvenile brain networks are flexible and can be shaped by the environment and experience.  Network stability increases with age, producing both costs and benefits.  Different species have evolved distinct patterns of plasticity reduction.  This project compares developmental changes in visual system plasticity across several mammalian species.  It will reveal how early sensory experience influences the way brain circuits are put together, and how ongoing sensory stimulation maintains the boundary between stability vs. flexibility of neural circuits.  Because the genomes of many mammals have now been sequenced, it is possible to employ genetic manipulations that until recently were only feasible in mice.  Comparative work is necessary to arrive at a more complete understanding of plasticity mechanisms and to determine whether animals with better vision and more reliance on visual perception for survival than mice (the current species of choice) might provide a better animal model for human visual development.  Taken together, the results of the study will provide important mechanistic information about how and why the brain develops and changes with experience, and which animal models are the most relevant for biomedical research.  Understanding the natural processes that control the boundary between plasticity and stability make it possible to manipulate that boundary for human benefit.  Results from this study will also provide basic information about neural network plasticity which may be useful for designing drug and rehabilitative therapies to treat or repair diseases or injuries of visual pathways specifically, and brain pathways in general.<br/><br/>Visual experience during an early critical period is required for normal development of visual cortical (V1) structure and function in carnivores and non-human primates.  Mice are now the preferred animal model for studies of visual pathway development and plasticity.  Visual development and plasticity of mice differs from that of primates in both known and unknown ways that may have important consequences for the generalization of research findings to humans.  Previous visual deprivation studies in hamsters and mice from the P.I.'s lab have shown that early visual experience is not required for normal developmental refinement of receptive fields in visual midbrain and visual cortex.  Instead, vision is necessary only for the maintenance of normal function in visual midbrain (SC) and V1 of adults.  The hypothesis to be tested in this study is that the degree to which a species' visual system development depends on visual experience is influenced by the extent to which vision is important in their ecological niche.  The predicted outcome is that nocturnal species whose young stay in a burrow (such as mice) will be less dependent on vision to shape the development of their visual pathways than diurnal species whose young spend more time above ground.  The timing and level of sensitivity to visual experience will be measured in several rodent species that differ in their daily activity patterns, nesting habits, and ecological niche.  The rodent data will be compared to that from a carnivore species that has traditionally been used in studies of visual system development and plasticity."
"1818674","On Statistical Modeling and Parameter Estimation for High Dimensional Systems","DMS","STATISTICS","09/06/2017","12/14/2017","Faming Liang","IN","Purdue University","Standard Grant","Gabor Szekely","08/31/2019","$120,658.00","","fmliang@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1269","","$0.00","The dramatic improvements in data collection and acquisition technologies over the last decades have enabled scientists to collect massive amounts of high-dimensional data that allow for monitoring and studying of complex systems.  Due to their intrinsic nature, many of the high-dimensional datasets, such as omics and genome-wide association study (GWAS) data, have a much smaller sample size compared to the dimension (referred to as the small-n-large-P problem). Current research on statistical modeling of small-n-large-P data focuses on linear and generalized linear models. However, these approaches are often not adequate for modeling complex systems, and estimation of the model parameters is challenging.  This project addresses two fundamental problems, statistical modeling and parameter estimation, toward a valid statistical analysis of high-dimensional data.  Successful completion of this project will generate hands-on tools for statistical inference of high-dimensional complex systems, which can benefit researchers in many areas of science and technology.  In particular, the proposed applications to biomedical studies will lead to accurate tools for detecting biomarkers associated with disease processes and tailoring optimal therapy for individual patients with complex diseases. The research results will be disseminated to the statistical and biomedical communities, via collaboration, conference presentations, books, and articles to be published in academic journals. The project will also have significant impact on education through the involvement of graduate students in the project, and incorporation of results into undergraduate and graduate courses.  In addition, the R package developed under this project will provide a valuable tool for statistical analysis of high-dimensional data.<br/><br/>The current approach to modeling small-n-large-P data focuses on linear and generalized linear models, and casts the problem as variable selection by imposing a sparsity constraint on parameter values.  Although these models have many advantages, such as simplicity and computational efficiency, estimation of the parameters is still a challenging problem.  While regularization is often used in these situations, it can perform poorly when the sample size is small and the variables are highly correlated.  Two new methods are proposed to address these concerns, namely, Bayesian neural network (BNN) and blockwise coordinate consistency (BCC).  The BNN method works by first fitting the data with a feed-forward neural network, conducting variable selection through network structure selection under a Bayesian framework, and resolving the associated computational difficulty via parallel computing. Compared to existing methods, BNN can lead to much more precise selection of relevant variables and outcome prediction for high-dimensional nonlinear systems.  The BCC method works by maximizing a new objective function, the expectation of the log-likelihood function, using a cyclic algorithm and iteratively finding consistent estimates for each block of parameters conditional on the current estimates of the other parameters.  The BCC method reduces the high-dimensional parameter estimation problem to a series of low-dimensional parameter estimation problems.  The preliminary results indicate that BCC can provide a drastic improvement in both parameter estimation and variable selection over regularization methods. The validity of the proposed methods will be rigorously studied and applied to biomarker discovery, precision medicine, and joint estimation of the regression coefficients and precision matrix for high-dimensional multivariate regression."
"1657596","CRII: RI: Enabling Manipulation of Object Collections via Self-Supervised Robot Learning","IIS","CRII CISE Research Initiation, Robust Intelligence","03/01/2017","06/27/2018","Tucker Hermans","UT","University of Utah","Standard Grant","David Miller","02/28/2019","$183,000.00","","thermans@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","026Y, 7495","7495, 8228, 9251","$0.00","While manipulation of individual objects in cluttered, real-world settings has received substantial attention, the problem of directly manipulating collections of objects has been left unexplored. This project investigates to what extent robots can autonomously manipulate such object collections. This project facilitates autonomous manipulation methods suitable for use in home robotic assistants. Such assistive robots stand to make a substantial impact in increasing the quality of life of older adults and persons with certain degenerative diseases. Additionally, the robot skills investigated in this project are suitable for manipulation in areas damaged in natural or man-made disasters, where building rubble and other debris need to be cleared. The project supports the development and presentation of an interactive robotics lecture for low-income school children, teaching them fundamentals of computer programming.<br/><br/><br/>The research goal of this project is to enable robots to manipulate and reason about groups of objects en masse. The hypothesis of this project is that treating object collections as single entities enables data-efficient, self-supervised learning of contact locations for pushing and grasping grouped objects. This project investigates a novel neural network architecture for self-supervised manipulation learning. The convolutional neural network model takes as input sensory data and a robot hand configuration. The network learns to predict as output a manipulation quality score for the given inputs. When presented with a novel scene, the robot can perform manipulation inference by evaluating the current sensory data, while directly optimizing the manipulation score predicted by the network over different hand configurations. The project supports the development of experimental protocols and the collection of associated data for dissemination to stimulate research activity in manipulation of object collections."
"1714334","SHF:Small:Scalable Spiking Neural Network Enabled by Probabilistic and Non-Volatile Synapses","CCF","Software & Hardware Foundation","09/01/2017","08/30/2017","Lawrence Pileggi","PA","Carnegie-Mellon University","Standard Grant","Sankar Basu","08/31/2020","$450,000.00","Samuel Pagliarini","pileggi@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7945, 8089","$0.00","Scaling of integrated circuit (IC) technology for the past few decades has enabled remarkable advancement of computing speed and power efficiency, as evidenced by the cellphone computing that we hold in our hands today that would have corresponded to room-size super computers not long ago. But as we reach fundamental physical limits for scaling to smaller feature sizes at nanometer scale, massive amounts of data can be stored, and super-fast circuits can process the data, such that now the overall system performance is limited by the bottleneck that forms with transferring the data between the memory and the processor. For this reason, alternative computation models, particularly those based on brain-inspired (neuromorphic) computation, have recently resurged as a potential new computing paradigm for certain classes of computing applications and problems. This requires advancements in devices, circuits and computing architectures, along with creation of the education platform that will allow future engineers and computer scientists to advance and exploit them.<br/><br/>At the core of this proposed work is the use of a novel magnetic device that is combined with traditional integrated circuit technology to enable a scalable and power efficient neuromorphic computer chip. The design will be optimized to handle ""big data"" problems that require extremely power efficient implementations, such as real-time processing of a video stream for a medical imaging application. Such implementations are challenging for various reasons, most notably the storing of values for a large number of artificial synapse weights, and efficient methods to compute random numbers that are needed to make artificial neuron spiking decisions. Carnegie Mellon researchers are focused on addressing these two key challenges in the proposed work."
"1657469","CRII: RI: Joint Models of Language and Context for Robotic Language Acquisition","IIS","CRII CISE Research Initiation","08/01/2017","07/20/2017","Cynthia Matuszek","MD","University of Maryland Baltimore County","Standard Grant","Tatiana Korelsky","07/31/2020","$163,057.00","","cmat@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","026Y","7495, 8228","$0.00","As robots become smaller, less expensive, and more capable, they are able to perform an increasing variety of tasks, leading to revolutionary improvements in domains such as automobile safety and manufacturing. However, their inflexibility makes them hard to deploy in human-centric environments such as homes and schools, where their tasks and environments are constantly changing. Meanwhile, learning to understand language about the physical world is a growing research area in both robotics and natural language processing. The core problem is how the meanings of words are grounded in the noisy, perceptual world in which a robot operates. This project explores how robots can learn about the world from natural language in order to take instructions and learn about their environment naturally and intuitively from people. The ability to follow directions reduces the adoption barrier for robots in domains such as assistive technology, education, and caretaking, where interactions with non-specialists are crucial. Such robots have the potential to ultimately improve autonomy and independence for populations such as aging-in-place elders; for example, a manipulator arm that can learn from a user?s explanation how to handle food or open novel containers would directly affect the independence of persons with dexterity concerns such as advanced arthritis. <br/><br/>This is an exploratory investigation of how linguistic and perceptual models can be expanded during interaction, allowing robots to understand novel language about unanticipated domains. In particular, the focus is on developing new learning approaches that correctly induce joint models of language and perception, building data-driven language models that add new semantic representations over time. The work combines semantic parser learning, which provides a distribution over possible interpretations of language, with perceptual representations of the underlying world. New concepts are added on the fly as new words and new perceptual data are encountered, and a semantically meaningful model can be trained by maximizing the expected likelihood of language and visual components. This integrated approach allows for effective model updates with no explicit labeling of words or percepts. This approach will be combined with experiments on improving learning efficiency by incorporating active learning, leveraging a robot's ability to ask questions about objects in the world."
"1651083","Parsed and Audio-Aligned Corpus of Bilingual Russian Child Speech (BiRCh)","BCS","Linguistics, Robust Intelligence","08/01/2017","07/09/2020","Sophia Malamud","MA","Brandeis University","Standard Grant","Tyler Kendall","05/31/2022","$410,715.00","Nianwen Xue, Irina Dubinina","smalamud@brandeis.edu","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","SBE","1311, 7495","1311, 7495, 9178, 9179, 9251, SMET","$0.00","Language, thought, and culture are intricately connected, but the way this relationship plays out over the lifespan of an individual, especially a bilingual person, is not well understood.  Bilinguals constitute a large portion of the US population and around the world. This project researches the language of bilingual speakers in Russian immigrant communities to gain a better understanding of fundamental properties of linguistic knowledge, language acquisition and maintenance, the nature of language variation and change, and stability of native speaker knowledge.  Studying the language of immigrants is also important, because it will help build understanding and respect for these often stigmatized linguistic practices. The large, open-access database of bilingual and monolingual Russian speech created during the project will allow education policy makers and practitioners to make appropriate decisions concerning bilingual children in American schools and to create educational resources for heritage speakers of Russian who are an invaluable language resource for the country. Ultimately, this database can also help natural language processing applications for Russian, such as enhancing opportunities for cross-cultural communication online, and making new and less available publications quickly accessible through summarization and machine translation.<br/><br/>The project will construct an open-access online database documenting the speech of two types of bilinguals: émigré adults and young bilingual children in Russian-speaking families in the US and Germany, with a control group of monolingual families with small children in Russia.  This first-of-its-kind database will serve as a tool for comparing linguistic behavior across populations and over time, investigating correlations between grammatical, lexical, and sociolinguistic variables. It will contain audio-aligned transcripts and will be annotated for morphology (e.g., ""feminine noun"") and syntax (e.g., ""relative clause""), which will allow researchers to study frequencies of constructions in both the parents' and children's speech.  The database will enable researchers to tease apart several possible causes of the differences between the home language of bilingual children and adults, and the speech of monolinguals: normal processes of language change or the influence of the majority language and culture; incomplete learning or forgetting of the home language; or universal cognitive and linguistic principles."
"1740235","E2CDA: Type I: Collaborative Research: Nanophotonic Neuromorphic Computing","CCF","Energy Efficient Computing: fr","09/15/2017","07/27/2019","Volker Sorger","DC","George Washington University","Continuing Grant","Sankar Basu","08/31/2021","$552,897.00","Tarek El-Ghazawi, Vikram Narayana","sorger@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","015Y","7945","$0.00","With this program, the National Science Foundation has challenged the scientific community to help solve the problem of energy efficient computing. Data centers around the world spend almost equal amount of overhead power in cooling and power conversion for every Watt of computation. As society's appetite for information continues to grow, data centers will eventually consume a significant portion of the world's electricity. Today, the entire information technology industry relies on digital electronics, i.e. electrons carrying digital bits, which impose a barrier for how efficient a computational task can be, regardless of how brilliantly software routines can be engineered. This project will explore the physics of lightwaves to craft a ""photonic"" processor that breaks that efficiency barrier. This interdisciplinary endeavor will attempt to generate foundational science which will support future innovation in the tech industry. To the end of help building the workforce for the future, the investigators will continue their commitments in mentoring graduate and undergraduate students, and support broadening participation of women, minorities and underrepresented groups through STEM outreach, and NSF REU sites.<br/><br/>The approach unites neuromorphic (neural network-inspired) architectures, nanophotonic devices, and emerging fabrication platforms. The technical challenge requires vertical integration of new optical devices, logic units, and control software. It utilizes diverse expertise of faculty members in nanophotonic devices, photonic systems and computer engineering. Specifically, the key points of innovation will include: 1) ultra-efficient electro-optic modulators that will act as neurons at the nanoscale; 2) reconfigurable hardware interconnection fabric on-chip that uses light as information carrier; and 3) creation of control software and benchmarks that are required to orient the project within the broader field of efficient computing. A strong experimental thrust to design, build, and demonstrate these proposed systems will serve to reduce the risk of development of this technology and pave the way for other researchers to join the emerging field of nanophotonic neuromorphic computing."
"1729086","DMREF: Collaborative Research: Predictive Modeling of Polymer-Derived Ceramics: Discovering Methods for the Design and Fabrication of Complex Disordered Solids","DMR","OFFICE OF MULTIDISCIPLINARY AC, DMREF","10/01/2017","08/15/2017","Jinwoo Hwang","OH","Ohio State University","Standard Grant","John Schlueter","09/30/2020","$296,924.00","","hwang.458@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1253, 8292","054Z, 1253, 7433, 8400","$0.00","Non-technical Description: In the broader context of the materials-by-design grand challenge, this project will focus on developing a novel methodology for accurate design and fabrication of complex disordered solids using a combination of advanced computational and experimental techniques. Complex disordered solids are non-crystalline materials for which the fundamental building blocks are typically molecules or molecule fragments, and therefore they have great potential for tunable structure and properties for various applications of great scientific and technological importance. The key feature of our novel approach is to develop an efficient iterative loop that involves simulating the atomic structure of complex disordered solids, subsequently characterizing the resultant structures/properties, and sending the information back to fabrication conditions for further optimization. This new development is significant because it will demonstrate a computation-based design principle for systematically obtaining the growth parameters needed to make complex disordered materials with targeted properties. Ultimately, that ability can be directed to produce materials that are optimized for particular applications. It is envisioned that the results of this project will be transferrable to a wide range of complex disordered material types, growth methods, and structural/functional properties. The complete system is designated as the amorphous materials designer (AMD) program. During the construction of the AMD, students from high school up though Ph.D. graduate school will be trained by the investigators in all aspects of the research including materials simulation, fabrication, and characterization using advanced state-of-the-art methods.<br/><br/>Technical Description: The research will focus on developing an ab initio molecular dynamics (AIMD) and hybrid reverse Monte Carlo (HRMC) simulation algorithm, augmented by ab initio based energy constraints, that couples with experimental input and feedback, using a series of thin-film amorphous preceramic polymers (a-BC:H, a-SiBCN:H, and a-SiCO:H) as suitably complex and technologically relevant case studies. The unique utility of modern solid-state nuclear magnetic resonance techniques to obtain specific bonding and connectivity information and the sensitive medium-range order information available from fluctuation electron microscopy - a specialized technique based on transmission electron microscopy - will be combined with neutron diffraction and more routine physical and electronic structure characterization methods to provide input and constraints for the simulations. The HRMC modeling efforts will be optimized via particle swarm optimization and subsequently used to train an artificial neural network (ANN) that will predictively link the parameters used to simulate a desired material with the growth parameters needed to fabricate said material. Consequently, the investigators expect to substantially advance the state of the art and surmount traditional challenges associated with (1) identifying non-global potential energy minima for materials produced under non-thermodynamic conditions and (2) aligning simulation and growth process timescales. This effort will benefit technology and society by advancing the science of design of complex disordered solids. The novelty of the effort lies in developing the algorithms and rule-sets that will tie together growth, characterization, and simulation, as well as in developing strategies for mapping (not necessarily reproducing) fabrication conditions and desired properties, and it is this that takes the effort from evolutionary to potentially revolutionary. The PIs also plan to release the AMD program as open source and build a user community around it by ensuring that interested researchers are able to contribute to the AMD codebase. This will allow a wider growth of the project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1719538","Sums of Squares Polynomials in Optimization, Combinatorics, and Computer Vision","DMS","COMPUTATIONAL MATHEMATICS, Combinatorics","07/01/2017","06/02/2017","Rekha Thomas","WA","University of Washington","Standard Grant","Leland Jameson","06/30/2021","$250,000.00","","thomas@math.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1271, 7970","9263","$0.00","Many questions in science and engineering can be modeled as questions in polynomial optimization, in which the goal is to find an optimal solution given a set of practical constraints on the model parameters. This field has undergone a revolution in the last two decades by incorporating novel ideas originating from several fields of mathematics and computer science. These advances have made polynomial optimization a viable tool in many applications. An important example is computer vision, for which a key goal is to estimate and characterize the three-dimensional shapes within a scene, given a set of two-dimensional images. This research project tackles such challenges by combining techniques from optimization, computer vision, and combinatorics. Special emphasis is placed on designing efficient and practical computational algorithms. Graduate students will be involved in this cross-disciplinary research project, providing the students with broad training in mathematics that intertwines theory and computation.<br/><br/>This research investigates three topics in the field of polynomial optimization. The first project extends prior work on positive semi-definite representations of polytopes for the creation of a canonical model of realization spaces of polytopes. This research has the potential to initiate a new algebraic viewpoint of polytopes via their slack ideals, simplifying geometric results and settling longstanding questions such as understanding projective uniqueness. For the second project the focus is on algebraic vision, for which the aim is to apply techniques from algebraic geometry and polynomial optimization to questions in computer vision, with potentially important practical application. Concurrently, the study of applied formulations has the potential to inspire new mathematical theories. Research in algebraic vision continues to create strong ties to the computer vision community and will help create an intellectual exchange between mathematics and vision, benefiting both fields and providing a stimulating training environment for mathematics students looking toward careers in industry. The long-term aim of the last project is to develop the computational aspects of polynomial optimization in the presence of symmetry.  Numerous problems in applications come with symmetries, and the ability to exploit this feature often determines whether or not the problem can be solved."
"1715195","RI:  Small:  Collaborative Research:   Seeing Surfaces:  Actionable Surface Properties from Vision","IIS","Robust Intelligence, NRI-National Robotics Initiati","09/01/2017","08/09/2017","Kristin Dana","NJ","Rutgers University New Brunswick","Standard Grant","Jie Yang","08/31/2021","$249,931.00","","kristin.dana@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7495, 8013","7495, 7923","$0.00","This project is to enable computers and robots the capability of estimating actionable, physical properties of surfaces (the feel) from their appearance (the looks). The key idea is to leverage the deeply interwoven relation between radiometric and physical surface characteristics. By learning models that make explicit the physical surface properties encoded in full and partial measurements of radiometric appearance properties, computers can estimate crucial physical properties of real-world surfaces from passive observations with novel camera systems. This project paves the path for integrating these models and estimation algorithms into scene understanding, robotic action planning, and efficient visual sensing. The research results provide a currently missing but fundamental capability to computer vision that benefits a number of applications in areas of computer vision, robotics, and computer graphics. The project provides hands-on research opportunities for both undergraduate and graduate students and are integrated in the PIs' undergraduate and graduate courses taught at Drexel and Rutgers. They are also used as a backdrop for K-12 outreach activities including high school and middle school mentorship programs. The data collection activities provide an ideal platform to expose K-12 students to physics and computer science.<br/><br/>This research investigates the methods to infer actionable surface properties from images and detailed surface reflectance measurements. The research activities are centered on four specific aims: 1) controlled and uncontrolled large-scale data collection of actionable physical properties and appearance measurements of everyday surfaces, 2) derivation of prediction models for deducing physical properties from local surface appearance, 3) integration of global semantic context including object and scene information, and 4) development of efficient appearance capture and its use for novel physics-from-appearance sensing. These research thrusts collectively answer the fundamental question of how computer vision can anticipate the physical properties of a surface without touching it and knowing what it is, laying the foundation for computational vision-for-action."
"1717688","CHS: Small: Collaborative Research: Measuring and Promoting the Quality of Online News Discussions","IIS","HCC-Human-Centered Computing","08/15/2017","06/07/2018","Paul Resnick","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","William Bainbridge","07/31/2020","$464,721.00","Ceren Budak","presnick@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7367","7367, 7923, 9251","$0.00","This project will amplify the efforts of people to bring out the best in other people in online conversations, and will make it easier for people to find high quality online conversations.  There are numerous concerns about the tone and content of online conversations on public affairs at the present time. At its best, everyday online debate can lead people to consider alternative perspectives and even change their minds. This happens in environments where people may disagree, but where they try to inform and convince each other rather than simply yell at each other.  The first goal of the research is to create automated classifiers to measure the quality of everyday online political talk. Classifiers will estimate the quality of online conversations about news articles in public venues such as Twitter, Facebook, Reddit, and the comments sections of news pages. A Conversation Finder tool (a website and a browser extension) will use the automated classifiers to recommend, in real time, venues where particular news articles are being discussed and where the quality scores are high. The second goal of the research is to create a Conversation Coach that helps the general public to improve the quality of conversation spaces they participate in, by helping them craft messages that directly contribute to quality and that indirectly inspire others. It will include a Message Assistant that extracts elements from conversations in order to help people craft messages and a Message Impact Assessor that predicts the likely impact of a draft message on the quality metrics for subsequent conversations.<br/><br/>Quality of online conversations will be measured in terms of a variety of dimensions that communication scholars have articulated as desirable. Training data for the classifiers will be collected from conversation participants in addition to trained coders, and experiments will be conducted to determine the most effective sequence of requests to make of conversation participants in order to maximize motivation to contribute.  Creation of the Conversation Recommender will lead to several intellectual contributions, including: (1) developing computational assists that help human raters achieve high inter-rater reliability; (2) identifying methods to motivate conversation participants to act as raters; (3) architecting neural-network based classifiers that achieve high prediction accuracy when trained using the collected ratings as training data; (4) developing techniques to make the classifiers produce interpretable results (explanations). Creation of the Conversation Coach will lead to two intellectual contributions: (1) identifying parts of conversations that can be automatically extracted and that writers find relevant and useful when composing messages; (2) architecting a predictive model that accurately estimates the impact of messages on subsequent conversation quality."
"1740262","E2CDA: Type I: Collaborative Research: Nanophotonic Neuromorphic Computing","CCF","Energy Efficient Computing: fr","09/15/2017","08/26/2019","Paul Prucnal","NJ","Princeton University","Continuing Grant","Sankar Basu","08/31/2021","$334,035.00","","prucnal@Princeton.EDU","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","015Y","7945","$0.00","With this program, the National Science Foundation has challenged the scientific community to help solve the problem of energy efficient computing. Data centers around the world spend almost equal amount of overhead power in cooling and power conversion for every Watt of computation. As society's appetite for information continues to grow, data centers will eventually consume a significant portion of the world's electricity. Today, the entire information technology industry relies on digital electronics, i.e. electrons carrying digital bits, which impose a barrier for how efficient a computational task can be, regardless of how brilliantly software routines can be engineered. This project will explore the physics of lightwaves to craft a ""photonic"" processor that breaks that efficiency barrier. This interdisciplinary endeavor will attempt to generate foundational science which will support future innovation in the tech industry. To the end of help building the workforce for the future, the investigators will continue their commitments in mentoring graduate and undergraduate students, and support broadening participation of women, minorities and underrepresented groups through STEM outreach, and NSF REU sites.<br/><br/>The approach unites neuromorphic (neural network-inspired) architectures, nanophotonic devices, and emerging fabrication platforms. The technical challenge requires vertical integration of new optical devices, logic units, and control software. It utilizes diverse expertise of faculty members in nanophotonic devices, photonic systems and computer engineering. Specifically, the key points of innovation will include: 1) ultra-efficient electro-optic modulators that will act as neurons at the nanoscale; 2) reconfigurable hardware interconnection fabric on-chip that uses light as information carrier; and 3) creation of control software and benchmarks that are required to orient the project within the broader field of efficient computing. A strong experimental thrust to design, build, and demonstrate these proposed systems will serve to reduce the risk of development of this technology and pave the way for other researchers to join the emerging field of nanophotonic neuromorphic computing."
"1715251","RI:  Small:  Collaborative Research:  Seeing Surfaces:  Actionable Surface Properties from Vision","IIS","Robust Intelligence, NRI-National Robotics Initiati","09/01/2017","08/09/2017","Ko Nishino","PA","Drexel University","Standard Grant","Jie Yang","08/31/2020","$250,000.00","","kon@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7495, 8013","7495, 7923","$0.00","This project is to enable computers and robots the capability of estimating actionable, physical properties of surfaces (the feel) from their appearance (the looks). The key idea is to leverage the deeply interwoven relation between radiometric and physical surface characteristics. By learning models that make explicit the physical surface properties encoded in full and partial measurements of radiometric appearance properties, computers can estimate crucial physical properties of real-world surfaces from passive observations with novel camera systems. This project paves the path for integrating these models and estimation algorithms into scene understanding, robotic action planning, and efficient visual sensing. The research results provide a currently missing but fundamental capability to computer vision that benefits a number of applications in areas of computer vision, robotics, and computer graphics. The project provides hands-on research opportunities for both undergraduate and graduate students and are integrated in the PIs' undergraduate and graduate courses taught at Drexel and Rutgers. They are also used as a backdrop for K-12 outreach activities including high school and middle school mentorship programs. The data collection activities provide an ideal platform to expose K-12 students to physics and computer science.<br/><br/>This research investigates the methods to infer actionable surface properties from images and detailed surface reflectance measurements. The research activities are centered on four specific aims: 1) controlled and uncontrolled large-scale data collection of actionable physical properties and appearance measurements of everyday surfaces, 2) derivation of prediction models for deducing physical properties from local surface appearance, 3) integration of global semantic context including object and scene information, and 4) development of efficient appearance capture and its use for novel physics-from-appearance sensing. These research thrusts collectively answer the fundamental question of how computer vision can anticipate the physical properties of a surface without touching it and knowing what it is, laying the foundation for computational vision-for-action."
"1742714","Group Travel Grant for the Doctoral Consortium of the IEEE Conference on Computer Vision and Pattern Recognition","IIS","Information Technology Researc, Robust Intelligence","06/01/2017","05/09/2017","Adriana Kovashka","PA","University of Pittsburgh","Standard Grant","Jie Yang","05/31/2018","$20,000.00","","kovashka@cs.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","1640, 7495","1640, 7495, 7556","$0.00","This grant partially supports the participation of students from US institutions in the Doctoral Consortium at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 in Honolulu, Hawaii. CVPR is the premier annual conference in computer vision with over 2000 senior, junior and student participants. It is held in North America and attended by members of the international research community. The broader impacts of this project include supporting the career development of some of the brightest junior researchers in computer vision, contributing to the research community in general by drawing attention to an important aspect of graduate student development, potentially increasing the number of active researchers and educators in STEM, and ensuring that the computer vision community, through its recent graduates, makes fast advances in solving problems that will benefit society as a whole. The Doctoral Consortium aims to have representation from a diverse group of participants in terms of gender, ethnic background, academic institution, and geographic location. <br/><br/>NSF support covers some of the costs for 15 selected US-based graduate students to attend the conference. Participants and recipients of travel support are selected by the 2017 CVPR Doctoral Consortium co-chairs. The Doctoral Consortium provides opportunities for senior PhD students to highlight their work, and to discuss their research and career options with faculty and researchers who have relevant expertise and experience. The opportunity to receive advice on their research work and career plans from experts from different institutions, and with potentially different perspectives, is often not available internally at one's own institution. This year's Doctoral Consortium features a poster session, one-on-one mentoring, and panel discussion. The last of these components was introduced in last year's event and was well-received."
"1744748","A Workshop on Computer Vision Challenges and Opportunities for Privacy and Security","CNS","Secure &Trustworthy Cyberspace","08/01/2017","07/24/2017","David Crandall","IN","Indiana University","Standard Grant","Dan Cosley","07/31/2018","$15,000.00","Apu Kapadia","djcran@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8060","025Z, 7434, 7556","$0.00","This award funds Student Travel Fellowships for US students attending a workshop on the intersection of Security and Privacy with Computer Vision at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). It helps cover the travel expenses and conference registration fees for these students, making it possible for them to attend the workshop and the conference, and to discuss their work or recent advances in the field with other attendees of the workshop or conference.<br/><br/>The workshop brings together privacy, security and computer vision experts to discuss how to better understand the potential threats of computer vision to people's security and privacy, as well as the potential opportunities and applications for enhancing them.  The workshop also helps spark interest among young researchers in the important emerging intersection between privacy and security and computer vision."
"1713989","EAPSI: Implementation of Advanced Computer Vision Techniques to Improve Real Time Imaging of Single Molecule Conformation Changes","OISE","EAPSI","06/01/2017","05/08/2017","Joshua Stuckner","VA","Stuckner                Joshua         A","Fellowship Award","Anne Emig","05/31/2018","$5,400.00","","","","Blacksburg","VA","240603592","","O/D","7316","5921, 5978, 7316","$0.00","The goal of this project is to implement advanced computer vision algorithms for the automatic analysis of ultra-slow motion movies of single molecules. This research will be conducted in collaboration with Dr. Koji Harano and his team at the University of Tokyo, who have for the first time directly observed a single molecule undergoing a particular and important change in shape. The event was captured on video using a powerful microscope equipped with a high speed camera. Understanding the shape, or conformation, changes of molecules is critical to the analysis of complex chemical systems and reactions. This technique produces over 100 gigabytes of data per second of video. Currently, the brute-force method that is used to compile this data into meaningful information requires tremendous computing time and power. Computer vision techniques will vastly increase the speed and accuracy of this analysis and lead more scientific output in this critical area. This project also represents an important exchange of scientific knowledge and techniques between the United States and Japan and will lead to future collaboration on similar projects.<br/><br/>Dr. Harano's team used an advanced imaging technique to observe and measure a molecular ó-bond rotation for the first time using an aberration corrected high resolution transmission electron microscope equipped with a 1600 fps electron counting direct detection camera. The massive datasets generated by this technique must be processed before meaningful analysis can take place. Presently the technique relies on mathematical cross-correlation algorithms for automatic sorting, classification, and alignment of frames; a task far too cumbersome for manual frame by frame sorting at 1600 fps. However, cross-correlation is somewhat of a brute-force approach and the speed and accuracy of this process can be greatly increased by implementing feature matching computer vision algorithms. This project will implement and evaluate advanced computer vision algorithms for the automatic processing of data generated by Dr. Harano's imaging technique.<br/><br/>This award, under the East Asia and Pacific Summer Institutes program, supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science."
"1656998","CRII: RI: Methods for Learning and Recovering Partially Embedded Logical Representations for Question Answering","IIS","CRII CISE Research Initiation, Robust Intelligence","08/15/2017","03/26/2019","Yoav Artzi","NY","Cornell University","Standard Grant","D.  Langendoen","07/31/2020","$191,000.00","","yoav@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","026Y, 7495","7495, 8228, 9251","$0.00","Providing effective access to non-experts to the ever-increasing amount of publicly available information is a challenging open problem. While search has been the dominant mode of access, it does not support complex queries, and provides a poor interface for natural language interaction with the growing number of virtual and embodied agents. The goal of this project is to develop representations and learning methods to enable systems that can respond accurately to complex natural language questions. This work will advance the field of natural language processing and have impact through the development of algorithms, linguistic representations, and applications, including natural language question answering interfaces for large knowledge bases. <br/><br/>A common approach to question answering, commonly known as semantic parsing, is to map questions to expressive logical representations. This is a challenging task that requires deriving the meaning of words, and combining them following the compositional structure of the sentence. Despite increasing research attention, building systems capable of such understanding requires significant expertise, and state-of-the-art systems are limited by the coverage of existing manually-curated knowledge bases, largely failing to benefit from unstructured text. This project proposes an approach that will (a) significantly reduce the engineering effort and expertise required to build question answering systems, (b) generalize beyond curated data to learn to answer complex factoid questions without a knowledge base, and (c) lay the foundations for a new general approach to recover semantic meaning. The key to this approach is the integration of logical and embedded representations of meaning. It is expected that mapping sentences to representations that combine logical and embedded elements will greatly reduce the amount of representation engineering required, enable complex sentence-level reasoning, and allow effective learning from raw text without access to a structured knowledge base."
"1745209","I-Corps: Computer Vision based Pre-Harvest Yield Mapping","IIP","I-Corps","10/01/2017","07/13/2017","Ibrahim Isler","MN","University of Minnesota-Twin Cities","Standard Grant","Anita La Salle","09/30/2018","$50,000.00","","isler@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is the practical deployment of a computer-vision based yield estimation system in fruit orchards.  With this technology, the farmers will be able to obtain a map of the number  and size of fruit across their farms from images recorded by a standard camera. This capability will provide farmers with a useful tool in planning their harvest and sales, as well as in managing the long-term health of their plants.  It  will also potentially reduce the inherent risk in fruit growing, thereby improving the affordability and availability of fresh fruit in the US.  Furthermore, by allowing the adoption of precision agriculture techniques to high value crops, this project may help save water and contribute to reduction of runoff pollution from fertilizer and chemicals.<br/><br/>This I-Corps project addresses the task of customer discovery for applications of the technology of vision-based yield detection and mapping in fruit orchards.  The core of the technology is a set of computer vision algorithms to detect fruit in images under natural farm conditions. This application presents several computer vision task challenges.  Fruit may be occluded by leaves and other fruit. Shadows and specularities make color information unreliable. Also, fruit across frames are matched to avoid double counting. Geometric algorithms are used for estimating fruit diameter. This I-Corps project will allow for customer discovery in this and other market spaces, test the product-market fit for the proposed yield estimation technology, and guide future  technology development in this area."
"1717965","CHS: Small: Collaborative Research: Measuring and Promoting the Quality of Online News Discussions","IIS","HCC-Human-Centered Computing","08/15/2017","06/07/2018","R. Garrett","OH","Ohio State University","Standard Grant","William Bainbridge","07/31/2020","$61,248.00","","garrett.258@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7367","7367, 7923, 9251","$0.00","This project will amplify the efforts of people to bring out the best in other people in online conversations, and will make it easier for people to find high quality online conversations.  There are numerous concerns about the tone and content of online conversations on public affairs at the present time. At its best, everyday online debate can lead people to consider alternative perspectives and even change their minds. This happens in environments where people may disagree, but where they try to inform and convince each other rather than simply yell at each other.  The first goal of the research is to create automated classifiers to measure the quality of everyday online political talk. Classifiers will estimate the quality of online conversations about news articles in public venues such as Twitter, Facebook, Reddit, and the comments sections of news pages. A Conversation Finder tool (a website and a browser extension) will use the automated classifiers to recommend, in real time, venues where particular news articles are being discussed and where the quality scores are high. The second goal of the research is to create a Conversation Coach that helps the general public to improve the quality of conversation spaces they participate in, by helping them craft messages that directly contribute to quality and that indirectly inspire others. It will include a Message Assistant that extracts elements from conversations in order to help people craft messages and a Message Impact Assessor that predicts the likely impact of a draft message on the quality metrics for subsequent conversations.<br/><br/>Quality of online conversations will be measured in terms of a variety of dimensions that communication scholars have articulated as desirable. Training data for the classifiers will be collected from conversation participants in addition to trained coders, and experiments will be conducted to determine the most effective sequence of requests to make of conversation participants in order to maximize motivation to contribute.  Creation of the Conversation Recommender will lead to several intellectual contributions, including: (1) developing computational assists that help human raters achieve high inter-rater reliability; (2) identifying methods to motivate conversation participants to act as raters; (3) architecting neural-network based classifiers that achieve high prediction accuracy when trained using the collected ratings as training data; (4) developing techniques to make the classifiers produce interpretable results (explanations). Creation of the Conversation Coach will lead to two intellectual contributions: (1) identifying parts of conversations that can be automatically extracted and that writers find relevant and useful when composing messages; (2) architecting a predictive model that accurately estimates the impact of messages on subsequent conversation quality."
"1652742","CAREER: Developing an Underspecified Representation for Temporal Information in Text","IIS","Robust Intelligence","03/15/2017","06/15/2020","Anna Rumshisky","MA","University of Massachusetts Lowell","Continuing Grant","D.  Langendoen","02/28/2022","$413,201.00","","arumshisky@gmail.com","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","7495","1045, 7495","$0.00","Despite the recent advances in automated processing of natural language, approaching general human-level understanding of text in many cases still remains challenging.  This CAREER project addresses one such challenge, automatically extracting and understanding the order and timing of events described in natural language text narratives.  It develops a computational framework for representing and extracting temporal information conveyed in text, with the end goal to enable realistic temporal reasoning from text.  It also engages student involvement in computer science research early on, and in particular is designed to attract female students to pursue careers in computer science and related areas.<br/><br/>As distinct from existing approaches, the proposed research assumes underspecification to be an integral property of temporal representation, supported by the notion of a coarse-grained event cluster.  It takes advantage of the micro-structure of narrative text by identifying event clusters and their narrative anchors which, together with default event times and durations, serve to organize the underspecified representation of the timeline.  It also addresses knowledge gaps in the current state-of-the-art in by developing three key components.  First, a novel representational scheme is employed to facilitate simple, intuitive choices for the annotators that minimize cognitive effort and reduce annotation error.  Question answering serves as the target application, with a focus on reading-comprehension questions that require temporal reasoning beyond understanding simple factoids.  Second, novel methods for intrinsic evaluation of annotation consistency are used to address problems with existing evaluation methods, which often produce varied results depending on specific strategies for crediting temporal relations inferred through transitive closure over the temporal relation graph.  The proposed methods rely on representing temporal relations as partially ordered sets and use linear extensions of these partial orders in order to evaluate inter-annotator agreement and system performance.  Finally, a new class of neural network-based models is explored that aim to recover partial order graphs over anchored event clusters.  These models use external memory components for cumulative discourse representation, and allow joint training for identifying coreferent event mentions, grouping the recovered events into roughly-simultaneous event clusters, and establishing typed links between them.  The models incorporate a representation for default event order and timing, as well as argument structure for events and quantitative inference over temporal expressions."
"1708906","Distributionally Robust Control and Incentives with Safety and Risk Constraints","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/01/2017","03/11/2019","Insoon Yang","CA","University of Southern California","Standard Grant","Radhakisan Baheti","07/31/2021","$318,910.00","","insoonya@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","ENG","7607","092E","$0.00","Massive data collected from the Internet-of-Things and cyber-physical systems can have transformative impacts on our society, spanning from personalized medicine to urban infrastructure systems. However, several concerns related to robustness, safety, risk, and reliability have been raised centered on how to incorporate such large-scale data into solving critical decision-making problems, as the data and the estimated statistical models are often inaccurate. Thus, the proposed research will establish a control-theoretic foundation to resolve this issue by allowing distributional errors in the statistical models and by developing control strategies that are robust against the errors. The potential application domains of the proposed distributionally robust control tools include battery management systems, power grids, food supply chains, manufacturing systems and personalized medicine. With the successful implementations of the proposed control tools in such domains, we will be able to improve individual safety and quality of life, and the reliability of data-driven control systems, which would build high confidence in society. The research outcomes in this project will also be used for (i) the USC Chevron Frontiers in Energy Research Summer Camp which is one of our K-12 STEM outreach efforts; (ii) USC Women in Science and Engineering programs that provide hands-on research experiences to undergraduate and (iii) open house and workshops in Viterbi Center for Engineering Diversity to train and recruit educationally-disadvantaged underrepresented students. <br/><br/>The overarching goal of our proposed research is to develop theoretical foundations and computational methods for distributionally robust control problems associated with safety-critical and/or non-cooperative systems that operate with limited information. The proposed tools can contribute to the following three fundamental areas:  <br/>1. Stochastic control theory: The proposed research aims to establish a game theoretical and algorithmic foundation of distributionally robust control methods for nonlinear stochastic systems when faced with ambiguous distributional information about uncertain variables. In particular, we will investigate a duality-based dynamic programming solution to alleviate the infinite-dimensionality issue in the control problem and combine it with (deep) neural network- and occupation measure based methods to systematically adjust computational complexity and solution accuracy.<br/>2. Safety and risk aware control theory: We will extend stochastic reachability analysis methods to cases with imperfect information about the probability distribution of disturbances. This distributionally robust reachability tool will be used to specify the worst-case probability that the system fails to stay in the safe range and the worst-case risk of system loss. Based on the safety and risk specifications, we will then propose a systematic approach to synthesize a safety preserving and risk-aware control law that is robust against disturbance distributional ambiguity. <br/>3. Incentive contract theory: Incentive contracts under moral hazard can be used to coordinate noncooperative sub-systems controlled by local agents in which local control actions and uncertain variables cannot be monitored by a central coordinator. To broaden the applicability of the contracts to engineering and socio-technical problems, we will generalize the theory in two directions: (i) integrating engineering systems with nontrivial dynamics into contracts, and (ii) constructing incentive contracts in a distributionally robust fashion."
"1701107","PFI: AIR-TT: Commercializing a new genre of Intelligent Science Stations for informal and formal learning","IIP","PFI-Partnrships for Innovation, Accelerating Innovation Rsrch","07/15/2017","07/09/2019","Ken Koedinger","PA","Carnegie-Mellon University","Standard Grant","Jesus Soriano Molla","12/31/2020","$216,000.00","Nesra Yannier","Koedinger@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","ENG","1662, 8019","116E, 1662, 8019, 9251","$0.00","This PFI: AIR Technology Translation project aims to build and commercialize a reusable mixed-reality platform that engages children in scientific experimentation and uses computer vision and automated feedback to help them effectively learn scientific principles. The project focuses on translating prior science and technology research that produced the first Intelligent Science Station, called EarthShake.  EarthShake is a mixed-reality educational game in which children discover physical properties of balance by running experiments with block towers on a simulated earthquake table. This project has the potential to have substantial societal, commercial and educational impacts. It aims to impact millions of children and families that spend time in Children's and Science Museums, libraries, schools, and indoor play spaces. It will transform traditional hands-on exhibits and play spaces into intelligent learning environments that foster children's curiosity and improve their learning in an engaging and collaborative way. It will also help to transform the traditional educational system in schools, introducing an interactive, engaging, mixed-reality system that can help students understand the underlying evidence for scientific principles, not just memorize them.  With over 270 Children's and Science Museums in the US alone, over 800 malls with play spaces for children, 100s of indoor play/learning spaces, over 4,000 clubs and after-school programs, 119,487 libraries, 36,767 private and charter elementary schools and 103,460 elementary schools, this project can have a commercial impact in addition to societal and educational benefit.<br/><br/>The system uses computer vision to follow children's progress and provide intelligent guidance in a predict-observe-explain scientific inquiry process. Experiments demonstrate that grades K to 3 children learn much more from EarthShake than from an otherwise identical flatscreen tablet or laptop implementation showing as much as a 5 times greater pre to post increase on tests of prediction, explanation, and stable tower building.  The initial prototype will be translated into a reusable platform for Intelligent Science Stations and demonstrated in scalable development of multiple instances of three games, EarthShake, RaceCars, and BalloonScale. The technical challenges are to generalize the vision algorithm to work across the variety of objects and configurations in these games and create modular hardware components that provide for scale in development, distribution, and new products. This project draws on technical expertise in computer vision/AI, mechatronics, tangible interfaces, physical computing, human/child-computer interaction, and educational technology.<br/><br/>Intelligent Science Stations bridge the advantages of physical and virtual worlds to improve children's science and inquiry learning in an enjoyable and collaborative way and can be commercialized to do so in various settings, indoor play spaces, museums, libraries, and schools. The project contributes to the general robotics challenge by creating an automated intelligent tutoring system operating in the physical 3D world, incorporating sensors and perceptual algorithms to track what learners are doing, actuators to manipulate the scientific apparatus, and an intelligent dialogue system to orchestrate interaction.  It addresses technical challenges of creating a generalizable vision algorithm and a modular physical hardware platform that allows customers to easily switch between different applications. It will create new knowledge about how to develop a general and reusable mixed-reality platform that integrates scientific apparatus including mechatronic components (e.g,. sensors, actuators, servo motors, physical experimental set-up) with an intelligent tracking system (including hardware and software components with an AI vision algorithm)."
"1820693","RI: Small: Collaborative Research: Structured Inference for Low-Level Vision","IIS","Robust Intelligence","11/09/2017","04/13/2018","Ayan Chakrabarti","MO","Washington University","Standard Grant","Jie Yang","06/30/2021","$158,720.00","","ayan@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7495","7495, 7923, 9251","$0.00","Vision is a valuable sensing modality because it is versatile. It lets humans navigate through unfamiliar environments, discover assets, grasp and manipulate tools, react to projectiles, track targets through clutter, interpret body language, and recognize familiar objects and people. This versatility stems from low-level visual processes that somehow produce, from ambiguous retinal measurements, useful intermediate representations of depth, surface orientation, motion, and other intrinsic scene properties. This project establishes a mathematical and computational foundation for similar low-level processing in machines. The key challenge it addresses is how to usefully encode and exploit the fact that, visually, the world exhibits substantial intrinsic structure. By advancing understanding of low-level vision in machines, this project makes progress toward computer vision systems that can compare to vision in humans, in terms of accuracy, reliability, speed, and power-efficiency.<br/><br/>This research revisits low-level vision, and develops a comprehensive framework that possesses a common abstraction for information from different optical cues; the ability to encode scene structure across large regions and at multiple scales; implementation as parallel and distributed processing; and large-scale end-to-end learnability. The project approaches low-level vision as a structured prediction task, with ambiguous local predictions from many overlapping receptive fields being combined to produce a consistent global scene map that spans the visual field. The structured prediction models are different from those used for categorical tasks such as semantic segmentation, because they are specifically designed to accommodate the distinctive requirements and properties of low-level vision: continuous-valued output spaces; ambiguities that may form equiprobable manifolds; extreme scale variations; and global scene maps with higher-order piecewise smoothness. By strengthening the computational foundations of low-level vision, this project strives to enable many kinds of vision systems that are more efficient and more versatile, and it strives to have impacts across the breadth of computer vision."
"1710940","Low-power Neuromorphic Chip Architecture with in situ Deep Learning","ECCS","CCSS-Comms Circuits & Sens Sys","07/15/2017","08/09/2019","Pinaki Mazumder","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Jenshan Lin","06/30/2020","$330,000.00","","mazum@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","7564","105E","$0.00","As ultra-low-power sensing devices are being deployed expansively in all-pervasive wireless computing and consumer electronic products that now profoundly impact our everyday life, enormous amount of data are continuously amassed from these ubiquitous sensors. To exploit these data efficiently for elevating application-specific objectives, built-in intelligences are warranted in these sensor platforms to endow transitions from the traditional rule-based computing paradigm to emerging data-driven computing methods. Deep learning on multi-layered networks of neurons has provided such an opportunity to achieve intelligent computing through learning from the abundant data collected by sensing devices. How to make the current energy-intensive deep learning that mainly runs on clusters of general-purpose central processing units (CPUs) and graphics processing units (GPUs) amenable to various low-power systems remains to be a formidable challenge. This project envisages inventing hardware-friendly learning techniques and designing customized low-power deep learning hardware capable of real-time in-situ learning. The innovative hardware will be implemented on various low-power platforms to significantly accelerate the deployment of the nascent Internet-of-Things (IoT) technology. The low-power deep learning chip will enable energy-constraint systems to sense, process, organize, and utilize the data more intelligently. Useful information associated with the collected data can be extracted and exploited at the front end, thereby reducing the system response time and the energy consumption in wireless communications.<br/><br/>In this research project, energy-efficient spiking neural networks (SNNs) will be used to construct deep learning networks. By leveraging the sparsity in multi-dimensional input data such as image, audio and video, the use of event-triggered SNNs can potentially result in significant energy savings. Building deep neural networks in SNNs also has the advantage of good scalability as CMOS technology advances beyond 10 nm. Through address-event representation, hundreds of sub-SNNs can be interconnected to build large SNNs solving disparate types of large-scale problems. Underlying difficulties owing to lack of effective learning algorithms in SNNs will be addressed by formulating the modulated spike-timing-dependent plasticity (STDP) learning rules. Through these bio-inspired on-line new learning rules, hardware-based SNNs can be designed for the event-triggered computation and deep learning. The neural hardware will be at first prototyped and validated on commercial FPGA boards, before realizing digitally by using nano-scale CMOS technology. Finally, in order to further reduce energy dissipation as warranted in ultra-low-power wearable systems, emergent memristor, i.e., analog resistive memory technology will be co-integrated into CMOS chip to mimic high-density artificial synapses. Variation-resilient architectures and algorithms will be developed to fully exploit the density of the memristor arrays, while tolerating their concomitant conductance variations due to several manufacturing limitations. Further, the integration of research and education will train future engineering workforce encompassing minority and female. Instructive materials developed under this project will be disseminated to research communities and practicing engineers by leveraging the NSF supported nanoHub repository."
"1662029","Integrated Computer Vision System Based on Human Eye Motion","CMMI","Special Initiatives, Dynamics, Control and System D","07/01/2017","03/24/2020","Jun Ueda","GA","Georgia Tech Research Corporation","Standard Grant","Robert Landers","06/30/2021","$399,020.00","","jun.ueda@me.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1642, 7569","030E, 034E, 091Z, 8024, 8086","$0.00","The goal of this project is to build a robotic vision system that mimics human eye motions. In particular, this research will lead to an enhanced vision system that can achieve quick scanning capability (saccadic eye motion) and smooth object following capability (smooth-pursuit) with minimal computational overhead and using an inexpensive generic hardware.  Current state-of-the-art systems require significant computer post-processing and expensive hardware.  This project will lead to a novel camera positioning system (hardware) and real-time vision (software) system that does not require any post-processing and produces ready-to-use images in real-time. To achieve this goal, the project draws upon the biomechanical and cognitive principles of human vision. The innovative methods and algorithms that can produce motions with speed, accuracy, and smoothness to mimic human eye can greatly enhance capabilities of autonomous vehicles used in a wide variety of applications such as agriculture, military, security, and traffic monitoring. The project will integrate research and outreach activities for K-12 and undergraduate students. <br/><br/>This project studies a bio-inspired dynamics-based method for coordinating motion control and image processing where the system can mechanically displace the field of view by a large angle between frames. The concept behind this research is to merge the system dynamics area and image processing area while previous studies focused solely on either mechanical design or image processing.  The method is motivated by the principles of human vision for effective image de-blurring and panoramic image stitching. In coordination with inherently discrete and rapid ocular movements, the developed image processing methods inspired by ocular physiology will mimic saccades and smooth-pursuit in a fast-moving robotic eye.  A piezoelectrically driven robotic camera positioning mechanism will be employed to demonstrate the effectiveness of this ocular physiology-inspired approach. Hardware architecture that can synchronize image processing and real-time motion control will be configured and tested. A panoramic image of a scene will be generated from multiple images acquired during saccades by removing motion blur and stitching images within a single frame rate. The system architecture will be optimized to best coordinate hardware and software for real-time motion control."
"1729227","DMREF: Collaborative Research: Predictive Modeling of Polymer-Derived Ceramics: Discovering Methods for the Design and Fabrication of Complex Disordered Solids","DMR","OFFICE OF MULTIDISCIPLINARY AC, DMREF","10/01/2017","08/15/2017","Paul Rulis","MO","University of Missouri-Kansas City","Standard Grant","John Schlueter","09/30/2020","$791,955.00","Nathan Oyler, Michelle Paquette","rulisp@umkc.edu","5100 Rockhill Road","Kansas City","MO","641102499","8162355839","MPS","1253, 8292","054Z, 1253, 7433, 8004, 8400, 9150","$0.00","Non-technical Description: In the broader context of the materials-by-design grand challenge, this project will focus on developing a novel methodology for accurate design and fabrication of complex disordered solids using a combination of advanced computational and experimental techniques. Complex disordered solids are non-crystalline materials for which the fundamental building blocks are typically molecules or molecule fragments, and therefore they have great potential for tunable structure and properties for various applications of great scientific and technological importance. The key feature of our novel approach is to develop an efficient iterative loop that involves simulating the atomic structure of complex disordered solids, subsequently characterizing the resultant structures/properties, and sending the information back to fabrication conditions for further optimization. This new development is significant because it will demonstrate a computation-based design principle for systematically obtaining the growth parameters needed to make complex disordered materials with targeted properties. Ultimately, that ability can be directed to produce materials that are optimized for particular applications. It is envisioned that the results of this project will be transferrable to a wide range of complex disordered material types, growth methods, and structural/functional properties. The complete system is designated as the amorphous materials designer (AMD) program. During the construction of the AMD, students from high school up though Ph.D. graduate school will be trained by the investigators in all aspects of the research including materials simulation, fabrication, and characterization using advanced state-of-the-art methods.<br/><br/>Technical Description: The research will focus on developing an ab initio molecular dynamics (AIMD) and hybrid reverse Monte Carlo (HRMC) simulation algorithm, augmented by ab initio based energy constraints, that couples with experimental input and feedback, using a series of thin-film amorphous preceramic polymers (a-BC:H, a-SiBCN:H, and a-SiCO:H) as suitably complex and technologically relevant case studies. The unique utility of modern solid-state nuclear magnetic resonance techniques to obtain specific bonding and connectivity information and the sensitive medium-range order information available from fluctuation electron microscopy - a specialized technique based on transmission electron microscopy - will be combined with neutron diffraction and more routine physical and electronic structure characterization methods to provide input and constraints for the simulations. The HRMC modeling efforts will be optimized via particle swarm optimization and subsequently used to train an artificial neural network (ANN) that will predictively link the parameters used to simulate a desired material with the growth parameters needed to fabricate said material. Consequently, the investigators expect to substantially advance the state of the art and surmount traditional challenges associated with (1) identifying non-global potential energy minima for materials produced under non-thermodynamic conditions and (2) aligning simulation and growth process timescales. This effort will benefit technology and society by advancing the science of design of complex disordered solids. The novelty of the effort lies in developing the algorithms and rule-sets that will tie together growth, characterization, and simulation, as well as in developing strategies for mapping (not necessarily reproducing) fabrication conditions and desired properties, and it is this that takes the effort from evolutionary to potentially revolutionary. The PIs also plan to release the AMD program as open source and build a user community around it by ensuring that interested researchers are able to contribute to the AMD codebase. This will allow a wider growth of the project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1729176","DMREF: Collaborative Research: Predictive Modeling of Polymer-Derived Ceramics: Discovering Methods for the Design and Fabrication of Complex Disordered Solids","DMR","OFFICE OF MULTIDISCIPLINARY AC, DMREF","10/01/2017","08/15/2017","Ridwan Sakidja","MO","Missouri State University","Standard Grant","John Schlueter","09/30/2020","$111,121.00","","ridwansakidja@missouristate.edu","901 South National","Springfield","MO","658970027","4178365972","MPS","1253, 8292","054Z, 1253, 7433, 8400, 9150","$0.00","Non-technical Description: In the broader context of the materials-by-design grand challenge, this project will focus on developing a novel methodology for accurate design and fabrication of complex disordered solids using a combination of advanced computational and experimental techniques. Complex disordered solids are non-crystalline materials for which the fundamental building blocks are typically molecules or molecule fragments, and therefore they have great potential for tunable structure and properties for various applications of great scientific and technological importance. The key feature of our novel approach is to develop an efficient iterative loop that involves simulating the atomic structure of complex disordered solids, subsequently characterizing the resultant structures/properties, and sending the information back to fabrication conditions for further optimization. This new development is significant because it will demonstrate a computation-based design principle for systematically obtaining the growth parameters needed to make complex disordered materials with targeted properties. Ultimately, that ability can be directed to produce materials that are optimized for particular applications. It is envisioned that the results of this project will be transferrable to a wide range of complex disordered material types, growth methods, and structural/functional properties. The complete system is designated as the amorphous materials designer (AMD) program. During the construction of the AMD, students from high school up though Ph.D. graduate school will be trained by the investigators in all aspects of the research including materials simulation, fabrication, and characterization using advanced state-of-the-art methods.<br/><br/>Technical Description: The research will focus on developing an ab initio molecular dynamics (AIMD) and hybrid reverse Monte Carlo (HRMC) simulation algorithm, augmented by ab initio based energy constraints, that couples with experimental input and feedback, using a series of thin-film amorphous preceramic polymers (a-BC:H, a-SiBCN:H, and a-SiCO:H) as suitably complex and technologically relevant case studies. The unique utility of modern solid-state nuclear magnetic resonance techniques to obtain specific bonding and connectivity information and the sensitive medium-range order information available from fluctuation electron microscopy - a specialized technique based on transmission electron microscopy - will be combined with neutron diffraction and more routine physical and electronic structure characterization methods to provide input and constraints for the simulations. The HRMC modeling efforts will be optimized via particle swarm optimization and subsequently used to train an artificial neural network (ANN) that will predictively link the parameters used to simulate a desired material with the growth parameters needed to fabricate said material. Consequently, the investigators expect to substantially advance the state of the art and surmount traditional challenges associated with (1) identifying non-global potential energy minima for materials produced under non-thermodynamic conditions and (2) aligning simulation and growth process timescales. This effort will benefit technology and society by advancing the science of design of complex disordered solids. The novelty of the effort lies in developing the algorithms and rule-sets that will tie together growth, characterization, and simulation, as well as in developing strategies for mapping (not necessarily reproducing) fabrication conditions and desired properties, and it is this that takes the effort from evolutionary to potentially revolutionary. The PIs also plan to release the AMD program as open source and build a user community around it by ensuring that interested researchers are able to contribute to the AMD codebase. This will allow a wider growth of the project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1717607","SHF: Small: Enabling Software Engineering Virtual Assistant Technology","CCF","Software & Hardware Foundation","09/01/2017","05/22/2017","Collin McMillan","IN","University of Notre Dame","Standard Grant","Sol Greenspan","08/31/2021","$407,218.00","","collin.mcmillan@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7798","7923, 7944","$0.00","The objective of this research project is to address key barriers in adapting natural language processing techniques to problems in creating virtual assistants for software engineering.  Virtual assistants such as Siri, Cortana, and Alexa are claiming an increasing role in computing for everyday tasks, but multiple barriers prevent existing virtual assistant technology from being applied to software engineering tasks.  The long-term goal of the project is that virtual assistants will improve productivity for software engineers.<br/><br/>This proposal targets two of those barriers: 1) conversation analysis and modeling, and 2) reference expression generation.  The first of these problems, in a nutshell, is that experiments in natural language modeling conversations tend to cover topics with similar outcomes, while conversations about software may have a much wider range of possible outcomes.  The second problem is that much research in natural language processing is focused on how humans refer to physical objects that have attributes that are universally preferred while in contrast, software artifacts tend not to have measurable attributes that people use as descriptions.  The chief broader impact is an application to assistive technology for persons who are visually impaired.  Virtual assistants have the potential to alleviate barriers-to-entry into computing careers faced by visually impaired persons by creating a voice interface for answering software development questions."
"1724237","S&AS: FND: COLLAB: Learning Manipulation Skills Using Deep Reinforcement Learning with Domain Transfer","IIS","S&AS - Smart & Autonomous Syst","09/01/2017","06/25/2019","Kate Saenko","MA","Trustees of Boston University","Standard Grant","Jie Yang","08/31/2020","$316,000.00","","saenko@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","039Y","046Z, 9102, 9251","$0.00","This project develops new methods of using deep reinforcement learning to solve real world robotics problems. The project focuses on robotic manipulation tasks such as grasping, opening doors, helping out in the home, performing repairs aboard Navy ships, etc. The key operation in all of the above is the ability for the robot to reliably manipulate objects, parts, or tools with its hands in order to perform a task. The project leverages deep reinforcement learning: a new approach to robotic learning that is capable of learning both perceptual features and control policies simultaneously. This project could have important benefits for a variety of practical applications including: explosive ordnance disposal for our military, materials handling aboard Navy ships, dexterous robotic assistants for NASA astronauts in space, assistive technologies that could help seniors age in place longer, better capabilities for handling radioactive materials during nuclear cleanup, assistance for ergonomically challenging tasks in manufacturing, and general assistance in the office and the home.<br/><br/>This research investigates novel deep reinforcement learning approaches for robotic grasping and manipulation that work well in previously unseen, unstructured environments and compose end-to-end tasks from simpler sub-task controllers. The research is built on two main results from research team's recent work, the deep learning approach to grasping and domain adaptation methods for deep neural networks. The research is guided by the following three key ideas: 1) learning in simulation and then using domain transfer techniques to adapt the solutions to reality; 2) simplifying learning for visuomotor control by using planning to estimate the value function; and 3) using symbolic task and motion planning to perform end-to-end tasks by sequencing learned controllers and planned arm/hand motions. The research team performs extensive evaluations to ensure that the system is able to perform novel instances of a task, e.g., those in a context that the robot has not seen before."
"1724191","S&AS: FND: COLLAB: Learning Manipulation Skills Using Deep Reinforcement Learning with Domain Transfer","IIS","S&AS - Smart & Autonomous Syst","09/01/2017","08/16/2017","Robert Platt","MA","Northeastern University","Standard Grant","Jie Yang","08/31/2020","$300,000.00","","r.platt@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","039Y","046Z","$0.00","This project develops new methods of using deep reinforcement learning to solve real world robotics problems. The project focuses on robotic manipulation tasks such as grasping, opening doors, helping out in the home, performing repairs aboard Navy ships, etc. The key operation in all of the above is the ability for the robot to reliably manipulate objects, parts, or tools with its hands in order to perform a task. The project leverages deep reinforcement learning: a new approach to robotic learning that is capable of learning both perceptual features and control policies simultaneously. This project could have important benefits for a variety of practical applications including: explosive ordnance disposal for our military, materials handling aboard Navy ships, dexterous robotic assistants for NASA astronauts in space, assistive technologies that could help seniors age in place longer, better capabilities for handling radioactive materials during nuclear cleanup, assistance for ergonomically challenging tasks in manufacturing, and general assistance in the office and the home.<br/><br/>This research investigates novel deep reinforcement learning approaches for robotic grasping and manipulation that work well in previously unseen, unstructured environments and compose end-to-end tasks from simpler sub-task controllers. The research is built on two main results from research team's recent work, the deep learning approach to grasping and domain adaptation methods for deep neural networks. The research is guided by the following three key ideas: 1) learning in simulation and then using domain transfer techniques to adapt the solutions to reality; 2) simplifying learning for visuomotor control by using planning to estimate the value function; and 3) using symbolic task and motion planning to perform end-to-end tasks by sequencing learned controllers and planned arm/hand motions. The research team performs extensive evaluations to ensure that the system is able to perform novel instances of a task, e.g., those in a context that the robot has not seen before."
"1821962","US Ignite: Focus Area 1: Predictable Wireless Networking and Collaborative 3D Reconstruction for Real-Time Augmented Vision","CNS","CISE Research Resources","08/31/2017","02/22/2018","Hongwei Zhang","IA","Iowa State University","Standard Grant","Deepankar Medhi","09/30/2020","$550,371.00","","hongwei@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","2890","","$0.00","Eliminating the line-of-sight constraint of human vision and machine vision, the developed network systems foundations of predictable wireless networked and 3D reconstruction will enable 'see-through vision' which will transform the ways humans and engineered systems interact with environments and thus have far-reaching impact on domains such as road transportation, public safety, and disaster response. This project develops the network systems foundation for a vehicle equipped with sensors and an augmented reality display to indicate the presence of other nearby vehicles hidden by obstacles.  In collaboration with Wayne State University (WSU) police and Ford Research and leveraging the WSU living lab and the OpenXC open-source platform for connected vehicles, the project will take an integrated approach to the research, deployment, and dissemination of the wireless network systems for see-through vision. This project proposes a cross-layer framework for addressing physical-domain uncertainties and the interdependencies between wireless networking and 3D reconstruction, and it develops novel algorithms for predictable wireless networking and real-time wireless networked 3D reconstruction. Using the developed network system, this project will develop a see-through vision application for human-driving. The wireless networked see-through vision system will be deployed in the WSU police patrol vehicles, and the project team will outreach to the Detroit and State of Michigan police as well as open-source communities for broad adoption and deployment of the see-through vision system.<br/><br/>With the bold objective of eliminating the line-of-sight constraint of human vision and machine vision, this project addresses wireless networking and 3D reconstruction challenges in a holistic cross-layer framework. By integrating research investigation with systems development and deployment, this project will make the following significant contributions: 1) Effectively leveraging multi-scale physical structures of traffic flows, the multi-scale approach to resource management in vehicular wireless networks not only ensures predictable vehicular wireless networking, it also transforms fundamental challenges of vehicular networks to ones similar to those of mostly-immobile networks, thus enabling the exploration of fundamental, generically-applicable principles and mechanisms for predictable wireless networking; 2) the multi-scale approach to joint scheduling, channel assignment, power control, and rate control enables predictable control of per-packet transmission reliability in the presence of fast-varying network and environmental conditions such as wireless channel attenuation, internal and external interference, data traffic dynamics, and vehicle mobility; 3) the real-time scheduling algorithm enables controllable exploration of real-time capacity regions for system-level optimization; 4) the collaborative 3D reconstruction model integrates visual sensors in a divide-and-conquer fashion, and it enhances the capability of networked vision as well as its robustness to physical uncertainties; 5) the co-design of collaborative 3D reconstruction and wireless networking permits adaptive communication capacity allocation to optimize the quality of 3D reconstruction; 6) the attention-aware see-through vision application creates a new research field of vision augmentation by uniquely integrating computer vision and computer graphics research and by proposing a practical solution for displaying augmented 3D vision."
"1717885","CSR:  Small:  Collaborative Research:  GAMBIT:  Efficient Graph Processing on a Memristor-based Embedded Computing Platform","CNS","CSR-Computer Systems Research","10/01/2017","08/02/2017","Hai Li","NC","Duke University","Standard Grant","Matt Mutka","09/30/2020","$100,000.00","","hai.li@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7354","7923","$0.00","Recently, graph processing received intensive interests in light of a wide range of needs to understand relationships. Graph analytics are widely used in key domains in our society, such as cyber security, social media, infrastructure monitoring (e.g., smart building), natural language processing, system biology, recommendation systems. These important applications all fall into fast-growing sectors in computer science and engineering research. On the other hand, in many emerging applications, the graph analytics are ideally performed in the edge (e.g., a mobile or embedded system) in order to allow the relationships between events to be discovered in the field where they are unfold. Unfortunately, the existing embedded systems equipped with conventional computing units like CPU/GPU cannot efficiently process large graphs in real time. Instead, large data centers are required to perform the graph processing, either incurring extra latency and energy due to data communication or only providing forensic (offline) graph analysis. This research aims to effectively enable graph analytics in embedded system with disruptive emerging technology. <br/><br/>To support graph analytic applications with the limited hardware resources in embedded systems, this project seeks to develop GAMBIT -- a memristor-based embedded computing framework for efficient graph processing. Our research program aims to develop multi-layer techniques to enable highly efficient (e.g., 1000X) and scalable real-time graph analytics in embedded systems (i.e., network edge). It contains research efforts across circuit, architecture, system and vertical integration. (1) At the circuit level, the project proposes a memristor-based graph computing core to enable efficient computations for graph processing. (2) At the architecture level, the project proposes the complete memristor-based graph processing architecture for partitioned graph and various algorithms. (3) At the system level, the project develops a graph analytics framework for embedded systems and integrates it with a popular embedded OS. (4) For integration, the project proposes to develop an emulator of the proposed architecture and cross-layer HW/SW co-design techniques. This project contributes to society through engaging high-school and undergraduate students from minority-serving institutions into research, attracting women and under-represented groups into graduate education, expanding the computer engineering curriculum with graph processing and other emerging applications in embedded systems, disseminating research infrastructure for education and training, and collaborating with the industry."
"1717984","CSR:  Small:  Collaborative Research:  GAMBIT:  Efficient Graph Processing on a Memristor-based Embedded Computing Platform","CNS","CSR-Computer Systems Research","10/01/2017","08/02/2017","Xuehai Qian","CA","University of Southern California","Standard Grant","Matt Mutka","09/30/2020","$250,000.00","","xuehai.qian@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7354","7923","$0.00","Recently, graph processing received intensive interests in light of a wide range of needs to understand relationships. Graph analytics are widely used in key domains in our society, such as cyber security, social media, infrastructure monitoring (e.g., smart building), natural language processing, system biology, recommendation systems. These important applications all fall into fast-growing sectors in computer science and engineering research. On the other hand, in many emerging applications, the graph analytics are ideally performed in the edge (e.g., a mobile or embedded system) in order to allow the relationships between events to be discovered in the field where they are unfold. Unfortunately, the existing embedded systems equipped with conventional computing units like CPU/GPU cannot efficiently process large graphs in real time. Instead, large data centers are required to perform the graph processing, either incurring extra latency and energy due to data communication or only providing forensic (offline) graph analysis. This research aims to effectively enable graph analytics in embedded system with disruptive emerging technology. <br/><br/>To support graph analytic applications with the limited hardware resources in embedded systems, this project seeks to develop GAMBIT -- a memristor-based embedded computing framework for efficient graph processing. Our research program aims to develop multi-layer techniques to enable highly efficient (e.g., 1000X) and scalable real-time graph analytics in embedded systems (i.e., network edge). It contains research efforts across circuit, architecture, system and vertical integration. (1) At the circuit level, the project proposes a memristor-based graph computing core to enable efficient computations for graph processing. (2) At the architecture level, the project proposes the complete memristor-based graph processing architecture for partitioned graph and various algorithms. (3) At the system level, the project develops a graph analytics framework for embedded systems and integrates it with a popular embedded OS. (4) For integration, the project proposes to develop an emulator of the proposed architecture and cross-layer HW/SW co-design techniques. This project contributes to society through engaging high-school and undergraduate students from minority-serving institutions into research, attracting women and under-represented groups into graduate education, expanding the computer engineering curriculum with graph processing and other emerging applications in embedded systems, disseminating research infrastructure for education and training, and collaborating with the industry."
"1748109","EAGER:  Tracing Privacy-Policy Statements into Code for Privacy-Aware Mobile App Development","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","08/15/2017","06/05/2018","Xiaoyin Wang","TX","University of Texas at San Antonio","Standard Grant","Sol Greenspan","07/31/2020","$144,288.00","Jianwei Niu","Xiaoyin.Wang@UTSA.EDU","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","1714, 8060","025Z, 065Z, 7434, 7916, 9178, 9251","$0.00","Privacy for smartphone and mobile applications users present unprecedented threats. In the United States, privacy policies serve as the primary means to inform users about how mobile apps process privacy data. The application developers are responsible for implementing privacy policies so that the code corresponds to the policies. Currently, there are no techniques for tracing high-level privacy practices into code.  New research is needed to develop automatic privacy-aware development tools, as well as tools to determine if the code implements the policies correctly. <br/><br/>To automatically trace high-level privacy practices in privacy policies into application code, the project borrows the idea of context-based classification from information extraction in natural language processing (NLP).  In particular, both the natural-language-based policies and the program code are subjected to statistical NLP techniques to determine relationships between the policies and their implementations.  The assumption that NLP techniques can be applied to code is based on recent work that has established the ""naturalness"" of software, in the sense that statistical NLP techniques appear to work just as well for computer programs as they do for, say, English. The project will mine a large set of privacy policies and corresponding code to find out how, or to what extent, policies manifest themselves in code. To the extent that consistency between privacy policies and code can be determined, new approaches to privacy policy understanding and enforcement might be possible."
"1661497","ABI Development: Leveraging NSF-funded national cyberinfrastructure to spearhead biological discovery with Galaxy","DBI","ADVANCES IN BIO INFORMATICS","07/01/2017","07/21/2020","Anton Nekrutenko","PA","Pennsylvania State Univ University Park","Continuing Grant","Peter McCartney","06/30/2021","$1,659,890.00","Nancy Ide, Vincent Hilser, James Taylor, Jeremy Goecks","anton@bx.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","BIO","1165","","$0.00","Due to the rapidly increasing volume of biological data from sequencing, imaging, and other technologies, data processing needs in the Life Sciences are now on par with physical and engineering disciplines. Importantly, the distributed nature of data generation in biology makes this situation even more challenging. Today one can hardly find a research institution or university without multiple high-throughput DNA sequencing machines, and there are often references to a ""data crisis"" in biology. Federal agencies, and the NSF in particular, are investing heavily in cyberinfrastructure by supporting development of high performance computing (HPC) resources such as the Extreme Science and Engineering Discovery Environment (XSEDE). Yet to a large extent, these resources remain unknown to biological researchers who overwhelmingly continue to rely on fragile in-house computation. The goal of this project is to ensure effective utilization of federal funds that have been invested into development of the national computing infrastructure. This project will extend the Galaxy software platform to leverage existing NSF hardware resources, increasing the value of existing infrastructure for biology researchers that were previously unable to take full advantage of these resources.<br/><br/>This project will follow a comprehensive approach that addresses the needs of experimental scientists, tool developers, and administrators of high performance compute systems (HPC). Access to national compute infrastructure will be expanded so that Galaxy will function as a middleware interface to existing heterogeneous environments such as XSEDE or individual systems such as Jetstream. Software components necessary to optimize Galaxy as a link between researchers and existing HPC will be developed based on pilot projects with the Texas Advanced Computing Center (TACC), XSEDE, PSC, and Indiana University. (2) XSEDE resources to enable interactive data exploration and visualization will be leveraged to expand Galaxy's current capacity for dynamic scientific data analysis.  Integration with Interactive Analysis Environments, such as Jupyter or RStudio will allow manipulation and creation of Galaxy datasets using common scripting languages. Taking advantage of XSEDE resources will enable Galaxy's interactive environments and visual analytics to scale to large datasets and sophisticated workflows. (3) Sustainable training and outreach will focus on creating and disseminating curricula that enable investigators to learn skills needed to analyze large datasets. Creation of pre-configured infrastructure components for running workshops and develop modules for undergraduate and graduate face-to-face and on-line classes will expand the current educational portfolio to scale support for increasing numbers of Galaxy users, including disciplines beyond life sciences such as Natural Language Processing. Outcomes of this project will be available at http://galaxyproject.org and https://github.com/galaxyproject."
"1718398","III: Small: Collaborative Research: Towards End-to-End Computer-Assisted Fact-Checking","IIS","Info Integration & Informatics","09/01/2017","05/17/2018","Jun Yang","NC","Duke University","Standard Grant","Amarda Shehu","08/31/2020","$194,503.00","","junyang@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7364","7364, 7923, 9251","$0.00","This project will develop ClaimBuster, an end-to-end system for computer-assisted fact-checking. This system will monitor live discourses, social media, and news to catch factual claims, detect matches with a curated repository of fact-checks from professionals, and deliver the matches instantly to readers and viewers. For various types of new claims not checked before, ClaimBuster will automatically check them against knowledge databases and report if they are truthful. For novel claims where humans must be brought into the loop, the system will provide algorithmic and computational tools to assist laypersons and professionals in understanding and vetting the claims. ClaimBuster, upon completion of the proposed work, is positioned to become the first-ever automated fact-checking system for use on a broad spectrum of factual claims. Its use will be expanded to verify claims in various types of narratives, discourses and documents such as sports news, legal documents, and financial reports. It can benefit a large base of potential users including consumers, publishers, corporate competitors, and legal professionals, among others. It directly benefits consumers by improving information accuracy and transparency. It helps news organizations speed their fact-checking process and also ensure the accuracy of their own news stories. Businesses can use ClaimBuster to identify falsehoods in their competitors' and their own reports and press releases. It also assists professionals such as lawyers in verifying documents.<br/><br/>ClaimBuster will use database query, data mining, and natural language processing techniques to aid fact-checking. The detailed research tasks in this project will be as follows. (1) Investigate how to model factual claims and produce their internal representations. For this, the team will create taxonomies of claim templates in different domains, categorize claims based on the taxonomies, and generate internal representations through semantic parsing of the claims' textual forms. Such domain-specific modeling and internal representation of claims will enable novel methods and systematic, coherent solutions for other components of the system. (2) For algorithmic fact-checking, they will devise novel methods for translating claims into structured queries, keyword queries and natural language questions. Results of these queries over general and domain-specific databases and knowledge graphs will be compared with the answers embedded in the claims themselves, to verify if the claims check out. (3) By viewing claims as parameterized queries, they will develop methods based on perturbation analysis to find counter-arguments to claims and to find ""interesting"" factlets from datasets. These results will help ClaimBuster in identifying ""cherry-picking"" claims -- claims that are correct but misleading."
"1719054","III: Small: Collaborative Research: Towards End-to-End Computer-Assisted Fact-Checking","IIS","Info Integration & Informatics","09/01/2017","07/25/2017","Chengkai Li","TX","University of Texas at Arlington","Standard Grant","Amarda Shehu","08/31/2021","$320,780.00","Mark Tremayne","cli@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7364","7364, 7923","$0.00","This project will develop ClaimBuster, an end-to-end system for computer-assisted fact-checking. This system will monitor live discourses, social media, and news to catch factual claims, detect matches with a curated repository of fact-checks from professionals, and deliver the matches instantly to readers and viewers. For various types of new claims not checked before, ClaimBuster will automatically check them against knowledge databases and report if they are truthful. For novel claims where humans must be brought into the loop, the system will provide algorithmic and computational tools to assist laypersons and professionals in understanding and vetting the claims. ClaimBuster, upon completion of the proposed work, is positioned to become the first-ever automated fact-checking system for use on a broad spectrum of factual claims. Its use will be expanded to verify claims in various types of narratives, discourses and documents such as sports news, legal documents, and financial reports. It can benefit a large base of potential users including consumers, publishers, corporate competitors, and legal professionals, among others. It directly benefits consumers by improving information accuracy and transparency. It helps news organizations speed their fact-checking process and also ensure the accuracy of their own news stories. Businesses can use ClaimBuster to identify falsehoods in their competitors' and their own reports and press releases. It also assists professionals such as lawyers in verifying documents.<br/><br/>ClaimBuster will use database query, data mining, and natural language processing techniques to aid fact-checking. The detailed research tasks in this project will be as follows. (1) Investigate how to model factual claims and produce their internal representations. For this, the team will create taxonomies of claim templates in different domains, categorize claims based on the taxonomies, and generate internal representations through semantic parsing of the claims' textual forms. Such domain-specific modeling and internal representation of claims will enable novel methods and systematic, coherent solutions for other components of the system. (2) For algorithmic fact-checking, they will devise novel methods for translating claims into structured queries, keyword queries and natural language questions. Results of these queries over general and domain-specific databases and knowledge graphs will be compared with the answers embedded in the claims themselves, to verify if the claims check out. (3) By viewing claims as parameterized queries, they will develop methods based on perturbation analysis to find counter-arguments to claims and to find ""interesting"" factlets from datasets. These results will help ClaimBuster in identifying ""cherry-picking"" claims -- claims that are correct but misleading."
"1737419","CAREER: Holistic Scene Understanding with Multiple Hypotheses from Vision Modules","IIS","Robust Intelligence","02/01/2017","06/12/2018","Dhruv Batra","GA","Georgia Tech Research Corporation","Continuing Grant","Jie Yang","08/31/2020","$435,122.00","","dbatra@vt.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","1045, 7495","$0.00","This project develops algorithms and techniques for holistic scene understanding from images. The key barrier to building the next generation of vision systems is ambiguity. For example, a patch from an image may look like a face but may simply be an incidental arrangement of tree branches and shadows. Thus, a vision module operating in isolation often produces nonsensical results, such as hallucinating faces floating in thin air. This project develops a visual system that jointly reasons about multiple plausible hypotheses from different vision modules such as 3D scene layout, object layout, and pose estimation. The developed technologies have the potential to improve vision systems and make fundamental impact - from self-driving cars bringing mobility to the physically impaired, to unmanned aircrafts helping law enforcement with search and rescue in disasters. The project involves research tightly integrated with education and outreach to train the next generation of young scientists and researchers.  <br/><br/>This research addresses the fundamental challenge in joint reasoning by extracting and leveraging a small set of diverse plausible hypotheses or guesses from computer vision modules (e.g. a patch may be a {sky or a vertical surface} x {face or tree branches}). This project generates new knowledge and techniques for (1) generating a small set of diverse plausible hypotheses from different vision modules, (2) joint reasoning over all modules to pick a single hypothesis from each module, and (3) reducing human annotation effort by actively soliciting user feedback only on the small set of plausible hypotheses. <br/><br/>Project Webpage: http://computing.ece.vt.edu/~dbatra"
"1633360","BIGDATA: Collaborative Research: IA: F: Latent and Graphical Models for Complex Dependent Data in Education","IIS","STATISTICS, Project & Program Evaluation, Big Data Science &Engineering","01/01/2017","09/07/2016","Jingchen Liu","NY","Columbia University","Standard Grant","Finbarr Sloane","12/31/2020","$800,671.00","Zhiliang Ying","jcliu@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1269, 7261, 8083","4444, 7433, 8083, 8251","$0.00","This is a comprehensive research proposal on the statistical modeling and analysis for educational assessment. This research addresses issues concerning fundamental statistical problems that arise in the analysis of Big Data in education. The research focus is on modeling and inference for large-scale data with complex dependence and structures (such as high-dimensional response and process data). These data arise from the introduction of new methods of testing student knowledge that rely on scenarios presented to the students and on simulation-based environments where student responses to a simulated environment are tested. This research is collaborative between Columbia University  and the Educational Testing Service.<br/><br/>The topics studied include latent graphical modeling for high-dimensional item response data, modeling and segmentation of process data via dictionary models, estimation of item-attribute relationship, dimension reduction, theoretical analysis and computational methods for the proposed models. The analysis combines techniques and concepts from mathematics and probability and applies them to nonlinear statistical models and data analysis. The proposed model combines latent variable and graphical approaches for high-dimensional data; for modeling process data, recent advances in modeling and segmenting techniques for natural language processing will be investigated. In the theoretical development,  several algebraic concepts to formulate model identifiability and perform combinatorial analysis on high-dimensional discrete spaces will be studied. In addition, optimization algorithms will be developed using recent advances in numerical methods."
"1720268","EAGER: Collaborative: BystanderBots: Automated Bystander Intervention for Cyberbullying Mitigation","CNS","Secure &Trustworthy Cyberspace","08/15/2017","01/18/2018","Suma Bhat","IL","University of Illinois at Urbana-Champaign","Standard Grant","Balakrishnan Prabhakaran","07/31/2020","$200,000.00","Giulia Fanti","spbhat2@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8060","025Z, 065Z, 7434, 7916, 8225, 9102","$0.00","Bullying has lasting negative psychological and physical effects on victims, bystanders, and bullies alike; online settings can magnify both the scale and impact of these effects, as anonymity can embolden people to make hostile posts about individuals or groups.  This project aims to reduce the prevalence of such posts through the design of active, automated ""bystander interventions"" in online comment threads.  Bystander interventions, in which one or more witnesses to a bullying incident pressures the bully to stop, are often effective in schoolyards, but people are often reluctant to intervene in online scenarios.  Instead, a computer program could post comments that contain these interventions, potentially reducing follow-on aggression from the original poster or others who might pile on -- if bullies perceive these posts as coming from human bystanders, and if bullies under the cover of pseudonyms react to bystander interventions as they do in in-person confrontations.   The project will proceed in three main stages.  The first stage involves improving cyberbullying detection through better detection of non-standard language use associated with bullying in a particular commenting system.  The second stage involves developing a dialogue system that acts like a human bystander, creating messages that look appropriate in the context of given a comment thread and that contain psychologically-valid bystander interventions.  The third stage involves deploying the tool in a large video sharing site and monitoring its ability to detect and, through interventions, mitigate further bullying.  If successful, the project could have real impacts in reducing online aggression in social media systems while reducing the need for (and possible harms to) human moderators; the tools will also be released to the community to support other kinds of research around how chatbots and humans might interact in online comments.<br/><br/>The work on detection aims to advance natural language processing (NLP) and computational pragmatics, particularly around non-canonical language use, because state-of-the-art bullying detection schemes typically use bag-of-words approaches that do not consider the linguistic and structural features of cyberbullying.  The team will explore how to identify both explicit indicators of bullying, by developing topic models based on complex features where particular topics are more often associated with bullying, and implicit indicators, through looking for words whose use in a given context diverges from their location in other contexts. The context will be represented as a subspace of words, where the words themselves occur as low-dimensional word embeddings. The dialogue generation portion of the project will characterize and represent properties of effective bystander interventions from the psychology literature.  This representation will drive a dialogue manager designed to generate bystander responses automatically so that the responses contain features that are both believable and are known to be effective in reducing bullying online.  These components will first be evaluated through offline testing, using comment data labeled for bullying content and human ratings of the generated dialogue.  Once a reasonably effective pipeline has been built, it will be evaluated in a series of online experiments in which comment threads are monitored and automated bystander responses generated for some, but not all, threads detected as containing bullying.  The software will log the monitored threads and any generated responses, along with behavior both before and after the automated bystander response in a particular thread; these data will allow the team to evaluate the impact of the bystander intervention on bullying incidents later in the thread."
"1720365","EAGER: Collaborative: BystanderBots: Automated Bystander Intervention for Cyberbullying Mitigation","CNS","Secure &Trustworthy Cyberspace","08/15/2017","08/07/2017","Dorothy Espelage","FL","University of Florida","Standard Grant","Balakrishnan Prabhakaran","07/31/2019","$99,993.00","","espelage@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","8060","025Z, 065Z, 7434, 7916, 8225, 9102","$0.00","Bullying has lasting negative psychological and physical effects on victims, bystanders, and bullies alike; online settings can magnify both the scale and impact of these effects, as anonymity can embolden people to make hostile posts about individuals or groups.  This project aims to reduce the prevalence of such posts through the design of active, automated ""bystander interventions"" in online comment threads.  Bystander interventions, in which one or more witnesses to a bullying incident pressures the bully to stop, are often effective in schoolyards, but people are often reluctant to intervene in online scenarios.  Instead, a computer program could post comments that contain these interventions, potentially reducing follow-on aggression from the original poster or others who might pile on -- if bullies perceive these posts as coming from human bystanders, and if bullies under the cover of pseudonyms react to bystander interventions as they do in in-person confrontations.   The project will proceed in three main stages.  The first stage involves improving cyberbullying detection through better detection of non-standard language use associated with bullying in a particular commenting system.  The second stage involves developing a dialogue system that acts like a human bystander, creating messages that look appropriate in the context of given a comment thread and that contain psychologically-valid bystander interventions.  The third stage involves deploying the tool in a large video sharing site and monitoring its ability to detect and, through interventions, mitigate further bullying.  If successful, the project could have real impacts in reducing online aggression in social media systems while reducing the need for (and possible harms to) human moderators; the tools will also be released to the community to support other kinds of research around how chatbots and humans might interact in online comments.<br/><br/>The work on detection aims to advance natural language processing (NLP) and computational pragmatics, particularly around non-canonical language use, because state-of-the-art bullying detection schemes typically use bag-of-words approaches that do not consider the linguistic and structural features of cyberbullying.  The team will explore how to identify both explicit indicators of bullying, by developing topic models based on complex features where particular topics are more often associated with bullying, and implicit indicators, through looking for words whose use in a given context diverges from their location in other contexts. The context will be represented as a subspace of words, where the words themselves occur as low-dimensional word embeddings. The dialogue generation portion of the project will characterize and represent properties of effective bystander interventions from the psychology literature.  This representation will drive a dialogue manager designed to generate bystander responses automatically so that the responses contain features that are both believable and are known to be effective in reducing bullying online.  These components will first be evaluated through offline testing, using comment data labeled for bullying content and human ratings of the generated dialogue.  Once a reasonably effective pipeline has been built, it will be evaluated in a series of online experiments in which comment threads are monitored and automated bystander responses generated for some, but not all, threads detected as containing bullying.  The software will log the monitored threads and any generated responses, along with behavior both before and after the automated bystander response in a particular thread; these data will allow the team to evaluate the impact of the bystander intervention on bullying incidents later in the thread."
"1633353","BIGDATA: Collaborative Research: IA: F: Latent and Graphical Models for Complex Dependent Data in Education","IIS","Project & Program Evaluation, Big Data Science &Engineering","01/01/2017","02/02/2017","Matthias von Davier","NJ","Educational Testing Service","Standard Grant","Finbarr Sloane","12/31/2020","$319,626.00","Qiwei He","mvondavier@ets.org","Center for External Research","Princeton","NJ","085402218","6096832734","CSE","7261, 8083","4444, 7433, 8083, 8244, 8251","$0.00","This is a comprehensive research proposal on the statistical modeling and analysis for educational assessment. This research addresses issues concerning fundamental statistical problems that arise in the analysis of Big Data in education. The research focus is on modeling and inference for large-scale data with complex dependence and structures (such as high-dimensional response and process data). These data arise from the introduction of new methods of testing student knowledge that rely on scenarios presented to the students and on simulation-based environments where student responses to a simulated environment are tested. This research is collaborative between Columbia University  and the Educational Testing Service.<br/><br/>The topics studied include latent graphical modeling for high-dimensional item response data, modeling and segmentation of process data via dictionary models, estimation of item-attribute relationship, dimension reduction, theoretical analysis and computational methods for the proposed models. The analysis combines techniques and concepts from mathematics and probability and applies them to nonlinear statistical models and data analysis. The proposed model combines latent variable and graphical approaches for high-dimensional data; for modeling process data, recent advances in modeling and segmenting techniques for natural language processing will be investigated. In the theoretical development,  several algebraic concepts to formulate model identifiability and perform combinatorial analysis on high-dimensional discrete spaces will be studied. In addition, optimization algorithms will be developed using recent advances in numerical methods."
"1740822","TRIPODS: Data Science for Improved Decision-Making: Learning in the Context of Uncertainty, Causality, Privacy, and Network Structures","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, INSPIRE","10/01/2017","08/23/2017","Kilian Weinberger","NY","Cornell University","Standard Grant","A. Funda Ergun","09/30/2021","$1,496,655.00","Steven Strogatz, Giles Hooker, Jon Kleinberg, David Shmoys","kilianweinberger@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","041Y, 1253, 8078","047Z, 060Z, 062Z","$0.00","The researchers propose to create a center of data science for improved decision-making that combines expertise from computer science, information science, mathematics, operations research, and statistics. Their goal is to pursue basic research that will contribute to the theoretical foundations of data science.  The research topics chosen have applications that can benefit society as a whole and integrate the perspectives of the disciplines that the project brings together. The five concrete research directions proposed are: Privacy and Fairness, Learning on Social Graphs, Learning to Intervene, Uncertainty Quantification, and Deep Learning. The aim of the Center is to advance knowledge in these areas and to broaden the range of disciplines and perspectives that can provide contributions to these challenging issues. The researchers plan to incorporate the community beyond Cornell through online seminars, workshops, and student conferences. <br/><br/>The research findings will provide an urgently needed foundation for data science in several topic areas of importance to society. As the center is placed at the intersection of multiple disciplines, the intellectual merit spans all disciplines involved and findings may translate to new algorithms and approaches in each one of them.  <br/><br/>The research focus spans five core areas. <br/><br/>1. Privacy and Fairness.  As data science becomes pervasive across many areas of society, and as it is increasingly used to aid decision-making in sensitive domains, it becomes crucial to protect individuals by guaranteeing privacy and fairness.  The investigators propose to research the theoretical foundations to providing such guarantees and to surface inherent limitations. <br/><br/>2. Learning on Social Graphs. Many of the fundamental questions in applying data science to the interactions between individuals and larger social systems involve the social networks that underpin the connections between individuals. The researchers will develop new techniques for understanding both the structure of these networks and the processes that take place within them.<br/><br/>3. Learning to Intervene. Data-driven approaches to learning good interventions (including policies, recommendations, and treatments) inspire challenging questions about the foundations of sequential experimental design, counterfactual reasoning, and causal inference.<br/><br/>4. Uncertainty Quantification. Quantifying uncertainty about specific predictions or conclusions represents a key need in data science, especially when applied to decision-making with potential consequences to human subjects. The researchers will develop statistical tools and theoretical guarantees to assess the uncertainties of predictions made by popular algorithms in data science. <br/><br/>5. Deep Learning. Deep Learning algorithms have made impressive advances in practical settings.  Although their basic building blocks are well understood, there is still ambiguity about what they learn and why they generalize so well. There are indications that they may learn data manifolds and that the type of optimization algorithm influences generalization. <br/><br/>Advances in our theoretical understanding of these phenomena requires combined efforts from optimization, statistics, and mathematics but could lead to insights for all aspects of data science.<br/><br/>Funds for the project come from CISE Computing and Communications Foundations, MPS Division of Mathematical Sciences, MPS Office of Multidisciplinary Activities, and Growing Convergent Research. (Convergence can be characterized as the deep integration of knowledge, techniques, and expertise from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. This project promotes Convergence by bringing together communities representing many disciplines including mathematics, statistics, and theoretical computer science as well as engaging communities that apply data science to practical research problems.)"
"1740250","SI2:SSE: MAtrix, TEnsor, and Deep-Learning Optimized Routines (MATEDOR)","OAC","Software Institutes","09/01/2017","09/25/2018","Azzam Haidar","TN","University of Tennessee Knoxville","Standard Grant","Stefan Robila","08/31/2021","$400,000.00","Stanimire Tomov","haidar@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004","7433, 8004, 8005","$0.00","A number of scientific software applications from important fields, including applications in deep learning, data mining, astrophysics, image and signal processing, hydrodynamics, and more, do many computations on small matrices (also known as ""tensors"") and using widely available standard linear-algebra software libraries. Scientists are trying to make these applications run faster by running them on advanced high performance computing (HPC) systems, that are heterogeneous systems that use processors of many different types, such as ""accelerators"" - that use of specialized computer hardware to perform some functions more efficiently than standard, general-purpose processors - and ""co-processors"" - that can run certain specialized functions in parallel with the central processor. However, standard linear algebra software libraries cannot make use of these specialized hardware components, and so the scientific applications mentioned above do not become much faster. Many existing linear algebra libraries, including libraries supplied by commercial vendors of computing technology have been tried to no avail. This issue is now critical because advancements in science from important fields are being held back due to the lack of progress in speeding up software. This project will address this through research and development that will create efficient software that can repetitively execute tensor operations grouped together in ""batches"" and which can be written to run very efficiently and quickly on the types of hardware components that exist in HPC systems. In addition to the research and development, several students will be engaged in the project, thus helping develop a critically needed component of the U.S. workforce.<br/><br/>The trend in high performance computing (HPC) toward large-scale, heterogeneous systems with GPU accelerators and coprocessors has made the near total absence of linear algebra software for small matrix or tensor operations especially noticeable. Given the fundamental importance of numerical libraries to science and engineering applications of all types, the need for libraries that can perform batched operations on small matrices or tensors has become acute. This MAtrix, TEnsor, and Deep-learning Optimized Routines (MATEDOR) project seeks to provide a solution to this problem by developing a sustainable and portable library for such small computations. Future releases of MATEDOR are expected to have a significant impact on application areas that use small matrices and tensors and need to exploit the power of advanced computing architectures. Such application areas include deep-learning, data mining, metabolic networks, computational fluid dynamics, direct and multi-frontal solvers, image and signal processing, and many more. This team has a proven record of providing software infrastructure that is widely adopted and used, that supports ongoing community contributions, and that becomes incorporated in vendor libraries (e.g., Intel's MKL and NVIDIA's CUBLAS) and other software tools and frameworks (e.g., MATLAB and R). Students will be regularly integrated into the project activities, and this group of PIs has an exceptionally strong record of community outreach, having given numerous performance optimization and software tutorials at conferences and Users Group meetings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1702957","WiFiUS: RF Sensing in Internet of Things: When Deep Learning Meets CSI Tensor","CNS","Special Projects - CNS, Networking Technology and Syst","04/15/2017","04/01/2019","Shiwen Mao","AL","Auburn University","Standard Grant","Alexander Sprintson","03/31/2020","$339,994.00","","smao@auburn.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","1714, 7363","7218, 7363, 8229, 9150","$0.00","Internet of Things (IoT) refers to a worldwide network of interconnected uniquely addressable things based on standard communication protocols. In recent years, medical care and healthcare are recognized as one of the most attractive application areas of the IoT, with applications ranging from patient and equipment tracking in medical facilities to health monitoring. The goal of this project is to investigate an IoT healthcare monitoring system, which is likely to continue to grow in popularity as it utilizes IoT in various ways for improving the quality of patient care. The outcomes from this project will significantly improve the state-of-the-art of RF sensing in the IoT, and provide a significant step toward enriched user experience at greatly reduced costs. The educational plan includes developing and enhancing undergraduate and graduate-level courses in the field. The simulation tool and testbed will provide excellent opportunities for students to gain hands-on experience in the cutting-edge technology. Outcomes from this project will be disseminated through publications, presentations, and a project website. The PIs are fully committed to increasing participation from under-represented groups in research, and will continue to further such efforts via outreach, in particular, through collaboration with Historically Black Colleges and Universities (HBCU).<br/><br/>Specifically, this project aims to gain a deep understanding of RF sensing in healthcare IoT by exploiting advanced statistics and learning techniques, and to develop effective algorithms to make healthcare IoT more efficient. This research intends to significantly reduce the cost and improve the accuracy of RF sensing in healthcare IoT with intelligent exploration of CSI tensor and deep learning. The research work falls into the following three interwoven thrusts. (i) RF Sensing in Indoor IoT Environments: to address several fundamental challenges in RF sensing in indoor IoT environments, including developing a stochastic model for CSI based fingerprinting, incorporating nested arrays, and applying deep learning for indoor device-free localization. (ii) RF Sensing for Healthcare: to develop effective solutions for contact-free and long-term vital signs and human behavior monitoring, using RF and inaudible sound signals. (iii) System integration and evaluation: we will integrate the components from the first two thrusts into a healthcare IoT environment to understand the interaction of these components and evaluate the overall system performance."
"1738247","Student Travel and Activities Support for 2017 International Semantic Web Conference (ISWC 2017)","IIS","Info Integration & Informatics","04/15/2017","04/10/2017","Lalana Kagal","MA","Massachusetts Institute of Technology","Standard Grant","Maria Zemankova","03/31/2019","$30,000.00","","lkagal@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7364","7364, 7556","$0.00","This award will support about twenty U.S.-based graduate students attending the 16th International Semantic Web Conference (ISWC 2017).  The conference, which will be held October 21-25, 2017 in Vienna, Austria, is the premier major international forum for state-of-the-art research on all aspects of the Semantic Web and fostering new directions in the future generation of the World Wide Web.  Student support helps cover the travel costs for US students, making it possible for them to attend the conference.  This allows students to meet key members the Semantic Web research community, it gives them the opportunity to disseminate their work, and it provides a venue for them to interact with future national and international scientific collaborators. Doctoral students will participate in the Doctoral Consortium - a full day event where students can get critical, but encouraging, feedback on their work from senior members of the community.  They can also benefit from the career mentoring lunch, where experienced members of the community from both academia and industry answer questions in an informal setting.<br/><br/>The International Semantic Web Conference, which is now in its sixteenth year, is an interdisciplinary conference that includes work on: Data Management, Natural Language Processing, Knowledge Representation and Reasoning, Ontologies and Ontology Languages, Semantic Web Engineering, Linked Data, User Interfaces and Applications.  It regularly has several hundred attendees.  In addition to the main technical tracks, the conference includes a variety of events that provide opportunities for deeper interaction among researchers at different institutions or industries, at different stages of their research careers, and researchers who are interested in many different aspects of Semantic Web Research. Additional information can be accessed at the ISWC 2017 conference web site (http://iswc2017.semanticweb.org/)."
"1659836","REU Site: MedIX: Medical Informatics Experiences in Undergraduate Research","IIS","RSCH EXPER FOR UNDERGRAD SITES","03/15/2017","08/02/2019","Daniela Raicu","IL","DePaul University","Standard Grant","Wendy Nilsen","08/31/2021","$446,384.00","Jacob Furst","draicu@cti.depaul.edu","1 East Jackson Boulevard","Chicago","IL","606042287","3123627595","CSE","1139","9250","$0.00","The main objectives of the Medical Informatics (MedIX) Research Experience for Undergraduates (REU) Site program are to engage undergraduate students in challenging research projects, encourage them to pursue graduate education, and expose students to interdisciplinary research, especially at the border of information technology and medicine. The MedIX REU Site runs for three summers, with ten student participants doing research ten weeks each summer under the supervision of five mentors dedicated to undergraduate research. The REU Site is hosted by two interdisciplinary laboratories: the Medical Informatics Laboratory at DePaul University and the Imaging Research Institute at the University of Chicago. The research environment offers the students the opportunity to interact with computer scientists, medical physicists, and medical doctors.<br/><br/>All of the projects are inspired by state-of-the-art research questions in imaging informatics. Students work as part of faculty-undergraduate teams on new problems ranging from traditional image processing (e.g., computer-aided diagnosis, breast density assessment, cancer detection) to structured reporting and natural language processing of radiology reports, to workflow and process re-engineering to the application of data mining and ontology-based means for image annotation and markup (e.g., lung nodule detection and interpretation). Ultimately, this REU site will help support students interested in imaging informatics, broaden participation in computer science. In addition, each project has the long-term potential to increase the quality of healthcare available to people everywhere. More information on the program can be found at the MedIX REU Site web site (http://facweb.cs.depaul.edu/research/vc/medix)."
"1722310","SBIR Phase I:  Apple Yield Mapping using Computer Vision","IIP","SBIR Phase I","07/01/2017","07/11/2017","Patrick Plonski","MN","Farm Vision Technologies Inc","Standard Grant","Muralidharan Nair","02/28/2019","$225,000.00","","pahplonski@gmail.com","287 Wilder St N","Saint Paul","MN","551045127","6512537877","ENG","5371","5371","$0.00","The broader impact/commercial potential of this project is the practical deployment of a computer-vision based yield estimation system in fruit orchards.  This will provide fruit farmers with a useful tool in planning their harvest and sales, as well as in managing the long-term health of their plants.  This will potentially reduce the inherent risk in fruit growing, thereby improving the affordability and availability of fresh fruit in the US.  Better certainty may particularly help small growers, because they have less existing capability to manage risk.  Furthermore, by allowing adoption of precision agriculture techniques techniques to high value crops, this project may help save water and contribute to reduction of runoff pollution from fertilizer and chemicals.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project addresses the problem of vision-based fruit detection and mapping of fruit, for purposes of yield mapping.  The research objectives are to improve the commercial usability and robustness of existing yield-mapping approaches, so that they can be applied in production fruit orchards.  The anticipated technical results of this project include demonstration of useful yield mapping of apples, in a variety of real world production orchard environments, in a variety of weather and lighting conditions."
"1647170","US Ignite: Focus Area 1: Fast Autonomic Traffic Congestion Monitoring and Incident Detection through Advanced Networking, Edge Computing, and Video Analytics","CNS","Secure &Trustworthy Cyberspace","01/01/2017","09/07/2016","Abdallah Khreishah","NJ","New Jersey Institute of Technology","Standard Grant","Deepankar Medhi","12/31/2020","$599,993.00","Joyoung Lee, Chengjun Liu, Nirwan Ansari","abdallah@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","8060","015Z","$0.00","Video-based traffic monitoring systems have been widely used for traffic management, incident detection, intersection control, and public safety operations. Current designs pose critical challenges. First, it relies heavily on human operators to monitor and analyze video images. Second, commercially available computer vision technologies cannot satisfactorily handle severe conditions, such as weather and glare, which significantly impair video image quality. Third, the simultaneous transmission of numerous video  signals to a central facility creates extreme demands on the communications network, which can lead to jamming. This project presents a novel approach that incorporates wireless sensor networks, hierarchical edge-computing, and advanced computer vision technology. The methods can be expanded to address a wide spectrum of potential applications including wrong-way driving alerts, congestion detection under bad weather conditions, accident scene management support, suspect vehicle tracking, wildfire detection and alert, and emergency evacuation, which could save lives and hundreds of billions of dollars annually. It also aligns with the smart city initiative.<br/><br/>By using bluetooth/WiFi detection technology, the trajectories and speeds of vehicles equipped with such devices will be collected. This information, along with the captured video data, will be  analyzed by the proposed computer vision software, installed at the edge of the network on cloudlets, to perform fast detection and prioritization of the video streams from different cameras. The proposed hierarchical edge-computing paradigm will not only enable real-time big data analysis at the edge but will also be demonstrated and actualized to perform timely efficient video analytics. Depending on the weather conditions, different detection and prioritization algorithms will be activated. Video coding will then be implemented to transmit the selected video streams to the central back-end system for further processing. If an incident is detected by the algorithm either at the edge or at the back-end, a necessary feedback action will be taken, such as calling an emergency group, the highway safety dispatch, or the police. Under a technical partnership with New Jersey Department of Transportation, multiple pilot tests of the proposed system will be implemented on selected highway corridors designated by the department."
"1652617","CAREER: Understanding Vision and Natural Motion Statistics Through the Lens of Prediction","IIS","Robust Intelligence","03/01/2017","01/28/2019","Stephanie Palmer","IL","University of Chicago","Continuing Grant","Kenneth Whang","02/28/2022","$549,880.00","","sepalmer@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7495","1045, 7495, 8089, 8091","$0.00","The visual input to the brain is transformed even before signals leave the eye, and these computations produce an efficient representation of the structure of the natural visual world. Previous work by the PI has shown that this processing can include repackaging of information for optimal prediction. This suggests a new approach to neural encoding. While many previous studies have sought to characterize what stimuli in the past gave rise to a subsequent response, this work asks what future stimuli those responses predict. The proposed project will derive the best possible predictor given the way objects move in the outside world and quantify how close the brain gets to this optimum. Viewing the brain through the lens of prediction develops a principle of neural coding and computation that can bridge brain regions, from the retina to higher visual areas. A component of this plan involves measuring and quantifying the predictive components of natural motion. In doing so, a public database of natural motion will be created that will be a lasting tool for the neuroscience and computer vision communities. An associated educational program will bring over 100 local middle school children to campus each year for hands-on neuroscience experiments, and will instill in a large group of graduate students the rewards and responsibilities of science teaching.<br/><br/>The research proposed here explores prediction in the visual system in a variety of ways: by computing efficiency bounds on the predictive encoding of complex motion, by developing quantitative methods to test these bounds in neural datasets, by measuring the statistics of motion in natural scenes, and by describing how, mechanistically, the brain achieves this performance. Hypotheses about how the brain performs optimal predictive computations may be constrained by the structure of predictable events in the natural visual world. To measure these statistics, a new natural movie database will be constructed by making high-speed, high-pixel-depth recordings of natural scenes. By quantifying motion in these data, this project will yield statistical and generative models of natural motion that will inform our understanding of the natural world and provide a compact way to recapitulate natural motion in silico. These stimuli will be used to test whether neural systems optimally encode information relevant for prediction. The work will also test what adaptive and otherwise non-linear processing steps underlie optimal prediction in the brain.<br/>"
"1802358","CAREER: From Data to Knowledge: Extracting and Utilizing Concept Graphs in Online Environments","IIS","Info Integration & Informatics","09/13/2017","05/08/2018","Cornelia Caragea","KS","Kansas State University","Continuing grant","James French","03/31/2019","$193,323.00","","cornelia@uic.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","7364","1045, 7364","$0.00","Knowledge bases today are central to the successful utilization of information available in the large and growing amounts of digital data on the Web. Such technologies have started to unleash a transformation of Web search from a keyword match to discovery, learning, and creativity, which are crucial to promoting the goal of knowledge discovery. Unfortunately, the search for information remains inherently difficult for significant portions of the Web such as the Scholarly Web, which contains many millions of scientific documents. For example, PubMed has over 20 million documents, whereas Google Scholar is estimated to have more than 100 million. Open-access digital libraries such as CiteSeerX, which acquire freely-available research articles from the Web, witness an increase in their document collections as well. Despite attractive advancements by scholarly search portals, semantic search technologies that ""understand"" complex concepts and their relations and can systematically satisfy users' intricate information needs have yet to be investigated on the Scholarly Web. The goal of this project is to design solutions to make information more accessible and comprehensible to Scholarly Web users in particular, and Web users in general, and to help them discover knowledge more effectively and efficiently. The approach taken will be to develop an integrated framework, focusing on the extraction and utilization of scholarly knowledge graphs in online scholarly environments. Educationally, this work will involve: training of graduate, undergraduate, and high-school students, particularly encouraging the participation of women and underrepresented groups in the research efforts; curriculum development and integration of research into courses taught by the PI; exposure of students to industry and international experiences; and education for the general public.<br/> <br/>The project will target the following research objectives: (1) explore the construction of scholarly knowledge graphs that combine evidence from multiple resources in an open information extraction framework; (2) design and develop novel algorithms for the detection and analysis of interesting and previously unknown connections between concepts, in order to enforce knowledge discovery on the Scholarly Web; and (3) investigate the utility of scholarly knowledge graphs in a question answering system. The results of this research will be integrated into the CiteSeerX digital library (http://citeseerx.ist.psu.edu). The software, tools, and benchmark datasets, which will be developed during the course of this project will be made publicly available. All findings will be shared with the research community through publications in academic journals and presented in Information Retrieval, Text Mining and Natural Language Processing conferences. For further information, see the project web page:  http://www.cse.unt.edu/~ccaragea/skg.html."
"1661036","Assessing Complex Collaborative STEM Learning at Scale with Epistemic Network Analysis","DRL","ECR-EHR Core Research","08/01/2017","09/17/2018","David Shaffer","WI","University of Wisconsin-Madison","Continuing Grant","Finbarr Sloane","07/31/2022","$1,470,847.00","Parameswaran Ramanathan, Michael Gleicher, Jeffrey Linderoth","dws@education.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","EHR","7980","8817","$0.00","This project was submitted in response to EHR Core Research (ECR) program announcement NSF 15-509. The ECR program of fundamental research in STEM education provides funding in critical research areas that are essential, broad and enduring. EHR seeks proposals that will help synthesize, build and/or expand research foundations in the following focal areas: STEM learning, STEM learning environments, STEM workforce development, and broadening participation in STEM. The ECR program is distinguished by its emphasis on the accumulation of robust evidence to inform efforts to (a) understand, (b) build theory to explain, and (c) suggest interventions (and innovations) to address persistent challenges in STEM interest, education, learning, and participation.<br/><br/>In this project, researchers will develop a statistical analysis technique for measuring how people learn. With prior NSF funding, members of the team created epistemic network analysis (ENA), a technique for creating network models of complex and collaborative thinking in science, technology, engineering, and mathematics. ENA is being used by more than 40 researchers at 19 universities to answer a wide range of research questions in learning sciences, cognitive neuroscience, engineering education, environmental science education, medical and surgical education, and history of science. The proposed research and development will create an online toolkit that lets researchers upload audio, video, text, or log-file data, automatically transcribe the audio data, develop and validate automated codes using supervised natural language processing tools, and produce ENA models. This will make it possible for researchers analyze data on how people learn without requiring simultaneous expertise in automated transcription, data segmentation, coding, and network modeling. It will also make it possible to conduct analyses of learning using the large volumes of data that are currently generated by online learning tools, significantly expanding capacity for research on learning.<br/><br/>In this project, the research team will conduct fundamental research on learning in science, technology, engineering, and mathematics (STEM) by developing and extending a theory-based statistical analysis technique for using network analysis to model complex and collaborative STEM thinking (CCST). With prior NSF funding, members of the team created epistemic network analysis (ENA), a technique for creating dynamic models of CCST. Data on CCST typically come in one of two forms: Video and audio recordings of interactions from classrooms and workplaces, or log files from online interactions. Data in audio or video form must be transcribed. Text data from recordings or log files have to be coded, or annotated to indicate what elements of CCST are present in the data, and where those elements are located. With existing tools, researchers must complete these steps by hand, or use their own techniques for automated transcription and coding. However, many skilled CCST researchers are not simultaneously experts in the sciences of automated transcription, automated identification, validation, and application of codes, and network analysis. The proposed research and development will create ENAlysis, an online toolkit that provides a seamless, automated analysis pipeline from raw data to final results. The project will thus develop innovative methods for measuring STEM learning and expand access to a proven, theory-based technique for analyzing CCST. The result will be expanded use of powerful learning analytic techniques to model CCST, which will significantly improve assessment of STEM thinking and learning and inform policy and practice."
"1740858","TRIPODS: UA-TRIPODS - Building Theoretical Foundations for Data Sciences","CCF","TRIPODS Transdisciplinary Rese, Algorithmic Foundations","09/01/2017","09/04/2019","Hao Zhang","AZ","University of Arizona","Continuing Grant","Christopher Stark","08/31/2020","$1,416,498.00","Stephen Kobourov, David Glickenstein, Joseph Watkins","hzhang@math.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","041Y, 7796","047Z, 062Z, 7926, 9251","$0.00","UA-TRIPODS aims to build a transformative and highly interactive data science hub in the Southwestern United States for the integration of research, education, and outreach in the theoretical foundations of data science.  Through research, education, and outreach targeted to high school students through postdoctoral fellows, UA-TRIPODS will help establish and encourage a diverse population to pursue careers in quantitative and computational data science.  The intellectual merits of the project include identifying fundamental areas where progress in the theoretical foundations is imperative, developing mathematical and algorithmic principles of data science, and basic research on topics ranging from the theory of large scale networks and optimization to statistical modeling for natural language processing and Bayesian methods.  This is accomplished by extending long-standing collaborations with experts in the foundational disciplines - theoretical computer science, mathematics, and statistics - and motivated by problems in the domain sciences and industry.  New mathematical, statistical, and algorithmic principles will strengthen the foundations of theoretical data science, while driving discovery and innovation for emerging applications.  UA-TRIPODS will also cultivate new partnerships with local industries engaged in data-centric enterprises in astronomy, environmental science, genomics, lunar and planetary sciences, medicine, transportation, and optical sciences.<br/><br/>Research Working Groups will pursue fundamental questions in a number of different areas in the theoretical foundations of data science.  By ensuring each group includes researchers from each area of the foundational disciplines, while also pairing these groups with science and industry partners, UA-TRIPODS will ensure that these projects will benefit from a truly transdisciplinary collaboration.  Progress in collaboration will be measured by specialized program evaluation metrics.  Yearly workshops, seminars, and brainstorming/visioning activities will enable UA-TRIPODS to lay the foundation for the future of training and research in data science.  In addition, this project will support the development of a new undergraduate degree in Statistics and Data Science at the University of Arizona, expanded online offerings, and facilitation of cooperation in graduate education (both among the foundational disciplines and with the domain sciences).  Students will be provided with opportunities to experience first-hand data analysis and interdisciplinary research and education.  UA-TRIPODS will also engage partnering colleges and universities across the Southwest in the institute activities and then utilize these connections to support novel and effective inter-university and intra-university research and educational programs.  Partnerships with Hispanic Serving Institutions and Tribal Colleges in southern California, Arizona, New Mexico, and western Texas will be strengthened."
"1738489","SBIR Phase II:  Innovative visual search and similarity for decor, apparel, and style","IIP","SMALL BUSINESS PHASE II","09/15/2017","09/14/2017","Sean Bell","CA","GrokStyle Inc.","Standard Grant","Peter Atherton","08/31/2019","$747,959.00","","founders@grokstyle.com","450 Townsend St. Suite 207","San Francisco","CA","941071510","6075270508","ENG","5373","5373, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is to develop visual search for product recognition in the furniture and home décor vertical.  Text-based searches have revolutionized the ability of people to complete tasks more quickly and efficiently as they are able to find the information they desire in an organized, compiled, and logical manner. Visual search provides the next level of disruption in search capabilities by allowing users to find information even more rapidly and accurately by using images. The deep learning-based software being developed will allow consumers to find products they are interested in, and co-purchase related products, quickly. Further, users will be more engaged through exposure to designer photographs of products (inspirational photography). By helping customers find exactly what they are looking for in a timely manner, user engagement and productivity will be increased. Further, related style-based recommendations will increase purchasing overall.  Increased spending stimulates economic growth by increasing taxable revenue by retailers, and through increased sales taxes generated from the purchases. <br/><br/>This Small Business Innovative Research Phase II project seeks to develop a visual search engine that is poised to disrupt retail and ecommerce by switching the focus from text-based to visual search-based exploration. The platform initially targets interior décor and furniture where deep learning techniques are trained to recognize products across a wide range of conditions. In Phase II, the software deep learning architectures will be generalized to enable a broader range of products, and to allow customers more control over design decisions and choices. A client-facing REST API will allow retailers, designers, and media companies to programmatically access functionality of the platform, and build their own user interfaces and apps on top of the deep learning technology. Lastly, it is proposed to develop a white-label app that can be customized for individual retailers who want to distribute this visual search capability to their customers.  Achieving these objectives will create state-of-the-art performance in visual search for applications in interior design, apparel search, real estate search, and product look-up."
"1661263","COLLABORATIVE RESEARCH: Learning Progressions on the Development of Principle-based Reasoning in Undergraduate Physiology (LeaP UP)","DUE","ECR-EHR Core Research","07/15/2017","09/14/2018","Jennifer Doherty","WA","University of Washington","Continuing Grant","Dawn Rickey","06/30/2021","$973,035.00","Mary Pat Wenderoth, Jenny McFarland","doherty2@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","EHR","7980","8244, 8817","$0.00","The complex societal problems of increasing agricultural production to meet the needs of 9 billion people by 2050, and caring for an aging population with increasingly more complex neurological and cardiovascular health issues require future scientists, physicians, and allied health professionals to develop expertise in organismal physiology. In physiology, as in other disciplines, becoming an expert in a field requires the abilities to recognize, understand and effectively reason using the principles of the discipline. During their college careers, science students often rely on rote memorization rather than principle-based reasoning to solve problems, and this leads to context-bound thinking that fails to build robust understandings. Such students can, for example, list the steps involved in muscle contraction, but cannot predict what will happen when a mutation is introduced in a muscle protein. This project will develop a ""learning progression"" to document how college students can develop more and more sophisticated principle-based reasoning over time to understand the physiology of animals and plants in both introductory biology and anatomy and physiology courses. Based on this learning progression, the project team will also develop open-ended assessment questions that can be scored via computer. Collectively, these tools will have the potential to transform how college students learn physiology, and to significantly enhance the quality of their resulting understanding and ability to solve related problems. The project, entitled Learning Progressions on the Development of Principle-based Reasoning in Undergraduate Physiology (LeaP UP), is supported by the Education and Human Resources Core Research Program, which funds fundamental research in STEM learning and learning environments, broadening participation in STEM, and STEM workforce development.<br/><br/>The LeaP UP project will develop a learning progression that describes how undergraduate students develop principle-based reasoning in the use of flux (flow down gradients) and mass balance (Conservation of Mass) in physiology. The learning progression will then guide the creation of constructed response assessments and associated computer scoring models that instructors can use to determine where their students are along the spectrum of understanding. The project team will capitalize on cutting edge advances in natural language processing and text analysis to create computer programs to accurately predict how experts would score students' responses to the conceptual constructed-response assessments. These automated scoring methods will rapidly score responses from large numbers of college students nationwide and allow the investigators to map national trends in students levels of understanding of students as they move through their undergraduate Biology and pre-Allied Health curricula at community colleges to large research universities. Thus, the tools developed will provide an organizing framework for the future redesign of undergraduate physiology curricula."
"1660643","COLLABORATIVE RESEARCH: Learning Progressions on the Development of Principle-based Reasoning in Undergraduate Physiology (LeaP UP)","DUE","ECR-EHR Core Research","07/15/2017","06/24/2020","Kevin Haudek","MI","Michigan State University","Continuing Grant","Dawn Rickey","06/30/2021","$485,955.00","John Merrill, Mark Urban-Lurain, Joyce Parker","haudekke@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","EHR","7980","8244, 8817","$0.00","The complex societal problems of increasing agricultural production to meet the needs of 9 billion people by 2050, and caring for an aging population with increasingly more complex neurological and cardiovascular health issues require future scientists, physicians, and allied health professionals to develop expertise in organismal physiology. In physiology, as in other disciplines, becoming an expert in a field requires the abilities to recognize, understand and effectively reason using the principles of the discipline. During their college careers, science students often rely on rote memorization rather than principle-based reasoning to solve problems, and this leads to context-bound thinking that fails to build robust understandings. Such students can, for example, list the steps involved in muscle contraction, but cannot predict what will happen when a mutation is introduced in a muscle protein. This project will develop a ""learning progression"" to document how college students can develop more and more sophisticated principle-based reasoning over time to understand the physiology of animals and plants in both introductory biology and anatomy and physiology courses. Based on this learning progression, the project team will also develop open-ended assessment questions that can be scored via computer. Collectively, these tools will have the potential to transform how college students learn physiology, and to significantly enhance the quality of their resulting understanding and ability to solve related problems. The project, entitled Learning Progressions on the Development of Principle-based Reasoning in Undergraduate Physiology (LeaP UP), is supported by the Education and Human Resources Core Research Program, which funds fundamental research in STEM learning and learning environments, broadening participation in STEM, and STEM workforce development.<br/><br/>The LeaP UP project will develop a learning progression that describes how undergraduate students develop principle-based reasoning in the use of flux (flow down gradients) and mass balance (Conservation of Mass) in physiology. The learning progression will then guide the creation of constructed response assessments and associated computer scoring models that instructors can use to determine where their students are along the spectrum of understanding. The project team will capitalize on cutting edge advances in natural language processing and text analysis to create computer programs to accurately predict how experts would score students' responses to the conceptual constructed-response assessments. These automated scoring methods will rapidly score responses from large numbers of college students nationwide and allow the investigators to map national trends in students levels of understanding of students as they move through their undergraduate Biology and pre-Allied Health curricula at community colleges to large research universities. Thus, the tools developed will provide an organizing framework for the future redesign of undergraduate physiology curricula."
"1740305","SI2-SSE: Improving Scikit-Learn Usability and Automation","OAC","Software Institutes","09/01/2017","06/19/2020","Nicolas Hug","NY","Columbia University","Standard Grant","Stefan Robila","08/31/2021","$399,356.00","","nh2611@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8004","7433, 8004, 8005","$0.00","Machine learning is a central component in many data-driven research areas, but its adoption is limited by the often complex choice of data processing, model, and parameter settings. The goal of this project is to create software tools that enable automatic machine learning, that is, solving predictive analytics tasks without requiring the user to explicitly specify the algorithm or model parameters used for prediction.  The software developed in this project will enable a wider use of machine learning, by providing tools to apply machine learning without requiring knowledge of the details of the algorithms involved.<br/><br/>The project, supported by the Office of Advanced Cyberinfrastructure, and the Division of Computing and Communication Foundations, extends the existing scikit-learn project, a machine learning library for Python, which is widely used in academic research across disciplines.  The project will add features to this library to lower the amount of expert knowledge required to apply models to a new problem, and to facilitate the interaction with automated machine learning systems. The project will also create a separate software package that includes models for automatic supervised learning, with a very simple interface, requiring minimal user interaction. In contrast to existing research projects, this project focuses on creating easy-to-use tools that can be used by researchers without extensive training in machine learning or computer science."
"1650512","Center for Visual and Decision Informatics (CVDI) I/UCRC site at the University of Virginia","CNS","Special Projects - CNS, IUCRC-Indust-Univ Coop Res Ctr","03/01/2017","06/11/2020","Peter Beling","VA","University of Virginia Main Campus","Continuing Grant","Ann Von Lehmen","02/28/2022","$430,811.00","Hongning Wang, William Scherer, Donald Brown, Matthew Gerber","pb3a@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","1714, 5761","5761, 9251","$0.00","The Center for Visual and Decision Informatics (CVDI) National Science Foundation Industry University Cooperative Research Center (IUCRC) was established in 2012 by the University of Louisiana at Lafayette (UL Lafayette) and Drexel University. CVDI has been successful in meeting the IUCRC program management requirements during its Phase 1, with demonstrated excellent growth trajectory in terms of research funding, industry members, and academic sites, and will continue into Phase 2 operation. This award aims to help establish a Phase II CVDI site at the University of Virginia (UVA) that complements and strengthens the research base and industrial support of the existing sites of CVDI. The Center's activities will generate societal benefits by addressing areas of national priority such as finance, cyber-physical systems, and security, and will deliver data science innovations with value to both government and industry. UVA CVDI will follow a strategy to broaden Science, Technology, Engineering, and Mathematics (STEM) participation: (1) involve under-represented groups through the NSF Research Experience for Undergraduates program; (2) recruit graduate students from neighboring Historically Black Colleges and Universities; (3) collaborate with K-12 STEM programs for data science outreach; and (4) increase student internships opportunities by leveraging CVDI industrial members and national initiatives, as the Graduate Education for Minorities program.<br/><br/>CVDI will serve as a major research and innovation center for solving challenges in core areas of data science such as big data analytics, visual analytics, augmented intelligence, and decision informatics. The UVA site brings complementary and synergistic research strength in a number of important technical areas, including: cognitive assistance, information retrieval, natural language processing, latent structure learning, reinforcement learning, data fusion and distributed learning, and agent-based models and simulation. The UVA site's focus on decision analytics and human-in-the-loop data gathering, analysis, and exploitation complements the activities and interests of the other sites. Application domains for the UVA site also reinforce and complement those of the other sites. These include: computational law, manufacturing, IoT and sensing, data privacy and security, finance and banking, internet analytics, and national defense."
"1704701","SaTC: CORE: Medium: Understanding and Fortifying Machine Learning Based Security Analytics","CNS","Secure &Trustworthy Cyberspace","08/01/2017","09/17/2019","Duen Horng Chau","GA","Georgia Tech Research Corporation","Continuing Grant","Wei-Shinn Ku","07/31/2021","$1,200,000.00","Le Song, Wenke Lee, Taesoo Kim","polo@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8060","025Z, 7434, 7924","$0.00","Preliminary research has demonstrated that by gaining control of the input data or the computation procedures, attackers can render machine learning based security analysis ineffective. The history of cybersecurity suggests that such attacks will become more prevalent in the real-world soon. This project undertakes the challenge of developing a systematic, foundational, and practical framework to understand attacks, quantify vulnerabilities, and fortify machine learning based security analytics. The framework's effectiveness is evaluated and demonstrated in realistic, user-facing environments, using real malware datasets. This project aims to fundamentally change how machine learning based systems will be designed, developed and deployed for security and malware analytics, cybersecurity more broadly, and numerous other application areas in science, education, and technology, as the use of machine learning is ubiquitous. The findings can lead to new breeds of adaptive defense systems that are highly resilient to current and future security attacks, helping protect the nation and its citizens from cyber harm.<br/><br/>This project combines multiple novel ideas synergistically, organized into four inter-related research thrusts: (1) machine learning theoretical framework, based on machine teaching and active learning, for understanding attacks, quantifying vulnerabilities, and measuring the capabilities of adversaries and model robustness; (2) algorithmic techniques for machine learning resilience, to adaptively counter adversaries' feature and sample manipulation strategies; (3) extensive evaluation of the identified attack and defense strategies with real and mutated malware datasets, on existing security systems, and demonstrate the improved attack resilience of the new, fortified machine learning system; (4) system-level countermeasures in real-world user-facing security analysis environments."
"1813571","III: Small: Collaborative Research: Keyphrase Extraction in Document Networks","IIS","Info Integration & Informatics","10/26/2017","05/08/2018","Cornelia Caragea","KS","Kansas State University","Continuing Grant","Maria Zemankova","08/31/2019","$64,960.00","","cornelia@uic.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","7364","7218, 7364, 7923, 9150","$0.00","Keyphrases for a document concisely describe the document using a small set of phrases (i.e., sequences of contiguous words in a document). For example, the keyphrases ""social networks"" and ""interest targeting"" quickly provide us with a high-level topic description (i.e., a summary) of a document focused on targeting interest for recommending services such as products and news to users, in the context of social networks. Given today's very large collections of documents, these keyphrases are extremely important not only for summarizing a document, but also for the search and retrieval of relevant information. However, keyphrases are not always available directly. Instead, they need to be gleaned from the many details in documents. This project addresses the problem of automatic keyphrase extraction from research papers, which are enablers of the sharing and dissemination of scientific discoveries. The goal of the project is to explore accurate approaches that automatically discover and extract keyphrases in documents, using document networks, which will help users handle and digest more information in less time during these ""big data"" times. Educationally, this research will involve training of both graduate and undergraduate students in the active area of research of keyphrase extraction, which has high impact in many real-world applications such as online advertising, document categorization, recommendation, and summarization, Web search and discovery, and topic tracking in newswire.  Although much research to date has been done on automatic keyphrase extraction, no previous approaches have captured the impact of documents on one another via the citation relation that connects documents in a network. This project will investigate models that take into consideration the linkage between citing and cited documents in a document network and will explore various qualitative and quantitative aspects of the question: ""What are the key phrases or concepts in a document?"" Scalable iterative algorithms will be designed and developed that capture different aspects of documents (e.g., topics or concepts), as well as the impact of one document on another (e.g., influence or topic evolution) in a document network. The results of this research will have a direct pipeline to the CiteSeerX digital library (http://citeseerx.ist.psu.edu). The software, tools, and benchmark datasets developed during the course of this project will be broadly disseminated via the project website (http://people.cs.ksu.edu/~ccaragea/keyphrases.html). All findings will be shared to the research community through publications in academic journals and presented in Information Retrieval, Text Mining and Natural Language Processing conferences."
"1800446","CDS&E: Collaborative Research: Machine Learning on Dynamical Systems via Topological Features","DMS","CDS&E-MSS","08/16/2017","11/15/2017","Elizabeth Munch","MI","Michigan State University","Standard Grant","Christopher Stark","08/31/2019","$101,672.00","","muncheli@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","8069","8083, 9263","$0.00","Objects whose state changes over time, known as dynamical systems, describe a large number of natural and engineered processes; therefore, developing a deeper understanding of their behavior is of great importance.  While sometimes it is possible to derive mathematical models that describe the evolution of a dynamical system, these models are almost always an abstraction of the physical system and, therefore, have a limited ability to predict how the system will change in time.  Further, when the system under investigation is large or too complicated with several factors influencing its behavior, it may simply be impossible to describe the system with the corresponding descriptive equations.  Consequently, in the absence of adequate analytical models it becomes necessary to instrument the dynamical system with sensors and use the resulting data to understand its characteristics.  Specifically, the change in the state of a dynamic system is often governed by an underlying skeleton that gives the overall behavior a shape, and thus the shape of the skeleton directly governs the system behavior.  Most of the time, this shape of the underlying skeleton is unknown and can be easily masked by the complicated and rich system signals.  The emergent field of topological data analysis (TDA), a branch of mathematics that quantifies the shape of data, is capable of revealing information that is invisible to other existing methods by providing a high level X-ray of the skeleton governing the dynamics.  However, the information-rich structures provided by TDA still need to be interpreted in order to classify the dynamics and predict future outcomes.  To accomplish this, the principal investigators will leverage ideas from machine learning, a field of study that investigates algorithms that can learn from the data and use the acquired knowledge for classification and prediction.  However, the mathematical theory that elucidates how machine learning can operate on the features extracted using TDA currently does not exist.  Hence, this work will develop the necessary, novel mathematical and computational tools at the intersection of topological data analysis (TDA), dynamical systems, and machine learning.<br/><br/>The principal investigators seek to understand and formulate the foundations of machine learning when the important features of a dynamical system are summarized by descriptors generated with topological data analysis (TDA).  Although these signatures provide an information-rich structure for the evolution of the dynamics, current literature has only been utilizing a fraction of the available information in order to identify, predict, and classify different dynamic behavior.  One of the current impediments to further exploring the relationship between TDA and dynamical systems is the lack of machine learning theory that can operate on these structures. Therefore, the success of our effort will lead to (1) the establishment of a novel, general, and robust machine learning framework for studying dynamic signals via topological signatures, (2) better understanding of the relationship between TDA and dynamical systems via the use of these methods on real and synthetic data, and (3) the integration of the new knowledge into the investigators' educational programs, which will provide timely training of well-equipped next generation scientists and engineers."
"1652674","CAREER: From Data to Knowledge: Extracting and Utilizing Concept Graphs in Online Environments","IIS","Info Integration & Informatics","06/01/2017","04/12/2017","Cornelia Caragea","TX","University of North Texas","Continuing grant","James French","12/31/2017","$95,593.00","","cornelia@uic.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","7364","1045, 7364","$0.00","Knowledge bases today are central to the successful utilization of information available in the large and growing amounts of digital data on the Web. Such technologies have started to unleash a transformation of Web search from a keyword match to discovery, learning, and creativity, which are crucial to promoting the goal of knowledge discovery. Unfortunately, the search for information remains inherently difficult for significant portions of the Web such as the Scholarly Web, which contains many millions of scientific documents. For example, PubMed has over 20 million documents, whereas Google Scholar is estimated to have more than 100 million. Open-access digital libraries such as CiteSeerX, which acquire freely-available research articles from the Web, witness an increase in their document collections as well. Despite attractive advancements by scholarly search portals, semantic search technologies that ""understand"" complex concepts and their relations and can systematically satisfy users' intricate information needs have yet to be investigated on the Scholarly Web. The goal of this project is to design solutions to make information more accessible and comprehensible to Scholarly Web users in particular, and Web users in general, and to help them discover knowledge more effectively and efficiently. The approach taken will be to develop an integrated framework, focusing on the extraction and utilization of scholarly knowledge graphs in online scholarly environments. Educationally, this work will involve: training of graduate, undergraduate, and high-school students, particularly encouraging the participation of women and underrepresented groups in the research efforts; curriculum development and integration of research into courses taught by the PI; exposure of students to industry and international experiences; and education for the general public.<br/> <br/>The project will target the following research objectives: (1) explore the construction of scholarly knowledge graphs that combine evidence from multiple resources in an open information extraction framework; (2) design and develop novel algorithms for the detection and analysis of interesting and previously unknown connections between concepts, in order to enforce knowledge discovery on the Scholarly Web; and (3) investigate the utility of scholarly knowledge graphs in a question answering system. The results of this research will be integrated into the CiteSeerX digital library (http://citeseerx.ist.psu.edu). The software, tools, and benchmark datasets, which will be developed during the course of this project will be made publicly available. All findings will be shared with the research community through publications in academic journals and presented in Information Retrieval, Text Mining and Natural Language Processing conferences. For further information, see the project web page:  http://www.cse.unt.edu/~ccaragea/skg.html."
"1719097","RI: SMALL: Fast Prediction and Model Compression for Large-Scale Machine Learning","IIS","Robust Intelligence","08/15/2017","08/14/2017","Cho-Jui Hsieh","CA","University of California-Davis","Standard Grant","Weng-keen Wong","03/31/2019","$450,000.00","","chohsieh@cs.ucla.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7495","7495, 7923","$0.00","In order to handle large-scale problems, many algorithms have been proposed for improving the training speed of machine learning models. However, in many real world applications the bottleneck is at the prediction phase instead of the training phase due to the time and space complexity of prediction. Unlike the training phase that can run for several hours on multiple machines, the prediction phase usually runs on real-time systems; as a result, each prediction has to be done in a few seconds in order to provide immediate feedback to users. Furthermore, applications that run on mobile devices have even more strict constraints on memory capacity and computational resources. To address these issues, this research develops a new family of machine learning algorithms with faster prediction time and smaller model size. The outcome of this project creates a fundamental shift in the applicability of machine learning models to real-time online systems and on-device applications. Software packages and experimental platforms are made available to the public after being tested on applications. Besides the research objectives, the PI also pursues educational objectives including promoting undergraduate research, involving under-represented minorities in science and engineering, and developing undergraduate and graduate data science curriculums.<br/><br/>The goal of this project is to develop novel approaches for reducing prediction time and model size of machine learning algorithms. In particular, the project focuses on machine learning applications with large output space (matrix factorization, extreme multi-class/multi-label classification), and highly nonlinear models (kernel methods and deep neural networks). A series of approximation algorithms are studied, including tree-based algorithms, clustering approaches, and sub-linear time search algorithms. A unified framework is developed for these algorithms and the trade-off between accuracy and prediction time/model size is studied both in theory and in practice. The proposed algorithms are evaluated on a broad range of real world applications, including online web services and on-device applications."
"1718549","AF: Small: Learning and Optimization with Strategic Data Sources","CCF","Algorithmic Foundations","09/01/2017","06/26/2017","Yiling Chen","MA","Harvard University","Standard Grant","A. Funda Ergun","08/31/2021","$450,000.00","","yiling@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7923, 7926, 7932","$0.00","The goal of this research project is to develop new results in machine learning and optimization when training data for machine learning or information about optimization problems is acquired from strategic sources.  We are blessed with unprecedented abilities to connect with people all over the world: buying and selling products, sharing information and experiences, asking and answering questions, collaborating on projects, borrowing and lending money, and exchanging excess resources.  These activities result in rich data that scientists can use to understand human social behavior, generate accurate predictions, find cures for diseases, and make policy recommendations.  Machine learning and optimization traditionally take such data as given, for example treating them as independent samples drawn from some unknown probability distribution.  However, such data are possessed or generated by people in the context of specific rules of interaction.  Hence, what data become available and the quality of available data are results of strategic decisions.  For example, people with sensitive medical conditions may be less willing to reveal their medical data in a survey and freelance workers may not put in a good-faith effort in completing a task.  This strategic aspect of data challenges fundamental assumptions in machine learning and optimization.  The research project takes a holistic view that jointly considers data acquisition with learning and optimization.  It will bring improved benefits in business, government, and societal decision-making processes where machine learning and optimization are widely applicable.  The research project also involves the mentoring of PhD students, innovation in graduate teaching, and engagement of members of underrepresented groups in research.<br/><br/>The PI will pursue a broad research agenda developing a fundamental understanding of how acquiring data from strategic sources affects the objectives of machine learning and optimization.  The first set of goals aims to develop a theory for machine learning when a learning algorithm needs to purchase data from data holders who cannot fabricate their data but each have a private cost associated with revealing their data.  A notion of economic efficiency for machine learning will be established.  The second set of goals will further advance the frontier of machine learning by designing joint elicitation and learning mechanisms when data are acquired from strategic agents but the quality of the contributed data cannot be directly verified.  The third set of goals will develop optimization algorithms with good theoretical guarantees when parameters of an optimization problem may be unknown initially but the algorithm designer can gather information about the parameters from strategic agents."
"1717290","RI: Small: Modern Machine Learning Algorithms for Ranking from Pairwise and Higher-Order Comparisons","IIS","Robust Intelligence","09/01/2017","07/27/2017","Shivani Agarwal","PA","University of Pennsylvania","Standard Grant","Rebecca Hwa","08/31/2021","$445,473.00","","ashivani@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7495","7495, 7923","$0.00","The problem of ranking a large number of items from comparisons among a few items at a time plays a crucial role in many areas, including recommender systems, crowdsourcing, marketing, and econometrics. In modern settings, as the numbers of items to be ranked increase and corresponding datasets grow in size and complexity, it is critical to re-visit the classical algorithms currently used for these problems and to design new algorithms that can better scale to modern needs under fewer assumptions. This project will design modern machine learning algorithms for such problems, while training PhD students and postdoctoral scientists in the interdisciplinary skills needed to design novel machine learning algorithms for problems involving modern datasets. Other broader impacts of the project will also include organization of workshops and/or tutorials to disseminate the results of the research conducted here, survey articles aimed at conveying the ideas to a broad scientific audience, and activities designed to increase participation of under-represented groups in STEM education opportunities and careers. <br/><br/>The problem of ranking from pairwise comparisons has been studied in several fields, including statistics, operations research, social choice, and computer science, and several algorithms have been developed; however, very little has been understood in terms of how these different algorithms relate to each other, under what conditions they succeed (or fail), and how insights from one can be used to improve another. Algorithms for ranking from higher-order comparisons are even less well understood. The project will develop a strong understanding of the conditions under which various pairwise ranking algorithms succeed (or fail), and use insights from this understanding to develop modern machine learning algorithms with strong performance guarantees for ranking from pairwise as well as higher-order comparisons. Specifically, the project will investigate the following three directions: <br/><br/>(1) Understanding conditions on pairwise models under which current algorithms succeed or fail.<br/>(2) Design of new machine learning algorithms for ranking from pairwise comparisons.<br/>(3) Ranking from higher-order comparisons.<br/><br/>The project will bring a unified perspective to the study of ranking from pairwise comparisons, which hitherto has been scattered across different disciplines; develop new machine learning algorithms that improve the state of the art for a variety of ranking objectives; and initiate a systematic study of ranking from higher-order comparisons, a nascent area at the intersection of machine learning, statistics and econometrics."
"1714440","III: Small: Algorithms and Theoretical Foundations for Approximate Bayesian Inference in Machine Learning","IIS","Info Integration & Informatics","08/01/2017","08/31/2017","Roni Khardon","MA","Tufts University","Continuing Grant","Aidong Zhang","01/31/2019","$497,115.00","","rkhardon@iu.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7364","7364, 7923, 7924","$0.00","Over the last two decades Bayesian models have become central in machine learning. Bayesian models often hypothesize latent (non-observed) variables with explanatory or predictive power toward observed phenomena. The challenge is to infer the state of these variables or a belief over that state from observed data. For example, one might try to infer a user's preferences from observations about their own behavior and the behavior of other users. The goal of this project is to develop general approximate inference algorithms that work across large families of Bayesian models so that solutions can be widely reused. The algorithmic work will be complemented by developing a learning theory for approximate Bayesian inference in machine learning. The theoretical framework will aim to prove performance guarantees for Bayesian prediction algorithms and inform the design of algorithms with desirable properties. The project will contribute to basic scientific research, advancing core goals in machine learning. The project will support training and research of PhD students and therefore will directly support human development. Through classroom teaching and outreach the project will expose a larger population of students to machine learning and its potential in applications.<br/><br/>More concretely, the project will investigate non-conjugate Bayesian latent variable models, i.e., it will avoid the often used but limiting simplifying assumption of conjugacy. On the algorithmic side the project will aim to generalize the paradigm of variational message passing for non-conjugate graphical models, and to develop stochastic variational inference algorithms using optimal structured approximations for large sub-families of such models. The proposed sub-families will capture the properties of many important problems in the literature. Exploratory research in several specific applications further motivates the work and will be used to test the algorithms. The project will develop a new angle for theoretical analysis of Bayesian algorithms, deriving performance guarantees on their expected error. A core idea is to view variational inference algorithms through the so-called agnostic learning framework where guarantees sought are relative to the best that can be done within a specific limited class of approximations. This will provide a fresh outlook that informs the design of algorithms with desired performance guarantees. The expected scientific impact of the project is having better algorithms with well understood performance characteristics and applicable for a larger class of machine learning problems."
"1718203","NeTS: Small: Collaborative Research: Fast Online Machine Learning Algorithms for Wireless Networks","CNS","Networking Technology and Syst","08/15/2017","07/07/2017","Rayadurgam Srikant","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alexander Sprintson","07/31/2021","$250,000.00","","rsrikant@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7363","7923","$0.00","In addition to their traditional use for human-to-human or human-to-machine communication, wireless networks are also envisioned to form the backbone of many emerging applications in health care, transportation, and power distribution.  The smooth operation of these applications critically depends on the satisfactory operation of the  wireless networks that are responsible for the efficient and timely transfer of information between agents, despite abruptly changing network conditions and stringent application-specific service requirements<br/>Existing state-of-the-art wireless network resource allocation does not account for the stringent and changing requirements of the application demands. This research proposes to leverage and extend the emerging area of machine learning to develop new approaches to low-delay wireless networks that are necessary for the support of essential services with applications in diverse domains including low-cost healthcare, energy savings, and security. Advances made in this research will benefit the society-at-large by enabling efficient and low-cost access to such services in the future. The project will also help advance the training and education of future engineers with a strong foundation on both the theoretical underpinnings and the practical considerations for the design of efficient wireless network algorithms. <br/><br/>The broad objective of this project is to develop a unified machine learning and resource allocation framework for future wireless networks that can adapt to rapidly changing dynamics and statistics at the physical layer and the increasingly stringent service requirements at the application layer. The fundamental problem in machine learning is to make decisions in a stochastic system when the statistical model underlying the system is unknown a priori. While there has been much activity on this problem, many features unique to wireless networks are not considered, including rapidly changing network dynamics, interactions and dependencies among multiple users, and transient delay performance. The focus of the proposal is to design fast, online learning algorithms which lead to dramatic improvements in network performance, by taking into account the unique characteristics of wireless networks."
"1642411","SI2-SSE: STAMLA: Scalable Tree Algorithms for Machine Learning Applications","OAC","Software Institutes","01/01/2017","09/08/2016","Vincent Reverdy","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","12/31/2019","$499,992.00","Robert Brunner","vreverdy@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","7433, 8004, 8005","$0.00","The voluminous growth in data together with the burgeoning field of data science have greatly increased the demand for machine learning, a field of computer science that focuses on the development of programs that can learn and change in response to new data. With increasing access to large volumes of data, practitioners often resort to machine learning to construct more precise models of nature, or to learn fundamentally new concepts. For example, machine learning can help improve the accuracy of weather and climate predictions, model the efficacy of drugs and their interactions, and identify specific features, such as a face, from a large set of images or videos. But with the growth of data volumes, the speed with which machine can learn from data is decreasing. As a result, new techniques are required to accelerate learning algorithms so that they can be applied to larger and more complex data sets. This work will develop new approaches to improve the performance of a wide class of machine learning algorithms. Specifically, this work will leverage the C++ programming language and recent research into fundamental bit operations to make fast tree-like data structures that underlie many of the most commonly used implementations of machine learning algorithms. In particular, algorithms in the scikit-learn library, the most widely used machine learning library written in the Python programming language, will be accelerated by using our these new tree data structures. Given the widespread adoption of the scikit learn library, this work will impact diverse fields from Astronomy to Biology to Geoscience to Physics. The scikit learn library is also one of the more popular libraries for teaching (and understanding) machine learning. With an explosion of books, blogs, and tutorials that use scikit learn algorithms and pipelines to demonstrate specific types of machine learning such as classification, regression, clustering, and feature extraction, this project will immediately impact a wide range of people from seasoned practitioners, engineers gaining additional training, and students at universities and colleges across the nation. In addition, these tree data structures will be submitted for inclusion in the C++ standard, which would impact millions of developers world-wide. Finally, the algorithms will be implemented under an open source license in a public forum.<br/><br/><br/>The STAMLA project aims at developing efficient and scalable tree algorithms inspired from high performance simulation codes for machine learning. Over the last few years, machine learning has become a popular technique in data mining to extract information from data sets, build models and make predictions across a wide range of application areas. However, current tools have been built in high-level languages with more focus on functionalities than on pure performance. But as scientific experiments are accumulating more and more data, and as complex models are requiring larger and larger training sets, scalability issues are emerging. At the same time, in high performance computing, petascale simulations have shown that fundamental data structure optimizations can have a significant impact on overall code performance. In particular, by replacing straightforward tree implementations with implicit trees based on hash tables, simulation codes are able to make the most of modern architecture, leveraging cache and vectorization. This research project will apply this knowledge to machine learning algorithms in order to overcome the limitations of existing libraries and make analyses of extremely large data sets possible. The proposed research includes the development of three library layers, built on top of each other. The first layer is a library of fast bit manipulation tools for tree indexing. It extends existing research that has already demonstrated two to three orders of magnitude improvements compared to standard solutions provided by compilers. The second layer is a tree building blocks library developed using generative programming techniques. This layer will provide developers with generic tools to build efficient implicit trees for specific domains and optimized at compile time to make the most of the targeted architecture. Finally, the third layer consists in a contribution package to the scikit-learn library to leverage the data structures introduced in the second layer. Together, these three layers form a consistent set that propagates low level optimizations based on high performance computing practices to one of the most widely used high level machine learning library. As machine learning is domain independent, the results of this project have the potential to impact all data intensive applications relying on machine learning algorithms based on tree data structures. Moreover, in addition to being developed in an open source framework via a public repository, the three library layers will be released through different channels: 1) the bit manipulation tools will aim at standardization in the C++ language through a collaboration with the ISO C++ Standards Committee 2) the tree building blocks will be proposed for inclusion in the Boost C++ libraries and 3) the machine learning algorithms will be published as a contribution package of the scikit-learn library. These channels will ensure a large adoption of the tools developed throughout this project, and their long-term support by well established communities."
"1717950","SaTC: CORE: Small: Multi-Party High-dimensional Machine Learning with Privacy","CNS","Secure &Trustworthy Cyberspace","09/01/2017","02/26/2019","Quanquan Gu","VA","University of Virginia Main Campus","Standard Grant","Nina Amla","08/31/2021","$498,624.00","David Evans, Quanquan Gu","qgu@cs.ucla.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8060","025Z, 7434, 7923","$0.00","Individuals and organizations can frequently benefit from combining their data to learn collective models. However, combining data to enable multi-party learning is often not possible.  It may not be permitted due to privacy policies, or may be considered too risky for a business to expose its own data to others. In addition, high-dimensional data are prevalent in modern data-driven applications. Learning from high-dimensional data owned by differential organizations is even more challenging, due to the bias introduced by the high-dimensional machine learning methods. The overarching goal of this project is to address these challenges by developing methods that enable a group of mutually distrusting parties to securely collaborate to apply high dimensional machine learning methods to produce a joint model without exposing their own data. This project enables owners of sensitive data to jointly learn models across their datasets without exposing that data and providing meaningful privacy guarantees.  It produces open source software tools and has many important societal applications, including its use in analyzing electronic health records across multiple hospitals to identify medical correlations what could not be found by any individual hospital.  <br/><br/><br/>The key of multi-party high-dimensional machine learning is to find an efficient way to produce an accurate aggregate model that reflects all of the data, by combining local models that are developed independently based on individual data sets. The strategy of this project is to combine two emerging research directions: distributed machine learning, which seeks to distribute machine learning algorithms across hosts and produce an aggregate model by combining multiple local models; and secure multi-party computation, which enables a group of mutually distrusting parties to jointly compute a function without leaking information about their private inputs or any intermediate results. It also incorporates differential privacy-based mechanisms into multi-party high dimensional learning, which further protects the individual data points in each party. The results of this research have the potential to impact both the machine learning and security research communities. The education plan of this project includes developing open course materials that integrate privacy and machine learning, and provide research-based training opportunities for both undergraduate and graduate students in computer science, systems engineering, and medical informatics. It actively gets underrepresented groups involved in research projects, and trains a new generation of interdisciplinary researchers."
"1705592","SusChEM: Machine learning blueprints for greener chelants","CBET","COMPUTATIONAL MATHEMATICS, MSPA-INTERDISCIPLINARY, EnvS-Environmtl Sustainability","08/01/2017","07/20/2017","John Keith","PA","University of Pittsburgh","Standard Grant","Bruce Hamilton","07/31/2021","$299,999.00","Eric Beckman","jakeith@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","ENG","1271, 7454, 7643","8007, 8248, 9263","$0.00","1705592 (Keith). Chelating agents have recently been identified as a key category of chemical products that are ripe for greener design. It is hypothesized that identifying better alternatives will require far broader explorations of chemical compound space than what is possible with conventional trial and error experimentation. In this project, accurate quantum chemistry calculations will be used to train state-of-the-art machine learning methods that will allow prediction of structures of greener chelating agents.<br/><br/>The machine learning method that will be developed promises a novel route to rapidly predict properties of chelant/metal complexes, not only with higher accuracy but with six orders of magnitude less computational time than conventional predictive quantum chemistry methods (e.g. Kohn-Sham Density Functional Theory). With this computational tool, it will be possible to rapidly screen through about 100,000 hypothetical chelant structures to predict those that would bind strongly to different metal ions. The project will also screen these complexes to see which have high propensities to degrade in reasonable timeframes, and which have low probabilities of being toxic. The top candidates from this novel screening approach will then be experimentally synthesized and tested. This will validate if quantum chemistry-based machine learning would be a transformative tool for environmental sustainability and green chemical design by being a more predictive supplement and/or alternative to conventional QSAR models. Four basic scientific questions will be addressed by the project. First, state-of-the-art computational quantum chemistry will be used to develop a quantitative understanding of which chelant/metal complex properties (bond energies, pKas, etc.) best correlate with overall chelant stability constants in aqueous solutions. Second, machine learning methods will be developed to be used to drive in silico searches for non-traditional molecular structures that would be able to bind as strong (or stronger) to different metal ions as EDTA. Third, additional computational screening procedures will then be used to find which of these promising chelant structures are unlikely to be non-toxic and nonpersistent in nature. Finally, it will be experimentally validated which of the novel chelants identified via computation would be industrially synthesized and economically viable for widespread use. If successful, this research effort would signify a paradigm shift for computer-aided design of greener chelants used in detergents, treatments of heavy metal poisoning, metal extraction for soil treatments, waste remediation, sequestering normally occurring radioactive materials from hydraulic fracturing sites, and water purification. This project will lay important foundational work that is needed to introduce new state-of-the-art computational modeling tools with greater predictive capacity than widely used QSAR models. All developed computer programs and accompanying tutorials for how to use the programs will be made freely available on the website of the PI. The educational component of this project will develop a computer game, ""Chelate-it"", which will allow students to quantify different fundamental chemical bonding concepts involved in chelation and then use that knowledge to design novel chelant structures on their own. The computer game will be tested in a summer school program at the University of Pittsburgh for underrepresented 10th grade students. The capacity for the computer game to educate students about chemical bonding, environmental sustainability engineering, and research will be assessed."
"1712554","Collaborative Research: Statistical Inference Using Random Forests and Related Methods","DMS","POP & COMMUNITY ECOL PROG, STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2017","07/19/2017","Giles Hooker","NY","Cornell University","Standard Grant","Gabor Szekely","08/31/2021","$215,078.00","","giles.hooker@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1182, 1269, 7454","8007, 8083","$0.00","This project seeks to develop methods to quantify uncertainty in machine learning algorithms and <br/>to incorporate machine learning and statistical inference. Machine learning has been enormously <br/>successful at using data to make predictions; it is used in an extensive range of applications <br/>from handwriting recognition to high frequency trading to driverless cars and personalized medicine. <br/>However, while machine learning algorithms make good predictions, they tell humans very little <br/>about how those predictions were arrived at: What were the important factors? How did they affect <br/>the prediction? They also don't distinguish predictions for which there is a lot of information <br/>about the probability of different outcomes (even if that covers a wide range) from those where <br/>very little information is available. For example, a machine learning algorithm may very accurately <br/>predict whether a person is likely to develop diabetes, but provides little if any information <br/>regarding how that person might lower his or her risk. This project will build on initial <br/>mathematical theory to develop methods to explain how Random Forests arrive at their predictions <br/>and how statistically confident those predictions are, and produce ways to link machine learning <br/>methods to other statistical models.<br/><br/>This project seeks to develop methods to quantify uncertainty in machine learning algorithms <br/>and to incorporate machine learning and statistical inference. The project will extend on a <br/>theoretical framework representing Random Forests as U-statistics to produce a practical <br/>implementation of statistical uncertainty quantification in machine learning. In particular, <br/>it will improve on methods to estimate sample variability in Random Forest predictions, develop <br/>computationally efficient screening tools for covariate and interaction selection, and incorporate <br/>ensemble methods as non-parametric terms in partially-linear models while retaining statistical <br/>inference via a modified boosting algorithm. These methods will be demonstrated on a citizen <br/>science data base in ornithology and in various biomedical applications."
"1712041","Collaborative Research:  Statistical Inference Using Random Forests and Related Methods","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2017","07/19/2017","Lucas Mentch","PA","University of Pittsburgh","Standard Grant","Gabor Szekely","08/31/2021","$119,802.00","","lkm31@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","MPS","1269, 7454","8007, 8083","$0.00","This project seeks to develop methods to quantify uncertainty in machine learning algorithms and to incorporate machine learning and statistical inference. Machine learning has been enormously successful at using data to make predictions; it is used in an extensive range of applications from handwriting recognition to high frequency trading to driverless cars and personalized medicine. However, while machine learning algorithms make good predictions, they tell humans very little about how those predictions were arrived at: What were the important factors? How did they affect the prediction? They also don't distinguish predictions for which there is a lot of information about the probability of different outcomes (even if that covers a wide range) from those where very little information is available. For example, a machine learning algorithm may very accurately predict whether a person is likely to develop diabetes, but provides little if any information regarding how that person might lower his or her risk. This project will build on initial mathematical theory to develop methods to explain how Random Forests arrive at their predictions and how statistically confident those predictions are, and produce ways to link machine learning methods to other statistical models.<br/><br/><br/>This project seeks to develop methods to quantify uncertainty in machine learning algorithms and to incorporate machine learning and statistical inference. The project will extend on a theoretical framework representing Random Forests as U-statistics to produce a practical implementation of statistical uncertainty quantification in machine learning. In particular, it will improve on methods to estimate sample variability in Random Forest predictions, develop computationally efficient screening tools for covariate and interaction selection, and incorporate ensemble methods as non-parametric terms in partially-linear models while retaining statistical inference via a modified boosting algorithm. These methods will be demonstrated on a citizen science data base in ornithology and in various biomedical applications."
"1717045","NeTS: Small: Collaborative Research: Fast Online Machine Learning Algorithms for Wireless Networks","CNS","Networking Technology and Syst","08/15/2017","07/07/2017","Atilla Eryilmaz","OH","Ohio State University","Standard Grant","Alexander Sprintson","07/31/2021","$250,000.00","","eryilmaz.2@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7363","7923","$0.00","In addition to their traditional use for human-to-human or human-to-machine communication, wireless networks are also envisioned to form the backbone of many emerging applications in health care, transportation, and power distribution.  The smooth operation of these applications critically depends on the satisfactory operation of the  wireless networks that are responsible for the efficient and timely transfer of information between agents, despite abruptly changing network conditions and stringent application-specific service requirements<br/>Existing state-of-the-art wireless network resource allocation does not account for the stringent and changing requirements of the application demands. This research proposes to leverage and extend the emerging area of machine learning to develop new approaches to low-delay wireless networks that are necessary for the support of essential services with applications in diverse domains including low-cost healthcare, energy savings, and security. Advances made in this research will benefit the society-at-large by enabling efficient and low-cost access to such services in the future. The project will also help advance the training and education of future engineers with a strong foundation on both the theoretical underpinnings and the practical considerations for the design of efficient wireless network algorithms. <br/><br/>The broad objective of this project is to develop a unified machine learning and resource allocation framework for future wireless networks that can adapt to rapidly changing dynamics and statistics at the physical layer and the increasingly stringent service requirements at the application layer. The fundamental problem in machine learning is to make decisions in a stochastic system when the statistical model underlying the system is unknown a priori. While there has been much activity on this problem, many features unique to wireless networks are not considered, including rapidly changing network dynamics, interactions and dependencies among multiple users, and transient delay performance. The focus of the proposal is to design fast, online learning algorithms which lead to dramatic improvements in network performance, by taking into account the unique characteristics of wireless networks."
"1717205","III: Small: Collaborative Research: High-Dimensional Machine Learning Methods for Personalized Cancer Genomics","IIS","Info Integration & Informatics","08/01/2017","09/05/2017","Jian Ma","PA","Carnegie-Mellon University","Continuing Grant","Sylvia Spengler","07/31/2021","$200,000.00","","jianma@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7364, 7923, 7924","$0.00","The key to success in personalized and precision cancer genomics lies in: (1) discovering and understanding the molecular-level mechanisms of how genetic alterations influence various cellular processes relevant to cancer, and (2) utilizing molecular signatures to tailor more personalized treatment strategies for patients. In order to achieve these goals, various high-throughput experimental methods have been developed in recent years to obtain information about a patient's cancer genome sequence, mRNA expression, protein expression, epigenetic readout, and other detailed information about a patient's tumor. However, algorithms that fully harness such a massive amount of high dimensional data to yield biomedical insights are often lacking. This project will advance the field of data-driven complex modeling of cancer genomic data for personalized cancer treatment by developing novel algorithms that use emerging and new techniques in high-dimensional machine learning. The results of this research have the potential to impact both the machine learning field and the computational genomics field. The educational components integrated with the research program will develop new curriculum materials, involve undergraduate students and underrepresented groups in research, and train a new generation of interdisciplinary graduate researchers. <br/><br/>This project consists of two synergistic research thrusts to develop novel high-dimensional machine learning algorithms for analyzing high-throughput cancer genomic data. First,  the project will develop high-dimensional graphical models for multi-view data modeling to integrate data from heterogeneous genome-wide data sources. Second, it will devise novel high-dimensional collaborative learning methods for personalized drug recommendation. The high-dimensional graphical models will be used to estimate networks for different cancer subtypes.  These networks will then  be integrated into the recommendation algorithms, which in turn will help improve the multi-view graphical model estimation. This project will enhance the ability to interpret large-scale cancer genomics data by pinpointing the roles of complex molecular interactions in cancer onset and progression, which will enable  novel ways to more effectively discover personalized molecular signatures and more targeted potential treatments of cancer. Such technical innovation and conceptual advancement have the potential to reshape the way that one approaches graphical model estimation and its role in biological contexts. The project will potentially open up new possibilities for both theoreticians and practitioners in machine learning and computational biology as well as other disciplines."
"1718633","SHF:Small:Neuromorphic Architectures for On-line Learning","CCF","Software & Hardware Foundation","08/15/2017","08/10/2017","Tarek Taha","OH","University of Dayton","Standard Grant","Sankar Basu","07/31/2021","$439,998.00","Guru Subramanyam","ttaha1@udayton.edu","300 COLLEGE PARK AVE","DAYTON","OH","454690104","9372292919","CSE","7798","7923, 7945","$0.00","With the increasingly large volumes of data being generated in all fields, it is difficult to draw meaningful understanding from the information. Deep learning is a collection of new algorithms that have been developed recently to make it easier to understand large volumes of data. These algorithms typically have two phases of operation: training and inference. In the training phase, the algorithms learn how to interpret data, while in the inference phase the trained algorithms process new data based on what they learned earlier. Training generally requires high power computing. This project will develop novel computing systems for training that require low power consumption. This makes them suitable for portable systems, and hence could enable the design of significantly smarter products that learn continuously from their environment and are able to better interact with the environment. The proposed work includes outreach to K-12 students and also training of undergraduate, graduate, and minority students. <br/><br/>The novel computing systems to be developed will employ memristor circuits to accelerate the training phase of deep learning algorithms. Memristors are nanoscale resistive memory devices. The PIs will develop and characterize the memristors and then design deep learning circuits for training based on the characterized memristor devices. The PIs will also design computing systems based on the training circuits to be developed. These computing systems will have applications in a broad range of fields, including low power consumer products and high power clusters of computers."
"1759824","CDS&E: Collaborative Research: Machine Learning on Dynamical Systems via Topological Features","DMS","CDS&E-MSS","08/16/2017","09/21/2017","Firas Khasawneh","MI","Michigan State University","Standard Grant","Christopher Stark","08/31/2020","$88,004.00","","khasawn3@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","8069","8083, 9263","$0.00","Objects whose state changes over time, known as dynamical systems, describe a large number of natural and engineered processes; therefore, developing a deeper understanding of their behavior is of great importance.  While sometimes it is possible to derive mathematical models that describe the evolution of a dynamical system, these models are almost always an abstraction of the physical system and, therefore, have a limited ability to predict how the system will change in time.  Further, when the system under investigation is large or too complicated with several factors influencing its behavior, it may simply be impossible to describe the system with the corresponding descriptive equations.  Consequently, in the absence of adequate analytical models it becomes necessary to instrument the dynamical system with sensors and use the resulting data to understand its characteristics.  Specifically, the change in the state of a dynamic system is often governed by an underlying skeleton that gives the overall behavior a shape, and thus the shape of the skeleton directly governs the system behavior.  Most of the time, this shape of the underlying skeleton is unknown and can be easily masked by the complicated and rich system signals.  The emergent field of topological data analysis (TDA), a branch of mathematics that quantifies the shape of data, is capable of revealing information that is invisible to other existing methods by providing a high level X-ray of the skeleton governing the dynamics.  However, the information-rich structures provided by TDA still need to be interpreted in order to classify the dynamics and predict future outcomes.  To accomplish this, the principal investigators will leverage ideas from machine learning, a field of study that investigates algorithms that can learn from the data and use the acquired knowledge for classification and prediction.  However, the mathematical theory that elucidates how machine learning can operate on the features extracted using TDA currently does not exist.  Hence, this work will develop the necessary, novel mathematical and computational tools at the intersection of topological data analysis (TDA), dynamical systems, and machine learning.<br/><br/>The principal investigators seek to understand and formulate the foundations of machine learning when the important features of a dynamical system are summarized by descriptors generated with topological data analysis (TDA).  Although these signatures provide an information-rich structure for the evolution of the dynamics, current literature has only been utilizing a fraction of the available information in order to identify, predict, and classify different dynamic behavior.  One of the current impediments to further exploring the relationship between TDA and dynamical systems is the lack of machine learning theory that can operate on these structures. Therefore, the success of our effort will lead to (1) the establishment of a novel, general, and robust machine learning framework for studying dynamic signals via topological signatures, (2) better understanding of the relationship between TDA and dynamical systems via the use of these methods on real and synthetic data, and (3) the integration of the new knowledge into the investigators' educational programs, which will provide timely training of well-equipped next generation scientists and engineers."
"1656996","CRII: RI: New Methods for Learning to Personalize from Observational Data with Applications to Precision Medicine and Policymaking","IIS","CRII CISE Research Initiation","04/15/2017","04/10/2017","Nathan Kallus","NY","Cornell University","Standard Grant","Weng-keen Wong","03/31/2020","$175,000.00","","kallus@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","026Y","7495, 8228","$0.00","Personalization has long been a central problem in machine learning with successful applications in news and product recommendation, where training personalized recommendation models is usually based on repeated cheap experiments. A question of growing importance is how to translate this success to emergent problems such as precision medicine, where personalization appears to be key. The project will develop new and powerful methods, backed up by solid theory, to address increasingly urgent problems in personalization and its applications to precision medicine and policymaking. Moreover, the project will itself investigate applications to precision medicine and policymaking with an aim of developing specific guidelines that can be followed by practitioners. More generally, the research will lead to progress at the intersection of machine learning and causality, which in turn will advance our understanding of decision making from large-scale data. In precision medicine, the methods developed as part of this research will lead to improved patient outcomes through statistically efficient learning of the best way to personalize based on demographic and genetic characteristics. The research also has impact on policymaking, where personalization can be used to target educational interventions and improve the success of programs aimed at reducing recidivism, which in turn will reduce rates of incarceration and corrections spending. Implementations of the new personalization methods will be distributed as free, open-source packages for R and Python. These packages will provide a complete toolset for any doctor, sociologist, and other scientist or practitioner to develop highly effective personalization models for their application based solely on observational data. The research effort includes training and advising graduate and undergraduate students, with an emphasis on engaging with groups under represented in the field. Research results will be disseminated in public fora, including diversity-focused venues that offer an added outreach opportunity.<br/><br/>Medicine and related contexts have the property that experimentation can be prohibitively small-scale, costly, dangerous, and/or unethical, in comparison to passive data collection. Luckily, massive and ever expanding datasets are available, including hospitals' electronic medical records, with richer and richer data available from increased genotyping practices. However, such datasets are purely observational and non-experimental, where the isolated causal effect of a particular treatment is hidden by a myriad confounding factors and needs to be carefully mined out. Since, as it turns out, standard approaches to the problem based on predictive analyses fall short in this setting, this gives rise to urgently important methodological questions as to how to adapt the success of black-box machine learning to the prescriptive purpose of learning how to personalize treatments for maximal causal effect based on completely observational data. The purpose of this research project is to work toward advancing current machine learning methodology to step up to this emerging challenge by developing personalization theory, methods, and applications. Personalization is at the core of machine intelligence theory and applications. The problem of learning to personalize has been an exciting area of research over the last decade, with a strong focus on collaborative filtering and recommendation applications for web services. At the same time, among the machine learning community, there has been a tremendous growth of interest both in causal inference from observational data and in medical applications. Work on the research will result in advances in machine learning and causal inference and in stronger connections between machine learning, causal inference, personalization, and medicine."
"1717206","III: Small: Collaborative Research: High-Dimensional Machine Learning Methods for Personalized Cancer Genomics","IIS","Info Integration & Informatics","08/01/2017","09/01/2017","Quanquan Gu","VA","University of Virginia Main Campus","Continuing grant","Sylvia Spengler","11/30/2018","$300,000.00","","qgu@cs.ucla.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7364","7364, 7923","$0.00","The key to success in personalized and precision cancer genomics lies in: (1) discovering and understanding the molecular-level mechanisms of how genetic alterations influence various cellular processes relevant to cancer, and (2) utilizing molecular signatures to tailor more personalized treatment strategies for patients. In order to achieve these goals, various high-throughput experimental methods have been developed in recent years to obtain information about a patient's cancer genome sequence, mRNA expression, protein expression, epigenetic readout, and other detailed information about a patient's tumor. However, algorithms that fully harness such a massive amount of high dimensional data to yield biomedical insights are often lacking. This project will advance the field of data-driven complex modeling of cancer genomic data for personalized cancer treatment by developing novel algorithms that use emerging and new techniques in high-dimensional machine learning. The results of this research have the potential to impact both the machine learning field and the computational genomics field. The educational components integrated with the research program will develop new curriculum materials, involve undergraduate students and underrepresented groups in research, and train a new generation of interdisciplinary graduate researchers. <br/><br/>This project consists of two synergistic research thrusts to develop novel high-dimensional machine learning algorithms for analyzing high-throughput cancer genomic data. First,  the project will develop high-dimensional graphical models for multi-view data modeling to integrate data from heterogeneous genome-wide data sources. Second, it will devise novel high-dimensional collaborative learning methods for personalized drug recommendation. The high-dimensional graphical models will be used to estimate networks for different cancer subtypes.  These networks will then  be integrated into the recommendation algorithms, which in turn will help improve the multi-view graphical model estimation. This project will enhance the ability to interpret large-scale cancer genomics data by pinpointing the roles of complex molecular interactions in cancer onset and progression, which will enable  novel ways to more effectively discover personalized molecular signatures and more targeted potential treatments of cancer. Such technical innovation and conceptual advancement have the potential to reshape the way that one approaches graphical model estimation and its role in biological contexts. The project will potentially open up new possibilities for both theoreticians and practitioners in machine learning and computational biology as well as other disciplines."
"1734980","NCS-FO: Collaborative Research: A Neurally-Inspired, Event-Based Computer Vision Pipeline","IIS","IntgStrat Undst Neurl&Cogn Sys","10/01/2017","08/07/2017","Garrett Kenyon","NM","New Mexico Consortium","Standard Grant","Kenneth Whang","12/31/2019","$224,431.00","David Mascarenas","garkenyon@gmail.com","4200 West Jemez Road, Suite 301","Los Alamos","NM","875442587","5054124200","CSE","8624","8089, 8091, 8551, 9150","$0.00","The goal of this research is to make machines more intelligent by more closely mimicking biology, specifically, harnessing the information present in event-driven input along with top-down and lateral feedback.  Although computers continue to make inroads into everyday life, they still cannot perform many tasks that are readily performed by humans and other animals, or lag far behind their biological counterparts.  For example, on publically available datasets designed to measure performance on object detection tasks, the best computer software running on the fastest available hardware fails to locate between 20-40% of the human-annotated targets.  No animal would long survive, at least not in the wild, if its visual system made so many errors.  This project therefore will investigate how information processing strategies used by biological neural systems can be exploited to design more powerful computer hardware and software.   The project will have impact on training in neurally-inspired approaches to computer design, and technological leadership in neuromorphic computing for both defense and commercial uses.<br/><br/>The project will investigate processing mechanisms that are ubiquitous in biology but typically are not found in computer software and hardware.  The first of these mechanisms is event-driven input.  The retina sends visual information to the brain in the form of discrete pulses that propagate down the optic nerve.  Likewise, the cochlea encodes sound as action potentials or spikes that propagate along the auditory nerve.  In both cases, the precise time at which a spike occurs in a given nerve fiber, relative to the time at which spikes occur in other nerve fibers, can encode information that is critical to subsequent processing by the brain.  In the case of the retina, relative spike timing may help to separate foreground from background regions or even help us to read more quickly the words on this page.  In the cochlea, relative timing can be used to distinguish one sound source from another, which in turn allows us to hear what our companion is saying even while in a noisy room.  In contrast, the types of artificial neural networks most commonly used today do not employ event-driven input.  This research will test the hypothesis that event-driven input can be used to improve the performance of artificial neural networks on image and audio segmentation tasks.  The second biological processing mechanism to be investigated is top-down feedback.  Whereas most artificial neural networks studied today are strictly feed-forward, the vast preponderance of synapses in the brain arise from top-down and lateral feedback connections.  A mathematically tractable model of top-down and lateral feedback will be used to test the hypothesis that such connections can be used to improve the performance of artificial neural networks on object localization tasks.  Whether such feedback can reduce the susceptibility of networks to adversarial training will also be explored.  Finally, the project will explore whether a new type of neuromorphic chip that self-organizes in response to environmental input can learn more powerful representation from event-driven input, and develop strategies for combining such chips into hierarchical networks with lateral and top-down feedback."
"1734871","NCS-FO: Collaborative Research: A Neurally-Inspired, Event-Based Computer Vision Pipeline","IIS","IntgStrat Undst Neurl&Cogn Sys","10/01/2017","08/07/2017","Michael Flynn","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Kenneth Whang","09/30/2019","$110,000.00","Wei Lu, Zhengya Zhang","mpflynn@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8624","8089, 8091, 8551","$0.00","The goal of this research is to make machines more intelligent by more closely mimicking biology, specifically, harnessing the information present in event-driven input along with top-down and lateral feedback.  Although computers continue to make inroads into everyday life, they still cannot perform many tasks that are readily performed by humans and other animals, or lag far behind their biological counterparts.  For example, on publically available datasets designed to measure performance on object detection tasks, the best computer software running on the fastest available hardware fails to locate between 20-40% of the human-annotated targets.  No animal would long survive, at least not in the wild, if its visual system made so many errors.  This project therefore will investigate how information processing strategies used by biological neural systems can be exploited to design more powerful computer hardware and software.   The project will have impact on training in neurally-inspired approaches to computer design, and technological leadership in neuromorphic computing for both defense and commercial uses.<br/><br/>The project will investigate processing mechanisms that are ubiquitous in biology but typically are not found in computer software and hardware.  The first of these mechanisms is event-driven input.  The retina sends visual information to the brain in the form of discrete pulses that propagate down the optic nerve.  Likewise, the cochlea encodes sound as action potentials or spikes that propagate along the auditory nerve.  In both cases, the precise time at which a spike occurs in a given nerve fiber, relative to the time at which spikes occur in other nerve fibers, can encode information that is critical to subsequent processing by the brain.  In the case of the retina, relative spike timing may help to separate foreground from background regions or even help us to read more quickly the words on this page.  In the cochlea, relative timing can be used to distinguish one sound source from another, which in turn allows us to hear what our companion is saying even while in a noisy room.  In contrast, the types of artificial neural networks most commonly used today do not employ event-driven input.  This research will test the hypothesis that event-driven input can be used to improve the performance of artificial neural networks on image and audio segmentation tasks.  The second biological processing mechanism to be investigated is top-down feedback.  Whereas most artificial neural networks studied today are strictly feed-forward, the vast preponderance of synapses in the brain arise from top-down and lateral feedback connections.  A mathematically tractable model of top-down and lateral feedback will be used to test the hypothesis that such connections can be used to improve the performance of artificial neural networks on object localization tasks.  Whether such feedback can reduce the susceptibility of networks to adversarial training will also be explored.  Finally, the project will explore whether a new type of neuromorphic chip that self-organizes in response to environmental input can learn more powerful representation from event-driven input, and develop strategies for combining such chips into hierarchical networks with lateral and top-down feedback."
"1657179","CRII: RI: Matching Image Features with Correctness Predictions","IIS","CRII CISE Research Initiation, EPSCoR Co-Funding","09/01/2017","07/15/2019","Victor Fragoso","WV","West Virginia University Research Corporation","Standard Grant","Jie Yang","08/31/2020","$127,955.00","","victor.fragoso@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","026Y, 9150","7495, 8228, 9150","$0.00","The project develops methods for measuring the correctness of image-feature correspondences, which helps systems to identify good and bad correspondences. The project investigates robust estimators and 3D-point cloud compression algorithms that leverage the correctness measures of image-feature correspondences to increase their effectiveness. The developed methods can have many applications, such as image stitching for the creation of panoramas, 3D model reconstruction from photo collections, vision-based navigation in self-driving cars and robots, and augmented reality. The project integrates research with education by developing 3D computer vision courses and involving undergraduate and graduate students in this fundamental research effort. <br/><br/>This research develops theoretically grounded confidence measures for classifiers using the statistical theory of extreme values. In particular, the project investigates confidence measures for the nearest neighbor classifier, a widely used classifier in many computer vision and other applications. The project investigates the confidence measures in two computer vison tasks: robust estimation and scene compression. The research focuses on using the developed confidence measures to enable the adaptive use of non-minimal samples for hypotheses generation and fast convergence in RANSAC-based estimators. Furthermore, the project develops a scene-compression algorithm based on a convex optimization method, of which objective function considers spatial coverage and visual distinctiveness. The project studies ways to enforce visual distinctiveness using confidence measures derived from this work and applies them to different applications to demonstrate the robustness and efficiency of estimating the correctness of image-feature correspondences."
"1706130","Multiscale plenoptic imaging and direct computation of turbulent channel flows laden with finite-size solid particles","CBET","PMP-Particul&MultiphaseProcess","07/01/2017","12/20/2019","Lian-Ping Wang","DE","University of Delaware","Standard Grant","William Olbricht","06/30/2021","$300,000.00","Jingyi Yu","lwang@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","ENG","1415","9150","$0.00","CBET - 1706130<br/>PI: Wang, Lian-Ping<br/><br/>Turbulent flows that contain particles occur in a variety of industrial, biological, and environmental processes.  The analysis of these flows is challenging, because interactions between the suspended particles and the surrounding fluid not only affect dynamics of the particles, but also influence characteristics of the overall flow.  This award will support development of computational and experimental tools to better understand how particle-fluid interactions at the particle scale influence transport in turbulent flow at large scales.  An imaging system will be constructed to record three-dimensional velocity fields of particle-laden fluids in turbulent open-channel flow.  Experimental results will be compared with numerical simulations of the identical flow to validate the simulation methods and provide additional insight into relationships between particle dynamics and flow.  The research team will incorporate results from the project in a new short course that will combine computational fluid dynamics with new techniques in computational imaging and computer vision.  An instructional module illustrating these methods will be developed for high school students and teachers participating in the High School Summer Research program at the University of Delaware.<br/><br/>A plenoptic particle tracking velocimetry system will be constructed that can measure the three-component velocity fields in three-dimensional, turbulent, particle-laden flows at both micro- and macro-scales. This velocimetry method uses a plenoptic camera that can record many images of a collection of particles from different viewpoints and angles at the same time.  Then, computer vision algorithms are applied to recover accurately the instantaneous positions of the particles in three dimensions, which can be used to find the velocity fields.  In addition, the particle tracking system in this project will be designed to capture the rotational velocity of individual particles. In parallel, a highly scalable particle-resolved simulation tool based on the mesoscopic lattice Boltzmann approach will be applied to resolve the three-dimensional motion at all scales.  Results from the simulations will be compared with experimental data at all scales.  At the particle scale, these data include trajectories, velocities, accelerations, and angular velocities of particles, particle-wall and particle-free surface interactions, as well as local flow statistics near the surface of a solid particle. At the system scale, the tools will be used to study turbulence modulation, flow drag, and flow transition in the presence of the finite-size solid particles."
"1734633","NRI: INT: SCHooL: Scalable Collaborative Human-Robot Learning","IIS","NRI-National Robotics Initiati","09/01/2017","08/18/2017","Ken Goldberg","CA","University of California-Berkeley","Standard Grant","David Miller","08/31/2021","$1,374,893.00","Anca Dragan, Pieter Abbeel, Stuart Russell","goldberg@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8013","8086","$0.00","To be useful in warehouses, homes, and other environments from schools to retail stores, robots will need to learn how to robustly manipulate a wide variety of objects.  For instance, to enhance the productivity of human workers, service and factory robots could keep specified surfaces clear by identifying, grasping, and relocating objects to appropriate locations.  Pre-programming robots to perform such complex manipulation tasks is not feasible; instead this project will investigate scalable robot manipulation, where multiple robots collaboratively learn from multiple humans.  The project will contribute new models, algorithms, software, and experimental data to advance the state-of-the-art in deep learning, human-robot interaction, and cloud robotics.  To broadly convey the results of this research to students and the public, the project will create a book and video with the Lawrence Hall of Science and the African Robotics Network.<br/> <br/>Two primary gaps in current understanding of co-robotic Learning from Demonstration (LfD) are: 1) the absence of a theoretical framework that encompasses humans and robots to produce cooperative learning behaviors as optimal solutions; and 2) the lack of research linking LfD with deep learning, hierarchical planning, and human-robot interaction. The project addresses those gaps with a unified theoretical framework based on Inverse Reinforcement Learning and game-theoretic models of communication between humans and robots, treating LfD as a scalable co-robotic process in which multiple humans and multiple networked robots work in a distributed set of environments to maximize a collective set of reward functions and humans learn how to become more effective demonstrators for robots. The research can be applied to almost any context where robots can learn from human demonstrations and will be evaluated in ""surface decluttering"" benchmarks of increasing complexity over the course of the project."
"1745125","CAREER: Exact Algorithms for Learning Latent Structure","IIS","Robust Intelligence","01/01/2017","07/28/2017","David Sontag","MA","Massachusetts Institute of Technology","Standard Grant","Rebecca Hwa","06/30/2020","$350,764.00","","dsontag@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","1045, 7495","$0.00","One of the fundamental tasks in science is to infer the causal relationships between variables from data, and to discover hidden phenomena that may affect their outcome. We can attempt to automate this scientific process by searching over probabilistic models of how the observed data might be influenced by unobserved (latent) factors or variables. Machine learning of such models provides insight into the underlying domain and a means of predicting the latent factors. However, it is challenging to search over the exponentially many models, and existing algorithms are unable to scale to large amounts of data.<br/><br/>The goal of this CAREER award will provide novel algorithms to circumvent this computational intractability. Based on a classical idea in statistics called the method-of-moments, the new algorithms will be applied in bioinformatics to discover regulatory modules from disease expression profiles, and in health care to predict a patient's clinical state using data from their electronic medical record. A key component of the project is to involve high school students from disadvantaged backgrounds in the research to inspire them to pursue STEM careers.<br/><br/>The project advances machine learning by introducing several new techniques for unsupervised and semi-supervised learning of Bayesian networks. The project overcomes the computational challenges associated with maximum-likelihood estimation by developing new method-of-moment based algorithms for learning latent variable models, focusing on settings where inference itself may be intractable. This includes Bayesian networks of discrete variables where a top layer consists of latent factors and a bottom layer consists of the observed data, a form of discrete factor analysis. The proposed algorithms run in polynomial time and are guaranteed to learn a close approximation to the true model.<br/><br/>The techniques developed as part of this project have the potential to be transformative in the social and natural sciences by enabling the efficient and accurate discovery of latent variables from discrete data. Furthermore, in collaboration with emergency department clinicians, the new algorithms will be applied to learn models relating diseases to symptoms from noisy and incomplete data that is routinely collected as part of electronic medical records. This will advance the field of machine learning in health care by providing algorithms that generalize between institutions without the need for a large amount of labeled training data.<br/><br/>The insights about exploratory data analysis developed as part of this project will be integrated into innovative curriculum in data science, both as part of an undergraduate class and new Master's classes. The project will bring students from nearby high schools to NYU throughout the academic year and during the summer to learn about machine learning through participation in the proposed research, having them use the unsupervised learning algorithms to discover new medical insights. The PI will also develop and deliver tutorials on machine learning to clinicians and the health care industry."
"1657155","CRII: III: Robust Machine Learning Methods for Messy Data","IIS","CRII CISE Research Initiation","05/01/2017","03/01/2017","James Zou","CA","Stanford University","Standard Grant","Sylvia Spengler","08/31/2019","$174,999.00","","jamesz@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","026Y","7364, 8228","$0.00","Messy data is ubiquitous in modern science. Data come from heterogeneous sources; there are many latent confounding factors; and it is often unclear what are the relevant questions to ask and models to use. This reality is in sharp contrast with the usual modeling assumptions of machine learning and statistics, where data are assumed to come from well-specified models and the hypotheses to test are clearly laid out. The glaring gap between standard theory and the actual practice of messy data is a major contributor to the reproducibility crises across science and prevents researchers from harnessing the full insights from data. This project will develop rigorous mathematical foundations and robust machine learning algorithms to address the core challenges of messy data. The PI will explore novel techniques to quantify and reduce different types of selection biases that arise from exploratory data analysis. The PI will also investigate algorithms to perform statistical inference when the model is mis- or under-specified. The project will apply these new methods to tackle challenging problems in human population genomics. <br/><br/>The PI recently initialized a framework based on information usage to quantify the magnitude of over-fitting and bias arising from data exploration. This project will significantly expand this framework. In particular the PI will apply this information usage approach to quantify and reduce bias in data generated from adaptive experimentation, such as online A/B testing and more general multi-arm bandits. Related to over-fitting is the problem of mis- and under-specified statistical models. The PI has recently developed method-of-cumulant approaches to learn probabilistic models when the observations are perturbed by unknown and arbitrary interference. A promising direction of research is to extend this approach to more general settings that allow for nonlinear interference and to develop software tools for the broad data science community. Genomics exemplify many of the challenges of messy data-genomic data typically requires substantial exploratory analysis and faces modeling uncertainty. This makes genomics a high impact domain to apply the new messy data algorithms developed here. Bio-medical databases are interactively analyzed by many researchers and thus are particularly prone to exploration bias and overfitting. The PI will explore piloting the information usage framework on the bio-medical data hubs being created at Stanford in order to quantify and reduce exploration bias. As a part of the project, PI is also developing courses, workshops and tutorials to bring together researchers and practitioners across machine learning, statistics, information theory and bio-medical data science to address the ubiquitous challenge of messy data."
"1701289","Dissertation Research: Revealing the spatial distribution of risk in animal groups","IOS","Animal Behavior","04/01/2017","03/17/2017","Daniel Rubenstein","NJ","Princeton University","Standard Grant","Jodie Jawor","03/31/2019","$13,190.00","Matthew Grobis","dir@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","BIO","7659","9178, 9179","$0.00","Non-technical Abstract<br/>Animal groups are remarkable for their ability to interact with the environment in a way that individuals are unable to, such as through migration, collective intelligence, or predator avoidance. For example, predators attacking highly coordinated fish schools have little success due to how efficient information transmission is in the group. The technological challenges of filming and tracking large groups of animals have limited our understanding of how they are so effective at predator avoidance, but recent advances in computer vision and high-speed filming now allow us to accurately recreate these information transmission networks and show how information moves through animal groups. Insights into these networks have put forth predictions that are at odds with long-established hypotheses on where in the group animals are safest: classic behavioral ecology theory suggests the center, while new network data suggests the edge of the group. The research carried out here will for the first time test these competing hypotheses. Live interactions between a northern pike predator and schools of golden shiners will be filmed in the laboratory and then recreated from a sensory network perspective using sophisticated computer vision software to better understand how information is transferred and the ramifications of individual location in the group. The experiment will provide insights into the fields of behavioral ecology, sensory ecology, game theory, and network science. It will provide scientific and statistical training to graduate and undergraduate students, and findings will be disseminated at scientific conferences, through blogging, and through science outreach to local high schools.<br/><br/>Technical Abstract<br/>For decades, Hamilton's Selfish Herd theory has served as the expectation for the spatial distribution of predation risk in animal groups. In this model, cohesive grouping emerges as animals move to position other individuals between themselves and a potential hidden predator, hence minimizing their ""domain of danger"". Support for this theory has been mixed for mobile animal groups such as fish schools, however, because the Selfish Herd theory does not allow for prey to respond to the predator. Real predator-prey interactions, on the other hand, are dynamic. Until very recently, directly testing Hamilton's Selfish Herd theory in fish schools has been impossible due to technological limitations on the quality and quantity of behavior data. Recent advances in computer vision and high-speed cameras, however, now allow for accurate measures of the fine-scale movements of all members of fish schools, as well as estimations of the visual information available to them. Networks constructed from this visual information are an accurate predictor of how movement decisions transfer through schools of fish. Here research will directly test the non-exclusive hypotheses of whether spatial positioning or network structure is a more accurate predictor of mortality risk by filming predator-prey interactions between golden shiners and a northern pike predator in the laboratory."
"1833287","CAREER: Machine Learning through the Lens of Economics (And Vice Versa)","IIS","Robust Intelligence","09/01/2017","02/11/2019","Jacob Abernethy","GA","Georgia Tech Research Corporation","Continuing Grant","Rebecca Hwa","01/31/2020","$310,469.00","","prof@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","1045, 7495","$0.00","Machine Learning (ML) is the study of leveraging data and computational resources to obtain prediction and decision-making algorithms that function well in the presence of uncertainty. The techniques employed to design and study ML algorithms typically involve concepts and tools from probability, statistics, and optimization; the language of economics, on the other hand, is conspicuously absent. It is rare to encounter terms such as marginal price, utility, equilibrium, risk aversion, and such, in the ML research literature. This gap is significant and belies the reality that the broad interest in Machine Learning, and its sudden growth spurt as a research field, can be ascribed to its potential for generating economic value across many segments of society. This NSF CAREER projectadvances an already-emerging relationship between Machine Learning and the fields of microeconomic theory and finance. This will begin with the development of mathematical tools that enable a semantic correspondence between learning-theoretic objects and economic abstractions. For example, the project shows that many algorithms can be viewed as implementing a market economy, where learning parameters are associated with prices, parameter updates are viewed as transactions, and under certain conditions learned hypotheses can be extracted as market-clearing price equilibria. In addition to developing this link, the project research raises a number of intriguing questions and explores several surprising and novel applications with benefits to computer science more broadly. <br/><br/>Among several such applications stemming from the new theoretical connections are:<br/>1. Developing new models for distributed computing for learning and estimation tasks: The economic lens gives new insights into a robust and effective model for decentralization of data-focused tasks.<br/>2. Designing new techniques for crowdsourcing and labor decentralization via collaborative mechanisms involving financial payment schemes: This builds off of the success of platforms like Amazon's Mechanical Turk as well as the Netflix Prize and the prediction challenge company Kaggle.<br/>3. Developing a market-oriented model for data brokerage and financially-efficient learning: As information is increasingly traded in market environments, we aim to answer questions such as ""what is the marginal value of a unit of data?""<br/><br/>The project will also develop the Michigan Prediction Team, a data-science focused program for formulating and solving prediction and learning challenges that develop from across the University of Michigan as well as externally. The group primarily targets undergraduates with graduate student mentors, and Team has a strong interdisciplinary focus."
"1741341","BIGDATA: F: Towards Automating Data Analysis: Interpretable, Interactive, and Scalable Learning via Discrete Probability","IIS","Big Data Science &Engineering","10/01/2017","09/19/2017","Suvrit Sra","MA","Massachusetts Institute of Technology","Standard Grant","Sylvia Spengler","09/30/2021","$1,024,363.00","Stefanie Jegelka","suvrit@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8083","7433, 8083","$0.00","As machine learning (ML) permeates all areas of science and technology, demands in diverse data domains, inference questions, resource limitations and reliability fuel several new conceptual and algorithmic challenges. Examples of current shortcomings that limit the full use of machine learning include suboptimal use of data and algorithms; painstaking hand-tuning and model search; validation of results and  difficulties in generalization; limited interactivity with humans; encoding of domain knowledge; and lack of interpretability, among others. Progress on these questions has the potential to impact the successful adoption and use of machine learning in a broad range of fields. With the above motivation, the goal of this project is to create a novel suite of models and algorithms for analyzing complex datasets, with a particular focus on the following three factors crucial for next-generation machine learning: (1) interpretability; (2) interactivity; and (3) automated learning. The overarching technical concept underlying this proposal is the concept of negative dependence in discrete probability. This project lays theoretical foundations for a new set of tools grounded in this concept. Besides practical impacts, the methods to be studied in the project motivate new theoretical questions, and will help increase interest in the underlying mathematics.<br/><br/>The practical impact of the proposed work has the potential to benefit society on multiple fronts. Via collaborations, the PIs will evaluate the developed methods in healthcare (seeking to ultimately impact patient care and well-being), systems biology (to help with research on cancer and diabetes, among others), and materials science (to help discover safer, functional materials more efficiently). The project will also directly have educational impact: training of graduate students, providing material for data science courses at all levels, and outreach to the community via general talks as well as focused lectures at conferences and workshops, including workshops and events targeted at women in Data Science. <br/><br/>Technically, the PIs will develop: (1) New tools, models, and algorithms for interactive data analysis, especially for experimental design, information collection, interpretable machine learning, hypothesis testing, performance validation, and architecture learning; (2) Theoretical analysis, such as convergence and complexity (statistical and computational); and (3) Open-source implementations of all key algorithms and frameworks."
"1738305","D3SC: EAGER: Data-driven development of fluorescent sensors for bio-imaging","CHE","Chemical Measurement & Imaging, CESER-Cyberinfrastructure for","08/01/2017","07/05/2017","Ming Xian","WA","Washington State University","Standard Grant","Kelsey Cook","07/31/2020","$300,000.00","Shuiwang Ji","mxian@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","MPS","6880, 7684","7234, 7433, 7916, 8084","$0.00","Chemical information is growing dramatically, fueled by the massive amount of data generated by researchers in a variety of fields. Conventional approaches require that scientists sift through data from many sources to try to develop a comprehensive picture of the field when making a new chemical compound or studying a chemical processes.  This conventional approach is slow and difficult as researchers can miss important trends.  With the support from the Chemical Measurement and Imaging (CMI) Program in the Division of Chemistry and the Cyberinfrastructure for Emerging Science and Engineering Research (CESER) Program in the Office of Advanced Cyberinfrastructure, Professor Xian at Washington State University (WSU) and Professor Ji are teaching machines to collect massive data from literature, analyze them, and come up with new design principles to make new sensors.  The project is in response to the Data-Driven Discovery Science in Chemistry Dear Colleague Letter (D3SC-DCL).  The team is using data mining and computer learning to develop a new generation of sensors for the detection of hydrogen sulfide (H2S).  Hydrogen sulfide is an important signaling molecule that is associated with biological processes such as high blood pressure, atherosclerosis, coronary heart diseases.  The coupling of computer science techniques with chemical problems enables Professors Xian and Ji and their students to predict the most promising sensor design without making and testing a large number of sensors empirically. The research group ""trains their computer"" by testing the predicted sensors designs and providing feedback for the next iteration; this is machine learning.  If successful, the machine learning methods may be expanded to sensor development for the detection of many different compounds, especially those that are important in biological and chemical processes (such as biological warfare and pharmaceutical development). The research provides unique training opportunities for undergraduate and graduate students by providing rich experiences in both chemistry and data science.  These activities build the workforce of non-traditional chemistry trainees to meet data-driven research and development needs in industry and academia. Professor Xian also actively works with undergraduate students from underrepresented minority groups by participating the Pacific Northwest Louis Stokes Alliance for Minority Participation (PNW-LSAMP) Program at WSU.<br/><br/>Professors Xian and Ji are building a database of H2S sensors with searchable parameters.  They are carrying out data-driven optimization of the sensors based on advanced machine learning and data mining techniques. A combined data-driven discovery framework of unsupervised learning and supervised multi-task learning is developed to predict the important properties of the sensors. This approach is used to identify the most suitable fluorophores and H2S-reaction sites for the design of optimal sensors, which can then be synthesized and validated. These studies may advance our understanding of machine learning and data mining as well as chemical predication and sensor development. This interdisciplinary project provides a unique platform to attract students to chemistry and train them at the interface of chemistry, data science, and computation science. Broadening participation efforts include students from underrepresented minority groups through the Pacific Northwest Louis Stokes Alliance for Minority Participation (PNW-LSAMP) Program at WSU."
"1734260","The effect of nameability on categorization","BCS","Perception, Action & Cognition, ECR-EHR Core Research","09/01/2017","08/23/2017","Gary Lupyan","WI","University of Wisconsin-Madison","Standard Grant","Michael Hout","08/31/2020","$546,577.00","Gregory Zelinsky, Haley Vlach","lupyan@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","SBE","7252, 7980","7252, 8817","$0.00","Humans have remarkable abilities to find deep similarities in what are superficially different experiences and to use those similarities to help categorize their experiences. This process of categorization allows people to learn from examples and to generalize that knowledge to new situations. Categorization is also a fundamental aspect of human reasoning. This work investigates whether a key contributor to human categorization success is the ability to name the features and dimensions that compose the categories. Testing the causal link between naming and categorizing helps us understand why children with poorer language skills often go on to have poor academic outcomes, even in domains that appear to be nonverbal in nature like geometry. This work enhances scientific infrastructure by developing a freely available online tool for measuring the nameability of any visual or auditory set of items for adults and children, which will help researchers and educators more effectively describe learning strategies and allow for better understanding the sources of children's reasoning errors.<br/><br/>To explain what makes some categories harder to learn than others, researchers have typically posited a fixed set of features that are available to the learner. But where do the features come from? This work tests the hypothesis that an important source of features is the words people learn when learning a language. On this view, the words of a language do not simply map onto pre-existing conceptual distinctions, but are one of the contributing factors that create the distinctions. This hypothesis is tested using a series of category-learning and category-induction experiments with adults and a unique population of linguistically deprived children. The experiments systematically test the extent to which ease of naming predicts categorization success and tests the causal involvement of language by using verbal interference protocols. The behavioral work is guided by computational modeling using convolutional neural networks to help disentangle whether a category is easy to learn because it is nameable or whether it is nameable because it is easy to learn. This work is among the first to use deep-learning models to understand human categorization."
"1723128","Collaborative Research:  Connecting Submodularity and Restricted Strong Convexity","DMS","CDS&E-MSS","09/01/2017","08/28/2019","Sahand Negahban","CT","Yale University","Continuing Grant","Christopher Stark","08/31/2021","$160,000.00","","sahand.negahban@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","MPS","8069","8083, 9263","$0.00","Structured estimation problems arise in a variety of contexts including astronomy, genomics, and computer vision. This project aims to develop methods that can use the additional structure in order to estimate statistical models effectively, while also using the structure for computational improvements. This work seeks to connect ideas in combinatorial optimization and statistical estimation to develop computationally tractable methods for performing structured statistical estimation.<br/><br/>This project provides an integrated program to explore and connect combinatorial optimization and statistical estimation. Modern statistical challenges have become increasingly dependent on understanding both the computational and statistical issues. Many modern statistical estimation problems rely on imposing additional structure in order to reduce the statistical complexity and provide interpretability. Unfortunately, these structures often are combinatorial in nature and result in computationally challenging problems. In parallel, the combinatorial optimization community has placed significant effort in developing algorithms that can approximately solve such optimization problems in a computationally efficient manner. The focus of this project is to expand upon ideas that arise in combinatorial optimization and connect those algorithms and ideas to statistical questions. The research directions of this project are split into three main thrusts unified by the concept of weak submodularity: (a) cardinality constrained optimization and its applications to general statistical optimization problems; (b) matrix estimation problems including low-rank matrix estimation and semi-definite programming problems as well as problems in sparse dictionary learning; and (c) a general theoretical understanding of weak submodularity and specifically analyzing how to develop algorithms in this regime that work well for large-scale datasets."
"1723052","Collaborative Research:  Connecting Submodularity and Restricted Strong Convexity","DMS","CDS&E-MSS","09/01/2017","08/18/2017","Georgios-Alex Dimakis","TX","University of Texas at Austin","Standard Grant","Christopher Stark","08/31/2020","$90,000.00","","dimakis@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","MPS","8069","8083, 9263","$0.00","Structured estimation problems arise in a variety of contexts including astronomy, genomics, and computer vision. This project aims to develop methods that can use the additional structure in order to estimate statistical models effectively, while also using the structure for computational improvements. This work seeks to connect ideas in combinatorial optimization and statistical estimation to develop computationally tractable methods for performing structured statistical estimation.<br/><br/>This project provides an integrated program to explore and connect combinatorial optimization and statistical estimation. Modern statistical challenges have become increasingly dependent on understanding both the computational and statistical issues. Many modern statistical estimation problems rely on imposing additional structure in order to reduce the statistical complexity and provide interpretability. Unfortunately, these structures often are combinatorial in nature and result in computationally challenging problems. In parallel, the combinatorial optimization community has placed significant effort in developing algorithms that can approximately solve such optimization problems in a computationally efficient manner. The focus of this project is to expand upon ideas that arise in combinatorial optimization and connect those algorithms and ideas to statistical questions. The research directions of this project are split into three main thrusts unified by the concept of weak submodularity: (a) cardinality constrained optimization and its applications to general statistical optimization problems; (b) matrix estimation problems including low-rank matrix estimation and semi-definite programming problems as well as problems in sparse dictionary learning; and (c) a general theoretical understanding of weak submodularity and specifically analyzing how to develop algorithms in this regime that work well for large-scale datasets."
"1719578","Generalized Matrix Functions: Theory, Algorithms, and Applications","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","05/10/2017","Michele Benzi","GA","Emory University","Standard Grant","Leland Jameson","07/31/2020","$250,016.00","","mbenzi@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","MPS","1271","9263","$0.00","In recent years the scientific enterprise, like daily existence, has been greatly affected by the availability of new technologies that have enabled the collection of an unprecedented amount of data. This continuous creation of enormous amounts of raw digital information demands new, efficient ways to extract useful content and filter out the noise inherent in any data-gathering process. Mathematical and computational techniques of data analysis offer powerful tools that can be brought to bear, but new challenges arise on a daily basis. This requires the constant refinement and improvement of existing techniques, as well as the development of new ones. This research project aims to produce a new body of mathematical knowledge and new computational techniques that will enhance the ability to tackle challenges arising from data-intensive fields of science and engineering including computer vision, compressed sensing, control, and others. Quantitative finance, network analysis and synthesis, and medical imaging are other areas where the research can be expected to have an impact. The principal investigator will study a class of mathematical objects known as generalized matrix functions and aims to exploit the resulting knowledge to develop new, efficient computer methods for data analysis. Training of a PhD student in computational mathematics is also an integral part of the project.<br/><br/>The principal investigator aims to develop the theory of generalized matrix functions, a type of matrix function based on the singular value decomposition of a (possibly rectangular) matrix. The resulting theory is intended to be the basis for the development of numerical methods for the efficient approximate evaluation of such matrix functions. The focus will be primarily on large-scale problems for which the (full) singular value decomposition cannot be computed. Techniques based on sparsity and low-rank approximations will be combined with Krylov-type methods (especially the Lanczos and Golub--Kahan algorithms) to design fast algorithms for solving a variety of problems involving generalized matrix functions. The algorithms will be applied to problems such as low-rank matrix optimization, the regularization of discrete inverse problems, and the analysis of directed networks. As a by-product of this research, new algorithms for the computation of standard matrix functions where the matrix argument is only available in factored form will be derived and analyzed. This research represents a new direction in numerical linear algebra and is expected to produce useful numerical tools for the solution of a variety of problems in data science and optimization."
"1709097","Geometry and Topology of Convex Projective Manifolds","DMS","TOPOLOGY","09/01/2017","05/24/2017","Samuel Ballas","FL","Florida State University","Standard Grant","Swatee Naik","08/31/2021","$139,278.00","","ballas@math.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","MPS","1267","","$0.00","Projective geometry is the geometry of perspective, whose practitioners over time have included Greek philosopher/mathematicians studying incidence properties of lines in the plane, Renaissance artists attempting to render more realistic frescoes, and computer scientists pioneering computer graphics and vision techniques. This geometry comes from projecting points in a higher dimensional space along lines to a lower dimensional projective space. Unlike Euclidean geometry, this geometry has no well-defined notions of distance or angle. Its only meaningful geometric notion is incidence (for example, intersections of lines and inclusion of points in lines). In principle, this inability to measure distance initially seems like a drawback; however, in practice it provides a unified framework for studying seemingly disparate and incongruous geometries simultaneously. For example, projective space has pieces that serve as models for the familiar Euclidean geometry, the non-Euclidean spherical and hyperbolic geometries, and other exotic geometries, such as de Sitter and anti de Sitter space, that are of interest in modern physics. Recently, there has been increased interest in properly convex domains, which are interesting pieces of projective space that share many properties with hyperbolic space but enjoy interesting deformation properties absent in the hyperbolic setting. A primary focus of this project is to produce more of these properly convex examples and to understand their geometric, dynamic, and algebraic properties in a systematic fashion.  Due to built-in connections with perspective and computer vision, many of the low dimensional examples under study in this project can be effectively visualized and rendered with the aid of a computer to produce vibrant dynamic graphics. This feature will allow the involvement of students with limited mathematical background in portions of the research as well as conveying the spirit of many of the important results to the broader non-mathematical community. <br/><br/>Properly convex domains are subsets of projective space that are disjoint from a projective hyperplane and convex in the affine space produced by removing such a hyperplane from projective space. Hyperbolic space serves as the prime example of a properly convex domain via the Klein model. Properly convex domains and their quotients by discrete groups share many properties with hyperbolic space and hyperbolic orbifolds. A main point of this proposal is to understand how familiar concepts in hyperbolic geometry manifest themselves in properly convex geometry. Three main aims of the project are 1) developing a properly convex theory of Dehn surgery that can be used to produce examples of closed properly convex manifolds from non-compact ones, 2) investigating how dynamic properties of the fundamental group of a projective manifold manifest themselves geometrically, in analogy with geometric finiteness for hyperbolic manifolds, and 3) using properly convex structures to produce subgroups of the special linear group with interesting algebraic properties (such as thinness). In addition to the obvious potential to better understand geometric and dynamical aspects of properly convex manifolds, this project should also yield a deeper understanding of hyperbolic geometry by elucidating which geometric features of hyperbolic geometry are consequences of uniform negative curvature and which are consequences of more general geometric structure."
"1705047","CSR: Medium: Collaborative Research: Scale-Out Near-Data Acceleration of Machine Learning","CNS","CSR-Computer Systems Research","10/01/2017","07/20/2020","Nam Sung Kim","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Matt Mutka","09/30/2021","$600,000.00","","nskim@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7354","7924","$0.00","A growing number of commercial and enterprise systems increasingly rely on machine learning algorithms. This shift is, on the one hand, due to the breakthroughs in machine learning algorithms that extract insights from massive amounts of data. Therefore, such systems need to process ever-increasing amounts of data, demanding higher memory bandwidth and capacity. However, the bandwidth between processors and off-chip memory has not increased due to various stringent physical constraints. Besides, data transfers between the processors and the off-chip memory consume orders of magnitude more energy than on-chip computation due to the disparity between interconnection and transistor scaling.<br/><br/>Exploiting recent 3D-stacking technology, the researcher community has explored near-data processing architectures that place processors and memory on the same chip. However, it is unclear whether or not such processing-in-memory (PIM) attempts will be successful for commodity computing systems due to the high cost of 3D-stacking technology and demanded change in existing processor, memory and/or applications. Faced with these challenges, the PIs are to investigate near-data processing platforms that do not require any change in processor, memory and applications, exploiting deep insights on commodity memory subsystems and network software stack. The success of this project will produce inexpensive but powerful near-data processing platforms that can directly run existing machine learning applications without any modification.<br/>"
"1832811","CAREER: Machine Learning-Based Approaches Toward Combatting Abusive Behavior in Online Communities","IIS","HCC-Human-Centered Computing","08/16/2017","04/20/2020","Eric Gilbert","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","William Bainbridge","01/31/2021","$482,075.00","","eegg@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7367","1045, 7367","$0.00","This research aims to computationally model abusive online behavior to build tools that help counter it, with the goal of making the Internet a more welcoming place.  Since its earliest days, flaming, trolling, harassment and abuse have plagued the Internet. This project will lay bare the structure of online abuse over many types of online conversations, a major step forward for the study of computer-mediated communication. This will result from modeling abuse with statistical machine learning algorithms as a function of theoretically inspired, sociolinguistic variables, and will entail new technical and methodological advances. This work will enable a transformative new class of automated and semi-automated applications that depend on computationally generated abuse predictions. The education and outreach plan is deeply tied to the research activities, and focuses on scaling-up the research's broader impacts.  A public application programming interface (API) will enable developers and online community managers around the world to integrate into their own sites the defenses against abuse developed by this research.<br/><br/>The work will consist of two major phases. In the first, the research will develop a deep understanding of abusive online behavior via statistical machine learning techniques. Specifically, the work will appropriate theories from social science and linguistics to inform the creation of features for robust statistical machine learning algorithms to predict abuse. These proposed abuse models will enable a brand new, transformative class of mixed-initiative artifacts capable of intervening in social media and online communities. In the second phase, this project will explore this newly enabled class of artifacts by building, deploying and evaluating sociotechnical tools for combatting abuse. Specifically, it will explore two classes of tools that use the abuse predictions: shields and moderator tools. The first, shields, will proactively block inbound abuse from reaching people. The second class of tools, moderator tools, will flag and triage abuse for community moderators."
"1701026","PFI:AIR-TT:  Automating Animal Welfare Tasks to Improve Animal Health and Human Wellbeing","IIP","Accelerating Innovation Rsrch","06/15/2017","06/21/2017","Donald Brightsmith","TX","Texas A&M AgriLife Research","Standard Grant","Jesus Soriano Molla","11/30/2019","$197,129.00","","dbrightsmith@cvm.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454375","9798626777","ENG","8019","8019","$0.00","This PFI: AIR Technology Translation project focuses on using research on the benefits of positive reinforcement training to create a digital system that improves animal wellbeing while automating tasks related to common types of animal care. This digital enrichment system is important because animal husbandry, especially tasks related to animal welfare, take a great deal of caretaker time, money, and effort that can cause caretaker burnout. Contented animals are important to multiple industries: research animals undergo significant physiological and behavioral shifts under stress, which biases scientific studies; contented farm animals have increased immune function and need less medication; conservation breeding of endangered species depends on healthy breeding animals; and ease in pet keeping encourages the health benefits of pet ownership. <br/><br/>The project will result in a manufacture ready prototype for use with parrots that has the following features: it will offer toys and food items on a preprogramed schedule, track and log bird behavior, and run interactive ""apps"" that provide rewards for cognitive thinking challenges, exercise, and natural behavior expression. Parrots are an unusual group of animals, as they exist simultaneously in endangered conservation breeding, pet ownership, commercial pet breeding, and research laboratories. This proposed digital enrichment system's ability to provide many different types of stimuli at a cost of ~$300 per unit provides many advantages when compared to the $3,000-8,000 systems that exist the laboratory market and perform only limited husbandry tasks. There are basically no similar items in this primary market space, the companion bird market. Current state of the art in the market space for other species are items like monkey restraint chairs with levels and light panels, enriched poultry confinement cages (dust bath, perches, nest), and low functionality single species dog or cat products such as the iCPooch feeder.<br/><br/>This project addresses the following technology gaps as it translates from research discovery to commercial application. The complex actions and decisions made by positive reinforcement trainers must become a set of programmed instructions carried out by the app. The system requires specialized low processing power computer vision, and computer hearing that work in a variety of environments and for a variety of species. The technology must seamlessly integrate into the home of the animal, be low maintenance, replace person-hours of work, and be based on the best behavioral science.  These needs will be addressed by multidisciplinary teams of animal trainers, ornithologists, computer vision experts, computer programmers, and engineers working in the lab and aviary to create software and hardware combinations that overcome these technology gaps. The animal health and outcomes monitored will include immune function, body condition, bone density, and expression of natural behaviors. During development, the project will explore crossover applications for zoo and agricultural applications as appropriate. In addition, the project leaders will work with undergraduate and graduate students to provide them with hands-on experience working with industry partners and developing products for commercial applications.  <br/><br/>The project engages three important partners in this technology translation effort: The Schubot Exotic Bird Health Center (a research center with 150 publications on bird health), Hill Country Aviaries (housing and breeding about 2,000 birds of 60 species), and Chris Biro (a bird show owner who provides online training courses and engages several hundred thousand people in outreach annually). This team will provide the researchers with the environment for joint development, pilot testing, customer outreach, and guided commercialization to help transform this research discovery into commercial reality."
"1659288","REU Site: Interdisciplinary Integration in Statistical Learning and Data Mining","DMS","WORKFORCE IN THE MATHEMAT SCI","05/01/2017","01/31/2017","Cuixian Chen","NC","University of North Carolina at Wilmington","Standard Grant","Swatee Naik","04/30/2021","$253,683.00","Yishi Wang","chenc@uncw.edu","601 South College Road","Wilmington","NC","284033201","9109623167","MPS","7335","9250","$0.00","This NSF supported REU Site will provide undergraduate students interdisciplinary research experience in statistical learning and data mining with applications in computer vision and pattern recognition at the University of North Carolina Wilmington (UNCW), for ten weeks during each summer of 2017-2019. UNCW has an institutional commitment to undergraduate education and research through applied learning, and is ideally suited to provide a complete REU experience for the participants. This project is motivated by the shortage of data scientists with analytical skills, recent surge of interests from students, and bringing in awareness of data science career options in academics, industry, and government. The program is designed to involve students in undergraduate research experiences through applied learning and to provide opportunities to develop quantitative and critical thinking skills, and opportunities to improve effective communication skills with professionals from other disciplines.   <br/> <br/>The intellectual focus of the program is to introduce contemporary statistical learning theory and data mining techniques, with applications in analyzing human facial features. Students will be given lectures on the significant impact of computer vision and pattern recognition, challenges in human image analysis, review of fundamentals in mathematics and statistics, image preprocessing, and contemporary statistical learning theory and data mining techniques. The following topics will be discussed in detail: data cleaning and visualization, dimension reduction, regression and classification, software engineering, high performance computing, etc. Research projects are applications of these techniques and emphasize on real world application with interdisciplinary integration. The values of the problem, background, modeling assumptions, statistical theory, numerical solutions, and visualization with computational technology and its interpretation will be well articulated among the participants. Critical and reflective thinking are encouraged, under proper intervenes by mentors, through team-based collaboration and cooperation, group meetings and feedback from weekly presentation and reports."
"1725057","MRI:  Acquisition of Light Detection and Ranging (LIDAR) Scanner for Computer Vision Research and Interdisciplinary Education","CNS","Special Projects - CCF","09/01/2017","08/30/2017","Brittany Morago","NC","University of North Carolina at Wilmington","Standard Grant","Rita Rodriguez","08/31/2019","$118,977.00","","moragob@uncw.edu","601 South College Road","Wilmington","NC","284033201","9109623167","CSE","2878","1189, 2878","$0.00","This project, acquiring a Leica ScanStation P30, aims to develop methods for aligning two dimensional (2D) and three dimensional (3D) data and to assemble educational tools using the produced 3D visualizations. The LIDAR (LIght Detection And Ranging) scanner will be used by professors and students studying and performing research in the fields of computer science, film, and art at the institution. The group builds on the researcher's existing work that creates navigable 3D environments from sets of 2D pictures and videos that are registered with 3D LIDAR scans. With the instrumentation this project would enable the following intertwined goals.<br/>(i) Build an interdisciplinary research group to further study how to describe and register multi-modality data, or data obtained with different cameras and sensors.<br/>(ii) Use the developed methods to build educational 3D applications with collaborators in film and history.<br/>(iii) Develop new, cross-disciplinary courses that use the proposed scanner to provide access for many students to the state-of-the-art technology.<br/>The developed courses would help the investigator train and recruit students for her research lab that can work on interdisciplinary projects.<br/><br/>Although at present collecting and storing both 2D images and 3D LIDAR scans might be easier and more economical, fusing such multi-modality data remains challenging. Confusion is often created in automatic matching methods by the repetition and ambiguity that often occur in man-made scenes as well as the variety of properties on different renderings of the same subject. Image sets collected over a period of time during which lighting conditions and scene content may have changed, different artistic rendering, as well as varying sensor types (Camera, LIDAR, etc.), can all contribute to visual variations in data sets. However, by incorporating contextual information to visualize the regional properties that intuitively exist in each imagery source, the proponent has successfully addressed many of the obstacles. Her research group will also study how human perception can be used for registering 3D LIDAR data with other types of visual information."
"1740312","BIGDATA: F: Collaborative Research: Practical Analysis of Large-Scale Data with Lyme Disease Case Study","IIS","Big Data Science &Engineering","09/01/2017","03/07/2019","Blake Hunter","CA","Claremont McKenna College","Standard Grant","Wendy Nilsen","08/31/2019","$290,125.00","","bhunter@cmc.edu","500 E. Ninth St.","Claremont","CA","917115929","9096077085","CSE","8083","7433, 8083","$0.00","Recent technological and scientific advances have allowed the acquisition of vast amounts of various types of data. Such an abundance of information should lead to new scientific understanding and breakthroughs. However, the large-scale nature of this data introduces serious complications that choke classical data analysis techniques, leading to a stagnation of scientific progress in many areas. This issue requires novel mathematical techniques in order to effectively extract and analyze the information. This project will use Lyme disease data (through a collaboration with LymeDisease.org) as a motivating example in the design and testing of the methods, as it serves as a prime example of complex large-scale data with very significant impact to a fast growing community. The results of this project will thus have swift societal impact; for example, analysis on the LymeData will not only further the understanding of the disease itself, but will also lead to more accurate and precise diagnoses, and more personalized and effective treatments for patients. In addition, this proposal will support the education of postdoctoral, graduate and undergraduate students, and facilitate outreach efforts aimed especially at increasing the participation of under-represented populations. To accomplish this task, in addition to the activities funded by this proposal, the PIs will utilize existing programs such as the Women In Technology Sharing Online (WitsOn) program, Women in Data Science and Mathematics Research Collaboration Workshop (WiSDM), and MAPS 4 College of Los Angeles, all in which the PIs are already actively involved, to recruit under-represented populations and to promote the mathematical and technical sciences.<br/><br/>The fundamental research in this project will center around three main objectives, each addressing a particularly important challenge that arises in large-scale data applications. The first goal is to design innovative data completion techniques that are practical for big data; this will involve the design and theoretical development of data completion methods using non-random (and non-uniform) observation patterns, adaptive sampling schemes, and utilizing additional structures hidden in the observations. Rather than using classical (computationally expensive) convex programming techniques, the project will focus on extremely efficient simple solvers that can be run in real-time during an inference task. Secondly, the team proposes two novel deep learning approaches for inferential tasks that (i) are extremely computationally efficient and can thus be applied to massive datasets, and (ii) achieve the accuracy benefits of modern deep learning approaches, which improve upon state of the art methods. Third, the project will develop critical data fusion techniques that allow data from a wide variety of sources to be analyzed in an aggregated manner. Lastly, the team  proposes to combine these three data analysis tasks in a novel multi-stage feedback design where outputs from data completion, deep learning inferences and fusion will be cycled back as inputs to these mechanisms for an iterative and robust inference framework. Progress on these goals will yield new mathematical frameworks in data science, and provide techniques that will be directly applied to large-scale data to allow efficient and powerful data analysis."
"1737939","NSF Student and Junior Researcher Travel Grant for 2017 Computational Geometry Week (CG Week 2017)","CCF","ALGORITHMIC FOUNDATIONS","07/01/2017","05/02/2017","Joseph S. Mitchell","NY","SUNY at Stony Brook","Standard Grant","Rahul Shah","06/30/2018","$35,000.00","","joseph.mitchell@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7796","7556, 7929","$0.00","This award supports participation in the 33rd International Symposium on Computational Geometry, July 4-7, 2017. SOCG is a focused conference that considers how to develop algorithms to solve problems best stated in geometric form. Research presented at previous SOCG conferences has had significant impact on the US economy in diverse areas that include algorithms for geometric data processing, computer vision, geographic information systems, manufacturing, graphics and visualization, optimization, computer-aided design, and networks. <br/><br/>This award aims to increase the impact of this conference on students, particularly those <br/>from under-represented groups, by encouraging and enabling their participation, especially in cases <br/>where travel expenses would otherwise preclude their attendance. Participation in a top-level event <br/>such as SOCG can be educating, motivating, and useful for networking, both with other students <br/>and with more senior scientists. This support dovetails nicely with the Young Researchers <br/>Forum, a successful satellite event added to the conference in 2012 to encourage and promote <br/>computational geometry research by students and postdocs."
"1602333","SCH: EXP: A Quantitative Platform for CT Colonography","IIS","Smart and Connected Health","02/15/2017","06/17/2019","Aly Farag","KY","University of Louisville Research Foundation Inc","Standard Grant","Wendy Nilsen","01/31/2021","$757,797.00","Albert Seow, Gerald Dryden, Goetz Kloecker","aly.farag@louisville.edu","Atria Support Center","Louisville","KY","402021959","5028523788","CSE","8018","8018, 8061, 9150, 9251","$0.00","Computed-Tomography Colonoscopy (CTC) refers to visualization of the colon surface (lumen) by radiologists following an abdominal CT scan of prepped patients. Removal of colonic polyps is performed by a minimally-invasive procedure known as Optical Colonoscopy.  A proper synchronization between CTC for early detection of polyps and OC for their removal is the most effective and economical approach to prevent colon cancer, which has over 97% rate of recovery with early detection. Through a sequence of image analysis steps, a three-dimensional (3D) representation of the colon can be constructed, which is then visualized by radiologists, using a virtual camera, in order to examine the colon surface for abnormalities and polyps.   Visualization involves extraction of the centerline of the 3D colon representation, which is the optimum datum for the virtual camera.  In this project, a quantitative platform for CTC, based on modern computer vision and graphics, is proposed to quantify and improve the visualization process with respect to image resolution, imaging artifacts, colon topology and polyps' locations. Enhancing the detection and classification of polyps in terms of sensitivity and specificity will improve early detection of colon cancer, a major national healthcare concern in terms of mortality and cost. The proposed simulation platform will have applications in biomedical education and training, and in industrial applications for surface inspection and image-based visualization of hidden tubular objects, in addition to computer-aided manufacturing and digital printing.<br/><br/>Visualization in CTC consist of four sequential steps:  i) filtering the CT scan for removal of scanner noise and resolving partial volume effects; ii) segmentation of the resulting images to isolate the colon surface from other structures in the abdomen, which appear in the abdominal CT scan (e.g., the liver, pancreas and small intestines); iii) generation of a 3D colon representation using computational geometry and graphics; and iv) visualization of the 3D representation, by radiologists, using virtual cameras to examine the lumen surface and detect colonic polyps. Each of these steps are based on solid mathematical foundation developed in the image analysis and computer graphics literature in the past two decades. Filtering may be achieved using anisotropic diffusion filtering, whereas segmentation can be achieved by fusion of statistical and variational methods, which separates the colon tissues from the other anatomies and provides a continuous/connected representation of the segmented colon. 3D reconstruction of the segmented colon is performed by common methodologies in graphics such the marching-cube algorithm. Visualization is an elaborate step, which involves generating the centerline of the 3D reconstruction, and a proper allocation of the virtual camera. The centerline may be generated by variational calculus and the level sets method. Common errors in visualization are due to the uncertainties in imaging (e.g., scanner-induced noise, partial volume effects, prep residuals and patient motion) and patient-specific circumstances which may lead to distorted or disconnected colon reconstructions. These errors may obscure the location, shape and texture of polyps, leading to erroneous diagnosis. This project aims at developing a novel simulation for the front-end of CTC, an accurate sensor planning for the virtual cameras. The proposed simulation will provide unique understanding of CTC and will lead to discovery of methods for image-guided interventions to efficiently remove colonic polyps by Optical Colonoscopy and surgical planning for colectomy. The simulation platform will be applicable in biomedical education, training of resident radiologists and healthcare professionals, and will also promote various applications outside the biomedical domain."
"1712957","Investigation of Bayes Procedures: Theory, Modeling, and Computation","DMS","STATISTICS","07/01/2017","05/19/2017","Chao Gao","IL","University of Chicago","Standard Grant","Gabor Szekely","06/30/2020","$200,000.00","","chaogao@galton.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","1269","","$0.00","Bayesian analysis is a widely used technique in data science for estimation, prediction, and interpretation. The new era of complex and big data imposes unprecedented challenges to Bayesian statistics. This research project addresses these new challenges from three different perspectives. First, the investigator will study the relation between prior knowledge and scientific conclusion by conducting rigorous mathematical analysis in the framework of Bayesian statistics. Second, the investigator aims to find novel ways of modeling data sets that can take into account new features of modern big data. Finally, the investigator intends to push the boundary of Bayesian computation by inventing new algorithms that are both fast and theoretically sound. The results of the research are expected to have a positive impact in areas that apply Bayesian statistics on a routine basis, including population genetics, astronomy, computer vision, political science, social science, and animal science.<br/><br/>Bayesian analysis is an important statistical framework for both modeling and computation. However, applying Bayesian procedures correctly when encountering a specific problem is non-trivial. The selections of prior, likelihood, and algorithm all influence the final conclusion drawn from a posterior distribution. Despite many successful applications of Bayesian analysis in various scientific areas, solid theoretical foundations on how to perform Bayesian inference are still lacking. The goal of this project is to develop a coherent theory on optimal Bayesian inference. Specifically, the investigator will study: 1) Bayesian theory: optimal posterior contraction in parametric, nonparametric and high-dimensional models; 2) Bayesian modeling: likelihood functions that are free of nuisance parameters and Bayesian edge-exchangeable network analysis; 3) Bayesian computation: algorithmic and statistical properties of variational inference; and 4) applications to single-cell RNA sequencing analysis."
"1711335","CCSS: Collaborative Research: Ubiquitous Sensing for VR/AR Immersive Communication: A Machine Learning Perspective","ECCS","CCSS-Comms Circuits & Sens Sys","07/01/2017","06/28/2017","Nicholas Mastronarde","NY","SUNY at Buffalo","Standard Grant","Lawrence Goldberg","06/30/2021","$150,000.00","","nmastron@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","ENG","7564","153E","$0.00","Virtual and augmented reality systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then used to construct an immersive representation of the scene on the user's head mounted display. Such systems are poised to enable and enhance numerous important applications, e.g., inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, due to its emerging nature, virtual/augmented reality immersive communication is presently limited to gaming or entertainment demonstrations featuring off-line captured/computer-generated content, studio-type settings, and high-end workstations to sustain its high data/computing workload. Moreover, there is little understanding of the fundamental trade-offs between the required signal acquisition density and sensor locations across space and time, the dynamics of the captured scene (motion, geometry, and textures), the available network and system resources, and the delivered immersion quality. This renders existing solutions impractical for deployment on bandwidth and energy constrained remote sensors. The project addresses these challenges via rigorous analysis and concerted algorithmic and application advances at the intersection of multi-view space-time sensing and signal representation, delay-sensitive communication, and machine learning. Education and outreach activities will immerse students in the exciting areas of visual sensing, wireless communications, and machine learning, and will engage underrepresented students spanning K-12 through undergraduate levels.<br/><br/>The objective of this project is to efficiently capture a remote environment using multiple camera sensors with the highest possible reconstruction quality under limited sampling and communication resources. This is achieved through four interrelated research tasks: (i) analysis of optimal space-time sampling policies that determine the sensors' locations and sampling rates to minimize the remote scene's reconstruction error; (ii) design of optimal signal representation methods that embed the sampled data jointly across space and time according to the allocated sampling rates; (iii) design of online learning sampling policies based on spectral graph theory that take sampling actions while exploring new sensor locations in the absence of a priori scene viewpoint signal knowledge; and (vi) design of computationally efficient self-organizing reinforcement learning methods that allow the wireless sensors to compute optimal transmission scheduling policies that meet the low-latency requirements of the overlaying virtual/augmented reality application while conserving their available energy. Integration, experimentation, and prototyping activities will be conducted to asses and validate the research advances in real-world settings. These technical advances will enable diverse applications of transformative impact."
"1718929","NETS: SMALL: Fault and Performance Management in Carrier-Grade Virtual Networks Over Multiple Clouds","CNS","Networking Technology and Syst","08/01/2017","08/01/2017","Raj Jain","MO","Washington University","Standard Grant","Deepankar Medhi","07/31/2021","$299,999.00","","Jain@CSE.WUSTL.Edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7363","7923, 9150","$0.00","This project seeks to carry out research leading to the development of an intelligent fault, configuration and performance management framework for virtual carrier networks. The research activity will consider improvements to the management framework in a multiple cloud setting so that the high availability and resiliency requirements of carrier grade networks can be realized. To accomplish this objective,<br/>the project team will apply novel Machine Learning (ML) algorithms to perform network fault detection and isolation.<br/><br/>The research activity has 3 main thrusts: 1) adapting classical machine learning algorithms to target a very dynamic, distributed and real-time environment; 2) developing deductive-predictive fault and performance detection and localization techniques; and 3) developing a hybrid 'central-distributed' technique for training the machine learning models. A key project outcome is to enhance understanding of how Network Function Virtualization (NFV) management, cloud management and operations support can leverage machine learning. The research team will develop new techniques for training machine learning models, and a testbed will be developed to validate the approach."
"1721737","STTR Phase I:  Development of a Machine Learning Platform to Predict Surgical Complications","IIP","STTR Phase I","07/01/2017","06/30/2017","Bora Chang","CA","kelaHealth Inc","Standard Grant","Nancy Kamei","12/31/2018","$225,000.00","","learn@kelahealth.com","301 Howard St","San Francisco","CA","941050000","9199728231","ENG","1505","1505, 8018, 8023, 8032, 8042, 9150","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is to create a personalized, precision-based practice paradigm for surgery that improves surgical outcomes and reduces the cost of healthcare. This paradigm utilizes individual patient characteristics with machine-learning algorithms to accurately predict the risk of post-surgical complications. Additionally, it offers the possibility of enacting impactful interventions among high-risk patients, while reducing unnecessary therapies among low-risk patients, thereby improving surgical outcomes, maximizing the efficiency of healthcare, and minimizing cost. In a value-based care model, this paradigm aligns the goals of health systems, surgeons, and patients.<br/><br/>The proposed project combines individual patient data with machine-learning algorithms to effectively predict surgical complication risk and improve surgical clinical outcomes. Currently, 13% of 50 million surgical procedures performed in the United States annually result in a surgical complication, half which are potentially avoidable. A primary cause of avoidable complications include significant variable in risk assessment and standardized preventative practice. Therefore, the principal objective of this proposal is to develop machine-learning predictive models of various surgical complications (wound, cardiac, respiratory, renal, etc.), which provides an objective risk assessment for surgeons. Additionally, this risk assessment platform will allow stratification of patients into high- vs. low-risk categories and link patients with risk-appropriate preventative interventions at the point-of-care."
"1703812","CSR: Medium: Collaborative Research: Scale-Out Near-Data Acceleration of Machine Learning","CNS","CSR-Computer Systems Research","10/01/2017","06/28/2017","Hadi Esmaeilzadeh","GA","Georgia Tech Research Corporation","Continuing Grant","Matt Mutka","05/31/2018","$235,643.00","","hadi@eng.ucsd.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7354","7924","$0.00","A growing number of commercial and enterprise systems increasingly rely on machine learning algorithms. This shift is, on the one hand, due to the breakthroughs in machine learning algorithms that extract insights from massive amounts of data. Therefore, such systems need to process ever-increasing amounts of data, demanding higher memory bandwidth and capacity. However, the bandwidth between processors and off-chip memory has not increased due to various stringent physical constraints. Besides, data transfers between the processors and the off-chip memory consume orders of magnitude more energy than on-chip computation due to the disparity between interconnection and transistor scaling.<br/><br/>Exploiting recent 3D-stacking technology, the researcher community has explored near-data processing architectures that place processors and memory on the same chip. However, it is unclear whether or not such processing-in-memory (PIM) attempts will be successful for commodity computing systems due to the high cost of 3D-stacking technology and demanded change in existing processor, memory and/or applications. Faced with these challenges, the PIs are to investigate near-data processing platforms that do not require any change in processor, memory and applications, exploiting deep insights on commodity memory subsystems and network software stack. The success of this project will produce inexpensive but powerful near-data processing platforms that can directly run existing machine learning applications without any modification.<br/>"
"1734454","NRI: INT: COLLAB: Robust, Scalable, Distributed Semantic Mapping for Search-and-Rescue and Manufacturing Co-Robots","IIS","NRI-National Robotics Initiati","09/01/2017","08/03/2017","Roberto Tron","MA","Trustees of Boston University","Standard Grant","James Donlon","08/31/2021","$395,035.00","","tronroberto@gmail.com","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","8013","8086","$0.00","The goal of this project is to enable multiple co-robots to map and understand the environment they are in to efficiently collaborate among themselves and with human operators in education, medical assistance, agriculture, and manufacturing applications. The first distinctive characteristic of this project is that the environment will be modeled semantically, that is, it will contain human-interpretable labels (e.g., object category names) in addition to geometric data. This will be achieved through a novel, robust integration of methods from both computer vision and robotics, allowing easier communications between robots and humans in the field. The second distinctive characteristic of this project is that the increased computation load due to the addition of human-interpretable information will be handled by judiciously approximating and spreading the computations across the entire network. The novel developed methods will be evaluated by emulating real-world scenarios in manufacturing and for search-and-rescue operations, leading to potential benefits for large segments of the society. The project will include opportunities for training students at the high-school, undergraduate, and graduate levels by promoting the development of marketable skills.<br/><br/>The project will advance the state of the art in robust semantic mapping from multiple robots by 1) developing a new optimization framework that can handle large, dynamic, uncertain environments under significant measurement errors, 2) explicitly allowing and studying interactions and information exchanges with humans with an hybrid discrete-continuous extension of the optimization framework, and 3) allowing an intelligent use and sharing of the limited computational resources possessed by the network of co-robots as a whole by enabling approximations and balancing of the computations. These developments will be driven by two particular case studies: a job-shop (small factory) scenario, where robots and fixed cameras are used to track and assist human workers during production and assembly of parts; and a classic search-and-rescue scenario, where operators use an heterogeneous team of robots to quickly assess damages and to discover survivors. These two applications, when considered together, highlight all the limitations of the currently prevalent geometric mapping solutions, and will be used as benchmarks for the project's results."
"1712596","Collaborative Research: Statistical Estimation with Algebraic Structure","DMS","STATISTICS","07/01/2017","07/03/2019","Philippe Rigollet","MA","Massachusetts Institute of Technology","Continuing Grant","Gabor Szekely","12/31/2020","$200,000.00","","rigollet@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","","$0.00","Scientific and engineering disciplines ranging from structural biology to computer vision rely on data collection and analysis to guide scientific discovery. Critically, in such applications, the systems under study constrain and govern the structure of information in collected data. The goal of this research project is to develop a family of statistical models that enables a systematic extraction of relevant statistical information from these datasets by bringing together interdisciplinary concepts from statistics and optimization. The approach under development aims to provide a new set of statistical tools that is adapted to this class of problems and that could have a transformative impact on several scientific disciplines.<br/><br/>This project is articulated around a core set of techniques to analyze datasets in the context of a latent algebraic structure, often arising from the physical laws underlying the data collection processes. Unlike more traditional statistical problems where a linear underlying structure is often built into the model, data-driven science generates problems with algebraic but often non-linear structure. The project focuses on problems of central importance in a variety of scientific and engineering disciplines, including signal processing, structural biology, and computer vision, that share a similar feature: the need to leverage algebraic structure in order to extract information from data. The project aims at developing a systematic approach to analyze this family of problems, together with a general procedure to construct computationally efficient algorithms using low-rank tensor decomposition. Importantly, these methods can be proved to be statistically optimal and therefore make the most efficient use of collected data."
"1743010","EAGER: Advanced Machine Learning Techniques to Discover Disease Subtypes in Cancer","IIS","Info Integration & Informatics","07/01/2017","05/14/2020","Wei Ding","MA","University of Massachusetts Boston","Standard Grant","Amarda Shehu","06/30/2021","$165,881.00","Ping Chen, Kourosh Zarringhalam","wei.ding@umb.edu","100 Morrissey Boulevard","Dorchester","MA","021253300","6172875370","CSE","7364","7364, 7916, 9251","$0.00","A significant challenge in the analysis of large-scale genomic and molecular profiles of cancer is the identification of distinct, molecularly independent disease subtypes and the association of these with clinically relevant outcomes. The barriers to identifying molecularly-defined, clinically relevant subtypes have been the high-dimensionality of the feature space, limited sample sizes, and low recurrence rate of mutations between patients. The intellectual merits of this project are to develop theory, algorithms, and implementation for robust and scalable network-based machine learning and data mining techniques in high-dimensional gene expression and gene mutation data for disease subtype discovery in cancer. The results of the project can help to identify individual cancer, pan-cancer, and sex-specific subtypes to better understand the nature of cancer and to develop the most efficacious therapeutic strategies. The mathematical and machine-learning models developed in this study are general biological network-induced regularization models that are applicable in a broad range of supervised, semi-supervised, and unsupervised learning problems.<br/><br/>The goal of this project is to design novel network-based learning models that optimally integrate prior biological knowledge on gene regulatory mechanisms into learning algorithms. New group-based and Laplacian-based regularization techniques and restricted manifold learning in matrix factorization are investigated to design reproducible models for disease subtyping. This is the first study to build an efficient toolkit for cancer subtype discovery that fully integrates discrete mutational profiles and continuous gene expression data. The project provides extensive cross-disciplinary training in Computer Science, Mathematics, and Engineering. The models developed during this study can be broadly applied as more precision genomic medicine data become available."
"1730183","CRI II-NEW: IIS: Omniview Multi-modal Sensor Laboratory for Understanding Human Interactions in Ubiquitous Environments","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2017","04/28/2017","Sean Banerjee","NY","Clarkson University","Standard Grant","Wendy Nilsen","05/31/2021","$746,916.00","Natasha Banerjee","sbanerje@clarkson.edu","8 Clarkson Avenue","Potsdam","NY","136761401","3152686475","CSE","7359","7359","$0.00","Understanding how people interact with objects and with each other is an important research area in human-computer interaction, particularly in contexts where phones, cameras, sensors, voice assistants, robots, and other computing devices help people accomplish their goals.  To study these interactions, this project will equip a lab with state of the art equipment for capturing human activity that doesn't require attaching markers, wires, or sensors to people, and develop software to manage the large amounts of captured data and interfaces that help researchers and designers make use of the data.  Much of the envisioned fundamental research will focus on how age, gender, physical ability, and experience level affect the way people interact with objects, as well as on using social cues such as emotions, inter-person distances, and gestures to understand how people interact with each other.  A number of researchers at the lead investigators' institution will use the lab for related projects around people- and object-aware technologies, including assistive robotics, self-driving vehicles, teams and collaboration, and smart environments.  The lab will also provide training for a postdoctoral researcher and research opportunities for undergraduates, support a number of courses taught at the PIs' institution, and provide new opportunities for interdisciplinary research.<br/><br/>The PIs will develop a temporally synchronized and spatially calibrated sensor system that includes force plates, microphones, RGB and infrared cameras, and Kinect sensors, and deploy it in 20x20x12 foot lab.  Data will be synchronized using linear time codes (LTCs), including custom hardware previously developed by the PIs to add LTC data to Kinects, and managed by a large cluster of computers and network attached storage devices.  The infrastructure will use computer vision and sound reconstruction algorithms on the raw sensor data to reconstruct 3D spatiotemporal data that provides a full range 3D visualization of human interactions.  By combining data from multiple sensor modalities in 3D, the infrastructure enables research in strengthening recognition of gestures and emotions of people engaged in interactions by analysis of prosody changes, face points, body skeletons, spatiotemporal motions, linguistics, heat signatures, and emotion- or task-driven physical impact.  Further, the PIs will develop large repositories of omniview multi-modal 3D spatiotemporal data, and conduct research on tying these repositories to sensors on next-generation ubiquitous devices to make them people- and object-aware."
"1734362","NRI: INT: COLLAB: Robust, Scalable, Distributed Semantic Mapping for Search-and-Rescue and Manufacturing Co-Robots","IIS","NRI-National Robotics Initiati","09/01/2017","03/07/2019","Dario Pompili","NJ","Rutgers University New Brunswick","Standard Grant","James Donlon","08/31/2020","$442,161.00","","pompili@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8013","8086, 9251","$0.00","The goal of this project is to enable multiple co-robots to map and understand the environment they are in to efficiently collaborate among themselves and with human operators in education, medical assistance, agriculture, and manufacturing applications. The first distinctive characteristic of this project is that the environment will be modeled semantically, that is, it will contain human-interpretable labels (e.g., object category names) in addition to geometric data. This will be achieved through a novel, robust integration of methods from both computer vision and robotics, allowing easier communications between robots and humans in the field. The second distinctive characteristic of this project is that the increased computation load due to the addition of human-interpretable information will be handled by judiciously approximating and spreading the computations across the entire network. The novel developed methods will be evaluated by emulating real-world scenarios in manufacturing and for search-and-rescue operations, leading to potential benefits for large segments of the society. The project will include opportunities for training students at the high-school, undergraduate, and graduate levels by promoting the development of marketable skills.<br/><br/>The project will advance the state of the art in robust semantic mapping from multiple robots by 1) developing a new optimization framework that can handle large, dynamic, uncertain environments under significant measurement errors, 2) explicitly allowing and studying interactions and information exchanges with humans with an hybrid discrete-continuous extension of the optimization framework, and 3) allowing an intelligent use and sharing of the limited computational resources possessed by the network of co-robots as a whole by enabling approximations and balancing of the computations. These developments will be driven by two particular case studies: a job-shop (small factory) scenario, where robots and fixed cameras are used to track and assist human workers during production and assembly of parts; and a classic search-and-rescue scenario, where operators use an heterogeneous team of robots to quickly assess damages and to discover survivors. These two applications, when considered together, highlight all the limitations of the currently prevalent geometric mapping solutions, and will be used as benchmarks for the project's results."
"1711592","CCSS: Collaborative Research: Ubiquitous Sensing for VR/AR Immersive Communication: A Machine Learning Perspective","ECCS","CCSS-Comms Circuits & Sens Sys","07/01/2017","06/28/2017","Jacob Chakareski","AL","University of Alabama Tuscaloosa","Standard Grant","Lawrence Goldberg","06/30/2020","$220,000.00","","jacobcha@njit.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","ENG","7564","153E","$0.00","Virtual and augmented reality systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then used to construct an immersive representation of the scene on the user's head mounted display. Such systems are poised to enable and enhance numerous important applications, e.g., inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, due to its emerging nature, virtual/augmented reality immersive communication is presently limited to gaming or entertainment demonstrations featuring off-line captured/computer-generated content, studio-type settings, and high-end workstations to sustain its high data/computing workload. Moreover, there is little understanding of the fundamental trade-offs between the required signal acquisition density and sensor locations across space and time, the dynamics of the captured scene (motion, geometry, and textures), the available network and system resources, and the delivered immersion quality. This renders existing solutions impractical for deployment on bandwidth and energy constrained remote sensors. The project addresses these challenges via rigorous analysis and concerted algorithmic and application advances at the intersection of multi-view space-time sensing and signal representation, delay-sensitive communication, and machine learning. Education and outreach activities will immerse students in the exciting areas of visual sensing, wireless communications, and machine learning, and will engage underrepresented students spanning K-12 through undergraduate levels.<br/><br/>The objective of this project is to efficiently capture a remote environment using multiple camera sensors with the highest possible reconstruction quality under limited sampling and communication resources. This is achieved through four interrelated research tasks: (i) analysis of optimal space-time sampling policies that determine the sensors' locations and sampling rates to minimize the remote scene's reconstruction error; (ii) design of optimal signal representation methods that embed the sampled data jointly across space and time according to the allocated sampling rates; (iii) design of online learning sampling policies based on spectral graph theory that take sampling actions while exploring new sensor locations in the absence of a priori scene viewpoint signal knowledge; and (vi) design of computationally efficient self-organizing reinforcement learning methods that allow the wireless sensors to compute optimal transmission scheduling policies that meet the low-latency requirements of the overlaying virtual/augmented reality application while conserving their available energy. Integration, experimentation, and prototyping activities will be conducted to asses and validate the research advances in real-world settings. These technical advances will enable diverse applications of transformative impact."
"1740531","I-Corps: Enabling Electronic Design using Data Intelligence","IIP","I-Corps","04/01/2017","04/03/2017","Peng Li","TX","Texas A&M Engineering Experiment Station","Standard Grant","Anita La Salle","09/30/2018","$50,000.00","","lip@ece.ucsb.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project stems from its data intelligence approach to empower electronic design automation. The semiconductor industry provides vital hardware backbone of the information technology age through an extremely wide range of integrated circuits (ICs) in computing devices and consumer electronics.  Modern IC development process is bottlenecked by growing chip design complexity, e.g. measured by large device count and functionality diversity, and ever-demanding requirements on computing performance and power/energy efficiency. Advanced IC manufacturing processes are costly, and yet have unavoidable process variations, making fabricated chips susceptible to failures. With its revenue reaching $7.8 billion in 2015, the electronic design automation (EDA) industry supplies indispensable tools and methodologies that make IC design possible. The potential market and societal impact of the proposed EDA innovation is substantial. This technology can help semiconductor and chip design companies develop integrated circuits of improved performance and robustness with a reduced time-to-market and development cost.<br/><br/>This I-Corps project demonstrates novel machine learning algorithms targeting electronic design automation. As the complexity of integrated circuits scales up rapidly, the need for smart design tools is prominent. The EDA industry is in the early phase of rapid integration of machine learning algorithms into commercial IC design flows. The learning methods focused in this project significantly improve the accuracy of statistical regression and classification over the current-state-of-the-art, and offer the much needed understanding of the underlying structure of the data. Built upon the focused machine learning algorithms, the targeted EDA technology can efficiently process simulation or measured performance data of existing chip designs, and intelligently learn the complex hidden relationships between performance specifications, design parameters, and manufacturing conditions. As a result, it offers a powerful data science solution to IC design optimization, verification, and debug.   Implemented as high-performance parallel software design tools, the technology will bring the power of machine learning to the field of electronic design."
"1712605","I-Corps: Automatic Data Capture System using Machine Learning","IIP","I-Corps","01/01/2017","12/14/2016","Mequanint Moges","TX","University of Houston","Standard Grant","Steven Konsek","09/30/2017","$50,000.00","","MMoges@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to transform existing data entry and document archiving systems by introducing an Automatic Data Capture System (ADCS) enhanced by machine learning.  Today many sectors, including the mortgage industry, manufacturing, healthcare, and government agencies, are using cumbersome data entry and document processing techniques. This process not only adds substantially to cost, but also increases inaccuracies in the data pool, cutting into the competitive advantage of industries and agencies. The new system eliminates traditional methods of data extraction and document classification by using a self-learning software program that gathers information from any scanned document in any paper-intensive business processes.  Implementing Robotic Process Automation in data entry business processes in a structured way can accurately extract and process data much faster at a very low cost point.  In addition, the proposed project plan will enhance knowledge about and research on electronic data capture systems used in healthcare, allowing the development of more efficient data-processing techniques that can improve patient care and speed up billing cycles.<br/><br/>This I-Corps project will implement an effective solution that eliminates time-consuming and error-prone manual data entry. The project will provide valuable insight into the process of scanning, data capture techniques, and machine learning.  The development of technical skills will also be enhanced by an innovative learning framework that explores the application of Convolutional Neural Networks (CNNs) to data capture and processing.  This proprietary approach to utilizing CNNs in a non-traditional manner has led to exceptional results in identifying and extracting information from scanned documents, making it possible for the first time to fully automate paper-intensive business processes. The commercial success of such a software program is validated by the efforts several companies are making to develop a similar product themselves because currently available technologies, such as Optical Character Recognition (OCR), cannot offer the level of accuracy required. This new Automated Data Capture System will also enhance research on virtual data systems for representing, querying, and automating data derivation - research that is used in applications such as virtual data language interpreters and data grids."
"1741340","BIGDATA: F: Scalable and Interpretable Machine Learning: Bridging Mechanistic and Data-Driven Modeling in the Biological Sciences","IIS","STATISTICS, Big Data Science &Engineering","10/01/2017","09/14/2017","Bin Yu","CA","University of California-Berkeley","Standard Grant","Victor Roytburd","09/30/2021","$900,000.00","James Bentley Brown","binyu@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1269, 8083","7433, 8083, 8251","$0.00","With the rapid advances in information technology, an age of rich data has dawned in nearly every scientific field. Such data hold the potential to guide decision-making and accelerate understanding of complex processes such as human development and disease progression. For instance, massive databases on gene expression and other molecular processes can be used to build models to predict the drivers of a disease. Predictive models are an important step in understanding these complex systems, but equally important is the human interpretability of such models, e.g. to derive mechanistic insights into what factors drive disease onset in order to identify an appropriate course of treatment.  Next Generation Sequencing (NGS) technologies have led to a profound shift in how biological data are collected, assaying individual genomic elements that act as part of organized, stereospecific groups to drive emergent biological phenomena. These modern data call for new statistics/data science principles and scalable algorithms to advance the frontier of science.<br/><br/>This project focuses on developing novel scalable statistical machine learning algorithms that are predictable, stable and interpretable, and can be used to guide decision-making and discovery in biological systems. This project aims to build insights into how individual genomic elements act in concert by developing interpretable and stable supervised learning algorithms with state of the art predictive accuracy along with scalable, open source software. Many machine learning algorithms with state of the art predictive accuracies are capable of learning complicated rules that might govern complex systems but are difficult for humans to interpret. The research builds on iterative Random Forests (iRF), an algorithm recently developed by the PIs that recovers the high-order, human interpretable, Boolean type interactions that are important parts of the state-of-the-art predictive accuracy in Random Forests. The proposed work will develop and validate approaches for refining interactions recovered by iRF to produce testable hypotheses for follow-up studies, along with inference methods to assess the uncertainty associated with these hypotheses. These approaches and methods will be implemented in Apache Spark to ensure scalability to massive datasets in genomics and beyond.  Implementation of the methods for the large-scale applications will leverage cloud computing resources provided through an agreement between commercial cloud service providers and NSF for the BIGDATA solicitation."
"1740325","BIGDATA: F: Collaborative Research: Practical Analysis of Large-Scale Data with Lyme Disease Case   Study","IIS","Big Data Science &Engineering","09/01/2017","08/13/2017","Deanna Needell","CA","University of California-Los Angeles","Standard Grant","Wendy Nilsen","08/31/2020","$470,911.00","","deanna@math.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","8083","7433, 8083","$0.00","Recent technological and scientific advances have allowed the acquisition of vast amounts of various types of data. Such an abundance of information should lead to new scientific understanding and breakthroughs. However, the large-scale nature of this data introduces serious complications that choke classical data analysis techniques, leading to a stagnation of scientific progress in many areas. This issue requires novel mathematical techniques in order to effectively extract and analyze the information. This project will use Lyme disease data (through a collaboration with LymeDisease.org) as a motivating example in the design and testing of the methods, as it serves as a prime example of complex large-scale data with very significant impact to a fast growing community. The results of this project will thus have swift societal impact; for example, analysis on the LymeData will not only further the understanding of the disease itself, but will also lead to more accurate and precise diagnoses, and more personalized and effective treatments for patients. In addition, this proposal will support the education of postdoctoral, graduate and undergraduate students, and facilitate outreach efforts aimed especially at increasing the participation of under-represented populations. To accomplish this task, in addition to the activities funded by this proposal, the PIs will utilize existing programs such as the Women In Technology Sharing Online (WitsOn) program, Women in Data Science and Mathematics Research Collaboration Workshop (WiSDM), and MAPS 4 College of Los Angeles, all in which the PIs are already actively involved, to recruit under-represented populations and to promote the mathematical and technical sciences.<br/><br/>The fundamental research in this project will center around three main objectives, each addressing a particularly important challenge that arises in large-scale data applications. The first goal is to design innovative data completion techniques that are practical for big data; this will involve the design and theoretical development of data completion methods using non-random (and non-uniform) observation patterns, adaptive sampling schemes, and utilizing additional structures hidden in the observations. Rather than using classical (computationally expensive) convex programming techniques, the project will focus on extremely efficient simple solvers that can be run in real-time during an inference task. Secondly, the team proposes two novel deep learning approaches for inferential tasks that (i) are extremely computationally efficient and can thus be applied to massive datasets, and (ii) achieve the accuracy benefits of modern deep learning approaches, which improve upon state of the art methods. Third, the project will develop critical data fusion techniques that allow data from a wide variety of sources to be analyzed in an aggregated manner. Lastly, the team  proposes to combine these three data analysis tasks in a novel multi-stage feedback design where outputs from data completion, deep learning inferences and fusion will be cycled back as inputs to these mechanisms for an iterative and robust inference framework. Progress on these goals will yield new mathematical frameworks in data science, and provide techniques that will be directly applied to large-scale data to allow efficient and powerful data analysis."
"1745561","CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems","CNS","CPS-Cyber-Physical Systems","06/01/2017","09/15/2017","Siddhartha Srinivasa","WA","University of Washington","Standard Grant","Wendy Nilsen","09/30/2019","$366,453.00","","siddhartha.srinivasa@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7918","7918, 8235","$0.00","CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems<br/><br/>Assistive machines - like powered wheelchairs, myoelectric prostheses and robotic arms - promote independence and ability in those with severe motor impairments. As the state- of-the-art in these assistive Cyber-Physical Systems (CPSs) advances, more dexterous and capable machines hold the promise to revolutionize ways in which those with motor impairments can interact within society and with their loved ones, and to care for themselves with independence. However, as these machines become more capable, they often also become more complex. Which raises the question: how to control this added complexity? A new paradigm is proposed for controlling complex assistive Cyber-Physical Systems (CPSs), like robotic arms mounted on wheelchairs, via simple low-dimensional control interfaces that are accessible to persons with severe motor impairments, like 2-D joysticks or 1-D Sip-N-Puff interfaces. Traditional interfaces cover only a portion of the control space, and during teleoperation it is necessary to switch between different control modes to access the full control space. Robotics automation may be leveraged to anticipate when to switch between different control modes. This approach is a departure from the majority of control sharing approaches within assistive domains, which either partition the control space and allocate different portions to the robot and human, or augment the human's control signals to bridge the dimensionality gap. How to best share control within assistive domains remains an open question, and an appealing characteristic of this approach is that the user is kept maximally in control since their signals are not altered or augmented. The public health impact is significant, by increasing the independence of those with severe motor impairments and/or paralysis. Multiple efforts will facilitate large-scale deployment of our results, including a collaboration with Kinova, a manufacturer of assistive robotic arms, and a partnership with Rehabilitation Institute of Chicago. <br/><br/>The proposal introduces a formalism for assistive mode-switching that is grounded in hybrid dynamical systems theory, and aims to ease the burden of teleoperating high-dimensional assistive robots. By modeling this CPS as a hybrid dynamical system, assistance can be modeled as optimization over a desired cost function. The system's uncertainty over the user's goals can be modeled via a Partially Observable Markov Decision Processes. This model provides the natural scaffolding for learning user preferences. Through user studies, this project aims to address the following research questions: (Q1)  Expense: How expensive is mode-switching? (Q2)  Customization Need: Do we need to learn mode-switching from specific users? (Q3)  Learning Assistance: How can we learn mode-switching paradigms from a user? (Q4)  Goal Uncertainty: How should the assistance act under goal uncertainty? How will users respond? The proposal leverages the teams shared expertise in manipulation, algorithm development, and deploying real-world robotic systems. The proposal also leverages the teams complementary strengths on deploying advanced manipulation platforms, robotic motion planning and manipulation, and human-robot comanipulation, and on robot learning from human demonstration, control policy adaptation, and human rehabilitation. The proposed work targets the easier operation of robotic arms by severely paralyzed users. The need to control many degrees of freedom (DoF) gives rise to mode-switching during teleoperation. The switching itself can be cumbersome even with 2- and 3-axis joysticks, and becomes prohibitively so with more limited (1-D) interfaces. Easing the operation of switching not only lowers this burden on those already able to operate robotic arms, but may open use to populations to whom assistive robotic arms are currently inaccessible. This work is clearly synergistic: at the intersection of robotic manipulation, human rehabilitation, control theory, machine learning, human-robot interaction and clinical studies. The project addresses the science of CPS by developing new models of the interaction dynamics between the system and the user, the technology of CPS by developing new interfaces and interaction modalities with strong theoretical foundations, and the engineering of CPS by deploying our algorithms on real robot hardware and extensive studies with able-bodied and users with sprinal cord injuries."
"1661202","Improving Science Problem Solving with Adaptive Game-Based Reflection Tools","DRL","ECR-EHR Core Research","04/15/2017","04/25/2019","James Lester","NC","North Carolina State University","Continuing Grant","Amy Baylor","03/31/2021","$1,300,000.00","Roger Azevedo","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","EHR","7980","8817","$0.00","The project is supported by the Education and Human Resources Core Research program, which supports fundamental research in STEM learning and learning environments. Reflection plays a critical role in student learning and encompasses a broad range of cognitive and metacognitive processes enabling students to (1) think critically about their learning processes, (2) integrate new information with prior knowledge, (3) form and adapt learning strategies, (4) view concepts and knowledge from multiple perspectives, (5) generate self-explanations to enrich conceptual understanding, (6) compare learning processes and artifacts to those created by experts and peers, and (7) make metacognitive judgments about knowledge. The National Research Council recently concluded that systematic reflection is essential for deep learning and effective educational practice. This project investigates a suite of theoretically grounded, adaptive game-based reflection tools to scaffold students' cognitive and metacognitive reflection with the overarching objective of improving middle school students' science problem-solving processes. These reflection tools are integrated as part of the existing Crystal Island learning environment.<br/><br/>In studying middle school students' cognitive and metacognitive reflective processes, three key research questions will be addressed: 1) How do embedded and retrospective reflection scaffolds promote improved learning outcomes including science problem-solving skills, science content knowledge, metacognitive awareness, and reflection skills? 2) How can learning analytics be leveraged to extend models of reflection and self-regulated learning for problem solving within game-based learning environments? and 3) How can we create adaptive scaffolding for game-based learning environments that best foster reflection during and following science problem solving? The project builds upon a theoretical framework of self-regulated learning to inform the development of embedded and retrospective scaffold tools, gathering both process data (e.g., log data, eye tracking, think-aloud) and outcome data (e.g., pre- post- learning, transfer, metacognition, self-efficacy) to inform design principles and descriptive models of the role of reflection in students' problem-solving processes. Key project outcomes include an empirically grounded theoretical framework for reflection-enhanced learning and learning analytic techniques that yield predictive models of reflection in science problem solving."
"1712730","Collaborative Research:  Statistical Estimation with Algebraic Structure","DMS","STATISTICS","07/01/2017","09/13/2019","Alexander Wein","NY","New York University","Continuing Grant","Gabor Szekely","06/30/2021","$279,998.00","","awein@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","MPS","1269","","$0.00","Scientific and engineering disciplines ranging from structural biology to computer vision rely on data collection and analysis to guide scientific discovery. Critically, in such applications, the systems under study constrain and govern the structure of information in collected data. The goal of this research project is to develop a family of statistical models that enables a systematic extraction of relevant statistical information from these datasets by bringing together interdisciplinary concepts from statistics and optimization. The approach under development aims to provide a new set of statistical tools that is adapted to this class of problems and that could have a transformative impact on several scientific disciplines.<br/><br/>This project is articulated around a core set of techniques to analyze datasets in the context of a latent algebraic structure, often arising from the physical laws underlying the data collection processes. Unlike more traditional statistical problems where a linear underlying structure is often built into the model, data-driven science generates problems with algebraic but often non-linear structure. The project focuses on problems of central importance in a variety of scientific and engineering disciplines, including signal processing, structural biology, and computer vision, that share a similar feature: the need to leverage algebraic structure in order to extract information from data. The project aims at developing a systematic approach to analyze this family of problems, together with a general procedure to construct computationally efficient algorithms using low-rank tensor decomposition. Importantly, these methods can be proved to be statistically optimal and therefore make the most efficient use of collected data."
"1720487","Fine-Scale Singularity Detection in Multi-Dimensional Imaging with Regular, Orientable, Symmetric, Frame Atoms with Small Support","DMS","COMPUTATIONAL MATHEMATICS, MSPA-INTERDISCIPLINARY, Modulation","07/15/2017","07/20/2017","Emanuel Papadakis","TX","University of Houston","Standard Grant","Leland Jameson","06/30/2021","$250,000.00","Demetrio Labate","mpapadak@math.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","MPS","1271, 7454, 7714","8007, 8091, 9263","$0.00","One of life's essential characteristic is movement. Whether it is the spectacular, delicate dance of the unblemished, white swan in Tchaikovsky's Swan Lake, or the early attempts of a toddler to use his or her hands, the neurology of movement is uniquely common for all animals and humans: learning a motor skill, and the necessary muscle coordination. With practice the skill is perfected. Finally, the retainment of this experience-based learning process is the conclusion of this learning process. The ultimate goal of this project is to provide new tools to neuroscientists who study the biological basis of learning at the cell level using live animals. This function is facilitated by a number of anatomical changes in the structure of the cytoplasm of neural cells, such as the formation of lengthy branches known as axons and dendrites, and at a fine scale of dendritic spines and axonal buttons. The latter are less anatomically permanent structures arising on the surface of these cytoplasmic extensions. Dendritic spines and axonal buttons form synapses, which are the communication gateways between neurons. The research team will develop mathematical and computational tools for automatizing the study of spine populations in live neurons, and of their time-evolution during learning. The anticipated outcomes will provide neuroscientists with a number of software tools which will automatize the analysis of synaptic strength and its evolution with learning. These findings will contributed to the better understanding the biological mechanisms of autism and drug addictions.<br/><br/>The investigators on this project will develop algorithms for the 3D digital segmentations of dendritic surfaces including spines from 3D images acquired with a certain type of microscope, which uses laser light and works as a scanner by exploiting the natural ability of neurons to fluoresce. They aim to generate accurate binary reconstructions of a dendritic arbor including its spines. The primary challenge in this project is that image acquisition of live neurons has a resolution which provides limited detail of the spines. Often, images contain noise which further complicates the extraction of accurate, binary 3D reconstructions of dendritic surfaces showing spine details. Overcoming this problem is a core goal of the project because spine volume estimation quantifies synaptic strength. These unique challenges lead the investigators to the development of novel mathematical tools for fine scale analysis. They will build ensembles of short in size 3D imaging, 3D-orientation selective, frame-based filters, suitable for sensing curves and surfaces in noisy images. These filters will be designed to respond to local changes of image smoothness. Information obtained from these filters at various scales, will be utilized as input for multilayer, deep-learning inspired neural networks which will determine in an image which voxels belong to spine surfaces. Further algorithmic tools will be developed to track every spine of a dendrite individually over time. The same filtering tools will be used in a different application domain, the generation of illumination neutral images, in real-time. This will help the fast, high throughput removal of the effects of uneven illumination in images inhibiting the detection, by software or the naked eye of contours associated with shapes or textures in a scene. The investigators will develop the mathematical theory of illumination neutralization using concepts from fractal and microlocal analysis. The illumination neutralization algorithm is envisioned to work for real-time video analysis and in conjunction with face verification algorithms with the potential to be used in face recognition, laser microscopy and remote sensing applications."
"1704309","RI: Medium: Collaborative Research: Understanding and Editing Visual Sentiment","IIS","Robust Intelligence","07/01/2017","07/17/2020","GuoJun Qi","FL","The University of Central Florida Board of Trustees","Continuing Grant","Jie Yang","06/30/2021","$485,688.00","Wei Wang","guojunq@gmail.com","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7495","7495, 7924","$0.00","The project develops computer vision and pattern recognition technologies for visual sentiment understanding and visual sentiment editing. The interdisciplinary research team investigates the problem of understanding how images and video convey emotion. The project develops methods to infer, edit, and synthesize visual sentimental content in image/videos, in addition to their semantic contents. The project applies developed technologies to reduce violence from multimedia materials for children, and negative psychological impacts from social media for posttraumatic stress disorder (PTSD) patients. The project integrates research and education by creating new interdisciplinary courses and training graduate students.  The project builds connection with the veteran academic resource center on the campus to help PTSD patients to recover from mental health problems. The research team also shares collected data with research communities.<br/><br/>This research develops visual sentiment understanding algorithms through joint extraction of sentiments and semantics, in order to advance the understanding of how semantic entities substantiate and carry sentiments at a fine-grained object or pixel level. Computer vision algorithms and psychometric assessment techniques are combined to automatically analyze visual and recognize sentiments and emotions from multimedia materials and social media contents posted and shared by veterans. The research also explores methods of visual sentiment editing to reduce violence from multimedia materials and social media contents. The research can help (1) to protect children from accessing violent multimedia materials, and (2) to provide appropriate social media contents for applications of automatically detecting violent contents from veteran-shared multimedia."
"1704337","RI: Medium:  Collaborative Research: Understanding and Editing Visual Sentiments","IIS","Robust Intelligence","07/01/2017","09/07/2017","Jiebo Luo","NY","University of Rochester","Continuing Grant","Jie Yang","06/30/2021","$301,873.00","","jluo@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7495","7495, 7924","$0.00","The project develops computer vision and pattern recognition technologies for visual sentiment understanding and visual sentiment editing. The interdisciplinary research team investigates the problem of understanding how images and video convey emotion. The project develops methods to infer, edit, and synthesize visual sentimental content in image/videos, addition to their semantic contents. The project applies developed technologies to reduce violence from multimedia materials for children, and negative psychological impacts from social media for posttraumatic stress disorder (PTSD) patients. The project integrates research and education by creating new interdisciplinary courses and training graduate students.  The project builds connection with the veteran academic resource center on the campus to help PTSD patients to recover from mental health problems. The research team also shares collected data with research communities.<br/><br/>This research develops visual sentiment understanding algorithms through joint extraction of sentiments and semantics, in order to advance the understanding of how semantic entities substantiate and carry sentiments at a fine-grained object or pixel level. Computer vision algorithms and psychometric assessment techniques are combined to automatically analyze visual and recognize sentiments and emotions from multimedia materials and social media contents posted and shared by veterans. The research also explores methods of visual sentiment editing to reduce violence from multimedia materials and social media contents. The research can help (1) to protect children from accessing violent multimedia materials, and (2) to provide appropriate social media contents for applications of automatically detecting violent contents from veteran-shared multimedia."
"1726069","MRI: Acquisition of Adaptive Cluster for Performance and Forensics Analysis of Distributed Machine Learning","CNS","ATW-Alan T Waterman Award, NMS-National Medal of Science, SSIP-Summer Scholars Internshi, Major Research Instrumentation, Special Projects - CNS","10/01/2017","03/14/2019","Ryan Benton","AL","University of South Alabama","Standard Grant","Rita Rodriguez","09/30/2020","$147,100.00","Jordan Shropshire, William Glisson","rbenton@southalabama.edu","307 University Boulevard","Mobile","AL","366880002","2514606333","CSE","049Y, 050Y, 053Y, 1189, 1714","1189, 9150, 9251","$0.00","This project, acquiring a computational cluster, aims to provide opportunities for research in machine-learning, algorithm development, and protection of information in multiple environments. The capacity to evaluate and analyze performance and residue data generation in data mining, machine learning computations, should allow better control and less risk of breaches in cybersecurity. The availability of these enhancements would also permit use of these systems for applied, interdisciplinary research using large-scale data and cross-correlation analyses for predictive modeling. The investigators measure performance systematically to support forensic analysis of data residues, in order to detect possible security risks in the use of such platforms. The procurement of the instrumentation yields a significant expansion in data mining, security, and forensics research. Core research foci in cyber security, digital forensics, and data mining research enables a work plan based on defined problems in distributed computing environments related to performance, algorithms, and data security. The gained instrument and expertise provide the institution with the ability to support national level customers such as U.S. Army Aviation & Missile Research, Development and Engineering Center, National Institute of Health (NIH), and other government agencies that depend on effective and secure distributed learning to analyze and process sensitive data. A key issue with solving both the forensics challenges and the performance analysis is having access to an instrument such that investigators can<br/>- Tune, adjust, and redeploy environments, <br/>- Take nodes offline to be examined forensically, <br/>- Ensure a consistent baseline exists against which other environments are compared, and <br/>- Run meaningful experiments within their discipline while enabling the collection of valuable performance and forensics data (permitting non-data mining and utilizing digital forensics).<br/>The proposed adaptive cluster instrument enables these four goals.<br/><br/>Broader Impacts:<br/>The computational research capabilities provide essential resources for undergraduate and graduate students' research as well as for training and exposure activities involving K-12. Students will be afforded hands-on opportunities in research and classroom activities directly utilizing the cluster. The skills gained should lead directly to internships and permanent employment opportunities for doctoral students in the six colleges and/or institutes. The university services a high percentage of rural and financially depressed areas; has a 34% enrollment of non-white students, with 60% female enrollment. The instrumentation offers exposure opportunities for cohort activities to Scholarships for Service Program."
"1703883","CHS: Medium: Data Driven Biomechanically Accurate Modeling of Human Gait on Unconstrained Terrain","IIS","HCC-Human-Centered Computing","09/01/2017","08/16/2017","Dimitris Metaxas","NJ","Rutgers University New Brunswick","Standard Grant","Ephraim Glinert","08/31/2020","$1,182,466.00","Kang Li, Mubbasir Kapadia","dnm@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7367","7367, 7924","$0.00","Modeling human gait parametrically, efficiently and accurately is an open and challenging problem with many applications such as ergonomics, animation, biomechanics, rehabilitation, physical therapy, virtual reality and entertainment.  Gait is a complex process, because numerous human joint degrees-of-freedom have to be simultaneously coordinated and adapted to varying types of terrain, types of gait and related kinematics and dynamics.  As a consequence, no general-purpose models of gait kinematics on unconstrained complex terrain exist, and even basic kinematic gait databases and related ground forces that can form the basis for developing such models do not provide accurate information.  Most gait data are collected by the commonly used surface marker systems, which suffer from soft tissue artifacts and are not sufficient for a full understanding of gait kinematics.  Developing a parameterized human gait for unconstrained terrain will have broad impact on the fields of virtual reality, next generation shoe design, computer animation, workplace safety, ergonomics, sports medicine, biomedical and clinical research to aid people with gait abnormalities.  Project outcomes, including algorithms and datasets, will ultimately be incorporated in the curricula of many fields including computer science and biomedical engineering, and will result in a better informed community of people working on gait modeling and its applications.<br/><br/>Using recently purchased novel all-terrain gait treadmills, motion capture, video cameras, ground reaction forces based on force plates, and high speed ultrasonic images of knee and ankle joints, the PI and his team have collected gait data from multiple people.  The first goal of this project is to integrate all of this data by exploiting novel computer vision and multimodal optimization algorithms to produce an anatomically correct human skeleton with human specific joints, human shape, ground reaction forces and kinematic data.  The second goal is to develop an efficient, accurate, and general purpose parameterized model of human gait kinematics and biomechanics capable of reproducing the data, and more importantly, predicting and generalizing human specific gait for new types of terrain.  The gait variations to be modeled range from very slow shuffling to normal walking on a variety of terrain conditions such as inclined and declined slope, left and right cross-slope, and up and down stairs."
"1710859","Data-driven distributed control of mobile robotic networks: Where machine learning meets game theory","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","07/01/2017","06/15/2017","Minghui Zhu","PA","Pennsylvania State Univ University Park","Standard Grant","Anthony Kuh","06/30/2021","$300,000.00","","muz16@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","7607","1653","$0.00","Mobile robotic networks; (e.g., fleets of unmanned aerial vehicles) offer expanded capabilities for recognized military uses as well as a wide variety of civilian uses. There are several factors that contribute to their increasing potential and importance. In particular, technological advances have enabled smaller platforms with increased sensing, communication, and processing capabilities. In addition, autonomous operations offer several competitive advantages such as persistent surveillance that exceeds human fatigue limitations or remote operation capabilities without the logistical transport costs for assets and personnel. <br/><br/>Intellectual Merit: Distributed control becomes key to fully realize the potentials of mobile robotic networks. Current distributed control paradigms are mainly model-based and inadequate to handle significant uncertainties, including (1) environmental uncertainties; i.e., unforeseeable elements in unstructured environments where mobile robots operate; (2) dynamic uncertainties; i.e., inaccuracies of the physical dynamics of mobile robots. To bridge the gaps, this project will leverage reinforcement learning, an area of machine learning, and game theory, initially developed in economics, to develop a new data-driven (more specifically, model-free) distributed control framework. The developed framework is model-free, fully distributed, autonomous, and its performance is rigorously provable. The framework will significantly improve the autonomy of mobile robots when they face significant environmental uncertainties and dynamic uncertainties especially in long-term missions. <br/><br/>Broader Impacts: Successful completion of this research will provide engineering guidelines in analysis, synthesis and prototyping of mobile robotic networks which can effectively operate in unstructured environments. The research findings profoundly impact a variety of engineering disciplines, including scientific data collection, homeland security operations and intelligent transportation systems. The proposed research is interdisciplinary and involves interactions among game theory, machine learning, control, robotic motion planning and distributed algorithms. This will lead to educational and training opportunities that cross traditional disciplinary boundaries for high-school, undergraduate and graduate students in STEM. The collaborations with industrial partners stress the potentials to make an impact beyond academia."
"1743285","Workshop Support to Encourage Student Investigation of Gesture and Dialog","IIS","HCC-Human-Centered Computing, Robust Intelligence","06/01/2017","04/14/2020","Richard Voyles","IN","Purdue University","Standard Grant","Tatiana Korelsky","05/31/2021","$14,345.00","Juan Wachs","rvoyles@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7367, 7495","7367, 7495, 7556","$0.00","The connection between humans and their machines has never been more significant or more fundamental to our culture and our activities of daily living. While more prevalent in younger generations, humans of all ages are becoming increasingly dependent and engaged by devices and the human/machine relationship is in a state of extremely rapid change and growing importance to the national economy. Smartphones, robotic vacuum cleaners, web-enabled kitchen appliances, and virtual personal assistants are all around us and competing for our attention and our discretionary spending. In fact, we are engaging much more in a dialog with our machines, than ever before, eschewing the prior modalities of one-way command/response. Gestural interfaces have become so commonplace that many toddlers have begun to expect bi-directional interaction from inanimate objects. Much of this advancement has been driven by technological prowess and the race toward what is possible, rather than careful human-centric analysis and design. This is largely due to the complex interdisciplinary issues that are raised and the difficulties in bringing together the right collection of expertise to address them. The Workshop on Adaptive-Shot Learning for Gesture Understanding and Production, a scientific workshop on the intersection of gesture, machine learning and embodied intelligence, seeks to increase interest and activities in the interdisciplinary study of gesture and dialog for human/machine interaction through technical exchange and interaction which can lead to scientific advancements with high economic payoff and deep impact on well-being. Advancing the theoretical bases of these fields and their intersection can have enormous societal impact and will stimulate careful, multi-dimensional study of the field, promoting a diversity of thought and, ultimately, greater satisfaction for human users.<br/><br/>The goals of the Workshop include several things. First, increasing interest in the intersection of the fields of gesture recognition, verbal and non-verbal dialog, and embodied intelligence is key. This will result in a richer body of knowledge which, in turn, will result in more responsive, natural, and effective human/machine interfaces. Second, increasing the engagement of this younger generation of device users in the research and teaching of how these fields overlap is important. Familiar devices are important tools to engage new potential scientists and philosophers in the excitement of new discoveries. Finally, attracting a broad audience that cross-cuts the diverse fields of study of relevance to these new interaction modalities to enable outside-the-box thinking. The Workshop is organized by a diverse group of researchers from computer science, engineering, technology, psychology, and design and is held in Washington, DC, in connection to the IEEE Automatic Face and Gesture Recognition Conference, May 30-June 3, 2017. Travel grants supported by this grant are being offered, specifically targeting junior grad students with novel ideas, to join the established researchers to present their ideas in an informal setting. The diverse panel of organizers will recruit applicants to a gentle competition for these travel awards to encourage excellent submissions. This will achieve the goals, further educational attainment, and advance diversity of thought."
"1719133","CCF: CIF: Small: Interactive Learning from Noisy, Heterogeneous Feedback","CCF","Comm & Information Foundations","07/15/2017","07/07/2017","Kamalika Chaudhuri","CA","University of California-San Diego","Standard Grant","Phillip Regalia","06/30/2021","$499,999.00","Tara Javidi","kamalika@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7923, 7936, 9102","$0.00","The goal of this project is to develop interactive learning frameworks and methods that can learn predictors based on complex, imperfect feedback adaptively solicited in an on-line fashion from human annotators. Such predictors can significantly benefit the practice of machine learning by making it more accessible in domains where annotations are expensive. Currently, beyond a handful of heuristic studies, the only well-understood interactive learning setting is active binary classification, where a single annotator interactively provides labels to a learning algorithm. The main challenge in exploiting richer feedback is that human responses are inherently inconsistent and imperfect. This project will overcome this challenge by assuming that the responses come from unknown probability distributions with some mild yet realistic properties, which will be exploited to provide methods that can learn reliably from complex feedback.<br/><br/>Specifically, this project will introduce a general framework for interactive learning from imperfect, complex feedback, and develop methods for three common cases: (1) Active Learning with Abstention Feedback, where annotators can either provide a label or declare I Don't Know (2) Active Learning for Multiclass Classification, where the goal is to learn a classifier for a large number of classes and (3) Active Learning with Feedback from Multiple Annotators, where the goal is to combine feedback from many labelers with varying amounts of expertise subject to a budget. These problems will be approached through two main tools -- adaptive hypothesis testing and surrogate loss minimization. Combining these approaches will lead to principled algorithms for building accurate machine learning predictors with low annotation cost, which in turn, will benefit the practice of machine learning in domains where annotated data is expensive."
"1652038","CAREER: Building Energy-Efficient IoT Frameworks - A Data-Driven and Hardware-Friendly Approach Tailored for Wearable Applications","IIS","Info Integration & Informatics, CPS-Cyber-Physical Systems","02/15/2017","02/03/2020","Fengbo Ren","AZ","Arizona State University","Continuing Grant","Sylvia Spengler","01/31/2022","$438,875.00","","renfengbo@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7364, 7918","1045, 7364, 7918, 9102, 9251","$0.00","Sensor energy efficiency is the top critical concern that hinders long-term monitoring in energy-constrained Internet-of-things (IoT) applications. Conventional compressive sensing techniques fail to achieve satisfactory performance in IoT and especially wearable applications due to the lack of prior knowledge about signal models and the overlook of individual variability. The research goal of this CAREER plan is to develop a data-driven and hardware-friendly IoT framework to fundamentally address the unmet energy efficiency need of IoT and especially wearable applications. This will be accomplished by a systematic approach that seamlessly integrates compressive sensing and data analytics in compressed domains using deep learning methods. The proposed research will provide a transformative IoT framework that significantly reduces the data size for transmission from sensors to cloud while improving the overall quality of information delivery and bringing signal intelligence closer to users. The research outcomes will directly impact a variety of IoT applications, such as long-term environmental sensing for monitoring the airborne quality, radiation, water quality, hazardous chemicals, and many other environment indicators, by allowing compressive sensors to be deployed in energy-constrained environments to perform precise information acquisition over a significantly increased time span impossible with existing technologies. The proposed framework will also advance wearable technologies to enable important progress in transforming the existing healthcare model from episodic examination for disease diagnosis and treatment to continuous monitoring for disease prediction and prevention. This will make our healthcare systems more effective and economic and improve the overall quality of living for billions of individuals. The PI will take advantage of his affiliation with the I/UCRC Center for Embedded Systems at ASU to engage industry sponsors to accelerate technology adoption and transfer to benefit the society at large. The PI also plans to undertake an ambitious education program to actively engage and impact a diverse population of K-12, undergraduate, and graduate students to take away the PI?s research and create more values for the community in the long term.<br/><br/>The specific research objectives are to 1) formulate problems and develop efficient solvers to construct binary near-isometry embedding matrices to enable effective data compression on sensors through compressive sampling; 2) train deep neuron networks to decode information directly from the compressive samples for on-chip data analytics; 3) prototype the proposed framework in wearable hardware and evaluate the system performance over a variety of physiological signals. The research outcomes will allow future IoT devices to precisely sense and transfer the information of interest specified by users in an energy-efficient manner rather than recording imprecise data in raw forms as in existing approaches. The findings from this research will advance the theory development of data-driven compressive sensing by filling the current knowledge gap on how to design near-isometry embedding matrices with binary constraints that are essential for cost-effective hardware mapping. It will also uncover the intrinsic connections between compressive sensing and deep learning by establishing a viable data analytics solution for decoding high-level information directly from compressive samples. On the integration of research and education, the PI will enhance the current curriculum to better prepare students for careers in both industry and academic. The PI will take advantage of the FURI program at ASU to engage undergraduate students in research to foster their interest and motivation to pursue graduate degrees. ASU has one of the largest Hispanic and Native American student populations in the nation. The PI will make strong personal efforts to encourage the recruitment, retention, and advancement of the underrepresented groups. The PI will also collaborate with the Fulton Engineering Education Outreach office to initiate an exciting high school teacher training program, which aims to increase the level of literacy and interest in STEM fields of a large body of high school students through advanced coursework development."
"1800317","AF: Small: New Directions in Learning Theory","CCF","Algorithmic Foundations","08/01/2017","10/18/2017","Avrim Blum","IL","Toyota Technological Institute at Chicago","Standard Grant","A. Funda Ergun","05/31/2019","$98,041.00","","avrim@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926","$0.00","This project is to develop core principles and technologies for systems that learn from observation and experience in order to better help their users.  While there is already a significant body of work in the area of machine learning, today's interconnected world provides both new challenges and new opportunities that classic methods are not able to address or take advantage of.  This project has three main thrusts.  The first involves development of methods that can extract useful information from auxiliary sources in addition to traditional labeled data.  This includes methods for quickly learning multiple related tasks by taking advantage of ways in which they relate to each other.  The second involves approaches for learning about what different users or agents want by observing the results of their interactions.  Finally, the third thrust involves development of new rigorous methods for quickly estimating the amount of resources that would be needed to solve a given learning task.  Broader impacts of the project include the training of a diverse set of graduate students, improving undergraduate curricula with respect to machine learning technology, and developing a new book for advanced undergraduates on algorithms and analysis for data science.<br/><br/>More specifically, the first main thrust of this work involves a combination of unsupervised, semi-supervised, and multi-task learning. This work will investigate problems of estimating error rates from unlabeled data, unifying co-training and topic models, learning multiple related tasks from limited supervision, and learning new representations of data using tools from high-dimensional geometry.  The second main thrust will focus on reconstructing estimates of agent utilities from observing the outcomes of economic mechanisms such as combinatorial auctions.  This thrust also includes problems of learning the rules of unknown mechanisms from experimentation.  Finally, the last thrust focuses on development of the theory of property testing for machine learning problems, with the goal of quickly estimating natural formal measures of complexity of a given learning task."
"1652539","CAREER:  Scaling Up Knowledge Discovery in High-Dimensional Data Via Nonconvex Statistical Optimization","IIS","Info Integration & Informatics","08/01/2017","07/09/2018","Quanquan Gu","VA","University of Virginia Main Campus","Continuing grant","Aidong Zhang","12/31/2018","$199,777.00","","qgu@cs.ucla.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7364","1045, 7364","$0.00","The past decade has witnessed a surge of research activities on knowledge discovery in high-dimensional data, among which convex optimization-based methods are widely used. While convex optimization algorithms enjoy global convergence guarantees, they are not always scalable to high-dimensional massive data. Motivated by the empirical success of nonconvex methods such as matrix factorization, the objective of this project is to develop a new generation of principled nonconvex statistical optimization algorithms to scale up high-dimensional machine learning methods. This project amplifies the utility of high-dimensional knowledge discovery methods in various fields such as computational genomics and recommendation systems. It incorporates the resulting research outcomes into curriculum development and online courses, to train a new generation of machine learning and data mining practitioners. In addition, special training is provided to K-12 students and community college students for a broader education of modern data analysis techniques.<br/><br/><br/>This project consists of three synergistic research thrusts. First, it develops a family of nonconvex algorithms for structured sparse learning, including extensions to both parallel computing and distributed computing. Second, it devises a unified nonconvex optimization framework for low-rank matrix estimation, which covers a wide range of low-rank matrix learning problems such as matrix completion and preference learning. Several acceleration techniques are also explored. Third, it develops a family of alternating optimization algorithms, to solve the bi-convex optimization problem for estimating various complex statistical models. This project integrates modern optimization techniques with model-based statistical thinking, and provides a systematic way to design nonconvex high-dimensional machine learning methods with strong theoretical guarantees. The targeted applications include but not limited to computational genomics, neuroscience, and recommendation systems."
"1735752","EXP: Development of Human Language Technologies to Improve Disciplinary Writing and Learning through Self-Regulated Revising","IIS","ECR-EHR Core Research, Cyberlearn & Future Learn Tech","08/01/2017","04/30/2019","Rebecca Hwa","PA","University of Pittsburgh","Standard Grant","Tatiana Korelsky","07/31/2021","$557,555.00","Diane Litman, Amanda Godley","hwa@cs.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7980, 8020","8045, 8841, 9251","$0.00","Writing and revising are essential parts of learning, yet many college students graduate without<br/>demonstrating improvement or mastery of academic writing. This project explores the feasibility<br/>of improving students' academic writing through a revision environment that integrates natural<br/>language processing methods, best practices in data visualization and user interfaces, and<br/>current pedagogical theories. The environment will support and encourage students to develop<br/>self-regulation skills that are necessary for writing and revising, including goal-setting, selection<br/>of writing strategies, and self-monitoring of progress. As a learning technology, the environment<br/>can be applied on a large scale, thereby improving the writing of diverse student populations,<br/>including English learners. Additionally, the project's multidisciplinary training of graduate<br/>students is focused on increasing diversity in cyberlearning research and development. <br/><br/>Three stages of investigation are planned. First, to analyze data on students' revision behaviors,<br/>a series of experiments are conducted to study interactions between students<br/>and variations of the revision writing environment. Second, the collected data forms the gold<br/>standard for developing an end-to-end system that automatically extracts revisions between<br/>student drafts and identifies the goal for each revision. Multiple extraction algorithms are<br/>considered, including phrasal alignment based on semantic similarity metrics and deep learning<br/>approaches. To identify the goal of a revision, a supervised classifier is trained from the gold<br/>standard. A diverse set of features and the representations of the identified goals (e.g.,<br/>granularity, scope) are explored. In addition to the ""extract-then-classify"" pipeline, an alternative<br/>joint sequence labeling model is also developed. The labeling of sequences is used to<br/>recognize revision goals and the sequences are mutated to generate possible corrections of<br/>sentence alignments for revision extraction. The writing environment is iteratively refined,<br/>augmenting the interface prototyping through frequent user studies. Third, a complete<br/>end-to-end system that integrates the most successful component models is deployed in<br/>college-level writing classes. Student progress is tracked across multiple assignments."
"1746128","Collaborative Research: EAGER: SCIENCE: Systemic Cultivation of Inclusive Equitable Nurturing Classroom Ecology","IIS","Cyberlearn & Future Learn Tech","09/01/2017","08/29/2017","Holly Lawson","OR","Portland State University","Standard Grant","Chia Shen","08/31/2020","$89,000.00","","holly.lawson@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","CSE","8020","7916, 8045","$0.00","In this collaborative proposal, a team of human-computer interaction and learning science researchers will collaborate with science education practitioners to develop and study a novel learning genre that aims to promote equity in science education for 4th-9th graders. The research program targets students with visual impairments (VIs), who face many challenges in the education system, especially in science courses. Unlike subjects in the humanities and social sciences, science education relies heavily on visualizations using charts, diagrams, and images, in addition to print materials. Consequently, students with VIs cannot readily access those visualizations and are burdened with misconceptions of science-related constructs. The proposed learning genre addresses this critical need for equitable access by including innovative  multimodal artifacts and a new pedagogical methodology for science education for both sighted and visually impaired students. These multimodal artifacts called Sensables, the innovation at the center of the learning genre, leverage burgeoning technologies and movements such as maker technology and culture, computer vision, and mobile devices. Sensables are 3D printed models that have ""hotspots"" that respond to a user's touch by playing a media file on a nearby device. These media files can be audio descriptions of the model component, related sounds or images, or even braille annotations. The investigators will study how students learn with these artifacts, and create a web-based sharing and discussion portal, software tools, and instructional resources for teachers to promote broad adoption of the learning genre in the classroom.<br/><br/>Through iterative design and engineering, evaluation, and data analysis, this interdisciplinary design-based research program will contribute to both the human-computer interaction and learning science fields. The investigators will conduct a single case design study to evaluate and refine the design of Sensables. The project's output includes new curricular materials and a pedagogical methodology for using Sensables in science education. The study will answer the following research questions: (1) What aspects of the Sensables artifacts and pedagogical methodology afford learning? (2) To what extent do Sensables help students learn material compared to current standards? (3) What type of science content are Sensables best suited for? Similarly, what type of content is not appropriate for learning with Sensables? The investigators will develop and disseminate their research outcome to the academic community as well as education and disability professionals to pave the way for broad use and adoption of this learning technology."
"1746123","Collaborative Research: EAGER: SCIENCE: Systemic Cultivation of Inclusive Equitable Nurturing Classroom Ecology","IIS","Cyberlearn & Future Learn Tech","09/01/2017","08/29/2017","Shiri Azenkot","NY","Cornell University","Standard Grant","Chia Shen","08/31/2020","$211,001.00","","shiri.azenkot@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8020","7916, 8045","$0.00","In this collaborative proposal, a team of human-computer interaction and learning science researchers will collaborate with science education practitioners to develop and study a novel learning genre that aims to promote equity in science education for 4th-9th graders. The research program targets students with visual impairments (VIs), who face many challenges in the education system, especially in science courses. Unlike subjects in the humanities and social sciences, science education relies heavily on visualizations using charts, diagrams, and images, in addition to print materials. Consequently, students with VIs cannot readily access those visualizations and are burdened with misconceptions of science-related constructs. The proposed learning genre addresses this critical need for equitable access by including innovative  multimodal artifacts and a new pedagogical methodology for science education for both sighted and visually impaired students. These multimodal artifacts called Sensables, the innovation at the center of the learning genre, leverage burgeoning technologies and movements such as maker technology and culture, computer vision, and mobile devices. Sensables are 3D printed models that have ""hotspots"" that respond to a user's touch by playing a media file on a nearby device. These media files can be audio descriptions of the model component, related sounds or images, or even braille annotations. The investigators will study how students learn with these artifacts, and create a web-based sharing and discussion portal, software tools, and instructional resources for teachers to promote broad adoption of the learning genre in the classroom.<br/><br/>Through iterative design and engineering, evaluation, and data analysis, this interdisciplinary design-based research program will contribute to both the human-computer interaction and learning science fields. The investigators will conduct a single case design study to evaluate and refine the design of Sensables. The project's output includes new curricular materials and a pedagogical methodology for using Sensables in science education. The study will answer the following research questions: (1) What aspects of the Sensables artifacts and pedagogical methodology afford learning? (2) To what extent do Sensables help students learn material compared to current standards? (3) What type of science content are Sensables best suited for? Similarly, what type of content is not appropriate for learning with Sensables? The investigators will develop and disseminate their research outcome to the academic community as well as education and disability professionals to pave the way for broad use and adoption of this learning technology."
"1725663","SPX: Collaborative Research: Multicore to Wide Area Analytics on Streaming Data","CCF","SPX: Scalable Parallelism in t","08/15/2017","08/02/2017","Phillip Gibbons","PA","Carnegie-Mellon University","Standard Grant","A. Funda Ergun","07/31/2021","$492,000.00","","gibbons@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","042Y","026Z","$0.00","In today's big data era, there is an urgent need for methods that can quickly derive analytical insights from large volumes of data that are continuously generated.  Such streaming data include video, audio, activity logs, and sensor data, and are generated on a massive scale all over the world.  The need for real-time streaming analytics can only be fulfilled with the help of appropriately designed parallel and distributed algorithms. However, parallel and distributed computing systems come in a variety of shapes and sizes, and algorithms should be designed to match the characteristics of the underlying system. This project develops methods for analyzing massive streaming data on computing systems ranging from machines with multiple cores sharing memory to geo-distributed data centers communicating over wide-area networks. The results of this research are expected to improve the efficiency, latency, and throughput of streaming analytics.  Due to the foundational nature of the analytical tasks considered, results of this project will impact disciplines that use large-scale machine learning and graph analytics, including cybersecurity, social network analysis, and transportation. Resulting software will be released as toolkits on stream processing platforms, and deployed in a smart-city camera infrastructure. Synergy between the research goals and the teaching goals of the PIs will lead to new instructional material in existing courses as well as development of new courses in data analytics. Individuals from underrepresented groups will be included as a part of the project. The project will benefit from and strengthen collaborations between academia, industry, and national labs on streaming analytics.<br/> <br/>The first technical thrust of the project is on designing shared memory parallel algorithms for computation on data streams, that can achieve a high throughput and fast convergence for complex analytics tasks. The second thrust is on designing distributed streaming algorithms that can tolerate variable communication delays and adapt to available bandwidth in a wide-area network, through identifying good tradeoffs between freshness of results and volume of communication. These advances will be studied in the context of fundamental graph analytics and machine learning tasks such as subgraph counting, graph connectivity and clustering, matrix factorization, and deep networks. The project will utilize the vast body of theory and techniques developed in the realm of parallel computing in the design of methods for processing streaming data, leading to a toolkit of techniques that can be reused across applications. The project will also lead to advances in sequential streaming and incremental algorithms for certain problems; for instance, problems in machine learning that use iterative convergent methods. Based on the techniques designed, the project will design and build a hierarchical parameter server that operates effectively across the spectrum from multicore machines to data centers to wide-area data sources.<br/>"
"1645737","CPS: Synergy: Image-Based Indoor Navigation for Visually Impaired Users","ECCS","CPS-Cyber-Physical Systems","02/01/2017","09/21/2016","Marco Duarte","MA","University of Massachusetts Amherst","Standard Grant","Radhakisan Baheti","01/31/2021","$749,993.00","Aura Ganz","mduarte@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","ENG","7918","153E, 7918","$0.00","Severe visual impairment and blindness preclude many essential activities of daily living. Among these is independent navigation in unfamiliar indoor spaces without the assistance of a sighted companion. We propose to develop PERCEPT-V: an organic vision-driven, smartphone-based indoor navigation system, in which the user can navigate in open spaces without requiring retrofit of the environment. When the user seeks to obtain navigation instructions to a chosen destination, the smartphone will record observations from multiple onboard sensors in order to perform user localization. Once the location and orientation of the user are estimated, they are used to calculate the coordinates of the navigation landmarks surrounding the user. The system can then provide directions to the chosen destination, as well as an optional description of the landmarks around the user.<br/><br/>We will focus on addressing the cyber-physical systems technology shortcomings usually encountered in the development of indoor navigation systems. More specifically, our project will consider the following transformative aspects in the design of PERCEPT-V: (i) Image-Based Indoor Localization and Orientation: PERCEPT-V will feature new computer vision-based localization algorithms that reduce the dependence on highly controlled image capture and richly informative images, increasing the reliability of localization from images taken by blind subjects in crowded environments; (ii) Customized Navigation Instructions: PERCEPT-V will deliver customized navigation instructions for sight-impaired users that accounts for diverse levels of confidence and operator capabilities. A thorough final evaluation study featuring visually impaired participants will assess our hypotheses driving the design and refinements of PERCEPT-V using rigorous statistical analysis."
"1733860","AitF: Mechanism Design and Machine Learning for Peer Grading","CCF","Algorithms in the Field, Algorithmic Foundations","09/01/2017","04/06/2018","Jason Hartline","IL","Northwestern University","Standard Grant","Tracy Kimbrel","08/31/2021","$716,000.00","Eleanor O'Rourke, Douglas Downey","hartline@eecs.northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7239, 7796","7932, 9102, 9251","$0.00","This project explores the design and analysis of peer grading technology.  A peer grading system is an online tool that collects student submissions, assigns review tasks to the students and graders, and aggregates reviews to produce assessments of both the submissions and the peer reviews.  The PIs have developed a prototype system and have collected preliminary evidence that suggests that peer review has important potential benefits:<br/><br/>1.  Learning by reviewing: Students learn from critical assessment of other students' work.  In the PIs' prototype at Northwestern, 60% of the students reported that peer review helped them learn course material and 55% of the students reported that peer review helped them to prepare better homework solutions themselves.<br/><br/>2.  Reduced grading staff: Peer grading reduces the grading load on course staff and allows for effective teaching with larger classes.  This is especially important currently, as interest in computer science classes increases at a faster pace than teaching resources.  In the PIs' prototype at Northwestern, the course staff graded 1/5 of the student submissions.<br/><br/>3.  Promptness of feedback: Reduced teacher grading enables prompt feedback to students.  In the PIs' prototype at Northwestern, peer reviews were available within three days and final assessment of both the submission and peer reviews were available within five days.  Prior to introducing peer review, assessments took one to two weeks.<br/><br/>A peer grading system is comprised of three main components:<br/><br/>1.  The review matching algorithm determines which peers should review which submissions and which submissions should be reviewed by the teacher.<br/><br/>2.  The submission grading algorithm aggregates the reviews of the peers and the submissions and assigns grades to the submissions.<br/><br/>3.  The review grading algorithm compares the peer reviews with the teacher reviews and assigns grades to the peer reviews.  Without this algorithm, peers may not put effort into providing quality reviews, and the reviews will be neither accurate for grading nor beneficial for the peer.<br/><br/>The details of these algorithms are crucial for the proper working of the peer review system.  A main research effort of this project is to identify the algorithms to use for each of these components.  The review matching algorithm affects the accuracy of the subsequent grading algorithms and the grading load of the teacher.  The submission grading algorithm determines which peer reviews are accurate and which are inaccurate and uses this understanding to assign grades to the submissions that are representative of the submission quality.  The review grading algorithm incentivizes the peers to put in sufficient effort to determine whether a submission is good or bad and it is calibrated so that good reviews and bad reviews get the appropriate review grades.  <br/><br/>The PIs have implemented prototypes of these algorithms as part of a peer grading system that has been prototyped in Northwestern computer science classes.  However, the space of possible algorithms is large and the PIs' work on the prototype has yet to determine the algorithms that combine to give the best education outcomes.  A main focus of this project will be improving the understanding of which algorithms lead to the best education outcomes.<br/><br/>Theoretical work in algorithms and machine learning provides a starting point for the project's study of good algorithms for peer grading systems.  A key endeavor of the project is translating and applying these theoretical algorithms to the peer grading domain.  As one example, proper scoring rules are a natural approach for grading the peer reviews.  However, test runs of the PIs' prototype implementation suggest that these rules might not be so good in practice.  Both new models and algorithms are needed in theory, and these new algorithms need to work in practice."
"1651122","Collaborative Research:   AGNES:  Algebraic Geometry NorthEastern Series","DMS","ALGEBRA,NUMBER THEORY,AND COM","03/01/2017","02/03/2017","Christian Schnell","NY","SUNY at Stony Brook","Standard Grant","Janet Striuli","02/29/2020","$37,325.00","Samuel Grushevsky","christian.schnell@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","MPS","1264","7556","$0.00","This award supports participation in six AGNES (Algebraic Geometry Northeastern Series) conferences which will be held at Stony Brook University on April 21-23, 2017, at Northeastern University in Fall 2017, at Rutgers University in Spring 2018, at Brown University in Fall 2018, at University of Massachusetts Amherst in Spring 2019, and at Boston College in Fall 2019. AGNES is a semi-annual algebraic geometry meeting organized at a rotating location by an association of northeastern universities. A guiding goal of AGNES is to introduce graduate students to a broad spectrum of current research in algebraic geometry. While meeting this goal, AGNES has become an important center for algebraic geometry in the US. The conference has an extensive record of promoting collaboration and has developed new tools for training and mentoring future leaders in algebraic geometry. These consist of graduate mini-schools, career panels, poster presentations for junior participants (including undergraduate students), and events specifically targeting members of groups that are under-represented in the mathematical sciences.<br/><br/>Algebraic geometry is a sub-field of mathematical sciences that studies solution sets of polynomial equations. It has deep connections to many other areas of pure mathematics, such as topology, arithmetic, number theory, differential geometry, dynamical systems, and homological algebra. At the same time algebraic geometry has found important applications in applied mathematics and computer science (computer vision, geometric complexity theory, numerical methods in algebraic statistics, etc). The centerpiece research activity of each AGNES meeting will be 7-8 invited lectures given by speakers showcasing different facets of algebraic geometry. The scientific scope of AGNES will be further expanded by lectures from neighboring mathematical subjects (such as arithmetic geometry, dynamics, complex geometry, and computational geometry) and by the various supporting activities described above. Further information can be found at the conference website: http://www.agneshome.org/"
"1651082","Collaborative Research: AGNES: Algebraic Geometry NorthEastern Series","DMS","ALGEBRA,NUMBER THEORY,AND COM","03/01/2017","03/05/2019","Maksym Fedorchuk","MA","Boston College","Continuing Grant","Janet Striuli","02/29/2020","$34,200.00","Dawei Chen, Brian Lehmann","maksym.fedorchuk@bc.edu","140 Commonwealth Avenue","Chestnut Hill","MA","024673800","6175528000","MPS","1264","7556","$0.00","This award supports participation in six AGNES (Algebraic Geometry Northeastern Series) conferences which will be held at Stony Brook University on April 21-23, 2017, at Northeastern University in Fall 2017, at Rutgers University in Spring 2018, at Brown University in Fall 2018, at University of Massachusetts Amherst in Spring 2019, and at Boston College in Fall 2019. AGNES is a semi-annual algebraic geometry meeting organized at a rotating location by an association of northeastern universities. A guiding goal of AGNES is to introduce graduate students to a broad spectrum of current research in algebraic geometry. While meeting this goal, AGNES has become an important center for algebraic geometry in the US. The conference has an extensive record of promoting collaboration and has developed new tools for training and mentoring future leaders in algebraic geometry. These consist of graduate mini-schools, career panels, poster presentations for junior participants (including undergraduate students), and events specifically targeting members of groups that are under-represented in the mathematical sciences.<br/><br/>Algebraic geometry is a sub-field of mathematical sciences that studies solution sets of polynomial equations. It has deep connections to many other areas of pure mathematics, such as topology, arithmetic, number theory, differential geometry, dynamical systems, and homological algebra. At the same time algebraic geometry has found important applications in applied mathematics and computer science (computer vision, geometric complexity theory, numerical methods in algebraic statistics, etc). The centerpiece research activity of each AGNES meeting will be 7-8 invited lectures given by speakers showcasing different facets of algebraic geometry. The scientific scope of AGNES will be further expanded by lectures from neighboring mathematical subjects (such as arithmetic geometry, dynamics, complex geometry, and computational geometry) and by the various supporting activities described above. Further information can be found at the conference website: http://www.agneshome.org/"
"1650459","Collaborative Research: AGNES: Algebraic Geometry NorthEastern Series","DMS","ALGEBRA,NUMBER THEORY,AND COM","03/01/2017","02/27/2018","Melody Chan","RI","Brown University","Continuing Grant","Janet Striuli","02/29/2020","$32,725.00","Dan Abramovich","mtchan@math.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1264","7556, 9150","$0.00","This award supports participation in six AGNES (Algebraic Geometry Northeastern Series) conferences which will be held at Stony Brook University on April 21-23, 2017, at Northeastern University in Fall 2017, at Rutgers University in Spring 2018, at Brown University in Fall 2018, at University of Massachusetts Amherst in Spring 2019, and at Boston College in Fall 2019. AGNES is a semi-annual algebraic geometry meeting organized at a rotating location by an association of northeastern universities. A guiding goal of AGNES is to introduce graduate students to a broad spectrum of current research in algebraic geometry. While meeting this goal, AGNES has become an important center for algebraic geometry in the US. The conference has an extensive record of promoting collaboration and has developed new tools for training and mentoring future leaders in algebraic geometry. These consist of graduate mini-schools, career panels, poster presentations for junior participants (including undergraduate students), and events specifically targeting members of groups that are under-represented in the mathematical sciences.<br/><br/>Algebraic geometry is a sub-field of mathematical sciences that studies solution sets of polynomial equations. It has deep connections to many other areas of pure mathematics, such as topology, arithmetic, number theory, differential geometry, dynamical systems, and homological algebra. At the same time algebraic geometry has found important applications in applied mathematics and computer science (computer vision, geometric complexity theory, numerical methods in algebraic statistics, etc). The centerpiece research activity of each AGNES meeting will be 7-8 invited lectures given by speakers showcasing different facets of algebraic geometry. The scientific scope of AGNES will be further expanded by lectures from neighboring mathematical subjects (such as arithmetic geometry, dynamics, complex geometry, and computational geometry) and by the various supporting activities described above. Further information can be found at the conference website: http://www.agneshome.org/"
"1728933","DMREF: High Throughput Design of Metallic Glasses with Physically Motivated Descriptors","DMR","METAL & METALLIC NANOSTRUCTURE, DMREF","10/01/2017","07/27/2020","Dane Morgan","WI","University of Wisconsin-Madison","Standard Grant","John Schlueter","09/30/2021","$1,298,636.00","John Perepezko, Paul Voyles, Izabela Szlufarska, Dan Thoma","ddmorgan@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1771, 8292","054Z, 094Z, 095Z, 7433, 8004, 8400","$0.00","Non-technical Description: Silica-based glasses are familiar to most of us from our experiences with everything from windows to wineglasses. However, when cooled quickly enough, some metal alloys can also form a glassy state. Metallic glasses have attractive properties such as high elastic modulus, excellent strength, good biocompatibility, and the ability to be processed like plastics. Applications include packaging, arterial stents, water purification, and Micro-Electro-Mechanical Systems gears and springs. The slowest cooling rate that still forms a glass is called the critical cooling rate. Although many metals can form a glass, only a rare set of alloys have slow enough critical cooling rate that they can form a significant bulk volume of glassy material, and these alloys are said to have good glass forming ability. Despite the importance of these materials and years of research, there are still no rigorous, consistent, and quantitative rules to predict the actual glass forming ability of a metallic alloy system. To solve this problem, this project will develop an extensible materials informatics framework for predicting the glass-forming ability of metal alloys, and then apply that framework to develop aluminum- and magnesium-based alloys with improved glass forming ability.<br/><br/>Technical description: In order to discover new aluminum- and magnesium-based bulk metallic glasses with superior glass-forming ability, the team will execute a dual-loop iterative materials design approach. A rapid materials design loop will provide high-throughput materials discovery by integrating experimental and simulated data with machine learning methods. An unprecedented body of experimental data on glass forming ability and basic mechanical properties will be generated by combinatorial 3D printing synthesis, followed by rapid optical, microscopy, thermal, and nanomechanical characterization. A similarly unique database of liquid and glass thermodynamic, kinetic, and structural properties will be determined by automated, high-throughput ab initio molecular dynamics. Machine-learning methods, trained on the data and physically motivated descriptors from existing experiments and the ab initio molecular dynamics simulation, will search a space of up to hundreds of thousands of potential alloys for the most promising candidates, which will then be synthesized, characterized and used to refine the models. Slower descriptor design loop studies will study select alloys in detail with fluctuation electron microscopy and extensive simulations to develop improved descriptors, which will then be incorporated into the rapid materials design loop and further validated by their predictive ability. This work will produce the first set of large-scale databases with both true measures of glass forming ability and extensive thermophysical data from simulations, and integrate them to generate physical descriptor driven machine-learning models for iterative new metallic glass search and discovery. The PIs also plan to release the Materials Simulation Toolkit - Machine Learning (MASTML) as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the MASTML codebase. This will allow a wider growth of the project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1737746","ATD:   Geospatial Graphical Models of Human Response to Emergencies","DMS","ATD-Algorithms for Threat Dete, , , ","08/15/2017","07/26/2018","Adrian Dobra","WA","University of Washington","Continuing Grant","Leland Jameson","07/31/2021","$250,219.00","Nathalie Williams","adobra@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","046Y, P412, P413, P414","6877","$0.00","Emergencies such as natural disasters and terrorist attacks occur frequently around the globe and regularly cause massive destruction. The goal of this project is to improve the ability to use publicly-available large-scale human-generated geospatial data sources for identification of emergency events. Currently, the largest impediment to using such data to identify emergency events is that very little is known about spatiotemporal patterns of human behavioral responses to these events. This project seeks to create novel machine learning tools to identify spatiotemporal patterns of reactions in human behavioral responses to emergency events using human-generated geospatial data sources.  The key research goals are: (1) build a corpus of geolocated emergency and non-emergency events; (2) develop state of the art machine learning models that detect geospatial clusters of locations with anomalous human behavior based on aggregated geolocated human activity data, together with advanced geographic information system visualizations; and (3) create a catalogue of behavioral signatures characterizing emergency and non-emergency events. The project will improve knowledge about human responses to emergency events by studying their behavioral signatures through key social science theories on fear or violence and crime, the social psychology of fear, threat, and risk, and the sociology of disasters.<br/><br/>Geospatial graphical models will be at the core of the machine learning tools for emergency-event identification. These novel models generalize Gaussian graphical models to non-Gaussian multiway tensor data with spatial and temporal dependencies across multiple dimensions, and represent contributions to statistical and machine learning theory. Multiway tensor geospatial graphical models have the ability to capture the uncertainty in unknown multivariate interactions together with known dependencies (e.g., spatial) in a coherent manner. Non-separable spatiotemporal versions of these models will be developed as extensions of Bayesian dynamic linear models, and of multivariate factor models with correlated residuals. By exploiting the theory on Markov marked point processes and multivariate birth-death processes, this project will develop efficient, robust methods of inference and model determination. The methodological research on geospatial graphical models has a vast domain of applicability that encompasses all the scientific fields in which the analysis of multivariate spatiotemporal datasets is key: public health policy, social and political sciences, finance, electrical engineering, and computer science. These models are directly applicable in the analysis of high-resolution smartphone-based GPS locations datasets, which are now actively being collected in studies that seek to characterize the relationship between geographic and contextual attributes of the environment and human energy balance, to study segregation, environmental exposure, and accessibility in social science research, or to understand the relationship between health-risk behavior in adolescents and community disorder. The patterns and cross-national variance in response to emergency events will also inform the social sciences."
"1664848","QuBBD: Collaborative Research: Quantifying Morphologic Phenotypes in Prostate Cancer - Developing Topological Descriptors for Machine Learning Algorithms","DMS","CDS&E-MSS","08/01/2017","05/19/2020","Carola Wenk","LA","Tulane University","Standard Grant","Branislav Vidakovic","07/31/2021","$479,293.00","Jonathon Brown, Brian Summa, Andrew Sholl","cwenk@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","MPS","8069","8083, 9150","$0.00","The long-term goal of this project is to develop quantitative methodology for detecting geometric and topological features in point clouds extracted from (histology) images.  Of particular relevance, this project considers the setting of prostate cancer classification, which is based on a pathologist grading of histology slides using the Gleason grading system.  These pathology slides are a source of biomedical big data that are increasingly available as archived material.  Developing these quantitative methods will be a significant advance towards a (semi-)automated quantification of prostate cancer aggressiveness.  This award supports an interdisciplinary team of investigators in computational mathematics, computer science, biomedical engineering, and pathology to develop mathematical and computational tools based on topological descriptors and machine learning in order to distinguish between different morphological types of prostate cancer.<br/><br/>This research will develop quantitative topological descriptors (e.g., persistence diagrams and summaries) that describe natural histologic phenotypes in prostate cancer, in order to provide explanatory information to assist in providing improved diagnostics/prognostics and insight into the best course of treatment for the patient. This will be accomplished through developing graphical models via unsupervised machine learning that increase our understanding of prostate cancer subtypes. The long-term goal is to develop imaging biomarkers that better identify indolent from aggressive prostate cancer compared to existing, subjective, and variable human observer analyses (i.e., the Gleason score). This project takes steps towards a novel quantitative methodology for prostate cancer classification, as well as towards developing topological methods for statistically distinguishing different types of glandular architectures."
"1741564","I-Corps: An Objective Clinical Machine Learning Imaging Technology","IIP","I-Corps","04/15/2017","04/13/2017","Roman Lubynsky","MA","Massachusetts Institute of Technology","Standard Grant","Anita La Salle","09/30/2018","$50,000.00","","rml@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is creating computer-aided classification methods to better identify and distinguish various diseases. Skin issues are the most frequently addressed issues presented in primary care: over one-third of primary care visits include at least one skin problem. Skin issues are visual manifestations of many different types of diseases, including allergies, injuries, bacterial infections, viral infections, and hormonal fluctuations. A physician's ability to address skin issues is highly dependent on a physician's prior visual training experiences. However, many physicians are not regularly familiar with the over 2,000 dermatology issues that can arise in a patient. The algorithms developed here allow for objective and much more accurate screening of exposed skin to improve evaluation of patient issues in many different care settings. This could be implemented for many different types of skin diseases and other visual constructions of images to assist physicians visually and vastly augment their sensitivity and specificity at identifying dermatology issues and their underlying causes.<br/><br/>This I-Corps project is based on using image processing and machine learning to distinguish dermatological diseases. Image-capturing electronics such as consumer-grade cameras and smartphones are becoming increasingly ubiquitous, capable, and affordable. Image processing and machine learning based on annotated images has shown great potential for algorithms to differentiate between images capturing different objects and patterns. The developments here have created a classification system that differentiates suspicious from non-suspicious lesions that is easy for a physician to use in a clinical setting to help with clinical decision-making. The proof of concept is based on a large dataset of prospective clinical images where the algorithms classified malignant cases and the methods resulted in a statistically significant discrimination between suspicious and non-suspicious lesions."
"1725702","SPX: Collaborative Research: Multicore to Wide Area Analytics on Streaming Data","CCF","SPX: Scalable Parallelism in t","08/15/2017","12/02/2019","Srikanta Tirthapura","IA","Iowa State University","Standard Grant","Tracy Kimbrel","07/31/2021","$308,000.00","","snt@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","042Y","026Z","$0.00","In today's big data era, there is an urgent need for methods that can quickly derive analytical insights from large volumes of data that are continuously generated.  Such streaming data include video, audio, activity logs, and sensor data, and are generated on a massive scale all over the world.  The need for real-time streaming analytics can only be fulfilled with the help of appropriately designed parallel and distributed algorithms. However, parallel and distributed computing systems come in a variety of shapes and sizes, and algorithms should be designed to match the characteristics of the underlying system. This project develops methods for analyzing massive streaming data on computing systems ranging from machines with multiple cores sharing memory to geo-distributed data centers communicating over wide-area networks. The results of this research are expected to improve the efficiency, latency, and throughput of streaming analytics.  Due to the foundational nature of the analytical tasks considered, results of this project will impact disciplines that use large-scale machine learning and graph analytics, including cybersecurity, social network analysis, and transportation. Resulting software will be released as toolkits on stream processing platforms, and deployed in a smart-city camera infrastructure. Synergy between the research goals and the teaching goals of the PIs will lead to new instructional material in existing courses as well as development of new courses in data analytics. Individuals from underrepresented groups will be included as a part of the project. The project will benefit from and strengthen collaborations between academia, industry, and national labs on streaming analytics.<br/> <br/>The first technical thrust of the project is on designing shared memory parallel algorithms for computation on data streams, that can achieve a high throughput and fast convergence for complex analytics tasks. The second thrust is on designing distributed streaming algorithms that can tolerate variable communication delays and adapt to available bandwidth in a wide-area network, through identifying good tradeoffs between freshness of results and volume of communication. These advances will be studied in the context of fundamental graph analytics and machine learning tasks such as subgraph counting, graph connectivity and clustering, matrix factorization, and deep networks. The project will utilize the vast body of theory and techniques developed in the realm of parallel computing in the design of methods for processing streaming data, leading to a toolkit of techniques that can be reused across applications. The project will also lead to advances in sequential streaming and incremental algorithms for certain problems; for instance, problems in machine learning that use iterative convergent methods. Based on the techniques designed, the project will design and build a hierarchical parameter server that operates effectively across the spectrum from multicore machines to data centers to wide-area data sources.<br/>"
"1650256","Collaborative Research: AGNES: Algebraic Geometry NorthEastern Series","DMS","ALGEBRA,NUMBER THEORY,AND COM","03/01/2017","10/12/2018","Paul Hacking","MA","University of Massachusetts Amherst","Continuing Grant","Janet Striuli","02/29/2020","$33,590.00","Eyal Markman","hacking@math.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","MPS","1264","7556","$0.00","This award supports participation in six AGNES (Algebraic Geometry Northeastern Series) conferences which will be held at Stony Brook University on April 21-23, 2017, at Northeastern University in Fall 2017, at Rutgers University in Spring 2018, at Brown University in Fall 2018, at University of Massachusetts Amherst in Spring 2019, and at Boston College in Fall 2019. AGNES is a semi-annual algebraic geometry meeting organized at a rotating location by an association of northeastern universities. A guiding goal of AGNES is to introduce graduate students to a broad spectrum of current research in algebraic geometry. While meeting this goal, AGNES has become an important center for algebraic geometry in the US. The conference has an extensive record of promoting collaboration and has developed new tools for training and mentoring future leaders in algebraic geometry. These consist of graduate mini-schools, career panels, poster presentations for junior participants (including undergraduate students), and events specifically targeting members of groups that are under-represented in the mathematical sciences.<br/><br/>Algebraic geometry is a sub-field of mathematical sciences that studies solution sets of polynomial equations. It has deep connections to many other areas of pure mathematics, such as topology, arithmetic, number theory, differential geometry, dynamical systems, and homological algebra. At the same time algebraic geometry has found important applications in applied mathematics and computer science (computer vision, geometric complexity theory, numerical methods in algebraic statistics, etc). The centerpiece research activity of each AGNES meeting will be 7-8 invited lectures given by speakers showcasing different facets of algebraic geometry. The scientific scope of AGNES will be further expanded by lectures from neighboring mathematical subjects (such as arithmetic geometry, dynamics, complex geometry, and computational geometry) and by the various supporting activities described above. Further information can be found at the conference website: http://www.agneshome.org/"
"1651014","Collaborative Research: AGNES:  Algebraic Geometry NorthEastern Series","DMS","ALGEBRA,NUMBER THEORY,AND COM","03/01/2017","02/27/2018","Lev Borisov","NJ","Rutgers University New Brunswick","Continuing Grant","Janet Striuli","02/29/2020","$35,640.00","Anders Buch","borisov@math.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","MPS","1264","7556","$0.00","This award supports participation in six AGNES (Algebraic Geometry Northeastern Series) conferences which will be held at Stony Brook University on April 21-23, 2017, at Northeastern University in Fall 2017, at Rutgers University in Spring 2018, at Brown University in Fall 2018, at University of Massachusetts Amherst in Spring 2019, and at Boston College in Fall 2019. AGNES is a semi-annual algebraic geometry meeting organized at a rotating location by an association of northeastern universities. A guiding goal of AGNES is to introduce graduate students to a broad spectrum of current research in algebraic geometry. While meeting this goal, AGNES has become an important center for algebraic geometry in the US. The conference has an extensive record of promoting collaboration and has developed new tools for training and mentoring future leaders in algebraic geometry. These consist of graduate mini-schools, career panels, poster presentations for junior participants (including undergraduate students), and events specifically targeting members of groups that are under-represented in the mathematical sciences.<br/><br/>Algebraic geometry is a sub-field of mathematical sciences that studies solution sets of polynomial equations. It has deep connections to many other areas of pure mathematics, such as topology, arithmetic, number theory, differential geometry, dynamical systems, and homological algebra. At the same time algebraic geometry has found important applications in applied mathematics and computer science (computer vision, geometric complexity theory, numerical methods in algebraic statistics, etc). The centerpiece research activity of each AGNES meeting will be 7-8 invited lectures given by speakers showcasing different facets of algebraic geometry. The scientific scope of AGNES will be further expanded by lectures from neighboring mathematical subjects (such as arithmetic geometry, dynamics, complex geometry, and computational geometry) and by the various supporting activities described above. Further information can be found at the conference website: http://www.agneshome.org/"
"1650462","Collaborative Research: AGNES: Algebraic Geometry NorthEastern Series","DMS","ALGEBRA,NUMBER THEORY,AND COM","03/01/2017","02/03/2017","Alina Marian","MA","Northeastern University","Standard Grant","Janet Striuli","02/28/2021","$36,220.00","Ana-Maria Castravet","A.Marian@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","MPS","1264","7556","$0.00","This award supports participation in six AGNES (Algebraic Geometry Northeastern Series) conferences which will be held at Stony Brook University on April 21-23, 2017, at Northeastern University in Fall 2017, at Rutgers University in Spring 2018, at Brown University in Fall 2018, at University of Massachusetts Amherst in Spring 2019, and at Boston College in Fall 2019. AGNES is a semi-annual algebraic geometry meeting organized at a rotating location by an association of northeastern universities. A guiding goal of AGNES is to introduce graduate students to a broad spectrum of current research in algebraic geometry. While meeting this goal, AGNES has become an important center for algebraic geometry in the US. The conference has an extensive record of promoting collaboration and has developed new tools for training and mentoring future leaders in algebraic geometry. These consist of graduate mini-schools, career panels, poster presentations for junior participants (including undergraduate students), and events specifically targeting members of groups that are under-represented in the mathematical sciences.<br/><br/>Algebraic geometry is a sub-field of mathematical sciences that studies solution sets of polynomial equations. It has deep connections to many other areas of pure mathematics, such as topology, arithmetic, number theory, differential geometry, dynamical systems, and homological algebra. At the same time algebraic geometry has found important applications in applied mathematics and computer science (computer vision, geometric complexity theory, numerical methods in algebraic statistics, etc). The centerpiece research activity of each AGNES meeting will be 7-8 invited lectures given by speakers showcasing different facets of algebraic geometry. The scientific scope of AGNES will be further expanded by lectures from neighboring mathematical subjects (such as arithmetic geometry, dynamics, complex geometry, and computational geometry) and by the various supporting activities described above. Further information can be found at the conference website: http://www.agneshome.org/"
"1664858","QuBBD: Collaborative Research: Quantifying Morphologic Phenotypes in Prostate Cancer - Developing Topological Descriptors for Machine Learning Algorithms","DMS","CDS&E-MSS","08/01/2017","08/14/2017","Brittany Fasy","MT","Montana State University","Standard Grant","Branislav Vidakovic","07/31/2021","$420,706.00","John Sheppard","brittany@cs.montana.edu","309 MONTANA HALL","BOZEMAN","MT","597172470","4069942381","MPS","8069","8083, 9150","$0.00","The long-term goal of this project is to develop quantitative methodology for detecting geometric and topological features in point clouds extracted from (histology) images.  Of particular relevance, this project considers the setting of prostate cancer classification, which is based on a pathologist grading of histology slides using the Gleason grading system.  These pathology slides are a source of biomedical big data that are increasingly available as archived material.  Developing these quantitative methods will be a significant advance towards a (semi-)automated quantification of prostate cancer aggressiveness.  This award supports an interdisciplinary team of investigators in computational mathematics, computer science, biomedical engineering, and pathology to develop mathematical and computational tools based on topological descriptors and machine learning in order to distinguish between different morphological types of prostate cancer.<br/><br/>This research will develop quantitative topological descriptors (e.g., persistence diagrams and summaries) that describe natural histologic phenotypes in prostate cancer, in order to provide explanatory information to assist in providing improved diagnostics/prognostics and insight into the best course of treatment for the patient. This will be accomplished through developing graphical models via unsupervised machine learning that increase our understanding of prostate cancer subtypes. The long-term goal is to develop imaging biomarkers that better identify indolent from aggressive prostate cancer compared to existing, subjective, and variable human observer analyses (i.e., the Gleason score). This project takes steps towards a novel quantitative methodology for prostate cancer classification, as well as towards developing topological methods for statistically distinguishing different types of glandular architectures."
"1702496","SHF: Medium: Collaborative Research: Machine Learning Enabled Network-on-Chip Architectures for Optimized Energy, Performance and Reliability","CCF","Software & Hardware Foundation","06/01/2017","07/25/2017","D Brian Ma","TX","University of Texas at Dallas","Continuing Grant","Yuanyuan Yang","05/31/2021","$250,000.00","","d.ma@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7798","7924, 7941","$0.00","Network-on-Chip (NoC) architectures have emerged as the prevailing on-chip communication fabric for multicores and Chip Multiprocessors (CMPs). However, as NoC architectures are scaled, they face serious challenges. A key challenge in addressing optimized NoC architecture design today is the plethora of performance enhancing, energy efficient and fault tolerant techniques available to NoC designers and the large design space that must be navigated to simultaneously reduce power, improve reliability, increase performance and maintain QoS. <br/><br/>This research proposes a new cross-layer, cross-cutting methodology spanning circuits, architectures, machine learning algorithms, and applications, aimed at designing energy-efficient, reliable and scalable NoCs. This research will result in (1) novel cross-layer design techniques that take a holistic approach of simultaneously reducing power consumption, while still achieving reliability and performance goals for NoCs, (2) a fundamental understanding of the use of hardware-amenable ML for NoC design optimization, (3) software and hardware techniques for monitoring and collecting critical data and key design parameters during network execution to optimize NoC design, and (4) modeling and simulation tools that will improve the architecture community's design methodologies for evaluating scalable NoCs. The proposed research bridges a very important gap between hardware architects who design power management and fault tolerant techniques at the circuit and architecture level and machine learning scientists who develop predictive and optimization techniques. Due to its cross-cutting nature, the proposed research has the potential to significantly transform the design of next-generation CMPs and System-on-Chips (SoCs) where complex decisions have to be made that affect the power, performance and reliability. The research will also play a major role in education by integrating discovery with teaching and training. The PIs are committed and will continue to expand on outreach activities as part of the proposed project by making the necessary efforts to attract and train minority students in this field."
"1703013","SHF: Medium: Collaborative Research: Machine Learning Enabled Network-on-Chip Architectures for Optimized Energy, Performance and Reliability","CCF","Software & Hardware Foundation","06/01/2017","04/16/2018","Avinash Karanth","OH","Ohio University","Continuing Grant","Yuanyuan Yang","05/31/2021","$524,000.00","Razvan Bunescu","karanth@ohio.edu","108 CUTLER HL","ATHENS","OH","457012979","7405932857","CSE","7798","7798, 7924, 7941, 9251","$0.00","Network-on-Chip (NoC) architectures have emerged as the prevailing on-chip communication fabric for multicores and Chip Multiprocessors (CMPs). However, as NoC architectures are scaled, they face serious challenges. A key challenge in addressing optimized NoC architecture design today is the plethora of performance enhancing, energy efficient and fault tolerant techniques available to NoC designers and the large design space that must be navigated to simultaneously reduce power, improve reliability, increase performance and maintain QoS. <br/><br/>This research proposes a new cross-layer, cross-cutting methodology spanning circuits, architectures, machine learning algorithms, and applications, aimed at designing energy-efficient, reliable and scalable NoCs. This research will result in (1) novel cross-layer design techniques that take a holistic approach of simultaneously reducing power consumption, while still achieving reliability and performance goals for NoCs, (2) a fundamental understanding of the use of hardware-amenable ML for NoC design optimization, (3) software and hardware techniques for monitoring and collecting critical data and key design parameters during network execution to optimize NoC design, and (4) modeling and simulation tools that will improve the architecture community's design methodologies for evaluating scalable NoCs. The proposed research bridges a very important gap between hardware architects who design power management and fault tolerant techniques at the circuit and architecture level and machine learning scientists who develop predictive and optimization techniques. Due to its cross-cutting nature, the proposed research has the potential to significantly transform the design of next-generation CMPs and System-on-Chips (SoCs) where complex decisions have to be made that affect the power, performance and reliability. The research will also play a major role in education by integrating discovery with teaching and training. The PIs are committed and will continue to expand on outreach activities as part of the proposed project by making the necessary efforts to attract and train minority students in this field."
"1749494","EAGER: Low-Energy Architectures for Machine Learning","CCF","SOFTWARE & HARDWARE FOUNDATION","09/15/2017","09/08/2017","Keshab Parhi","MN","University of Minnesota-Twin Cities","Standard Grant","Sankar Basu","08/31/2018","$125,000.00","","parhi@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","7916, 7945, 8089","$0.00","Machine learning systems and classifiers will be part of future smart devices. Industrial internet-of-things (IIOT) and cyber-physical systems (CPS) will be equipped with real-time feature extraction and classification to provide feedback and/or warning signals in some cases. Smart medical devices can analyze signals and trigger therapy to improve human health. Security systems can analyze activity data and thwart planned attacks. Reducing energy consumption in these smart devices is critical for increasing battery life in portable applications. This proposal addresses techniques to reduce energy consumption in feature extraction and classification. The broader impacts will be in demonstrating a new approach for feature extraction and classification with significantly less energy consumption without degrading sensitivity and specificity, along with training and educating graduate and undergraduate students in related disciplines through laboratory and computational experiences.<br/><br/>The proposed framework computes features and classifies the test data using a simple level-1 classifier that makes use of low precision. If the classification is successful, then the process terminates. Otherwise the level-2 classifier is invoked. The level-2 classifier makes use of higher precision for the feature extraction and classification; however, it reuses the low-precision results of the level-1 classifier. The process is repeated in an iterative manner until the test sample is classified with a high probability. The proposed approach differs from existing approaches in the sense that the classifier at a certain level is trained using only the training samples that do not contain the samples that were correctly classified in prior levels. The precision at the different levels of feature extraction and classification are the same for both training and test phases. This is expected to lead to higher classification accuracy. The features and classifiers are computed using approximate computing in an incremental manner. Other innovative aspects include: selection of classes of features that require less energy (e.g., time-domain vs. frequency-domain), ranking of these features using techniques such as minimally-redundant maximally-relevant (mRMR) and use of classifiers such as classification and regression tree (CART) or AdaBoost. Approximate computing of features and classifiers in an incremental manner will be investigated to reduce overall energy consumption while maintaining high sensitivity and specificity. Training of the P-Boost classifier and testing the classifier will be based on same precision; thus there is no disconnect between the precision of the classifiers used for training and testing. The proposed ""holistic"" approach is likely to result in significant savings in energy consumption compared to state-of-the-art machine learning systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820085","CAREER: Next Generation Patient Simulators","IIS","HCC-Human-Centered Computing","09/01/2017","01/18/2018","Laurel Riek","CA","University of California-San Diego","Continuing Grant","Ephraim Glinert","04/30/2020","$191,196.00","","lriek@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7367","1045, 7367","$0.00","It is estimated that in the United States many thousands of people are killed each year and billions of dollars lost due to medical errors.  The PI argues that one way to reduce the incidence of such errors is through education involving human patient simulator (HPS) systems.  Although perhaps the most commonly used android robots in America, a critical technology gap is that none of the commercially available HPS systems exhibit realistic facial expressions, gaze, or mouth movements, despite the vital importance of these cues in how providers assess and treat patients.  The PI's goal in this project is to address this shortcoming by developing novel expression synthesis algorithms and social control methods, thereby advancing the fields of affective computing and human-robot interaction.  To these ends the PI will model facial features characteristic of 3 pathologies (stroke, cerebral palsy and dystonia, the latter being a neurological disorder in which the muscles of the trunk, shoulders, and neck go into spasm so that the head and limbs are held in unnatural positions), and 2 affective states (pain and drowsiness).  She will synthesize these facial features on a new HPS system and create a model of shared social control for operators of expressive robots, and evaluate their impact on educational and task outcomes.  <br/><br/>Broader Impacts:  This research will transform the state of the art in HPS technology by enabling educators to run simulations currently impossible with commercially available technology, thereby leading to more realistic training experiences for doctors, nurses, and combat medics, which will ultimately improve healthcare.  It will create new facial models of stroke, dystonia and cerebral palsy, which may impact fields such as computer vision and biometrics while also enhancing our understanding of these disorders and providing a means for educating people how to better interact with those suffering from these disabilities and/or to quickly recognize signs of stroke.  The PI will also conduct substantial mentoring activities for undergraduates and outreach activities for K-12 students."
"1747997","EAGER: Developing Teaching Assistant Expertise with a Sensor-Based Learning System","IIS","Cyberlearn & Future Learn Tech","09/01/2017","03/19/2020","Amy Ogan","PA","Carnegie-Mellon University","Standard Grant","Amy Baylor","08/31/2020","$348,000.00","John Zimmerman","aeo@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8020","7916, 8045, 8841, 9251","$0.00","While a college degree is essential for many jobs or career paths, many college students receive a less than ideal educational experience. For years, research has shown that moving away from large lectures and increasing student engagement and participation significantly improves learning. However, most colleges still rely on lectures where students passively receive information from professors, and instructors continue to teach in the ways in which they themselves were taught. This project addresses this large-scale societal problem of understanding and supporting instructor learning by investigating how to support teaching assistants (TAs) to acquire student-centered teaching skills before they transition to faculty. This exploratory research project will investigate the development of an advanced learning technology system that integrates multimodal sensory data to deliver near real-time data on teaching practices. The combined reflection and training system will provide rapid and frequent feedback and instruction on good strategies to support student-centered teaching beliefs and behaviors. In turn, the project will advance understanding of how instructors engage with personal informatics to impact beliefs and behaviors related to teacher professional development.<br/><br/>In this exploratory project, a learning system for TAs will be developed, consisting of an interconnected set of modules that support them in acquiring student-centered teaching practices through the provision of a dashboard with near-real-time multimodal classroom data. The system draws upon technical and socio-technical advances in sensing arrays, computer vision, intelligent environments, and personal informatics, together with frameworks of professional development in higher education. The project advances computing by expanding the capabilities of state of the art multimodal sensing approaches to achieve non-invasive sensing at classroom-scale. Through a series of design-based research studies with TAs who are teaching STEM college courses, the project investigates:  a) the ways in which this system can present personal data to foster productive self-doubt, b) how engagement with personal data leads to a motivation to change, and c) how data-driven instruction can foster self-efficacy in teaching."
"1740391","RAISE: Deep Gravitational Wave Exploration, Instrumental Insights and Noise Removal Through Machine Learning","CCF","LIGO RESEARCH SUPPORT, OFFICE OF MULTIDISCIPLINARY AC, Comm & Information Foundations, INSPIRE","07/01/2017","06/14/2017","Szabolcs Marka","NY","Columbia University","Standard Grant","Phillip Regalia","06/30/2021","$1,000,000.00","Zsuzsanna Marka, John Wright, Imre Bartos","smarka@phys.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1252, 1253, 7797, 8078","049Z, 7936","$0.00","This is a RAISE Award supported by the Office of Integrative Activities, the Signal Processing Systems program of the Division of Computing and Communications (CCF) of the Computer & Information Science & Engineering Directorate (CISE), the Office of Multidisciplinary Activities of the Mathematical and Physical Sciences Directorate (MPS) and the Gravitational Physics program of Physics Division in MPS. The recent discovery of gravitational waves from colliding black unveiled a new era of broad opportunities for studying the Cosmos. The coming years will bring about the proliferation of detections of black hole mergers as well as other sources of gravitational waves, including stellar explosions. For every discovery, there will be numerous weak gravitational wave signals buried in the detector noise that will be difficult to unearth. Gravitational-wave detectors are incredibly complex systems, where there are myriads of independent ways noise sources can interfere with the recorded data, occasionally producing curious data artifacts that are difficult to distinguish from gravitational waves. Machine learning is uniquely suited to make sense of this complexity, and disentangle data from the noise to broaden our horizon to detecting gravitational waves.<br/><br/>The PIs will design machine-learning techniques to make sense of LIGO's 400,000 auxiliary data channels and identify patterns in detector behavior to enable the identification of cosmic signals in the midst of highly non-linear and non-Gaussian background noise. The PIs will research and use optimal strategies, including sparse regression and robust principal component analysis, to distinguish detector or environmental artifacts from astrophysical signals to discover gravitational waves that otherwise could have remained invisible."
"1705121","III: Medium: Scalable Machine Learning for Genome-Wide Association Analyses","IIS","Info Integration & Informatics","07/01/2017","06/17/2020","Sriram Sankararaman","CA","University of California-Los Angeles","Continuing Grant","Sylvia Spengler","06/30/2021","$1,250,098.00","Ameet Talwalkar","Sriram@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364","7364, 7924, 9251","$0.00","Over the past decade, genome-wide association studies (GWAS) have discovered genetic variants associated with numerous diseases as well as other complex phenotypes. Despite their success, major gaps remain in our understanding of how genetic changes affect phenotype. These gaps, coupled with advances in high-throughput technologies to measure genetic variation, have motivated GWAS of increasingly larger scale. However, the statistical and computational challenges posed by the scale and complexity of these studies present a critical bottleneck in realizing their promise.  These recent advances in scalable ML provide the potential for paradigm-shifting advances in the field of GWAS. However, these concepts have yet to be rigorously explored in the context of the GWAS modeling and testing problems. Exploring the intersection of these domains introduces fundamentally new statistical and computational challenges. The team will develop a suite of modeling and testing methods that target massive modern genomics datasets. The techniques that we will build upon include low-rank matrix approximation, kernel methods and matrix completion. They will also provide open-source software tailored to parallel and distributed computing environments to facilitate wide-spread adoption of methods.<br/><br/><br/><br/>Exploring GWAS through the lens of scalable machine learning introduces several research directions and requires the development of novel algorithms and analyses. Firstly, the focus of much scalable ML research has been on the statistical task of prediction, while GWAS inference problems also emphasize hypothesis testing and parameter estimation. Characterizing the behavior of scalable ML methods in these novel settings is a challenging open problem. The team will develop principled GWAS modeling and testing methods. The results to also be of great interest to the scalable ML community. Secondly, while scalable ML techniques are designed to be general purpose and domain-agnostic, the GWAS setting introduces rich biologically-motivated domain knowledge that needs to be leveraged to improve the quality of inference. Statistical models that are able to encode this prior knowledge while still permitting efficient inference will be developed. Ultimately the algorithms will be implemented as efficient parallel and distributed algorithms for these core modeling and testing problems, and develop robust open-source implementations that leverage modern computing infrastructure.1The proposed methods will dramatically improve the scalability of current GWAS analyses, on the one hand, while enabling the development of increasingly realistic genomic models, on the other. Collaborations and open-source artifacts will enable the wide-spread adoption of these methods by the human genetics community. This project will lead to a closer interaction of the genomics and machine learning communities at UCLA and outside."
"1702980","SHF: Medium: Collaborative Research: Machine Learning Enabled Network-on-Chip Architectures Optimized for Energy, Performance and Reliability","CCF","Software & Hardware Foundation","06/01/2017","07/25/2017","Ahmed Louri","DC","George Washington University","Continuing Grant","Yuanyuan Yang","05/31/2021","$449,955.00","","louri@email.gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7798","7924, 7941","$0.00","Network-on-Chip (NoC) architectures have emerged as the prevailing on-chip communication fabric for multicores and Chip Multiprocessors (CMPs). However, as NoC architectures are scaled, they face serious challenges. A key challenge in addressing optimized NoC architecture design today is the plethora of performance enhancing, energy efficient and fault tolerant techniques available to NoC designers and the large design space that must be navigated to simultaneously reduce power, improve reliability, increase performance and maintain QoS. <br/><br/>This research proposes a new cross-layer, cross-cutting methodology spanning circuits, architectures, machine learning algorithms, and applications, aimed at designing energy-efficient, reliable and scalable NoCs. This research will result in (1) novel cross-layer design techniques that take a holistic approach of simultaneously reducing power consumption, while still achieving reliability and performance goals for NoCs, (2) a fundamental understanding of the use of hardware-amenable ML for NoC design optimization, (3) software and hardware techniques for monitoring and collecting critical data and key design parameters during network execution to optimize NoC design, and (4) modeling and simulation tools that will improve the architecture community?s design methodologies for evaluating scalable NoCs. The proposed research bridges a very important gap between hardware architects who design power management and fault tolerant techniques at the circuit and architecture level and machine learning scientists who develop predictive and optimization techniques. Due to its cross-cutting nature, the proposed research has the potential to significantly transform the design of next-generation CMPs and System-on-Chips (SoCs) where complex decisions have to be made that affect the power, performance and reliability. The research will also play a major role in education by integrating discovery with teaching and training. The PIs are committed and will continue to expand on outreach activities as part of the proposed project by making the necessary efforts to attract and train minority students in this field."
"1725322","SPX: Collaborative Research: Mongo Graph Machine (MGM): A Flash-Based Appliance for Large Graph Analytics","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/12/2017","Keshav Pingali","TX","University of Texas at Austin","Standard Grant","Marilyn McClure","09/30/2020","$279,935.00","","pingali@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","042Y","026Z","$0.00","We live in the age of big data. In many problem domains such as data-mining, machine learning, scientific computing, and the study of social networks, the data deals with relationships between pairs of entities, and is represented by a data structure called a graph. Graphs of interest today may have hundreds of billions of entities, and trillions of relationships between these entities. Large-scale graph processing is typically done in data-centers which are huge clusters of power hungry computers. The proposed Mongo Graph Machine (MGM) project will explore a different solution known as out-of-core processing. In this system, graphs will be stored in flash memory, which is much cheaper, denser and cooler than DRAMs, and processed using a combination of specialized circuits called FPGAs in tandem with a conventional processor. A programming system will be developed to hide this complexity from the end-user. The resulting system will be small enough to fit under a desk and dramatically more energy-efficient while providing powerful graph processing capability.<br/><br/>The MGM project will address the problem of storing and processing extreme-scale graphs by using in-storage acceleration based on NAND flash chips with an attached FPGA. A single machine can accommodate 1 TB to 16 TBs of flash memory using current NAND technology. This configuration provides the flash storage necessary to store very large graphs and the computational power necessary to saturate the bandwidth of the flash. To address the programming problem for this architecture, the project will develop compiler technology and FPGA accelerators that will permit developers to write applications in the high-level programming model, leaving it to the system to exploit parallelism and optimize memory accesses for the access characteristics of flash storage. The software system will be based on the Galois system, which has been shown to scale to hundreds of processors on large shared-memory machines."
"1725303","SPX: Collaborative Research: Mongo Graph Machine (MGM): A Flash-Based Appliance for Large Graph Analytics","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/12/2017","Professor Arvind","MA","Massachusetts Institute of Technology","Standard Grant","Marilyn McClure","09/30/2020","$520,001.00","","arvind@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","042Y","026Z","$0.00","We live in the age of big data. In many problem domains such as data-mining, machine learning, scientific computing, and the study of social networks, the data deals with relationships between pairs of entities, and is represented by a data structure called a graph. Graphs of interest today may have hundreds of billions of entities, and trillions of relationships between these entities. Large-scale graph processing is typically done in data-centers which are huge clusters of power hungry computers. The proposed Mongo Graph Machine (MGM) project will explore a different solution known as out-of-core processing. In this system, graphs will be stored in flash memory, which is much cheaper, denser and cooler than DRAMs, and processed using a combination of specialized circuits called FPGAs in tandem with a conventional processor. A programming system will be developed to hide this complexity from the end-user. The resulting system will be small enough to fit under a desk and dramatically more energy-efficient while providing powerful graph processing capability.<br/><br/>The MGM project will address the problem of storing and processing extreme-scale graphs by using in-storage acceleration based on NAND flash chips with an attached FPGA. A single machine can accommodate 1 TB to 16 TBs of flash memory using current NAND technology. This configuration provides the flash storage necessary to store very large graphs and the computational power necessary to saturate the bandwidth of the flash. To address the programming problem for this architecture, the project will develop compiler technology and FPGA accelerators that will permit developers to write applications in the high-level programming model, leaving it to the system to exploit parallelism and optimize memory accesses for the access characteristics of flash storage. The software system will be based on the Galois system, which has been shown to scale to hundreds of processors on large shared-memory machines."
"1744637","Workshop on the Dynamic Interaction of Embodied Human and Machine Intelligence; Marconi State Historic Park, Marshall, California; June 2018","CMMI","M3X - Mind, Machine, and Motor, Perception, Action & Cognition","08/01/2017","07/19/2017","Ramesh Balasubramaniam","CA","University of California - Merced","Standard Grant","Robert Scheidt","07/31/2019","$49,995.00","","ramesh@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092012039","ENG","058Y, 7252","030E, 034E, 063Z, 7252, 7556, 8024","$0.00","This award will support an interdisciplinary workshop to study how humans and intelligent machines, such as robots, may behave while physically interacting with each other, in contrast to situations where they only exchange information. These questions will become increasingly important as humans begin to come into frequent physical contact with intelligent machines, such as when navigating shared spaces, riding in or walking near autonomous vehicles, or interacting with caregiving or physical therapy robots.<br/><br/>This workshop will gather experts from cognitive neuroscience, behavioral science, dynamics and control, robotics, optimization, and computer science to define key challenges in analyzing the dynamic behavior of systems of embodied human and machine intelligences. Due to rapidly developing capabilities in machine learning, and wide scale deployment of autonomous and intelligent robotic systems, humans in the near future will regularly find themselves physically interacting with intelligent technology. Such intelligent technology might include home assistance robots, physical therapy robots, autonomous vehicles, and smart buildings. The dynamics of physically coupled of humans and intelligent machines have so far received little study, despite the potential for unexpected behavior."
"1738925","Support US-based Students to Attend the BioImage Informatics Conference 2017 (BII 2017)","IIS","Info Integration & Informatics","07/01/2017","07/06/2017","Jie Zhou","IL","Northern Illinois University","Standard Grant","Sylvia Spengler","06/30/2018","$18,000.00","","jzhou@niu.edu","301 Lowden Hall","De Kalb","IL","601152828","8157531581","CSE","7364","7364, 7556","$0.00","The BioImage Informatics (BII) conference is among the foremost meetings on image informatics for life sciences. It aims to establish a unique communication network for researchers working in the fields of image processing, computer vision, and image-based analytics, for applications in biology. Since the first conference in 2005, it has been attracting computer scientists and biologists from all over the world. The BII 2017 will be held in Banff, Canada from September 19th to September 21st, 2017. The conference will include invited talks by leading scientists, oral and poster presentations selected from peer-reviewed submissions, technical special sessions, as well as competitions and challenges. This award will fund a diverse set of graduate student participation in this important meeting."
"1717871","SaTC: CORE: Small: Collaborative: Data-driven Approaches for Large-scale Security Analysis of Mobile Applications","CNS","Secure &Trustworthy Cyberspace","08/15/2017","07/18/2017","Doina Caragea","KS","Kansas State University","Standard Grant","Sol Greenspan","07/31/2021","$200,000.00","","dcaragea@ksu.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","8060","025Z, 7434, 7923, 9102","$0.00","This project investigates how to apply big-data analysis techniques to analyze mobile apps for the Android platform, for the purpose of accurately identifying security problems therein. A major challenge is the scale of the problem, with thousands of new apps entering the online app markets on a daily basis. Current technologies cannot keep up with the pace of the threats, and malware are regularly found in both large-scale marketplaces such as the official Google Play market and in third-party markets. The project adopts a number of advanced machine learning and data mining techniques to tackle those challenges. The large number of apps in the markets allows an automated machine learning algorithm to better capture security-related patterns and trends in the data, so that it can predict with good accuracy which apps may have security problems. Those apps are worth the more in-depth and expensive analysis that usually requires significant human effort. This creates an effective triage to deal with the scale challenge, and can be used by industry to scale the security vetting process of mobile apps. Artifacts produced from the research are released in open source and benefit practitioners. New courses on mobile apps and their security are developed. Undergraduate students are involved in this research. Underrepresented groups, including female students, also participate in the research. The materials developed from the research are used to further enrich cybersecurity education opportunities in the PIs' multiple outreach platforms in their institutions, to enable a large student body to benefit from the project.<br/><br/><br/>The project designs solutions to tackle the unique challenges in applying machine learning for mobile app security analysis, most of which are due to the big data nature of the problem. A key scientific challenge faced in mobile app security analysis is the difficulty in obtaining high-quality ground truth. Many times one has to rely upon imperfect data in training and evaluation. The research experiments with a number of approaches to deal with the noise due to the imperfect labels, including semi-supervised learning algorithms, which can learn from small amounts of labeled data, or even from positive data only, together with unlabeled data. The project also explores a novel approach that uses social media information to acquire additional information to improve the ground truth and/or the prediction accuracy."
"1717862","SaTC:  CORE:  Small:  Collaborative:  Data-driven Approaches for Large-scale Security Analysis of Mobile Applications","CNS","Secure &Trustworthy Cyberspace","08/15/2017","07/18/2017","Xinming Ou","FL","University of South Florida","Standard Grant","Sol Greenspan","07/31/2021","$200,000.00","","xou@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","8060","025Z, 7434, 7923","$0.00","This project investigates how to apply big-data analysis techniques to analyze mobile apps for the Android platform, for the purpose of accurately identifying security problems therein. A major challenge is the scale of the problem, with thousands of new apps entering the online app markets on a daily basis. Current technologies cannot keep up with the pace of the threats, and malware are regularly found in both large-scale marketplaces such as the official Google Play market and in third-party markets. The project adopts a number of advanced machine learning and data mining techniques to tackle those challenges. The large number of apps in the markets allows an automated machine learning algorithm to better capture security-related patterns and trends in the data, so that it can predict with good accuracy which apps may have security problems. Those apps are worth the more in-depth and expensive analysis that usually requires significant human effort. This creates an effective triage to deal with the scale challenge, and can be used by industry to scale the security vetting process of mobile apps. Artifacts produced from the research are released in open source and benefit practitioners. New courses on mobile apps and their security are developed. Undergraduate students are involved in this research. Underrepresented groups, including female students, also participate in the research. The materials developed from the research are used to further enrich cybersecurity education opportunities in the PIs' multiple outreach platforms in their institutions, to enable a large student body to benefit from the project.<br/><br/><br/>The project designs solutions to tackle the unique challenges in applying machine learning for mobile app security analysis, most of which are due to the big data nature of the problem. A key scientific challenge faced in mobile app security analysis is the difficulty in obtaining high-quality ground truth. Many times one has to rely upon imperfect data in training and evaluation. The research experiments with a number of approaches to deal with the noise due to the imperfect labels, including semi-supervised learning algorithms, which can learn from small amounts of labeled data, or even from positive data only, together with unlabeled data. The project also explores a novel approach that uses social media information to acquire additional information to improve the ground truth and/or the prediction accuracy."
"1714212","Nonlinear Manifold Learning of Protein Folding Funnels from Delay-Embedded Experimental Measurements","DMS","MATHEMATICAL BIOLOGY","08/01/2017","07/20/2017","Andrew Ferguson","IL","University of Illinois at Urbana-Champaign","Standard Grant","Junping Wang","09/30/2018","$210,000.00","","andrewferguson@uchicago.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","7334","","$0.00","Proteins are the molecular engines that perform biological functions essential to life. A major milestone in the understanding of protein behavior emerged with the advent of the ""new view"" of protein folding. This perspective conceives of protein structure, stability, and dynamics as governed by a molecular-level landscape not unlike a relief map describing the surface of the Earth. Each point on the landscape - analogous to latitude and longitude - corresponds to a particular spatial arrangement of protein atoms. The altitude of each point on the map defines protein stability - unstable conformations lie on mountaintops and stable conformations within valley floors. Determining these landscapes is a key goal of protein biology since they are useful in the understanding of natural proteins and design of synthetic proteins as drugs, enzymes, and molecular machines. It is relatively straightforward to calculate these landscapes for small proteins using computer simulations, but it has not been possible to do so from experimental measurements. It is the primary objective of this research to combine mathematical tools from the modeling of dynamical systems with machine learning approaches to analyze high-dimensional datasets to determine approximate protein folding landscapes directly from experimental data. The approach will first be validated in computer simulations of small proteins where the folding landscape is known. Theoretical analyses will place bounds on how close the approximate landscapes are to the true landscapes, and place conditions on the experimental data required for their determination. Ultimately the approach will be applied to experimental measurements of a tuberculosis protein. The computational analysis tool will be released as user-friendly software for free public download. Positive research experiences have great benefits for undergraduate success and retention, and this award will support summer and academic year research opportunities. New educational outreach materials will be developed for the University of Illinois ""Engineering Open House"" to promote awareness of materials science and engineering among middle- and high-school students.<br/><br/>The aim of this work is to integrate nonlinear manifold learning with dynamical systems theory to reconstruct protein folding landscapes from experimental time series measuring a single system observable. The ""new view"" of protein folding revolutionized understanding of folding as a conformational search over rugged and funneled free energy landscapes parameterized by a small number of emergent collective variables, with transformative implications for the understanding and design of proteins as drugs, enzymes, and molecular machines. It is now relatively routine to determine multidimensional folding landscapes from computer simulations in which all atomic coordinates are known, but it has not been possible to do so from experimental measurements of protein dynamics that are restricted to small numbers of coarse-grained observables. This research project integrates Takens' delay embeddings with nonlinear manifold learning using diffusion maps to first project univariate time series in an experimentally measurable observable into a high-dimensional space in which the dynamics are C1-equivalent to those in real space, and then extract from this space a topologically and geometrically equivalent reconstruction of the folding funnel to that which would have been determined from knowledge of all atomic coordinates. The reconstructed landscape preserves the topology of the true funnel - the metastable configurations and folding pathways - but the topography may be perturbed, i.e., the heights and depths of the free energy peaks and valleys. The three primary objectives of this work are to (i) validate the approach in molecular dynamics simulations of small proteins for which the true landscape is known, (ii) place conditions on the sampling resolution and signal-to-noise ratio in experimental measurements for robust landscape recovery, and theoretical bounds on the induced topographical perturbations, and (iii) apply the approach to experimental single-molecule Forster resonance energy transfer (smFRET) measurements on the lid-opening and closing dynamics of Mycobacterium tuberculosis protein tyrosine phosphatase (Mtb-PtpB)."
"1717355","RI: Small: Low Cost Technologies to Improve the Quality of 3D Scanning","IIS","Robust Intelligence","09/01/2017","08/17/2017","Gabriel Taubin","RI","Brown University","Standard Grant","Jie Yang","08/31/2021","$449,994.00","","taubin@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7923, 9150","$0.00","This project develops a new mathematical framework, algorithms, and optical designs to increase the resolution and precision of line and area 3D scanners, and to minimize the hardware cost.  Combining simple optical design techniques and novel mathematical formulations, this project builds the next generation of low cost high resolution 3D scanner systems based on active illumination and 3D triangulation.  The project designs, fabricates, and demonstrates prototype 3D scanners based on the new technologies. The project integrates research with education. Courses taught by the principal investigator on a regular basis provide the necessary mathematics, software, and practical details to design and build 3D scanners using off-the-shelf low cost semiconductor lasers, small LED projectors, and web-cams, as well as digital geometry processing software. Undergraduate and graduate students learn in these courses to design and build low cost 3D scanners based on the technologies being developed in this project. By significantly lowering the cost of high resolution 3D scanning systems, the technology, software and methods being developed in this project have significant impact in the multiple applications in computer vision, computer graphics, and robotics.  <br/><br/>The research studies a new mathematical formulation for 3D optical triangulation based on interval arithmetic, where 3D points are only determined within certain bounds along the camera rays, and multiple measurements are used to tighten these bounds.  The project introduces the ""Line Segment Cloud"" as an alternative surface representation, and as a tool to visualize the measurement errors within the proposed framework.  The project develops new variational surface reconstruction methods with line segment constraints tailored to line segment clouds.  Enabled by the new formulation, the project is constructing: 1) novel laser line and structured light super-resolution 3D scanners, where the projectors are mounted on small displacement motion platforms; 2) structured light area 3D scanners where the different 3D patterns are projected by fixed non-coaxial LED slide projectors, and neither DLP or LCD projector engines are used; and 3) super-resolution 3D scanners based on volume carving, optimized for existing line and area based 3D scanners, where camera and projector are rigidly mounted with respect to each other.  The project is also developing techniques to design laser line projectors optimized for optical triangulation using inexpensive optical components, where the thickness of the projected line is significantly reduced and the depth of field is increased. The project develops new calibration methods and performance metrics to evaluate these technologies."
"1718306","RI:  Small:  Collaborative Research:  Hidden Parameter Markov Decision Processes: Exploiting Structure in Families of Tasks","IIS","Robust Intelligence","08/01/2017","08/04/2017","Finale Doshi-Velez","MA","Harvard University","Standard Grant","Rebecca Hwa","07/31/2021","$242,000.00","","finale@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7495","7495, 7923","$0.00","Part 1<br/>Machine learning has the potential to automate many complex, real-life tasks. However, learning algorithms typically require a substantial amount of data from each specific task they are asked to solve, requiring repeated interactions with the world, each of which take time and effort. Many real-life learning scenarios involve repeated interactions with tasks that are similar, but not identical. For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs - each has a similar disease but a different progression, requiring individualized treatment; a robot may have to manipulate objects of different size and weight - each requiring similar but not identical grasping strategies. In such cases treating all of the tasks as the same results in poor performance, but learning to solve each as if they were completely different takes far too long. This project will develop intelligent agents that can use knowledge gained when solving prior tasks to much more rapidly learn new tasks that are similar but not quite the same.<br/><br/>The principal technical component of this project will lie in rigorously defining what it means for tasks to be related and in producing algorithms for leveraging that definition to enable rapid learning. To do so, the project will introduce the Hidden-Parameter Markov Decision Process, which models a family of tasks through a parameter which describes variation through the family but is hidden from the learner. The project will investigate methods that exploit this structure by learning a model of task variation and then seeking to identify the parameter value for each specific task. The planned work will focus on healthcare applications, where families of related but distinct tasks are common (i.e. each patient will have unique characteristics).  However, the project aims to produce foundational learning algorithms applicable to many application areas, ranging from robotics to systems design. This research will also be integrated into the courses taught by the PIs at Harvard and Brown and made available online; the PIs will include a diverse population, including REUs, both in these classes and in their research groups.<br/><br/>Part 2<br/>Many real-life learning scenarios involve repeated interactions with tasks that have similar, but not identical, dynamics.  For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs; a robot may have to manipulate objects of different size and weight.  These cases describe a family of related tasks, each of which is similar but not quite the same. An intelligent agent should be able to transfer knowledge learned during previous experiences to rapidly solve new tasks in the same family. However, while many algorithms have been developed to transfer knowledge, the lack of a model of task relatedness inhibits our ability to formally understand the benefits of such algorithms or the structure they exploit.<br/><br/>The planned work will model such scenarios by embedding the tasks on a low dimensional manifold that captures relevant variation between instances.  Each location on this manifold (unobserved by the agent) describes a task instance, forming a sufficient statistic for solving the task in the context of the task family.  Preliminary work by the PIs has shown that it is possible to learn such a manifold after solving just a few individual task instances and enable the rapid optimization of policies for new task instances.  Building on these promising initial results, the PIs plan to: 1) Develop methods for task family characterization, by determining whether a collection of tasks can be modeled via a single manifold or consists of several clusters; whether a new task belongs to an existing cluster or manifold; and if so, and whether or not transfer is worthwhile. 2) Scale inference by adapting recent results from machine learning to deal with large state and action spaces. 3) Generate policies using Bayesian reinforcement learning algorithms, and by exploiting formal links between state and policy representations.<br/><br/>In addition to synthetic domains, progress on these directions will be applied to problems of treatment optimization for patients with HIV, sepsis, and depression via clinical collaborations that the PIs have with world-experts in these diseases."
"1658987","Algorithm Development For Reconstruction Of Design Elements","BCS","Archaeology","05/01/2017","02/27/2017","Karen Smith","SC","University of South Carolina at Columbia","Standard Grant","John Yellen","04/30/2020","$212,978.00","Song Wang, Jun Zhou, Colin Wilder","SMITHKY2@mailbox.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","SBE","1391","1391, 9150","$0.00","Archaeologists, forensic scientists, and air crash examiners often find objects of evidence broken in many pieces, and if the number is large they face the problem of how to match, edge shapes and surface markings to fit the fragments back together. In cases where individual sites often yield thousands of pieces this problem can be particularly acute. This research will develop a computer assisted program, ""SnowVision"" to aid in fitting fragments of objects together.  The research is of significance for its use not only at archaeological sites but also for the general applicability of the algorithms developed. The project will provide new insights into a novel line of research for computer vision scientists. The open-source nature of the project means that the products delivered by the team can be addressed to multiple challenges. In addition to the educational impacts, the project includes outreach efforts to Native-American tribes and institutions.<br/><br/>Dr. Karen Smith and her research team are addressing this issue by focusing on a specific type of prehistoric Southeastern US pottery which contains paddle-stamped designs. For the development and implementation of SnowVision, Dr. Smith and her team focus on the designs embellished on Swift Creek pottery, ca. AD 200-800, found on sites throughout the Deep South. Swift Creek's curvilinear designs, which were carved onto wooden paddles and subsequently stamped onto pottery vessels, offer insights into networks of exchange, ritual, and mobility at an unprecedented level of granularity when paddle matches are found across archaeological contexts, as they so often are. Limitations of time, design expertise, and collections access have been the primary impediments to advancement in the study of these pre-Columbian networks, for which design matches provide unequivocal evidence. SnowVision will increase the speed by which researchers are able to identify design matches from different archaeological sites. This will, in turn, incentivize the digitization of understudied collections. By making the software and related pottery sherd images available online, SnowVision also will increase the reach of collections curated in different museums. Dissemination of the designs will continue to raise awareness of the deep decorative traditions of our Nation's indigenous communities. Sherd images will be archived with Open Context, where sherd data can be leveraged with data from other research projects."
"1659688","REU Site: Multidisciplinary Research for Undergraduates in Nanomaterials for Energy and Biological Applications","EEC","EWFD-Eng Workforce Development","03/15/2017","03/03/2017","Mangilal Agarwal","IN","Indiana University","Standard Grant","Amelia Greer","02/28/2021","$359,848.00","Yogesh Joglekar","agarwal@iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","ENG","1360","116E, 9178, 9250","$0.00","This Research Experiences for Undergraduates (REU) Site Program, Multidisciplinary Research for Undergraduates in Nanomaterials for Energy and Biological Applications, at Indiana University-Purdue University Indianapolis (IUPUI), focused on  nanomaterials for energy and biological applications will provide a diverse cohort of undergraduate students with a unique and immersive research experience infused with training and professional development programming. Nanotechnology represents an exciting and multidisciplinary topic at the forefront of discovery-based science, and is well-suited to accommodate undergraduate researchers from a broad range of STEM disciplines. At IUPUI, the Integrated Nanosystems Development Institute (INDI) provides a collaborative environment where undergraduates will be challenged with nanomaterials research projects ranging from advanced biology to energy storage and generation. Faculty mentors within INDI span the Schools of Engineering & Technology, Science, Medicine, and Dentistry. This collaborative atmosphere not only encourages broad thinking and deep learning, but also showcases the breadth of nanotechnology while enhancing communication skills, problem solving, and students' ability to work within a diverse community of researchers. With the goal of offering a research-based summer program designed to instill students with the confidence and resources to pursue and excel in future advanced degree programs and professional careers, this site focuses on engagement that combines cutting-edge science with workshops that range from public speaking and career options to graduate school admissions and GRE preparation. In addition, IUPUI's existing NSF-funded Research Experiences for Teachers (RET) and Innovative Technology Experiences for Students and Teachers (ITEST) programs allow unique opportunities for REU students to interact with K-12 teachers and students from surrounding metropolitan, high-needs school districts which will enhance RET-developed products (learning modules) and positively impact/inspire high school students to consider undergraduate programs in STEM.  <br/><br/>Over three years, this REU Site will: 1) Annually recruit 10 undergraduate students, a majority from underrepresented groups, from universities and community colleges with limited research opportunities and graduate programming, to participate in a summer program that will jumpstart their interest in academic research and higher learning; 2) Utilize the multidisciplinary nature of nanomaterials to impact students from across STEM fields and develop the skills needed to succeed in academic research; 3) work to establish strong professional attributes within participants, including strong communication skills as well as ethical behavior and societal responsibilities pertaining to STEM research; and 4) provide exciting and relevant research topics while cultivating a desire for career trajectories requiring advanced degrees. The organizational structure of the proposed REU-site will aid in meeting these goals and ensuring program success. Student recruitment and diversity will be achieved through established partnerships with local and national undergraduate-serving universities/community colleges with limited research offerings. In addition, IUPUI provides a campus-wide infrastructure for undergraduate summer, academic-year research opportunities, and a solid foundation for graduate school planning, career development, and fostering lifelong active learning."
"1642611","Collaborative Proposal:  EarthCube RCN: Connecting the Earth Science and Cyberinfrastructure communities to advance the analysis of high resolution topography data","EAR","EarthCube","08/15/2017","08/10/2017","Paola Passalacqua","TX","University of Texas at Austin","Standard Grant","Dena Smith","07/31/2021","$102,000.00","","paola@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","GEO","8074","","$0.00","This EarthCube Research Coordination Network (RCN) will bring together communities that create and use high resolution map data, including those that conduct research on earth surface processes and those that create the technology to make these types of complex data usable. Members of this network will work to share currently available resources and best practices, and to develop new tools to make data more available to researchers. Training will focus on teaching graduate student and early career researchers to access and use high resolution map data to answer earth science research questions. <br/><br/>Vast quantities of High Resolution Terrain (HRT) data have been collected, with applications ranging from scientific research to commercial sector engineering. Full scientific utilization of these HRT data is still limited due to challenges associated with the storage, manipulation, processing, and analysis of these data. The cyberinfrastructure community, including computer vision, computer science, informatics, and related engineering fields are developing advanced tools for visualizing, cataloging, and classifying imagery data including point clouds. Yet, many of these tools are most applicable to engineered structures and small datasets, and not to heterogeneous landscapes. Together the earth science and cyberinfrastructure communities have the opportunity to test and validate emerging tools in challenging landscapes (e.g., heterogeneous and multiscale landforms, vegetation structures, urban footprints). In particular, this RCN will be focused on four themes: (1) coordination of the analysis of HRT data across the earth surface processes and hydrology communities to identify work-flows and best practices for data analysis; (2) identification of cyberinfrastructure tool development needs as new technologies for HRT data acquisition emerge; (3) use of HRT data for numerical models validation and integration of HRT data information in models; (4) training in HRT best practices, and data processing and analysis work-flows."
"1748387","EAGER: Leveraging Synthetic Data for Visual Reasoning and Representation Learning with Minimal Human Supervision","IIS","Robust Intelligence","08/15/2017","08/15/2017","Yong Jae Lee","CA","University of California-Davis","Standard Grant","Jie Yang","07/31/2020","$200,000.00","","yongjaelee@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7495","7916","$0.00","This project investigates how synthetic data created using computer graphics can be used for developing algorithms that understand visual data. Synthetic data provides flexibility that is difficult to obtain with real-world imagery, and enables opportunities to explore problems that would be difficult to solve with real-world imagery alone. This project develops new algorithms for reasoning about object occlusions, and for self-supervised representation learning, in which useful image features are developed without the aid of human-annotated semantic labels. The project provides new algorithms that have the potential to benefit applications in autonomous systems and security. In addition to scientific impact, the project performs complementary educational and outreach activities that engage students in research and STEM.<br/><br/>This research explores novel algorithms that learn from synthetic data for visual reasoning and representation learning. While the use of synthetic data has a long history in computer vision, it has mainly been used to complement natural image data to solve standard tasks. In contrast, this project uses synthetic data to make advances in relatively unexplored problems, in which ground-truth is difficult to obtain given real-world imagery. The project consists of three major thrusts, each of which exploits the fact that a user has full control of everything that happens in a synthetic dataset. In Thrust I, it investigates a novel approach to representation learning using synthetic data, and in Thrust II, it extends the algorithm to disentangle task-specific and general-purpose features. Finally, in Thrust III, it explores a novel approach for reasoning about object occlusions."
"1718214","SaTC:  CORE:  Small:  Collaborative:  Data-driven Approaches for Large-scale Security Analysis of Mobile Applications","CNS","Secure &Trustworthy Cyberspace","08/15/2017","07/18/2017","Sankardas Roy","OH","Bowling Green State University","Standard Grant","Sol Greenspan","07/31/2021","$100,000.00","","sanroy@bgsu.edu","302 Hayes Hall","Bowling Green","OH","434030230","4193722481","CSE","8060","025Z, 7434, 7923","$0.00","This project investigates how to apply big-data analysis techniques to analyze mobile apps for the Android platform, for the purpose of accurately identifying security problems therein. A major challenge is the scale of the problem, with thousands of new apps entering the online app markets on a daily basis. Current technologies cannot keep up with the pace of the threats, and malware are regularly found in both large-scale marketplaces such as the official Google Play market and in third-party markets. The project adopts a number of advanced machine learning and data mining techniques to tackle those challenges. The large number of apps in the markets allows an automated machine learning algorithm to better capture security-related patterns and trends in the data, so that it can predict with good accuracy which apps may have security problems. Those apps are worth the more in-depth and expensive analysis that usually requires significant human effort. This creates an effective triage to deal with the scale challenge, and can be used by industry to scale the security vetting process of mobile apps. Artifacts produced from the research are released in open source and benefit practitioners. New courses on mobile apps and their security are developed. Undergraduate students are involved in this research. Underrepresented groups, including female students, also participate in the research. The materials developed from the research are used to further enrich cybersecurity education opportunities in the PIs' multiple outreach platforms in their institutions, to enable a large student body to benefit from the project.<br/><br/><br/>The project designs solutions to tackle the unique challenges in applying machine learning for mobile app security analysis, most of which are due to the big data nature of the problem. A key scientific challenge faced in mobile app security analysis is the difficulty in obtaining high-quality ground truth. Many times one has to rely upon imperfect data in training and evaluation. The research experiments with a number of approaches to deal with the noise due to the imperfect labels, including semi-supervised learning algorithms, which can learn from small amounts of labeled data, or even from positive data only, together with unlabeled data. The project also explores a novel approach that uses social media information to acquire additional information to improve the ground truth and/or the prediction accuracy."
"1745829","EAGER:   SSDIM:   Simulated and Synthetic Data for Interdependent Communications and Energy Critical Infrastructures","CMMI","Special Initiatives, ","09/01/2017","08/25/2017","Arif Sarwat","FL","Florida International University","Standard Grant","Walter Peacock","08/31/2020","$200,000.00","","asarwat@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","ENG","1642, Q231","036E, 041E, 042E, 1057, 7916","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) project will develop new mathematical foundations and computer-based learning theories for generating a wide range of simulated and fully-synthetic datasets that model interdependent communications and energy infrastructures in urban settings. These enhanced datasets and associated data building tools will provide a large-scale test data related to interdependent critical infrastructures (ICIs). New simulated and synthetic data generation tools will enable increasing the resiliency and flexibility of ICIs, improving their security during extreme weather conditions and other threats. This project will involve students from diverse backgrounds in engineering, computer science and psychology, who will be trained on pertinent research approaches related to the challenges of simulated and synthetic data modeling. The education plan includes a new data-centric course called Methods for Creating Simulated and Synthetic Data, as well as a large-scale involvement of graduate and undergraduate students in big data and smart community research. Broad dissemination is ensured by enabling an open-access repository of datasets created from the results of the funded research, as well as any program codes or related tools used to generate and analyze such data. The open-access testbed is capable of supporting both the research needs of the host institution as well as the requirement of non-proprietary multi-domain open datasets by other users.<br/><br/>This project will develop a scientific basis for the generation of simulated and synthetic data on ICIs, such as communication and energy. The objective is to develop models that can accurately reconstruct, simulate, and evaluate a robust theoretical framework of ICI function by leveraging available real-world datasets. This research will lead to several innovations: 1) An advanced Transfer Learning technique that generates simulated data on ICIs using available real-world information, leading to improved characterization of how interdependencies can form or disappear over time; 2) a Hierarchical Bayesian method-based technique for the creation of synthetic data that enables ICIs to optimally manage their shared resources in response to failures from day-to-day operations, natural disasters, or malicious attacks; 3) a Long-/Short-term Memory-based deep learning method for predicting simulated data on human-in-the-loop cognitive modeling of human behavior and the effects of their decision-making in response to unexpected incidents and events involving urban ICIs; and 4) a quality-feedback loop verification and data management approach to fine-tune the simulated and synthetic data by comparing it against available real-world data over a realistic network with a large-scale simulator that integrates ICIs over an urban setting."
"1711773","RET in Engineering and Computer Science Site for Machine Learning to Enhance Human-Centered Computing","CNS","RES EXP FOR TEACHERS(RET)-SITE","07/01/2017","10/16/2018","Jeffrey Popyack","PA","Drexel University","Standard Grant","Allyson Kennedy","12/31/2020","$599,979.00","Erin Solovey, Dario Salvucci, William Mongan","Popyack@Drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","1359","1359","$0.00","This award renews an exemplary Research Experiences for Teachers (RET) Site on human-centered computing research at Drexel University. The RET site nurtures a long-term partnership between Drexel and greater Philadelphia Metropolitan Area high schools and community colleges to advance science and engineering education by providing cutting-edge summer research experiences to cohorts of high school teachers and community college teachers in the areas of Science, Technology, Engineering, and Mathematics (STEM). The research projects will focus on machine learning to enhance human-centered computing. THE RET Site will provide hands-on experiments and regular interactions with faculty mentors who are actively involved in leading-edge research related to the RET Site theme. The participating teachers will translate their research experiences and knowledge into classroom practice by developing instructional modules and course materials that they will introduce in their classrooms and share with other teachers in their school district. These activities all contribute to the formation of a community of practice partnership between the Drexel faculty and the surrounding high schools and community colleges that has the potential to significantly enhance and improve STEM education in the greater Philadelphia area.<br/><br/>RET participants will attend a 6-week summer institute to participate in cutting-edge research projects with mentoring from computer science faculty who lead human-centered computing projects.  The projects focus on areas such as brain-computer interfaces, smart garments for health care, driver distraction, and smart textiles.  The objectives of the program include:  building partnerships between high schools, community colleges, industry, and the university;  introducing teachers to cutting edge computer science research with social impact; informing and exciting teachers about computer science principles and computational thinking;  producing learning materials which emphasize social relevance for use in high school and community college STEM curricula;  and expanding the pipeline of students studying STEM and computing curricula in college.  The RET Site project will provide a platform for the participating teachers to develop effective practical problem-based instructional materials and laboratory modules that they will share with other teachers in their school district during the academic year. The teacher-created modules and lessons will be disseminated locally through the Drexel REThink project web site and nationally through TeachEngineering.org, a nationally recognized repository for searchable, standards-based engineering curricula, as well as the Advanced Placement Computer Science Principles Teacher community.   The RET Site project will boost the quality of instruction in the Greater Philadelphia area high schools and community colleges and strengthen the curricula. The excitement of learning about innovative computing technologies can inspire the students to pursue further computing education and related careers.  The Drexel RET Site program can synergize community computer science education initiatives that lay the foundation for quality computing education in the schools and provide for the future computing workforce needs of the community."
"1642620","Collaborative Proposal:  EarthCube RCN: Connecting the Earth Science and Cyberinfrastructure communities to advance the analysis of high resolution topography data","EAR","EarthCube","08/15/2017","08/10/2017","Christopher Crosby","CO","UNAVCO, Inc.","Standard Grant","Dena Smith","07/31/2021","$147,823.00","","crosby@unavco.org","6350 Nautilus Dr.","Boulder","CO","803015394","3033817500","GEO","8074","","$0.00","This EarthCube Research Coordination Network (RCN) will bring together communities that create and use high resolution map data, including those that conduct research on earth surface processes and those that create the technology to make these types of complex data usable. Members of this network will work to share currently available resources and best practices, and to develop new tools to make data more available to researchers. Training will focus on teaching graduate student and early career researchers to access and use high resolution map data to answer earth science research questions. <br/><br/>Vast quantities of High Resolution Terrain (HRT) data have been collected, with applications ranging from scientific research to commercial sector engineering. Full scientific utilization of these HRT data is still limited due to challenges associated with the storage, manipulation, processing, and analysis of these data. The cyberinfrastructure community, including computer vision, computer science, informatics, and related engineering fields are developing advanced tools for visualizing, cataloging, and classifying imagery data including point clouds. Yet, many of these tools are most applicable to engineered structures and small datasets, and not to heterogeneous landscapes. Together the earth science and cyberinfrastructure communities have the opportunity to test and validate emerging tools in challenging landscapes (e.g., heterogeneous and multiscale landforms, vegetation structures, urban footprints). In particular, this RCN will be focused on four themes: (1) coordination of the analysis of HRT data across the earth surface processes and hydrology communities to identify work-flows and best practices for data analysis; (2) identification of cyberinfrastructure tool development needs as new technologies for HRT data acquisition emerge; (3) use of HRT data for numerical models validation and integration of HRT data information in models; (4) training in HRT best practices, and data processing and analysis work-flows."
"1717569","RI:   Small:  Collaborative Research:  Hidden Parameter Markov Decision Processes: Exploiting Structure in Families of Tasks","IIS","Robust Intelligence","08/01/2017","08/04/2017","George Konidaris","RI","Brown University","Standard Grant","Rebecca Hwa","07/31/2021","$208,000.00","","George_konidaris@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7923, 9150","$0.00","Part 1<br/>Machine learning has the potential to automate many complex, real-life tasks. However, learning algorithms typically require a substantial amount of data from each specific task they are asked to solve, requiring repeated interactions with the world, each of which take time and effort. Many real-life learning scenarios involve repeated interactions with tasks that are similar, but not identical. For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs - each has a similar disease but a different progression, requiring individualized treatment; a robot may have to manipulate objects of different size and weight - each requiring similar but not identical grasping strategies. In such cases treating all of the tasks as the same results in poor performance, but learning to solve each as if they were completely different takes far too long. This project will develop intelligent agents that can use knowledge gained when solving prior tasks to much more rapidly learn new tasks that are similar but not quite the same.<br/><br/>The principal technical component of this project will lie in rigorously defining what it means for tasks to be related and in producing algorithms for leveraging that definition to enable rapid learning. To do so, the project will introduce the Hidden-Parameter Markov Decision Process, which models a family of tasks through a parameter which describes variation through the family but is hidden from the learner. The project will investigate methods that exploit this structure by learning a model of task variation and then seeking to identify the parameter value for each specific task. The planned work will focus on healthcare applications, where families of related but distinct tasks are common (i.e. each patient will have unique characteristics).  However, the project aims to produce foundational learning algorithms applicable to many application areas, ranging from robotics to systems design. This research will also be integrated into the courses taught by the PIs at Harvard and Brown and made available online; the PIs will include a diverse population, including REUs, both in these classes and in their research groups.<br/><br/>Part 2<br/>Many real-life learning scenarios involve repeated interactions with tasks that have similar, but not identical, dynamics.  For example, an immunologist may encounter HIV patients with different comorbid conditions and latent viral reservoirs; a robot may have to manipulate objects of different size and weight.  These cases describe a family of related tasks, each of which is similar but not quite the same. An intelligent agent should be able to transfer knowledge learned during previous experiences to rapidly solve new tasks in the same family. However, while many algorithms have been developed to transfer knowledge, the lack of a model of task relatedness inhibits our ability to formally understand the benefits of such algorithms or the structure they exploit.<br/><br/>The planned work will model such scenarios by embedding the tasks on a low dimensional manifold that captures relevant variation between instances.  Each location on this manifold (unobserved by the agent) describes a task instance, forming a sufficient statistic for solving the task in the context of the task family.  Preliminary work by the PIs has shown that it is possible to learn such a manifold after solving just a few individual task instances and enable the rapid optimization of policies for new task instances.  Building on these promising initial results, the PIs plan to: 1) Develop methods for task family characterization, by determining whether a collection of tasks can be modeled via a single manifold or consists of several clusters; whether a new task belongs to an existing cluster or manifold; and if so, and whether or not transfer is worthwhile. 2) Scale inference by adapting recent results from machine learning to deal with large state and action spaces. 3) Generate policies using Bayesian reinforcement learning algorithms, and by exploiting formal links between state and policy representations.<br/><br/>In addition to synthetic domains, progress on these directions will be applied to problems of treatment optimization for patients with HIV, sepsis, and depression via clinical collaborations that the PIs have with world-experts in these diseases."
"1642610","Collaborative Proposal: EarthCube RCN: Connecting the Earth Science and Cyberinfrastructure communities to advance the analysis of high resolution topography data","EAR","EarthCube","08/15/2017","08/10/2017","Nancy Glenn","ID","Boise State University","Standard Grant","Dena Smith","07/31/2021","$50,081.00","","nancyglenn@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","GEO","8074","9150","$0.00","This EarthCube Research Coordination Network (RCN) will bring together communities that create and use high resolution map data, including those that conduct research on earth surface processes and those that create the technology to make these types of complex data usable. Members of this network will work to share currently available resources and best practices, and to develop new tools to make data more available to researchers. Training will focus on teaching graduate student and early career researchers to access and use high resolution map data to answer earth science research questions. <br/><br/>Vast quantities of High Resolution Terrain (HRT) data have been collected, with applications ranging from scientific research to commercial sector engineering. Full scientific utilization of these HRT data is still limited due to challenges associated with the storage, manipulation, processing, and analysis of these data. The cyberinfrastructure community, including computer vision, computer science, informatics, and related engineering fields are developing advanced tools for visualizing, cataloging, and classifying imagery data including point clouds. Yet, many of these tools are most applicable to engineered structures and small datasets, and not to heterogeneous landscapes. Together the earth science and cyberinfrastructure communities have the opportunity to test and validate emerging tools in challenging landscapes (e.g., heterogeneous and multiscale landforms, vegetation structures, urban footprints). In particular, this RCN will be focused on four themes: (1) coordination of the analysis of HRT data across the earth surface processes and hydrology communities to identify work-flows and best practices for data analysis; (2) identification of cyberinfrastructure tool development needs as new technologies for HRT data acquisition emerge; (3) use of HRT data for numerical models validation and integration of HRT data information in models; (4) training in HRT best practices, and data processing and analysis work-flows."
"1651909","CAREER: Improving Adaptive Decision Making in Interactive Learning Environments","IIS","Robust Intelligence, Cyberlearn & Future Learn Tech","03/01/2017","03/05/2020","Min Chi","NC","North Carolina State University","Continuing Grant","Jie Yang","02/28/2022","$547,810.00","","mchi@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","7495, 8020","1045, 7495","$0.00","Interactive Learning Environments (ILEs) hold great promise for improving student performance in STEM education.  While, traditionally, such systems have focused on teaching students subject matter, an equally important facet is to teach them how become better learners.  The objective of this project is to develop an integrated research and education program that investigates the how to improve decision-making in ILEs and the impact of integrating ILE decisions with user-initiated decisions. The primary research goal lies in creating and improving ILEs directly from data, using state-of-the-art machine-learning techniques.  The primary educational goal lies in preparing students to act independently and make good choices in new situations for which they do not immediately know how to act.  The work will contribute in enabling ILEs to be more effective by improving student performance in STEM domains, by teaching students to make effective pedagogical decisions, and by making the decisions made by such systems more transparent to both teachers and domain experts.<br/><br/>The project will develop and empirically evaluate a general decision-making framework across three ILE or STEM domains. The project naturally integrates both the research and education goals. More specifically, the project will 1) advance research on the application of Reinforcement Learning by adapting it to make hierarchical decisions similar to those of human experts; 2) advance the understanding of ILEs and Reinforcement Learning algorithms by inducing compact policies that highlight key decisions and that can inform one?s understanding of the educational domain; and 3) close the loop by using data-driven policies to support student decision-making and eventually improve their long-term problem-solving abilities through hybrid human-machine interactive decision making in vivo experimentation.<br/>"
"1722440","SBIR Phase I:  Hardware and Software Systems for High Throughput, High Cell Density Fermentation","IIP","SMALL BUSINESS PHASE I","07/01/2017","07/10/2017","Matthew Ball","CA","Culture Robotics, Inc","Standard Grant","Ruth M. Shuman","06/30/2018","$225,000.00","","matt@culturerobotics.com","180 Steuart St #193554","San Francisco","CA","941059992","6506906913","ENG","5371","5371, 8029","$0.00","This SBIR Phase I project proposes to develop a high throughput, imagery-based single cell analysis system.  The system will be used to characterize high cell density polymicrobial samples captured during biological research.  Initially, the technology will be applied to the problem of quantifying extremely low levels of microbial contamination in biological samples.  The quantification and control of microbial contamination is an important challenge in industrial fermentation and in the manufacturing of biologics.  Methods for rapid and sensitive detection of microbial contaminants do not exist.  Traditional contamination detection methods involve time-consuming culturing of cells or manual and imprecise checks with a microscope.  The proposed technology will provide automated and rapid sample characterization, allowing a speed of analysis that is 1000 times faster than current systems.  This increased throughput will additionally allow lower levels of contamination to be quantified.  While initially applied to research, the technology can also be used as an analytical and diagnostic tool in the medical field.  For example, this analysis system could be used to search for malformed red blood cells in a human sample that might arise due to a disease like sickle cell anemia, or to determine the relative abundance and morphology of immune cells, an analysis that would help identify certain leukemias.<br/><br/>This project will create a hardware platform and a parallelized computer vision system that is capable of fully characterizing each cell in a 1mL biological sample in under five minutes.  To analyze samples taken from a fermentation process with high cell densities requires the micrography system to be capable of processing over one billion cells in the analysis window.  The project aims to develop four subsystems: a high pressure microfluidic system for separating and isolating cells, an imaging system, a set of high throughput algorithms to classify the cellular imagery data and a horizontally-scalable computing platform on which to run the classifying code.  The project will begin by developing a low throughput, fully functioning prototype.  The efforts will then focus on developing the four primary subsystems in parallel, using knowledge gained in the prototyping efforts to guide the later work.  Additional phases will focus on the integration of high throughput versions of the subsystems.  The fully realized single cell micrography system will represent a leap forward in speed and specificity compared to traditional cytometry systems.  With no manual intervention and in an open-ended, label-free manner, researchers will be able to conduct parts per billion-level inspection of a biological sample."
"1716500","SaTC: CORE: Small: Collaborative: Exploiting Physical Properties in Wireless Networks for Implicit Authentication","CNS","Secure &Trustworthy Cyberspace","09/01/2017","08/30/2017","Yingying Chen","NJ","Stevens Institute of Technology","Standard Grant","Phillip Regalia","01/31/2018","$339,950.00","","yingche@scarletmail.rutgers.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","8060","025Z, 065Z, 7434, 7923, 9102","$0.00","The rapid development of information technology not only leads to great convenience in our daily lives, but also raises significant concerns in the field of security and privacy. Particularly, the authentication process, which serves as the first line of information security by verifying the identity of a person or device, has become increasingly critical. An unauthorized access could result in detrimental impact on both corporation and individual in both secrecy loss and privacy leakage. Unlike many existing studies on user/device authentication, which either employ specialized or expensive hardware that needs experts for installation and calibration or require users' active involvement, the emerging low-cost and unobtrusive authentication solution without the users' participation is particularly attractive to effectively complement conventional security approaches. Due to the rich wireless connectivity and unique signal characteristics in pervasive wireless environments, this project takes a different view point by exploiting unique physical properties in wireless networks to facilitate implicit authentication for both human and mobile devices. The proposed research could advance our knowledge in exploiting the physical layer information in wireless networks to capture unique physiological and behavioral characteristics from human during their daily activities. It could also enhance our understanding in developing deep learning techniques to authenticate people based on their activities in the physical environments. Additionally, the educational efforts include curriculum development, K-12 and undergraduate involvement, and underrepresented student engagement in research.<br/><br/>This project focuses on building a holistic framework that leverages fine-grained radio signals available from the commercial wireless networks to perform implicit user/device authentication. The proposed framework aims to advance the foundation of integrating fine-grained physical properties in wireless networks to enhance wireless security. The research reveals that the fine-grained signal properties in wireless networks are capable to capture unique physiological and behavioral characteristics from human in both stationary and mobile daily activities. The proposed framework develops smart segmentation on the wireless signals and extract unique features that enable the capability of distinguishing individual. It further develops deep learning techniques to authenticate people based on their daily activities in the physical environments. The authentication process does not require active user involvement nor require the user to wear any device. This project also develops efficient techniques to detect the presence of user spoofing and localize attackers to facilitate the employment of a broad array of defending strategies."
"1762268","CAREER: Scaling Approximate Inference and Approximation-Aware Learning","IIS","Info Integration & Informatics","09/01/2017","04/27/2020","Wolfgang Gatterbauer","MA","Northeastern University","Continuing Grant","Wei Ding","03/31/2022","$539,398.00","","wolfgang@ccis.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7364","1045, 7364, 9251","$0.00","The last decade has seen an enormous increase in our ability to gather and manage large amounts of data; business, healthcare, education, economy, science, and almost every aspect of society are accumulating data at unprecedented levels. The basic premise is that by having more data, even if uncertain and of lower quality, we are also able to make better-informed decisions. To make any decisions, we need to perform ""inference"" over the data, i.e. to either draw new conclusions, or to find support for existing hypotheses, thus allowing us to favor one course of action over another. However, general reasoning under uncertainty is highly intractable, and many state-of-the-art systems today perform approximate inference by reverting to sampling. Thus for many modern applications (such as information extraction, knowledge aggregation, question-answering systems, computer vision, and machine intelligence), inference is a key bottleneck, and new methods for tractable approximate inference are needed.<br/><br/>This project addresses the challenge of scaling inference by generalizing two highly scalable approximate inference methods and complementing them with scalable methods for parameter learning that are ""approximation-aware."" Thus, instead of treating the (i) learning and the (ii) inference steps separately, this project uses the approximation methods developed for inference also for learning the model. The research hypothesis is that this approach increases the overall end-to-end prediction accuracy while simultaneously increasing scalability. Concretely, the project develops the theory and a set of scalable algorithms and optimization methods for at least the following four sub-problems: (1) approximating general probabilistic conjunctive queries with standard relational databases; (2) learning the probabilities in uncertain databases based on feedback on rankings of output tuples from general queries; (3) approximating the exact probabilistic inference in undirected graphical models with linearized update equations; and (4) complementing the latter with a robust framework for learning linearized potentials from partially labeled data."
"1652835","CAREER: Fast, Accurate Estimation and Prediction using Markov Logic","IIS","Robust Intelligence","03/01/2017","03/18/2019","Vibhav Gogate","TX","University of Texas at Dallas","Continuing Grant","Rebecca Hwa","02/28/2022","$330,848.00","","Vibhav.Gogate@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","1045, 7495","$0.00","Markov logic networks (MLNs) are routinely used in a wide variety of application domains including information extraction, computer vision, bio-informatics, and natural language understanding to represent and reason about relational and probabilistic knowledge. However, inference and learning in them is extremely challenging and despite tremendous progress in recent years several key real-world reasoning tasks remain out of reach. The goal of this CAREER award is to vastly improve the scalability and accuracy of learning and inference algorithms for MLNs and thus solve much larger and harder reasoning problems than is possible today. The award also includes a tightly integrated education and outreach component. Specifically, it (1) involves high school students as well as undergraduate students, especially young women in development and model building exercises, encouraging them to pursue career in research; (2) yields open source software tools to facilitate and broaden the adoption of MLN technology; and (3) promotes standardization of datasets and evaluation methodologies via organization of inference competitions.<br/><br/>The key technical contribution of the proposed research is to address two fundamental limitations of lifted probabilistic inference algorithms, the dominating approach for inference and learning in MLNs. First, existing lifted methods primarily exploit exact symmetries and ignore approximate symmetries. Second, MLNs having large number of symmetries are often not expressive enough to accurately model complex, real-world dependencies and hidden phenomena. The CAREER award addresses these issues by developing: (1) principled approaches that exploit approximate symmetries; (2) novel learning algorithms that use structured latent variables to induce diverse, highly expressive MLNs; and (3) a unifying message-passing framework called lifted structured message passing that systematically exploits approximate symmetries and structured representations for solving a range of inference tasks defined over MLNs including marginal estimation, maximum-a-posteriori estimation and marginal maximum-a-posteriori estimation."
"1648563","STTR Phase I:  Autonomous Landing of sUAS onto Moving Platforms","IIP","STTR Phase I","01/01/2017","12/14/2016","Gaemus Collins","CA","Planck Aerosystems Inc","Standard Grant","Muralidharan Nair","04/30/2018","$224,912.00","Randal Beard","gaemus@planckaero.com","2065 Kurtz St.","San Diego","CA","921102014","6192305049","ENG","1505","1505, 6840, 8034, 8035, HPCC","$0.00","The broader impact/commercial potential of this project is the expansion of autonomous unmanned aerial systems (UAS, or drones) to new maritime operational environments and commercial markets. The proposed technology will enable small UAS to operate from vessels moving at sea, without the need for a dedicated pilot or installed hardware, even while far from shore and beyond the reach of established communication networks. Small UAS can offer the same aerial perspective provided by manned helicopters at a fraction of the size, cost, and risk. Real-time aerial imagery from UAS will supply maritime operators with invaluable information about their surroundings at sea, which is not available by any other means. This information is critical for many maritime applications, including fishing, ocean monitoring, scientific exploration, maritime surveillance, and search, and rescue. This information will offer a particularly large and immediate impact for 98% of worldwide commercial fishing vessels (those that do not carry embarked manned helicopters for fish-finding) by dramatically reducing their fuel costs; providing net economic and environmental gains for the industry.<br/><br/>This Small Business Technology Transfer (STTR) Phase I project will develop algorithms and software to enable small UAS autonomously and reliably land onto a moving platform at sea. Commercially-available small UAS can offer invaluable real-time aerial imagery for maritime operators. But, this technology is not currently in widespread use due to technological barriers. In particular, the key enabling technology is the ability to autonomously and reliably land a small UAS onto a moving platform. The research objective is to develop algorithms and software that enable small UAS to autonomously operate from moving vessels at sea. Computer vision algorithms automatically detect the host vessel and the dedicated landing area. Data fusion algorithms estimate the relative drone-boat position and orientation in real time, including compensation for vessel roll, pitch, & heave. Precision control algorithms optimize the drone?s trajectory for save, reliable, autonomous launch and landing. A prototype system will be built by integrating the STTR-developed software with commercially available hardware components."
"1820624","SaTC: CORE: Small: Collaborative: Exploiting Physical Properties in Wireless Networks for Implicit Authentication","CNS","Secure &Trustworthy Cyberspace","10/01/2017","12/18/2017","Yingying Chen","NJ","Rutgers University New Brunswick","Standard Grant","Phillip Regalia","08/31/2021","$339,950.00","","yingche@scarletmail.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8060","025Z, 065Z, 7434, 7923, 9102","$0.00","The rapid development of information technology not only leads to great convenience in our daily lives, but also raises significant concerns in the field of security and privacy. Particularly, the authentication process, which serves as the first line of information security by verifying the identity of a person or device, has become increasingly critical. An unauthorized access could result in detrimental impact on both corporation and individual in both secrecy loss and privacy leakage. Unlike many existing studies on user/device authentication, which either employ specialized or expensive hardware that needs experts for installation and calibration or require users' active involvement, the emerging low-cost and unobtrusive authentication solution without the users' participation is particularly attractive to effectively complement conventional security approaches. Due to the rich wireless connectivity and unique signal characteristics in pervasive wireless environments, this project takes a different view point by exploiting unique physical properties in wireless networks to facilitate implicit authentication for both human and mobile devices. The proposed research could advance our knowledge in exploiting the physical layer information in wireless networks to capture unique physiological and behavioral characteristics from human during their daily activities. It could also enhance our understanding in developing deep learning techniques to authenticate people based on their activities in the physical environments. Additionally, the educational efforts include curriculum development, K-12 and undergraduate involvement, and underrepresented student engagement in research.<br/><br/>This project focuses on building a holistic framework that leverages fine-grained radio signals available from the commercial wireless networks to perform implicit user/device authentication. The proposed framework aims to advance the foundation of integrating fine-grained physical properties in wireless networks to enhance wireless security. The research reveals that the fine-grained signal properties in wireless networks are capable to capture unique physiological and behavioral characteristics from human in both stationary and mobile daily activities. The proposed framework develops smart segmentation on the wireless signals and extract unique features that enable the capability of distinguishing individual. It further develops deep learning techniques to authenticate people based on their daily activities in the physical environments. The authentication process does not require active user involvement nor require the user to wear any device. This project also develops efficient techniques to detect the presence of user spoofing and localize attackers to facilitate the employment of a broad array of defending strategies."
"1717356","SaTC: CORE: Small: Collaborative: Exploiting Physical Properties in Wireless Networks for Implicit Authentication","CNS","Secure &Trustworthy Cyberspace","09/01/2017","12/12/2019","Xiaonan Guo","IN","Indiana University","Standard Grant","Phillip Regalia","08/31/2021","$160,000.00","","xg6@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8060","025Z, 065Z, 7434, 7923","$0.00","The rapid development of information technology not only leads to great convenience in our daily lives, but also raises significant concerns in the field of security and privacy. Particularly, the authentication process, which serves as the first line of information security by verifying the identity of a person or device, has become increasingly critical. An unauthorized access could result in detrimental impact on both corporation and individual in both secrecy loss and privacy leakage. Unlike many existing studies on user/device authentication, which either employ specialized or expensive hardware that needs experts for installation and calibration or require users' active involvement, the emerging low-cost and unobtrusive authentication solution without the users' participation is particularly attractive to effectively complement conventional security approaches. Due to the rich wireless connectivity and unique signal characteristics in pervasive wireless environments, this project takes a different view point by exploiting unique physical properties in wireless networks to facilitate implicit authentication for both human and mobile devices. The proposed research could advance our knowledge in exploiting the physical layer information in wireless networks to capture unique physiological and behavioral characteristics from human during their daily activities. It could also enhance our understanding in developing deep learning techniques to authenticate people based on their activities in the physical environments. Additionally, the educational efforts include curriculum development, K-12 and undergraduate involvement, and underrepresented student engagement in research.<br/><br/>This project focuses on building a holistic framework that leverages fine-grained radio signals available from the commercial wireless networks to perform implicit user/device authentication. The proposed framework aims to advance the foundation of integrating fine-grained physical properties in wireless networks to enhance wireless security. The research reveals that the fine-grained signal properties in wireless networks are capable to capture unique physiological and behavioral characteristics from human in both stationary and mobile daily activities. The proposed framework develops smart segmentation on the wireless signals and extract unique features that enable the capability of distinguishing individual. It further develops deep learning techniques to authenticate people based on their daily activities in the physical environments. The authentication process does not require active user involvement nor require the user to wear any device. This project also develops efficient techniques to detect the presence of user spoofing and localize attackers to facilitate the employment of a broad array of defending strategies."
"1738321","SBIR Phase II:  Predictive Algorithms for Water Point Failure","IIP","SBIR Phase II","09/01/2017","09/24/2018","Daniel Wilson","CO","SweetSense Inc.","Standard Grant","Benaiah Schrag","10/31/2020","$899,475.00","Evan Thomas","daniel.wilson@sweetsensors.com","2536 N Gilpin St","Denver","CO","802050000","3035504671","ENG","5373","169E, 5373, 8032","$0.00","This Small Business Innovation Research (SBIR) Phase II project will develop and apply machine learning statistical tools to Internet of Things (IoT) water delivery and water quality sensors. This will enable prediction and preemptive response to water point failures. The resilience of these environmental services is dependent upon credible and continuous indicators of reliability, leveraged by funding agencies to incentivize performance among service providers. In many locations, these service providers are utilities providing access to clean water, safe sanitation, and reliable energy. However, in some rural areas, there remains a significant gap between the intent of service providers and the impacts measured over time. Achieving the SBIR Phase II core objectives will help close the loop on effective and clean water delivery.  IoT sensors and services will address one of the most critical public health gaps by enabling delivery of reliable and safe water.<br/><br/>IoT solutions for this environment may help address these information asymmetries and enable improved decisions and response. However, given the remote and power constrained environments and the high degree of variability between fixed infrastructure including age, materials, pipe diameters, power quality, rotating equipment vendors (pumps and generators), servicing, and functionality, any IOT solution would have to either be bespoke engineering, or compensate for these site-wise complexities through analytics. Instead, our SBIR II approach is to develop universal, solar powered cellular and satellite IOT hardware for each service type, and addresses site complexities through cloud-based sensor fusion and statistical learning. In this way, we significantly reduce hardware and logistical costs, and provide value to our customers through service delivery analytics. In Phase I, we demonstrated the application of simple sensors and sophisticated machine learning to identify off-nominal service delivery across a cohort of water pumps of various designs. We developed a universal electrical borehole sensor compatible with disparate fixed infrastructure, and we demonstrated solving the problem of heterogeneous customer hardware with a homogeneous sensor platform and adaptive machine learning backend."
"1652561","CAREER: Robots that Help People","IIS","Robust Intelligence","03/15/2017","06/19/2020","Stefanie Tellex","RI","Brown University","Continuing Grant","Jie Yang","02/28/2022","$549,437.00","","stefie10@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","1045, 7495","$0.00","As robots become more prevalent, it is crucial to develop ways for people to collaborate with them.  This proposal aims to create collaborative robots through a combination of communication, perception, and action.  Existing approaches are typically tailored to specific applications; yet people want to talk to robots about everything they can see and do.  To address such limitations, this project will create a unified framework to enable robots to communicate with people to learn their needs, plan how to achieve them, and then perceive and act in the world in order to meet those needs.  The research will be demonstrated with robots that can assist with household tasks, such as cooking and cleaning, as well as in manufacturing settings, such as collaborative assembly.  The project will expose many people to collaborative robotics through an internship program with local high schools, a regional robotics conference, and the Million Object Challenge.<br/><br/>This project will create a model, the Human-Robot Collaborative Partially Observable Markov Decision Process, that enables robots to 1) automatically acquire object-oriented models of objects in the physical world; 2) communicate with people to understand their needs and how to meet them; and 3) act to change the world in ways that meet people's needs.  Creating a unified framework requires bridging gaps between different aspects of the robotic system.  This project focuses on creating a single probabilistic graphical model to represent the robot's states and actions in a hierarchical framework, allowing the robot to make plans that take into account its own uncertainty and to communicate with a person about everything it can see and everything it can do.  Focusing on collaboration leads to reformulations of traditional problems in computer vision, planning, and natural language understanding enabling the robot to collaborate in new and more natural ways.  <br/>"
"1747544","EAGER:   Collaborative Research:   Inverse Procedural Material Modeling for Battery Design","IIS","CONDENSED MATTER & MAT THEORY, Info Integration & Informatics, Data Cyberinfrastructure, Materials Eng. & Processing","08/01/2017","07/26/2017","Bedrich Benes","IN","Purdue University","Standard Grant","Maria Zemankova","12/31/2018","$150,000.00","Edwin Garcia","bbenes@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1765, 7364, 7726, 8092","023E, 077E, 7364, 7433, 7916, 8084, 8396, 8399","$0.00","Nearly all portable electronic devices commonly used today -- cameras, phones, music players and the like -- rely on rechargeable Lithium-ion batteries. Improvements in the capabilities of these devices can be achieved by improving the design of these batteries. This work will produce new computational methods for designing batteries with desirable properties such as high power output and long lifespan. The new computational methods will use techniques that have successfully described complex volumetric structures (such as porous rocks and sponges) in computer graphics for film and games. These computer graphics techniques will be applied to describing the materials in batteries. Instead of focusing on finding volumetric structures that give the correct visual appearance, the new computational methods will focus on structures that produce the correct performance characteristics such as power density. The new volumetric descriptions will be used to generate a large number of potential volumetric materials, and these models will be characterized in terms of battery properties and performance. Using recently developed machine learning techniques, this large number of potential models will be converted into a form that is convenient to use in battery design. In addition to providing tools to create improved portable batteries, the new computational methods have the potential to be further extended and applied to other problems involving materials with complex volumetric structure such as understanding geologic measurements and designing conservation strategies for cultural heritage monuments and artifacts.<br/><br/>A straightforward approach to battery design is to theorize material microstructures, run forward simulations to assess their performance, and evaluate the results. However, simulations require hours (up to 50 hours on current multi-core systems for power density calculations), making forward simulation prohibitively expensive for iterative design. The design process can be dramatically improved if an inverse function is available that can produce a microstructure description given desired performance characteristics. Barriers to creating such an inverse function are the complexity of microstructure descriptions and the relationship between structure and performance. To create an inverse function, we need a microstructure description that is lower in dimension than a full enumeration of a high-resolution grid. A procedural model can provide such a lower dimensional description. The approach explored in this project for finding appropriate procedural models is based on combining and transforming models that have been successful in other problem domains to fit data from real battery material measurements. Given an appropriate procedural model, the design problem is reduced to determining the procedural model parameters that generate the input; a problem called ""inverse procedural modeling"". Even with a compact microstructure description, the problem is too complex to be mathematically inverted. Rather than attempt to find a mathematical function, machine learning (deep neural networks) are used. A database of microstructures and their performance characteristics will be populated synthetically with example microstructures computed from a large sampling of procedural model parameters. Forward simulations will be run on these samples to compute properties (tortuosity and area density) and performance characteristics (power and energy density.) Machine learning optimizations will then be used to find the relationship between model parameters and performance characteristics and this relationship will be used in the design process. The overall method of finding procedural models to fit data and then learning the relationships from synthetic data generated from the models brings the power of new data-driven approaches to the domain of battery design. The software, data and publications resulting from this project will be available at the project website (http://hpcg.purdue.edu/Eager2018/)."
"1700404","Characterization of Trace Spaces and Differential Structures on Subsets of Euclidean Space","DMS","ANALYSIS PROGRAM","07/15/2017","08/27/2019","Arie Israel","TX","University of Texas at Austin","Continuing Grant","Marian Bocea","06/30/2021","$152,998.00","","arie@math.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","MPS","1281","","$0.00","This research project is focused on applications of extension theory to problems in geometry, partial differential equations, computer science, and data processing. In the machine learning paradigm, one typically observes a collection of data that represents the measurements of a physical process. The data may be represented in terms of a large number of variables. To learn the physical structures inherent to the data, one looks for a small number of latent variables that describe a low-dimensional manifold which contains or ""passes close to"" the data points. Whereas classical statistical regression looks for a linear relationship in the data, manifold learning allows for possibly non-linear descriptions. The principal investigator plans to develop practical manifold learning algorithms using techniques from extension theory. Consider for instance the problem of interpolating a set of data by a smooth, convex hypersurface. Whereas all currently available solutions to this problem require that the data points be sampled uniformly from the entire surface, the principal investigator proposes an approach which would work even when there are no sampled data on large pieces of the surface. The proposed research will introduce new connections between the fields of harmonic analysis and machine learning.<br/><br/>The past few decades have witnessed the development of abstract notions of differentiation and curvature on nonsmooth spaces. To study singular solutions to mean curvature flow, for instance, it is important to develop a generalized notion of curvature. Another achievement has been Cheeger's theory of differentiation on metric measure spaces. Cheeger's spaces carry a notion of distance and volume, but surprisingly, they lack the structure of local coordinate charts usually required to define a differential calculus. One shortcoming of this approach is its inherent limitation to first order theories. That is, one can define a first order differential operator, but the notion of a second order operator, such as the Laplacian or heat operator, is meaningless at this level of abstraction. The principal investigator proposes an alternate perspective: Assume the space is embedded as a subset of a Euclidean space. One can take the differential calculus on the Euclidean space and push it forward to define a calculus on the subset. This simple technique allows one to define high-order tangent and cotangent bundles on subsets of Euclidean space. An issue with the approach is that the computation of the bundles is highly nontrivial. The first order tangent bundle can be defined by a standard blow-up argument, but the higher order ""paratangent bundles"" are difficult to understand. By focusing on a class of explicit examples of algebraic varieties with cuspidal singularities, the principal investigator will find new methods for computing with these abstract spaces. The principal investigator will develop a notion of divided difference quotients on algebraic and semialgebraic sets."
"1747522","EAGER:   Collaborative Research:  Inverse Procedural Material Modeling for Battery Design","IIS","CONDENSED MATTER & MAT THEORY, Info Integration & Informatics, Data Cyberinfrastructure, Materials Eng. & Processing","08/01/2017","07/26/2017","Holly Rushmeier","CT","Yale University","Standard Grant","Maria Zemankova","07/31/2018","$100,000.00","","rushmeier@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","1765, 7364, 7726, 8092","023E, 077E, 7364, 7433, 7916, 8084, 8396, 8399","$0.00","Nearly all portable electronic devices commonly used today -- cameras, phones, music players and the like -- rely on rechargeable Lithium-ion batteries. Improvements in the capabilities of these devices can be achieved by improving the design of these batteries. This work will produce new computational methods for designing batteries with desirable properties such as high power output and long lifespan. The new computational methods will use techniques that have successfully described complex volumetric structures (such as porous rocks and sponges) in computer graphics for film and games. These computer graphics techniques will be applied to describing the materials in batteries. Instead of focusing on finding volumetric structures that give the correct visual appearance, the new computational methods will focus on structures that produce the correct performance characteristics such as power density. The new volumetric descriptions will be used to generate a large number of potential volumetric materials, and these models will be characterized in terms of battery properties and performance. Using recently developed machine learning techniques, this large number of potential models will be converted into a form that is convenient to use in battery design. In addition to providing tools to create improved portable batteries, the new computational methods have the potential to be further extended and applied to other problems involving materials with complex volumetric structure such as understanding geologic measurements and designing conservation strategies for cultural heritage monuments and artifacts.<br/><br/>A straightforward approach to battery design is to theorize material microstructures, run forward simulations to assess their performance, and evaluate the results. However, simulations require hours (up to 50 hours on current multi-core systems for power density calculations), making forward simulation prohibitively expensive for iterative design. The design process can be dramatically improved if an inverse function is available that can produce a microstructure description given desired performance characteristics. Barriers to creating such an inverse function are the complexity of microstructure descriptions and the relationship between structure and performance. To create an inverse function, we need a microstructure description that is lower in dimension than a full enumeration of a high-resolution grid. A procedural model can provide such a lower dimensional description. The approach explored in this project for finding appropriate procedural models is based on combining and transforming models that have been successful in other problem domains to fit data from real battery material measurements. Given an appropriate procedural model, the design problem is reduced to determining the procedural model parameters that generate the input; a problem called ""inverse procedural modeling"". Even with a compact microstructure description, the problem is too complex to be mathematically inverted. Rather than attempt to find a mathematical function, machine learning (deep neural networks) are used. A database of microstructures and their performance characteristics will be populated synthetically with example microstructures computed from a large sampling of procedural model parameters. Forward simulations will be run on these samples to compute properties (tortuosity and area density) and performance characteristics (power and energy density.) Machine learning optimizations will then be used to find the relationship between model parameters and performance characteristics and this relationship will be used in the design process. The overall method of finding procedural models to fit data and then learning the relationships from synthetic data generated from the models brings the power of new data-driven approaches to the domain of battery design. The software, data and publications resulting from this project will be available at the project website (http://hpcg.purdue.edu/Eager2018/)."
"1700058","Combinatorial Representation Theory","DMS","Combinatorics","07/01/2017","06/07/2017","Rosa Orellana","NH","Dartmouth College","Standard Grant","Tomek Bartoszynski","12/31/2020","$113,770.00","","Rosa.C.Orellana@Dartmouth.edu","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551421","6036463007","MPS","7970","9150","$0.00","Algebraic objects are often perceived as complicated and mysterious. In combinatorial representation theory one seeks simpler ways to represent them in order to understand, organize and apply them to other subjects. This project deals with tensor products of representations, which is a way to combine representations. The main problem that the project seeks to understand is how to decompose tensor products into sums of simpler representations. One can think about it as the problem of recovering individual signals from a mixture of signals. The tensor product decomposition is a difficult problem but also very important, because it shows up in many fields, including algebraic combinatorics, complexity theory, and statistics, and has applications in medicine, computer vision, physics, chemistry, and fast matrix multiplication.<br/><br/>A fundamental open problem in combinatorial representation theory is to describe in the decomposition of the tensor product the multiplicities of two irreducible representations of the symmetric group; these multiplicities are called Kronecker coefficients. The problem of finding a combinatorial interpretation for these coefficients has been labeled as one of the ""main problems in combinatorial representation theory."" The PI and collaborators have been able to connect the Kronecker coefficients to the partition algebra. This connection led to the discovery of the ""universal characters"" of the symmetric group, which are symmetric functions that specialize to characters of the symmetric group when evaluated at roots of unity. These symmetric functions connect several difficult problems in combinatorial representation theory and will lead to the resolution of several of these problems."
"1723996","S&AS: FND: Learning-Enabled Autonomous 3D Exploration for Underwater Robots","IIS","S&AS - Smart & Autonomous Syst","09/01/2017","07/25/2017","Brendan Englot","NJ","Stevens Institute of Technology","Standard Grant","James Donlon","08/31/2021","$353,334.00","","brendan.englot@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","039Y","046Z","$0.00","There are often high costs and safety risks associated with humans performing work underwater, which is frequently required to inspect the health of our subsea infrastructure and environment. This motivates a need for smart and taskable autonomous robots that can monitor and inspect the subsea environment, as well as explore their surroundings when they do not have access to an accurate prior model. Without precise instructions on where and how to explore, the ideal taskable robot should be able to produce comprehensive, accurate maps of its surroundings, make repeated decisions about where to travel next, and ensure that it avoids collisions in the process of doing so. This project will leverage machine learning techniques to produce and deploy new algorithms with the potential to enhance both the speed and efficiency with which underwater robots explore unknown environments, and to enable further gains in performance as the exploring robots gain more experience.<br/><br/>Specifically, this project will introduce machine learning techniques to (1) build more descriptive occupancy maps from the sparse and noisy sonar data that typifies the subsea domain; (2) to save computational effort in the potentially exhaustive evaluation of many candidate sensing actions; and (3) to learn effective behaviors for exploring complex, unstructured environments that truly require three-dimensional spatial reasoning. The real-time, 3D exploration task will be managed in concert with other relevant objectives, such as minimizing localization and map uncertainty, and the time and energy expenditures associated with travel. A related goal is to develop robot systems whose performance improves with experience, dynamically choosing the most effective decision-making tools in its portfolio and self-parameterizing the most appropriate map representations for the task at hand. <br/>"
"1719031","PFI:BIC: iWork, a Modular Multi-Sensing Adaptive Robot-Based Service for Vocational Assessment, Personalized Worker Training and Rehabilitation.","IIP","PFI-Partnrships for Innovation, IIS Special Projects","09/01/2017","09/04/2019","Fillia Makedon","TX","University of Texas at Arlington","Standard Grant","Kaitlin Bratlie","08/31/2021","$1,031,638.00","Vassilis Athitsos, Nicolette Hass, Morris Bell","makedon@cse.uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","ENG","1662, 7484","116E, 1662, 9251","$0.00","Automation, foreign competition, and the increasing use of robots replacing human jobs, stress the need for a major shift in vocational training practices to training for intelligent manufacturing environments, so-called ""Industry 4.0"".  In particular, vocational safety training using the latest robot and other technologies is imperative, as thousands of workers lose their job or die on the job each year due to accidents, unforeseen injuries, and lack of appropriate assessment and training.  The objective of this Partnerships for Innovation: Building Innovation Capacity (PFI:BIC) project is to develop iWork, a smart robot-based vocational assessment and intervention system to assess the physical, cognitive and collaboration skills of an industry worker while he/she performs a manufacturing tasks in a simulated industry setting and collaborating with a robot to do the task. The aim is to transform traditional vocational training and rehabilitation practices to an evidence-based and personalized system that can be used to (re)train, retain, and prepare workers for robotic factories of the future. The need for personalized vocational training, rehabilitation and accurate job-matching is essential to ensuring a strong manufacturing sector, vital to America's economic development and ability to innovate. The iWork service is ""smart"" because it can adjust and adapt to the individual's abilities as it assesses him/her and help decide on the type of tasks needed to test and train, based on the job's complexity, difficulty or familiarity to the worker.  The iWork system integrates human expert knowledge to overcome or compensate for detected worker constraints. <br/>Research has shown that robot trainers can increase motivation and sustain interest, increase compliance and learning, and provide training for specific and individual needs. The iWork system aims to assess and train both the human and the work-assistive robot, as they collaborate on a manufacturing job.  The projected outcome is low-cost vocational training solutions that can have substantial economic and societal benefits to diverse economic sectors. Most importantly, if successful, projected outcomes could impact how millions of persons seeking a manufacturing job are trained, including those facing a type of learning, physical or aging disability. The system's mobile, low cost methods accelerate recognizing a worker's specific needs and improve the ability of the vocational expert to make correlations between cognitive and physical assessments, thus empowering traditional practices with user-centric targeted training methods. In addition, the project's robot-based emphasis on safety and risk assessment, can reduce liability costs and productivity setbacks faced by industry, due to manufacturing accidents. <br/>The iWork system uses computational methods in reinforcement (machine) learning, data mining, collaborative filtering and human robot interaction to collect and analyze multi-sensing worker data during a manufacturing human-robot collaboration simulation. Data collected and analyzed come from sensors, wearables, and explicit user feedback measuring worker movements, eye gazes, errors made, performance delays, human-robot interactions, physiological metrics, and others, depending on the task. The system has a closed loop architecture composed of four phases: assessment, recommendation, intervention (or adjustment), and evaluation, with a human expert in the loop. The system generates recommendations for personalized interventions to the expert, at different loop intervals. Use of the latest developments in sensing technologies, robotics and intelligent communications, assess the ability to enhance the intelligence of a robot co-worker with more human-like learning and collaboration abilities to support the human in achieving a task.  The system is modular and customizable to a particular manufacturing task, domain or worker robot. Two types of robots are used, socially assistive robots that provide non-contact user assistance through feedback and physically assistive robots that provide cognitive, physical and collaboration skill training. To predict risks of injury due to inattention, age, vision, or physical and mental issues, motion analysis and kinematics experiments are conducted to determine the type of safety training needed, to assess how well a human interacts with a collaborative robot, and how best to train the robot to help the human overcome identified physical and other deficiencies in performing a given task. The project integrates three main areas of expertise, engineered service system design, where assistive robots interact with and train each other to collaborate; computing, sensing, and information technologies, where machine learning, data mining and recommender algorithms are used to identify behavioral patterns of interest, and recommend targeted interventions; and human factors and cognitive engineering that deploy methods from the team's expertise in workplace assessment, personalized psychiatric intervention, and evaluation methods of vocational satisfaction, work habits, work quality, etc., as they relate to job preparation and retention.<br/>The project has an interdisciplinary team of experts from two collaborating universities, University of Texas Arlington (UTA) and Yale University, representing several fields, including human factors, psychology, computing, and industrial organization. The project deploys two primary industry partners, SoftBank Robotics (San Francisco, CA) manufacturer of humanoid service robots, and InteraXon (Canada), producing mobile EEG devices, who provides hardware, software and know-how to enhance iWork's functionality in cognitive activity monitoring. The broader context partners include, C8Sciences (USA), Assistive Technology Resources (USA), Barrett Technologies Inc. (USA), and the Dallas Veteran Affairs Research Corp. (USA)."
"1730568","IUSE/PFE:RED: PPSE - Transforming Programmers to Professional Software Engineers through Curricular Innovation, Inclusive Pedagogy, and Faculty Development","EEC","PFE\RED - Professional Formati","07/01/2017","06/27/2017","Venkat Gudivada","NC","East Carolina University","Standard Grant","Edward Berger","06/30/2022","$2,000,000.00","William Sugar, Junhua Ding, Qin Ding, Marjorie Ringler","gudivadav15@ecu.edu","Office Research Administration","Greenville","NC","278584353","2523289530","ENG","012Y","110E, 1340","$0.00","Many computer science departments in general, and less-research-intensive departments in particular, experience high dropout of undergraduate students. This is particularly worrisome given that the US Bureau of Labor Statistics predicts that by 2020, half of all STEM jobs will be in computing. Several research advances and technological innovations have fundamentally changed the computing discipline in recent years. Such advances include cloud and mobile computing, web technologies, high-performance computing, social media, big data, and machine learning. Similar advances have also occurred in learning science and educational technologies. Learning science informs how students actually learn and educational technologies leverage mobile computing and learning analytics to enable formal and informal learning. Millennial students learn differently and the above advances provide unprecedented opportunities to enhance student learning. This project seeks to dramatically improve computer science students' retention and graduation rates through a transformational process that involves curricular innovations, design and development of inclusive pedagogy, and faculty development. Both technical and professional skills will be interspersed throughout the curriculum to better prepare them for industry careers. The project addresses diversity in student learning through personalization and  enables a significant increase in the participation of underrepresented groups in computing education and careers. Lastly, the project strives to increase the number of transfer students from community colleges and early college high schools completing undergraduate computing degrees in universities. The results of the project will be widely disseminated across the country to help other computer science departments improve their students' success.<br/><br/>The goals of the project are (1) transform the programming-centric computer science education approach to a systems-oriented and software engineering-centric one, (2) infuse professional skills development process into the entire curriculum, (3) dramatically increase retention and graduation rates, (4) recruit significantly more students from underrepresented groups (5) personalize teaching and learning in both formal and informal settings, and (6) work with community colleges and early college high schools in the region to increase the number of transfer students and enhance their success in college. The project is both innovative and revolutionary. The project will design a non-course-centric curriculum, develop an innovative and inclusive pedagogy, use free and open-source software to provide a compelling and contextualized teaching and learning environment. Professional skills development will be interspersed throughout the curriculum through team projects, internship and co-op opportunities, undergraduate research and entrepreneurship, and professional networking. The project will improve retention and graduation rates by employing research-based approaches such as providing motivating and inviting academic environment, proactive efforts in student advising and degree planning, and fostering positive faculty-student relationships. Participation of underrepresented groups will be significantly increased through collaborative partnerships with community colleges and early college high schools. Personalization of teaching and learning materials will be achieved through structuring and development of materials through fine-granular authoring and semantic markup. To overcome faculty resistance barriers, the project employs a change model which is based on Appreciative Inquiry, which reflects local context and constraints. The change model drives the research questions we investigate in the project. It takes an unprecedented, bold, and systemic approach to the professional formation of software engineers."
"1722516","SCH: INT: Personalized Real-Time Learning of Optimal Diagnostic Tests using Multi-Modal Clinical Data","IIS","Info Integration & Informatics, Smart and Connected Health","09/01/2017","07/13/2020","William Hsu","CA","University of California-Los Angeles","Standard Grant","Sylvia Spengler","08/31/2021","$1,338,533.00","Mihaela van der Schaar","whsu@mednet.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364, 8018","7364, 8018, 8062, 9251","$0.00","This project harnesses the growing amount of data that is captured in the electronic health record to discover the optimal diagnostic pathway for an individual patient. A multidisciplinary team with expertise in decision modeling, radiology/clinical practice, informatics, and machine learning will investigate approaches that transform data into actionable knowledge, enabled by a new class of clinical decision support algorithms that actively learn from available clinical data. The objective of this project is to develop and evaluate a data-driven framework for decision support that helps clinicians to deliver individualized patient care by discovering optimal sequences of actions and to diagnose patients in a timely, accurate, and cost-effective manner. The project addresses challenges related to finding relevant information from large, longitudinal patient data; learning sequences of actions from past patient cases; and handling uncertainty that is inherent to the practice of medicine. The new algorithms will improve how observational clinical data can be used to generate evidence that improves healthcare delivery, efficiency, and ultimately, realizes precision medicine and improves patient outcomes. A diverse group of graduate students will be trained in an interdisciplinary manner to translate algorithms and data science concepts into applications that have real-world clinical utility, and with a clear understanding of the technical and cognitive challenges.<br/><br/><br/>The proposed research will create a generalizable framework for learning from healthcare data to discover optimal actions for individual patients with the following objectives: (1) to determine what combination of diagnostic procedures (e.g., imaging, labs, biopsy) should be used to achieve an accurate and timely diagnosis, and in what sequence; and (2) to demonstrate that learning such pathways can be done using available data, allowing the new methodology to be applied in a wide range of clinical domains. Novel aspects of this project are three-fold: (1)Dealing with an environment that is unknown and changing over time in unpredictable ways through a novel adaptive learning approach to discover the most informative features that are predictive of subsequent actions taken in real-time. This builds upon earlier work in relevance learning to dynamically elucidate the relationships between clinical features and possible actions. (2)Developing a new type of bandit algorithm that not only discovers the next best diagnostic test to order, but also identifies additional information that is needed to make a definitive diagnosis. The accuracy and value of diagnostic tests are dependent on many factors (e.g., technology, patient characteristics). The team will assess how prior information from similar patients can speed-up learning given these factors. (3)Providing confidence bounds about the risks and benefits of selecting a specific diagnostic exam to perform. These confidence bounds can be easily understood by clinicians. The performance and utility of this approach will be demonstrated using a prospective study that solicits physician feedback about specific recommendations for a given patient case and learns from situations in which the physician does not follow the system's recommendation."
"1737744","ATD:   The Foundations of Dynamic Drone-Based Threat Detection","DMS","ATD-Algorithms for Threat Dete, ","09/01/2017","07/25/2018","Guillermo Sapiro","NC","Duke University","Continuing Grant","Leland Jameson","08/31/2021","$199,850.00","Qiang Qiu","guillermo.sapiro@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","MPS","046Y, Q218","6877","$0.00","Drone-based threat detection enables unprecedented coverage and flexibility in understanding human dynamics, with applications to real-time identification of unusual events and forecast of future threats. With these new possibilities come unique challenges, from highly dynamic scene changes to the need for low-cost operation. This project focuses on the foundations of video analysis technology for such dynamic drone-based threat detection. The work ranges from mathematical foundations in the area of learning and modeling to applications such as people tracking and identification. In terms of data, the project includes collection and analysis of drone-based video data, sharing data and the developed code with the community at large. The project will not only contribute to the emerging area of drone-based threat analysis but will also provide fundamental building blocks for modern visual data exploitation. Components of this project will be incorporated in online image-processing classes.<br/> <br/>The work investigates fundamental problems motivated by drone-based video analysis, including orientation invariance, image hashing, multi-modality modeling, and progressive unsupervised self-learning. The project develops and exploits underlying mathematical foundations, such as subspace modeling and invariant filter design. All the work has efficiency as its goal; this being manifested from the development of memory and computationally efficient forest hashing to the development of oriented response networks with significantly reduced deep models for orientation invariance. To enable state-of-the-art performance, the project utilizes successful machine learning frameworks, including deep convolution neural networks, random forests, hashing, and latent-SVM. This is approached with fundamental enabling redesigns and developments in the areas of robust learning, invariant learning, unsupervised self-learning, and multimodal hashing. The contributions are critical for data-limited learning, cross-modality learning, and computationally/memory efficient systems. The project aims to develop and exploit underlying mathematical foundations, such as subspace modeling, invariant filter design and learning, robust geometry-based learning, and information-based code aggregation. The theoretical and computational contributions are expected to result in efficient implementations of threat detection for dynamic environments, drone videos being a particularly important example."
"1734492","NRI: INT: COLLAB: Integrated Modeling and Learning for Robust Grasping and Dexterous Manipulation with Adaptive Hands","IIS","NRI-National Robotics Initiati","09/01/2017","07/27/2017","Kostas Bekris","NJ","Rutgers University New Brunswick","Standard Grant","James Donlon","08/31/2021","$867,729.00","Abdeslam Boularias","kostas.bekris@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8013","8086","$0.00","Robots need to effectively interact with a large variety of objects<br/>that appear in warehouses and factories as well as homes and offices.<br/>This requires robust grasping and dexterous manipulation of everyday<br/>objects through low cost robots and low complexity solutions.<br/>Traditionally, robots use rigid hands and analytical models for such<br/>tasks, which often fail in the presence of even small errors. New<br/>compliant hands promise improved performance, while minimizing<br/>complexity, and increased robustness. Nevertheless, they are<br/>inherently difficult to sense and model. This project combines ideas<br/>from different robotics sub-fields to address this limitation. It<br/>utilizes progress in machine learning and builds on a strong tradition<br/>in robot modeling. The objective is to provide adaptive, compliant<br/>robots that are better in grasping objects in the presence of multiple<br/>unknown contact points and sliding or rolling objects in-hand. The<br/>broader impact will be strengthened by the open release of new or<br/>modified robot hand designs, improved control algorithms and software,<br/>as well as corresponding data sets. Furthermore, academic<br/>dissemination will be accompanied by educational outreach to<br/>undergraduate and high school students.<br/><br/>Towards the above objective, the first step will be the definition of<br/>new hybrid models appropriate for adaptive, compliant hands.  This<br/>will happen by improving analytical solutions and extending them to<br/>allow adaptation based on data via novel, time-efficient learning<br/>methods. The objective is to capture model uncertainty inherent in<br/>real-world interactions; a process that suffers from data scarcity.<br/>In order to reduce the amount of data required for learning, different<br/>models will be tailored to specific tasks through an automated<br/>discovery of these tasks and of underlying motion primitives for each<br/>one of them. This task identification process will operate iteratively<br/>with learning and utilize improved models to discover new tasks. It<br/>can also provide feedback for improved hand design. Once these<br/>learning-based and task-focused models are available, they will be<br/>used to learn and synthesize controllers for grasping and in-hand<br/>manipulation. To learn controllers, this work will consider a<br/>model-based, reinforcement learning approach, which will be evaluated<br/>against alternatives. For controller synthesis, existing tools for<br/>this purpose will be integrated with task planning primitives and<br/>extended through learning processes to identify the preconditions<br/>under which different controllers can be chained together. The project<br/>involves extensive evaluation on a variety of novel adaptive hands and<br/>robotic arms designed in the PIs' labs. Modern vision-based solutions<br/>will be used to track grasped objects and provide feedback for<br/>learning and closed-loop control.  The evaluation will measure whether<br/>the developed hybrid models can significantly improve robustness of<br/>grasping and the effectiveness of dexterous manipulation."
"1740798","Collaborative Research: Community-Building and Infrastructure Design for Data-Intensive Research in Computer Science Education","DRL","ECR-EHR Core Research","09/01/2017","08/15/2017","Ken Koedinger","PA","Carnegie-Mellon University","Standard Grant","Wu He","02/28/2021","$299,760.00","","Koedinger@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","EHR","7980","7433, 8083","$0.00","The Building Community and Capacity in Data Intensive Research in Education program seeks to enable research communities to develop visions, teams, and capabilities dedicated to creating new, large-scale, next-generation data resources and relevant analytic techniques to advance fundamental research for areas of research covered by the Education and Human Resources Directorate. Successful proposals will outline activities that will have significant impacts across multiple fields by enabling new types of data-intensive research. Online educational systems, and the large-scale data streams that they generate, have the potential to transform education as well as our scientific understanding of learning. Computer Science Education (CSE) researchers are increasingly making use of large collections of data generated by the click streams coming from eTextbooks, interactive programming environments, and other smart content. However, CSE research faces barriers that slow progress: 1) Collection of computer science learning process and outcome data generated by one system is not compatible with that from other systems. 2) Computer science problem solving and learning (e.g., open-ended coding solutions to complex problems) is quite different from the type of data (e.g., discrete answers to questions or verbal responses) that current educational data mining focuses on. This project will build community and capacity among CSE researchers, data scientists, and learning scientists toward reducing these barriers and facilitating the full potential of data-intensive research on learning and improving computer science education. The project will bring together CSE tool building communities with learning science and technology researchers towards developing a software infrastructure that supports scaled and sustainable data-intensive research in CSE that contributes to basic science of human learning of complex problem solving. The project will support community-building and infrastructure capacity-building whose ultimate goal is to develop and disseminate infrastructure that facilitates three aspects of CSE research: (1) development and broader re-use of innovative learning content that is instrumented for rich data collection, (2) formats and tools for analysis of learner data, and (3) best practices to make large collections of learner data and associated analytics available to researchers in CSE, data science, or learning science. To achieve these goals, a large community of researchers will be engaged to define, develop, and use critical elements of this infrastructure toward addressing specific data-intensive research questions.The project will host workshops, meetings, and online forums leveraging existing communities and building new capacities toward significant research outcomes and lasting infrastructure support.<br/><br/>This project will provide an infrastructure that can support various kinds of research in CSE domain as a one-stop-shop, and will be the first to focus on full-cycle educational research infrastructure in any domain. CSE tool developers and educators will become more productive at creating and integrating advanced technologies and novel analytics. Learning researchers will have better tools for analyzing the huge amounts of learner data that modern digital education software produces. Data scientists will have rich new datasets in which to explore new machine learning and statistical techniques. Collectively, these efforts will reduce barriers to educational innovation and support scientific discoveries about the nature of complex learning and how best to enhance it. The project will support scientific investigations through community meetings and mini-grants to others addressing questions such as: What is the optimal ratio of solution examples and problem-solving practice? How do computational thinking skills emerge? In what quanta are programming skills acquired? Can automated tutoring of programming be effective at scale in enhancing student learning?. Many of the innovations developed under this project will directly impact learning in any discipline. Educational software will more quickly be developed in the future, that more easily generates meaningful learner data, which in turn can be more easily analyzed."
"1722436","SBIR Phase I:  A novel approach to STEM education through a personalized mobile cooking app for K-8 students","IIP","SMALL BUSINESS PHASE I","06/01/2017","06/12/2017","Layla Sabourian","CA","Chef Koochooloo","Standard Grant","Rajesh Mehta","11/30/2018","$225,000.00","","layla@chefkoochooloo.com","179 Georgetown Court","Mountain View","CA","940435265","6504636041","ENG","5371","110E, 5371, 8031, 8032, 8039, 9177","$0.00","This SBIR Phase II project deals with the development of a gamified educational platform meant to excite and inspire children to learn about the sciences through the medium of cooking. By engaging students with interactive content in K-8 classrooms, after school programs, and special education settings, the proposed innovation aims to simultaneously accomplish three outcomes that are important to K-8 students' education and healthy living. These outcomes include: (1) helping students develop a highly practical skill (cooking), (2) generating excitement about and enhancing learning in several scientific disciplines (particularly nutrition science, STEM, and human geography), and (3) promoting a highly desirable behavior in students (the development of healthy eating habits). This project proposes to develop a prototype version of the innovative educational technology. It builds upon an existing proof-of-concept application that integrates nutrition science (in the form of cooking recipes) with human geography (different countries and cultures) and significantly enhances this application by innovatively linking the cooking-related content to a STEM curriculum and testing the addition of a variety of capabilities meant to increase user engagement and learning through the application. This Phase I proposal will research the feasibility and usability of the product, and investigate the extent to which the proposed technology is able to simultaneously achieve the three stated outcomes in a way that outperforms traditional paper-based school curricula. With the market for mobile learning products valued at $12.2 billion, the company's three-pronged approach to improving learning and health-related outcomes for K-8 students stands to generate significant revenue. The team estimates that the business can reach roughly 2 million in sales in the first three years following the product?s launch.<br/><br/>The proposed research will explore (1) various ways for the system to automatically link cooking concepts with STEM concepts in a way that enhances learning and engagement and produces a curriculum compliant with Next Generation Science Standards, (2) the application of emotional intelligence to data gathered, such that the technology will track students? mood and food choices, and provide suggestions based on an optimal match between the two, and (3) the use of adaptive learning to effectively deliver the above curriculum in a targeted and personalized manner (including accommodations for children with special needs during Phase II). These outcomes will be achieved by creating and testing new capabilities for the system, including the ability to monitor user performance, a points-based progression system, personalization, customized STEM games, and matching students with recipes and lesson plans based on their culinary preferences and emotional moods, the majority of which will be implemented via the use of machine learning. Overall, the goal of the small business is to develop several different app features that incorporate the above-mentioned approaches, and to determine, through repeated and rigorous classroom testing with K-8 students, which combination of features maximizes the learning and behavioral outcomes that the application is targeting."
"1735003","NCS-FO: Collaborative Research: Integrative Foundations for Interactions of Complex Neural and Neuro-inspired Systems with Realistic Environments","IIS","IntgStrat Undst Neurl&Cogn Sys","09/01/2017","08/07/2017","John Doyle","CA","California Institute of Technology","Standard Grant","Kenneth Whang","08/31/2020","$369,200.00","","doyle@cds.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","8624","8089, 8091, 8551","$0.00","This project will integrate the capabilities of deep learning networks into a biologically inspired architecture for sensorimotor control that can be used to design more robust platforms for complex engineered systems. Studies of the flexibility and contextual aspects of sensorimotor planning and control will extend existing paradigms for human-robot interactions and serve as the foundation for creating personal assistants that are able to operate in natural settings. Growing understanding of how these layered architectures are organized in the brain to produce highly robust, flexible, and efficient behavior will have many applications to rapidly evolving technologies in complex environments, including the Internet of Things, autonomous transportation, and sustainable energy networks.<br/><br/>The nervous system is a layered architecture, seamlessly integrating high-level planning with fast lower level sensing, reflex, and action in a way that this project aims to both understand more deeply and mimic in advanced technology. The central goal is to develop a theoretical framework for layered architectures that takes into account both system level functional requirements and hardware constraints. There are both striking commonalities and significant differences between biology and technology in using layered architectures for active feedback control. The most salient and universal hardware constraints are tradeoffs between speed, accuracy, and costs (to build, operate, and maintain), and successful architectures cleverly combine diverse components to create systems that are both fast and accurate, despite being built from parts that are not.   Recent progress has made it possible to integrate realistic features and constraints for sensorimotor coordination in a coherent and rigorous way using worst-case L-infinity bounded uncertainty models from robust control, but much remains to explore to realize its potential in neuroscience and neuro-inspired engineering. Another application of this framework will be to software defined networking, which explicitly separates data forwarding and data control, and provides an interface through which network applications (such as traffic engineering, congestion control and caching) can programmatically control the network.  This makes it a potential ""killer app"" for a theory of integrated planning/reflex layering; research collaborators will be eager to deploy new protocols on testbeds and at scale."
"1735004","NCS-FO: Collaborative Research: Integrative Foundations for Interactions of Complex Neural and Neuro-Inspired Systems with Realistic Environments","IIS","IntgStrat Undst Neurl&Cogn Sys","09/01/2017","08/07/2017","Terrence Sejnowski","CA","The Salk Institute for Biological Studies","Standard Grant","Kenneth Whang","08/31/2020","$481,411.00","","terry@salk.edu","10010 N TORREY PINES RD","LA JOLLA","CA","920371002","8584534100","CSE","8624","8089, 8091, 8551","$0.00","This project will integrate the capabilities of deep learning networks into a biologically inspired architecture for sensorimotor control that can be used to design more robust platforms for complex engineered systems. Studies of the flexibility and contextual aspects of sensorimotor planning and control will extend existing paradigms for human-robot interactions and serve as the foundation for creating personal assistants that are able to operate in natural settings. Growing understanding of how these layered architectures are organized in the brain to produce highly robust, flexible, and efficient behavior will have many applications to rapidly evolving technologies in complex environments, including the Internet of Things, autonomous transportation, and sustainable energy networks.<br/><br/>The nervous system is a layered architecture, seamlessly integrating high-level planning with fast lower level sensing, reflex, and action in a way that this project aims to both understand more deeply and mimic in advanced technology. The central goal is to develop a theoretical framework for layered architectures that takes into account both system level functional requirements and hardware constraints. There are both striking commonalities and significant differences between biology and technology in using layered architectures for active feedback control. The most salient and universal hardware constraints are tradeoffs between speed, accuracy, and costs (to build, operate, and maintain), and successful architectures cleverly combine diverse components to create systems that are both fast and accurate, despite being built from parts that are not.   Recent progress has made it possible to integrate realistic features and constraints for sensorimotor coordination in a coherent and rigorous way using worst-case L-infinity bounded uncertainty models from robust control, but much remains to explore to realize its potential in neuroscience and neuro-inspired engineering. Another application of this framework will be to software defined networking, which explicitly separates data forwarding and data control, and provides an interface through which network applications (such as traffic engineering, congestion control and caching) can programmatically control the network.  This makes it a potential ""killer app"" for a theory of integrated planning/reflex layering; research collaborators will be eager to deploy new protocols on testbeds and at scale."
"1661088","Collaborative research: Neural and cognitive strengthening of conceptual knowledge and reasoning in classroom-based spatial education","DRL","ECR-EHR Core Research","04/15/2017","04/05/2019","David Kraemer","NH","Dartmouth College","Continuing Grant","Gregg Solomon","03/31/2021","$322,679.00","","David.J.M.Kraemer@Dartmouth.edu","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551421","6036463007","EHR","7980","1544, 8089, 8091, 8212, 8817","$0.00","Spatial thinking is a powerful driver of success in the STEM classroom and spatial thinking is a major predictor of future STEM success in the workforce. The brain systems that support spatial thinking have been well mapped by neuroscience to allow clear interpretation of new brain-imaging data. Recent advances in tools used to analyze brain activity allow detection of changes in the brains of students that signify accurate learning of STEM concepts. This advance may open a window onto biomarkers of precisely the type of learning that is the goal of educators. Using these new brain analysis methods, this project, a collaboration involving researchers from James Madison University, Georgetown University, Northwestern University, and Dartmouth College, will investigate how changes in the spatial thinking network support learning of specific STEM concepts, and how changes in the classroom can facilitate changes in the brain related to spatial thinking. This cross-disciplinary project brings together experts in geoscience classroom education, spatial cognition, and the neural bases of learning and reasoning. This team is committed to bridging the conspicuous gap between the laboratory and the high school classroom. A confluence of advances in neuroimaging, and the research team's partnership with Virginia school systems make this effort timely and tractable. Identifying possible effects of sex and STEM-related anxieties on conceptual learning in the brain, and testing the effectiveness of spatial education for reducing disparities, this research will point to critical targets for intervention. The project is funded by the EHR Core Research (ECR) program, which supports work that advances the fundamental research literature on STEM learning.<br/><br/>This project seeks to understand the neural mechanisms of spatial learning, to advance of spatial education, and to identify factors that affect disparities in STEM learning and participation. The research team will collect functional magnetic resonance imaging (fMRI) and behavioral data from students before and after learning in a high school geoscience course that uses a novel spatially-based curriculum to teach STEM concepts and spatial reasoning. Pilot data on this spatial curriculum have begun to characterize the underlying cognitive and neural mechanisms at work, and show promising effects of transfer to STEM problem solving and core measures of spatial ability. Consistent with methods that have demonstrated success in the lab (but not yet the classroom), the research team will use multivariate neural representations of a group of highly experienced and specially trained teachers as an expert standard to determine neural markers of students? conceptual knowledge and spatial reasoning. Leveraging recent multivariate pattern analysis (MVPA) and machine-learning advances in brain imaging, the team will compare the neural patterns of students before and after learning to test for a trajectory that moves students closer to expert representations. This project will also test, for the first time, whether it is possible to compare different curricula based on how much they strengthen the representation of a concept in the brain. Similarly, this work will test whether spatial education leads students to engage spatial brain resources for STEM-related reasoning, and seek to compare curricula on this basis. The project will test whether neural data add predictive value to traditional testing (e.g. conventional unit tests) for subsequent retention of conceptual knowledge and spatial reasoning. Assessments of STEM-related anxieties (e.g., math and spatial anxiety) and analyses of sex-related effects on cognitive and neural outcomes will newly characterize factors that influence disparities in STEM learning and participation."
"1758028","CAREER: Bayesian Nonparametric Learning for Large-Scale Structure Discovery","IIS","Robust Intelligence","09/01/2017","05/24/2018","Erik Sudderth","CA","University of California-Irvine","Continuing Grant","Rebecca Hwa","02/28/2021","$331,349.00","","sudderth@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","1045, 7495","$0.00","CAREER: Bayesian Nonparametric Learning for Large-Scale Structure Discovery<br/><br/>This CAREER project will advance the state-of-the-art for automated discovery of structure within data as diverse as images and video, natural language, audio sequences, and social and biological networks.  Contemporary applications of statistical machine learning are dominated by parametric models.  This approach constructs models of pre-determined size (with a finite-dimensional vector of parameters which) are tuned using training data.  To be effective, the underlying structure of such models must be manually specified by experts with application-specific knowledge.  This presumed structure imposes limits on what can possibly be learned even from very big datasets.<br/><br/>Bayesian nonparametric models instead define distributions on models of arbitrary size with infinite-dimensional spaces of functions, partitions, or other combinatorial structures.  They lead to flexible, data-driven unsupervised learning algorithms, and models whose internal structure continually grows and adapts to new observations.  Bayesian nonparametric models, while promising, are an incompletely-developed technology posing significant challenges to practice.  This CAREER project will increase the practical feasibility and impact of Bayesian nonparametric approaches by pursuing three interrelated themes:<br/><br/>1) Nonparametric Model Design and Evaluation.  New families of models for data with hierarchical, spatial, temporal, or relational structure are investigated.  Quantitative validation of the statistical assumptions and biases inherent in these models will be emphasized, evaluating whether these align with the empirical statistics of significant application areas.<br/><br/>2) Reliable Structure Discovery.  Statistical inference algorithms which move beyond the local moves of standard (and widely used) Monte Carlo and variational methods will be developed.  Compelling examples indicate that local optima are a significant issue for contemporary methods, so a family of novel algorithms is proposed, which dynamically adjust model complexity as learning proceeds.<br/><br/>3) Scalable and Extensible Nonparametric Learning.  Common patterns across a wide range of popular nonparametric models are identified, which suggest a corresponding family of scalable and parallelizable online learning algorithms.  The ""memoized"" online variational inference algorithm avoids some practical instabilities and sensitivities of conventional methods, while allowing provably correct optimization of the nonparametric model structure and complexity.<br/><br/>An extensible ""BNPy: Bayesian Nonparametric Learning in Python"" software package is under development to allow easy application of the novel learning algorithms to a wide range of current and future BNP models.  The education and outreach plan of this CAREER project leverages this software to create interdisciplinary undergraduate research teams exploring applications in the natural and social sciences, and a week-long summer school on Bayesian nonparametrics to be held twice at Brown University's Institute for Computational and Experimental Research in Mathematics (ICERM)."
"1740775","Collaborative Research: Community-Building and Infrastructure Design for Data-Intensive Research in Computer Science Education","DRL","ECR-EHR Core Research","09/01/2017","08/15/2017","Peter Brusilovsky","PA","University of Pittsburgh","Standard Grant","Wu He","02/28/2021","$271,216.00","","peterb@mail.sis.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","EHR","7980","7433, 8083","$0.00","The Building Community and Capacity in Data Intensive Research in Education program seeks to enable research communities to develop visions, teams, and capabilities dedicated to creating new, large-scale, next-generation data resources and relevant analytic techniques to advance fundamental research for areas of research covered by the Education and Human Resources Directorate. Successful proposals will outline activities that will have significant impacts across multiple fields by enabling new types of data-intensive research. Online educational systems, and the large-scale data streams that they generate, have the potential to transform education as well as our scientific understanding of learning. Computer Science Education (CSE) researchers are increasingly making use of large collections of data generated by the click streams coming from eTextbooks, interactive programming environments, and other smart content. However, CSE research faces barriers that slow progress: 1) Collection of computer science learning process and outcome data generated by one system is not compatible with that from other systems. 2) Computer science problem solving and learning (e.g., open-ended coding solutions to complex problems) is quite different from the type of data (e.g., discrete answers to questions or verbal responses) that current educational data mining focuses on. This project will build community and capacity among CSE researchers, data scientists, and learning scientists toward reducing these barriers and facilitating the full potential of data-intensive research on learning and improving computer science education. The project will bring together CSE tool building communities with learning science and technology researchers towards developing a software infrastructure that supports scaled and sustainable data-intensive research in CSE that contributes to basic science of human learning of complex problem solving. The project will support community-building and infrastructure capacity-building whose ultimate goal is to develop and disseminate infrastructure that facilitates three aspects of CSE research: (1) development and broader re-use of innovative learning content that is instrumented for rich data collection, (2) formats and tools for analysis of learner data, and (3) best practices to make large collections of learner data and associated analytics available to researchers in CSE, data science, or learning science. To achieve these goals, a large community of researchers will be engaged to define, develop, and use critical elements of this infrastructure toward addressing specific data-intensive research questions.The project will host workshops, meetings, and online forums leveraging existing communities and building new capacities toward significant research outcomes and lasting infrastructure support.<br/><br/>This project will provide an infrastructure that can support various kinds of research in CSE domain as a one-stop-shop, and will be the first to focus on full-cycle educational research infrastructure in any domain. CSE tool developers and educators will become more productive at creating and integrating advanced technologies and novel analytics. Learning researchers will have better tools for analyzing the huge amounts of learner data that modern digital education software produces. Data scientists will have rich new datasets in which to explore new machine learning and statistical techniques. Collectively, these efforts will reduce barriers to educational innovation and support scientific discoveries about the nature of complex learning and how best to enhance it. The project will support scientific investigations through community meetings and mini-grants to others addressing questions such as: What is the optimal ratio of solution examples and problem-solving practice? How do computational thinking skills emerge? In what quanta are programming skills acquired? Can automated tutoring of programming be effective at scale in enhancing student learning?. Many of the innovations developed under this project will directly impact learning in any discipline. Educational software will more quickly be developed in the future, that more easily generates meaningful learner data, which in turn can be more easily analyzed."
"1840866","BIGDATA: Collaborative Research: F: Stochastic Approximation for Subspace and Multiview Representation Learning","IIS","CDS&E-MSS, Big Data Science &Engineering","09/01/2017","07/23/2018","Han Liu","IL","Northwestern University","Standard Grant","Sylvia Spengler","08/31/2021","$359,185.00","","hanliu@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","8069, 8083","7433, 8083, 8084, 8251","$0.00","Unsupervised learning of useful features, or representations, is one of the most basic challenges of machine learning. Unsupervised representation learning techniques capitalize on unlabeled data which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data, disentangles underlying factors of variation by incorporating universal AI priors such as smoothness and sparsity, and is useful across multiple tasks and domains. <br/><br/>This project aims to develop new theory and methods for representation learning that can easily scale to large datasets. In particular, this project is concerned with methods for large-scale unsupervised feature learning, including Principal Component Analysis (PCA) and Partial Least Squares (PLS). To capitalize on massive amounts of unlabeled data, this project will develop appropriate computational approaches and study them in the ?data laden? regime. Therefore, instead of viewing representation learning as dimensionality reduction techniques and focusing on an empirical objective on finite data, these methods are studied with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation approaches, such as Stochastic Gradient Descent (SGD) and Stochastic Mirror Descent, that are incremental in nature and process each new sample with a computationally cheap update. Furthermore, this view enables a rigorous analysis of benefits of stochastic approximation algorithms over traditional finite-data methods. The project aims to develop stochastic approximation approaches to PCA and PLS and related problems and extensions, including deep, and sparse variants, and analyze these problems in the data-laden regime."
"1733857","AITF: Learning and Adapting Sparse Recovery Algorithms for RF Spectrum Sensing","CCF","Algorithms in the Field","10/01/2017","08/23/2017","John Wright","NY","Columbia University","Standard Grant","Tracy Kimbrel","09/30/2021","$649,963.00","Peter Kinget","jw2966@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7239","7363","$0.00","Wireless communications technology plays a critical role in society, supporting personal communication, business, defense and security, family connectivity, and entertainment. As new applications emerge, the demand for connectivity is increasing at a rapid pace. This development has put a strain on the available resources for communications: the spectrum is a finite natural resource; the most useful portions for mobile applications lie roughly from 700 MHz to 6 GHz. Large portions of it have been allocated to primary users, many of whom play socially critical roles. To accommodate the growing demand for wireless connectivity, there is a need for devices that can sense and utilize available spectrum in an opportunistic manner, while not interfering with primary users. The challenge is to do this in a time- and energy-efficient manner, on mobile devices. A general approach, which promises order-of-magnitude improvements in energy efficiency for rapidly detecting large interfering signals, uses a combination of new hardware to take a small number of measurements of the spectrum as a whole, and nontrivial algorithms to interpret those measurements. This project develops from machine learning to learn algorithms that are adapted to the specific characteristics of the hardware sensor, improving handling of non-linearities, yielding lower power sensors with improved sensitivity. The researchers are mentoring graduate and undergraduate students, whose work crosses disciplinary boundaries, and disseminating the results through new course development and a new textbook. <br/><br/>The project studies methodologies for learning and adapting algorithms for sparse recovery for RF spectrum sensing, leveraging a known connection to artificial neural networks, in which the structure of the algorithm dictates the topology and weights of the network. These weights can then be adapted and optimized to fit the characteristics of a physical sensor. A major promise of this approach is the ability to adapt to modeling errors, while simultaneously producing recovery methods that are more sensitive, more robust, and implementable in a simple and efficient manner. The project is developing a principled and transparent methodology, including theoretical characterizations of when and why it is possible to learn reconstruction and support recovery procedures that are effective in both typical and worst-case senses. The project studies these problems both for linear inverse problems and for nonlinear problems, both for sensing the spectrum at a single time, and for integrating information over time. The project experimentally evaluates the impact of these methodologies on the efficiency and sensitivity of hardware sensors, realized as integrated circuits."
"1654268","CAREER:   Form and Function in Cortical Neuronal Networks","DMS","MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY, Modulation, Division Co-Funding: CAREER","07/01/2017","07/13/2020","Robert Rosenbaum","IN","University of Notre Dame","Continuing Grant","Junping Wang","06/30/2022","$333,236.00","","robert.rosenbaum@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","7334, 7454, 7714, 8048","1045, 1096, 8007, 9179","$0.00","The cerebral cortex is the central processing area of the brain.  Nearly all higher cognitive functions depend on the activity patterns of cortical neuron populations.  These activity patterns are shaped by the structure of connectivity between cortical neurons and, conversely, cortical connectivity is shaped by cortical activity through changes in connection strengths called ""synaptic plasticity.""  This project will develop novel computational models and mathematical analyses to understand the interplay between cortical connectivity and cortical activity, and how it gives rise to cognitive functions like sensory processing and motor learning.  The results will lead to a better understanding of how the cortex functions and how it dysfunctions in disease states.  The results will also inform the development of biologically-inspired machine learning algorithms.  Graduate and undergraduate students will be involved in all aspects of the research and benefit from coursework inspired by the research projects.  Students at a public high school will be guided in a project to incorporate learning algorithms from the research into a robot, providing hands on experience with applied mathematics, computer programming, and robotics.<br/><br/>The project comprises three sub-projects that build on each other.  The first sub-project utilizes the notion of excitatory-inhibitory balance in cortical networks to derive scaling laws for connection strengths at large network size.  Previous theoretical work elucidates the computational utility of an inverse square root scaling law and recent experimental work provides biological evidence for it, but the mechanisms through which the law could emerge are unknown.  Candidate biological mechanisms will be analyzed mathematically and evaluated in collaboration with an experimental neuroscientist.  For the second project, the theory of neuronal networks with excitatory and inhibitory balance will be refined and extended to account for the imprecision of balance observed in experiments.  Preliminary calculations suggest that this imprecision is mathematically necessary, produces biologically realistic activity, and invokes intricate response properties that have implications for artificial neural networks used for image analysis.  The third project will develop and analyze a mathematical model of motor learning that combines Hebbian plasticity in a cortical pathway with reward modulated plasticity in a sub-cortical pathway, inspired by experimental observations.  This two-pathway motor learning algorithm has implications for biological motor learning, motor disease, and robotics."
"1735123","EXP: Automatically Synthesizing Valid, Personalized, Formative Assessments of CS1 Concepts","IIS","STEM + Computing (STEM+C) Part, Cyberlearn & Future Learn Tech","09/01/2017","05/12/2020","Amy Ko","WA","University of Washington","Standard Grant","Amy Baylor","08/31/2020","$620,027.00","Min Li","ajko@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","005Y, 8020","019Z, 8045, 8841, 9251","$0.00","Millions of people worldwide are trying to learn to code in schools, colleges, universities, and online. To do so successfully, they need significant practice and meaningful feedback that responds to their specific confusions and builds upon the knowledge they already have. Unfortunately, good practice content is challenging to create at scale and skilled teachers capable of providing meaningful feedback are rare and often inaccessible. Moreover, popular online learning technologies only provide a fixed amount of static content and do little to provide meaningful feedback. Because of this, many people give up learning to code, and only those with privileged access to friends, family, mentors, or teachers who can provide this support persist. This limits access to this critical 21st century literacy and ultimately harms the gender, racial, ethnic, and intellectual diversity of our computing workforce. <br/><br/>This project seeks to address part of this problem, applying advances in programming languages research that enable the creation of infinite amounts of diverse practice content, and advances in machine learning to build models of what learners do and do not know. By applying these two advances in computer science, the project will create a novel online learning technology that automatically generates assessment content, provides detailed immediate feedback about solutions, and uses assessment information of learners' performance to generate more personalized practice that individually targets concepts that learners are struggling to master. In creating such a system, new techniques for generating practice problems and new approaches to modeling learner knowledge of introductory programming concepts will be realized. The project will also explore students' current approach to practice, then deploy the new system into a large introductory classroom setting to experimentally measure its impact on learning and confidence. If the system is effective, the discoveries have the potential to provide learners in a range of settings more effective practice and learning. This may in turn improve both the competency and diversity of learners who further engage in computing education."
"1740102","Collaborative Research: SI2:SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/24/2017","Michael Sokoloff","OH","University of Cincinnati Main Campus","Standard Grant","Stefan Robila","08/31/2021","$224,621.00","","mike.sokoloff@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1253, 7244, 8004","7433, 8004, 8005","$0.00","In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  <br/><br/>The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed."
"1659983","SBIR Phase II:  Game-Based Learning for Organic Chemistry Using Mechanisms","IIP","SBIR Phase II","03/01/2017","08/26/2019","Julia Winter","MI","Alchemie Solutions, Inc.","Standard Grant","Rajesh Mehta","08/31/2020","$1,289,841.00","","julia@alchem.ie","4735 Walnut Lake Road","Bloomfield Hills","MI","483011328","2482275095","ENG","5373","079E, 165E, 169E, 5373, 8031, 8032, 8039","$0.00","This Small Business Innovation Research Phase II project answers the call that science students go beyond memorizing facts to understand content on a deeper, conceptual level. In chemistry, this goal is particularly difficult to achieve because the underlying concepts describe the behaviors of particles that are not directly observable to students. College instructors are also under added pressure to transform their teaching methods to help ensure student retention and success. In the subject area of organic chemistry, this transformation is even more important, due to the relatively high fail-rate in the course, especially for under-represented minorities and first generation students. The mobile learning tools and data collection platform in this project would help to solve both of these issues with an innovative method for intuitive learning and assessment which helps to make molecules and reactions come alive with game-based mobile applications. The game apps are playable by students of all ages, so the concepts of organic chemistry, as well as other science courses, become familiar and accessible as early as middle school. The broader vision is to open the pipeline for students to progress into STEM careers which have been difficult to reach in the past.<br/><br/>This project makes the theoretical touchable for organic chemistry students by building mobile game-based learning tools based on mechanisms, a key underlying concept used to teaching the course. This project will produce the Mechanisms suite of game apps, and bring an intuitive, tactile interface to learning chemistry. The research and development of this phase of the project will expand the user interaction model from Phase I to multiple modules of content for students. The data from the mobile learning tools will be synthesized with machine learning techniques to create an adaptive method to ensure the applications provide the appropriate level of challenge to the student learner. Clinical and longitudinal efficacy studies will be part of the research effort as the game modules are developed and released. The data platform will be optimized to integrate with multiple learning management systems and to be readily expandable to subjects beyond organic chemistry. The dashboard of the platform will allow both instructors and students to access the data and inform learning processes to achieve greater comprehension and success in the course. Commercialization will be achieved through direct-to-student downloads, subscriptions of the data platform by institutions, and licensing the technology to courseware providers."
"1722246","Statistical Methods for Discrete-Valued High-Dimensional Time Series with Applications to Neuroscience","DMS","OFFICE OF MULTIDISCIPLINARY AC, CI REUSE, CDS&E-MSS","08/01/2017","08/09/2017","Ali Shojaie","WA","University of Washington","Standard Grant","Christopher Stark","07/31/2021","$300,000.00","","ashojaie@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1253, 6892, 8069","1253, 8004, 8083, 8091, 9263","$0.00","The advent of high-dimensional time series from neuroscience, including EEG/MEG, fMRI and spike train data, has sparked a new interest in the analysis of multivariate time series data, particularly, to decipher the dynamics of brain connectivity networks. Despite significant recent progress, the vast majority of existing approaches for analyzing high-dimensional time series focus on real-valued time series from Gaussian noise and perturbation models. However, emerging applications in neuroscience involve discrete-valued time series, such as point processes and categorical observations. This project aims to develop flexible and scalable statistical machine learning methods and efficient software tools for inferring brain connectivity networks using discrete-valued high-dimensional time series data from neuroscience. <br/><br/><br/>Large-scale brain connectivity networks often involve complex nonlinear and multi-scale interactions that are usually unknown in practice.  Applications of parametric models in such settings may not provide an accurate window into the brain's dynamics, especially if the model assumptions are violated. This research bridges this gap by developing scalable statistical machine learning methods and theory for flexible nonparametric analysis of high-dimensional discrete-valued time series. In particular, this project will develop (i) clustering and variable screening methods for high-dimensional point processes, (ii) an efficient and general nonparametric estimation framework for network discovery from a general class of point processes, and (iii) a novel regularized estimation framework with provable identifiablity guarantees for network reconstruction from high-dimensional categorical time series. Theoretical properties of these methods will be investigated, and efficient open-source software tools will be developed to facilitate the application of the methods by the scientific community. Together, these tools provide a comprehensive framework for analysis of high-dimensional discrete values time series arising in various neuroscience applications, and will advance the current state of statistical machine learning methods for the analysis of high-dimensional time series. The PIs also plan to release the software developed as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the codebase of the software developed. This will allow a wider growth of the project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1903074","CHS: Medium: Collaborative Research: Social Learning in Mixed Human-Robot Groups for People with Disabilities","IIS","HCC-Human-Centered Computing","08/01/2017","11/29/2018","John Bricout","MN","University of Minnesota-Twin Cities","Continuing Grant","Ephraim Glinert","01/31/2019","$11,836.00","","jbricout@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7367","7367, 7924","$0.00","Assistive robots promise to improve the lives of many people with disabilities in the near future.  But whether due to traumatic spinal cord injury, early onset multiple sclerosis, or the common effects of advancing age, the variety of physical and mental disabilities, and the different psychological reaction of each individual to them, make it impossible to program one-size-fits-all behaviors for assistive robots.  To achieve its full potential, the assistive robot must learn to match the type and degree of assistance offered to the disability level and preferences of the user, as well as to the user's environment and the level of trust between the user and the robot.  Thus, training the robot to fit the individual user is essential - but requiring all users to train all aspects of robot behavior is unrealistic. In this collaborative project involving faculty at two institutions, the PIs argue that a possible solution may derive from the observation that whenever a user needs to train a robot for a new behavior, it is likely that there are other users with similar disabilities, preferences and environments who might also benefit from this behavior.  The PIs will develop techniques which enable the learning of behaviors in human+robot pairs, the identification of possible beneficiaries of the new behaviors, and the transfer of these behaviors to these beneficiaries (where transferring a behavior from one human+robot pair to another might involve the transfer of code and data for the robot and/or the transfer of skills to the human user).  This research will demonstrate how mixed human+robot interaction can alter the relationship between users and their environment, while also rendering physical interaction between robot and human safer and more efficient.  The work will have broad national impact because of the expected rapid growth in coming years of the elderly segment of the population.<br/><br/>The PIs will pursue four thrusts to achieve their vision.  They will design adaptive algorithms and controllers (e.g., for sliding-scale robot autonomy) which allow a robot to be an effective facilitator of user interaction with novel environments during activities of daily living (ADLs).  They will develop models of human+robot trust in the context of assistive robot technology, and examine the effect of trust on the user experience.  They will implement social agents through which the community of users with a specific disability via their social networks can help in the creation and adoption of new solutions for ADL tasks.  And they will validate the ability of human+robot exchanges to increase functionality and performance of ADLs for disabled individuals.  The research will build on recent advances in robot control, psychological models of social learning, and models of social networks, as well as machine learning techniques of collaborative filtering and recommendation.  Project outcomes will include the creation of social agents that can interact on behalf of the user, discover learning opportunities, and actively participate in the transfer of learning.  The work will contribute to our understanding of how users can partner, both individually and collectively, with assistive robots, and will answer open questions relating to the interoperability and intelligibility of knowledge developed in one learning system to another."
"1718901","NeTS: Small: Learning-Guided Network Resource Allocation: A Closed-Loop Approach","CNS","Networking Technology and Syst","09/01/2017","09/06/2018","Xin Liu","CA","University of California-Davis","Standard Grant","Alexander Sprintson","08/31/2021","$479,000.00","Huasen Wu","liu@cs.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7363","7923","$0.00","Based on network measurement and user behavior data, much recent work has studied the modeling and prediction of network utility and user experience using machine learning techniques. While it provides important insights, prediction itself is often not the ultimate goal in networks. Ideally, a network could identify users with poor experience and take proper actions to proactively improve the overall performance. To achieve this goal, the project advocates a closed-loop approach that uses learning-aided utility model to explicitly guide resource allocation in networks and uses feedback to (in)validate and improve the learned utility model. This investigation provides important insights in understanding, designing, and analyzing learning-model-aided resource optimization algorithms. Furthermore, because of its generality, this closed-loop approach can be applied in other systems with the following characteristics: 1) the system is too complex to rely on domain knowledge only to build a white-box utility model; 2) there exists sufficient data so that a utility model can be learned; 3) to maximize the overall utility, one can optimize over certain control variables that affect the utility value; and 4) there exists a feedback loop so that the effect of the control can be observed. The outcome of the project can be applied to such systems in different disciplines. <br/><br/>Utilizing this proposed framework is highly challenging due to the unknown and noisy nature of the network utility function, and in the context of high dimensionality, coupled resource constraints, and non-convex optimization. To address these challenges, the project considers two complementary approaches: a greedy approach and an integrated approach. The greedy approach has much flexibility in applying diverse learning models, which may fit different application scenarios better in practice, but is difficult to analyze. The integrated approach builds upon Gaussian Process (GP) bandits that integrate both the constructed model and model uncertainty in resource allocation decisions. This approach is more amenable to theoretical analysis, although highly challenging. In both approaches, one needs to optimally allocate resource based on the learned models. The contribution of the project comes from solving the corresponding non-convex optimization problems. The last step is to use the closed-loop feedback to build a better or optimal utility model. The integrated approach aims to develop hierarchical GP bandit algorithms for dimensionality reduction, ideally with theoretical performance guarantees. The greedy approach leverages perturbed-exploration schemes for general learning models and strives for practicality and generality."
"1657002","Coding Science Internships: Authentic Learning Experiences to Support Students' Science and Programming Practices and Broaden Participation in Computer Science","DRL","STEM + Computing (STEM+C) Part, ITEST-Inov Tech Exp Stu & Teac","06/01/2017","05/22/2019","Eric Greenwald","CA","University of California-Berkeley","Continuing Grant","Chia Shen","05/31/2021","$1,151,234.00","Ari Krakowski","eric.greenwald@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","EHR","005Y, 7227","1544, 8212","$0.00","This project will advance efforts of the Innovative Technology Experiences for Students and Teachers (ITEST) program to better understand and promote practices that increase students' motivations and capacities to pursue careers in fields of science, technology, engineering, or mathematics (STEM) by developing, broadly implementing, and systematically investigating two 10-day computer programming instructional sequences.  The new instructional sequences will integrate computer science and science learning experiences through simulated internships for core middle school science classrooms, and are designed to increase student dispositions toward, and capacity for, computer programming and computational thinking. Ultimately, the intervention is designed to support broader participation in computer science (CS) fields of study and careers, with particular emphasis on females. This project seeks to accomplish this goal by: (1) immersing up to 4000 11-14 year old youth (Grades 6-8) students in simulated internships that mirror the collaborative and computational work of practicing scientists, and that can be embedded within a school's core science curriculum; (2) offering a more inclusive model of computer science work that can expand students' perception of the nature and value of computer programming and encourage a broader range of students, and females in particular, to identify as possible programmers; (3) gathering evidence that can advance and deepen the field's understanding of how students' computer science knowledge and practices develop within the context of science learning experiences; and 4) identifying specific factors, designs, and practices likely to engage students in CS, improve student dispositions toward STEM and CS-related occupations, and that are likely to improve the capacity of teachers and districts to support CS education. The project extends prior work aimed at incorporating coding and computational thinking into the school in the following ways: an explicit focus on collaborative discourse and collaborative problem solving, including that within digitally-mediated discussion forums; backend data logging of student interactions within the simulated internships' digital environments in order to analyze how student understanding develops at the intersection of science and computer science; just-in-time teacher learning via an educative curriculum to support system capacity and broader impact; and a research and development model that explicitly incorporates school, district, and state policy level stakeholders in the design process, in order to build an understanding of how the intervention, and those like it, can be successfully and sustainably implemented. The project is also supported by the STEM+Computing program (STEM+C) to advance research on how students' computer science knowledge and practices develop within the context of science learning experiences and improve student dispositions toward STEM and CS-related occupations.<br/><br/>The mixed-methods research agenda for this project will be guided by four questions: 1) What specific design features and instructional strategies of the CS Internships are most important for broadening student participation in CS?; 2) What aspects of the CS Internships are most important to support sustainability of CS and science integration?; 3) What factors, design features, and practices are most important for supporting productive student engagement in, and teacher facilitation of, collaboration and discourse (both in-person and digital) in STEM?; and 4) What aspects of student understanding may be revealed when students are able to manipulate the code behind scientific models? In Year 1, the project will pilot and iteratively develop the first of two Coding Science Internships, with methodology grounded in design-based implementation research. Research activities will include observations of piloted lessons and teacher interviews. Also in Year 1, the project will also begin to examine how students develop computational thinking and computer programming practices, through cognitive interviews with students in pilot classrooms. In Year 2, the project will begin broad implementation and systematic investigation of the internship developed in Year 1, and begin iteratively developing the second internship, which will be broadly implemented in Year 3. Research activities related to broad implementation in Years 2 & 3 will include pre-post measures for students (including scales measuring disposition toward CS, and CS and science practices), and teachers (including scales measuring science and technological pedagogical content knowledge and competency beliefs for CS instruction); daily engagement surveys for students; and daily intervention feasibility and perceived value surveys for teachers. In addition to analyses of variance and covariance, the project will employ mediation analysis to examine interactions among key variables contributing to any observed learning gains. Years 2 & 3 will also feature extensive capture of student data generated through interactions with the digital resources (e.g, ""clickstream"" and metadata, submission data and discourse within the digital discussion forums). Learning analytics methods (including machine learning, Bayesian network modeling, and Latent Dirichlet Allocation) applied to these massive data sets will be aimed at providing more subtle insight into student development of computational thinking as it applies to science, and possible learning trajectories for the integrated development of computer science and science practices."
"1734190","NRI: INT: COLLAB: Integrated Modeling and Learning for Robust Grasping and Dexterous Manipulation with Adaptive Hands","IIS","NRI-National Robotics Initiati","09/01/2017","07/27/2017","Aaron Dollar","CT","Yale University","Standard Grant","James Donlon","08/31/2021","$632,500.00","","aaron.dollar@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","8013","8086","$0.00","Robots need to effectively interact with a large variety of objects<br/>that appear in warehouses and factories as well as homes and offices.<br/>This requires robust grasping and dexterous manipulation of everyday<br/>objects through low cost robots and low complexity solutions.<br/>Traditionally, robots use rigid hands and analytical models for such<br/>tasks, which often fail in the presence of even small errors. New<br/>compliant hands promise improved performance, while minimizing<br/>complexity, and increased robustness. Nevertheless, they are<br/>inherently difficult to sense and model. This project combines ideas<br/>from different robotics sub-fields to address this limitation. It<br/>utilizes progress in machine learning and builds on a strong tradition<br/>in robot modeling. The objective is to provide adaptive, compliant<br/>robots that are better in grasping objects in the presence of multiple<br/>unknown contact points and sliding or rolling objects in-hand. The<br/>broader impact will be strengthened by the open release of new or<br/>modified robot hand designs, improved control algorithms and software,<br/>as well as corresponding data sets. Furthermore, academic<br/>dissemination will be accompanied by educational outreach to<br/>undergraduate and high school students.<br/><br/>Towards the above objective, the first step will be the definition of<br/>new hybrid models appropriate for adaptive, compliant hands.  This<br/>will happen by improving analytical solutions and extending them to<br/>allow adaptation based on data via novel, time-efficient learning<br/>methods. The objective is to capture model uncertainty inherent in<br/>real-world interactions; a process that suffers from data scarcity.<br/>In order to reduce the amount of data required for learning, different<br/>models will be tailored to specific tasks through an automated<br/>discovery of these tasks and of underlying motion primitives for each<br/>one of them. This task identification process will operate iteratively<br/>with learning and utilize improved models to discover new tasks. It<br/>can also provide feedback for improved hand design. Once these<br/>learning-based and task-focused models are available, they will be<br/>used to learn and synthesize controllers for grasping and in-hand<br/>manipulation. To learn controllers, this work will consider a<br/>model-based, reinforcement learning approach, which will be evaluated<br/>against alternatives. For controller synthesis, existing tools for<br/>this purpose will be integrated with task planning primitives and<br/>extended through learning processes to identify the preconditions<br/>under which different controllers can be chained together. The project<br/>involves extensive evaluation on a variety of novel adaptive hands and<br/>robotic arms designed in the PIs' labs. Modern vision-based solutions<br/>will be used to track grasped objects and provide feedback for<br/>learning and closed-loop control.  The evaluation will measure whether<br/>the developed hybrid models can significantly improve robustness of<br/>grasping and the effectiveness of dexterous manipulation."
"1717362","PFI:BIC - Development, Deployment and Evaluation of an Intelligent Service System for Personalized Early Literacy Learning Using Mobile Devices","IIP","PFI-Partnrships for Innovation","08/01/2017","02/06/2020","Cynthia Breazeal","MA","Massachusetts Institute of Technology","Standard Grant","Jesus Soriano Molla","07/31/2021","$999,817.00","Robin Morris, Hae Won Park","cynthiab@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","1662","1662","$0.00","Early learning is a key time to intervene for subsequent educational and economic achievement. Research shows that reading levels in early primary years can predict high school graduation rates. Effective Pre-K educational intervention programs are recognized as an effective return on investment (ROI) for society via improved employment opportunities, lower incarceration rates, and decreased need for social services. The scalability and ease-of-deployment of smart devices and other digital learning technologies has transformative potential to increase Kindergarten readiness outcomes that is widely accessible and affordable. Especially for children who cannot access quality pre-K education. However, modern digital early educational tools do not possess AI and machine learning algorithms for continuous assessment and modeling of children?s literacy and and oral language skills, nor are they capable of personalization to meet the diverse needs of individual children. The goal of this proposed research is to bridge this gap between the need for personalized interventions for early childhood learning with the ubiquity and affordability of mobile devices. The research team proposes to develop, deploy, and evaluate a system capable of automatic and continuous assessment, personalized content adaptation, and AI-powered educational mentorship to deliver high quality, scalable, and affordable early literacy interventions to young children.<br/><br/><br/>The proposed research will develop a smart system for personalized early learning on mobile devices, by developing three innovative technology components: 1. Embedded student assessment algorithms, that can estimate reading skill from diverse, multimodal data sources including audio, facial expression, and real-time gameplay/usage data. 2. Personalized mentoring algorithms that adapt educational content to individual patterns of behavior of app usage to optimize for individual learning outcomes. 3. Affect-aware content adaptation and personalization algorithms to optimize engagement and learning. These technologies shall be embedded within two novel interactive educational apps. The first is for storytelling and vocabulary development. The second is for reading.  Both use general assessment data, real-time audio, and facial expression recognition to estimate performance and to optimize motivation, engagement, and learning. These technologies will be embedded within industry partner Curious Learning's mobile software platform, which currently delivers early learning software for children to promote early literacy readiness of US kindergarten children. The project will evaluate the ability of these technologies to assess reading skill, foster vocabulary and oral language, and to improve engagement and learning by personalizing the Curious Learning system's educational content to children.  "
"1740765","Collaborative Research: Community-Building and Infrastructure Design for Data-Intensive Research in Computer Science Education","DRL","ECR-EHR Core Research","09/01/2017","08/15/2017","Clifford Shaffer","VA","Virginia Polytechnic Institute and State University","Standard Grant","Wu He","02/28/2021","$268,941.00","Stephen Edwards","shaffer@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","EHR","7980","7433, 8083","$0.00","The Building Community and Capacity in Data Intensive Research in Education program seeks to enable research communities to develop visions, teams, and capabilities dedicated to creating new, large-scale, next-generation data resources and relevant analytic techniques to advance fundamental research for areas of research covered by the Education and Human Resources Directorate. Successful proposals will outline activities that will have significant impacts across multiple fields by enabling new types of data-intensive research. Online educational systems, and the large-scale data streams that they generate, have the potential to transform education as well as our scientific understanding of learning. Computer Science Education (CSE) researchers are increasingly making use of large collections of data generated by the click streams coming from eTextbooks, interactive programming environments, and other smart content. However, CSE research faces barriers that slow progress: 1) Collection of computer science learning process and outcome data generated by one system is not compatible with that from other systems. 2) Computer science problem solving and learning (e.g., open-ended coding solutions to complex problems) is quite different from the type of data (e.g., discrete answers to questions or verbal responses) that current educational data mining focuses on. This project will build community and capacity among CSE researchers, data scientists, and learning scientists toward reducing these barriers and facilitating the full potential of data-intensive research on learning and improving computer science education. The project will bring together CSE tool building communities with learning science and technology researchers towards developing a software infrastructure that supports scaled and sustainable data-intensive research in CSE that contributes to basic science of human learning of complex problem solving. The project will support community-building and infrastructure capacity-building whose ultimate goal is to develop and disseminate infrastructure that facilitates three aspects of CSE research: (1) development and broader re-use of innovative learning content that is instrumented for rich data collection, (2) formats and tools for analysis of learner data, and (3) best practices to make large collections of learner data and associated analytics available to researchers in CSE, data science, or learning science. To achieve these goals, a large community of researchers will be engaged to define, develop, and use critical elements of this infrastructure toward addressing specific data-intensive research questions.The project will host workshops, meetings, and online forums leveraging existing communities and building new capacities toward significant research outcomes and lasting infrastructure support.<br/><br/>This project will provide an infrastructure that can support various kinds of research in CSE domain as a one-stop-shop, and will be the first to focus on full-cycle educational research infrastructure in any domain. CSE tool developers and educators will become more productive at creating and integrating advanced technologies and novel analytics. Learning researchers will have better tools for analyzing the huge amounts of learner data that modern digital education software produces. Data scientists will have rich new datasets in which to explore new machine learning and statistical techniques. Collectively, these efforts will reduce barriers to educational innovation and support scientific discoveries about the nature of complex learning and how best to enhance it. The project will support scientific investigations through community meetings and mini-grants to others addressing questions such as: What is the optimal ratio of solution examples and problem-solving practice? How do computational thinking skills emerge? In what quanta are programming skills acquired? Can automated tutoring of programming be effective at scale in enhancing student learning?. Many of the innovations developed under this project will directly impact learning in any discipline. Educational software will more quickly be developed in the future, that more easily generates meaningful learner data, which in turn can be more easily analyzed."
"1661074","Collaborative research: Neural and cognitive strengthening of conceptual knowledge and reasoning in classroom-based spatial education","DRL","ECR-EHR Core Research","04/15/2017","04/05/2019","Robert Kolvoord","VA","James Madison University","Continuing Grant","Gregg Solomon","03/31/2021","$151,567.00","","kolvoora@jmu.edu","MSC 5728","HARRISONBURG","VA","228077000","5405686872","EHR","7980","1544, 8089, 8091, 8212, 8817","$0.00","Spatial thinking is a powerful driver of success in the STEM classroom and spatial thinking is a major predictor of future STEM success in the workforce. The brain systems that support spatial thinking have been well mapped by neuroscience to allow clear interpretation of new brain-imaging data. Recent advances in tools used to analyze brain activity allow detection of changes in the brains of students that signify accurate learning of STEM concepts. This advance may open a window onto biomarkers of precisely the type of learning that is the goal of educators. Using these new brain analysis methods, this project, a collaboration involving researchers from James Madison University, Georgetown University, Northwestern University, and Dartmouth College, will investigate how changes in the spatial thinking network support learning of specific STEM concepts, and how changes in the classroom can facilitate changes in the brain related to spatial thinking. This cross-disciplinary project brings together experts in geoscience classroom education, spatial cognition, and the neural bases of learning and reasoning. This team is committed to bridging the conspicuous gap between the laboratory and the high school classroom. A confluence of advances in neuroimaging, and the research team's partnership with Virginia school systems make this effort timely and tractable. Identifying possible effects of sex and STEM-related anxieties on conceptual learning in the brain, and testing the effectiveness of spatial education for reducing disparities, this research will point to critical targets for intervention. The project is funded by the EHR Core Research (ECR) program, which supports work that advances the fundamental research literature on STEM learning.<br/><br/>This project seeks to understand the neural mechanisms of spatial learning, to advance of spatial education, and to identify factors that affect disparities in STEM learning and participation. The research team will collect functional magnetic resonance imaging (fMRI) and behavioral data from students before and after learning in a high school geoscience course that uses a novel spatially-based curriculum to teach STEM concepts and spatial reasoning. Pilot data on this spatial curriculum have begun to characterize the underlying cognitive and neural mechanisms at work, and show promising effects of transfer to STEM problem solving and core measures of spatial ability. Consistent with methods that have demonstrated success in the lab (but not yet the classroom), the research team will use multivariate neural representations of a group of highly experienced and specially trained teachers as an expert standard to determine neural markers of students? conceptual knowledge and spatial reasoning. Leveraging recent multivariate pattern analysis (MVPA) and machine-learning advances in brain imaging, the team will compare the neural patterns of students before and after learning to test for a trajectory that moves students closer to expert representations. This project will also test, for the first time, whether it is possible to compare different curricula based on how much they strengthen the representation of a concept in the brain. Similarly, this work will test whether spatial education leads students to engage spatial brain resources for STEM-related reasoning, and seek to compare curricula on this basis. The project will test whether neural data add predictive value to traditional testing (e.g. conventional unit tests) for subsequent retention of conceptual knowledge and spatial reasoning. Assessments of STEM-related anxieties (e.g., math and spatial anxiety) and analyses of sex-related effects on cognitive and neural outcomes will newly characterize factors that influence disparities in STEM learning and participation."
"1661089","Collaborative research: Neural and cognitive strengthening of conceptual knowledge and reasoning in classroom-based spatial education","DRL","ECR-EHR Core Research","04/15/2017","04/05/2019","David Uttal","IL","Northwestern University","Continuing Grant","Gregg Solomon","03/31/2020","$193,111.00","","duttal@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","EHR","7980","1544, 8089, 8091, 8212, 8817","$0.00","Spatial thinking is a powerful driver of success in the STEM classroom and spatial thinking is a major predictor of future STEM success in the workforce. The brain systems that support spatial thinking have been well mapped by neuroscience to allow clear interpretation of new brain-imaging data. Recent advances in tools used to analyze brain activity allow detection of changes in the brains of students that signify accurate learning of STEM concepts. This advance may open a window onto biomarkers of precisely the type of learning that is the goal of educators. Using these new brain analysis methods, this project, a collaboration involving researchers from James Madison University, Georgetown University, Northwestern University, and Dartmouth College, will investigate how changes in the spatial thinking network support learning of specific STEM concepts, and how changes in the classroom can facilitate changes in the brain related to spatial thinking. This cross-disciplinary project brings together experts in geoscience classroom education, spatial cognition, and the neural bases of learning and reasoning. This team is committed to bridging the conspicuous gap between the laboratory and the high school classroom. A confluence of advances in neuroimaging, and the research team's partnership with Virginia school systems make this effort timely and tractable. Identifying possible effects of sex and STEM-related anxieties on conceptual learning in the brain, and testing the effectiveness of spatial education for reducing disparities, this research will point to critical targets for intervention. The project is funded by the EHR Core Research (ECR) program, which supports work that advances the fundamental research literature on STEM learning.<br/><br/>This project seeks to understand the neural mechanisms of spatial learning, to advance of spatial education, and to identify factors that affect disparities in STEM learning and participation. The research team will collect functional magnetic resonance imaging (fMRI) and behavioral data from students before and after learning in a high school geoscience course that uses a novel spatially-based curriculum to teach STEM concepts and spatial reasoning. Pilot data on this spatial curriculum have begun to characterize the underlying cognitive and neural mechanisms at work, and show promising effects of transfer to STEM problem solving and core measures of spatial ability. Consistent with methods that have demonstrated success in the lab (but not yet the classroom), the research team will use multivariate neural representations of a group of highly experienced and specially trained teachers as an expert standard to determine neural markers of students? conceptual knowledge and spatial reasoning. Leveraging recent multivariate pattern analysis (MVPA) and machine-learning advances in brain imaging, the team will compare the neural patterns of students before and after learning to test for a trajectory that moves students closer to expert representations. This project will also test, for the first time, whether it is possible to compare different curricula based on how much they strengthen the representation of a concept in the brain. Similarly, this work will test whether spatial education leads students to engage spatial brain resources for STEM-related reasoning, and seek to compare curricula on this basis. The project will test whether neural data add predictive value to traditional testing (e.g. conventional unit tests) for subsequent retention of conceptual knowledge and spatial reasoning. Assessments of STEM-related anxieties (e.g., math and spatial anxiety) and analyses of sex-related effects on cognitive and neural outcomes will newly characterize factors that influence disparities in STEM learning and participation."
"1661065","Collaborative research: Neural and cognitive strengthening of conceptual knowledge and reasoning in classroom-based spatial education","DRL","ECR-EHR Core Research","04/15/2017","04/10/2019","Adam Green","DC","Georgetown University","Continuing Grant","Gregg Solomon","03/31/2021","$829,865.00","","aeg58@Georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","EHR","7980","1544, 8089, 8091, 8212, 8817","$0.00","Spatial thinking is a powerful driver of success in the STEM classroom and spatial thinking is a major predictor of future STEM success in the workforce. The brain systems that support spatial thinking have been well mapped by neuroscience to allow clear interpretation of new brain-imaging data. Recent advances in tools used to analyze brain activity allow detection of changes in the brains of students that signify accurate learning of STEM concepts. This advance may open a window onto biomarkers of precisely the type of learning that is the goal of educators. Using these new brain analysis methods, this project, a collaboration involving researchers from James Madison University, Georgetown University, Northwestern University, and Dartmouth College, will investigate how changes in the spatial thinking network support learning of specific STEM concepts, and how changes in the classroom can facilitate changes in the brain related to spatial thinking. This cross-disciplinary project brings together experts in geoscience classroom education, spatial cognition, and the neural bases of learning and reasoning. This team is committed to bridging the conspicuous gap between the laboratory and the high school classroom. A confluence of advances in neuroimaging, and the research team's partnership with Virginia school systems make this effort timely and tractable. Identifying possible effects of sex and STEM-related anxieties on conceptual learning in the brain, and testing the effectiveness of spatial education for reducing disparities, this research will point to critical targets for intervention. The project is funded by the EHR Core Research (ECR) program, which supports work that advances the fundamental research literature on STEM learning.<br/><br/>This project seeks to understand the neural mechanisms of spatial learning, to advance of spatial education, and to identify factors that affect disparities in STEM learning and participation. The research team will collect functional magnetic resonance imaging (fMRI) and behavioral data from students before and after learning in a high school geoscience course that uses a novel spatially-based curriculum to teach STEM concepts and spatial reasoning. Pilot data on this spatial curriculum have begun to characterize the underlying cognitive and neural mechanisms at work, and show promising effects of transfer to STEM problem solving and core measures of spatial ability. Consistent with methods that have demonstrated success in the lab (but not yet the classroom), the research team will use multivariate neural representations of a group of highly experienced and specially trained teachers as an expert standard to determine neural markers of students? conceptual knowledge and spatial reasoning. Leveraging recent multivariate pattern analysis (MVPA) and machine-learning advances in brain imaging, the team will compare the neural patterns of students before and after learning to test for a trajectory that moves students closer to expert representations. This project will also test, for the first time, whether it is possible to compare different curricula based on how much they strengthen the representation of a concept in the brain. Similarly, this work will test whether spatial education leads students to engage spatial brain resources for STEM-related reasoning, and seek to compare curricula on this basis. The project will test whether neural data add predictive value to traditional testing (e.g. conventional unit tests) for subsequent retention of conceptual knowledge and spatial reasoning. Assessments of STEM-related anxieties (e.g., math and spatial anxiety) and analyses of sex-related effects on cognitive and neural outcomes will newly characterize factors that influence disparities in STEM learning and participation."
"1719635","Conference on Nonconvex Statistical Learning, University of Southern California, May 26-27, 2017","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","04/15/2017","12/30/2016","Jong-Shi Pang","CA","University of Southern California","Standard Grant","Leland Jameson","03/31/2018","$15,000.00","Phebe Vayanos, Meisam Razaviyayn","jongship@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","006y, 1271, 8069","7556, 9263","$0.00","The two-day interdisciplinary Conference on Nonconvex Statistical Learning takes place at the campus of the University of Southern California on Friday, May 26, and Saturday, May 27, 2017. The website of the conference: https://sites.google.com/a/usc.edu/cnsl2017/home will be continuously updated prior to the conference and will provide a repository for the lectures of the meeting to be made available generally.  In today's digital world, huge amounts of data, i.e., big data, can be found in almost every aspect of scientific research and every walk of human activities.  These data need to be managed effectively for reliable prediction, inference, and improved decision making.  Statistical learning is an emergent scientific discipline wherein mathematical modeling, computational algorithms, and statistical analysis are jointly employed to address such a data management problem.  The aim of the conference is to bring together researchers at all levels from multiple disciplines, including computational and applied mathematics, optimization, statistics, and engineering to report on the state of the art of the conference subject and exchange ideas for its further development. Collaborations among the participants will be fostered with the goal of advancing the science of the field of statistical learning and promoting the interfaces of the involved disciplines. The format of the conference consists of roughly two dozen lectures given by expert researchers of the field.  Break times in-between the lectures are scheduled to allow discussions among all participants who will include graduate students, postdoctoral fellows, researchers in academia and industry, and faculty members in universities. This award provides support targeted for the travel expenses of junior participants. <br/> <br/>Till now, convex optimization has been a principal venue for solving many problems in statistical learning.  Yet there is increasing evidence supporting the use of nonconvex formulations to enhance the realism of the models and improve their generalizations.  Superior results and new advances have occurred in areas such as computational statistics, compressed sensing, imaging science, machine learning, bio-informatics and portfolio selection in which nonconvex functionals are employed to express model loss, promote sparsity, and enhance robustness. This conference provides a forum for the participants to report on their research and exchange ideas pertaining to the use of nonconvex functionals in statistical learning.  The topics are organized in four main streams: modeling, advances in computation, big-data statistical learning, and innovative applications.  The lectures will cover both theory and algorithms as well as promising directions for further research."
"1748614","Workshop: Mobilizing a Global Science of Learning to Address Future Challenges; Alexandria, VA;  2018","SMA","","09/01/2017","08/25/2017","Andrea Chiba","CA","University of California-San Diego","Standard Grant","Soo-Siang Lim","02/28/2019","$49,883.00","","chiba@cogsci.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","SBE","004y","059Z, 7556","$0.00","A primary purpose of this workshop proposal is to leverage the strengths of global scientific communities with substantial expertise in human, animal, neuromorphic, and machine learning. It seeks to leverage NSF investments in Science of Learning to develop a network of US and international researchers who are committed to interdisciplinary and integrative research in the study of learning and its implications for societal challenges in education, technology innovation and workforce preparation. The workshop will purposefully mobilize and engage US and international partners to identify shared interests and needs (expertise, infrastructure/organization) that can create new research opportunities with meaningful international experiences for students and faculty.  Funds will be used for planning and coordination activities to culminate in a consensus building workshop with the following goals: 1) Identify research priorities (and/or associated societal challenges) for which networks of scientists are poised to work together; 2) identify and build a community infrastructure that allows teams to quickly re-configure in response to research and societal challenges/opportunities; 3) identify ways to address learning in the growing divide between societies with and without science and technology resources, while maintaining important cultural features; and 4) Co-convene a International Science of Learning Summit meeting in tandem with the Science of Learning Awardees meeting to identify priorities in training needs and knowledge domains for science of learning.  <br/><br/>The successful fostering of a global community in the science of learning enables access to a large expertise base, and accelerated sharing and dissemination of knowledge across a broad spectrum of disciplines.  It builds capacity of interdisciplinary groups to collaborate, and strengthens and builds new partnerships for US researchers to work with international partners.  This in turn, enables greater fluidity and cooperation for student exchange and training in international laboratories"
"1718944","RI: Small: Integrative, Semantic-Aware, Speech-Driven Models for Believable Conversational Agents with Meaningful Behaviors","IIS","Robust Intelligence","09/01/2017","03/23/2020","Carlos Busso","TX","University of Texas at Dallas","Standard Grant","Tatiana Korelsky","08/31/2021","$518,116.00","","busso@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","7495, 7923, 8089, 9251","$0.00","This project will analyze, model and synthesize human behaviors to create a believable Conversational Agent (CA). A CA is a virtual agent that interacts with a user, displaying human-like behaviors not only through speech but also through facial expressions and head movements. Replicating or representing human behavior includes generating gestures that are synchronized with speech, convey appropriate meaning in the message, and respond to the behaviors displayed by the user. An appealing approach to synthesize human-like behaviors is the use of data-driven methods, which have the potential of capturing naturalistic variations of the behaviors. Modeling the dependencies between speech and gestures brings insights about verbal and nonverbal communication, underlying the production and coordination mechanisms used during natural human interactions. CAs can be used in a variety of health care applications, such as helping hearing impaired individuals and teaching social skills to autistic children. Tutoring systems that display human-like behaviors to communicate and acknowledge active listening will engage better with the students, helping them in their learning. The project promises a fertile ground for interdisciplinary training of graduate and undergraduate students. The models will be evaluated with an assistive agent (CA or embodied robot) interacting with UT Dallas students, serving as a platform to reach out students from all majors, especially woman and underrepresented minorities.<br/><br/>The project will take an integrative, cross-disciplinary approach to generate believable and meaningful behaviors by exploring the intrinsic relation between speech, head motion, and facial expressions, constrained by important aspects of spoken language. The planned research leverages some of the latest developments in the field of deep learning in an integrative fashion, pulling together acoustic features and semantic language structure, to build models that are able to account for the correlation between various facial and head movements. The speech-driven approach will capture the variability of human behavior in a manner that is not easily possible with rule-based approaches. Dialog acts and emotions will be inferred and used to constrain the speech driven models, capturing the relation between high-level conversational functions and facial gestures. The project will offer novel, principled methods to generate behaviors driven by synthesized speech, opening new application domain when only text is available. The approach will capture the acoustic variability in synthesized speech, while maintaining the temporal dependency between gestures and speech. The project will also explore schemes to modify the behaviors of the user by displaying carefully designed gestures generated with our data-driven framework. By tracking the behaviors of the user, the system will provide appropriate responses, closing the loop in the interaction."
"1744226","I-Corps: Combining High-throughput Automation and Organism Design Software","IIP","I-Corps","07/01/2017","07/12/2017","David Bernick","CA","University of California-Santa Cruz","Standard Grant","Anita La Salle","12/31/2018","$50,000.00","","dbernick@soe.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is the expansion and  development of the field of synthetic biology, with a number of possible applications in untapped markets. By applying research outcomes from synthetic biology, robotic automation, and machine learning, the designing and engineering of microorganisms opens the door to a realm of possibilities in chemical synthesis that never existed in the past. This work will allow us to build nano-scale factories, applying the experimental field of synthetic biology to the real-world demands of pharmaceutical companies, allowing them to bridge the gap between disease and cure, faster and cheaper than ever before. By industrializing the process of designing biological systems, it is possible to build a system that can automate innovation at the nano-scale level. This will focus research on developing new approaches, with a majority of the experimentation performed by robotic laboratories. <br/><br/>This I-Corps project automates the design and engineering of microorganisms for pharmaceutical production, by bringing together recent advances in the fields of synthetic biology, robotic automation, and machine learning. The approach translates innovations in these fields to form hypotheses that are tested using high-throughput automation to develop methods to predict novel function of enzymes and other biological parts. This will allow clients to produce compounds faster and more efficiently, freeing resources for development of new products.  Preliminary studies have been in the engineering of microorganisms for production of an alternative sugar. By coupling machine learning with high-throughput screening, testing, and experimentation, the approach will enable automated discovery, testing, and validation."
"1741619","I-Corps: Advanced Energy Data Analytics, Visualization, and Forecasting Platform for Energy Decision-Makers","IIP","I-Corps","06/15/2017","05/03/2017","Marilyn Brown","GA","Georgia Tech Research Corporation","Standard Grant","Nancy Kamei","11/30/2018","$50,000.00","","marilyn.brown@pubpolicy.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project includes developing an analytical platform is capable of empowering customers with big-data analytical capacity, allow them to design and test clean energy ideas on their own terms at a fast pace using evidence-based machine-learning-enabled algorithms, and to enhance their capability to communicate with stakeholders. By adopting the proposed technology, its users could reduce energy consumption, save financial and physical resources, and minimize environmental impact. The predictive feature of the technology could also allow its users to develop a long-term view about their individual purchasing behavior as well as collective planning actions, and make financially wise and environmentally friendly decisions regarding home appliance purchases, commercial technology deployment, as well as city and regional planning.<br/><br/>This I-Corps project aims to explore the commercial viability of an energy analytics platform that combines big-data management platforms, electricity financial models, machine-learning-enabled data-driven analytical tools, and a 3D visualization interface. The platform is pre-loaded with historical data that allows customers to benchmark their past electricity use patterns. The historical data will also serve as an input to customizable machine-learning algorithms that provide customers the power and flexibility to design evidence-driven actions that fit their electricity consumption needs and forecast the economic, environmental, and social impacts related to the actions in the mid- to long-term. Users of the technology will be able to track and assess their electricity use and expenditures, and any associated capital investments. The technology will also allow its users to see the social and macro-economic impact of their energy-related action by producing outputs on public health, job creation and local GDP growth."
"1737943","ATD: Multimode Machine Learning and Deep GeoNetworks for Anomaly Detection","DMS","ATD-Algorithms for Threat Dete, , , ","08/01/2017","02/04/2019","Thomas Strohmer","CA","University of California-Davis","Standard Grant","Leland Jameson","03/31/2021","$415,536.00","","strohmer@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","046Y, P412, P413, P414","6877","$0.00","This research effort creates mathematical concepts and numerical methods for the analysis of spatiotemporal datasets with a special emphasis on anomaly detection.  Early and accurate detection of unusual events and forecasts of future threats are critical in designing an effective response to them. Current algorithms for threat detection are often unable to keep up with the numerous demands, changing environments, and the huge amounts of spatiotemporal data that need to be processed and analyzed to accomplish these tasks.  Uncertainty, scale, non-stationarity, noise, and heterogeneity are fundamental issues impeding progress at all phases of the pipeline that creates knowledge from data.  The goal of this research effort is to develop novel mathematical concepts and computational methods that can detect anomalies in heterogenous, large-scale, spatiotemporal datasets.  Beyond the project's broad technological impact, it serves as a model for the kind of cross-disciplinary activity critical for research and education at the mathematics/engineering frontier.<br/><br/>The PI will devise efficient, robust, and scalable algorithms for unsupervised and semi-supervised learning.  In particular, the PI will focus on the development of two approaches: (i) A multimodal diffusion framework for unsupervised prediction of anomalies from spatiotemporal data. Here, multimode refers to the fact that data may have different modalities, such as text, images, geolocations, etc. (ii) A scalable framework for semi-supervised learning on graph-structured data, based on the aforementioned multimodal diffusion framework and on a novel variant of deep convolutional networks specifically designed to operate on spatiotemporal data.  The expected success of this project is based on existing achievements by the investigator in developing advanced mathematical concepts and turning them into real-world applications."
"1722801","SCH: INT: Data-In-Motion Prediction and Assessment of Acute Respiratory Distress Syndrome","IIS","Information Technology Researc, Special Projects - CCF, Smart and Connected Health","09/01/2017","09/05/2017","Kayvan Najarian","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Wendy Nilsen","08/31/2021","$1,299,371.00","Harm Derksen, Michael Sjoding, Theodore Iwashyna","kayvan@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1640, 2878, 8018","1640, 8018, 8062","$0.00","The goal of this project is to develop new computational approaches for synthesizing streams of real-time electronic health data for health monitoring and early disease detection. The team will utilize these technologies to address the problem of monitoring patients with lung disease to identify Acute Respiratory Distress Syndrome (ARDS). ARDS is an ideal problem, because it is frequently missed by clinicians with wide-ranging consequences to patients. The project will develop two emerging concepts in machine learning, learning with privileged information and uncertainty, both of which have relevance in healthcare. It will also develop new approaches for integrating different data types, including waveforms (e.g. electrocardiograms), images (e.g. chest x-rays), and numeric data (e.g. laboratory results) to more effectively assist clinicians in medical diagnosis. The project will also establish a multidisciplinary learning platform for computer-assisted health decision support systems to prepare students, postdocs, and early career clinical scientists in precision medicine using highly effective mathematical tools. It will also include participation of groups underrepresented in STEM through recruiting new students, integrating the research training in a highly diverse laboratory, and exploring multidisciplinary, research applied to real-world problems in biomedical science and engineering.<br/><br/>This project proposes to extend machine learning techniques to a) incorporate privileged information in algorithm training (data routinely available in retrospective databases but not live clinical environments) and b) to account of uncertainty in training labels (because even medical experts have uncertainty in medical diagnosis). These approaches will lead to more accurate and efficient algorithms for the detection of medical conditions where diagnostic uncertainty is common. The project will develop effective signal processing techniques to identify perturbations associated with respiratory insufficiency and ARDS development in time series data. The project will also develop image processing techniques that extract clinically relevant features from digital chest radiographs of the lungs that could improve the accuracy of real-time clinical diagnosis in many respiratory that are frequently difficulty to distinguish among. Finally, this project will integrate these novel methodologies to develop a clinical decision support system."
"1738492","SBIR Phase II:  Large-Scale Behavioral Analysis Utilizing Convolutional Neural Networks and Its Application to In-store Retail Marketing","IIP","SBIR Phase II","09/15/2017","07/28/2020","Everett Berry","IN","Perceive, Inc.","Standard Grant","Peter Atherton","09/30/2020","$909,999.00","","everett@perceiveinc.com","9059 Technology Ln","Fishers","IN","460382828","7654308561","ENG","5373","169E, 5373, 8032, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be to enhance infrastructure for research and education by exposing a previously unavailable dataset: fine-grained human interaction with a physical environment. Humans are always building on and shaping the world but there is little hard data to further examine the effects this has. Beyond retail, this technology could affect how teachers layout classrooms, how disaster workers provide relief, or how factories keep their workers safe. The subtle physical details that affect humans everyday will be understood and investigated in ways not possible without the proposed system. This technology will benefit society by increasing economic efficiency because retailers can meet customer needs more easily. This translates into large potential commercial value because of the size of the retail market and because retailers are under pressure to deliver differentiated customer experiences that customers cannot get online or at big-box stores. Such experiences are enabled by understanding the customer at a much deeper level which in turn is enabled by the technology proposed here.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project uses sensor placement models, statistical methods, and face recognition to fully realize the commercial potential resulting from the Phase I development of a video analytics system for understanding human behavior. Research in video-based behavior recognition has seen renewed excitement because of deep learning but current work only addresses pieces of the problem. Critical missing components are robust face identification and registration, a process to install as few cameras as possible while maximizing the viewable area of a store, and video analysis results that are meaningful when combined with other data such as retail transactions. The research objectives of extracting biometric data (such as facial features) from video, automatically computing the optimal positions of cameras, and correlating behavior metrics with business operations are essential to improve the retail experience for shoppers and make the developments undertaken in Phase I more commercially relevant to potential end users of the technology."
"1730129","II-EN: Collaborative Research: Enhancing the Parasol Experimental Testbed for Sustainable Computing","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2017","06/08/2017","Christopher Stewart","OH","Ohio State University","Standard Grant","Marilyn McClure","06/30/2021","$13,797.00","","cstewart@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7359","7359","$0.00","This project will enhance an experimental datacenter for sustainable computing. Datacenters consume vast amounts of energy, totaling about 1.8% of the US electricity usage in 2014. Thus, the energy efficiency, energy-related costs, and overall sustainability of datacenters are of critical concerns. NSF funded experimental green datacenter called Parasol has previously demonstrated that the combination of green design and intelligent software management systems can lead to significant reductions in energy consumption, carbon emission, and cost. The enhanced version of this project will update energy sources, network technologies and management software.<br/><br/>Running real experiments in live conditions using Parasol led to findings that were not possible in simulation. This proposal seeks to update and enhance Parasol with current and next generation power-efficient servers, improve network connectivity and integrate software-defined networking (SDN) and Wi-Fi capabilities, increase solar energy generation capacity, add a low emission fuel cell power source, diversify energy storage, and improve the cooling system to advance green computing. The PIs will need to update and enhance Parasol's current software stack for monitoring, programmatic control, and remote access for the new hardware enhancements. Specific research goals are resource management in green datacenters, that includes coordinated workload, cooling, and energy scheduling against environmental and load variability to maximize the benefits of green datacenters and to help improve grid power management leveraging accelerators such as GPUs and deep learning hardware, which promise excellent performance/watt ratios."
"1730043","II-EN: Collaborative Research: Enhancing the Parasol Experimental Testbed for Sustainable Computing","CNS","CSR-Computer Systems Research, CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2017","07/18/2018","Thu Nguyen","NJ","Rutgers University New Brunswick","Standard Grant","Marilyn McClure","06/30/2021","$701,713.00","Ulrich Kremer, Manish Parashar, Abhishek Bhattacharjee, Ivan Rodero","Tdnguyen@cs.Rutgers.Edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7354, 7359","7218, 7359","$0.00","This project will enhance an experimental datacenter for sustainable computing. Datacenters consume vast amounts of energy, totaling about 1.8% of the US electricity usage in 2014. Thus, the energy efficiency, energy-related costs, and overall sustainability of datacenters are critical concerns. NSF funded an experimental green datacenter called Parasol, which has previously demonstrated that the combination of green design and intelligent software management systems can lead to significant reductions in energy consumption, carbon emission, and cost. The enhanced version of this project will update energy sources, network technologies and management software.<br/><br/>Running real experiments in live conditions using Parasol led to findings that were not possible in simulation. This proposal seeks to update and enhance Parasol with current and next generation power-efficient servers, improve network connectivity and integrate software-defined networking (SDN) and Wi-Fi capabilities, increase solar energy generation capacity, add a low emission fuel cell power source, diversify energy storage, and improve the cooling system to advance green computing. The investigators will update and enhance Parasol's current software stack for monitoring, programmatic control, and remote access for the new hardware enhancements. Specific research goals are resource management in green datacenters, including coordinated workload, cooling, and energy scheduling against environmental and load variability to maximize the benefits of green datacenters and to help improve grid power management.  A specific goal is to leverage accelerators such as GPUs and deep learning hardware, which promise excellent performance/watt ratios."
"1726500","MRI: Acquisition of a Hybrid CPU/GPU High Performance Computing Cluster for Research and Education at Lamar University","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","10/01/2017","06/27/2018","Jing Zhang","TX","Lamar University Beaumont","Standard Grant","Stefan Robila","09/30/2021","$540,031.00","Tao Wei, Yueqing Li, Sujing Wang","jing.zhang@lamar.edu","4400 Port Arthur Road","Beaumont","TX","777055748","4098807670","CSE","1189, 7231","1189, 9251","$0.00","As traditional data processing devices are no longer adequate to handle complex data sets and large computations due to the continuing information explosion, a high performance computing cluster (HPCC) has become an essential instrument for a wide variety of leading-edge research and educational activities. Lamar University (LU), a medium-sized non-PhD granting four-year university with more than 15,000 students will acquire and deploy a HPCC to enhance compute-intensive and data-intensive studies and to facilitate discipline-specific and multidisciplinary research through a shared state-of-the-art computing platform. The instrumentation will strongly support LU's high priority current and future research needs as well as benefit a variety of regional academic institutions and industries.<br/><br/>Specifically, the project will acquire a hybrid CPU/GPU HPCC which will make it possible to deploy the best suited computing nodes to perform traditional CPU-based, GPU-based, and hybrid CPU/GPU-based data-intensive computing tasks at LU.  The resource will enable the exploration of creative research areas and establish new cross-disciplinary studies in the areas of imaging genomics, deep learning, big data, computational neuroscience, molecular physics, advanced materials research, scientific optimization, water and air quality analysis, transportation systems, electronic structure calculations, nucleic acid biomarker discovery and epigenetics, and many more. <br/><br/>Furthermore, as a shared research resource, the HPCC will not only promote cross-disciplinary collaborations among faculty members from different departments within the university, but also enable LU to promote and strengthen collaborative opportunities with other research institutions. In addition, the instrument will also become an essential educational tool with the potential to foster interest among faculty in the development of new courses that will integrate state-of-the-art research into undergraduate and graduate curricula.  Additionally, the project will provide access to the resource to users from other academic institutions and industrial partners in the Golden Triangle area in Southeast Texas.  Finally, the project will organize outreach activities for K-12 students from local Independent School Districts (ISDs) that have high minority and low-income ratios to study in science, technology, engineering, and mathematics (STEM) areas."
"1726017","MRI: Acquisition of a GPU-Based Cloud Infrastructure for Inter-/Multi-Disciplinary Research and Education at a Primarily Undergraduate Institution","CNS","Major Research Instrumentation","10/01/2017","09/15/2017","Shaoen Wu","IN","Ball State University","Standard Grant","Rita Rodriguez","09/30/2020","$260,000.00","Jesse Tye, Youfa Wang, Hong Xue, Jason Yang","swu@bsu.edu","2000 West University Avenue","Muncie","IN","473060155","7652851600","CSE","1189","1189","$0.00","This project, acquiring equipment to establish a GPU-based cloud, aims to enable big data related interdisciplinary projects, specifically collaborative research projects in computer science, large-scale medical data, computational chemistry, and geology. These include:<br/>- Intelligent Internet of Things (IoT),<br/>- Smart Health,<br/>- Spatial Computing, and <br/>- Computational Chemistry<br/>The intelligent IOT research would utilize the infrastructure to develop new deep learning models to enable smart living and navigation for vision impaired people. The big data enriched health research expects to find effective solutions to solve problems of obesity and non-communicable chronic diseases, serious threats to public health in the US and globally, using the instrumentation to compute the models with enormous health data. The geoscience research would enabled the establishment of ""American Spatial Data Portal"" (ASDP) that will provide a central repository of spatial data to expedite research on geospatial data mining and applications. A cloud geospatial education system would be developed on the instrument to transform the nation's geoscience education. The computational chemistry research would take advantage of the computation capability of the cloud to discover, design, and evaluate new and low costs catalysts for important chemical reactions, which could significantly change the life of the world. This GPU-based cloud is expected to facilitate the data and computation intensive research progress across many disciplines not only at this mainly undergraduate university, but also at other institutions through authorized and scheduled remote access. Furthermore, the infrastructure would be shared via high speed internet to other colleagues in the nation, and to international collaborators who are interested.<br/> <br/>Broader Impacts: <br/>By involving students in the activities of installation, configuration, and instrumentation, this project would provide valuable opportunities to train future scientists, engineers, and instrumentalists. The developed new cross-list courses and summer training program would train hundreds of both senior and graduate students on GPU and big data related knowledge and skills. Moreover, due to the appropriate mentoring that the investigators are able to offer, overall students and minorities can benefit."
"1739772","Collaborative Research: SI2: SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/24/2017","Mike Williams","MA","Massachusetts Institute of Technology","Standard Grant","Stefan Robila","08/31/2021","$275,000.00","","mwill@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1253, 7244, 8004","7433, 8004, 8005","$0.00","In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  <br/><br/>The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed."
"1718389","CCF-BSF:CIF: Small: Coding for Fast Storage Access and In-Memory  Computing","CCF","Comm & Information Foundations","09/01/2017","08/30/2017","Lara Dolecek","CA","University of California-Los Angeles","Standard Grant","Phillip Regalia","08/31/2020","$470,000.00","","dolecek@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797","7923, 7935, 9102","$0.00","Part 1:<br/><br/>Driven by the needs of mobile and cloud computing, demand for data storage is exhibiting steep growth, both in the direction of higher storage density as well as a simultaneous ambitious increase in access performance. A related exciting emerging trend driven by access challenges is in-memory computing, whereby computations are offloaded from the main processing units to the memory to reduce transfer time and energy. The challenges of future rapid storage access and in-memory computing cannot be addressed by the conventional storage architectures that inevitably trade off reliability and capacity for latency. This bottleneck calls for innovative research contributions that can simultaneously maximize the storage density, access performance, and computing functionality. This project addresses this imminent challenge by developing principled mathematical foundations that will underpin future computing systems possessing qualities necessary to address new data-intensive applications, focusing on fundamental performance bounds, algorithms, and practical channel coding methods. The results of this project will be demonstrated on modern data-driven and machine learning applications, will advance the repertoire of mathematical techniques in information sciences, and will directly impact future computer system architectures to meet the growing and wide ranging societal and scientific needs for computing and rapid data processing. Additionally, the proposal offers several mechanisms for broader impacts, including engagement with data storage and memory industry through the existing research center that the principal investigator is leading at UCLA, curriculum development and the introduction of new graduate courses in the UCLA on-line master's program in engineering, engagement of undergraduate researchers, and dissemination of the results through survey-style articles and tutorials.<br/><br/><br/>Part 2: <br/><br/>The project has the following three complementary research goals:<br/><br/>1) Invention of new channel codes for reliable and fast memory access for latency sensitive applications, with the study spanning general memories and specific schemes for resistive memories in particular. The proposed schemes will offer non-trivial extensions to vibrant coding subjects: codes with locality (algebraic and graph-based) and constrained coding; <br/><br/>2) Invention of new channel codes for which the decoding is performed directly in memory to enable simultaneously satisfying competing requirements on latency and reliability. Here, the decoder itself is subject to computational errors, themselves manifested in a data dependent sense. The analysis will lead to bounds and practical code designs robust to data-dependent errors. An exemplar will be codes designed using spatial coupling and decoded using windowed message passing decoders;<br/><br/>3) Development of novel fundamental bounds, algorithms, and channel codes for robust in-memory computing, with the focus on quantifying the robustness of computing primitives in statistical inference and other machine learning algorithms used in modern data-driven applications.  These include fundamental performance limits and new coding-based methods to simultaneously combat sneak paths and computing noise. Analysis will include coding for (noisy) Hamming/Euclidean similarity calculations, evaluated in the context of practical machine learning applications.<br/><br/>Results from this project will also contribute to the curriculum development at UCLA and will offer new opportunities for the engagement of undergraduate researchers from underrepresented groups."
"1742702","EAGER: Training Computers and Humans to Detect Misinformation by Combining Computational and Theoretical Analysis","CNS","Secure &Trustworthy Cyberspace","09/01/2017","06/27/2018","Dongwon Lee","PA","Pennsylvania State Univ University Park","Standard Grant","Sara Kiesler","08/31/2020","$316,000.00","S. Shyam Sundar","dongwon@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8060","025Z, 065Z, 7434, 7916, 8225, 9178, 9251","$0.00","Awareness of misinformation online is becoming an increasingly important issue, especially when information is presented in the format of a news story, because (a) people may over-trust content that looks like news and fail to critically evaluate it, and (b) such stories can be easily spread, amplifying the effect of misinformation.  Using machine learning methods to analyze a large database of articles labeled as more or less likely to contain misinformation, along with theoretical analyses from the fields of communication, psychology, and information science, the project team will first characterize what distinguishes stories that are likely to contain misinformation from others.  These characteristics will be used to build a tool that calls out characteristics of a given article that are known to correlate with misinformation; they will also be used to develop training materials to help people make these judgments.  The tool and training materials will be tested through a series of experiments in which articles are evaluated by the tool and by people both before and after undergoing training.  The goal is to have a positive impact on online discourse by improving both readers' and moderators' ability to reduce the impact of misinformation campaigns.  The team will make the models, tools, and training materials publicly available for others to use in research, in classes, and online.<br/><br/>The team will use two main approaches to characterize articles that are more likely to contain misinformation.  The first is a concept explication approach from the social sciences based on a deep analysis of research writing around information dissemination and evaluation. The second is a supervised machine learning approach to be trained on large datasets of labeled articles, including verified examples of misinformation.  Both approaches will consider characteristics of the content; of its visual presentation; of the people who create, consume, and share it; and of the networks it moves through.  These models will be translated into a set of weighted rules that combine the insights from the two approaches, then instantiated in Markov Logic Networks.  These leverage the strengths of both first order logic and probabilistic graphic models, allow for a variety of efficient inference methods, and have been applied to a number of related problems; the models will be evaluated offline against test data using standard machine learning techniques.  Finally, the team will develop training materials based on existing work from the International Federation of Library Associations and Institutions and on heuristic guidelines derived from the modeling work in the first two tasks, evaluate them through the experiments described earlier, and disseminate them online along with the developed models."
"1801103","Quantum Computing Workshop for Advancing Aerospace Sciences","DMS","COMPUTATIONAL MATHEMATICS","11/01/2017","10/19/2017","William Oates","FL","Florida State University","Standard Grant","Leland Jameson","10/31/2019","$10,000.00","Mohammed Hussaini","woates@eng.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","MPS","1271","7556, 9263","$0.00","A workshop entitled ""Quantum Computing Workshop for Advancing Aerospace Sciences"" is scheduled to be held in Suffolk, VA at the Lockheed Martin Center for Innovation on November 7-8, 2017.  More details on the workshop can be found at http://www.nianet.org/quantumcomputing/.  The goals of the workshop are to understand the challenges and opportunities to develop a new type of computing methodology that significantly increases computer speed and can be applied to science and engineering problems.  This has implications on a variety of applications including machine learning, materials design, personalized medicine, advanced weather prediction, energy distribution and optimization, among others.  Quantum computers use an entirely different computing paradigm in comparison to conventional computers.  Conventional computer hardware uses silicon to process information in terms of ""bits"" that can be encoded in either zeros or ones.  Quantum computers use what is known as ""qubits"" which allow information to be encoded by both zeros and ones at the same time using the unusual behavior of quantum mechanics.  This quantum behavior has been measured in different experimental systems by controlling and measuring light and electricity, for example.  However, transitioning these prototype machines into a quantum computer with a large number of qubits that can reliably processes information is still in its infancy.  This workshop will bring together industry leaders, government laboratory researchers, and university researchers to define new goals and objectives to transition how small scale quantum computing testbeds can be scaled up using advanced hardware and quantum algorithms.  This is expected to provide a roadmap for solving practical problems of interests to the general public and national security.  The invited speakers will discuss challenges and opportunities centered around the following four topic areas: 1) Quantum algorithms, 2) Quantum computing hardware, 3) Control and error correction of quantum systems, and 4) Aeroscience applications.  Fourteen global experts, including two female speakers, will discuss current research related to these four quantum computing areas. <br/><br/>The objective of this workshop is to bring together experts on quantum computing and quantum information to discuss the challenges, opportunities, and latest developments in quantum algorithms, hardware, and its impact on supporting aeroscience computations.  The goal is to develop a roadmap for success that connects mathematicians, physicists, computer scientists, and engineers to collectively explore capabilities where quantum speed-up may impact domain specific problems in science and engineering applications.  These specific areas may include computational materials science, fluid dynamics, uncertainty quantification, machine learning, among others.  This will include discussions on the latest advances in algorithm development, scalability, universal logic, and error correction with the aim to understand the next set of mathematical challenges required to control quantum systems, measure their outputs, and preserve their properties from outside disturbances.  Quantum speed-up is well known in specific algorithms that lead to exponential increases in factoring prime numbers (Shor's algorithm) and quadratic speed increases in unstructured searches (Grover's algorithm).  More recently, it has been suggested that exponential speed-up can be achieved in linear algebra problems.  Similar research has focused on new Monte Carlo quantum algorithms. This research is focused on identifying efficient means to solve partial differential equations on a quantum computer.  Based upon the chosen topics, we will focus discussions on these topics where quantum algorithms and hardware may converge onto methods that allow for efficient solutions to partial differential equations relevant to a broad area of mathematics, physics, materials science, and engineering problems.  Through discussions among quantum computing stakeholders in industry, government laboratories, and academia, we expect to facilitate building partnerships across disciplines and provide opportunities for young faculty and graduate students to learn about quantum computing.  We envision developing a roadmap that defines the next set of realistic challenges that can be met over the next 10-15 years."
"1707298","NeuroNex Innovation Award: Towards Automatic Analysis of Multi-Terabyte Cleared Brains","IOS","Cross-BIO Activities","09/01/2017","08/05/2019","Joshua Vogelstein","MD","Johns Hopkins University","Standard Grant","Sridhar Raghavachari","08/31/2020","$959,999.00","Michael Miller, Carey Priebe, Randal Burns, Florian Engert","jovo@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","BIO","7275","8091","$0.00","Three complimentary changes are revolutionizing the way neuroscientists study the brain. First, experimental advances allow neurobiologists to ""clear"" brains so that they become transparent, with the exception of a set of neurons that can be selected on the basis of their location, response properties, and genetic make-up. Second, technological advances have resulted in microscopes that can simultaneously image an entire ""sheet"" of this brain, thereby enabling rapid acquisition of whole brain volumes. Third, researchers are taking steps to educate neuroscientists to acquire these data. Together, this will result in a massive upswing in adoption of this experimental modality. However, acquiring the data is one step in the upward spiral of science that will yield transformative scientific results. The subsequent steps are computational. This project will develop cyberinfrastructure resources and software that enable storage and access of large CLARITY brain imaging datasets, alignment and registration to reference anatomical atlas and visualization of the datasets. Additional capabilities for automatic identification and localization of cell bodies and statistical analysis will be provided. The PIs will run annual hackathons for college students and sponsor a summer internship program for undergraduates to broaden the educational efforts in software development for neuroscience. Finally, mobile compliant digital education content will be created to complement existing online courses to target STEM students, and educate global citizens.<br/><br/>This project will build a prototype pipeline that operates on raw CLARITY brains and outputs the statistics of locations of cells in each region in the Allen Reference Atlas, as well as estimates of connectivity and similarity across regions and conditioned on different contexts. To do so, the PIs will leverage modern mathematical statistics (such as Large Deformation Diffeomorphic Metric Mapping for registration, Deep Learning and Random Forests for segmentation, and Statistical Graph Theory for analysis of the resulting conenctomes), as well as modern computational tools, including Docker containers to facilitate full reproducability, and semi-external memory algorithms and cloud computing to enable scalable analytics. To reach out to the broader community and educate them in the use of these tools, this project will provide tutorials deployed in the cloud. Together, this will facilitate the large community of users to both collect and analyze their data with ease. Many of the tools developed as part of this project will be easily extensible to other experimental modalities and neuroscience communities."
"1739191","INFEWS/T3:  Innovations for Sustainable Food, Energy, And Water Supplies In Intensively Cultivated Regions: Integrating Technologies, Data, And Human Behavior","CBET","Track 1 INFEWS, Track 3 INFEWS","10/01/2017","08/09/2018","Jeffrey Peterson","MN","University of Minnesota-Twin Cities","Continuing grant","Brandi Schottel","09/30/2021","$2,429,500.00","Vipin Kumar, Jason Hill, Axel Garcia, Amit Pradhananga","jmpeter@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","020Y, 022Y","004Z","$0.00","To keep pace with the demands of a growing global population, innovations are needed to meet the unprecedented challenge of producing more food in intensively cultivated regions with less energy and lower environmental impacts. In this project, researchers from the biophysical, socioeconomic, and computational sciences investigate two types of innovations using data from the northern U.S. Corn Belt. First, a novel oilseed crop, winter camelina, is being studied for its potential incorporation into existing corn-soybean rotations to produce a new supply of biodiesel energy while lowering water resource impacts and creating positive ecological benefits. Second, emerging systems of sustainability certification are being studied for their potential to lead to broad-scale adoption of this new cropping system. Detailed computational models are being evaluated and applied for systems-level assessments of two innovations: developing novel approaches to influence beneficial land use, and accounting for energy and environmental impacts within food supply chains. Because of the importance of the project results on the local economy, outreach activities are targeted towards the rural community, policy makers, the general public, and local watershed planners. Although this project focuses on the Northern Corn Belt, the approaches used in the research could be adopted to achieve beneficial outcomes for food, energy, and water systems elsewhere. <br/><br/>This research project I scomprised of four overlapping and interdependent research teams. The biophysical research team conducts cropping systems studies of corn-soybean in rotation with winter camelina at two Minnesota research stations. Experimental treatments vary by winter timing of planting and harvest and fertilization rates. Meteorological data along with soil, water, and crops data is being collected to develop management strategies for producers and to calibrate and evaluate the crop models that will be used. The socioeconomic research team collects data from surveys and randomized control trials to study the forces determining whether, and by whom, new cropping systems are likely to be adopted under different policy and market conditions. Of particular interest is the role of incentives from certification programs, including the feedback effects of using producer data for peer benchmarking. The data science team applies novel deep learning computational approaches to identify crops, including winter cover crops, from satellite imagery. Study plots for the biophysical plots provides training data for crop identification and the final statewide datasets are being incorporated in the socioeconomic analysis. Finally, the integrated modeling team develops a suite of connected modeling tools to quantify systems-level outcomes. These simulations shed light on the feasibility and impacts of innovations in the food-energy-water system under different scenarios, including spatial patterns and the role of socio-economic drivers."
"1720726","SBIR Phase I:  Machine Learning Software for Situation Awareness in the Operating Room for Improved Patient Flow","IIP","SMALL BUSINESS PHASE I","07/01/2017","07/11/2017","Ayman Fawaz","CA","orgo.ai Inc.","Standard Grant","Jesus Soriano Molla","12/31/2017","$225,000.00","","aymanfawaz@comcast.net","2150 Shattuck Avenue","Berkeley","CA","947041370","5104098782","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is a significant increase in the efficiency of hospital operations.  The project focuses on the sources of inefficiencies in the planning, staffing, coordination, and execution of surgical procedures.  Preliminary analysis of hospital data shows that the throughput of perioperative units can be increased significantly with the same staffing while reducing the delays that patients experience and improving the working conditions of the personnel.  Such improvements have the potential of saving upwards of $1B per year. Delays in obtaining and communicating updates on the status of surgeries and on actions that personnel should perform are major causes of inefficiency, as is the randomness of the tasks' duration. The timing of those messages depends on the knowledge of the state of the system and a prediction about its future evolution. Delays in gathering information and incorrect predictions of the effect of actions result in reduced efficiency.  The proposed innovation improves the selection and timing of messages.<br/><br/>The proposed project is based on a machine learning approach for the optimization of real-time messaging using actual hospital data. The approach combines new parametric models of real-time scheduling, stochastic gradient descent, and infinitesimal perturbation analysis. In this formulation, perturbation analysis computes the gradient of the objective function with respect to the timing of messages and results in an efficient algorithm.  The algorithm discovers the best time to send messages to optimize a combination of operating room efficiency and patient waiting times. The methodology is able to evaluate the complex cascading impact of scheduling decisions and to identify the best course of action when dealing with contingencies. Timely situation awareness will contribute to improved patient flow in the hospital."
"1658908","Workshop: Towards an Ecosystem of Simulation Models and Data; 5th INFORMS Simulation Society Research Workshop; University of Durham, United Kingdom; July 31 to August 2, 2017","CMMI","OE Operations Engineering","07/01/2017","03/08/2017","John Shortle","VA","George Mason University","Standard Grant","Georgia-Ann Klutke","12/31/2017","$5,000.00","","jshortle@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","ENG","006Y","7556","$0.00","The objective of this workshop is to bring together researchers and students from the domains of digital simulation and data science to accelerate the concordance of these distinct but overlapping technologies.   The ubiquity of data from diverse sources (sensors, social networks, Internet of Things) and evolving data-science technologies for machine learning, visual analytics, and collaborative modeling and analysis present great potential for transforming simulation practice, and both domains are crucial to the understanding and further evolution of data-driven decision support systems.  As data become a ubiquitous resource, and simulation models become increasing complex, the simulation community faces major challenges and opportunities in both exploiting available real-world data and in extracting insights from the massive data sets produced from large-scale simulation experiments.  The convergence of science from both research communities holds promise for tackling major research questions in both fields, such as overcoming barriers to data-driven, online simulation, assimilating and correcting data from multiple sources, simulation input uncertainty, and incorporating machine learning innovations into simulation.  This award will support the participation of graduate students who will become the next generation of leaders in the simulation community.<br/><br/>This workshop will be held at the University of Durham in the UK from July 31-August 2, 2017.  It is organized by three leading simulation researchers and brings together intellectual leaders from both simulation and data science around such topics as data assimilation, data farming, online data-driven simulations, robust simulation, database technology, metamodeling, machine learning, model calibration, and collaborative modeling and simulation.  The workshop organizers have identified eight topic areas, and focused groups of researchers will present and reflect upon emerging research in these areas and identify important directions for future research.  The workshop will be broadly promoted through major professional societies such as the Institute for Operations Research and the Management Sciences (INFORMS), the American Statistical Association (ASA) and the Association for Computing Machinery (ACM).  Results will be widely disseminated through a special issue of the ACM Transactions on Modeling and Computer Simulation."
"1664426","Nonlinear dimensionality reduction and enhanced sampling in molecular simulation using auto-associative neural networks","CHE","CONDENSED MATTER & MAT THEORY, Chem Thry, Mdls & Cmptnl Mthds","06/15/2017","06/07/2017","Andrew Ferguson","IL","University of Illinois at Urbana-Champaign","Standard Grant","Evelyn Goldfield","09/30/2018","$380,105.00","","andrewferguson@uchicago.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","1765, 6881","7433, 7573, 8084, 9216, 9263","$0.00","Andrew Ferguson of the University of Illinois at Urbana-Champaign is supported by an award from the Chemical Theory, Models and Computational Methods Program in the Chemistry Division to establish new theoretical approaches and computational tools to accelerate molecular simulations of protein folding. This project is cofunded by the Condensed Matter and Materials Theory Program in the Division of Materials Research. Proteins are molecular workhorses that perform the essential functions of life.  Proteins have evolved to adopt shapes that enable them to do these tasks. Determining the shape and motions of a protein can help reveal how it works and inform how to design new proteins to help treat disease, produce biofuels, or make new materials. Computer simulations of proteins are very useful in that they can identify the precise locations and motions of all the constituent atoms. For all but the smallest proteins, however, it is too computationally intensive to accurately predict their structure and motions even with powerful supercomputers. Ways to accelerate these simulations have been developed, but to work well they need good estimates of the structural rearrangements that the protein will make. This is a problem, since this is usually the question the simulations are trying to answer. In this work, Professor Ferguson is developing a new approach to accelerate protein folding simulations using a type of machine learning known as artificial neural networks so-called because they are based on the structure of neurons in the brain.  Neural networks allow computers to both determine these important structural pathways and use them to make simulations run faster. This new approach is being used to help understand large proteins involved in cancer and HIV infection.  It is also being incorporated into popular simulation software available for free public download. Professor Ferguson is providing research opportunities for undergraduates to work with him on this project and he is developing hands-on workshops in computational materials science as part of the Girls Learning About Materials (GLAM) summer camp at the University of Illinois. <br/><br/>The aim of this work is to establish a nonlinear machine learning approach to discover collective variables for protein folding and to use these variables to perform enhanced sampling in molecular dynamics simulations. The success of enhanced sampling techniques in accelerating conformational sampling is predicated on the availability of good collective variables (CVs) correlated with important molecular motions. Existing nonlinear dimensionality reduction techniques (e.g., diffusion maps, Isomap, land ocally linear embedding) can ably discover good CVs, but do not furnish the explicit coordinate mapping so that biased sampling must be conducted inefficiently and indirectly in proxy variables. This work establishes a new enhanced sampling approach based on auto-associative artificial neural networks (""autoencoders"") to discover CVs that are explicit differentiable functions of the atomic coordinates and to permit calculation of analytical biasing forces. This approach is termed MESA (Molecular Enhanced Sampling with Autoencoders). MESA is validated on the short peptides alanine dipeptide and tryptophan-cage, and deployed to discover metastable states and structural transitions in a kinase overexpressed in many cancers and an envelope protein presented on the surface of HIV. MESA is made broadly available to the molecular simulation community by collaborating with the developers of OpenMM and PLUMED to contribute the approach to future releases of these software packages. Positive research experiences have great benefits for undergraduate success and retention and this work supports 10-week summer research opportunities during each year of the award. Professor Ferguson is also developing new and exciting content for the highly successful Girls Learning About Materials (GLAM) summer camp at the University of Illinois to illustrate and promote computational materials science among female high school students and elevate female enrollment in STEM degree programs."
"1717943","CIF: Small: Distributed Statistical Inference with Compressed Data","CCF","Comm & Information Foundations","07/15/2017","07/07/2017","Lifeng Lai","CA","University of California-Davis","Standard Grant","Phillip Regalia","06/30/2021","$449,996.00","","lflai@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7797","7923, 7936","$0.00","Due to the rapid growth of size and scale of datasets and desire to harnessing parallel processing capabilities of multiple machines, distributed statistical inference and machine learning, in which available data are stored in multiple machines who are allowed to communicate with each other with limited communication budgets, have attracted significant research interests. There are two basic scenarios for the distributed setting: sample partition and feature partition. Although there have been many recent work on the design of inference algorithms for the sample partition scenario, there has been limited work on the feature partition scenario. The focus of this project is to characterize the fundamental limits and develop distributed statistical algorithms for the feature partition scenario from information theoretic perspective.<br/><br/>Compared with the sample partition scenario, the feature partition scenario is significantly more challenging. This research addresses these challenges by focusing on two research thrusts. Thrust 1 focuses on designing interactive encoding schemes for inference. The main idea is that, by interacting with each other, the terminals can coordinate their compression so that the decision maker can obtain more information about the parameter while using the same communication resources, which will lead to a better inference performance. Thrust 2 designs function computing schemes for inference, in which the machines compute a function of observations without recovering them first and then perform inference from this function. The main motivation for this idea is that recovering observations or a compressed version of them is not necessary in the distributed inference setup, as the final goal of the distributed inference is to infer the value of the unknown parameter."
"1660021","SBIR Phase II:  Science, Technology, Engineering, and Math (STEM) Career Literacy & Advocacy","IIP","SBIR Phase II","03/01/2017","08/26/2019","Melissa Risteff","CO","Couragion Corporation","Standard Grant","Rajesh Mehta","08/31/2020","$1,159,093.00","","melissa@couragion.com","649 Marion Street","Denver","CO","802183431","7204601744","ENG","5373","165E, 5373, 8031, 8032, 8039, 8240","$0.00","This project will improve the awareness and perception of careers that require science, technology, engineering, and math (STEM) competencies. Advocacy is a critical component of career readiness, yet current advocates (parents, guardians, educators, or members of the community) are often not in the position to inform students of potential career options. Career and workforce readiness programs are resource constrained, don't meet the needs of differing learning styles, have inherent bias, and are largely focused on compliance over student competency building. Many underrepresented youths who would otherwise succeed in STEM are often deterred by a lack of role models. If youth understood the opportunities, they could pursue academic pathways to amass skills that better prepare them to enter the workforce. Furthermore, educators need professional learning experiences and access to insights about their students in order to improve STEM teaching and learning. Helping individuals select rewarding and suitable degrees, training, and careers will increase the likelihood of higher job retention. As more individuals are inspired to pursue and stay in STEM, taxpayers will benefit from increased innovation which in turn will provide tax dollars to invest in such things as healthcare, national security, education, or humanitarian assistance.<br/><br/>The project will address technical challenges of amassing and distributing massive amounts of 3rd party STEM (Science, Technology, Engineering, and Math) resource data (both structured and unstructured), developing an information management system for educators and families, developing adaptive learning skill modules, advancing a second generation smart recommendation engine based on big data, machine learning and predictive modeling techniques, and performing database mining and creating data visualizations to derive meaningful workforce development insights. In addition, the project will involve controlled experiments and usability tests whereby a large amount of anonymized and aggregated student data and business/education entity feedback will be collected. The ultimate goals of the R&D and experiments are to validate that the resulting application, predictive models, information management practices, advocacy networks, and data visualizations have the desired outcome of boosting immediate and near-term student outcomes regarding STEM career intentions and actions."
"1734821","Collaborative Research: NSF-FO: Ground-Truth Analysis and Modeling of Entire Individual C. elegans Nervous Systems","DUE","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","09/01/2017","08/07/2017","Albert-Laszlo Barabasi","MA","Northeastern University","Standard Grant","Ellen Carpenter","08/31/2019","$237,499.00","","a.barabasi@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","EHR","7980, 8624","8089, 8091, 8551, 8817","$0.00","How does the brain compute? Understanding this process could lead to many advances in science and technology. The Boyden, Flavell, Barabasi, and Tegmark groups propose to examine how the cells within the brain of a simple animal work together to generate the computations that underlie behavior. The teams will study C. elegans, a small worm with just a few hundred neurons, yet capable of learning and adaptive behavior in complex real-world environments.  The teams will apply new technologies to measure and control the neural circuits of C. elegans, in order to investigate how they works.  The project will also generate new mathematical tools to analyze the data that is collected - tools that could help analyze how the brain goes wrong in disorders such as Parkinson's or Alzheimer's. Using the data acquired, the project will reveal how brain circuits compute, which could inspire new algorithms for machine learning and computer information processing.  These in turn could have broad impact on economic prosperity as well as in advancing human quality of life.  <br/><br/>The Boyden, Flavell, Barabasi, and Tegmark groups will launch a novel integrative endeavor to reveal how entire nervous systems - from sensory input neurons, to motor output neurons, and including the networks that underlie learning, decision making, and other processes - work together as emergent wholes to generate the computations that underlie behavior. They will utilize C. elegans, with just 302 neurons, yet capable of learning and adaptive behavior in complex real-world environments.  They will optimize and deploy novel technologies, including a new fluorescent voltage indicator for C. elegans, and a method for 3-D visualization of entire nervous systems with molecular information via physical expansion by up to 10,000 fold in volume. They will record neural and behavioral dynamics, imaging the activity of neurons throughout entire brains and even entire nervous systems of freely moving as well as fictively behaving C. elegans engaged in complex decision-making tasks, or forming new memories. They will then use expansion microscopy to map the structure and molecular profiles of entire individual nervous systems.  They will analyze the resultant network structures to determine how individual variation in these features connect to details of an individual's behavior, and make mathematical models of the relevant neural circuits capable of predicting how the nervous system would respond in complex contexts. The outcome of their work will yield radical new theories of how nervous systems operate, as well as a diversity of tools for the neuroscience and computational communities.<br/><br/>This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE)."
"1662692","Collaborative Research: Statistical Learning, Driving Simulator-Based Modeling, and Computationally Tractable Dynamic Traffic Assignment","CMMI","CIS-Civil Infrastructure Syst","08/01/2017","08/07/2017","Srinivas Peeta","IN","Purdue University","Standard Grant","Cynthia Chen","01/31/2019","$219,497.00","","srinivas.peeta@ce.gatech.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","1631","036E, 1057","$0.00","Congestion is familiar to anyone who relies on a privately owned or rented automobile, taxi, or public transit for commuting, shopping and errand running. Historically, engineers and scientists exploring traffic networks frequently build mathematical models with the intent of coaxing from them insights revealing how congestion may evolve over time. Unfortunately, such models may easily become so large and complex that they are unwieldly, and simplifications are needed in order to provide passengers and drivers with accurate and rapidly computable information pertinent to route choice and departure time selection. Toward that goal, this project will employ modern statistics, simulation experiments, and notions of competition among traffic network users for available road capacity to better depict and more efficiently compute the behaviors of drivers who rely on road networks. The broader impacts of this research will be substantial. In particular, the results of this research will allow commuters and urban freight carriers to make more informed travel decisions, and governmental organizations to better regulate travel decisions within heavily congested major metropolitan regions. This study will also provide system-level experiential learning opportunities for students entering the transportation workforce. <br/><br/>Specifically, through a combination of experiments and machine learning and model development, this project will aim to depict the noncooperative exploration of available routes and departure times by drivers and passengers seeking to fulfill their travel demands via metropolitan road networks. A key goal of the intended research will be the efficient computation of solutions to the most prevalent type of dynamic traffic assignment (DTA), namely so-called dynamic user equilibrium (DUE). It is the lack of closed-form travel-delay operators that makes DUE computation tedious and slow. The plan is to replace the existing, differential algebraic equation (DAE) system representing travel delay with closed-form, approximate delay operators based on a form of statistical learning known as Kriging. Ad hoc experiments based on such an approach show great promise for small networks, but are not definitive. The PIs will develop the envisioned models and make developed software available as free-ware or inexpensive apps."
"1712508","Identifying and Aiding At-Risk Students in Computing","DUE","IUSE","08/15/2017","05/11/2020","Leo Porter","CA","University of California-San Diego","Standard Grant","Paul Tymann","07/31/2021","$315,326.00","William Griswold, Christine Alvarado","leporter@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","EHR","1998","8209, 8244, 9178, 9251","$0.00","This project will address the problem of meeting computer science students' learning needs by identifying and supporting at-risk students.  Computer science (CS) education has been pushed to the foreground as the importance of computing in society continues to increase. Initiatives like the Hour of Code and CS10K have been successful in attracting students to computing courses, but this in turn has also strained instructor resources at many institutions. Over the past 40 years of computer science education, a concerning theme has emerged in terms of student success. Students fail at elevated rates compared to other STEM disciplinary courses and often leave the field altogether after poor early experiences.  Many learn less than instructors expect. The CS education community has done well to document these struggles and hypothesize on their antecedents, but has done less well in terms of intervening to help these students.  Leveraging a source of student process and learning data not available to earlier generations of researchers such as in-class clicker responses and fine-grained programming activity data allowed for identification of struggling students in introductory computing courses using machine learning techniques. The core intellectual merit of the project is the creation of practical and sharable methods and tools for instructors to identify struggling CS students early, an improved understanding of the factors that cause students to struggle, and written reports on the value of one-on-one or small group interventions for helping CS students improve.  <br/><br/>Specifically, the project aims to advance the team's preliminary work in this area by (1) broadening the applicability of the technique to more computing courses under differing circumstances, (2) interviewing students at-risk early in the term with the goal of identifying reasons for their struggles, and (3) piloting interventions with follow-up student interviews to better understand the effect of the interventions.  The broader impacts of this work will be the increased learning, success, and retention of computer science undergraduate students. Instructors armed with the tools of early and accurate identification of struggling students will be able to intervene before the students have fallen too far behind. Knowing why the students are struggling, and what actions might help, better equips instructors to intervene and help students. This offers the potential to grow the supply of capable computer scientists, which, despite increased national enrollments, will still fall short of industry demand. It also promises to help underrepresented groups, who are most apt to be at risk, thus improving gender, racial, ethnic, and socioeconomic equality in CS."
"1660216","SBIR Phase II:  An embedded and in-context professional learning platform for math problem-solving instruction","IIP","SBIR Phase II","03/15/2017","02/07/2019","Sheela Sethuraman","MA","CueThink","Standard Grant","Rajesh Mehta","02/28/2021","$1,287,318.00","","sheela@cuethink.com","8 Furbish Pond Lane","North Reading","MA","018642636","2243389328","ENG","5373","115E, 118E, 165E, 5373, 7218, 8031, 8032, 8039, 8240, 9102, 9177","$0.00","This project proposes to develop an innovative approach to improve and sustain math educators' problem solving teaching skills. Despite the expectations placed on math teachers by the Common Core State Standards, most are insufficiently prepared to teach students how to become critical thinkers. Much of this problem is due to limited pedagogical skills of teachers in providing adequate problem solving instruction and supports on top of teachers' own limited problem solving skills. This project remedies this with its integrated modules and powerful analytics engine that suggests learning pathways for both expert and novice teachers. They anchor their research in National Council of Teacher's of Mathematics Principles to Action. It will help teachers develop confidence and skills in planning and evaluating their lessons, as well as understanding student misconceptions and intervening in a timely manner. Teachers who approach problem solving with confidence inspire students to approach difficult math tasks the same way. This has great implications for how many students will continue to enroll in Science, Technology, Engineering and Math programs. In addition, the project sets the stage for educators to develop 21st Century skills including critical thinking, communication and collaboration - essential job skills for the young minds they mentor.<br/><br/>This effort refines and scales up their product, which is a web and mobile application that works seamlessly in conjunction with our current student-facing platform, to provide teachers with timely supports for improving students' problem-solving skills and math communication. This project will deliver professional development continuously and in-context using virtual peers, rich rubrics, interactive tools and actionable data. The analytics engine leverages adaptive learning models in order to build robust modules. The Data Collector Layer will contain interfaces for users to get recommendations, receive user feedback and provide other analysis reports. The Analytics Core Layer will be implemented using a collection of machine learning algorithms. The Service Layer will calculate recommendations based on user profile, user feedback, pre-stored best practices and other use cases. The Persistence Layer will store and get calculated data to recommendation engine's own database. The company plans to conduct several formative evaluations during the course of the project, as well as two pilot studies at the end of each year with a control and experiment group. The results will enable them to determine the effectiveness of ongoing, just-in-time supports for improving teachers' skills and confidence inside and outside the classroom."
"1734870","NCS-FO: Collaborative Research: Ground-Truth Analysis and Modeling of Entire Individual C. elegans Nervous Systems","DUE","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","09/01/2017","08/07/2017","Edward Boyden","MA","Massachusetts Institute of Technology","Standard Grant","Ellen Carpenter","08/31/2019","$707,296.00","Max Tegmark, Steven Flavell","esb@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","EHR","7980, 8624","8089, 8091, 8551, 8817","$0.00","How does the brain compute? Understanding this process could lead to many advances in science and technology. The Boyden, Flavell, Barabasi, and Tegmark groups propose to examine how the cells within the brain of a simple animal work together to generate the computations that underlie behavior. The teams will study C. elegans, a small worm with just a few hundred neurons, yet capable of learning and adaptive behavior in complex real-world environments.  The teams will apply new technologies to measure and control the neural circuits of C. elegans, in order to investigate how they works.  The project will also generate new mathematical tools to analyze the data that is collected - tools that could help analyze how the brain goes wrong in disorders such as Parkinson's or Alzheimer's. Using the data acquired, the project will reveal how brain circuits compute, which could inspire new algorithms for machine learning and computer information processing.  These in turn could have broad impact on economic prosperity as well as in advancing human quality of life.  <br/><br/>The Boyden, Flavell, Barabasi, and Tegmark groups will launch a novel integrative endeavor to reveal how entire nervous systems - from sensory input neurons, to motor output neurons, and including the networks that underlie learning, decision making, and other processes - work together as emergent wholes to generate the computations that underlie behavior. They will utilize C. elegans, with just 302 neurons, yet capable of learning and adaptive behavior in complex real-world environments.  They will optimize and deploy novel technologies, including a new fluorescent voltage indicator for C. elegans, and a method for 3-D visualization of entire nervous systems with molecular information via physical expansion by up to 10,000 fold in volume. They will record neural and behavioral dynamics, imaging the activity of neurons throughout entire brains and even entire nervous systems of freely moving as well as fictively behaving C. elegans engaged in complex decision-making tasks, or forming new memories. They will then use expansion microscopy to map the structure and molecular profiles of entire individual nervous systems.  They will analyze the resultant network structures to determine how individual variation in these features connect to details of an individual's behavior, and make mathematical models of the relevant neural circuits capable of predicting how the nervous system would respond in complex contexts. The outcome of their work will yield radical new theories of how nervous systems operate, as well as a diversity of tools for the neuroscience and computational communities.<br/><br/>This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE)."
"1659144","REU Site: Scientific Computing for Structure in Big or Complex Datasets","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/01/2017","02/13/2017","Burton Rosenberg","FL","University of Miami","Standard Grant","Joseph Maurice Rojas","02/28/2021","$360,000.00","","burt@cs.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","1139","9250","$0.00","This project provides for our future generation of effectively trained research scientists. The project anticipates future directions in science by training young scientists in the theoretical and practical use of computation to find, understand, and exploit structure in large or complex data sets. Large and complex data sets are ubiquitous in many areas of science, business and humanities. The project aims to broaden participation in scientific research by locating and encouraging talented students at institutions with fewer research offerings than has the REU site. Along with encouraging a career in science, the project aims to further engage each participant in a lifelong dialog with science and scientific research.<br/><br/>Training, mentorship, presentation, and peer engagement are all key elements addressed by this project to encourage our talented undergraduates to become research scientists. The project provides full scholarships to undergraduates to work with mentors in a ten week summer session on a project from the site's portfolio of projects. The program begins with a science boot camp. Throughout the experience the participants attend lecture series and engage with their cohort of REU participants, further facilitating growth as career scientists. At the conclusion, the participants present their research to their peers and the research community at the site, with some projects selected to be presented at a national conference. <br/><br/>The portfolio of mentored projects share the theme of computation to find structure in large or complex data sets. Projects feature computation either for the exploration or manipulation of data, such as for visualization or high performance GPU computation, or for the theoretical framework of computation, such as for understanding brain processes, deep learning, and neural nets. Because of the cross-cutting nature of computational science, the projects come from diverse fields with mentors from diverse departments, including Computer Science, Chemistry and Psychology, the Center for Computational Sciences, and the medical school."
"1753968","CAREER: Efficient Learning of Personalized Strategies","IIS","Robust Intelligence","08/01/2017","11/01/2017","Emma Brunskill","CA","Stanford University","Standard Grant","Roger Mailler","05/31/2020","$295,195.00","","ebrun@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","1045, 7495, 9251","$0.00","Online retailers frequently provide tailored product or movie recommendations. But the power of automated personalization, driven by data and statistics, could be far greater: imagine the impact on poverty reduction if all children had a personalized, self-improving tutoring system as part of their education. To realize this vision requires personalization systems that reason about both the immediate impact of a recommended item (e.g. will a learner immediately learn from a video lecture) as well as its longer term impact. For example, a recommended item or intervention may cause a user to change his/her preferences, state of knowledge, or reveal information about the user that was previously unknown. This requires methods for creating personalized strategies: adaptive rules about what decisions to make (whether or which ad to show, which pedagogical activity to provide) in which circumstances to maximize for long term outcomes. <br/><br/>This research involves developing new data-driven, machine learning approaches to construct such personalized strategies for related individuals, and using them towards improving the effectiveness of online mathematics educational systems.  The project frames personalized strategy creation as sequential decision making under uncertainty research. Though there have been many advances in sequential decision making under uncertainty, existing approaches have focused primarily on other application areas, like robotics, and fail to account or leverage for some of the special features that arise when interacting with people. These include that accurate simulation of people is difficult but prior data is often available, and that individuals are often related. This project contributes algorithms for mining existing datasets to create and precisely bound the expected performance of new high-quality strategies and for online policy learning across a series of similar sequential decision making tasks."
"1662968","Collaborative Research: Statistical Learning, Driving Simulator-Based Modeling, and Computationally Tractable Dynamic Traffic Assignment","CMMI","CIS-Civil Infrastructure Syst","08/01/2017","08/07/2017","Terry Friesz","PA","Pennsylvania State Univ University Park","Standard Grant","Yueyue Fan","07/31/2021","$246,049.00","","tfriesz@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","1631","036E, 1057","$0.00","Congestion is familiar to anyone who relies on a privately owned or rented automobile, taxi, or public transit for commuting, shopping and errand running. Historically, engineers and scientists exploring traffic networks frequently build mathematical models with the intent of coaxing from them insights revealing how congestion may evolve over time. Unfortunately, such models may easily become so large and complex that they are unwieldly, and simplifications are needed in order to provide passengers and drivers with accurate and rapidly computable information pertinent to route choice and departure time selection. Toward that goal, this project will employ modern statistics, simulation experiments, and notions of competition among traffic network users for available road capacity to better depict and more efficiently compute the behaviors of drivers who rely on road networks. The broader impacts of this research will be substantial. In particular, the results of this research will allow commuters and urban freight carriers to make more informed travel decisions, and governmental organizations to better regulate travel decisions within heavily congested major metropolitan regions. This study will also provide system-level experiential learning opportunities for students entering the transportation workforce. <br/><br/>Specifically, through a combination of experiments and machine learning and model development, this project will aim to depict the noncooperative exploration of available routes and departure times by drivers and passengers seeking to fulfill their travel demands via metropolitan road networks. A key goal of the intended research will be the efficient computation of solutions to the most prevalent type of dynamic traffic assignment (DTA), namely so-called dynamic user equilibrium (DUE). It is the lack of closed-form travel-delay operators that makes DUE computation tedious and slow. The plan is to replace the existing, differential algebraic equation (DAE) system representing travel delay with closed-form, approximate delay operators based on a form of statistical learning known as Kriging. Ad hoc experiments based on such an approach show great promise for small networks, but are not definitive. The PIs will develop the envisioned models and make developed software available as free-ware or inexpensive apps."
"1717368","III: Small: Robust Reinforcement Learning for Invasive Species Management","IIS","Info Integration & Informatics","08/15/2017","05/16/2018","Marek Petrik","NH","University of New Hampshire","Standard Grant","Sylvia Spengler","07/31/2021","$497,335.00","Jenica Allen","mpetrik@cs.unh.edu","51 COLLEGE RD SERVICE BLDG 107","Durham","NH","038243585","6038622172","CSE","7364","7364, 7923, 9150","$0.00","Invasive species cause significant ecological and economic damage in the US and worldwide. Mitigation of these problematic species is difficult because both treatments and surveillance are expensive. Fortunately, more computational tools, ecological information, and precise models are available than ever before. This project will leverage these advances to develop methods that can compute a new class of smart strategies for efficiently controlling invasive species. Such strategies must work well in face of the vast complexity of ecological systems and inherently limited observational data. Since an intervention to manage an invasive species can be very costly, yet have impacts that last years or decades, it is important to optimize treatments areas to mitigate risk. To manage these challenges, the project will develop methods that compute management strategies that are unaffected by the ecological complexities and data uncertainty. This research will also help to put a new class of data-driven management tools in the hands of land managers and decision makers. <br/><br/>Using data to optimize strategies for managing invasive species is a spatio-temporal optimization problem, which falls under the broader class of reinforcement learning. To tractably manage risk, the project will use the new methodology of robust optimization in the context of reinforcement learning. This research project will make four fundamental contributions that will advance the state of the art in methods for quantifying and mitigating uncertainty in complex data-driven decision-making. First, it will build a comprehensive and realistic dynamic system test-bed in which addressing uncertainty is paramount. This test-bed will constitute a dynamic mechanistic model of how invasive species thrive and spread. Second, it will develop practical algorithms for quantifying and modeling uncertainty due to imperfect observational data. The quantification algorithms will be based on insights to machine learning methods and the maximum entropy principle. Third, it will address model uncertainty which is due to the dynamic model simplifying reality. And fourth, it will develop new approaches to choosing a level of spatial aggregation to trade off between different error types."
"1738560","STTR Phase II:  User-Friendly Spirometer and Mobile App for Self-Management and Home Monitoring of Asthma Patients","IIP","STTR Phase II","09/15/2017","08/26/2019","Charvi Shetty","CA","KNOX Medical Diagnostics Inc.","Standard Grant","Henry Ahn","08/31/2022","$1,109,999.00","Ngoc Ly","charvi@aluna.io","175 Bluxome Street Unit 234","San Francisco","CA","941071552","4153200690","ENG","1591","1591, 165E, 8018, 8038, 8042, 8240","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase II project is the development of a user-friendly asthma management solution. The asthma management tool consists of a portable medical device and mobile app combination that measures lung function with the consistency and accuracy of a trained lab technician, displaying current asthma status, and providing health insights to act upon. During the 2-year duration of this proposal, the applicant will further develop a machine-learning algorithm that obtains the same level of consistency and accuracy as if a trained lab technician were coaching and correcting the asthmatic patient on proper usage. Steps will be taken to establish the efficacy of such technology through verification of classification by pulmonologists. In order to engage users to continue using the product over several months, gamification elements will be implemented into the mobile app. By expanding on the machine-model through a longitudinal study, earlier detection of asthma exacerbations may be identified. Early detection leads to improved self-management as measured by the reduction of severe asthma attacks, the use of systemic corticosteroids, hospitalizations, emergency department or urgent care visits related to asthma.<br/><br/>The proposed project aims to develop an asthma management tool that provides parents a simple way to reliably monitor their child's lung health, eliminating the guesswork associated with relying on symptoms alone. The rate of asthma continues to rise, with an increasing amount of healthcare utilization among asthmatic individuals. Effective technologies for proper management remain trapped within the hospital due to high costs and requirement of a skilled lab technician for proper measurement collection. This proposal aims to develop an algorithm alongside UCSF pediatric pulmonologists to gain consistent and accurate spirometry measurements, so that only proper measurements are acted upon. Our machine-learning algorithm will determine the cause of failure and prompt the user on corrective action to achieve a good quality measurement on a subsequent effort. By the end of this proposal period, the applicant will have a mobile app algorithm that is able to achieve the consistency of a trained lab technician, provide effective corrective feedback, engage users over the span of months for consistent lung monitoring, and potentially predict the onset of an asthma attack."
"1737732","EAGER: SC2: Load Prediction and Collision Coordination for Collaboration Channel","CNS","Information Technology Researc","04/01/2017","12/22/2017","Joseph Camp","TX","Southern Methodist University","Standard Grant","Monisha Ghosh","01/31/2018","$36,144.00","","camp@lyle.smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","CSE","1640","7363, 7916","$0.00","The demand for wireless bandwidth has grown exponentially in the past decade, motivating the need for novel spectrum access techniques. In this project, a hierarchical testing and implementation approach is used to generate and evaluate innovative spectral access techniques. To do so, the team is equipped with a broad range of backgrounds from FPGA development and machine learning to 3GPP standardization and extensive in-field experimentation. There are four particular areas of focus for the team: (i) network management including control structure and topology discovery, (ii) network discovery including modulation recognition and network recognition, (iii) spectrum access including channel selection, access mechanism design, and decision metrics, and (iv) link design including waveform selection, channel coding, and channel estimation. A central concept hinges on the role of the collaboration channel, on which the role of periodic and uniform levels of information exchange is studied related to channel availability across networks from no, partial, and full knowledge scenarios.  Using machine learning and on-the-fly training with observations of network decisions and resulting performance, the focus then shifts to designing for robustness with respect to greater levels of latency and heterogeneity that result from non-standard protocols and algorithms across a diverse set of radios.<br/><br/>The scope of the project includes the following three aspects.  First, an agile research approach is employed using a hierarchy of implementation complexity to evaluate multiple design ideas, progressing through simulation tools, software-defined platforms, and FPGA hardware implementations and weighting the time spent according to the measured success of each design idea. Second, the role of the collaboration channel is extensively studied and evaluated in terms of spectral efficiency and coordination across teams by attempting to predict spectral availability of all users across the relevant spectrum in a homogeneous network context and select the channel availability update rate and size to attempt to maximize performance of these two metrics. Third, these findings develop the basis for extension to networks with high levels of latency and heterogeneity across the network stack where flexibility of information exchange across the collaboration channel is far more critical, forcing on-the-fly training that are built into the nodes to observe and reinforce machine-based decision making."
"1730128","II-EN: Collaborative Research: Enhancing the Parasol Experimental Testbed for Sustainable Computing","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2017","06/08/2017","Anshul Gandhi","NY","SUNY at Stony Brook","Standard Grant","Marilyn McClure","06/30/2021","$24,215.00","Zhenhua Liu","anshul@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7359","7359","$0.00","This project will enhance an experimental datacenter for sustainable computing. Datacenters consume vast amounts of energy, totaling about 1.8% of the US electricity usage in 2014. Thus, the energy efficiency, energy-related costs, and overall sustainability of datacenters are of critical concerns. NSF funded experimental green datacenter called Parasol has previously demonstrated that the combination of green design and intelligent software management systems can lead to significant reductions in energy consumption, carbon emission, and cost. The enhanced version of this project will update energy sources, network technologies and management software.<br/><br/>Running real experiments in live conditions using Parasol led to findings that were not possible in simulation. This proposal seeks to update and enhance Parasol with current and next generation power-efficient servers, improve network connectivity and integrate software-defined networking (SDN) and Wi-Fi capabilities, increase solar energy generation capacity, add a low emission fuel cell power source, diversify energy storage, and improve the cooling system to advance green computing. The PIs will need to update and enhance Parasol's current software stack for monitoring, programmatic control, and remote access for the new hardware enhancements. Specific research goals are resource management in green datacenters, that includes coordinated workload, cooling, and energy scheduling against environmental and load variability to maximize the benefits of green datacenters and to help improve grid power management leveraging accelerators such as GPUs and deep learning hardware, which promise excellent performance/watt ratios."
"1710923","Data-driven adaptive robust operation of PV generation in distribution systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/01/2017","06/26/2017","Mohammad Khodayar","TX","Southern Methodist University","Standard Grant","Anthony Kuh","07/31/2021","$315,727.00","","mkhodayar@smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","ENG","7607","155E","$0.00","The objective of this research is to develop a novel data-driven decision support system (DSS) to determine efficient short-term operation strategies for accommodating large-scale PV generation and mitigating its adverse effects on distribution network reliability and security. The proposed DSS will 1) improve the spatiotemporal variability and uncertainty quantification for PV generation in distribution networks; 2) determine the accommodated variability and uncertainty boundaries of PV generation to ensure the economic efficiency and security of the distribution networks; 3) propose cost effective dynamic solutions that incorporate the temporal and spatial variability and uncertainty of demand and supply in the distribution networks; 4) capture the interactions among autonomous entities such as microgrids, distributed energy resources (DERs), and controllable demands; with distribution system operator (DSO). This research plan facilitates rapid dissemination of the generated knowledge to the research and education community. Specifically, it promotes innovative collaboration among graduate and undergraduate students to provide effective solutions for the current challenges in the distribution network operation. This project ensures the highest quality of integrated research and education to meet the emerging workforce and educational needs of the U.S. energy sector by introducing new curriculum for undergraduate and graduate programs, promoting interdisciplinary collaboration, recruiting underrepresented minorities and female students, and developing K-12 outreach activities.<br/><br/>The specific objectives of this research are as follows. a) develop a scalable data-driven approach that leverages a multi-task deep learning framework to provide improved spatiotemporal uncertainty measures for the large-scale PV generation in the distribution network. b) quantify the flexibility measures as tertiary regulation services and form distributionally adaptive robust optimization problems to quantify the accommodated spatiotemporal variability and uncertainty. c) provide a tight convex relaxation for the non-convex risk-averse short-term operation problem for the unbalanced distribution networks. The non-convexity in feasibility set is as a result of the introduced integer variables for switching and commitment decisions as well as the unbalanced AC power flow constraints. d) develop decentralized optimization framework to capture the spatial interdependence among the dynamic temporal decisions made by the autonomous entities and the DSO."
"1727459","The Power of Images: A Computational Investigation of Political Mobilization via Social Media","SES","Political Science","08/01/2017","07/19/2018","John Wilkerson","WA","University of Washington","Continuing grant","Brian Humes","07/31/2020","$301,577.00","","jwilker@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","SBE","1371","","$0.00","General Abstract<br/>Social media have been described by some scholars as a ""weapon of the weak."" Any person in the world with a mobile device and network connection can now communicate with a potential audience of unlimited size. This project proposes the first large-scale study of the roll of images in the social media communications of advocacy organizations, social protest movements, and extremist militant groups. The team will collect social media communications in real time and exploit recent ""deep learning"" advances from computer science to automatically label millions of images for objects (crowds, animals, weapons etc.), entities (specific actors), and emotions evoked (fear, sadness, humor, etc.).  The team seeks to assess whether, all else equal, these images promote political message engagement and diffusion.<br/><br/>Technical Abstract<br/>Research in psychology finds that images are easier for individuals to process cognitively, and have a larger impact on consciousness than do words.   Images should be especially important tools for traditionally marginalized groups that rely on outsider strategies to promote issue awareness and their views. This is new ground however, as there is no theory and no databases for studying the mobilizing effects of political social media communications. The research team will construct new social media databases for a wide range of political groups. The project will contribute new labeled political image collections to existing computer science image repositories. The team also proposes to offer national workshops on advanced image classification methods for social scientists, and will disseminate research findings to scholars and practitioners through conference presentations and peer-reviewed publications."
"1735740","EXP:  Collaborative Research:  Cyber-enabled Teacher Discourse Analytics to Empower Teacher Learning","IIS","ECR-EHR Core Research","09/01/2017","08/18/2017","Patrick Donnelly","CA","Chico State Enterprises","Standard Grant","John Cherniavsky","02/29/2020","$50,000.00","","pjdonnelly@csuchico.edu","Office of Sponsored Programs","Chico","CA","959290001","5308986811","CSE","7980","8045, 8841","$0.00","This project will use multiple sources of middle school classroom data to give feedback and assessment information to teachers so that their teaching ability is enhanced. The data includes anonymized student performance data (grades and standardized test results) and anonymized existing audio recordings of classroom discussions between students and teachers. The audio data will be used to analyze the student-teacher discussions for effectiveness of the student-teacher discussions in student learning. As the effectiveness measures are developed, feedback for instructional improvement will be provided to the teachers in a design cycle for continuous improvement. The technological innovations are in the analysis of the student-teacher discussions, in natural language understanding of student-teacher discussions, and in machine learning to classify effective from non-effective student-teacher discussions.<br/><br/>This project will advance cyber-enabled, teacher analytics as a new genre of technology that provides automated feedback on teacher performance with the goal of improving teaching effectiveness and student achievement. The exemplary implementation will autonomously analyze audio from real-world English and language arts classes for indicators of effective discourse to enable a new paradigm of datadriven reflective practice. The project emphasizes six theoretical dimensions of discourse linked to student achievement growth: goal clarity, disciplinary concepts, and strategy use for teacher-led discourse, and challenge, connection, and elaborated feedback for transactional discourse. The innovation aims to help teachers develop expertise on these dimensions and will be developed and tested in 9th grade classrooms in Western Pennsylvania. The team will first generate initial insights on how teacher discourse predicts student achievement via a re-analysis of large volumes (128 hours) of existing classroom audio. Next, they will design and iteratively refine hardware/software interfaces for efficient,flexible, scalable audio data collection by teachers. The data will be used to computationally model dimensions of effective discourse by combining linguistic, discursive, acoustic, and contextual analysis ofaudio with supervised and semi-supervised deep recurrent neural networks. The model-based estimates will be incorporated into an interactive analytic/visualization platform to promote data-driven reflective practice. After refinement via design studies, the impact of the innovation on instructional improvement and student literacy outcomes will be evaluated in a randomized control trial. Finally, generalizable insights will be identified at every stage of the project to promote transferability to future cyber-enabled,teacher-analytics technologies."
"1730126","CI-P: Physical robotic manipulation test facility","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2017","04/18/2018","Cindy Grimm","OR","Oregon State University","Standard Grant","Roger Mailler","11/30/2019","$115,699.00","Ravi Balasubramanian","grimmc@onid.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","1714, 7359","7359, 9251","$0.00","The robotics community has a rich history of research and development in teaching and training robots to pick up and manipulate objects. However, it has proven to be very difficult to transition this research from structured laboratory settings to real world settings, such as homes, small-scale industrial settings, and search and rescue domains. The development of large-scale, standardized testing and benchmarking of robotic manipulation approaches is necessary to move robotic manipulation from the research lab to the real world. Placing the burden of conducting these tests on every individual robotics researcher is inefficient, at best. Cleaning the captured data to make it available to other researchers requires additional work for the producers of the data - and even more work on potential users. All of this impedes progress for the community as a whole, and makes it difficult to bring to bear recent developments in deep learning. This project addresses this problem by setting the groundwork for a dedicated physical robotic grasping and manipulation testbed infrastructure that can be remotely accessed and operated by anyone doing research in this area.  <br/><br/>This testbed will provide several critical components that the robotics grasping and manipulation community needs: (i) ""Test suites"" that enable repeated testing and controlled manipulation of several variables that have confounded robotic grasping and manipulation research.  Variables include object and gripper material properties, compliance, force/torque of physical interaction, mass of manipulator elements and the objects, surface texture, low-level control algorithms, and higher-level planning techniques. (ii) Extensive instrumentation to capture (nearly) all aspects of the physical interaction, such as the forces, kinematics of movement, and three-dimensional geometry.  (iii) A modular and customizable human-robot interface for enabling robotic physical interaction.  Users will be able to directly control the robot using low-level interfaces, such as the knobs that control the movements of individual joints, or use  higher-level interfaces that encapsulate robot-object interactions, such as close the fingers until they contact the object.  (iv) Data collection, where the data will be made publicly available in a standardized form. The focus of this planning activity is to develop the necessary technological elements to prove the feasibility of such a test facility (automated object return in a fully instrumented space, a fully instrumented ""door"" to evaluate opening and closing doors) and to evaluate the community's needs in this area."
"1746031","EAGER: Joint Modeling and Querying of Social Media and Video Data","IIS","Info Integration & Informatics","09/01/2017","11/07/2018","Evangelos Christidis","CA","University of California-Riverside","Standard Grant","Maria Zemankova","03/31/2020","$232,000.00","Vassilis Tsotras, Amit Roy Chowdhury, Evangelos Papalexakis","evangelos.christidis@ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7364","7364, 7916, 9251","$0.00","As the amount of user generated data increases, it becomes more challenging to effectively search this data for useful information. There has been work on how to search text social media posts, such as Tweets, or videos.  However, searching on these sources using separate tools is ineffective because the information links between them are lost; for instance, one cannot automatically match social network posts with activities seen on a video. As an example, consider a set of tweets and videos (which may be posted on Twitter or other media) generated during a riot. A police detective would like to jointly search this data to find material related to a specific incident like a car fire. Some tweets (with no contained video) may comment on the car fire, while a video segment from another tweet shows the car during or after the fire. Linking the videos with the relevant social media posts, which is the focus of this project, can greatly reduce the effort in searching for useful information. The successful completion of this project has the potential to improve the productivity of people who search in social media, such as police detectives, journalists of disaster management authorities. This project will also strengthen and extend the ongoing undergraduate research and high school outreach activities of the investigators.<br/><br/>The objective of this project is to focus on the fundamental research tasks that would allow for joint modeling of social network and video data. Then, given a set of posts, the system would find relevant video segments and vice versa, by defining a common feature space for social media and video data. This proof-of-concept project will be evaluated on posts and videos shared on the Twitter platform.  This is the right time to tackle this problem given the recent advances in deep learning and big data management technologies. A key risk is that the semantics in a tweet may not be enough to map it to a video segment; for that, the context (e.g., tweets from closely related users) of the tweet may need to be leveraged."
"1744401","Convergence HTF: Workshop on Converging Human and Technological Perspectives in Crowdsourcing Research","IIS","INSPIRE","09/15/2017","08/23/2017","Heng Xu","PA","Pennsylvania State Univ University Park","Standard Grant","Meghan Houghton","12/31/2018","$49,124.00","Gautam Das, Senjuti Basu Roy","xu@american.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8078","060Z, 063Z","$0.00","Intelligent, interactive, and highly networked machines -- with which people increasingly interact -- are a growing part of the landscape, particularly in regard to work.  As automation today moves from the factory floor to knowledge and service occupations, research is needed to reap the benefits in increased productivity and increased job opportunities, and to mitigate social costs.  The workshop supported by this award will promote the convergence of computer science, data management, machine learning, education, and the social and behavioral sciences to define key challenges and research imperatives of the nexus of humans, technology, and work.  Convergence is the deep integration of knowledge, theories, methods, and data from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. This convergence workshop addresses the future of work at the human-technology frontier.<br/><br/>The specific focus of this workshop is on crowdsourcing -- the production of networked knowledge from public participation.  This is a new area of research, gaining attention from researchers who study human-computer interactions, data management, machine learning, human behavior, and business.  This workshop will bring together researchers from these and other relevant communities to (1) synthesize the diverse perspectives found in these different fields, (2) integrate different knowledge, theories and data to create a transdisciplinary and convergent research roadmap, and (3) catalyze new research directions and advance scientific discovery and innovation in crowdsourcing research.  The workshop will also contribute toward broadening participation in this area of research by proactively seeking inclusion of traditionally underrepresented persons."
"1745451","EAGER: SSDIM: Simulated and Synthetic Data Generation for Interdependent Natural Gas and Electrical Power Systems Based on Graph Theory and Machine Learning","CMMI","Special Initiatives, , ","09/01/2017","05/29/2019","Zhaoyu Wang","IA","Iowa State University","Standard Grant","Walter Peacock","08/31/2021","$400,000.00","","wzy@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","1642, Q231, R243","036E, 041E, 042E, 1057, 7916","$0.00","Natural gas and electric power systems have become the backbone of the U.S. energy infrastructure. This EArly-concept Grant for Exploratory Research (EAGER) project will investigate practical data-based approaches for producing simulated and synthetic datasets that faithfully represent the interdependence between the two critical infrastructure systems from mechanistic and human aspects. The project contributes to the grand national challenge of modernizing energy systems by laying the data foundation for future research in interdependent critical energy infrastructures. The research results will lead to publications as well as multi-disciplinary training opportunities that integrate data analysis and energy engineering for graduate and undergraduate students. By forging strategic alliances with the utilities in the Midwest and national laboratories, webinars on natural gas and power network data analysis will be given to a broad array of engineers and researchers on the results of this work. <br/><br/>The project will pioneer data-based approaches to understand and model the interdependence between natural gas and power networks. The interactive data generation method provides high-fidelity datasets with different spatial-temporal granularities and operation conditions. In particular, mechanistic principles and human impacts inherent in gas-electric systems are identified from practical data using graph-based and learning-based approaches. These generated datasets will be validated using practical data, and be available online through a project website, together with a list of use cases that leverage the data and existing modeling approaches to advance the understanding of gas/power network interdependence in terms of strong/weak coupling effects, economic operations, cascading outages, etc. The project promotes an interdisciplinary effort in science and technology from data analysis, graph theories, complex networks, as well as power and natural gas engineering to provide fundamental knowledge about synthetic data generation for critical interdependent infrastructures."
"1712591","Robust and Distributed Statistical Learning from Big Data","DMS","STATISTICS","07/15/2017","05/02/2020","Jianqing Fan","NJ","Princeton University","Continuing Grant","Gabor Szekely","06/30/2022","$480,000.00","","jqfan@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","MPS","1269","8083","$0.00","Big Data are ubiquitous in many areas of science, engineering, social sciences, and the humanities, and have significant impact in terms of technological innovation and economic development.    This project seeks to introduce effective methods for robust high-dimensional statistical inference that are insensitive to the potential poor quality of big data, and to develop distributed estimation that is needed for Big Data analysis, computing, and optimization.  The research will address several robust and distributed statistical inference problems for Big Data in genomics, genetics, neuroscience, machine learning, economics, and finance.   The project will advance our understanding of molecular mechanisms, biological processes, genetic associations, brain functions, and economic and financial risk.  Integration of research and education will be achieved through the involvement of undergraduate students, graduate students, and postdoctoral fellows, and the development of publicly available computer code for robust and distributed analysis of Big Data with sound theoretical support.  Working closely with industrial partners, the research will lead to increased collaborations between academia and industry.  <br/><br/>The project will lead to the development of novel statistical theory, methods, and algorithms for robust statistical inference from high-dimensional statistics and Big Data.  The first aim seeks to introduce a simple and widely applicable principle for robust inference via an appropriate shrinkage of observed data or loss functions.  This reduces the influence of outliers and heavy-tailed distributions, and weakens the moment conditions from sub-Gaussian distributions to bounded second moments for regression or fourth moments for covariance estimation.  The research includes plans to systematically develop the theory and methods for robust estimation of high-dimension means, and implementation of these methods to control false discovery rates in large scale inference for gene and transcripts selection, robust regularization of covariance and precision matrices, and their applications to robust principal component analysis, factor analysis and high-dimensional hypothesis testing.  In addition, robust sparse regression, model selection, and low-rank matrix recovery will also be investigated.  The second aim focuses on making the proposed robust procedures applicable to the Big Data environment via the development of distributed estimation and inference.  In particular, divide-and-conquer methods will be used to distribute the computation to node machines and to solve privacy and data ownership issues.  Approaches to reduce the information loss due to the distributed computation for likelihood based models via partial communication of the Hessian matrices will be investigated.   Two important classes of problems, trace regression and principal component analysis, will be used to illustrate the proposed methods."
"1723998","CRCNS US-German-Israeli Collaborative Research Proposal: Hierarchical Coordination of Complex Actions","BCS","Perception, Action & Cognition, CRCNS-Computation Neuroscience, Dynamics, Control and System D","09/01/2017","08/14/2017","Dagmar Sternad","MA","Northeastern University","Standard Grant","Betty Tuller","10/31/2021","$319,919.00","","d.sternad@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","SBE","7252, 7327, 7569","014Z, 030E, 034E, 7252, 7327, 7569, 8024, 8089, 9251","$0.00","There are many arenas where humans outperform machines. For example, when coordinated interaction with the physical environment is needed, humans (and animals) vastly out-perform modern robots.  This occurs despite the biological systems having far slower 'hardware' and 'wetware' and much greater complexity than even the most modern robots. This research project seeks to understand the role of complexity in human sensory and motor performance. Human walking under challenging balance conditions will be studied and the use of canes to enhance stability will be included.  The investigators emphasis on learning to balance in challenging environments should lead to new rehabilitation therapies (with or without robotic assistance) to aid recovery of balance and walking (e.g., after stroke). The researchers will create educational units suitable for online presentation to K-12 students and will devise exhibits based on their research for the Museum of Science in Boston.<br/><br/>The central hypothesis to be tested in this project is that complex movements involving physical interaction with objects are organized as a hierarchy formed of modules or primitives. Experiments will study how unimpaired humans learn to walk on narrow beams. Beams of different roundness will vary the challenge. Hand-held canes will alter the available support (like training wheels on a child's bicycle). Computer simulations combined with machine learning will study the benefits and drawbacks of organization as a hierarchy. New mathematical tools will be developed and tested to see if they enable insightful description of human performance in challenging conditions. The research involves a multinational collaboration among scientists from the U.S., Israel, and Germany, each with complementary expertise. The bridge between experimental and theoretical work and the diverse Principal Investigators will help to attract women into the traditionally male-dominated fields of computational neuroscience, robotics and control engineering.<br/><br/>Companion projects are being funded by the Federal Ministry of Education and Research, Germany (BMBF) and the US-Israel Binational Science Foundation (BSF)."
"1653435","CAREER: Stochastic Nested Composition Optimization: Theory and Algorithms","CMMI","OE Operations Engineering, CAREER: FACULTY EARLY CAR DEV","02/01/2017","01/19/2017","Mengdi Wang","NJ","Princeton University","Standard Grant","Georgia-Ann Klutke","01/31/2022","$500,000.00","","mengdiw@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","ENG","006Y, 1045","073E, 077E, 078E, 1045, 8023, 9102","$0.00","The objective of this Faculty Early Career Development (CAREER) award is to develop foundational theory and efficient computational tools for an important class of data-driven stochastic optimization problems, stochastic nested composition optimization. These nested composition problems arise in many areas such as risk management, machine learning, and online decision making, and are not amenable to classical methods of stochastic optimization. On the theory and methodology side, the project will advance both optimization and data analytics. On the education side, the project will result in curricular innovation that integrates optimization and data analytics in a unified way and offers new case studies on practical applications. The project will promote underrepresented minority students by involving them in frontier research. The research will produce new algorithms and analysis tools that will be useful for many data-intensive applications.  These methods will be empirically tested in a collaborative project with a local healthcare system, with the goal of improving healthcare delivery by reducing cost and improving quality of service. <br/><br/>Stochastic nested composition optimization constitutes a new class of stochastic optimization problems that involve nested nonlinear composition of multiple expectations and multi-level random variables. This project will (a) establish the basic complexity theory for two-level and multi-level stochastic nested composition optimization and their generalizations; (b) develop efficient algorithms that process streaming data with theoretical guarantees; (c) investigate several special cases of the problem and apply the results to modeling and optimizing healthcare decisions based on real clinical data (obtained through the collaboration with a NJ-based hospital chain); and (d) develop innovative curricula and research projects that can bring students at various levels to frontier technology. The nested composition provides a rich modeling tool for applications that require data-driven decision-making and optimization under uncertainty. A critical challenge is that the objective is no longer a linear functional of the data distribution, and thus existing theory and methods are inappropriate. The nonlinearity with respect to the distribution of data makes the problem fundamentally more difficult than most of the classical problems. Overcoming the analytical challenge calls for an integration of mathematical programming and stochastic analysis. If successful, the research will make a substantial contribution by expanding the scope of stochastic optimization. Theoretically, it will strengthen our mathematical understanding of stochastic optimization and establish foundational sample complexity bounds. Methodologically, it will provide new algorithms and analysis tools for several important problems in data-driven optimization and online learning. The results will establish important connections among several areas in mathematical programming, statistics, and machine learning."
"1735785","EXP:  Collaborative Research:  Cyber-enabled Teacher Discourse Analytics to Empower Teacher Learning","IIS","ECR-EHR Core Research","09/01/2017","08/18/2017","Sean Kelly","PA","University of Pittsburgh","Standard Grant","Amy Baylor","08/31/2021","$236,928.00","Amanda Godley","spkelly@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7980","8045, 8841","$0.00","This project will use multiple sources of middle school classroom data to give feedback and assessment information to teachers so that their teaching ability is enhanced. The data includes anonymized student performance data (grades and standardized test results) and anonymized existing audio recordings of classroom discussions between students and teachers. The audio data will be used to analyze the student-teacher discussions for effectiveness of the student-teacher discussions in student learning. As the effectiveness measures are developed, feedback for instructional improvement will be provided to the teachers in a design cycle for continuous improvement. The technological innovations are in the analysis of the student-teacher discussions, in natural language understanding of student-teacher discussions, and in machine learning to classify effective from non-effective student-teacher discussions.<br/><br/>This project will advance cyber-enabled, teacher analytics as a new genre of technology that provides automated feedback on teacher performance with the goal of improving teaching effectiveness and student achievement. The exemplary implementation will autonomously analyze audio from real-world English and language arts classes for indicators of effective discourse to enable a new paradigm of datadriven reflective practice. The project emphasizes six theoretical dimensions of discourse linked to student achievement growth: goal clarity, disciplinary concepts, and strategy use for teacher-led discourse, and challenge, connection, and elaborated feedback for transactional discourse. The innovation aims to help teachers develop expertise on these dimensions and will be developed and tested in 9th grade classrooms in Western Pennsylvania. The team will first generate initial insights on how teacher discourse predicts student achievement via a re-analysis of large volumes (128 hours) of existing classroom audio. Next, they will design and iteratively refine hardware/software interfaces for efficient,flexible, scalable audio data collection by teachers. The data will be used to computationally model dimensions of effective discourse by combining linguistic, discursive, acoustic, and contextual analysis of audio with supervised and semi-supervised deep recurrent neural networks. The model-based estimates will be incorporated into an interactive analytic/visualization platform to promote data-driven reflective practice. After refinement via design studies, the impact of the innovation on instructional improvement and student literacy outcomes will be evaluated in a randomized control trial. Finally, generalizable insights will be identified at every stage of the project to promote transferability to future cyber-enabled, teacher-analytics technologies."
"1735793","EXP:  Collaborative Research:  Cyber-enabled Teacher Discourse Analytics to Empower Teacher Learning","IIS","ECR-EHR Core Research","09/01/2017","08/18/2017","Sidney D'Mello","CO","University of Colorado at Boulder","Standard Grant","Amy Baylor","08/31/2021","$262,466.00","","sidney.dmello@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7980","8045, 8841","$0.00","This project will use multiple sources of middle school classroom data to give feedback and assessment information to teachers so that their teaching ability is enhanced. The data includes anonymized student performance data (grades and standardized test results) and anonymized existing audio recordings of classroom discussions between students and teachers. The audio data will be used to analyze the student-teacher discussions for effectiveness of the student-teacher discussions in student learning. As the effectiveness measures are developed, feedback for instructional improvement will be provided to the teachers in a design cycle for continuous improvement. The technological innovations are in the analysis of the student-teacher discussions, in natural language understanding of student-teacher discussions, and in machine learning to classify effective from non-effective student-teacher discussions.<br/><br/>This project will advance cyber-enabled, teacher analytics as a new genre of technology that provides automated feedback on teacher performance with the goal of improving teaching effectiveness and student achievement. The exemplary implementation will autonomously analyze audio from real-world English and language arts classes for indicators of effective discourse to enable a new paradigm of datadriven reflective practice. The project emphasizes six theoretical dimensions of discourse linked to student achievement growth: goal clarity, disciplinary concepts, and strategy use for teacher-led discourse, and challenge, connection, and elaborated feedback for transactional discourse. The innovation aims to help teachers develop expertise on these dimensions and will be developed and tested in 9th grade classrooms in Western Pennsylvania. The team will first generate initial insights on how teacher discourse predicts student achievement via a re-analysis of large volumes (128 hours) of existing<br/>classroom audio. Next, they will design and iteratively refine hardware/software interfaces for efficient,flexible, scalable audio data collection by teachers. The data will be used to computationally model dimensions of effective discourse by combining linguistic, discursive, acoustic, and contextual analysis of audio with supervised and semi-supervised deep recurrent neural networks. The model-based estimates will be incorporated into an interactive analytic/visualization platform to promote data-driven reflective practice. After refinement via design studies, the impact of the innovation on instructional improvement and student literacy outcomes will be evaluated in a randomized control trial. Finally, generalizable insights will be identified at every stage of the project to promote transferability to future cyber-enabled, teacher-analytics technologies."
"1738440","SBIR Phase II:  DATA-DRIVEN DECISION SUPPORT FOR EFFICIENT PATIENT PROGRESSION","IIP","SBIR Phase II","09/15/2017","07/23/2020","Eric Hamrock","MD","Stocastic, LLC","Standard Grant","Alastair Monk","07/31/2021","$809,981.00","","info@stocastic.com","629 S. Belnord Ave.","Baltimore","MD","212243804","410848750","ENG","5373","096Z, 169E, 5373, 8018, 8023, 8032, 8042, 8240","$0.00","The broader impact/commercial potential of the Small Business Innovation Research (SBIR) Phase II is to reduce the patient harm and financial burden created by the intensifying problem of emergency department (ED) crowding.  ED crowding is a threat to patient safety for high acuity patients and has been associated with avoidable morbidity and mortality across many conditions.  Concurrently, EDs are challenged to efficiently manage large volumes of low-acuity patients visiting with non-emergency conditions. The proposed electronic health record (EHR) integrated technology deploys novel machine learning algorithms that predict clinical events at actionable time-points in patients care pathways.  Characterizing the ED as a flow system, these decision support tools will concentrate on the root causes that exist at ED inflow and hospital outflow. This new foresight is expected to enable innovative hospital operational models that expedite patient progression (minimize patient waiting), improve patient safety, and directly translate to measurable cost-savings and/or revenue generation. The SBIR Phase II project should result in higher yield and scalable decision support technology while promoting the value of data-enabled science and engineering in healthcare. <br/><br/> <br/><br/>The proposed project objective is to greatly reduce ED crowding by advancing the data-science, decision-science, and operations research that underpins our decision support technology. This new technology is based upon a novel combination of data normalization, feature selection, and supervised machine learning methods to predict clinical events that drives clear action to optimize hospital resources.  This includes a decision support tool that functions at ED triage (inflow) to predict risk of critical events to empower safe separation of service streams for acutely ill and non-urgent patients. A complementary tool functions near hospital discharge (outflow) to predict expected discharge time enabling hospital-wide prioritization of resources required to expedite discharge. This unlocks downstream capacity and removes a major ED outflow bottleneck that creates prolonged waiting. The proposed technology is innovative by design to be scaled, yet adaptive to hospitals individual patient populations, operational objectives, and risk tolerances. The technology is expected to further advance learning with providers about how to consume new predictive and explanatory information for decision support.  The specific Phase II objectives are to continue the development of the technology by: (1) integrating feedback into prediction output, (2) developing performance monitoring capabilities, and (3) driving systems-based management of patient progression."
"1659833","REU Site:  Summer Undergraduate Program in Engineering Research at Berkeley (SUPERB):  Collecting and Using Big Data for the Public Good","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2017","01/17/2017","James Demmel","CA","University of California-Berkeley","Standard Grant","Joseph Maurice Rojas","01/31/2020","$259,200.00","","demmel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1139","9250","$0.00","It is evident to all of us that the widespread and growing availability of vast amounts of real-time data presents both a tremendous opportunity to improve our understanding of the world and make better automated decisions, as well as a great technical challenge to collect, communicate and process this data efficiently and reliably. Leading researchers in Electrical Engineering and Computer Science will mentor undergraduate students on their proposed REU projects. Potential impacts range from more efficient sensor networks, to improved wireless access to the data they produce, to better human robot interaction, to better analysis of medical data from microscopes, and even helping prevent nuclear war. The project seeks to address a multitude of societal problems that can be examined by collecting and using big data while inspiring students to dedicate themselves to this work. The project is committed to exposing a diverse group of undergraduate researchers that will expand the impact of this project and the engineering research pipeline. At the conclusion of the program, participants will be proficient in using big data to solve societal problems that have direct impacts on their communities. <br/><br/>The goal of the Summer Undergraduate Engineering Research project in the Electrical Engineering and Computer Sciences Department is to prepare and motivate a group of diverse competitive candidates for graduate student.  The focus of the REU site is electrical engineering and computer science to support collecting and using big date for the public good.  Students spend nine weeks during the summer working on high caliber research projects addressing technical challenges arising in collecting, communicating and processing the vast amounts of data becoming available both efficiently and reliably. This project covers the entire range of challenges and opportunities, from better sensor networks to collect this data more efficiently; to improved wireless resource management to move the data; to machine learning techniques for processing the images, including from microscopes, that make up much of the new data; to designing robots that can learn to interact better with humans based on the data they collect; to better enforcement of the Comprehensive Nuclear Test Ban Treaty by analyzing seismic data used to detect underground nuclear tests. The project will have research contributions in areas such as communications and networking, human computer interaction, machine learning, robotics, scientific computing and visualization."
"1718258","III: Small: A New Perspective on Grouped Variable Selection via Modern Optimization","IIS","Info Integration & Informatics","09/01/2017","08/16/2017","Rahul Mazumder","MA","Massachusetts Institute of Technology","Standard Grant","Wei Ding","08/31/2021","$318,000.00","","rahulmaz@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7364","7364, 7923","$0.00","This project will investigate new statistical learning methods for grouped variable selection problems that require addressing spatial proximity as well as physical, structural, and temporal constraints in data. For example, in genetic studies, it is often known that a group of genes in the same genetic pathway behaves as a group; in neuroscience applications, spatially contiguous regions of the brain often behave as homogeneous units; in industrial applications, with categorical covariates, a factor with multiple levels is often treated as a single unit. The project will design new statistical learning methods based on mathematical optimization that broadens the paradigm of disciplined statistical and computational modeling for grouped variable selection problems. The research will also involve mentoring of graduate students and collaborations with industrial partners. The project will involve curriculum development and the creation of software for public use. <br/><br/>The project will explore computational methods based on mixed integer optimization to address the grouped variable selection problem. While convex relaxation based procedures and greedy methods have played a significant role in this problem, the power and versatility of mixed integer optimization methods have been largely unexplored. The project will investigate this new direction, leveraging the advances in this field of mathematical optimization over the past ten to fifteen years. Successful execution of this project will create new tools, significantly enriching a statistician/machine learner's toolkit of interpretable models with principled computational and statistical properties. The project will investigate possible gains in statistical performance by using advanced computational methods over popularly used, computationally friendlier alternatives. The research will explore fundamental connections of the new approaches with existing methods. The research quest will stimulate activity at the intersection of machine learning, statistics, operations research, mathematical optimization and the applied domains. Software will be developed for the methods proposed."
"1652303","CAREER: Efficient Fine-grained Algorithms","CCF","Algorithmic Foundations","02/01/2017","02/11/2020","Barna Saha","MA","University of Massachusetts Amherst","Continuing Grant","A. Funda Ergun","01/31/2022","$432,034.00","","barna@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7796","1045, 7926","$0.00","Ever since the inception of computation, the fundamental question has been- what problems can be solved by computers? And which ones can be solved in a reasonable time? This brought in the notion of polynomial time solvable problems which are considered efficient vs NP-hard problems which take prohibitively long time to solve. The field of approximation algorithms, along with the theory of inapproximability, deals with which of the NP-hard problems can be solved efficiently by allowing approximate solutions. However, this crude distinction of algorithmic efficiency--polynomial vs NP-hard, is insufficient when handling today's large scale of data. We need a finer-grained design and analysis of algorithms that pinpoints the exact exponent of polynomial running time, and a better understanding of when a speed-up is not possible. Unfortunately, except for a few problem-specific innovations, the study of such algorithms is deeply lacking in the literature.<br/><br/>This project targets to build a unified theory of fine-grained algorithm design to study fast approximation algorithms, and their fine-grained hardness. Developing systematic techniques that emphasize on the trade-offs between running time, approximation and randomness, and aid in designing low-complexity parallel algorithms will significantly improve the state of the art. Moreover, motivated by core machine learning applications, the project proposes an alternate model of efficiency via classical query complexity. Tools from classical query complexity have previously inspired development of fast algorithms and vice versa; the PI expects similar connections to happen in this project. Elements of this endeavor will be integrated with new courses, many of the algorithms developed herein will be implemented and the close connection of the PI with industry will result in possible adaptation of methodologies.<br/><br/>The project will address a suite of important optimization problems, from long-standing open questions to modern problems with diverse applications. It will introduce fresh tools from additive combinatorics, fourier analysis, rate distortion theory, and circuit complexity for analysis of algorithms, and establishing lower bounds. Specifically, new generic techniques of amnesic dynamic programming, fast matrix-product over semiring, and low-degree polynomial method will be developed to design fine-grained approximation algorithms and their parallel counterparts. Time complexity may not always be the primary measure of efficiency. There are many core machine learning problems where query complexity, that quantifies the amount of labeled data acquired via active querying, is more important. The project will analyze for the first time the query complexity of basic learning problems, and explore its connections to developing fast algorithms."
"1724135","CRCNS US-German-Israeli Collaborative Research Proposal: Hierarchical Coordination of Complex Actions","BCS","Perception, Action & Cognition, CRCNS-Computation Neuroscience, Dynamics, Control and System D","09/01/2017","08/14/2017","Neville Hogan","MA","Massachusetts Institute of Technology","Standard Grant","Betty Tuller","10/31/2020","$300,000.00","","neville@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","7252, 7327, 7569","014Z, 030E, 034E, 7252, 7327, 7569, 8024, 8089","$0.00","There are many arenas where humans outperform machines. For example, when coordinated interaction with the physical environment is needed, humans (and animals) vastly out-perform modern robots.  This occurs despite the biological systems having far slower 'hardware' and 'wetware' and much greater complexity than even the most modern robots. This research project seeks to understand the role of complexity in human sensory and motor performance. Human walking under challenging balance conditions will be studied and the use of canes to enhance stability will be included.  The investigators emphasis on learning to balance in challenging environments should lead to new rehabilitation therapies (with or without robotic assistance) to aid recovery of balance and walking (e.g., after stroke). The researchers will create educational units suitable for online presentation to K-12 students and will devise exhibits based on their research for the Museum of Science in Boston.<br/><br/>The central hypothesis to be tested in this project is that complex movements involving physical interaction with objects are organized as a hierarchy formed of modules or primitives. Experiments will study how unimpaired humans learn to walk on narrow beams. Beams of different roundness will vary the challenge. Hand-held canes will alter the available support (like training wheels on a child's bicycle). Computer simulations combined with machine learning will study the benefits and drawbacks of organization as a hierarchy. New mathematical tools will be developed and tested to see if they enable insightful description of human performance in challenging conditions. The research involves a multinational collaboration among scientists from the U.S., Israel, and Germany, each with complementary expertise. The bridge between experimental and theoretical work and the diverse Principal Investigators will help to attract women into the traditionally male-dominated fields of computational neuroscience, robotics and control engineering. Companion projects are being funded by the Federal Ministry of Education and Research, Germany (BMBF) and the US-Israel Binational Science Foundation (BSF)."
"1845666","III: Small: Robust Large-Scale Data Mining for Knowledge Discovery in Depression Thought Records","IIS","Info Integration & Informatics","09/01/2017","07/23/2020","Heng Huang","PA","University of Pittsburgh","Standard Grant","Wendy Nilsen","07/31/2021","$487,880.00","","heng.huang@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7364","7364, 7923","$0.00","This project investigates new robust large-scale data mining and machine learning algorithms to solve critical computational challenges in mining massive depression thought records for cognitive behavior therapy. Depression is rapidly emerging as one of the major problems in our society and is also related to many other health conditions, such as stroke, diabetes, hypertension, HIV/AIDS, etc. Cognitive behavior therapy is the most extensively researched form of psychotherapy for depression, and the depression thought records from patients is the key component of cognitive behavior therapy. However, the process of reviewing and analyzing the depression thought records is extremely time consuming, which inhibits both clinical interviews and the training of new therapists. This project builds a novel data mining system to automatically discover knowledge from depression thought records for assisting therapists in selecting potential interventions and aiding new therapists in their development of cognitive behavior therapy skills. This project will facilitate the development of novel educational tools to enable new courses and enhance current courses. This project engages minority students and under-served populations in research activities to give them a better exposure to cutting-edge science research.<br/> <br/>To effectively and efficiently analyze large-scale depression thought records, this project explores the following research tasks. First, the project develops a robust semi-supervised learning model to categorize logical thinking errors of depression thought records. Second, the project investigates a joint multi-task method to simultaneously recognize the categories of thinking errors and emotions of depression thought records. Third, new multi-label and multi-instance learning is studied for identifying coping activities. Fourth, to analyze the multi-language depression thought records, robust transfer learning methods are developed for cross-language knowledge transfer. Meanwhile, parallel computational algorithms are designed and applied for large-scale depression thought record data mining. These novel data mining algorithms are designed to solve large-scale applications and automate the depression thought record data mining, which holds great promise for smart health."
"1652536","CAREER: Robust and Secure Multi-Modal Learning for Library-Scale Text Collections","IIS","Info Integration & Informatics","05/15/2017","06/17/2020","David Mimno","NY","Cornell University","Continuing Grant","Wei-Shinn Ku","04/30/2022","$449,413.00","","dm655@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364","1045, 7364, 9251","$0.00","The growth of social media and digitized libraries has made computational text analysis a vital tool for modern scholarship. But too often methods that work on standardized collections for expert users don't translate to real-world data analysis. In order to be useful, text mining methodologies need to balance theoretical power with practical application. Real data sets are noisy and complicated. More importantly, vast amounts of data cannot be shared directly due to copyright, including all published books after 1923. This project will develop tools that can be applied to limited, privatized views of documents. Algorithms will focus on reliability and efficiency, so that powerful techniques can be used by non-expert users on easily accessible hardware, such as the 10 million K-12 students using low-powered browser-based Chromebooks thereby increasing the societal impact of the work.<br/><br/>Unsupervised text mining methods such as topic models and word embeddings have become popular outside of machine learning because they operate on simple, widely-available representations and identify latent variables that represent recognizable themes, events, or concepts. But standard algorithms do not scale well, require full access to potentially sensitive text collections, and cannot take advantage of non-textual data such as images. Although recent work in spectral inference has produced improvements in speed, current methods are plagued by sensitivity to noisy observations. This work will develop a unified approach to unsupervised text mining based on matrix and tensor factorization. The project will focus on data rectification methods for input matrices, enabling simple algorithms to work dramatically better, even in the presence of sparse and noisy observations, while also reducing model uncertainty. The project will develop new methods for learning from private and sensitive documents by creating public views of non-public data. These will include both noisy representations of individual documents as well as corpus-level summary matrices, and support both strong non-identifiability and weaker non-expressivity criteria. Finally, the project will develop new tools for modeling images and text optimized for the way images actually accompany text in real corpora, rather than short, artificial captions. By jointly modeling large volumes of text and semantically related images, the project will enable users to search for contextually related images, not just visually similar images, and identify topics that are grounded in the visual world, not just in text. For further information see the project web page: http://mimno.infosci.cornell.edu"
"1703574","AF: Medium: Collaborative Research: Estimation, Learning, and Memory: The Quest for Statistically Optimal Algorithms","CCF","Algorithmic Foundations","07/01/2017","07/15/2019","Sham Kakade","WA","University of Washington","Continuing Grant","A. Funda Ergun","06/30/2021","$449,200.00","Gregory Valiant","sham@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","7924, 7926","$0.00","The goal of this project is to develop new, efficient algorithms that extract as much information as is possible from a given quantity of data.  In particular, this research aims to develop an understanding of how to leverage structure that is present in natural language settings, medical and genomic settings, and network- or graph-based settings. Many fundamental types of structure are encountered repeatedly in widely varying scientific and technological settings; our goal is to build on a recent body of work that focused on the simplest unstructured settings, and develop broadly applicable tools and insights to these diverse settings.   A central component of this project is a close interaction and transfer of ideas, problems, and techniques, between the theory community, the machine learning community, and the broader set of data-centric researchers and practitioners.<br/><br/>From a technical perspective, this research focuses on three fundamental types of structure: geometric structure, algebraic or low-rank structure, and the structure that is present in sequential<br/>data (such as natural language).   For the first two types of structure, the research focus is on understanding the possibilities and limitations in the sparse data regime where the amount of data is comparable to, or sublinear in, the dimensionality of the data.  In the third setting, the focus is on understanding the role of memory for learning and prediction tasks.<br/><br/>Beyond the direct research goals of the project, the PIs are extensively involved in teaching and outreach, including designing UW?s new data sciences curriculum, and developing new courses on algorithms and foundational aspects of data sciences at Stanford."
"1704417","AF:Medium:Collaborative Research:Estimation, Learning, and Memory: The Quest for Statistically Optimal Algorithms","CCF","Algorithmic Foundations","07/01/2017","07/21/2020","Gregory Valiant","CA","Stanford University","Continuing Grant","A. Funda Ergun","06/30/2021","$550,000.00","","gvaliant@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7796","7924, 7926","$0.00","The goal of this project is to develop new, efficient algorithms that extract as much information as is possible from a given quantity of data.  In particular, this research aims to develop an understanding of how to leverage structure that is present in natural language settings, medical and genomic settings, and network- or graph-based settings. Many fundamental types of structure are encountered repeatedly in widely varying scientific and technological settings; our goal is to build on a recent body of work that focused on the simplest unstructured settings, and develop broadly applicable tools and insights to these diverse settings.   A central component of this project is a close interaction and transfer of ideas, problems, and techniques, between the theory community, the machine learning community, and the broader set of data-centric researchers and practitioners.<br/><br/>From a technical perspective, this research focuses on three fundamental types of structure: geometric structure, algebraic or low-rank structure, and the structure that is present in sequential<br/>data (such as natural language).   For the first two types of structure, the research focus is on understanding the possibilities and limitations in the sparse data regime where the amount of data is comparable to, or sublinear in, the dimensionality of the data.  In the third setting, the focus is on understanding the role of memory for learning and prediction tasks.<br/><br/>Beyond the direct research goals of the project, the PIs are extensively involved in teaching and outreach, including designing UW?s new data sciences curriculum, and developing new courses on algorithms and foundational aspects of data sciences at Stanford."
"1819935","AF: Medium: Algorithmic Explorations of Networks, Markets, Evolution, and the Brain","CCF","Algorithmic Foundations","12/01/2017","12/20/2017","Christos Papadimitriou","NY","Columbia University","Continuing grant","Tracy Kimbrel","03/31/2019","$205,727.00","","cp3007@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7926, 7927, 7932","$0.00","Computer science is not just the scientific discipline behind the information technology revolution; it is also an apt framework for understanding the world around us.  This project is about applying the point of view of algorithms -- and their antithesis, complexity -- to understanding phenomena and challenges in a variety of domains, including the Internet, markets, evolution, and the brain.  To understand the Internet and the networks and markets it entails and enables, one must combine algorithms with ideas from economics and game theory.  This research will focus on online markets and particularly their dynamic (that is, multi-stage) nature, on incentives for improving congestion in network routing and air traffic, on algorithms for propagating influence in social networks, as well as new kinds of algorithms that take their inputs from competitors (who may choose to misrepresent their data).  The PI will continue his research on how computational insights can shed light on some key problems in evolution,  including certain rigorous connections between natural selection, machine learning, and a problem in Boolean logic.  Finally, the PI will work to reconcile learning algorithms with new insights from neuroscience.<br/><br/>The project includes research on certain crucial problems at the interface between computation and game theory/economics/networks,  while continuing past work employing computational concepts to elucidate evolution and, more recently, neuroscience.  The PI will  study the important problem of dynamic mechanism design in economics from the point of view of computational complexity and approximate implementation. He will also study mechanisms for managing congestion, with possible applications to air traffic control.  The project will explore the computational and graph-theoretic properties of several novel and promising game-theoretic models of network creation.  It will study from the complexity standpoint Nash equilibria with continuous strategies, and extensions of the Nash equilibrium concept beyond utility theory.  The project will also explore new and timely modes of computation in which all inputs (ultimately, all computational components) are provided by selfish rational agents. In evolution, the PI will explore the connections between learning algorithms, games, and natural selection, and a different connection between Boolean satisfiability and the emergence of novelty. The PI also plans to develop a new genre of learning algorithms that are more faithful to the new insights we are gaining into the brain.  Finally, from the standpoint of algorithms and complexity, the PI will look at several computational problems ranging from network variants of the set cover problem to linear programming and optimizing multivariate polynomials."
"1722399","SBIR Phase I:  Development of a Natural Language Dialogue System for the Blind and Visually Impaired to Enable Greater Efficiency in Remote Assistance","IIP","SBIR Phase I","07/01/2017","07/10/2017","Anirudh Koul","CA","Aira Tech Corp","Standard Grant","Nancy Kamei","03/31/2018","$225,000.00","","coderama@gmail.com","4225 Executive Square #460","La Jolla","CA","920378411","9494560460","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to dramatically increase the quality of life and economic independence for the nearly 22 million blind or visually impaired people in the US. The economic benefits include a significant reduction in the nearly $100B in annual economic losses from lower productivity due to visual impairment and in annual cost of social services to the blind or visually impaired.   This project will create low-cost, high-value tools and services that will give blind and visually impaired people the same level of environmental awareness as fully sighted people. At scale, this technology will result in the direct employment of more than 20,000 people to support the service, while also providing millions blind/visually impaired people with the tools and opportunity to join the work force.<br/> <br/>The proposed project will develop a real-time, life-enhancing service to the blind enabled by a mix of machine learning and a remote human assistant to create a real-time, semi-virtual, personal assistant with the quality of an in-person, trained human assistant.  In the proposed project, the core innovation is a high-level machine intelligence tool created by integration of  state-of-the-art  Natural  Language  Understanding  (NLU)  software and  Image  Recognition  and  Analysis (IA)  software  with  novel  inter-agent  routing  and  state-of-the-art  Graceful  Degradation,  or  seamless  error handling  and  resolution  by  either  software  or  human agents.  Despite the  incredible  advancements  in  NLU and  IA  agents,  integration  of  multiple  different  software  agents  into  a  single,  context-specific  application remains a difficult and risky development effort worthy of funding by the NSF. By  doing  this,  we  will  (1)  enhance  the  user  experience  with richer  environmental  feedback,  (2)  enhance the  productivity  of  our  human  agents,  allowing  individual  human  agents  to  serve  many  more  users  and thereby  create  scalability  that  will  make  the  service  both  self-sustaining  and  affordable,  and  finally  (3) increase  the  user?s  experience  of  personal  independence  through  interaction  with  machine-based  tools rather than emotionally and socially charged interactions with human agents."
"1704899","CSR: CHS: Medium: Collaborative Research: Improving Pedestrian Safety in Urban Cities using Intelligent Wearable Systems","CNS","Special Projects - CNS, CSR-Computer Systems Research","06/01/2017","09/18/2019","Xiaofan Jiang","NY","Columbia University","Continuing Grant","Erik Brunvand","05/31/2021","$766,642.00","Peter Kinget","jiang@ee.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1714, 7354","7924","$0.00","Using smartphones while walking poses an increasingly common safety problem for people in urban environments. Whether listening to music, texting, or talking, pedestrians that are absorbed with their smartphones are considerably less likely to notice important auditory cues of danger, such as the honks and sounds of approaching vehicles, putting pedestrians at far greater risk of being hit. This project aims to develop an intelligent wearable system that uses miniature microphones - embedded in earphones or headsets - to detect and locate approaching vehicles and warn the wearer of imminent dangers from cars, buses, motorbikes, trucks, and trams. <br/><br/>The system comprises multiple microphones embedded in a wearable headset, an ultra-low-power feature extraction and data  processing pipeline, and a set of machine-learning classifiers running on a smartphone. This project is organized in four research thrusts: (1) designing an architecture and data processing pipeline for a wearable system composed of heterogeneous embedded modules; (2) devising an ultra-low-power, analog, signal-processing Application-Specific Integrated Circuit (ASIC) for energy-efficient, on-board feature extraction; (3) modeling and optimizing machine-learning classifiers for acoustic event detection and localization; and (4) designing an interface and feedback mechanisms that are optimized for the users' perceptual, cognitive, and motor control abilities.<br/><br/>This research will help reduce pedestrian injuries and fatalities, and expand knowledge on designing wearable systems for enhancing safety in cities, workplaces, and the home. The underlying framework can be generalized to other systems - employing low-power signal processors and algorithms to solve real-time sensing and classification problems of many kinds. Research products will be made publicly available to anyone to apply these techniques in their own system designs. Course modules developed on embedded systems, mobile computing, and Internet-of-Things will be used at the three participating universities to train undergraduate and graduate students, and will be made available online."
"1704914","CSR: CHS: Medium: Collaborative Research: Improving Pedestrian Safety in Urban Cities using Intelligent Wearable Systems","CNS","CSR-Computer Systems Research","06/01/2017","06/18/2020","Josh New","NY","Barnard College","Continuing Grant","Erik Brunvand","05/31/2021","$125,916.00","","jnew@barnard.edu","3009 BROADWAY","New York","NY","100276598","2128542708","CSE","7354","7924, 9251","$0.00","Using smartphones while walking poses an increasingly common safety problem for people in urban environments. Whether listening to music, texting, or talking, pedestrians that are absorbed with their smartphones are considerably less likely to notice important auditory cues of danger, such as the honks and sounds of approaching vehicles, putting pedestrians at far greater risk of being hit. This project aims to develop an intelligent wearable system that uses miniature microphones - embedded in earphones or headsets - to detect and locate approaching vehicles and warn the wearer of imminent dangers from cars, buses, motorbikes, trucks, and trams. <br/><br/>The system comprises multiple microphones embedded in a wearable headset, an ultra-low-power feature extraction and data  processing pipeline, and a set of machine-learning classifiers running on a smartphone. This project is organized in four research thrusts: (1) designing an architecture and data processing pipeline for a wearable system composed of heterogeneous embedded modules; (2) devising an ultra-low-power, analog, signal-processing application-specific integrated circuit (ASIC) for energy-efficient, on-board feature extraction; (3) modeling and optimizing machine-learning classifiers for acoustic event detection and localization; and (4) designing an interface and feedback mechanisms that are optimized for the users' perceptual, cognitive, and motor control abilities.<br/><br/>This research will help reduce pedestrian injuries and fatalities, and expand knowledge on designing wearable systems for enhancing safety in cities, workplaces, and the home. The underlying framework can be generalized to other systems - employing low-power signal processors and algorithms to solve real-time sensing and classification problems of many kinds. Research products will be made publicly available to anyone to apply these techniques in their own system designs. Course modules developed on embedded systems, mobile computing, and Internet-of-Things will be used at the three participating universities to train undergraduate and graduate students, and will be made available online."
"1709568","Crack Growth During Fatigue in Ni Superalloys: Physical Origin of Stochastic Jumps and Their Predictive Role Using Statistical Approaches","DMR","METAL & METALLIC NANOSTRUCTURE, EPSCoR Co-Funding","06/01/2017","06/23/2020","Stefanos Papanikolaou","WV","West Virginia University Research Corporation","Standard Grant","Judith Yang","05/31/2021","$423,277.00","Xingbo Liu, Terence Musho","Stefanos.Papanikolaou@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","MPS","1771, 9150","8084, 9150","$0.00","Non-Technical Abstract<br/><br/>Strong, durable materials are an integral part of our society. One such class of materials found in turbine engines, used in the aerospace and marine industries, are known as superalloys. Superalloys exhibit excellent mechanical properties (strength, creep resistance, corrosion resistance). However, there is a catch in that these materials involve a large degree of structural disorder as a result of the required material manufacturing process. The effects of such disorder become even more pronounced at the high temperatures of turbines, due to sustained loading conditions, leading to microscopic damage and cracks in the material. These cracks are exacerbated over the lifetime of the machinery. Therefore, it is crucial to understand the behavior of these cracks and prevent catastrophic mechanical failure.<br/><br/>Crack initiation and growth in very heterogeneous materials not only can be detrimental but also very unpredictable, thus it requires statistical methods and protocols for assessing the reliability of components at various stages of fatigue loading. This project will advance the science of stochastic crack growth jumps during cyclic loading (fatigue) of metallic heterogeneous materials, with a particular focus on Ni superalloys. The usefulness of the mechanical noise produced by such little cracks is that it might contain distinctive statistical features that can identify the damage level in a turbine component. A team of engineers and scientists will combine multi-scale modeling approaches, statistical methods, and experiments to ultimately develop combined experiment and theory protocols for characterizing the fatigue-induced ""cracking noise"" and assessing the damage levels of mechanical components. Beyond superalloys, the very outcome of this research is to promote the progress of the fundamental understanding of fatigue damage and develop non-invasive structural prognosis methods. An educational outreach program is also  planned that involves graduate, undergraduate, and high-school students, as well as the general public, in the under-represented EPSCoR state of West Virginia.<br/><br/><br/>Technical Abstract <br/><br/>This project will advance the understanding of stochastic jumps during fatigue loading of Ni superalloys. A multi-scale modeling approach will be employed that will combine density functional theory (DFT) predictions with phase-field modeling. Machine-learning methods will be incorporated into the phase field model, which will be trained based on conducted experiments. The outcome of this research will be the fundamental understanding of fatigue damage that may be used to predict catastrophic failures, especially when there is limited statistical sampling.<br/><br/>A team of engineers and scientists will develop a novel pathway to predictive modeling of crack growth during fatigue loading in metallic superalloys: By statistically sampling the noise correlations at various stages of fatigue under the assumption of constant-stress short-time tests, we will build a predictive machine-learning framework using a direct multi-step forecasting strategy. In doing so, we will investigate the fundamental origin of stochastic crack growth jumps and will develop a probabilistic model that will incorporate a first-principles relationship of the cohesive energy, generated by density functional theory predictions and phase-field modeling. To validate our models, we will conduct a series of well-controlled experiments using in-situ SEM and we will track crack growth using DC resistance drop measurements. The statistical properties of crack growth noise at various stages as a function of temperature and environmental pressure will be compared to the multi-scale model predictions. The validated multi-scale model will then be used to investigate the probability distributions of crack growth events (classified in terms of crack-length changes) during the first few cycles to predict crack growth at late stages. The outcome will be a trained model that can predict failure based on early fatigue events.<br/><br/>This research project has a societal impact based on the fundamental physical origin of crack growth jumps during fatigue loading of metallic superalloys, which are commonly used on aircraft turbines and other hardware. The aim is to develop general protocols to promote early, safe prediction of crack growth in metallic alloys. In addition to societal impact, an educational outreach program is planned that involve training graduate, undergraduate, and high-school students, as well as the general public, in the under-represented EPSCoR state of West Virginia. The focus of training will be on the use of computational modeling materials science as well as the deep understanding of basic physical properties of crack growth, fracture, and non-equilibrium rare events. The PI will design a course that will introduce the fundamentals of non-equilibrium statistical mechanics and fracture to multidisciplinary, undergraduate engineering environments."
"1712788","CCF-BSF:CIF:Small:Signal Processing and Machine Learning on Manifolds, with Applications to Invariant Detection and Covariant Estimation","CCF","Comm & Information Foundations","07/01/2017","06/28/2017","Louis Scharf","CO","Colorado State University","Standard Grant","Phillip Regalia","12/31/2020","$449,993.00","Edwin Chong, Christopher Peterson","Louis.Scharf@ColoState.EDU","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7797","7923, 7935, 7936","$0.00","In many fields of engineering and applied science the problem is to extract relevant information from a signal or image. Certainly this describes the problem of identifying cyber attackers in data networks, spotting objects of interest in closed-circuit TV records, and classifying anomalies in medical images. The goal of this project is to develop a signal processing theory for detecting and classifying images that have undergone geometric transformations. Practical and topical examples are medical features viewed in variable magnification and orientation, and images of people in arbitrary orientations in crowded scenes. The solution to classification problems under such imaging conditions will advance medical practice and national defense. As a broader impact, the project prepares students for careers in mathematics and electrical engineering, with an expertise in signal processing and imaging science.<br/><br/>This project develops a theory of matched manifold detectors, based on a universal manifold embedding that extracts a subspace basis from an image. The basis itself codes for the coordinate transformation of the image, but its span is invariant to the transformation. Consequently the extracted subspace is an invariant statistic for detection, and the basis is a covariant statistic for the parameters of the transformation. Classification is then a problem of subspace matching on a Grassmann manifold, and identification of coordinate transformation is a problem of analyzing a subspace basis in a Stiefel manifold. We aim to adapt this theory to other problems where a transformation group turns out an orbit of images, all of which are to be classified as equivalent. The objective is to develop a theory of signal processing on manifolds that is as broad in its scope and as precise in its methodologies as modern subspace signal processing. Such a theory will augment statistical reasoning with geometrical reasoning, and bring new mathematical methods into play."
"1715735","Frame Compatibility: Discrete Versus Continuous Redundant Expansions, Strategies for Narrowing the Digital-Analog Gap","DMS","APPLIED MATHEMATICS","09/01/2017","05/10/2017","Bernhard Bodmann","TX","University of Houston","Standard Grant","Victor Roytburd","08/31/2021","$256,977.00","","bgb@math.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","MPS","1266","9251","$0.00","The theory of compressed sensing promises to revolutionize remote sensing such as radar, biomedical imaging, and perhaps even digital photography. The main insight from this theory is that a compressible signal can be acquired with much less effort than a signal with a high information content. However, these results are commonly based on mathematical models for signals that are already digitized and for sensors that measure randomly, which makes them somewhat disconnected from realistic physical signals and apparatuses. This work explores recent trends in narrowing the gap between theory and practice. Instead of digital signals, models for analog signals are used to define compressibility. The signal space includes the possibility of continuous changes without producing artifacts in the signal recovery procedure. This idea will be applied to radar, X-ray crystallography, and other sensing systems. The work is also anticipated to have application to neural networks that form the basis for modern machine learning algorithms. Although the application to machine learning is entirely concerned with digital data, the use of continuous models ensures that encoded information can be retrieved accurately. <br/><br/>Redundant, stable expansions with frames have become central to many applications of mathematics in data science, signal acquisition, and communications, from remote sensing to packet-based, wireless, fiber optical, or quantum communications and recently in compressed sensing and super-resolution. Despite the successes of the frame-based expansion and acquisition of signals, there is often a mismatch between the stylized mathematical signal and measurement models that are assumed and the corresponding physical models in the analog domain. For example, signal acquisition is typically described by specific linear functionals, not randomly chosen, unstructured ones. This research project addresses the need to improve compatibility between continuous and discrete representation spaces on a fundamental level. A typical model for analog signals is given by infinite-dimensional Hilbert spaces with a reproducing kernel and an associated expansion with respect to a continuous, highly coherent family of vectors. A natural measure of sparsity of a signal is in this setting the minimal number of kernel functions needed in its expansion. Signal acquisition is usually based on sampling from a group-invariant, discrete family of functionals. The expected outcomes of the project include: (1) accurate recovery for signals that are sparsely synthesized in a finite- or infinite-dimensional reproducing kernel space and measured with physically relevant sensing models, using a sparsity-inducing norm that is stable with respect to continuous deformations; (2) phase retrieval, signal recovery based on magnitudes of frame coefficients, in reproducing kernel Hilbert spaces such as multivariate Paley-Wiener spaces, which will be done using sparsity to demonstrate injectivity of measurements, stability, and feasibility of recovery algorithms in a general class of kernel spaces; and (3) a version of Mallat's scattering transform in a redundant representation with approximate invertibility based on phase retrieval and sparsity. The scattering transform extracts nonlinear features from data that are powerful descriptors in classification problems. It is designed from a viewpoint of desirable properties in the analog domain, but its application is mostly to digitized data of limited size, for which the claims need to be properly adapted. The investigators will use phase retrieval and sparsity to demonstrate the approximate invertibility of the transform, which is needed to verify the faithful encoding of data."
"1729487","DMREF: Collaborative Research: Accelerating Thermoelectric Materials Discovery via Dopability Predictions","DMR","DMREF","10/01/2017","08/08/2018","G. Jeffrey Snyder","IL","Northwestern University","Standard Grant","John Schlueter","09/30/2021","$383,999.00","","jeff.snyder@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","MPS","8292","054Z, 8396, 8400, 8607","$0.00","Non-technical Description: Thermoelectric devices, which transform heat flow into electrical power and vice versa, have the potential to revolutionize how society produces electricity and cooling.  However, thermoelectric materials suffer from poor power conversion efficiency and the search continues for new materials with enhanced performance. In this project, advances in computation and machine learning are leveraged to accelerate this search for advanced thermoelectric materials. These efforts build upon the prior NSF DMREF research of some of team members on predicting a material's potential for thermoelectric performance. High throughput screening focused on identifying semiconductors with desirable electronic and vibrational properties.  However, these efforts did not include a strong focus on the role of intrinsic defect or the potential for dopability.  In the next stage of this research, these critical components will be pursued through a mixture of high throughput theory, experimental validation, and machine learning.  Together, these efforts will yield accurate prediction of the thermoelectric potential for thousands of semiconductors and the realization of new materials for solid state power generation.  Beyond thermoelectric materials, these efforts to establish a dopability recommendation engine will be critical in the development of next generation microelectronic and optoelectronic materials such as transparent conductors and photovoltaic absorbers.   <br/><br/>Technical Description: The project's ultimate objective is to build a robust and accurate dopability recommendation engine to overcome the dopability bottleneck in thermoelectric materials discovery. The recommendation engine will use materials informatics to enable high-throughput predictions of dopability, relying only on quantities that are inexpensive to calculate, experimental measurements, and known structural/chemical features as inputs. It will thus allow dopability screening of thousands of compounds.  First, an accurate training set will be built for the recommendation engine containing native defect formation enthalpies and structural/chemical descriptors from a diverse array of thermoelectric-relevant compounds. Whereas prior dopant studies focused on single compounds, a new, automated calculation infrastructure will be leveraged that allows the rapid creation of an extensive training set, initially containing approximately 30 compounds but growing to over 100 during the project. Experimental charge transport and local dopant structure measurements will validate the training set. Second, the prediction engine will be trained on the data to extract patterns and correlations, and ultimately identify robust descriptors of dopability.  Initially, the engine will predict if `killer' defects limit the available dopant range. The engine will ultimately grow to suggest specific extrinsic dopants for compounds that pass this initial screening.  Together, this combination of accurate predictions of intrinsic transport properties (prior DMREF) and dopability (proposed DMREF) is expected to accelerate the discovery process for thermoelectric materials."
"1729594","DMREF: Collaborative Research: Accelerating Thermoelectric Materials Discovery via Dopability Predictions","DMR","DMREF","10/01/2017","08/20/2018","Eric Toberer","CO","Colorado School of Mines","Standard Grant","John Schlueter","09/30/2021","$1,009,018.00","Michael Toney, Vladan Stevanovic","etoberer@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","MPS","8292","054Z, 8396, 8400, 8607","$0.00","Non-technical Description: Thermoelectric devices, which transform heat flow into electrical power and vice versa, have the potential to revolutionize how society produces electricity and cooling.  However, thermoelectric materials suffer from poor power conversion efficiency and the search continues for new materials with enhanced performance. In this project, advances in computation and machine learning are leveraged to accelerate this search for advanced thermoelectric materials. These efforts build upon the prior NSF DMREF research of some of team members on predicting a material's potential for thermoelectric performance. High throughput screening focused on identifying semiconductors with desirable electronic and vibrational properties.  However, these efforts did not include a strong focus on the role of intrinsic defect or the potential for dopability.  In the next stage of this research, these critical components will be pursued through a mixture of high throughput theory, experimental validation, and machine learning.  Together, these efforts will yield accurate prediction of the thermoelectric potential for thousands of semiconductors and the realization of new materials for solid state power generation.  Beyond thermoelectric materials, these efforts to establish a dopability recommendation engine will be critical in the development of next generation microelectronic and optoelectronic materials such as transparent conductors and photovoltaic absorbers.   <br/><br/>Technical Description: The project's ultimate objective is to build a robust and accurate dopability recommendation engine to overcome the dopability bottleneck in thermoelectric materials discovery. The recommendation engine will use materials informatics to enable high-throughput predictions of dopability, relying only on quantities that are inexpensive to calculate, experimental measurements, and known structural/chemical features as inputs. It will thus allow dopability screening of thousands of compounds.  First, an accurate training set will be built for the recommendation engine containing native defect formation enthalpies and structural/chemical descriptors from a diverse array of thermoelectric-relevant compounds. Whereas prior dopant studies focused on single compounds, a new, automated calculation infrastructure will be leveraged that allows the rapid creation of an extensive training set, initially containing approximately 30 compounds but growing to over 100 during the project. Experimental charge transport and local dopant structure measurements will validate the training set. Second, the prediction engine will be trained on the data to extract patterns and correlations, and ultimately identify robust descriptors of dopability.  Initially, the engine will predict if `killer' defects limit the available dopant range. The engine will ultimately grow to suggest specific extrinsic dopants for compounds that pass this initial screening.  Together, this combination of accurate predictions of intrinsic transport properties (prior DMREF) and dopability (proposed DMREF) is expected to accelerate the discovery process for thermoelectric materials."
"1639658","EarthCube Data Infrastructure:   Collaborative Proposal:   A unified experimental-natural digital data system for analysis of rock microstructures","ICER","EarthCube","09/01/2017","08/18/2017","Matty Mookerjee","CA","Sonoma State University","Standard Grant","Eva Zanzerkia","08/31/2021","$93,791.00","Gurman Gill","matty.mookerjee@sonoma.edu","1801 East Cotati Avenue","Rohnert Park","CA","949283609","7076643972","GEO","8074","7433","$0.00","When viewed at the micro-scale, rocks reveal structures that help to interpret the processes and forces responsible for their formation.  These microstructures help to explain phenomena that occur at the scale of mountains and tectonic plates.  Interpretation of microstructures formed in nature during deformation is aided by comparison with those formed during experiments, under known conditions of pressure, temperature, stress, strain and strain rate, and experimental rock deformation benefits from the ground truth offered through comparison with rocks deformed in nature.  However, the ability to search for relevant naturally or experimentally deformed microstructures is hindered by the lack of any database that contains these data.  The researchers collaborating on this project will develop a single digital data system for rock microstructures to facilitate the critical interaction between and among the communities that study naturally and experimentally deformed rocks.  To aid in the comparison of microstructures formed in nature and experiment, we will link to commonly used analytical tools and develop a pilot project for automatic comparison of microstructures using machine learning.   <br/><br/>Rock microstructures relate processes at the microscopic scale to phenomena at the outcrop, orogen, and plate scales and reveal the relationships among stress, strain, and strain rate.  Quantitative rheological information is obtained through linked studies of naturally formed microstructures with those created during rock deformation experiments under known conditions.  The project will develop a single digital data system for both naturally and experimentally deformed rock microstructure data to facilitate comparison of microstructures from different environments.  A linked data system will facilitate interaction between practitioners of experimental deformation, those studying natural deformation and the cyberscience community.  The data system will leverage the StraboSpot data system currently under development in Structural Geology and Tectonics.  To develop this system requires: 1) Modification of the StraboSpot data system to accept microstructural data from both naturally and experimentally deformed rocks; and 2) Linking the microstructural data to its geologic context ? either in nature, or its experimental data/parameters.  The researchers will engage the rock deformation community with the goal of establishing data standards and protocols for data collection, and integrate our work with ongoing efforts to establish protocols and techniques for automated metadata collection and digital data storage.  To analyze the microstructures studied and/or generated by these communities, we will ensure StraboSpot data output is compatible with commonly used microstructural tools.  They will develop a pilot project for comparing and analyzing microstructures from different environments using machine-learning."
"1639641","EarthCube Data Infrastructure:  Collaborative Proposal:  A unified experimental-natural digital data system for analysis of rock microstructures","ICER","EarthCube","09/01/2017","08/18/2017","Philip Skemer","MO","Washington University","Standard Grant","Eva Zanzerkia","08/31/2021","$126,336.00","","pskemer@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","GEO","8074","7433, 9150","$0.00","When viewed at the micro-scale, rocks reveal structures that help to interpret the processes and forces responsible for their formation.  These microstructures help to explain phenomena that occur at the scale of mountains and tectonic plates.  Interpretation of microstructures formed in nature during deformation is aided by comparison with those formed during experiments, under known conditions of pressure, temperature, stress, strain and strain rate, and experimental rock deformation benefits from the ground truth offered through comparison with rocks deformed in nature.  However, the ability to search for relevant naturally or experimentally deformed microstructures is hindered by the lack of any database that contains these data.  The researchers collaborating on this project will develop a single digital data system for rock microstructures to facilitate the critical interaction between and among the communities that study naturally and experimentally deformed rocks.  To aid in the comparison of microstructures formed in nature and experiment, we will link to commonly used analytical tools and develop a pilot project for automatic comparison of microstructures using machine learning.   <br/><br/>Rock microstructures relate processes at the microscopic scale to phenomena at the outcrop, orogen, and plate scales and reveal the relationships among stress, strain, and strain rate.  Quantitative rheological information is obtained through linked studies of naturally formed microstructures with those created during rock deformation experiments under known conditions.  The project will develop a single digital data system for both naturally and experimentally deformed rock microstructure data to facilitate comparison of microstructures from different environments.  A linked data system will facilitate interaction between practitioners of experimental deformation, those studying natural deformation and the cyberscience community.  The data system will leverage the StraboSpot data system currently under development in Structural Geology and Tectonics.  To develop this system requires: 1) Modification of the StraboSpot data system to accept microstructural data from both naturally and experimentally deformed rocks; and 2) Linking the microstructural data to its geologic context ? either in nature, or its experimental data/parameters.  The researchers will engage the rock deformation community with the goal of establishing data standards and protocols for data collection, and integrate our work with ongoing efforts to establish protocols and techniques for automated metadata collection and digital data storage.  To analyze the microstructures studied and/or generated by these communities, we will ensure StraboSpot data output is compatible with commonly used microstructural tools.  They will develop a pilot project for comparing and analyzing microstructures from different environments using machine-learning."
"1639710","EarthCube Data Infrastructure:   Collaborative Proposal:  A unified experimental-natural digital data system for analysis of rock microstructure","ICER","EarthCube","09/01/2017","08/18/2017","Chris Marone","PA","Pennsylvania State Univ University Park","Standard Grant","Eva Zanzerkia","08/31/2020","$63,665.00","","cjm38@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","GEO","8074","7433","$0.00","When viewed at the micro-scale, rocks reveal structures that help to interpret the processes and forces responsible for their formation.  These microstructures help to explain phenomena that occur at the scale of mountains and tectonic plates.  Interpretation of microstructures formed in nature during deformation is aided by comparison with those formed during experiments, under known conditions of pressure, temperature, stress, strain and strain rate, and experimental rock deformation benefits from the ground truth offered through comparison with rocks deformed in nature.  However, the ability to search for relevant naturally or experimentally deformed microstructures is hindered by the lack of any database that contains these data.  The researchers collaborating on this project will develop a single digital data system for rock microstructures to facilitate the critical interaction between and among the communities that study naturally and experimentally deformed rocks.  To aid in the comparison of microstructures formed in nature and experiment, we will link to commonly used analytical tools and develop a pilot project for automatic comparison of microstructures using machine learning.   <br/><br/>Rock microstructures relate processes at the microscopic scale to phenomena at the outcrop, orogen, and plate scales and reveal the relationships among stress, strain, and strain rate.  Quantitative rheological information is obtained through linked studies of naturally formed microstructures with those created during rock deformation experiments under known conditions.  The project will develop a single digital data system for both naturally and experimentally deformed rock microstructure data to facilitate comparison of microstructures from different environments.  A linked data system will facilitate interaction between practitioners of experimental deformation, those studying natural deformation and the cyberscience community.  The data system will leverage the StraboSpot data system currently under development in Structural Geology and Tectonics.  To develop this system requires: 1) Modification of the StraboSpot data system to accept microstructural data from both naturally and experimentally deformed rocks; and 2) Linking the microstructural data to its geologic context ? either in nature, or its experimental data/parameters.  The researchers will engage the rock deformation community with the goal of establishing data standards and protocols for data collection, and integrate our work with ongoing efforts to establish protocols and techniques for automated metadata collection and digital data storage.  To analyze the microstructures studied and/or generated by these communities, we will ensure StraboSpot data output is compatible with commonly used microstructural tools.  They will develop a pilot project for comparing and analyzing microstructures from different environments using machine-learning."
"1636786","BD Spokes: SPOKE: NORTHEAST: Collaborative Research: Integration of Environmental Factors and Causal Reasoning Approaches for Large-Scale Observational Health Research","IIS","BD Spokes -Big Data Regional I","01/01/2017","08/31/2016","Gregory Cooper","PA","University of Pittsburgh","Standard Grant","Sylvia Spengler","12/31/2020","$111,075.00","","gfc@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","024Y","028Z, 7433, 8083","$0.00","Vast quantities of health, environmental, and behavioral data are being generated today, yet they remain locked in digital silos. For example, data from health care providers, such as hospitals, provide a dynamic view of health of individuals and populations from birth to death. At the same time, government institutions and industry have released troves of economic, environmental, and behavioral datasets, such as indicators of income/poverty, adverse exposure (e.g., air pollution), and ecological factors (e.g., climate) to the public domain. How are economic, environmental, and behavioral factors linked with health? This project will put together numerous sources of large environmental and clinical data streams to enable the scientific community to address this question. By breaking current data silos, the broader scientific impacts will be wide. First, this effort will foster new routes of biomedical investigation for the big data community. Second, the project will enable discoveries that will have behavioral, economic, environmental, and public health relevance.<br/><br/>This project will aim to assemble a first-ever data warehouse containing numerous health/clinical, environmental, behavioral, and economic data streams to ultimately enable causal discovery between these data sources. First, the team will integrate numerous health data streams by leveraging the Observational Health Data Sciences and Informatics (OHDSI, www.ohdsi.org) network, a virtual data repository that contains millions of longitudinal patient measurements, such as drugs and disease diagnoses. Second, the team will build a centralized data warehouse that contains important environmental, behavioral, and economic data across the United States, such as the Environmental Protection Agency air pollution AirData, the United States Census data on income and occupation statistics, and the National Oceanic Administration Association for climate and weather-related information. Third, the team will disseminate emerging computational methods for causal inference and machine learning to enable researchers to find causal links between environmental, economic, behavioral, and clinical factors. The team will leverage our broad collaborative network consisting of academic big data researchers, federal-level institutes (e.g., EPA, NOAA), and hospitals (e.g., Partners HealthCare) to integrate these data and to disseminate cutting edge machine learning tools. Lastly, the project will create training resources (e.g., interactive how-to guides), coordinate cross-institution student internships, and lead a hands-on workshop to demonstrate use of the integrated data warehouse. The ultimate goal of the project is to facilitate community-led and collaborative causal discovery through dissemination of integrated and open big data and analytics tools."
"1741057","BIGDATA: IA: Collaborative Research: In Situ Data Analytics for Next Generation Molecular Dynamics Workflows","IIS","Big Data Science &Engineering","10/01/2017","02/13/2018","Michela Taufer","DE","University of Delaware","Standard Grant","Almadena Chtchelkanova","08/31/2018","$979,987.00","Trilce Estrada-Piedra","taufer@utk.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","8083","7433, 7942, 8083, 9150","$0.00","Molecular dynamics simulations studying the classical time evolution of a molecular system at atomic resolution are widely recognized in the fields of chemistry, material sciences, molecular biology and drug design; these simulations are one of the most common simulations on supercomputers.  Next-generation supercomputers will have dramatically higher performance than do current systems, generating more data that needs to be analyzed (i.e., in terms of number and length of molecular dynamics trajectories). The coordination of data generation and analysis cannot rely on manual, centralized approaches as it does now.  This interdisciplinary project integrates research from various areas across programs such as computer science, structural molecular biosciences, and high performance computing to transform the centralized nature of the molecular dynamics analysis into a distributed approach that is predominantly performed in situ. Specifically, this effort combines machine learning and data analytics approaches, workflow management methods, and high performance computing techniques to analyze molecular dynamics data as it is generated, save to disk only what is really needed for future analysis, and annotate molecular dynamics trajectories to drive the next steps in increasingly complex simulations' workflows. <br/><br/>The investigators tackle the data challenge of data analysis of molecular dynamics simulations on the next-generation supercomputers by (1) creating new in situ methods to trace molecular events such as conformational changes, phase transitions, or binding events in molecular dynamics simulations at runtime by locally reducing knowledge on high-dimensional molecular organization into a set of relevant structural molecular properties; (2) designing new data representations and extend unsupervised machine learning techniques to accurately and efficiently build an explicit global organization of structural and temporal molecular properties; (3) integrating simulation and analytics into complex workflows for runtime detection of changes in structural and temporal molecular properties; and (4) developing new curriculum material, online courses, and online training material targeting data analytics. The project's harnessed knowledge of molecular structures' transformations at runtime can be used to steer simulations to more promising areas of the simulation space, identify the data that should be written to congested parallel file systems, and index generated data for retrieval and post-simulation analysis. Supported by this knowledge, molecular dynamics workflows such as replica exchange simulations, Markov state models, and the string method with swarms of trajectories can be executed ?from the outside? (i.e., without reengineering the molecular dynamics code)."
"1742032","Collaborative Research: Decision Model for Patient-Specific Motion Management in Radiation Therapy Planning","CMMI","OE Operations Engineering","02/01/2017","04/25/2017","Wanpracha Chaovalitwongse","AR","University of Arkansas","Standard Grant","Georgia-Ann Klutke","08/31/2018","$112,375.00","","artchao@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","ENG","006Y","076E, 078E, 8023","$0.00","A significant challenge in lung cancer radiation therapy (RT) is respiration-induced tumor motion, which hinders sufficient delivery of curative doses to target volumes. Although modern tumor motion management strategies for positron emission tomography/computed tomography (PET/CT)-guided RT are becoming more available, those techniques have yet to be fully incorporated into clinical practice. This is mainly because not every patient will benefit from a costly and lengthy motion-managed PET/CT scan due to high intra-patient and inter-patient variability of respiratory patterns. The objective of this project is to bridge the knowledge gap of which motion management method would best benefit an individual patient. This project will develop a new decision-making paradigm, in which machine learning techniques will be developed to characterize respiratory motion patterns and combine them with other diagnostic factors to predict the benefits from motion management methods for each individual patient. A decision-analytic cohort model will be developed to compare and evaluate the cost-effectiveness of the new decision paradigm and the traditional population-based radiation oncology practice of motion management based on our existing database of respiratory traces from more than 3,000 patients. While specifically applied to decisions surrounding respiratory motion management, the developed decision paradigm can be generalized and applied to other real life decision analysis problems.<br/><br/>This award supports fundamental research in data mining/machine learning and decision analysis, which will provide needed knowledge for the development of tools for effective management of patient-specific tumor motion. The modeling effort in this project will 1) establish a new mathematical foundation for supervised multivariate sparse variable selection and prediction to discover complicated multivariate relationships among high-dimensional variables; 2) construct a general integrated validation framework to rigorously test the cost-effectiveness of patient-specific health interventions. The new multivariate sparse variable selection and prediction approach can be used to build an interpretable prediction model, handle high-dimensional data with a low sample size, avoid under-shrinkage effect, and incorporate structured group selection. The cost-effectiveness analysis framework integrates the outcome of prediction model, the treatment effect and survival outcome model. This modeling aims to quantitatively estimate long-term cancer survival outcomes from improvement in patient-specific planning of radiation dosing by selective motion control."
"1749810","AF: EAGER: Identifying Opportunities in Pseudorandomness","CCF","Algorithmic Foundations","09/15/2017","09/11/2017","Omer Reingold","CA","Stanford University","Standard Grant","Tracy Kimbrel","08/31/2018","$175,000.00","","omreing@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7796","7916, 7926, 7927","$0.00","Pseudorandomness is the theory of generating objects that ""look random"" despite being constructed using little or no randomness.  The computational theory of pseudorandomness originated in the foundations of cryptography in the early 1980s and has since developed into a rich subfield of theoretical computer science in its own right.  The notions and constructs studied in the theory of pseudorandomness have implications for many different areas of research in computer science, communications, and mathematics.<br/><br/>This EArly-concept Grant for Exploratory Research (EAGER) project seeks to identify novel approaches to some of the most enduring and important challenges in pseudorandomness as well as opportunities for new applications of pseudorandomness.  The research will be closely integrated with the PIs' educational efforts.  In particular, the PIs will continue to develop new curricular, educational, and expository material that are made openly available for others to use.  Graduate and undergraduate students will be involved in all phases of this research, and will be given opportunities to publish and present their results at premier international conferences.  The PIs will also continue their extensive service to the scientific community.<br/><br/>Specifically, the project will try to uncover new approaches to topics such as:<br/><br/>1. The RL vs. L problem: trying to prove, unconditionally, that every randomized algorithm can be made deterministic with only a constant-factor loss in memory usage.<br/><br/>2. Cryptography: identifying optimally efficient constructions of cryptographic primitives from minimal assumptions.<br/><br/>3. Machine Learning: can pseudorandomness help in making machine learning robust to adversarial noise or to overfitting?"
"1663704","PREEVENTS Track 2: Collaborative Research: Ocean Salinity as a predictor of US hydroclimate extremes","ICER","PREEVENTS - Prediction of and","08/01/2017","08/01/2018","Caroline Ummenhofer","MA","Woods Hole Oceanographic Institution","Continuing Grant","Justin Lawrence","07/31/2021","$678,518.00","Caroline Ummenhofer","cummenhofer@whoi.edu","183 OYSTER POND ROAD","WOODS HOLE","MA","025431041","5082893542","GEO","034Y","","$0.00","Water availability is a fundamental necessity for society. As the largest moisture reservoir and ultimate moisture source, water from the oceans sustains terrestrial precipitation and is thus key to understanding variability in the water cycle on land. Floods and droughts represent extremes of the water cycle that have enormous consequences for society. In recent years Western drought has led to billions of dollars of agricultural losses and extensive wildfires, while floods produced similar losses in the South, Midwest and East of the US. They are caused by an excess or deficit of moisture exported from ocean to land.  Moisture evaporating from the ocean surface is the ultimate source for terrestrial precipitation. Thus, the availability of the oceanic moisture supply modulates the severity of hydroclimate extremes on land. As moisture exits the ocean, it leaves a signature in sea surface salinity. Recent studies have provided remarkable new evidence that salinity can be utilized as a skillful predictor of precipitation in the US Midwest, Southwest and other regions. The salinity precursors significantly outperform temperature-based predictors, especially in the years with heavy precipitation or exceptional drought.  Thus, sea surface salinity has great potential to provide a transformative improvement to seasonal forecasts of US hydroclimate extremes. This project will develop the scientific basis for a drought and flood early warning system for the US based on these new insights into the predictive potential of ocean salinity and the expanding salinity monitoring system that uses both in-situ measurements and satellites. This will lead to a number of societal benefits: lives saved and property preserved from wildfires and floods; improved crop yields resulting from more accurate seasonal rainfall forecasts; national security advances realized by better anticipation of destabilized regions affected by drought or flood crises; and more accurate forecasting of energy demand and the impact of water shortages on power plants. Several undergraduate students will have the opportunity to gain valuable research experience, and thus the project will help to train the next generation of climate scientists. Project findings will also be incorporated into graduate courses taught through the MIT/WHOI joint program and at Duke University, and the knowledge will be disseminated to the general public. <br/><br/>The processes that produce the newly identified relationships between extreme precipitation and sea surface salinity will be explored. Daily precipitation data and a Bayesian statistical framework will be used to sample the extreme events in the US. Based on the Bayesian inference, the pre-season salinity precursors will be explored and mechanisms by which the water cycle generates the salinity signatures determined by calculating atmospheric moisture fluxes and the terms in the surface salinity budget. In addition, the oceanic moisture flux onto land will be tracked, and the processes assessed by which extremes develop through the moisture supply and/or energy redistribution in the atmospheric column. Machine-learning algorithms to predict extremes using the sea surface salinity precursors will be developed and applied. Novel approaches will be used in this project, including the use of Bayesian statistics to identify the optimal sea surface salinity and temperature predictors for rainfall extremes, analysis of the oceanic salinity budget to identify the driving atmospheric variables, analysis of the atmospheric circulations that transport water from ocean to land, and the development of machine learning algorithms to provide optimal seasonal predictions of extreme drought or floods."
"1718116","SaTC: TTP: Small: Collaborative: Privacy-Aware Wearable-Assisted Continous Authentication Framework","CNS","Secure &Trustworthy Cyberspace","08/15/2017","07/28/2017","A. Selcuk Uluagac","FL","Florida International University","Standard Grant","Robert Beverly","12/31/2020","$357,200.00","Kemal Akkaya","suluagac@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","8060","025Z, 7434, 7923","$0.00","The login process for a mobile or desktop device does not guarantee that the person using it is necessarily the intended user. If one is logged in for a long period of time, the user's identity should be periodically re-verified throughout the session without impacting their experience, something that is not easily achievable with existing login and authentication systems. Hence, continuous authentication, which re-verifies the user without interrupting their browsing session, is essential. However, authentication in such settings is highly intrusive and may expose users' sensitive information to third parties. To address these concerns, this project develops a novel privacy-aware wearable-assisted continuous authentication (WACA) framework. User specific data is acquired through built-in sensors on a wearable device. The user data is goes through privacy-preserving operations throughout the authentication process. This login procedure can be applied to a wide variety of existing enterprise authentication systems such as university campuses, corporate Information Technology divisions, and government agencies. Prototype deployments at Florida International University (FIU) and Florida Atlantic University (FAU), both of which serve large and diverse student populations, provide valuable feedback for future improvements. Continuous authentication and digital privacy are timely and relevant topics in today's Internet-centric always-on society.  <br/><br/>This project exploits the ubiquitous nature of sensor-based wearables by designing an innovative usable continuous authentication mechanism. By leveraging the expertise of the project team on authentication, privacy-preservation, and machine learning, this project addresses the following problems: 1) Investigation of novel sensory features on wearable smartwatches and identification of an optimal subset of these features along with distance measures and machine-learning algorithms to strike the balance between accuracy and speed; 2) Discovery of novel privacy-preserving mechanisms based on secure noise-tolerant template generation and comparison techniques, multi-party computation, and homomorphic encryption; 3) Trade-offs between privacy and performance to optimize the scheme in terms of accuracy, efficiency, and security; 4) Security of sensor-based keystroke dynamics against some common attacks such as simple zero-effort, imitation, and more complex statistical attacks including, but not limited to, classical keyboard-only keystroke dynamics attacks; and 5) Development, testing, and deployment of the proposed framework with a rich set of users, devices, and usage context in a prototype system. The success of the WACA project will contribute to the growth of knowledge in privacy and authentication domains and to societal understanding of these matters."
"1740761","TRIPODS: Topology, Geometry, and Data Analysis (TGDA@OSU):Discovering Structure, Shape, and Dynamics in Data","CCF","TRIPODS Transdisciplinary Rese","10/01/2017","09/04/2019","Tamal Dey","OH","Ohio State University","Continuing Grant","Christopher Stark","09/30/2021","$1,500,000.00","Yusu Wang, David Sivakoff, Sebastian Kurtek, Facundo Memoli","dey.831@gmail.com","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","041Y","047Z, 062Z","$0.00","This project will advance the methodological and theoretical foundations of data analytics by considering the geometric and topological aspects of complex data from mathematical, statistical and algorithmic perspectives, thus enhancing the synergy between the Computer Science, Mathematics, and Statistics communities. Furthermore, this project will benefit a range of impactful scientific areas including medicine, neuronanatomy, machine learning, geographic information systems, mechanical engineering designs, and political science.  The research products will be implemented and disseminated through software packages and tutorials, allowing widespread application by industrial and academic practitioners.  Through this project, the PIs will develop curricula for cross-disciplinary, undergraduate and graduate education.  There is already extant data science curriculum offered jointly between Statistics and Computer Science and Engineering at The Ohio State University (OSU), including the recent Data Analytics undergraduate major, providing a platform to develop new courses and an opportunity to engage future industry leaders in basic research.  Additionally, this project aims to develop partnerships with the Translational Data Analytics and the Mathematical Biosciences Institutes at OSU, as well as other internal and external research and education centers.  Plans for workshops and summer schools are included for outreach and training purposes.<br/><br/>In the past few decades, a large number of models, methods, and algorithmic frameworks have been developed for data science.  However, as data become increasingly more complex, the field faces new challenges.  In particular, the non-Euclidean nature, the higher order connectivity, the hidden global cues, and the dynamics regulating the data pose further challenges to existing methods.  This project will explore and leverage the geometric and topological structures inherent in the data to tackle some of these problems.  The main aims are to discover, model and reveal information in the form of (i) structures in data, (ii) shapes from data, and (iii) dynamics underlying data.  This project leverages concepts from mathematical areas of differential and algebraic topology and geometry, applied statistics and combinatorics, and computational areas of algorithms, graph theory, and statistical/machine learning.  Research in geometric and topological data analysis has brought forth the need to recast and reinvestigate classical concepts in statistics and mathematics in the context of finite data, approximations, and noise.  This project investigates explicit or hidden structures behind data, such as cluster trees, which are the basis for understanding and efficient processing of data.  Additionally, the PIs aim to model the precise shape behind data globally or locally, which are essential for providing a platform where various statistical analyses can be carried out.  Particular examples include the shape space of surface models and the tree space of phylogenetic trees.  Finally, this project will consider dynamics in the data, where the interplay between temporal and topological/geometric features can lead to deeper insights.  All of these areas will inevitably be enriched by new applications."
"1718109","SaTC: TTP: Small: Collaborative: Privacy-Aware Wearable-Assisted Continous Authentication Framework","CNS","Secure &Trustworthy Cyberspace","08/15/2017","07/28/2017","Koray Karabina","FL","Florida Atlantic University","Standard Grant","Robert Beverly","07/31/2020","$142,280.00","","kkarabina@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","8060","025Z, 7434, 7923","$0.00","The login process for a mobile or desktop device does not guarantee that the person using it is necessarily the intended user. If one is logged in for a long period of time, the user's identity should be periodically re-verified throughout the session without impacting their experience, something that is not easily achievable with existing login and authentication systems. Hence, continuous authentication, which re-verifies the user without interrupting their browsing session, is essential. However, authentication in such settings is highly intrusive and may expose users' sensitive information to third parties. To address these concerns, this project develops a novel privacy-aware wearable-assisted continuous authentication (WACA) framework. User specific data is acquired through built-in sensors on a wearable device. The user data is goes through privacy-preserving operations throughout the authentication process. This login procedure can be applied to a wide variety of existing enterprise authentication systems such as university campuses, corporate Information Technology divisions, and government agencies. Prototype deployments at Florida International University (FIU) and Florida Atlantic University (FAU), both of which serve large and diverse student populations, provide valuable feedback for future improvements. Continuous authentication and digital privacy are timely and relevant topics in today's Internet-centric always-on society.  <br/><br/>This project exploits the ubiquitous nature of sensor-based wearables by designing an innovative usable continuous authentication mechanism. By leveraging the expertise of the project team on authentication, privacy-preservation, and machine learning, this project addresses the following problems: 1) Investigation of novel sensory features on wearable smartwatches and identification of an optimal subset of these features along with distance measures and machine-learning algorithms to strike the balance between accuracy and speed; 2) Discovery of novel privacy-preserving mechanisms based on secure noise-tolerant template generation and comparison techniques, multi-party computation, and homomorphic encryption; 3) Trade-offs between privacy and performance to optimize the scheme in terms of accuracy, efficiency, and security; 4) Security of sensor-based keystroke dynamics against some common attacks such as simple zero-effort, imitation, and more complex statistical attacks including, but not limited to, classical keyboard-only keystroke dynamics attacks; and 5) Development, testing, and deployment of the proposed framework with a rich set of users, devices, and usage context in a prototype system. The success of the WACA project will contribute to the growth of knowledge in privacy and authentication domains and to societal understanding of these matters."
"1639716","EarthCube Data Infrastructure:  Collaborative Proposal:  A unified experimental-natural digital data system for cataloging and analyzing rock microstructures","ICER","EarthCube","09/01/2017","08/18/2017","Yolanda Gil","CA","University of Southern California","Standard Grant","Eva Zanzerkia","08/31/2020","$60,500.00","","gil@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","GEO","8074","7433","$0.00","When viewed at the micro-scale, rocks reveal structures that help to interpret the processes and forces responsible for their formation.  These microstructures help to explain phenomena that occur at the scale of mountains and tectonic plates.  Interpretation of microstructures formed in nature during deformation is aided by comparison with those formed during experiments, under known conditions of pressure, temperature, stress, strain and strain rate, and experimental rock deformation benefits from the ground truth offered through comparison with rocks deformed in nature.  However, the ability to search for relevant naturally or experimentally deformed microstructures is hindered by the lack of any database that contains these data.  The researchers collaborating on this project will develop a single digital data system for rock microstructures to facilitate the critical interaction between and among the communities that study naturally and experimentally deformed rocks.  To aid in the comparison of microstructures formed in nature and experiment, we will link to commonly used analytical tools and develop a pilot project for automatic comparison of microstructures using machine learning.   <br/><br/>Rock microstructures relate processes at the microscopic scale to phenomena at the outcrop, orogen, and plate scales and reveal the relationships among stress, strain, and strain rate.  Quantitative rheological information is obtained through linked studies of naturally formed microstructures with those created during rock deformation experiments under known conditions.  The project will develop a single digital data system for both naturally and experimentally deformed rock microstructure data to facilitate comparison of microstructures from different environments.  A linked data system will facilitate interaction between practitioners of experimental deformation, those studying natural deformation and the cyberscience community.  The data system will leverage the StraboSpot data system currently under development in Structural Geology and Tectonics.  To develop this system requires: 1) Modification of the StraboSpot data system to accept microstructural data from both naturally and experimentally deformed rocks; and 2) Linking the microstructural data to its geologic context ? either in nature, or its experimental data/parameters.  The researchers will engage the rock deformation community with the goal of establishing data standards and protocols for data collection, and integrate our work with ongoing efforts to establish protocols and techniques for automated metadata collection and digital data storage.  To analyze the microstructures studied and/or generated by these communities, we will ensure StraboSpot data output is compatible with commonly used microstructural tools.  They will develop a pilot project for comparing and analyzing microstructures from different environments using machine-learning."
"1714091","SHF: Small: Making Strassen's Algorithm Practical","CCF","Software & Hardware Foundation","08/01/2017","07/19/2017","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Almadena Chtchelkanova","02/28/2021","$465,884.00","Margaret Myers, Field Van Zee","rvdg@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 7942, 9251","$0.00","High-performance linear algebra software libraries are at the core of scientific computing and machine learning applications.  At the core of many high-performance linear algebra libraries lies the matrix multiplication operation because many other matrix operations can be cast in terms of matrix multiplication and matrix multiplication itself can attain high performance.  Strassen?s algorithm, first proposed in 1969, is a clever scheme for reducing the number of arithmetic calculations that must be performed when computing a matrix multiplication.  It has mostly been a theoretical curiosity that has led to a sequence of improvements over the years. Some practical applications of Strassen?s algorithm for very large problem sizes have been encountered in, for example, the aerospace industry.  Very recently, it was shown that Strassen?s algorithm, and some of its variations, can be made practical for small problem sizes, opening up a range of new academic and practical directions of research.  The project will pursue these directions and will incorporate the advances in high-performance software libraries.  In essence, it will give the user a performance boost of up to around 30%, for free. <br/><br/>The proposed work will create a practical framework and analysis for the implementation of a broad family of Strassen-like algorithms, building on a model of computation that captures the interaction between software and hardware. This will yield the most thorough understanding to date of the practical implementation of such algorithms.  The proposed project will also deliver a software library for practical use in computational science and machine learning applications that cast computation in terms of matrix-matrix multiplication and/or tensor contractions, with a mechanism for choosing the best algorithm from that family. It builds on recent advances regarding the high-performance implementation of linear algebra software libraries.  What was shown was that such libraries can be composed from small kernels that can be highly optimized for a specific architecture.  These kernels have become the building blocks for traditional algorithms for matrix operations.  In this research, they also become the building blocks for high-performance algorithms that incorporate Strassen?s algorithm and closely related so-called fast matrix multiplication algorithms.  The resulting software will be released under open source license to facilitate its use and study. Pedagogical outreach will include the development of a Massive Open Online Course on ""Programming for Performance"" in which Strassen-like algorithms and their practical implementation will be a prominent enrichment. The project involves several members from traditionally underrepresented groups and will continue a long tradition of involvement by undergraduates."
"1723529","SCH: EAGER: New Approach: Early Diagnosis of Alzheimer's Disease Based on Magnetic Resonance Imaging (MRI) via High-Dimensional Image Feature Identification","IIS","Smart and Connected Health","08/01/2017","05/24/2018","Xiuzhen Huang","AR","Arkansas State University Main Campus","Standard Grant","Wendy Nilsen","07/31/2021","$257,785.00","Shuzhong Zhang","xhuang@astate.edu","504 University Loop East","Jonesboro","AR","724672760","8709722694","CSE","8018","7916, 8018, 9150, 9251","$0.00","Alzheimer's disease (AD) is a form of progressive neurodegenerative dementia and one of the most common diseases in the aging population. Early diagnosis of AD is strongly recommended for several reasons. First, it can helps to significantly reduce the social and economic impacts caused by AD and allow people to better manage and plan ahead. Second, it may provide more information for  researchers seeking new scientific approaches for early treatment and intervention. However, in current clinical practice early diagnosis is often a challenge. While neuroimaging is routinely collected in hospitals, it is very hard for radiologists to manually read the high-dimensional image data for analysis and interpretation. This project proposes untested but potentially transformative research approaches to identify high-dimensional image features for AD early diagnosis based on Magnetic Resonance Imaging (MRI). This project will advance the research in machine learning, optimization, statistics, image science and bioinformatics, and potentially be used to address other high-dimensional images besides brain images. The project also has broader impacts through cross-disciplinary research, training and education.<br/> <br/>This project has the following two aims: 1) Develop sparse coding based algorithms to identify features of structural MRI images for classifying AD patients and other diagnostic groups. This will allow the key structural features of images that separate AD patients, individuals with mild cognitive impairment (MCI) or healthy individuals to be identified. 2) Develop optimization and machine learning algorithms based on tensor Tucker core decomposition for high-dimensional image-marker detection from longitudinal functional MRI images. This approach should reduce the high computational complexity of marker detection from the longitudinal MRI images of AD patients. It is anticipated that the developed algorithms will enhance high-dimensional neuroimaging marker detection and diagnostic classification. This research project, if successful, will greatly impact the current practice of AD diagnosis by providing clinical doctors with the information from a larger population and also significantly easing the burden of radiologists."
"1724843","CIF21 DIBBs: PD: Cyberinfrastructure Tools for Precision Agriculture in the 21st Century","OAC","Hydrologic Sciences, Data Cyberinfrastructure","07/01/2017","05/09/2017","Michela Taufer","DE","University of Delaware","Standard Grant","Amy Walton","10/31/2018","$499,999.00","Rodrigo Vargas","taufer@utk.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1579, 7726","7433, 8048, 9150","$0.00","This interdisciplinary project applies computer science approaches and computational resources to large multidimensional environmental datasets, and synthesizes this information into finer resolution, spatially explicit products that can be systematically analyzed with other variables.  The main emphasis is ecoinformatics, a branch of informatics that analyzes ecological and environmental science variables such as information on landscapes, soils, climate, organisms, and ecosystems.  The project focuses on synthesis/computational approaches for producing high-resolution soil moisture datasets, and the pilot application is precision agriculture. The effort combines analytical geospatial approaches, machine learning methods, and high performance computing (HPC) techniques to build cyberinfrastructure tools that can transform how ecoinformatics data is analyzed.<br/><br/>The investigators build upon publicly available data collections (soil moisture datasets, soil properties datasets, and topography datasets) to develop: (1) tools based on machine-learning techniques to downscale coarse-grained data to fine-grained datasets of soil moisture information; (2) tools based on HPC techniques to estimate the degree of confidence and the probabilities associated with the temporal intervals within which soil-moisture-base changes, trends, and patterns occur; and (3) data- and user- interfaces integrating data preprocessing to deal with data heterogeneity and inaccuracy, containerized environments to assure portability, and modeling techniques to represent temporal and spatial patterns of soil moisture dynamics. The tools will inform precision agriculture through the generation and use of unique information on soil moisture for the coterminous United States.  Accessibility for field practitioners (e.g., local soil moisture information) is made possible through lightweight virtualization, mobile devices, and web applications.<br/> <br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Earth Sciences within the NSF Directorate for Geosciences."
"1740425","RCN: DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization","CCF","Special Projects - CCF","09/15/2017","03/18/2020","Tamra Carpenter","NJ","Rutgers University New Brunswick","Standard Grant","Tracy Kimbrel","08/31/2021","$500,000.00","Richard Karp, Shafrira Goldwasser, Tamra Carpenter, David Pennock","tcar@dimacs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","2878","7926","$0.00","Optimization tools and algorithms have transformed fields ranging from biology to finance, and they touch everyday lives through more efficient supply chains, better traffic management, and more secure power grids. New applications, particularly those stemming from machine learning and data science, are now challenging the field to solve larger and more complex problems on smaller devices in less time. The field is responding with innovative approaches leading to advances such as faster algorithms for maximum flow and near-real-time approximations, more efficient interior-point methods, and faster cutting-plane methods. Many of these breakthroughs bring together ideas from both continuous and discrete optimization. The DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization aims to accelerate progress by stimulating collaboration across the many communities of optimization. The planned activities bring together computer scientists, mathematicians, operations researchers, engineers, statisticians, and algorithm developers to advance both the foundations and applications of optimization. <br/><br/>The project begins with an intensive program at the Simons Institute during the fall semester of 2017 that launches the collaboration and builds momentum for activities conducted over the ensuing two years as part of the DIMACS Special Focus on Bridging Continuous and Discrete Optimization. The DIMACS special focus includes seven workshops that sustain the project through the end of 2019 and expand it to include more people and more topics. The project aims to improve the performance of optimization methods in challenging real settings with the potential to positively impact society by improving traditional applications in logistics, supply chains, engineering, infrastructure, and finance, as well as growing applications in machine learning and data science. The project will involve a large number of people in various scientific communities and expose them to new ideas, new problems, and new opportunities for collaboration. Participants will be diverse across a variety of dimensions, including women and other under-represented groups; a mix of junior and senior participants; people from multiple disciplines; and both industry and academic participants. There will also be international coordination with the Centre de Recherches Mathématiques and Polytechnique Montreal."
"1729149","DMREF: Collaborative Research: Accelerating Thermoelectric Materials Discovery via Dopability Predictions","DMR","DMREF","10/01/2017","08/13/2018","Elif Ertekin","IL","University of Illinois at Urbana-Champaign","Standard Grant","John Schlueter","09/30/2021","$370,000.00","","ertekin@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","8292","054Z, 8396, 8400, 8607","$0.00","Non-technical Description: Thermoelectric devices, which transform heat flow into electrical power and vice versa, have the potential to revolutionize how society produces electricity and cooling.  However, thermoelectric materials suffer from poor power conversion efficiency and the search continues for new materials with enhanced performance. In this project, advances in computation and machine learning are leveraged to accelerate this search for advanced thermoelectric materials. These efforts build upon the prior NSF DMREF research of some of team members on predicting a material's potential for thermoelectric performance. High throughput screening focused on identifying semiconductors with desirable electronic and vibrational properties.  However, these efforts did not include a strong focus on the role of intrinsic defect or the potential for dopability.  In the next stage of this research, these critical components will be pursued through a mixture of high throughput theory, experimental validation, and machine learning.  Together, these efforts will yield accurate prediction of the thermoelectric potential for thousands of semiconductors and the realization of new materials for solid state power generation.  Beyond thermoelectric materials, these efforts to establish a dopability recommendation engine will be critical in the development of next generation microelectronic and optoelectronic materials such as transparent conductors and photovoltaic absorbers.   <br/><br/>Technical Description: The project's ultimate objective is to build a robust and accurate dopability recommendation engine to overcome the dopability bottleneck in thermoelectric materials discovery. The recommendation engine will use materials informatics to enable high-throughput predictions of dopability, relying only on quantities that are inexpensive to calculate, experimental measurements, and known structural/chemical features as inputs. It will thus allow dopability screening of thousands of compounds.  First, an accurate training set will be built for the recommendation engine containing native defect formation enthalpies and structural/chemical descriptors from a diverse array of thermoelectric-relevant compounds. Whereas prior dopant studies focused on single compounds, a new, automated calculation infrastructure will be leveraged that allows the rapid creation of an extensive training set, initially containing approximately 30 compounds but growing to over 100 during the project. Experimental charge transport and local dopant structure measurements will validate the training set. Second, the prediction engine will be trained on the data to extract patterns and correlations, and ultimately identify robust descriptors of dopability.  Initially, the engine will predict if `killer' defects limit the available dopant range. The engine will ultimately grow to suggest specific extrinsic dopants for compounds that pass this initial screening.  Together, this combination of accurate predictions of intrinsic transport properties (prior DMREF) and dopability (proposed DMREF) is expected to accelerate the discovery process for thermoelectric materials."
"1721069","I-Corps: Probabilistically Detecting Controversy","IIP","I-Corps","03/01/2017","02/16/2017","James Allan","MA","University of Massachusetts Amherst","Standard Grant","Anita La Salle","02/28/2018","$50,000.00","","allan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is in leveraging past research on detecting controversy and bringing this technology to a commercial setting. The project provides actionable insights to organizations interested in securities trading, through the usage of sophisticated machine learning and information retrieval techniques for controversial topics. The project will initially explore markets in the financial services sector that derive some portion of the value for their clients from acquired data streams. Besides the financial industry, there are applications of the technology in reputation and crisis management, as well as scope for social impact by opening analysis and explanatory power of controversies to individual users. The project has the potential to help users become better informed and more capable of critically evaluating the often-overwhelming stream of content.<br/><br/>This I-Corps project technology relies on sophisticated machine learning and information retrieval techniques in order to automatically detect controversial topics. The enabling NSF-supported research recognized controversy by mapping a webpage to algorithmically-identified controversial topics. The proposed commercialization effort is unique because it addresses controversy and crisis situations. The project's hypothesis is that this technology can be successfully commercialized to the financial, equity-trading industry given their revenue model and cost structure. The project will use the lean startup method to perform customer interviews and explore the viability of a scalable business model."
"1721343","SBIR Phase I:  Robust Medical Data Aggregation to Enable Advanced Approaches to Precision Medicine","IIP","SBIR Phase I","07/01/2017","06/30/2017","Ganapati Srinivasa","OR","Omics Data Automation, Inc.","Standard Grant","Nancy Kamei","06/30/2018","$224,903.00","","gans@omicsautomation.com","12655 Beaverdam Road","Beaverton","OR","970052129","5034756660","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to enhance the impact of precision medicine by simultaneously addressing large-scale medical data aggregation and optimized computation that is cost-effective and to extend the utility of medical informatics well beyond current practice. Patient medical information comes in many diverse forms: genomic sequences, medical images, and clinical observations. The integration of these various data sources across patient populations have shown to reveal patterns and similarities among patients, which inform treatment options. With advances in imaging and genomic sequencing technologies, the sheer volume of available information is growing exponentially, straining current computational approaches, and creating an imminent need for scalable data integration. The ability to overcome this data mountain opens the door to support advanced analytics to support precision medicine and provide enhanced services to medical institutions. With these innovations, patients receive faster and more accurate diagnoses and treatments, clinicians deliver verified treatment decisions through patient cohort comparison, hospitals have better standard of care, and society is overall empowered by supporting global treatment options and well informed pharmaceutical development.<br/><br/>The proposed project will develop a scalable aggregation and analysis framework to integrate various patient data modalities to inform personalized diagnosis and therapy in precision medicine. Currently, information from different modalities exists in silos, hindering joint analysis and insight. While there has been research trying to leverage machine learning techniques in medical imaging, these efforts have generally focused on a single domain and not been able to integrate facts from other domains. This project will aggregate features from genomics, imaging and clinical characterization of patients into scalable databases and then use a distributed, parallel framework to enable efficient analytics on the resultant joint representation. The resulting platform will enable identification of cohorts based on both genotypes and phenotypes and empower powerful machine learning analyses to inform clinical decision systems or identification of new personalized therapies."
"1740990","BIGDATA:  IA:  Collaborative Research: In Situ Data Analytics for Next Generation Molecular Dynamics Workflows","IIS","Big Data Science &Engineering","10/01/2017","07/06/2018","Harel Weinstein","NY","Joan and Sanford I. Weill Medical College of Cornell University","Standard Grant","Almadena Chtchelkanova","09/30/2021","$497,056.00","Michel Cuendet, Harel Weinstein","hweinstein@inka.mssm.edu","1300 York Avenue","New York","NY","100654805","6469628290","CSE","8083","7433, 7942, 8083","$0.00","Molecular dynamics simulations studying the classical time evolution of a molecular system at atomic resolution are widely recognized in the fields of chemistry, material sciences, molecular biology and drug design; these simulations are one of the most common simulations on supercomputers.  Next-generation supercomputers will have dramatically higher performance than do current systems, generating more data that needs to be analyzed (i.e., in terms of number and length of molecular dynamics trajectories). The coordination of data generation and analysis cannot rely on manual, centralized approaches as it does now.  This interdisciplinary project integrates research from various areas across programs such as computer science, structural molecular biosciences, and high performance computing to transform the centralized nature of the molecular dynamics analysis into a distributed approach that is predominantly performed in situ. Specifically, this effort combines machine learning and data analytics approaches, workflow management methods, and high performance computing techniques to analyze molecular dynamics data as it is generated, save to disk only what is really needed for future analysis, and annotate molecular dynamics trajectories to drive the next steps in increasingly complex simulations' workflows. <br/><br/>The investigators tackle the data challenge of data analysis of molecular dynamics simulations on the next-generation supercomputers by (1) creating new in situ methods to trace molecular events such as conformational changes, phase transitions, or binding events in molecular dynamics simulations at runtime by locally reducing knowledge on high-dimensional molecular organization into a set of relevant structural molecular properties; (2) designing new data representations and extend unsupervised machine learning techniques to accurately and efficiently build an explicit global organization of structural and temporal molecular properties; (3) integrating simulation and analytics into complex workflows for runtime detection of changes in structural and temporal molecular properties; and (4) developing new curriculum material, online courses, and online training material targeting data analytics. The project's harnessed knowledge of molecular structures' transformations at runtime can be used to steer simulations to more promising areas of the simulation space, identify the data that should be written to congested parallel file systems, and index generated data for retrieval and post-simulation analysis. Supported by this knowledge, molecular dynamics workflows such as replica exchange simulations, Markov state models, and the string method with swarms of trajectories can be executed ?from the outside? (i.e., without reengineering the molecular dynamics code)."
"1714741","III: Small: Unsupervised Feature Selection in the Era of Big Data","IIS","Info Integration & Informatics","08/15/2017","08/04/2017","Jiliang Tang","MI","Michigan State University","Standard Grant","Wei Ding","07/31/2020","$480,398.00","Jiayu Zhou","tangjili@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7364","7364, 7923","$0.00","Feature selection has been proven to be efficient and effective in preparing high-dimensional data for data mining and machine learning applications, especially when the original features are important for model interpretation and knowledge extraction. The growth of data in both size and complexity accelerates rapidly as the dramatic increase of the capacity to collect data. Such big data has imposed tremendous challenges on traditional feature selection methods, which are usually designed to handle homogeneous and static data in a centralized fashion. Meanwhile in many real-world domains big data is unlabeled, which further exacerbates the difficulty. Therefore, the majority of existing feature selection methods are not well prepared for big data, and this thus calls for the development of novel unsupervised feature selection for unlabeled big data. The project extends the state-of-the-art feature selection research to a new frontier of taming big data. It has potential to benefit a number of real-world applications from various disciplines such as Computer Science, Business, Education, Politics, Healthcare and Bioinformatics. <br/><br/>This project proposes a suite of novel approaches for unsupervised feature selection to facilitate the computational understanding of big data, investigating associated fundamental research issues and developing effective algorithms. It consists of three major thrusts.  First, it studies various strategies to scale unsupervised feature selection to handle large-scale and distributed data; and investigates distributed unsupervised feature selection with structured features and under asynchronous updates. Second, it develops a family of heterogeneous unsupervised feature selection with multiple types of heterogeneity.  Third, it defines the unsupervised feature selection with various streaming scenarios, and develops new algorithms to improve the capability of unsupervised feature selection in handling the corresponding streaming settings. Disparate means are planned to disseminate the project and its findings, including web enabled data and software repositories, books, journal and conference publications, special-purpose workshops or tutorials, and external collaborations. The project lies at the confluence of feature selection, big data analysis, machine learning and data mining. It can be effectively integrated to undergraduate and graduate courses as well as in student research projects."
"1737785","Collaborative Research: Phylanx: Python based Array Processing in HPX","CCF","Software & Hardware Foundation, ","08/15/2017","08/03/2017","Hartmut Kaiser","LA","Louisiana State University","Standard Grant","Almadena Chtchelkanova","07/31/2019","$373,200.00","Steven Brandt","hkaiser@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7798, Q212","7942, 9150","$0.00","The availability and size of data sets has increased significantly over the course of the past decade. To enable the analysis of large data sets on High Performance Computing (HPC) resources while minimizing time- and energy-to-solution requires incorporating static and runtime information to determine the best possible data layout of the large data arrays used by an application to minimize data movement. The goal of this proposal is to deliver Phylanx, a general purpose framework supporting a variety of data science, machine learning, and statistically oriented applications. Phylanx is designed such that a user?s code will be able to perform efficiently on current and future architecture as long as the runtime system is maintained. This greatly reduces the maintenance burden and will increase the productivity of domain scientists. Phylanx lays a solid foundation for technology transfer from academia to industry and fills the gap between academic innovation and commercial application, by creating a software layer that industrial partners can feel confident relying upon. <br/>Phylanx is a scalable, array-based and distributed framework targeting HPC systems using the HPX, dynamic asynchronous task-based parallel runtime system. The dataflow-style capabilities exposed by HPX guarantee the preservation of all data-dependencies even for complex distributed workflows. This project overcomes some of the limitations of existing Big Data solutions such as Hadoop, Spark, and Flink by providing users the ability to: implement NumPy-styled expression-graphs using Python or C/C++, optimize these graphs for optimal data layout, distribution, tiling, and minimal communication overheads, and evaluate those graphs with high efficiency on a runtime interpreter targeting distributed HPC systems. Additionally, Phylanx uses greedy sub-modular techniques on the expression tree to provide a mathematically provable guarantee of optimal performance in machine learning domains and in data placement problems. The platform will provide implementations of 6 benchmarks which have been selected for their domain specificity in text, image, and graph applications."
"1638352","Research in Mathematics","DMS","MATHEMATICAL SCIENCES RES INST","09/01/2017","08/30/2019","Helmut Hofer","NJ","Institute For Advanced Study","Continuing Grant","Marian Bocea","08/31/2020","$7,998,750.00","Jean Bourgain, Peter Sarnak","hofer@ias.edu","EINSTEIN DRIVE","PRINCETON","NJ","085404907","6097348000","MPS","7333","","$0.00","The School of Mathematics (SoM) at the Institute for Advanced Study (IAS) has two primary goals.  First, it supports fundamental research in mathematics, encourages original thinking, and produces significant advances in knowledge.  Second, it invests in the development of a diverse pool of mathematical talent.  The School strongly encourages the participation of women and members of other groups underrepresented in mathematics, and is dedicated to furthering the education and careers of all of its postdoctoral fellows. Junior mathematicians benefit from both their interactions with senior scholars and the mentoring they receive from faculty members.  Mid-career and senior mathematicians consolidate their research projects and explore new research directions.    The School's emphasis on a unified, interdisciplinary mathematical perspective and its work in bringing together a diverse group of mathematicians has led to numerous unanticipated mathematical developments.  The School is in a strong position to identify, at an early stage, subject areas with the potential for important new developments. The School is led by eight permanent faculty members whose expertise span a wide spectrum of mathematics.  As a center for new collaborations and new lines of research, the School hosts approximately 75 visiting mathematicians annually whose collaborative research visits typically last 4-10 months.  This grant supports 23 of these scholars (17 postdoctoral fellows and 6 mid-career mathematicians), who join approximately 50 other mathematicians supported by funds from other sources.  It also provides funds to  host small groups of collaborators during the summer.<br/><br/>The School of Mathematics provides this diverse community of scholars with an interactive environment, rich in mathematical content and conducive to collaboration.  Each year SoM conducts a special program focused on a particularly exciting field of mathematics.  The programs for the next three years will be ""Locally Symmetric Spaces: Analytical and Topological Aspects"" in 2017-2018; ""Variational Methods in Geometry"" in 2018-2019; and ""Optimization, Statistics, and Theoretical Machine Learning"" in 2019-2020.  Each semester SoM facilitates a week-long workshop associated with the special topical program, as well as a separate week-long working group.  The ""Emerging Topics Working Group"" is a group of researchers selected to work on a topic which seems ripe for significant progress.  Other programs include ""Mathematical Conversations,"" an informal discussion group that meets once a week, and ""Summer Collaborators,"" small groups of invited researchers who collaborate on special research projects during the summer months.  In addition to these workshops and programs, the SoM offers regular seminars, lecture series, and reading groups.  All IAS lectures are videotaped and are available to the public. Visiting mathematicians supported by the grant receive individual and group mentoring from the permanent faculty members.  Their individual research projects dive into one or more of the following fields of study: Analysis, Partial Differential Equations of Applied Mathematics, Probability, Algebra, Algebraic Geometry, Lie Groups, Representation Theory, Differential Geometry, Topology, Mathematical Physics, Dynamical Systems, Computer Science, Discrete Mathematics, Theoretical Machine Learning, and Number Theory."
"1741040","BIGDATA: IA: Collaborative Research: In Situ Data Analytics for Next Generation Molecular Dynamics Workflows","IIS","Big Data Science &Engineering","10/01/2017","09/13/2017","Rafael Ferreira da Silva","CA","University of Southern California","Standard Grant","Almadena Chtchelkanova","09/30/2021","$516,000.00","Ewa Deelman","rafsilva@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8083","7433, 7942, 8083","$0.00","Molecular dynamics simulations studying the classical time evolution of a molecular system at atomic resolution are widely recognized in the fields of chemistry, material sciences, molecular biology and drug design; these simulations are one of the most common simulations on supercomputers.  Next-generation supercomputers will have dramatically higher performance than do current systems, generating more data that needs to be analyzed (i.e., in terms of number and length of molecular dynamics trajectories). The coordination of data generation and analysis cannot rely on manual, centralized approaches as it does now.  This interdisciplinary project integrates research from various areas across programs such as computer science, structural molecular biosciences, and high performance computing to transform the centralized nature of the molecular dynamics analysis into a distributed approach that is predominantly performed in situ. Specifically, this effort combines machine learning and data analytics approaches, workflow management methods, and high performance computing techniques to analyze molecular dynamics data as it is generated, save to disk only what is really needed for future analysis, and annotate molecular dynamics trajectories to drive the next steps in increasingly complex simulations' workflows. <br/><br/>The investigators tackle the data challenge of data analysis of molecular dynamics simulations on the next-generation supercomputers by (1) creating new in situ methods to trace molecular events such as conformational changes, phase transitions, or binding events in molecular dynamics simulations at runtime by locally reducing knowledge on high-dimensional molecular organization into a set of relevant structural molecular properties; (2) designing new data representations and extend unsupervised machine learning techniques to accurately and efficiently build an explicit global organization of structural and temporal molecular properties; (3) integrating simulation and analytics into complex workflows for runtime detection of changes in structural and temporal molecular properties; and (4) developing new curriculum material, online courses, and online training material targeting data analytics. The project's harnessed knowledge of molecular structures' transformations at runtime can be used to steer simulations to more promising areas of the simulation space, identify the data that should be written to congested parallel file systems, and index generated data for retrieval and post-simulation analysis. Supported by this knowledge, molecular dynamics workflows such as replica exchange simulations, Markov state models, and the string method with swarms of trajectories can be executed ?from the outside? (i.e., without reengineering the molecular dynamics code)."
"1704469","CSR: CHS: Medium: Collaborative Research: Improving Pedestrian Safety in Urban Cities using Intelligent Wearable Systems","CNS","Special Projects - CNS, CSR-Computer Systems Research","06/01/2017","09/17/2019","Shahriar Nirjon","NC","University of North Carolina at Chapel Hill","Continuing Grant","Erik Brunvand","05/31/2021","$320,782.00","","nirjon@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1714, 7354","7924","$0.00","Using smartphones while walking poses an increasingly common safety problem for people in urban environments. Whether listening to music, texting, or talking, pedestrians that are absorbed with their smartphones are considerably less likely to notice important auditory cues of danger, such as the honks and sounds of approaching vehicles, putting pedestrians at far greater risk of being hit. This project aims to develop an intelligent wearable system that uses miniature microphones - embedded in earphones or headsets - to detect and locate approaching vehicles and warn the wearer of imminent dangers from cars, buses, motorbikes, trucks, and trams. <br/><br/>The system comprises multiple microphones embedded in a wearable headset, an ultra-low-power feature extraction and data  processing pipeline, and a set of machine-learning classifiers running on a smartphone. This project is organized in four research thrusts: (1) designing an architecture and data processing pipeline for a wearable system composed of heterogeneous embedded modules; (2) devising an ultra-low-power, analog, signal-processing application-specific integrated circuit (ASIC) for energy-efficient, on-board feature extraction; (3) modeling and optimizing machine-learning classifiers for acoustic event detection and localization; and (4) designing an interface and feedback mechanisms that are optimized for the users' perceptual, cognitive, and motor control abilities.<br/><br/>This research will help reduce pedestrian injuries and fatalities, and expand knowledge on designing wearable systems for enhancing safety in cities, workplaces, and the home. The underlying framework can be generalized to other systems - employing low-power signal processors and algorithms to solve real-time sensing and classification problems of many kinds. Research products will be made publicly available to anyone to apply these techniques in their own system designs. Course modules developed on embedded systems, mobile computing, and Internet-of-Things will be used at the three participating universities to train undergraduate and graduate students, and will be made available online."
"1708299","Collaborative Research: ACI-CDS&E: Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES)","OAC","CDS&E-MSS, CDS&E","09/01/2017","08/24/2017","Saeid Nooshabadi","MI","Michigan Technological University","Standard Grant","Tevfik Kosar","08/31/2021","$349,988.00","","saeid@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","8069, 8084","026Z, 7433, 8084, 9263","$0.00","Embedded processors are ubiquitous, from toasters and microwave ovens, to automobiles, planes, drones and robots and are typically very small processors that are compute and memory constrained. Real-time embedded systems have the additional requirement of completing tasks within a certain time period to accurately and safely control appliances and devices like automobiles, planes, robots, etc. Convex optimization has emerged as an important mathematical tool for automatic control and robotics and other areas of science and engineering disciplines including machine learning and statistical information processing.  In many fields, convex optimization is used by the human designers as optimization tool where it is nearly always constrained to problems solved in a few hours, minutes or seconds. Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES) project takes advantage of the recent advances in embedded hardware and optimization techniques to explore opportunities for real-time convex optimization on the low-cost embedded systems in these disciplines in milli- and micro-seconds. The development of novel algorithms and their high-performance implementations for the real-time solution of practical engineering and scientific optimization problems on the embedded system will open new opportunities in the area of emerging computational science and engineering for cyber physical systems on low-cost platforms. Equally important is the CORES contributions to the education of the next generation of researchers and creators of future infrastructure for realtime computational systems for problems involving engineering optimization. Foremost, CORES will provide undergraduate and graduate level educational opportunities with a multidisciplinary breadth spanning areas as diverse as optimization theory, parallel algorithms for numerical optimization, embedded computer systems, and heterogeneous computing architectures.  Interactions with the control engineering and auto industries in the State of Michigan confirms the need for the development of expertise in this area for present and future engineering research and development. The results from CORES research will have an impact in the fields of engineering optimization and computing infrastructure for cyber physical systems.<br/><br/><br/><br/>The current algorithms for realtime convex optimization can only solve the problem with about a hundred unknowns in the Karush Kuhn Tucker (KKT) convex optimization matrices. This is because the realtime solution enforces a strict time limit on the linear solver (e.g., in microseconds) and  the current algorithms are not designed to fully utilize the limited compute power of the embedded system (e.g., a few CPU cores, plus a GPU). The CORES project will analyze the structure of complex multi-dimensional convex optimization algorithms and replaces the existing sequential implementations, which are the current performance bottleneck, with implementations of new tracking algorithms. Efficient implementations of the algorithms that can effectively leverage the compute power of the scalable heterogeneous system  architecture (SHSA) of the embedded system will be developed. The goal is to speed up the solution process and scale up the size of the optimization problems by orders of magnitude for realtime embedded applications such as control of complex cyber-physical systems (CPS). Specifically, CORES will focus on: (1) Development of high performance linear solvers that exploit the structures of the KKT matrices and leverage the compute power of SHSA and (2) Development of automatic code generation and analysis tools that analyze the structure of the convex optimization problem from a high level modeling language like MATLAB or PYTHON, perform a mapping to a decomposed parallel algorithm, and generate a hybridized multicore CPU and GPU code in OpenCL/CUDA format. Tools that CORES aims to develop come with hierarchical parallel-feature extraction, targeted for various computing elements of SHSA e.g. CPUs and GPU) in a way that eliminates the inefficiencies of inter-processors data sharing. Emerging SHSA combines general-purpose low-latency CPU cores with programmable high-bandwidth vector processing engines on a single platform, connected through a high speed data transfer engines that could still become the performance bottleneck. This feature creates unique opportunities for CORES, and others, to develop sophisticated and specialized computational algorithms and tools for engineering applications such as machine learning and autonomous vehicles  that can exploit such architectures for significantly enhancing performance and scaling up the problem size, while reducing the cost.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1709069","Collaborative Research: ACI-CDS&E: Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES)","OAC","CDS&E-MSS, CDS&E","09/01/2017","08/24/2017","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Tevfik Kosar","08/31/2021","$412,083.00","","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8069, 8084","026Z, 7433, 8084, 9263","$0.00","Embedded processors are ubiquitous, from toasters and microwave ovens, to automobiles, planes, drones and robots and are typically very small processors that are compute and memory constrained. Real-time embedded systems have the additional requirement of completing tasks within a certain time period to accurately and safely control appliances and devices like automobiles, planes, robots, etc. Convex optimization has emerged as an important mathematical tool for automatic control and robotics and other areas of science and engineering disciplines including machine learning and statistical information processing.  In many fields, convex optimization is used by the human designers as optimization tool where it is nearly always constrained to problems solved in a few hours, minutes or seconds. Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES) project takes advantage of the recent advances in embedded hardware and optimization techniques to explore opportunities for real-time convex optimization on the low-cost embedded systems in these disciplines in milli- and micro-seconds. The development of novel algorithms and their high-performance implementations for the real-time solution of practical engineering and scientific optimization problems on the embedded system will open new opportunities in the area of emerging computational science and engineering for cyber physical systems on low-cost platforms. Equally important is the CORES contributions to the education of the next generation of researchers and creators of future infrastructure for realtime computational systems for problems involving engineering optimization. Foremost, CORES will provide undergraduate and graduate level educational opportunities with a multidisciplinary breadth spanning areas as diverse as optimization theory, parallel algorithms for numerical optimization, embedded computer systems, and heterogeneous computing architectures.  Interactions with the control engineering and auto industries in the State of Michigan confirms the need for the development of expertise in this area for present and future engineering research and development. The results from CORES research will have an impact in the fields of engineering optimization and computing infrastructure for cyber physical systems.<br/><br/><br/><br/>The current algorithms for realtime convex optimization can only solve the problem with about a hundred unknowns in the Karush Kuhn Tucker (KKT) convex optimization matrices. This is because the realtime solution enforces a strict time limit on the linear solver (e.g., in microseconds) and  the current algorithms are not designed to fully utilize the limited compute power of the embedded system (e.g., a few CPU cores, plus a GPU). The CORES project will analyze the structure of complex multi-dimensional convex optimization algorithms and replaces the existing sequential implementations, which are the current performance bottleneck, with implementations of new tracking algorithms. Efficient implementations of the algorithms that can effectively leverage the compute power of the scalable heterogeneous system  architecture (SHSA) of the embedded system will be developed. The goal is to speed up the solution process and scale up the size of the optimization problems by orders of magnitude for realtime embedded applications such as control of complex cyber-physical systems (CPS). Specifically, CORES will focus on: (1) Development of high performance linear solvers that exploit the structures of the KKT matrices and leverage the compute power of SHSA and (2) Development of automatic code generation and analysis tools that analyze the structure of the convex optimization problem from a high level modeling language like MATLAB or PYTHON, perform a mapping to a decomposed parallel algorithm, and generate a hybridized multicore CPU and GPU code in OpenCL/CUDA format. Tools that CORES aims to develop come with hierarchical parallel-feature extraction, targeted for various computing elements of SHSA e.g. CPUs and GPU) in a way that eliminates the inefficiencies of inter-processors data sharing. Emerging SHSA combines general-purpose low-latency CPU cores with programmable high-bandwidth vector processing engines on a single platform, connected through a high speed data transfer engines that could still become the performance bottleneck. This feature creates unique opportunities for CORES, and others, to develop sophisticated and specialized computational algorithms and tools for engineering applications such as machine learning and autonomous vehicles  that can exploit such architectures for significantly enhancing performance and scaling up the problem size, while reducing the cost.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1639749","EarthCube Data Infrastructure:  Collaborative Proposal:  A unified experimental-natural digital data system for analysis of rock microstructures","ICER","EAR-Earth Sciences Research, EarthCube","09/01/2017","08/18/2017","Julie Newman","TX","Texas A&M University","Standard Grant","Eva Zanzerkia","08/31/2021","$162,911.00","","Newman@geo.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","GEO","6898, 8074","026Z, 7433","$0.00","When viewed at the micro-scale, rocks reveal structures that help to interpret the processes and forces responsible for their formation.  These microstructures help to explain phenomena that occur at the scale of mountains and tectonic plates.  Interpretation of microstructures formed in nature during deformation is aided by comparison with those formed during experiments, under known conditions of pressure, temperature, stress, strain and strain rate, and experimental rock deformation benefits from the ground truth offered through comparison with rocks deformed in nature.  However, the ability to search for relevant naturally or experimentally deformed microstructures is hindered by the lack of any database that contains these data.  The researchers collaborating on this project will develop a single digital data system for rock microstructures to facilitate the critical interaction between and among the communities that study naturally and experimentally deformed rocks.  To aid in the comparison of microstructures formed in nature and experiment, the researchers will link to commonly used analytical tools and develop a pilot project for automatic comparison of microstructures using machine learning.   <br/><br/>Rock microstructures relate processes at the microscopic scale to phenomena at the outcrop, orogen, and plate scales and reveal the relationships among stress, strain, and strain rate.  Quantitative rheological information is obtained through linked studies of naturally formed microstructures with those created during rock deformation experiments under known conditions.  The project will develop a single digital data system for both naturally and experimentally deformed rock microstructure data to facilitate comparison of microstructures from different environments.  A linked data system will facilitate interaction between practitioners of experimental deformation, those studying natural deformation and the cyberscience community.  The data system will leverage the StraboSpot data system currently under development in Structural Geology and Tectonics.  To develop this system requires: 1) Modification of the StraboSpot data system to accept microstructural data from both naturally and experimentally deformed rocks; and 2) Linking the microstructural data to its geologic context ? either in nature, or its experimental data/parameters.  The researchers will engage the rock deformation community with the goal of establishing data standards and protocols for data collection, and integrate our work with ongoing efforts to establish protocols and techniques for automated metadata collection and digital data storage.  To analyze the microstructures studied and/or generated by these communities, we will ensure StraboSpot data output is compatible with commonly used microstructural tools.  They will develop a pilot project for comparing and analyzing microstructures from different environments using machine-learning."
"1636795","BD Spokes: SPOKE: NORTHEAST: Collaborative Research: Integration of Environmental Factors and Causal Reasoning Approaches for Large-Scale Observational Health Research","IIS","BD Spokes -Big Data Regional I","01/01/2017","08/31/2016","Vasant Honavar","PA","Pennsylvania State Univ University Park","Standard Grant","Sylvia Spengler","12/31/2020","$95,367.00","","vhonavar@ist.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","024Y","028Z, 7433, 8083","$0.00","Vast quantities of health, environmental, and behavioral data are being generated today, yet they remain locked in digital silos. For example, data from health care providers, such as hospitals, provide a dynamic view of health of individuals and populations from birth to death. At the same time, government institutions and industry have released troves of economic, environmental, and behavioral datasets, such as indicators of income/poverty, adverse exposure (e.g., air pollution), and ecological factors (e.g., climate) to the public domain. How are economic, environmental, and behavioral factors linked with health? This project will put together numerous sources of large environmental and clinical data streams to enable the scientific community to address this question. By breaking current data silos, the broader scientific impacts will be wide. First, this effort will foster new routes of biomedical investigation for the big data community. Second, the project will enable discoveries that will have behavioral, economic, environmental, and public health relevance.<br/><br/>This project will aim to assemble a first-ever data warehouse containing numerous health/clinical, environmental, behavioral, and economic data streams to ultimately enable causal discovery between these data sources. First, the team will integrate numerous health data streams by leveraging the Observational Health Data Sciences and Informatics (OHDSI, www.ohdsi.org) network, a virtual data repository that contains millions of longitudinal patient measurements, such as drugs and disease diagnoses. Second, the team will build a centralized data warehouse that contains important environmental, behavioral, and economic data across the United States, such as the Environmental Protection Agency air pollution AirData, the United States Census data on income and occupation statistics, and the National Oceanic Administration Association for climate and weather-related information. Third, the team will disseminate emerging computational methods for causal inference and machine learning to enable researchers to find causal links between environmental, economic, behavioral, and clinical factors. The team will leverage our broad collaborative network consisting of academic big data researchers, federal-level institutes (e.g., EPA, NOAA), and hospitals (e.g., Partners HealthCare) to integrate these data and to disseminate cutting edge machine learning tools. Lastly, the project will create training resources (e.g., interactive how-to guides), coordinate cross-institution student internships, and lead a hands-on workshop to demonstrate use of the integrated data warehouse. The ultimate goal of the project is to facilitate community-led and collaborative causal discovery through dissemination of integrated and open big data and analytics tools."
"1636870","BD Spokes: SPOKE: NORTHEAST: Collaborative Research: Integration of Environmental Factors and Causal Reasoning Approaches for Large-Scale Observational Health Research","OAC","BD Spokes -Big Data Regional I","01/01/2017","08/31/2016","Chirag Patel","MA","Harvard University","Standard Grant","Sylvia Spengler","12/31/2019","$393,871.00","","chirag_patel@hms.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","024Y","028Z, 7433, 8083","$0.00","Vast quantities of health, environmental, and behavioral data are being generated today, yet they remain locked in digital silos. For example, data from health care providers, such as hospitals, provide a dynamic view of health of individuals and populations from birth to death. At the same time, government institutions and industry have released troves of economic, environmental, and behavioral datasets, such as indicators of income/poverty, adverse exposure (e.g., air pollution), and ecological factors (e.g., climate) to the public domain. How are economic, environmental, and behavioral factors linked with health? This project will put together numerous sources of large environmental and clinical data streams to enable the scientific community to address this question. By breaking current data silos, the broader scientific impacts will be wide. First, this effort will foster new routes of biomedical investigation for the big data community. Second, the project will enable discoveries that will have behavioral, economic, environmental, and public health relevance.<br/><br/>This project will aim to assemble a first-ever data warehouse containing numerous health/clinical, environmental, behavioral, and economic data streams to ultimately enable causal discovery between these data sources. First, the team will integrate numerous health data streams by leveraging the Observational Health Data Sciences and Informatics (OHDSI, www.ohdsi.org) network, a virtual data repository that contains millions of longitudinal patient measurements, such as drugs and disease diagnoses. Second, the team will build a centralized data warehouse that contains important environmental, behavioral, and economic data across the United States, such as the Environmental Protection Agency air pollution AirData, the United States Census data on income and occupation statistics, and the National Oceanic Administration Association for climate and weather-related information. Third, the team will disseminate emerging computational methods for causal inference and machine learning to enable researchers to find causal links between environmental, economic, behavioral, and clinical factors. The team will leverage our broad collaborative network consisting of academic big data researchers, federal-level institutes (e.g., EPA, NOAA), and hospitals (e.g., Partners HealthCare) to integrate these data and to disseminate cutting edge machine learning tools. Lastly, the project will create training resources (e.g., interactive how-to guides), coordinate cross-institution student internships, and lead a hands-on workshop to demonstrate use of the integrated data warehouse. The ultimate goal of the project is to facilitate community-led and collaborative causal discovery through dissemination of integrated and open big data and analytics tools."
"1725573","MRI: Acquisition of a Cyberinstrument for Interdisciplinary Computational Science and Engineering","CNS","Major Research Instrumentation","10/01/2017","09/15/2017","Amy Apon","SC","Clemson University","Standard Grant","Rita Rodriguez","09/30/2020","$994,161.00","Jill Gemmill, Dvora Perahia, Mashrur Chowdhury, Kuang-Ching Wang","aapon@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","1189","1189, 9150","$0.00","This project, acquiring, deploying, and maintaining a high performance computing (HPC) cluster, aims to support a broad range of data-enabled science and research training activities. Motivated by both fundamental and applied science agendas, the instrument will be utilized by at least the fourteen collaborative projects cited below:<br/>- Quality and Scalability of Topic Models;<br/>- Multigrid and Multiscale Optimization for Machine Learning and Networks;<br/>- Computational Studies of Multifunctional Polymers;<br/>- Gene Network Alignment at High-scale;<br/>- Modeling of Peptide-functionalized Polyethylene-glycol (PEG)-based Hydrogels;<br/>- Computer Simulations of Highly Thermostable Copolymer-enzyme Conjugates;<br/>- Enabling Modeling of Large Macromolecular Assemblages;<br/>- Computational Studies of Biomolecular Evolution;<br/>- Lattice Boltzmann Simulation of Vascular Blood Flow with Stents and Flow Diverters;<br/>- Automated Parallel Parameterization Methods;<br/>- Large-scale Simulations of Rare Events in Biological Systems;<br/>- Performance Optimization and Load Balancing for Large-scale Metal Cation Catalysts; <br/>- Fundamental Insights into Alkane Selective Oxidation in Atomic-scale Metal Cation Catalysts, and<br/>- Computation Determination of Chemical Stability of Nuclear Materials.<br/>The instrument is the primary resource for the projects in computer science, including scalable machine learning and methods for utilization of emerging memory and Graphics Processing Unit (GPU) technologies. Moreover, the cluster constitutes a much needed platform for collaborative development and execution of GPU-accelerated scientific applications. The solid-state disks (SSDs) and memory paths available through Omni-path enable high scalability of research applications that manage complex operations, Transforming the way scientific knowledge is attained and implemented to drive new technologies, new developments in computing, coupled with traditional computational methods, advance efforts in chemistry, physics, biology, and materials science. <br/><br/>Broader Impacts:<br/>The instrument serves as an essential tool for advancing inquiry for at least 20 faculty in the collaborative team from Clemson and Claflin University, an Historically Black University partner. Three hundred fifty or more graduate and undergraduate students participate in the research proposed and also in the educational projects that will use the instrument. Furthermore, configured as part of the Clemson ""Palmetto"" supercomputer cluster, the system is accessible to other faculty, staff, postdocs, and students and provides benefit to world-class operational user support and offers a path for sustainability for the instrument. The team includes nine women researchers and faculty participants who serve as role models. The participation of women, as well as in graduate programs at this institution, exceeds the national average. Outreach, education, and training activities will reach dozens of undergraduate participants at Claflin University. Current and proposed courses and seminars are planned at all levels."
"1660158","SBIR Phase II:  Big Data Analytics for Facility Operations and Management","IIP","SBIR Phase II","05/15/2017","07/24/2018","Xuesong Liu","PA","LeanFM Technologies, Inc.","Standard Grant","Peter Atherton","04/30/2020","$888,273.00","","udi@leanfmtech.com","100 S Commons","Pittsburgh","PA","152120000","4129532517","ENG","5373","169E, 5373, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project results from improving the efficiency in facilities management (FM) of institutional and commercial buildings by enabling a streamlined transition to efficient, proactive operations using the power of big data analytics. This provides an opportunity to reduce estimated $78.5 Billion - $127.3 Billion in waste due to reactive maintenance per year in the US commercial facilities market alone.  A data driven, proactive approach provides a unique opportunity that enable facilities managers to assess as-is conditions of assets, avoid non value-add activities and plan maintenance tasks to avoid failures and shutdown. This will contribute towards transforming a traditional industry to an advanced data-driven one.  It will also enable significant reduction in the disruptions caused to occupants due to failures in facilities.  Given that Americans spend 85-90% of their time indoors and any disruptions caused by facilities directly impact their qualities of lives, the broader societal impact of reducing failures in facilities is significant.  <br/><br/>This Small Business Innovation Research (SBIR) Phase II project intends to research, develop and demonstrate the feasibility of using big data analytics and machine learning to transform facilities operations and maintenance decisions. Owners and operators of the over five million commercial and institutional buildings in the United States are faced with the challenges of managing aging and crowded building infrastructure. They waste between 30% and 40% of resources by operating in a predominantly inefficient, reactive mode. This project targets development of computational mechanisms that automatically analyze integrated building information to identify patterns that lead to actionable insights that help reduce non value-add activities and improve resource utilization in FM daily operation and planning. By combining advanced machine learning technologies with existing building information modeling (BIM) resources, the company is proposing to develop high-impact, statistical and visual methods for optimizing the decision-making abilities of facility managers and with that, the performance of critical facilities infrastructure and maintenance crews.  The results of this research will include algorithms and methods to normalize heterogeneous building data, detect patterns and anomalies, from which actionable insights can be derived with domain knowledge, and generate qualitative and quantitative output appropriate for improved decision making in managing commercial facilities."
"1652431","CAREER: Scalable Record Linkage through the Microclustering Property","SES","Methodology, Measuremt & Stats","05/15/2017","04/29/2020","Rebecca Steorts","NC","Duke University","Continuing Grant","Cheryl Eavey","04/30/2022","$410,982.00","","beka@stat.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","SBE","1333","1045","$0.00","Duplicative information across multiple databases is a common problem, whether one is trying to accurately estimate the number of patients who have died from sepsis in the United States, the number of people who live in a congressional district, or the number of individuals who have died in armed conflicts. Before such questions can be answered accurately, duplicated information from databases must be removed in a systematic and accurate way. In the research literature, this process is commonly known as record linkage, de-duplication, or entity resolution. This CAREER award will develop general methods and scalable algorithms for record linkage so that pressing global issues can be addressed in real time or near real time. The modeling and computational tools to be developed will significantly increase the volume of data that can be analyzed. This project will enable researchers to address a broader range of scientific questions and advance research in multiple domains, including precision medicine, official statistics, and human rights.  To facilitate these advances and encourage further development, all algorithms will be released as open source software. In terms of education, the investigator will expand the Youth in Machine Learning (YiML) program to enable 50 high school students and 50 undergraduate students per year to participate in the bootcamp and skills-building workshops offered. This will enhance the pipeline of students prepared to study machine learning in future years. At an international level, the investigator will teach workshops at the International Society for Bayesian Analysis Meeting, including a YiML workshop for women.<br/><br/>This research project will develop flexible, general Bayesian nonparametric models for record linkage tasks that propagate the amount of linkage error exactly. The project also will develop scalable record linkage algorithms. By drawing on recent advances in clustering, Bayesian nonparametrics, and probablistic dimension-reduction algorithms, this project will advance the state-of-the-art in record linkage. The models and algorithms to be developed will attempt to solve the microclustering problem, which is at the core of this research. In collaboration with domain experts, the investigator will test the new methods using data sets from health care, official statistics, and human rights. The resulting estimates may provide useful information for policy makers in these areas."
"1715794","Mechanisms of Cytoplasmic Streaming","MCB","Cellular Dynamics and Function","08/01/2017","07/25/2017","Andreas Nebenfuehr","TN","University of Tennessee Knoxville","Standard Grant","Charles Cunningham","07/31/2021","$857,914.00","Vasileios Maroulas, Steven Abel","nebenfuehr@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","BIO","1114","7465","$0.00","This project will explain the intricate characteristics of intracellular transport systems by determining the biophysical mechanisms responsible for organelle movements in plant cells through a combination of biological, computational, and statistical approaches. Biological systems are characterized by highly complex traits that result from the interplay of much simpler components. One of the major challenges in modern biology is to define the interactions of these basic components in order to arrive at the properties of the full system. Intracellular transport along cytoskeletal filaments represents such a biological system that is tractable with today's technology. Importantly, this transport plays fundamental functions in establishing cell polarity, in mediating growth, and in responding to the environment or to pathogens. Therefore, better understanding of the mechanisms underlying intracellular movements as provided by this project may impact, for example, agricultural yield or treatment of diseases. This project will also establish a paradigm for the cross-disciplinary training of graduate students at the interface of molecular cell biology, computational biophysics, and statistical machine learning. This training will be extended to undergraduate and high school students who will participate in carefully selected research projects appropriate for their background. Broader dissemination of the research findings will occur via a dedicated website as well as through lectures for the general public.<br/><br/>Cytoplasmic streaming in plant cells is characterized by the rapid movement of organelles along the actin cytoskeleton. While it is known that these movements are driven by class XI myosin motor proteins, the precise mechanism for the propulsive mechanism is still debated. One model posits that myosin motors directly associate with individual organelles and pull them actively along actin filaments. Another model proposes that only a few organelles such as the ER directly bind to motors while all other organelles are propelled indirectly by binding to the actively moving organelle(s). A third model predicts that a small number of actively moving organelles generate a hydrodynamic flow in the cytoplasm that transports other organelles passively along this stream. This project will test these motility models by developing a series of rigorous tools to model, measure, and evaluate intracellular dynamics. First, stochastic models will be developed that translate the concepts of the three motility models into explicit biophysical descriptions for computer simulations. Second, novel analytical tools will be developed that are able to capture and describe the complex behavior of organelles in living cells with high spatiotemporal resolution. Third, a machine learning approach based on Bayesian considerations will be designed that can evaluate motility models based on experimental data. Combined with novel tools for experimental interference with specific organelle movements and identification of the cellular cargo of two representative myosin XI motors, this research will deliver a new level of understanding of intracellular transport along the cytoskeleton that will impact cell biological interpretations for all eukaryotic systems."
"1639738","EarthCube Data Infrastructure:  Collaborative Proposal:   A unified experimental-natural digital data system for analysis of rock microstructures","ICER","EarthCube","09/01/2017","08/18/2017","J. Douglas Walker","KS","University of Kansas Center for Research Inc","Standard Grant","Eva Zanzerkia","08/31/2021","$285,982.00","","jdwalker@ku.edu","2385 IRVING HILL RD","Lawrence","KS","660457568","7858643441","GEO","8074","7433, 9150","$0.00","When viewed at the micro-scale, rocks reveal structures that help to interpret the processes and forces responsible for their formation.  These microstructures help to explain phenomena that occur at the scale of mountains and tectonic plates.  Interpretation of microstructures formed in nature during deformation is aided by comparison with those formed during experiments, under known conditions of pressure, temperature, stress, strain and strain rate, and experimental rock deformation benefits from the ground truth offered through comparison with rocks deformed in nature.  However, the ability to search for relevant naturally or experimentally deformed microstructures is hindered by the lack of any database that contains these data.  The researchers collaborating on this project will develop a single digital data system for rock microstructures to facilitate the critical interaction between and among the communities that study naturally and experimentally deformed rocks.  To aid in the comparison of microstructures formed in nature and experiment, we will link to commonly used analytical tools and develop a pilot project for automatic comparison of microstructures using machine learning.   <br/><br/>Rock microstructures relate processes at the microscopic scale to phenomena at the outcrop, orogen, and plate scales and reveal the relationships among stress, strain, and strain rate.  Quantitative rheological information is obtained through linked studies of naturally formed microstructures with those created during rock deformation experiments under known conditions.  The project will develop a single digital data system for both naturally and experimentally deformed rock microstructure data to facilitate comparison of microstructures from different environments.  A linked data system will facilitate interaction between practitioners of experimental deformation, those studying natural deformation and the cyberscience community.  The data system will leverage the StraboSpot data system currently under development in Structural Geology and Tectonics.  To develop this system requires: 1) Modification of the StraboSpot data system to accept microstructural data from both naturally and experimentally deformed rocks; and 2) Linking the microstructural data to its geologic context ? either in nature, or its experimental data/parameters.  The researchers will engage the rock deformation community with the goal of establishing data standards and protocols for data collection, and integrate our work with ongoing efforts to establish protocols and techniques for automated metadata collection and digital data storage.  To analyze the microstructures studied and/or generated by these communities, we will ensure StraboSpot data output is compatible with commonly used microstructural tools.  They will develop a pilot project for comparing and analyzing microstructures from different environments using machine-learning."
"1716057","Nonsmooth Analysis and Numerical Optimization Techniques beyond Convexity","DMS","APPLIED MATHEMATICS","08/15/2017","08/11/2017","Mau Nguyen","OR","Portland State University","Standard Grant","Pedro Embid","07/31/2020","$119,984.00","","mnn3@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","MPS","1266","","$0.00","Convex analysis and optimization play a crucial role by providing the mathematical foundation and methods for solving problems in a variety of fields.  At the same time, recent applications in these fields require optimization techniques beyond convexity.  Although convex optimization techniques and numerical algorithms have been the topics of extensive research for more than 50 years, solving large-scale optimization problems without the presence of convexity remains a challenge.  In this project, the principal investigator aims to develop new theoretical results in convex and nonsmooth analysis, and new numerical algorithms, for the optimization of nonconvex functions that are not necessarily differentiable, especially functions that are the difference of convex functions.  Optimization problems of this sort arise in multi-facility location, clustering, machine learning, compressed sensing, and imaging applications.  The investigator and his colleagues develop, implement, and test numerical algorithms for solving such problems.  With no requirement on differentiability and convexity, these numerical algorithms bring new methods for solving complex optimization problems in different fields of application.<br/><br/>This project aims to develop new theory of nonsmooth analysis and optimization methods for solving optimization problems without imposing conditions of differentiability or convexity.  Based on a variational geometric approach, the first goal of this project is to develop new results in nonsmooth analysis to deal with optimization problems in which the objective functions are nondifferentiable and nonconvex.  This approach provides a systematic development of nonsmooth analysis, making it accessible to researchers from different fields.  The second goal of the project is to develop numerical algorithms for solving nonconvex optimization problems, especially those whose objective functions are representable as differences of convex functions, and to apply them to problems in multi-facility location, clustering and hierarchical clustering, machine learning, compressed sensing, and imaging.  The investigator and his colleagues particularly focus on problems that involve different norms or constraints, requiring advances in smoothing and initialization techniques.  They address the important issues of existence and uniqueness of optimal solutions of the models, initialization techniques based on global optimization methods, implementation of the algorithms for comparison and testing on artificial and real data sets, and the convergence rate of the algorithms.  The results contribute to the development of nonsmooth analysis and its use in building and analyzing numerical algorithms for nonsmooth optimization problems that are not convex."
"1639748","EarthCube Data Infrastructure:   Collaborative Proposal:   A unified experimental-natural digital data system for analysis of rock microstructures","ICER","EarthCube","09/01/2017","08/18/2017","Basil Tikoff","WI","University of Wisconsin-Madison","Standard Grant","Eva Zanzerkia","08/31/2020","$98,465.00","","basil@geology.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","GEO","8074","7433","$0.00","When viewed at the micro-scale, rocks reveal structures that help to interpret the processes and forces responsible for their formation.  These microstructures help to explain phenomena that occur at the scale of mountains and tectonic plates.  Interpretation of microstructures formed in nature during deformation is aided by comparison with those formed during experiments, under known conditions of pressure, temperature, stress, strain and strain rate, and experimental rock deformation benefits from the ground truth offered through comparison with rocks deformed in nature.  However, the ability to search for relevant naturally or experimentally deformed microstructures is hindered by the lack of any database that contains these data.  The researchers collaborating on this project will develop a single digital data system for rock microstructures to facilitate the critical interaction between and among the communities that study naturally and experimentally deformed rocks.  To aid in the comparison of microstructures formed in nature and experiment, we will link to commonly used analytical tools and develop a pilot project for automatic comparison of microstructures using machine learning.   <br/><br/>Rock microstructures relate processes at the microscopic scale to phenomena at the outcrop, orogen, and plate scales and reveal the relationships among stress, strain, and strain rate.  Quantitative rheological information is obtained through linked studies of naturally formed microstructures with those created during rock deformation experiments under known conditions.  The project will develop a single digital data system for both naturally and experimentally deformed rock microstructure data to facilitate comparison of microstructures from different environments.  A linked data system will facilitate interaction between practitioners of experimental deformation, those studying natural deformation and the cyberscience community.  The data system will leverage the StraboSpot data system currently under development in Structural Geology and Tectonics.  To develop this system requires: 1) Modification of the StraboSpot data system to accept microstructural data from both naturally and experimentally deformed rocks; and 2) Linking the microstructural data to its geologic context ? either in nature, or its experimental data/parameters.  The researchers will engage the rock deformation community with the goal of establishing data standards and protocols for data collection, and integrate our work with ongoing efforts to establish protocols and techniques for automated metadata collection and digital data storage.  To analyze the microstructures studied and/or generated by these communities, we will ensure StraboSpot data output is compatible with commonly used microstructural tools.  They will develop a pilot project for comparing and analyzing microstructures from different environments using machine-learning."
"1653322","SCH: CAREER:  Co-Robotic Ultrasound Sensing in Bioengineering","IIS","NRI-National Robotics Initiati, Smart and Connected Health","09/01/2017","07/22/2017","Emad Boctor Mikhail","MD","Johns Hopkins University","Standard Grant","Wendy Nilsen","08/31/2022","$419,902.00","","eboctor1@jhmi.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8013, 8018","1045, 8018, 8086","$0.00","Ultrasound imaging, while frequently used in heathcare, remains challenged by three important issues. First, a very significant percentage of ultrasonographers (63-91%) develop musculoskeletal disorders due to effort required to perform imaging tasks. Second, ultrasound imaging is limited by loss of resolution at increasing depths (e.g., in imaging of obese patients), significantly limiting imaging value with conventional ultrasound imaging. Finally, ultrasound imaging is most commonly qualitative in nature, and quantitative imaging (e.g., measurement of the speed of the ultrasounds signal) has been limited. There is significant gap and need for more accurate imaging of various organs and diseases. All these issues hinder unleashing the potential benefit of ultrasound technology serving a wider sector of patients in hospitals and most importantly outside hospitals, including point-of-cares and homes. All these seemingly distinct issues can be tackled and addressed via a co-robotic and multi-wave ultrasound framework. The objective of this proposal is to characterize fundamental principles at the intersection of robotics, ultrasound physics, signal processing, machine learning and bioengineering, which will enable a new generation of advanced ultrasound imaging technologies capable of providing cost-effective precise interventional guidance and high-quality quantitative diagnostic imaging to a wider sector of people at hospitals, points-of-care, and homes. Additionally, this proposal emphasizes the following educational objectives: (1) create hands-on robotic imaging undergraduate/graduate training curriculum relying on the MUSiiC toolkit; (2) bring research results to local schools including Centennial High School and involve their students in research; and (3) deploy low-cost wireless ultrasound systems and light-weight and human-safe robots in high schools and university classrooms. <br/><br/><br/>The research objective of this proposal is to characterize fundamental principles at the intersection of robotics, ultrasound physics, signal processing, machine learning and bioengineering, which will enable a new generation of advanced ultrasound imaging technologies capable of providing cost-effective precise interventional guidance and high-quality quantitative diagnostic imaging to a wider sector of people at hospitals, points-of-care, and homes. To achieve this objective, this proposal includes an integrated research-education plan consisting of three complementary and interconnected research thrusts. Thrust 1: Novel Multi-wave Ultrasonic and Robotic Imaging Devices focuses on novel multi-wave ultrasound imaging architectures and physics-based simulations that describe their performance under variable calibration and robot tracking accuracies, and beam-width and geometry limitations of ultrasound sensors.  Specifically, the project proposes ultrasonically smart tools, co-robotic multi-wave ultrasound systems, and active calibration platform and validation. Thrust 2: Robust Sensing and Co-Robotic Imaging focuses on using models, architectures and devices from Thrust 1 to endow surgical and interventional guidance with robust sensing and to devise and enable new imaging algorithms with both robust sensing and co-robotic intelligence. Specifically, the project uses a novel thermal imaging algorithm leveraging the unique multi-wave ultrasound architecture described in Thrust 1. Additionally, we explore co-robotic imaging to substantially enhance ultrasound resolution utilizing synthetic aperture reconstruction guided by the robot's precise and accurate motion trajectory.  Thrust 3: Education focuses on integrating research results into education at the high school, undergraduate, and graduate levels, while emphasizing participation by an underrepresented individuals (African American and women) in Biomedical Sciences and Engineering. The proposal will also bring research results to local schools including Centennial High School and involve their students in research. This can easily achieve this by relying on the team's Medical UltraSound Imaging and Intervention Collaboratory (MUSiiC) software toolkit and enabling smart phone and tablets to control ultrasound systems. The plan also includes deployment of a number of low-cost wireless ultrasound systems, along with light-weight and human-safe robots, to high school and university classrooms. The results from all three thrusts will be applied to systems for three clinical testbed setups, including cancer intervention, catheterization, and diagnostic imaging."
"1663138","PREEVENTS Track 2: Collaborative Research: Ocean Salinity as a predictor of US hydroclimate extremes","ICER","PREEVENTS - Prediction of and","08/01/2017","08/02/2018","Laifang Li","NC","Duke University","Continuing Grant","Justin Lawrence","07/31/2021","$260,036.00","","laifang.li@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","GEO","034Y","","$0.00","Water availability is a fundamental necessity for society. As the largest moisture reservoir and ultimate moisture source, water from the oceans sustains terrestrial precipitation and is thus key to understanding variability in the water cycle on land. Floods and droughts represent extremes of the water cycle that have enormous consequences for society. In recent years Western drought has led to billions of dollars of agricultural losses and extensive wildfires, while floods produced similar losses in the South, Midwest and East of the US. They are caused by an excess or deficit of moisture exported from ocean to land.  Moisture evaporating from the ocean surface is the ultimate source for terrestrial precipitation. Thus, the availability of the oceanic moisture supply modulates the severity of hydroclimate extremes on land. As moisture exits the ocean, it leaves a signature in sea surface salinity. Recent studies have provided remarkable new evidence that salinity can be utilized as a skillful predictor of precipitation in the US Midwest, Southwest and other regions. The salinity precursors significantly outperform temperature-based predictors, especially in the years with heavy precipitation or exceptional drought.  Thus, sea surface salinity has great potential to provide a transformative improvement to seasonal forecasts of US hydroclimate extremes. This project will develop the scientific basis for a drought and flood early warning system for the US based on these new insights into the predictive potential of ocean salinity and the expanding salinity monitoring system that uses both in-situ measurements and satellites. This will lead to a number of societal benefits: lives saved and property preserved from wildfires and floods; improved crop yields resulting from more accurate seasonal rainfall forecasts; national security advances realized by better anticipation of destabilized regions affected by drought or flood crises; and more accurate forecasting of energy demand and the impact of water shortages on power plants. Several undergraduate students will have the opportunity to gain valuable research experience, and thus the project will help to train the next generation of climate scientists. Project findings will also be incorporated into graduate courses taught through the MIT/WHOI joint program and at Duke University, and the knowledge will be disseminated to the general public. <br/><br/>The processes that produce the newly identified relationships between extreme precipitation and sea surface salinity will be explored. Daily precipitation data and a Bayesian statistical framework will be used to sample the extreme events in the US. Based on the Bayesian inference, the pre-season salinity precursors will be explored and mechanisms by which the water cycle generates the salinity signatures determined by calculating atmospheric moisture fluxes and the terms in the surface salinity budget. In addition, the oceanic moisture flux onto land will be tracked, and the processes assessed by which extremes develop through the moisture supply and/or energy redistribution in the atmospheric column. Machine-learning algorithms to predict extremes using the sea surface salinity precursors will be developed and applied. Novel approaches will be used in this project, including the use of Bayesian statistics to identify the optimal sea surface salinity and temperature predictors for rainfall extremes, analysis of the oceanic salinity budget to identify the driving atmospheric variables, analysis of the atmospheric circulations that transport water from ocean to land, and the development of machine learning algorithms to provide optimal seasonal predictions of extreme drought or floods."
"1636832","BD Spokes: SPOKE: NORTHEAST: Collaborative Research: Integration of Environmental Factors and Causal Reasoning Approaches for Large-Scale Observational Health Research","IIS","BD Spokes -Big Data Regional I","01/01/2017","08/31/2016","Noemie Elhadad","NY","Columbia University","Standard Grant","Sylvia Spengler","12/31/2020","$372,880.00","","noemie.elhadad@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","024Y","028Z, 7433, 8083","$0.00","Vast quantities of health, environmental, and behavioral data are being generated today, yet they remain locked in digital silos. For example, data from health care providers, such as hospitals, provide a dynamic view of health of individuals and populations from birth to death. At the same time, government institutions and industry have released troves of economic, environmental, and behavioral datasets, such as indicators of income/poverty, adverse exposure (e.g., air pollution), and ecological factors (e.g., climate) to the public domain. How are economic, environmental, and behavioral factors linked with health? This project will put together numerous sources of large environmental and clinical data streams to enable the scientific community to address this question. By breaking current data silos, the broader scientific impacts will be wide. First, this effort will foster new routes of biomedical investigation for the big data community. Second, the project will enable discoveries that will have behavioral, economic, environmental, and public health relevance.<br/><br/>This project will aim to assemble a first-ever data warehouse containing numerous health/clinical, environmental, behavioral, and economic data streams to ultimately enable causal discovery between these data sources. First, the team will integrate numerous health data streams by leveraging the Observational Health Data Sciences and Informatics (OHDSI, www.ohdsi.org) network, a virtual data repository that contains millions of longitudinal patient measurements, such as drugs and disease diagnoses. Second, the team will build a centralized data warehouse that contains important environmental, behavioral, and economic data across the United States, such as the Environmental Protection Agency air pollution AirData, the United States Census data on income and occupation statistics, and the National Oceanic Administration Association for climate and weather-related information. Third, the team will disseminate emerging computational methods for causal inference and machine learning to enable researchers to find causal links between environmental, economic, behavioral, and clinical factors. The team will leverage our broad collaborative network consisting of academic big data researchers, federal-level institutes (e.g., EPA, NOAA), and hospitals (e.g., Partners HealthCare) to integrate these data and to disseminate cutting edge machine learning tools. Lastly, the project will create training resources (e.g., interactive how-to guides), coordinate cross-institution student internships, and lead a hands-on workshop to demonstrate use of the integrated data warehouse. The ultimate goal of the project is to facilitate community-led and collaborative causal discovery through dissemination of integrated and open big data and analytics tools."
"1748764","EAGER: A Python Program Analysis Infrastructure to Facilitate Better Data Processing","CCF","CI REUSE, Software & Hardware Foundation","09/15/2017","09/11/2017","Xiangyu Zhang","IN","Purdue University","Standard Grant","Sol Greenspan","08/31/2019","$147,000.00","","xyzhang@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","6892, 7798","7433, 7916, 7944, 8004","$0.00","Python is the third most popular programming language, after C and Java,  and the most widely used language in Machine Learning and Data Science. Applications in Python are prone to human errors as much as those in other languages, or maybe more so due to the dynamic nature of Python. Therefore, tools to analyze, test, verify, and optimize Python applications are in a pressing need. Such tools are lagging or non-existent for Python. The root cause is the lack of infrastructure to support building practical and effective tools, which entails addressing the dynamic features of Python, such as dynamic typing, dynamic code loading/execution, and pervasive invocations to external library functions implemented in other languages.<br/> <br/>This project aims to explore the feasibility of building a Python program analysis infrastructure by developing two sample tools that rely upon a common set of infrastructural capabilities including the instrumentation, static analysis and symbolic analysis capabilities. The two sample tools are a data provenance tracking tool for machine learning applications and a bug finding tool to detect data format inconsistencies, which are the most dominant type of bugs in data processing. The provenance tool will demonstrate the importance of static analysis and program instrumentation, and the bug finding tool will demonstrate the importance of symbolic analysis. Both tools will illustrate the great benefits that can be brought to data scientists by advanced tools. In addition, they will illustrate that the aforementioned capabilities cannot be simply ported from existing infrastructures for other languages such as C and Java. The infrastructure will meet the pressing need of comprehensive tool building support for Python. A lot of cutting-edge synergistic research will be enabled across the CISE research community to serve data application programmers, data scientists and even end users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713003","High-dimensional Clustering: Theory and Methods","DMS","STATISTICS","07/01/2017","08/14/2019","Sivaraman Balakrishnan","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","06/30/2021","$380,000.00","Alessandro Rinaldo, Larry Wasserman","sbalakri@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","The past two decades have witnessed an explosion in the scale and complexity of data sets that arise in science and engineering. Broadly, clustering methods which discover latent structure in data are our primary tool for navigating, exploring and visualizing massive datasets. These methods have been widely and successfully applied in phylogeny, medicine, psychiatry, archaeology and anthropology, phytosociology, economics and several other fields. Despite its ubiquity, the widespread scientific adoption of clustering methods have been hindered by the lack of flexible clustering methods for high-dimensional datasets and by the dearth of meaningful inferential guarantees in clustering problems. Accordingly, the goal of this research is to develop new and effective methods for clustering complex data-sets, and to further develop an inferential grounding -- which will in turn lead to actionable conclusions -- for these methods. This research will lead to the development of new clustering methods, as well as to a deeper understanding of the fundamental limitations of methods aimed at uncovering latent structure in data. <br/><br/>The research component of this project consists of four aims designed to address related aspects of this high-level goal: (a) analyze and develop new clustering methods for high-dimensional datasets, with a particular focus on practically useful methods like mixture-model based clustering, and minimum volume clustering; (b) develop novel methods for inference in the context of clustering, motivated by scientific applications where it is important not only to cluster the data but also to clearly characterize the sampling variability of the discovered clusters; (c) develop fundamental lower bounds for high-dimensional clustering (d) develop novel methods for clustering functional data with inferential guarantees. These research components are closely coupled with concrete educational initiatives, including the development and broad dissemination of publicly-available software for high-dimensional clustering; tutorials and workshops at Machine Learning conferences and fostering further interactions between the Departments of Statistics and Machine Learning at Carnegie Mellon."
"1700535","Smart Manufacturing and Resources for Transforming the Future","DUE","Advanced Tech Education Prog","07/01/2017","06/06/2017","Shamus Funk","WI","Chippewa Valley Technical College","Standard Grant","John Jackman","06/30/2021","$899,993.00","Jim Kroehn, Greg Slupe, Tim Tewalt","sfunk1@cvtc.edu","620 W Clairemont Avenue","Eau Claire","WI","547016162","7158336419","EHR","7412","1032, 9178, SMET","$0.00","Smart Manufacturing and Resources for Transforming the Future (SMART Future) is promoting STEM exploration and education while strengthening the economies of rural areas through training in new manufacturing technologies. By preparing rural high school students for careers in high-growth fields like manufacturing and information technology, SMART Future is advancing new and innovative methods for producing goods, meeting the nation's changing employment needs, and improving educational and earning opportunities for teachers and youth in rural areas.<br/><br/>Through its use of a Mobile Simulation Laboratory, SMART Future is preparing technicians for industrial automation and technology careers and increasing the capacity of rural secondary teachers to provide education in the context of the emerging Industrial Internet of Things (IIoT) and Industry 4.0. The project goals are to expand STEM opportunities and prepare technicians for manufacturing and engineering careers through applied education of IIoT and Industry 4.0 concepts; and increase the capacity of rural secondary teachers to provide instruction in industrial automation. Drawing on industry expertise, SMART Future is providing dual credit for learning STEM principles in industrial automation, including programmable logic controllers (PLCs), microcontrollers, robotics, automated processes, machine-to-machine learning, computer networking and programming, applied mathematics, engineering design, precision measurement, physics, and mathematical logic."
"1748793","Workshop on Self-Driving Networks","CNS","Networking Technology and Syst","08/01/2017","07/27/2017","Nicholas Feamster","NJ","Princeton University","Standard Grant","Darleen Fisher","10/31/2019","$49,999.00","Jennifer Rexford","feamster@uchicago.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7363","7556","$0.00","This workshop brings together leading researchers from a range of disciplines across computer science to define a new research agenda in network measurement and data analytics with the goal of exploring how to design networks that manage themselves. These experts will explore taking advantage of advances in disciplines including machine learning, distributed systems, and formal methods to address growing requirements and constraints of modern networking applications. <br/><br/>Because of the proliferation of applications and services that now run over the Internet ranging from video streaming to Internet-connected smart home devices to augmented reality---the expectations for the performance, reliability, and security of our communications networks are greater than ever, as the number and diversity of applications that run on top of the network continue to proliferate, and as the volume of traffic on the network continues to grow. To meet these expectations, network operators work tirelessly to continuously collect troves of heterogeneous data from the network, analyze this data to infer characteristics about the network, and decide whether to change the network's configuration in response to network conditions (e.g., a shift in traffic demand or a cyber attack). Today, these three steps are decoupled: operators perform them separately, on different timescales, often in a slow or manual fashion that relies on intuition, as opposed to data, analysis, and inference. The vision for this workshop is that networks might one day be able to largely manage themselves through a combination of query-driven network measurement, automated inference techniques, and programmatic control. <br/><br/>Intellectual Merit: The research agenda lends itself to research problems that will foster advances in computer science, including the following areas: 1. Distributed systems that optimize the use of limited resources for complex tasks, including support for multiple simultaneous queries; New architectures to support programmable measurement in hardware; Algorithms that partition a network analytics query across a centralized stream processor and the distributed switches and network middleboxes. 2. New measurement techniques (beyond ""ping"" and ""traceroute"") that leverage the capabilities of P4-capable data planes (e.g., in-band telemetry); Software/hardware co-design for better network measurements; Clean-slate, problem-driven designs for new network measurement tools that might tackle problems in network measurement that have proved evasive (e.g., application quality of experience); Measurement of unified compute, storage, and networking infrastructure, including monitoring of container-based systems 3. Machine Learning and new algorithms for automated troubleshooting and ""what-if"" scenario evaluation; Development of parsimonious models that could be implemented (at least partially) at line rate on switch hardware; Prediction and inference over non-stationary datasets to changing traffic patterns. 4. Security and privacy through scalable algorithms and systems for detecting a broad range of attacks, from denial of service to data exfiltration; Better ways to monitor application performance without having to perform man-in-the-middle attacks on traffic. <br/><br/>Broader Impacts: Results from this workshop will be broadly distributed so that researchers in all of the areas noted above will benefit from the discussions, conclusions and recommendations resulting from the workshop.  Research inspired by the workshop could have broad societal impacts by helping network operators envision how to integrate measurement, data analysis, and configuration decisions and move toward automated network control."
"1737803","Collaborative Research: Phylanx: Python based Array Processing in HPX","CCF","","08/15/2017","08/14/2017","Kevin Huck","OR","University of Oregon Eugene","Standard Grant","Almadena Chtchelkanova","07/31/2018","$93,300.00","Allen Malony","khuck@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","Q212","7942","$0.00","The availability and size of data sets has increased significantly over the course of the past decade. To enable the analysis of large data sets on High Performance Computing (HPC) resources while minimizing time- and energy-to-solution requires incorporating static and runtime information to determine the best possible data layout of the large data arrays used by an application to minimize data movement. The goal of this proposal is to deliver Phylanx, a general purpose framework supporting a variety of data science, machine learning, and statistically oriented applications. Phylanx is designed such that a user?s code will be able to perform efficiently on current and future architecture as long as the runtime system is maintained. This greatly reduces the maintenance burden and will increase the productivity of domain scientists. Phylanx lays a solid foundation for technology transfer from academia to industry and fills the gap between academic innovation and commercial application, by creating a software layer that industrial partners can feel confident relying upon. <br/>Phylanx is a scalable, array-based and distributed framework targeting HPC systems using the HPX, dynamic asynchronous task-based parallel runtime system. The dataflow-style capabilities exposed by HPX guarantee the preservation of all data-dependencies even for complex distributed workflows. This project overcomes some of the limitations of existing Big Data solutions such as Hadoop, Spark, and Flink by providing users the ability to: implement NumPy-styled expression-graphs using Python or C/C++, optimize these graphs for optimal data layout, distribution, tiling, and minimal communication overheads, and evaluate those graphs with high efficiency on a runtime interpreter targeting distributed HPC systems. Additionally, Phylanx uses greedy sub-modular techniques on the expression tree to provide a mathematically provable guarantee of optimal performance in machine learning domains and in data placement problems. The platform will provide implementations of 6 benchmarks which have been selected for their domain specificity in text, image, and graph applications."
"1748198","MATDAT18:  Materials and Data Science Hackathon","DMR","BD Spokes -Big Data Regional I, OFFICE OF MULTIDISCIPLINARY AC, CONDENSED MATTER & MAT THEORY, CDS&E-MSS","09/15/2017","09/13/2017","Brian Reich","NC","North Carolina State University","Standard Grant","Daryl Hess","08/31/2018","$148,810.00","Sanguthevar Rajasekaran, Tim Mueller, Andrew Ferguson","brian_reich@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","MPS","024Y, 1253, 1765, 8069","028Z, 054Z, 7433, 7556, 8083, 8084","$0.00","The Division of Materials Research, the Division of Information and Intelligent Systems, and the Division of Mathematical Sciences contribute funds to this award. It supports the organization of a ""data hackathon"" in which interdisciplinary teams, each composed of both materials researchers and data scientists, will work together to apply advanced data science methods to address important, challenging problems in the intrinsically interdisciplinary field of materials research. The goal of this hackathon is to spark new collaborations in which important, challenging problems in materials science are addressed in novel ways by leading methods in data science.<br/><br/>The exponential increase in materials data and in available computing power has made it possible to generate and analyze large amounts of materials data. Initiatives such as the Materials Project have created publicly accessible databases containing the structure and properties of tens of thousands of materials. In addition to these general-purpose resources, individual research groups are generating large data sets for more specific materials research problems. One of the leading challenges in materials science and engineering is determining how to best make use of this abundance of materials data in the process to design and discover new materials with desired properties and accelerate their deployment in existing technologies and innovate new technologies. Despite the considerable progress that has been made in the application of data analytics and machine learning to materials science in recent years, there is still a fundamental problem in that most experts in materials science and engineering are not experts in data science, and data scientists are not experts in materials research. This ""data hackathon"" activity is aimed to bring the materials research community and the data science community together to attack significant problems where a data-centric approach may be able to make progress and to stimulate collaboration among the communities. <br/><br/>This ""data hackathon"" will be fundamentally interdisciplinary by construction. Small groups of researchers representing both data and materials sciences will work face-to-face for several days on substantial materials problems. It is hoped that materials researchers are exposed to cutting-edge statistics and machine-learning techniques, and data scientist are motivated to develop new methods to analyze the novel data streams produced by materials researchers. The participant application process will also emphasize inclusion of junior researchers and under-represented groups."
"1722199","SBIR Phase I:  Large Arrays for 5G MU-MIMO Wireless Access Operating in mm-wave Frequency Bands","IIP","SBIR Phase I","07/01/2017","07/10/2017","Sriramkumar Venugopalan","CA","RF Pixels, Inc.","Standard Grant","Muralidharan Nair","05/31/2018","$224,996.00","","sriram@rf-pixels.com","333 W El Camino Real Ste 250","Sunnyvale","CA","940871307","5103643110","ENG","5371","5371","$0.00","The commercial potential of this project lies in enabling much needed high wireless data rates and equal quality of service (QoS) to both densely populated areas as well as remote rural areas. 4G cellular and WiFi networks are facing a bottleneck due to proliferation of wireless devices and limited spectrum available in the currently allocated bands for wireless communications. Improving data connectivity would require new physical fiber to be laid out which is becoming impractical due to high costs vs. return on investment. The Federal Communications Commission has opened more spectrum in the mm-wave frequency spectrum (between 28-70 GHz) for mobile users to alleviate some of these issues. This project will enable spectrally efficient communication through frequency reuse for spatially separated mobile users in the mm-wave frequencies. It will also allow low cost dense deployment of 5G cellular base-stations to boost QoS. With this new paradigms in human machine interaction using machine learning can be advanced further. Remote educational programs, virtual and augmented reality (VR/AR) based learning, online health care and other new services around Internet of Things will be enhanced by better wireless connectivity. These technologies will uplift people?s lives delivering major economic and broad societal impact.<br/><br/>This Small Business Innovation Research Phase I project proposes a revolutionary architecture that will enable massive multi-user MIMO techniques for mm-wave wireless systems to solve the spectrum scarcity issue. The mm-wave radio front end module (FEM) can reuse the same frequencies for spatially diverse mobile users with active beamforming thus enhancing the capacity of current cellular networks by 100x. A multitude of modules (1000s) will be used in a large array as a part of the mm-wave FEM making it crucial to achieve excellent power efficiency at the module level. This project will investigate a variant of Doherty power amplifier (PA) architecture to achieve highly efficient mm-wave transmitter. Module to module variation and within module variations can disturb accurate beamforming capability. Impact of these variations on forming coherent beams will be studied. A new method to calibrate a large array to tune out any module to module variation will be proposed through sufficient redundancies incorporated in the module design. Low sensitivity to calibration errors will be achieved while also trying to lower the calibration time. A new algorithm that achieves accurate simultaneous tracking of spatial mobile users to within 5-degrees using the new architecture will be proposed."
"1749750","AF: EAGER: Identifying Opportunities in Pseudorandomness","CCF","ALGORITHMIC FOUNDATIONS","09/15/2017","09/11/2017","Salil Vadhan","MA","Harvard University","Standard Grant","Tracy Kimbrel","08/31/2018","$125,000.00","","salil_vadhan@harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7916, 7927","$0.00","Pseudorandomness is the theory of generating objects that ""look random"" despite being constructed using little or no randomness.  The computational theory of pseudorandomness originated in the foundations of cryptography in the early 1980s and has since developed into a rich subfield of theoretical computer science in its own right.  The notions and constructs studied in the theory of pseudorandomness have implications for many different areas of research in computer science, communications, and mathematics.<br/><br/>This EArly-concept Grant for Exploratory Research (EAGER) project seeks to identify novel approaches to some of the most enduring and important challenges in pseudorandomness as well as opportunities for new applications of pseudorandomness.  The research will be closely integrated with the PIs' educational efforts.  In particular, the PIs will continue to develop new curricular, educational, and expository material that are made openly available for others to use.  Graduate and undergraduate students will be involved in all phases of this research, and will be given opportunities to publish and present their results at premier international conferences.  The PIs will also continue their extensive service to the scientific community.<br/><br/>Specifically, the project will try to uncover new approaches to topics such as:<br/><br/>1. The RL vs. L problem: trying to prove, unconditionally, that every randomized algorithm can be made deterministic with only a constant-factor loss in memory usage.<br/><br/>2. Cryptography: identifying optimally efficient constructions of cryptographic primitives from minimal assumptions.<br/><br/>3. Machine Learning: can pseudorandomness help in making machine learning robust to adversarial noise or to overfitting?"
"1739800","CPS: Medium: Integrated control of biological and mechanical power for standing balance and gait stability after paralysis","CNS","IIS Special Projects","09/15/2017","09/12/2017","Roger Quinn","OH","Case Western Reserve University","Standard Grant","Sylvia Spengler","08/31/2021","$999,396.00","Ronald Triolo, Musa Audu","rdq@po.cwru.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","7484","7484, 7918, 7924","$0.00","Wearable exoskeletons are one of the primary advancements that help to alleviate the effects of spinal cord injury (SCI) including degenerative changes in organs of the body. Artificially stimulating the wearer's muscles to move his or her limbs has the additional benefit of maintaining musculature and improving circulation. The exoskeleton system developed in this project will use this ""muscles first"" approach with additional assistive power from electric motors on an as-needed basis. The major contribution of the project is that it will ensure stability of the person during standing and at normal walking speeds. The result will be that persons with SCI will be more comfortable standing and walking more erect and, therefore, be more socially engaged. The societal impact of this will be that persons with SCI will be better able to work and participate in social and leisure activities and in other behaviors associated with independent and productive lifestyles. In addition, Cleveland area high school students will be involved in the project and learn about human biomechanics and engineering methods.<br/><br/>This project addresses how cyber physical walking systems (CPWS) can be designed to be safe, secure, and resilient despite a variety of unanticipated disturbances and how real-time dynamic control and behavior adaptation can be achieved in a diversity of environments. Specifically, a CPWS will be developed that seamlessly integrates: (1) a person who has a spinal cord injury (SCI) with intact and excitable lower motor nerves; (2) an exoskeleton with controllably locked/unlocked and/or passively damped joints; (3) DC motors for need-dependent joint power assistance; and (4) computational algorithms that continuously and automatically learn to improve standing and walking stability. In this ""muscles first"" approach, functional neural stimulation (FNS) provides most of the joint torques for walking and for maximum health benefits and, thus, as-needed assistive joint motors may be small and lightweight. The specific aims are 1) Assist the user's muscles on an as-needed basis and for high-bandwidth stability control by adding small, low passive-resistance motor/transmission pairs to our CPWS; 2) Develop computational algorithms for system estimation, machine learning and stability control for SCI users standing and walking with a CPWS while minimizing upper extremity effort; 3) Verify system performance with able-bodied individuals and assess upper extremity reduction and balance control in individuals with SCI using the CPWS for standing and ambulation."
"1659645","REU Site: Big Data Analytics at Oklahoma State University","IIS","RSCH EXPER FOR UNDERGRAD SITES, EPSCoR Co-Funding","03/15/2017","05/21/2018","Christopher Crick","OK","Oklahoma State University","Standard Grant","Wendy Nilsen","02/29/2020","$356,618.00","K. George","chris.crick@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","1139, 9150","9150, 9250","$0.00","This project will support a three-year Research Experience for Undergraduates (REU) site at Oklahoma State University (OSU). The project is a ten-week summer institute for ten undergraduate students who are rising juniors or seniors in two-year colleges and four-year universities in Oklahoma and neighboring states. During the summer institute, the participants will engage in research in big data analytics; including, data collection, data cleansing, data analysis, data interpretation, and data visualization, under the mentorship and guidance of the Principal Investigator and other faculty mentors. The proposed REU site will impact the training of the 21st century workforce by broadening participation in computing research, exposing minority students to the research process, influencing their career decisions and increasing diversity with-in the field. Contact will be maintained with the participants after the summer institute through a virtual community site; this will encourage community-building within the whole cohort and support the participants' efforts to build their identity as researchers. The participants will also be exposed to research activities in industry through field trips and presentations from external speakers. The targeted institutions for recruitment have a high percentage of underrepresented minority undergraduate students, such as African Americans and Native Americans.<br/><br/>This project will support ten students each year as they engage in research in big data analytics. They will learn how to use machine learning algorithms such as neural networks, visualization techniques, such as isometric feature mapping, and statistical techniques such as regression and Gaussian processes. It is expected that participants will not only gain technical skills such as programming, but also develop broad abilities in abstraction and computational thinking. They will be introduced to all aspects of big data analytics in real-world projects, such as collecting and analyzing social media data. The participants will work as a cohort and will be included as co-authors in publications stemming from their work. The students will work in small groups while interacting with other groups and with graduate students, assisting each other to collect, schematize, structure, visualize, and learn from large-scale data sources. The proposed REU site will contribute towards the training of the 21st century workforce and broaden participation in computing research, especially in big data analytics, which is an important and swiftly-growing area for research."
"1718477","NeTS:Small:Optimal Learning Times for Task-Oriented Communication Networks","CCF","Comm & Information Foundations","09/01/2017","08/30/2017","Michael Neely","CA","University of Southern California","Standard Grant","Phillip Regalia","08/31/2021","$477,478.00","","mikejneely@gmail.com","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7935","$0.00","Communication networks must support a diverse set of tasks while quickly adapting to time-varying conditions. This project seeks to develop and characterize fast and adaptive network control methods. The methods respect the energy, computation, communication, and sensing resources of the network while maintaining high quality execution of each task and providing fair sharing across all users. The goal is to create smarter networks with faster response times and lower delays while adapting to changes in user mobility and device functionality. The research has broad applications to control theory, operations research, smart grid scheduling, economics, and game theory. This project will also train several graduate students on these topics.<br/><br/>This work is challenging because of time variation, mobility, and asynchronous start and stop times of each task. The fundamental optimization problems involve nonconvex ratios of time averages that have not been significantly explored. The optimal convergence times are unknown. Preliminary work of the principal investigator develops a new method for improving convergence time of subgradient-based convex programming. This project seeks to extend this method and to develop new methods for solving the more difficult problems of task-oriented stochastic networking. Another component of this work is the development of fundamental lower bounds on convergence times in this context. This can have a high impact on broader areas of optimization, stochastic control, online decision making, and machine learning."
"1661278","Labor Market Outcomes of STEM PhDs: Measuring Career Earnings and Occupation Trajectories","DGE","ECR-EHR Core Research","04/01/2017","03/16/2017","Gerald Marschke","MA","National Bureau of Economic Research Inc","Standard Grant","Earnestine Psalmonds","03/31/2021","$499,452.00","Andrew Wang","gerald_marschke@nber.org","1050 Massachusetts Avenue","Cambridge","MA","021385398","6178683900","EHR","7980","8816","$0.00","The National Bureau of Economic Research will create a new database to measure the labor market outcomes of STEM PhDs and postdocs. The research will measure flows of STEM graduates into different economic sectors, estimate the returns on educational investments for STEM PhDs and postdocs, and analyze the determinants of STEM labor demand in industry. The study will formulate and estimate new models of labor demand based on state-of-the-art econometric methods and innovative identification strategies using the new longitudinal data created by the project. The research results will produce a feedback mechanism for educators and policymakers on the outcomes of STEM graduates as well as the impact of STEM training on productivity in the economy, thus enhancing the infrastructure to conduct workforce development research. This project is supported by the Education and Human Resources Core Research Program, which funds fundamental research in STEM learning and learning environments, broadening participation in STEM, and STEM workforce development. <br/><br/>The project will construct a new panel data set of PhDs and postdocs containing both detailed demographic and employment information as well as employer information that will allow the researchers to track PhDs and postdocs forward and backward relative to their university training. The project aims to: (1) produce a new longitudinal data set on labor market outcomes of STEM PhD graduates and postdocs; (2) measure the flows of STEM graduates into different sectors of the economy; (3) estimate the returns to education for STEM PhDs and postdocs; and (4) analyze the determinant of STEM labor demand in industry. To achieve these goals researchers will link the Longitudinal Employer-Household Dynamics (LEHD) database to American Community Survey data, and use administrative data from universities to develop and validate machine learning algorithms to identify STEM PhDs and postdocs in the LEHD-ACS. They will also use a combination of databases, including firm and establishment information, and estimating equations to examine the industry demand for STEM workers. The analysis will provide an understanding of exogenous factors that affect the demand for STEM workers. This work will enable researchers to uncover labor market demands for specialized skills and increase the understanding of how university research contributes to the diffusion of new ideas in the economy.  Finally, researchers will evaluate the relationship between wages and university-based research training and investigate the extent to which the R&D expenditure and federal research funding intensity of the universities where the STEM worker trained influences earnings and later career prospects."
"1719932","Bundle Level Type Gradient Sliding Methods for Large Scale Convex Optimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","07/05/2017","Yunmei Chen","FL","University of Florida","Standard Grant","Leland Jameson","06/30/2020","$154,975.00","","yun@math.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1271","9263","$0.00","The goal of this research is to develop novel algorithms for tackling the computational challenges involved in analyzing data for applications with huge data sets. These include, for example, image processing, data mining, bioinformatics, and statistical learning. The algorithms to be developed in this research will be able to significantly reduce the number of required expensive computations, so that they can be applied to efficiently extract useful information from massive data sets. The research has the potential to advance the algorithms for large scale problems, and greatly increase the applicability for many emerging technologies. An example is the efficient reconstruction of images acquired using partial parallel magnetic resonance imaging. The development of the new methods will also enable researchers to build multi-level complex networks for better learning and prediction in many applications. This project also supports education through undergraduate and graduate student training, course development, and seminar and conference presentations. <br/><br/>This research intends to develop a novel class of accelerated bundle level type gradient sliding methods and related theories for solving large scale composite convex optimization problems and functional constrained convex optimization problems. This new class of algorithms is expected to achieve optimal iteration complexity for each component separately, but will be more general and able to handle the composition of functions with various degrees of smoothness. The algorithms offer the advantages of effectively using historical information, having a scalable scheme for solving the involved sub-problem, providing practical termination conditions for the gradient sliding, and do not impose restrictions on step sizes or require the information on the Lipschitz constants in the cost functions. Moreover, the development of these techniques for the functionally constrained problems will significantly reduce the iteration complexities and improve the practical performance of the existing techniques for functions that are smooth or weakly smooth. Further, the composite gradient sliding and accelerated approach reduces the number of gradient evaluations without increasing  the iteration complexity, while maintaining existing good properties of the approaches for the composite convex problems. The iteration complexity of all the new algorithms will be analyzed, and the practical performance will be validated through numerical simulations and for  practical applications  arising from imaging and machine learning."
"1601084","SCH: EXP: Smart Adaptive Adherence-Enhancing Intervention Strategies for Breast Cancer Prevention","IIS","OE Operations Engineering, Smart and Connected Health","01/01/2017","07/28/2016","Turgay Ayer","GA","Georgia Tech Research Corporation","Standard Grant","Wendy Nilsen","12/31/2020","$289,569.00","","turgay.ayer@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","006Y, 8018","071E, 072E, 073E, 077E, 078E, 8018, 8061","$0.00","Each year about 200,000 women are diagnosed with and more than 40,000 die from breast cancer, the most common female cancer in the US. Late detection significantly reduces survival; while 5-year survival is about 97 percent for early stage breast cancers, it is only about 20 percent for advanced stage cancers. Numerous clinical trials and community setting analyses have shown that repeat mammography use can significantly reduce breast cancer mortality. The reduction in breast cancer mortality due to screening however, is contingent upon adhering to screening recommendations and having consecutive on-schedule mammograms. Therefore, women who do not adhere to receiving repeat mammograms are at risk for developing advanced stage or incurable breast cancers. Indeed, adherence to cancer screening has been identified as a national top priority to reduce cancer mortality. In line with this initiative, the research objective of this project is to optimize the design and allocation of adaptive adherence-enhancing intervention (AEI) strategies to improve overall adherence to mammography screening, while reducing unnecessary costs. From a societal perspective, this research has the potential to significantly improve the efficiency of adherence-enhancing intervention strategies for more effective breast cancer prevention. Results from this research can inform breast cancer prevention policies at the level of the individual health plan, a state's comprehensive cancer control plan, and also at the national level in terms of guideline development. This project will also have an immediate impact on integration of research and learning, and enhancing diversity. Under this project, a PhD student will be trained to apply systems modeling methodologies to healthcare area. In addition, the investigators will engage several minority students into these research activities, and aim to attract them to engineering with a focus on healthcare. <br/><br/>This research will apply machine learning and adaptive stochastic dynamic control methodologies to learn patients' responses to adherence-enhancing interventions and optimize the use of intervention strategies accordingly.  If successful, this project will make several intellectual contributions. First, this will be the first study to optimize the design and allocation of adaptive AEI strategies for sustained mammography use. The team will develop flexible adaptive stochastic control models that capture key disease and intervention dynamics, conduct in depth structural analysis of analytical models, and develop tailored solution algorithms. In parameterizing such models, the team will use large national datasets to inform the models. Further, the team will test policies derived by the analytical models against some actual policies through a detailed simulation model to evaluate possible solutions and estimate the impact. The project's approaches are general and could be applied to other chronic diseases with historically low adherence rates to screening."
"1708925","Complex Genetic Variation and Physiology of Anaerobic Germination in Rice","IOS","NPGI PostDoc Rsrch Fellowship","07/01/2017","06/21/2017","Lina Castano-Duque","NC","Castano-Duque           Lina           M","Fellowship","Diane Okamuro","06/30/2020","$216,000.00","","","","Durham","NC","277057530","","BIO","054y, 8105","1329, 5927, 5978, 7137, 7174, 7577, 9109, BIOT","$0.00","This action funds an NSF National Plant Genome Initiative Postdoctoral Research Fellowship in Biology for FY 2017. The fellowship supports a research and training plan in a host laboratory for the Fellow who also presents a plan to broaden participation in biology. The host institutions for the fellowship are Duke University and the International Rice Research Institute (IRRI; Philippines). The sponsoring scientists are Drs. Thomas Mitchell-Olds and Shalabh Dixit. This award is co-funded by the Office of International Science and Engineering.<br/><br/>To achieve lower production costs for rice the current agricultural trend is to shift from transplanting seedlings to direct sowing of seeds. Those seeds may have to germinate under anaerobic conditions due to flash flooding. The study of complex genetic variation and physiology in rice under anaerobic germination is necessary to produce seeds with traits that can fulfill societal and agronomical needs.  The training objectives in this project are to: 1) learn experimental and analytical approaches for genome association analysis of complex trait variation; 2) learn data analysis approaches that involve genome association and physiology data using machine learning implementation; and, 3) participate and lead outreach activities for minority groups and develop a multi-year curriculum focused on food security with Maureen Joy Charter School in Durham (NC). <br/><br/>This project will study complex genetic variation and physiology of anaerobic germination (AG) in rice natural populations, by using new analytical methods to integrate diverse functional information to identify complex trait loci in genome wide association studies (GWAS). These methods can be applied to a range of traits in genetically tractable species. The main research goal is to identify candidate genes that influence quantitative variation for AG in rice, and to determine their mode of action. Specific objectives are to: 1) use GWAS to identify candidate genes associated with AG in sequenced rice genotypes; and, 2) determine the physiological and genetic correlations that contribute towards AG in rice. Data dissemination will be done through CYVERSE-http://www.cyverse.org/, IRIC- http://iric.irri.org/, and GenBank-http://www.ncbi.nlm.nih.gov/sra. Biological materials and seeds will be made available through IRRI-http://irri.org/our-work/seeds."
"1739505","CPS: Small: Recovery Algorithms for Dynamic Infrastructure Networks","CNS","CPS-Cyber-Physical Systems","11/01/2017","08/30/2017","Hamsa Balakrishnan","MA","Massachusetts Institute of Technology","Standard Grant","David Corman","10/31/2020","$449,765.00","","hamsa@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7918","7918, 7923","$0.00","Most critical infrastructures have evolved into complex systems comprising large numbers of interacting elements. These interactions result in the spread of disruptions, such as delays, from one part of the system to another, and even from one infrastructure to another. Effective tools for the analysis and control of real-world infrastructures need to account for the underlying dynamics.<br/><br/>The key insight in this research is that by learning data-driven models of infrastructure networks, and using these models to determine dynamics-aware recovery algorithms, we can greatly improve the resilience of critical infrastructure networks. We propose to address these challenges by:<br/>1. Learning and validating scalable representations of real systems from data. By considering continuous states, and by modeling the time-varying nature of connectivity as switching between network topologies, we propose to obtain a class of switched linear system models. Multilayer network models will be developed to account for airline networks, and multimodal systems.<br/>2. Characterizing resilience, both for the system as a whole, and in terms of individual nodes (e.g., susceptibility to network delays). The metrics to evaluate resilience will encompass both steady-state and transient behavior.<br/>3. Using the identified models to design optimal control algorithms that can enable recovery from disruptions, taking into account network dynamics, the uncertainty in operating environments, and the costs of decisions to restore service at various levels, at various times.<br/><br/>The results of the research will be validated using operational data, thereby yielding a set of tools for system diagnostics, analysis, and recovery. <br/><br/>Improving and maintaining critical infrastructures are among the grand challenges identified by the National Academy of Engineering. The proposed research will develop techniques grounded in network science, machine learning, and systems and control theory in order to effectively design and operate infrastructures. The development of common frameworks and abstractions for these infrastructures will enable the study of their interdependencies. With the rapid growth of intelligent infrastructures, the proposed research will benefit society, and also help attract and train the next generation of engineering professionals."
"1717916","III: Small: Topics in Temporal Marked Point Processes: Granger Causality, Imperfect Observations and Intervention","IIS","Info Integration & Informatics","09/01/2017","12/13/2018","Tuo Zhao","GA","Georgia Tech Research Corporation","Standard Grant","Maria Zemankova","08/31/2021","$450,000.00","","tzhao80@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364","7364, 7923","$0.00","Event sequences are ubiquitous in many important applications. For example, a major earthquake may trigger many after-shocks and a specific disease in early life leads to many symptoms and diseases later on.  A fundamental question is how different types of events relate to each other and how one type of event causes the occurrence of another type of event. The objective of this project is to address this question by leveraging the probabilistic and statistical methodology of temporal marked point processes, which models the instantaneous likelihood of an event occurrence using history dependent intensity functions. The project will specifically investigate the notion of causality that provides a general framework for tackling the problem of how to control a temporal marked point process. To make the methods practical, the project will also investigate statistical learning problems when point process data are noisy and incomplete. This research targets health informatics and e-commerce. For healthcare, the research has the potential to uncover clinically meaningful comorbidity in disease progression as well as to optimize treatment regimes for the purpose of improving healthcare outcomes. For e-commerce, the research has the potential to improve companies' operational efficiency and enhance user experiences.<br/><br/>This project will focus on machine learning and data mining methodology and algorithms for modeling, learning and control of temporal marked point processes. The goal is in understanding and modeling of how the occurrences of a specific type of events at present and future depend on the occurrences of events of the same and other types happened in the past, and how this dynamic dependency exhibits heterogeneity across a population and across time. Ultimately, it will leverage knowledge of the dynamic properties of temporal marked point processes to design intervention policies to change their time evolution so as to achieve more desirable outcomes.  The technical core of the project is to develop intensity based causal dynamic models of temporal marked point processes with the goal to extend Granger causality to the context of temporal point processes, to make algorithms for temporal marked point processes more practical by systematically investigating inference algorithms under a variety of imperfect observations and to develop methods for the manipulation and control of the time evolution of temporal marked point processes in order to achieve more desirable outcomes."
"1737984","ATD:   Estimation and Anomaly Detection for high-dimensional Data, Maps and Dynamic Processes","DMS","","08/15/2017","08/16/2017","Mauro Maggioni","MD","Johns Hopkins University","Standard Grant","Leland Jameson","07/31/2020","$250,000.00","","mauro.maggioni@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","Q218","6877","$0.00","The project focuses on the analysis of collections of regular, hyperspectral, LiDAR, and thermal images, and multi-modal data sets, both in terms of detection of anomalies and classification tasks.  The input data (such as images, spectra, etc.) for many threat detection problems is typically high-dimensional, corrupted by noise, and subject to nonlinear transformations due to environmental conditions.  Automatic threat detection problems typically face the fundamental curse of dimensionality: to achieve a target level of accuracy, the number of observations required is exponential in the dimension of the data.  This work focuses on the automated discovery of low-dimensional representations of the data, or at least of those features of the data that are sufficient to perform the task at hand. These representations will enable high statistical and computational performance in the above tasks even with a relatively small amount of data. The project will also focus on automatically modeling and learning interaction rules in interacting agent systems.<br/><br/>This project entails an overarching program of research aimed at detecting and exploiting intrinsic low-dimensionality and estimating low-dimensional models for data and certain types of high-dimensional data and agent-based systems. Low-dimensional probabilistic models for high-dimensional data, arising from Hyper-Spectral Imaging (HSI), LiDAR, and Near-Infrad/Night-Vision cameras, will be constructed, enabling efficient data encoding and decoding, statistical models for detecting background noise versus signals of interest, and anomaly detection. Novel techniques for understanding dependencies across multiple sensor modalities by studying maps in high-dimensions between data collected by different sensors will be developed and tested on a variety of multi-modal data sets. Novel machine learning techniques for learning from agent systems with unknown influence functions will be developed."
"1660219","SBIR Phase II:  Providing Automatic System Anomaly Management Software as a Service for Dynamic Complex Computing Infrastructures","IIP","SBIR Phase II","03/15/2017","03/22/2019","Jeremy Neuberger","NY","InsightFinder Inc.","Standard Grant","Peter Atherton","02/28/2021","$1,260,000.00","","jeremy@insightfinder.com","154 Grand Street","New York","NY","100133141","9196001004","ENG","5373","165E, 5373, 8032, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be to greatly improve the robustness and diagnosability of many computing infrastructures including both public and private computing clouds. The proposed technology will significantly reduce the occurrence of performance degradation and service downtime in cloud computing infrastructures, which can attract more users to adopt cloud computing technology and thus benefit society as a whole, which depends increasingly on cloud technology. The project will also advance the state of the art in cloud system reliability research by putting research results into real world use. <br/><br/>This Small Business Innovation Research (SBIR) Phase II project will transform system anomaly management for dynamic complex computing infrastructures. The novelty of the company's solution lies in three unique features: 1) predictive: the solution can raise advance alerts before a serious service outage occurs; 2) self-learning: the solution automatically infers alert conditions and performs automatic root cause analysis using machine learning algorithms; 3) adaptive: the technology adapts to dynamic systems. The proposed research will produce novel and practical anomaly prediction and diagnosis solutions that will be validated in real world computing infrastructures. Specifically, the project consists of three thrusts: 1) adaptive learning in dynamic environments; 2) real-time feature extraction and pattern recognition over system metric and log data; and 3) full stack root cause analysis. During the project the company will implement its software products and carry out case studies with prospective customers on real world computing infrastructures."
"1652943","CAREER: Robust Brain Imaging Genomics Data Mining Framework for Improved Cognitive Health","IIS","Info Integration & Informatics","02/15/2017","06/01/2020","Hua Wang","CO","Colorado School of Mines","Continuing Grant","Sylvia Spengler","01/31/2022","$393,198.00","","huawang@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","7364","1045, 7364, 8089, 8091, 9251","$0.00","The goal of this CAREER project is to identify and establish a new robust data mining framework for better modeling, understanding and analyzing brain imaging genomics data that combine the concepts of sparsity-induced learning models and new and more efficient computational algorithms. The proposed research in this project is innovative and crucial not only to facilitating the development of new data mining techniques, but also to addressing emerging scientific questions in brain imaging genomics, and to greatly supporting the BRAIN Initiative which has recently been unveiled by the U.S. Government and become a national goal. Integrated with the research in this project are the educational goals to create and broadly disseminate new curricular and K-12 outreach materials that focus both on the challenges of large-scale, heterogeneous-modal and high dimensional data processing and on the principles behind the robust data mining techniques for alleviating them.<br/><br/>This project focuses on designing principled data mining algorithms for analyzing multi-modal brain imaging genomics data to yield mechanistic understanding from gene to brain function and to phenotypic outcomes. Of particular interests are (1) large-scale non-convex sparse learning models with linear convergence algorithms, (2) linear computational cost multi-task multi-dimensional data integration algorithms, and (3) evaluation and validation in large-scale brain imaging genomics studies. The research in this project will enable new computational applications in a large number of research areas. The educational materials developed as part of this project will give K-12 students a taste of some of the many fascinating topics in the machine learning and data mining fields while communicating to students the relevance of their mathematics and science classes to futures in engineering."
"1659795","REU Site: Cybersecurity Research in a Multidisciplinary Environment","CNS","RSCH EXPER FOR UNDERGRAD SITES","03/01/2017","01/17/2017","Chunsheng Xin","VA","Old Dominion University Research Foundation","Standard Grant","Harriet Taylor","02/28/2021","$360,000.00","Khan Iftekharuddin","cxin@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","CSE","1139","9250","$0.00","This award establishes a new Research Experiences for Undergraduates (REU) Site at Old Dominion University.  The REU Site is led by faculty from the Center for Cybersecurity Education and Research (CCSER) at the university.  CCSER includes faculty from a large number of departments and colleges across the university that can provide a range of multidisciplinary research projects that focus on problems that are important and attractive to undergraduates.  The REU Site will host 10 students from across the nation to conduct research during the summer with faculty mentors.  The students will live in the Cybersecurity Living and Learning Community associated with CCSER.  The project plans to recruit a diverse cohort of undergraduate students each summer, including students from under-represented minorities, women, and veterans.   In addition to their research activities, the students will participate in other professional development activities that will prepare them for entering the computing workforce and for possible futures as researchers.<br/><br/>The REU site is led by faculty mentors from the Center for Cybersecurity Education and Research. The faculty of the Center have significant research expertise and offer state-of-the-art facilities that should provide a compelling research experience to undergraduates. This REU site offers unique multidisciplinary learning and research opportunities for undergraduate students in the inherently multidisciplinary cybersecurity discipline. Students will explore topics across multiple disciplines including networks, machine learning, decision science, sociology, criminal justice, and philosophy. Students will be advised by a multidisciplinary mentoring committee with faculty from at least 3 different areas. This approach should promote multidisciplinary collaborations among students and increase their interest toward careers in the important field of cybersecurity."
"1758599","Third Workshop on an Open Knowledge Network: Building the National Semantic Information Infrastructure","IIS","Big Data Science &Engineering","11/01/2017","11/02/2017","Sharat Israni","CA","University of California-San Francisco","Standard Grant","Sylvia Spengler","10/31/2018","$48,465.00","","sharat.israni@ucsf.edu","1855 Folsom St Ste 425","San Francisco","CA","941034249","4154762977","CSE","8083","8083","$0.00","This workshop engages a multidisciplinary community in a discussion about taking the initial steps in building an evolving Open Knowledge Network to encode all entities and the relationships among them, that would evolve continuously with new data and information. The OKN would thus be a fundamental building block in evolving the world-wide web to a new level of semantic understanding and application. The vision of such a knowledge graph is feasible given the availability of big data that are growing with time; advanced machine learning techniques that can aid in the creation of such a massive graph, and in its use; and, importantly, demonstration of the power of such knowledge graphs via extant commercial services such as Apple Siri; GoogleTalk; Amazon Alexa, and Microsoft Cortana. An Open Knowledge Network would enable a new generation of knowledge-rich intelligent applications and systems. <br/><br/>The workshop examines the applicability of this idea across multiple disciplines and applications domains.  Experts from a wide range of disciplines, including biomedicine, medicine, geosciences, finance, and manufacturing, will discuss domain-specific issues as well as identify common approaches and common issues that cut across domains. The workshop will develop concrete steps to be taken to help realize the vision of OKN.<br/><br/>Natural interfaces to large knowledge structures have the potential to impact science, education and business to an extent comparable to the WWW. While the first wave of such technology is now available in consumer services such as Siri, GoogleTalk, Cortana and Alexa, these services are limited in their scope of knowledge; not open to direct access or to contributors beyond their corporate firewalls; and, able to answer only relatively limited questions in specific business domains.  An open initiative of the type being proposed would allow for experimentation at scale across many user communities and would support research and innovation in academia, enable industry to experiment, and enable governments at various levels (local, state, federal) to create new services using Open Data. The vision for the proposed OKN is that it would aspire to be a listing of every known concept from the worlds of science, business, medicine and human affairs. It would include not just raw data, but semantic information in machine readable form. The architecture would allow contributors to encode the knowledge related to their topics of interest and, thus, connect that to the larger network, without having to go through so-called ""gatekeepers"". By providing an open service, OKN would enable the notion of ""permission-less innovation."" Indeed, an open resource like OKN may be in a better position to provide more trustworthy information/knowledge than proprietary, closed systems. The vision for creating such a common knowledge network resonates with the NSF Big Ideas of Harnessing the Data Revolution and Convergence."
"1650564","Phase I I/UCRC University of Alabama: Center for Efficient Vehicles and Sustainable Transportation Systems (EV-STS)","IIP","IUCRC-Indust-Univ Coop Res Ctr","02/01/2017","03/30/2020","Yang-Ki Hong","AL","University of Alabama Tuscaloosa","Continuing Grant","Prakash Balan","01/31/2022","$600,000.00","Timothy Haskew, Hwan-Sik Yoon","ykhong@eng.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","ENG","5761","5761, 8040, 9150","$0.00","The University of Alabama (UA) will establish a site within the NSF Industry/University Cooperative Research Center (I/UCRC) for Efficient Vehicles and Sustainable Transportation Systems (EV-STS), joining existing charter sites at the University of Louisville and Arizona State University. This center has been established to support the U.S. automotive/ground transportation industry?s efforts to meet demanding new federal regulations governing vehicle fuel economy and emissions, as well as society?s expectations for improved sustainability in economic and personal activities. The EV-STS Center engages the industry?s critical stakeholders - vehicle manufacturers, component and system suppliers, fleet operators, energy companies, ground transportation industry infrastructure providers, and state and local governments - in identifying important efficiency/sustainability related problems, and formulating research projects that develop innovative solutions to these problems.<br/><br/>The mission of the EV-STS Center and its UA Site is to leverage collaborations among corporate, government, and academic partners to conduct and disseminate industry-relevant research on technologies and tools that facilitate the design, manufacture, deployment, and operation of energy efficient, environmentally sustainable ground vehicles. The scope of this mission includes passenger cars, light- and heavy duty trucks, and motorized off-road equipment. It encompasses both vehicle-level technologies, and the infrastructure and transportation systems that incorporate ground vehicles. The mission is divided into four primary thrust areas: powertrains for full-electric vehicles and the entire continuum of electric-hybrid powertrains, including batteries, electric machines, power electronics, thermal management, packaging, etc., advanced internal combustion engines and alternative fuels, non-powertrain vehicle systems, and ground transportation systems and infrastructure. Within EV-STS, the UA Site will have a research focus on realizing sustainable electrified vehicles. Site-specific topic areas are likely to include magnetic materials for electric motors and generators, high performance electric motor controls, thermoelectric materials for automotive energy recovery, 5G-ready telematics antennas, and machine learning-based fuel efficiency estimation and operator assistance."
"1745477","Convergence HTF: RCN: Enhancing Small and Mid-level Farm Viability Through a Systems-based Research Network: Linking Technology and Sustainable Development and Practice","CMMI","S&CC: Smart & Connected Commun, FW-HTF-Adv Cogn & Phys Capblty, Special Initiatives, IUSE, INSPIRE","09/01/2017","05/16/2019","Divya Srinivasan","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jordan Berg","08/31/2022","$514,924.00","Alexander Leonessa, Kimberly Niewolny","sdivya1@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","ENG","033Y, 082Y, 1642, 1998, 8078","042Z, 060Z, 063Z, 116E, 9102, 9178, 9231, 9251","$0.00","This convergence Research Coordination Network addresses the future of work at the human-technology frontier in the context of small- and medium-sized farms. Intelligent, interactive, and highly networked machines -- with which people increasingly interact -- are a growing part of the landscape, particularly in regard to work.  As automation today moves from the factory floor to knowledge and service occupations, research is needed to reap the benefits in increased productivity and increased job opportunities, and to mitigate social costs.  Convergence is the deep integration of knowledge, theories, methods, and data from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. The Research Coordination Network supported by this award will define key challenges and research imperatives at the nexus of humans, technology, and work through the convergence of robotics, human factors, systems and control theory, neuromotor and cognitive sciences, machine learning, systems engineering, data analytics, precision agriculture, ergonomics, health and safety, and sustainability. This project promotes Convergence by exploring methods for incorporating a human-centric focus, including social and economic considerations, throughout the entire technology research and development process.<br/><br/>The specific focus of this Research Coordination Network is to build research collaborations among multiple stakeholders in order to fully realize the potential benefits of emerging technological capabilities in the context of small- and medium-sized farms. The stakeholders include academic researchers with complementary expertise in human factors, robotics, systems engineering, education, and sustainability, among other areas. Small- and medium-sized farms face a variety of serious economic and demographic challenges. Farming is a strenuous and dangerous occupation, which takes a physical toll on an aging workforce. Innovations in automation and robotics may augment individual capabilities as well as reduce the risk of injury, but with these potential benefits come the threat of job displacement and the possible exacerbation of economic inequity. Currently, creation and development of emerging technological solutions are typically driven along parallel, isolated lines of inquiry, with no awareness of the broad and interrelated nature of the issues involved. This research network seeks to provide researchers across relevant disciplines with this integrated perspective, as well as to involve stakeholders in the agricultural community at a much earlier stage in the technology development process."
"1664368","EAGER: Wearable Nanofabrication Designs Create Better Fitting Intelligent Prosthetic Sockets","CNS","CSR-Computer Systems Research","02/15/2017","02/13/2017","Ming-Chun Huang","OH","Case Western Reserve University","Standard Grant","Samee Khan","01/31/2019","$250,000.00","Hongping Zhao","ming-chun.huang@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","7354","7916","$0.00","The past decades have witnessed great progress in the prosthetics field through new materials and technologies such as targeted muscle reinnervation, powered knee, and ankle prosthesis. However, the biomechanical load due to the unnatural mechanical interaction between the soft tissues of the residual limb and the prosthetic socket is still not fully understood and needs to be further investigated. This interdisciplinary research project will create new sensing and computational methods enabling knowledge discovery for better prosthesis fitting quality with the goal of enhancing the design, fit, usability, and interface of prosthetic limbs for individuals with amputation in practice.<br/>The objective of this project is to investigate force distribution between the stump and socket by developing a wireless sensing device and a force visualization system. This proposal explores an accurate force-sensing design using a unique three-dimensional nanowall network structure, which allows the fabrication of a robust sensor array on a flexible platform for quantitative pressure and shear force measurement. This interdisciplinary research project consists of sensor material synthesis, device fabrication, platform and signal processing unit design for the stump-socket interface application, and a machine learning-based solution to analyze the information that affects the sense of comfortable fit. <br/><br/>The proposed research is multidisciplinary, bringing together the fields of digital design and test, human machine interface, sensors, and signal processing and extending the impact of mobile sensing and health technology to the prosthetics research. A team of investigators and students from Case Western Reserve University and the Cleveland Veterans Affairs Medical Center with complementary expertise will carry out this collaborative project. The proposed solution is expected to help prosthetists to design better devices and make the fitting procedure convenient and straightforward. Students working on the project will receive training across multiple disciplines."
"1650465","I/UCRC: Center for Unmanned Aircraft Systems Phase II Site: Virginia Tech","CNS","Special Projects - CNS, IUCRC-Indust-Univ Coop Res Ctr, , , , , , , , , , , , , ","03/01/2017","04/20/2020","Craig Woolsey","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Behrooz Shirazi","02/28/2022","$1,191,475.00","Timothy McLain, Kevin Kochersberger","cwoolsey@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1714, 5761, Q307, Q325, Q326, R126, R133, R169, R207, R268, S186, S259, S281, S365, U127","5761, 8237, 9251","$0.00","The Center for Unmanned Aircraft Systems (C-UAS) addresses the issues common to the unmanned aircraft system (UAS) industry that limit widespread application across national security, scientific, civil, and commercial domains. Research within the UAS industry is driven by both technical gaps existing for specific high-value applications and the current under-developed regulatory framework that is needed for integration of UAS into the national airspace. The full value of unmanned aircraft systems, especially for a broad range of scientific and civil applications, cannot be realized without significant multidisciplinary research efforts such as those proposed here. Toward that goal, C-UAS investigates and develops new algorithms, architectures, and operational procedures for unmanned aircraft systems. The center contributes to the advancement of the state of the art for UAS through its research at the center's universities and by training graduate students in areas supporting the advancement of UAS. <br/><br/>The research pursued in C-UAS has potential application to unmanned aircraft of all sizes. The primary focus of research activities, however, is on small unmanned aircraft systems (SUAS), which feature aircraft with wingspans in the 1 ft to 8 ft range. C-UAS university sites have distinguished themselves with their experimental flight test demonstrations on these smaller platforms. The research interests and needs of industry in the area of UAS align well with the skills, knowledge, and background of the university participants in the center. Research focus areas for the Virginia Tech site can be described in terms of (1) technical areas and (2) application areas. Technical topic areas in which Virginia Tech has particular strength and interest include:  (i) Advanced flight control (e.g., methods to construct robust, secure, and mathematically certified control algorithms), (ii) Airworthiness, cybersecurity, and reliability (e.g., reliability prediction methods that incorporate imprecise uncertainties intrinsic to small UAS), (iii) Machine vision and machine learning (e.g., software for real-time, on-board intelligent image processing), (iv) Multiphysics design optimization (e.g., aircraft design optimization tools that incorporate unsteady fluid/structure interaction), and (v) Wireless communication (e.g., the use of software defined radio and intelligent communication protocols that adapt the information flow to the environmental conditions). Application topic areas in which Virginia Tech has particular strength and interest include:  (i) Agriculture (e.g., methods to monitor plant pathogen transport), (ii) Civil infrastructure (e.g., bridge inspection), and (iii) Geographic information systems. Specific research projects proposed by Virginia Tech faculty members in these technical and application areas are selected annually by the Industry Advisory Board (IAB)."
"1745365","EAGER: Top-down processes to extract meaning from images","BCS","Cognitive Neuroscience","10/01/2017","07/12/2017","Gabriel Kreiman","MA","Children's Hospital Corporation","Standard Grant","Kurt Thoroughman","09/30/2019","$300,000.00","","gabriel.kreiman@tch.harvard.edu","300 LONGWOOD AVENUE","Boston","MA","021155737","6179192729","SBE","1699","1699, 7916","$0.00","Humans rapidly process images and scenes so they can understand what is occurring in the world around them. Humans do this so well that they outperform existing computational models' ability to understand what elements exist in the scene, their location, or the actions they are involved in. One limitation of computational models is that they do not provide a detailed interpretation of a scene's individual components the way that humans do. For example, the computational model may successfully label an image as containing a horse, but humans will also naturally identify smaller components of the horse, such as the eyes, ears, mouth, mane, legs, tail, and so on. Identifying these individual components and their relationships is an essential part of human visual processing. Such differences in visual understanding create a challenge for constructing artificial computational systems that see and interpret the world similarly to humans. These fundamental limitations are related to the fact that existing computational systems rely primarily on what is called 'bottom-up processing', the sequential processing of visual features from simple to complex ones, which does not account for how human cognition influences meaningful recognition of the image. Our main goal is to investigate the computational principles and neurobiological systems that allow for integration of cognitive experience within visual processing. We combine psychological studies in humans, neurophysiological recordings of brain tissue, and computational work to build an integrative model capable of extracting complex meaning from images, in a way that more closely resembles human capabilities. The research will have broad implications in understanding how the brain processes images and the neural circuits that are involved. Additionally, the insights obtained from this project could have applications in a broad range of domains including robot vision, automatic navigation, surveillance, and automatic clinical image understanding. As part of the project we will establish a summer course based on the research products in which we will train the next generation of scholars at the interface of brains, minds, and machines. <br/><br/>In the human brain, information flows both from low to higher visual areas, as well as in the opposite direction throughout ventral visual cortex. This bi-directional processing has a fundamental role in cortical computations. Yet, the functions implemented by the top-down components form an open problem in visual cognition. Understanding the limitations of feed-forward processing (from lower to higher visual regions) will shed light on the mechanisms by which prior knowledge is integrated with bottom-up inputs, and guide development of new algorithms for extracting useful meaning from sensory input. Applications of deep network models now play a significant role in machine learning across multiple domains, but these fail to capture fundamental aspects of visual processing. Our research program examines the possibility that existing feed-forward recognition models constitute a first stage leading to the initial activation of category candidates, which is incomplete and often inaccurate. The first stage then triggers the application of class-specific processes, which recover a richer and more accurate interpretation of the visible scene, and reject initial false candidates. The proposal involves three main components: (i) Psychophysics experiments to evaluate the accuracy and speed of how humans extract meaning from images; (ii) Invasive neurophysiological recordings along the human ventral visual cortex to understand the neural circuits involved in extracting meaning from a novel data set of minimal images; (iii) A computational model that integrates bottom-up computations with top-down signals to extract meaning from images. The research efforts will be combined with educational and outreach activities aimed at disseminating the scientific insights and incorporating cutting-edge research into training opportunities for undergraduate and graduate students."
"1738070","EAGER:  SC2:  Team Dragon Radio","CNS","Special Projects - CNS, Networking Technology and Syst","04/01/2017","03/27/2017","Kapil Dandekar","PA","Drexel University","Standard Grant","Monisha Ghosh","03/31/2019","$99,978.00","Nagarajan Kandasamy, Steven Weber, William Mongan, Geoffrey Mainland","dandekar@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","1714, 7363","7363, 7916","$0.00","This project on collaborative spectrum sharing brings together multiple research areas not typically considered together on to a single radio platform, where a successful implementation promotes potentially transformative advances in communications and networking. From an educational perspective, this program develops new undergraduate and graduate teaching laboratories and projects. Human capital is developed through these coursework and laboratory experiences, and the students also benefit through collaboration with both industrial and international partners.<br/><br/>This project uses its Dynamic Time Division Multiple Access (D-TDMA) technology for synchronizing radio nodes for optimal collaboration. GPU programming is also leveraged for low-latency, wide-band RF spectrum awareness. Reconfigurable FPGA design for OFDM and the use of machine learning algorithms are used for decision-making and ad-hoc routing. The team also uses an Observe Orient Decide and Act (OODA) cycle for coordinating with the radio environment and other participating nodes/networks. All development takes place on dedicated Linux-based machines connected to USRP X310 software-defined radio nodes."
"1704636","CHS: Medium: Collaborative Research: Managing Stress in the Workplace: Unobtrusive Monitoring and Adaptive Interventions","IIS","HCC-Human-Centered Computing","08/01/2017","09/14/2018","Ricardo Gutierrez-Osuna","TX","Texas A&M Engineering Experiment Station","Continuing Grant","Todd Leen","07/31/2021","$399,850.00","","rgutier@cs.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7367","7367, 7924","$0.00","Workplace stress is a serious problem that has a direct and negative impact on health, happiness, and productivity.  Current approaches for both measuring stress and reducing it are limited; measurements typically rely on self-report or obtrusive sensors, while people often don't seek treatment until the stress has built to dangerous levels (or at all, if they are afraid of other people's judgments). Common workplace sources of stress are noise, distractions and time pressure. This project's goal is to develop methods both to detect stress and provide personalized relaxation exercises, in real time and in the work context.  To detect stress, the research team will apply machine learning to study how well data from commonly available devices at work such as webcams, fitness trackers, and keyboards can predict individuals' stress levels.  To reduce stress, the team will develop a suite of brief relaxation exercises and a system that uses predicted stress levels to recommend different exercises, learning over time which ones work best for a particular person.  These predictive models and interventions will be tested in a long-term study in a real office environment, both validating the work and providing direct effects on experimental participants' well-being.  The project will also have direct educational impacts for groups underrepresented in STEM fields and generate anonymized datasets that other researchers can use. <br/><br/>The team will develop experimental methods to reliably extract stress cues from commodity devices, using a suite of cognitive tasks that represent knowledge work and typical workplace stressors (e.g., time pressure, noise, distractions).  Participants will perform the tasks and experience stressors while the team collects behavioral data from the commodity devices and ground truth stress measurements using physiological signals derived from thermal imaging.  The team will evaluate how well features derived from the sensed behavioral data, using different sets of devices, can predict the ground truth stress data and how it varies based on specific stressors.  The team will also develop a framework to deliver brief stress-reduction exercises that promote deep breathing, a proven effective and learnable stress reduction technique.  The team will use iterative prototyping to develop novel, engaging mobile apps that use biofeedback, games, and music to support breathing exercises; these will be delivered by a multi-arm bandit-based recommendation system that considers the current context (predicted stress and stressors, time of day, particular computer activities) along with historical exercise adherence and results to suggest effective exercises.  The stress sensing models and intervention framework will be validated through a series of lab and field studies with information workers at a software company, collecting stress data in situ with ecological momentary assessment techniques, validated survey instruments for stress and affect, and interviews."
"1737230","BSF: 2016257:  Building Models for Reading Comprehension in Specialized Domains from Scratch","CCF","Special Projects - CCF","09/01/2017","08/31/2017","Vivek Srikumar","UT","University of Utah","Standard Grant","Tracy Kimbrel","08/31/2020","$35,039.00","","svivek@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","2878","014Z","$0.00","Machine learning algorithms are increasingly allowing people to search, structure, and access the textual information created daily in every possible domain. In areas with abundant annotated data, where supervised learning algorithms can be applied, algorithms for text understanding have had success in structuring text and providing natural language interfaces. When building a system in a new domain for which there is little to no data, however, data collection and annotation data can be prohibitively expensive.  This project explores a protocol for developing text understanding systems that read text and provide a natural language interface in a particular domain (such as biology or history) -- this can allow specialized communities to have digital access to data that is otherwise locked in text.  The project also trains students as part of an international collaboration -- this award supports travel of the US-based researchers to collaborate in a project funded by the US-Israel Binational Science Foundation.   <br/><br/>The project encompasses both data collection and model training, and considers the interaction between the two. To replace expert annotations it uses crowdsourcing workers in an iterative procedure that starts training a structured predictor from almost no data. It creates an interactive framework in which users ask questions and verify candidate answers that are later used to retrain the system. It aims to jointly train over multiple domains, and use domain adaptation methods to transfer knowledge from one domain to another. <br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1745562","Convergence ROL: RCN: Cross-Scale Processes Impacting Biodiversity","DEB","Biodiversity: Discov &Analysis, INSPIRE","09/01/2017","08/23/2017","Ana Carnaval","NY","CUNY City College","Standard Grant","Katharina Dittmar","08/31/2022","$499,713.00","Bette Loiselle, Renato Figueiredo, Jeannine Cavender-Bares","acarnaval@ccny.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","BIO","1198, 8078","060Z, 1664","$0.00","Biodiversity on Earth -- comprising an estimated 10 million or more different species -- provides crucial ecosystem services to the planet, including the cycling of nutrients, gases, and water, provision of food, medicine, energy, and shelter. Because biodiversity is essential to the health of the planet, it is important to understand how it is generated, maintained and lost. This topic, however, is extraordinarily complex. Biodiversity distribution patterns and ecosystem services are regulated by processes that operate across multiple hierarchical levels of organization, temporal dimensions, and spatial scales. This Research Coordination Network brings together a diverse set of researchers to integrate data and explore novel concepts that will rapidly advance the field. Researchers will explicitly investigate processes that span hierarchical levels to identify novel properties that could not have been predicted by investigating the individual parts alone. Biologists working at different scales of organization will lead the effort, in coordination with researchers with expertise in machine learning, modeling, and mathematics to ensure the required cyberinfrastructure will advance in sync with new biodiversity and ecological forecasting theory. Annual meetings and workshops will offer diverse training opportunities in biodiversity informatics and scientific communication to students and faculty. The network will establish student research immersion opportunities and extensive cross-disciplinary training through exchanges among biodiversity, environmental biology, and computer sciences laboratories at the collaborating institutions. Based on participant feedback, the Research Coordination Network will adapt best student-centric practices of collaborative research, and report them to the scientific community. Envisioned products include perspective, synthetic, proof-of-concept, and data-driven publications, and presentations at scientific meetings, webinars, and learning modules. The project also will provide outstanding education and networking opportunities to scientists at different career levels, institutions, and cultural backgrounds, contributing to the establishment of a diverse and well-trained workforce in the U.S.<br/><br/>The non-linearity of the complex mechanisms regulating biodiversity and ecological processes makes predictions difficult, and requires diverse data and novel analytical methods to make forecasting more accurate. This Research Coordination Network promotes convergence by bringing together a diverse set of biologists, environmental biologists, computer scientists, and mathematicians to explore the cross-scale processes regulating the Rules of Life, and the theory, models, and cyberinfrastructure needed to analyze them. This Research Coordination Network will focus on four major topics: 1) how to incorporate cross-scale processes into models of biodiversity patterns and predictions about the ecosystem functions they provide; 2) exploration and expansion of novel biodiversity monitoring approaches to better understand patterns and processes acting across scales (particularly through the use of proximal and remote sensing methods); 3) challenges and possible solutions in bioinformatics and cyberinfrastructure to foster new ways to handle storage, integration, and visualization of complex biological and environmental data; and 4) novel approaches to enhance public awareness about the complexity, value, and role of biodiversity. This convergence approach promises to provide novel insights into fundamental questions about biodiversity and ecological forecasting that will have a broad impact on the biological sciences, and society more generally."
"1710716","RET Site: Research Experiences for Teachers in Big Data","CNS","RSCH EXPER FOR UNDERGRAD SITES, RES EXP FOR TEACHERS(RET)-SITE","04/01/2017","09/20/2019","Paul Oh","NV","University of Nevada Las Vegas","Standard Grant","Allyson Kennedy","03/31/2021","$600,000.00","Kazem Taghva, Paul Oh","paul.oh@unlv.edu","4505 MARYLAND PARKWAY","Las Vegas","NV","891549900","7028951357","CSE","1139, 1359","1359, 9150","$0.00","This award establishes a new Research Experiences for Teachers (RET) Site focused on Big Data at the University of Nevada - Las Vegas (UNLV). The project involves a partnership between UNLV and the local Clark County School District (CCSD).  Cohorts of high school teachers in the areas of computer science and technology will participate in summer research projects with UNLV faculty mentors who are actively involved in current research that uses Big Data.  The research projects will focus on topics such as Big Data analytics for the next generation of transportation systems and multimodal data analytics for mobile health.  The participating teachers will translate their research experiences and knowledge into classroom practice by developing instructional modules and course materials that they will introduce in their classrooms and share with other teachers in their school district. These activities all contribute to the formation of a community of practice between UNLV faculty and local educators that has the potential to significantly enhance and improve computer science and technology education in the Clark County School District.<br/><br/>RET participants will attend a 6-week summer institute on the UNLV campus to participate in cutting-edge research projects with mentoring from computer science faculty who lead projects that involve Big Data techniques and analytics. The RET Site research topics focus on projects which involve activities such as data collecting, data representation, data visualization, and machine learning.  These are all areas that are of increasing importance in our data-centric world for teachers and K-12 students. The RET Site project will provide a platform for the participating teachers to develop problem-based instructional materials and laboratory modules that they will share with other teachers in their school district. The excitement of learning about Big Data and related research can inspire the high school students to pursue further computing education and related careers.  The RET Site program will strengthen ongoing partnerships between UNLV and the Clark County School District and lay the foundation for quality computing education in the schools and provide for the future computing workforce needs of the community."
"1718802","RI: Small: Collaborative Research: A Topological Analysis of Uncertainly Representation in the Brain","IIS","Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","08/15/2017","08/04/2017","Chao Chen","NY","CUNY Queens College","Standard Grant","Weng-keen Wong","12/31/2018","$289,992.00","Jin Fan","chao.chen.1@stonybrook.edu","65 30 Kissena Blvd","Flushing","NY","113671575","7189975400","CSE","7495, 8624","7495, 7923, 8089, 8091","$0.00","Characterizing how brain regions activate, collaborate, and interact in cognition empowers us with advanced approaches to help humans make the right decisions on high stress jobs, prevent drug abuse, and treat neurological disorders. This project will study cognitive control in terms of the uncertainty representation, namely, how brains execute the same cognitive task with different levels of uncertainty. Based on theory and algorithms in topology data analysis, the project will analyze brain functional MRI images using novel topological descriptors, which directly model global interactions between brain regions in a principled manner. These descriptors will be used in novel learning models to discover brain activity patterns that are crucial for uncertainty representation. The outcome of the project will include (1) new knowledge in uncertainty representation, e.g., fine-scale activity patterns and interactions between brain regions correlated to the uncertainty level; (2) new topological analysis tools for brain imaging study. This project will bring research and educational opportunities to graduate and undergraduate students from both computer science and neuroscience. The PIs will also mentor students from underrepresented groups and high school students through the CUNY College Now program.<br/><br/>This project will create new computational topology algorithms to extract rich information from the intrinsic structure of data. Novel machine learning methods will be created in order to leverage the topological structures for not only prediction, but also knowledge discovery. A novel interactive data exploration platform based on topological features will be developed for brain imaging study. These techniques and software will be validated on task-evoked fMRI data to produce quantitative assessments of accuracy and to characterize advantages and limitations of these approaches. Domain experts will validate the quality of the approach in validating scientific hypotheses and data exploration."
"1729971","Collaborative Research:   CI-P: ShapeNet: An Information-Rich 3D Model Repository for Graphics, Vision and Robotics Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2017","06/07/2017","Thomas Funkhouser","NJ","Princeton University","Standard Grant","James Donlon","08/31/2018","$33,000.00","","funk@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7359","7359","$0.00","The goal of this project is to plan the development of a richly annotated repository of 3D models called ShapeNet that currently exists only in a preliminary form. ShapeNet will include 3-4 million 3D models of everyday objects in 4-5 thousand categories, in a variety of representations. Models in the ShapeNet repository will be annotated with multiple annotation types: geometric (parts, symmetries), semantic (keywords for the shape and its parts), physical (weight, size), and functional (affordances, scene context). The availability of ShapeNet data, capturing the 3D geometry of a significant fraction of object categories in the world, together with associated detailed meta-data and semantic information, will catalyze major developments in graphics, vision and robotics by providing adequate data against which new proposed techniques and methodologies for shape or scene analysis and synthesis can be vetted -- and with which machine learning algorithms can be trained. ShapeNet can be considered an encyclopedia that facilitates the creation of intelligent systems and agents capable of operating autonomously in the world --- because they can have deep knowledge of that world.<br/><br/>While most of the ShapeNet models will be initially found on the Web, the annotations will be obtained through an active learning combination of modest human input (including crowd-sourcing), extensive algorithmic transport, and human verification. During the planning period the effort will focus on mathematical representations of the semantic knowledge associated with 3D models, as well as on a design framework for key algorithms allowing knowledge transport from one model to another. Further challenges to be addressed include the quantification of data quality issues and the specification of all the multimodal (3D, image, language) UIs and APIs needed for users to be able to exploit and search this wealth of data, or to contribute additional models and annotations to it."
"1800957","Real-Time Feedback-Enabled Simulation Modeling of Dynamic Construction Processes","CMMI","CIS-Civil Infrastructure Syst","09/01/2017","02/05/2018","Amir Behzadan","TX","Texas A&M University","Standard Grant","Yueyue Fan","12/31/2019","$133,726.00","","abehzadan@tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","ENG","1631","029E, 036E, 039E, 1057, CVIS","$0.00","According to the U.S. Census Bureau, in 2015, the U.S. construction industry will surpass $1 Trillion Dollars in spending. Construction and infrastructure projects consist of interconnected networks of people, equipment, and materials. Most often, finding optimal work strategies, and making timely operational decisions that lead to maximum productivity while minimizing project completion cost and time is not trivial. Unlike manufacturing and industrial systems, construction projects involve dynamic (constantly evolving) layouts, complex resource interactions, uncertainties in workflows and processes, and unforeseen conditions that can result in deviations from plans and unwanted delays. Figures show that only 30 percent of construction projects finish on time and within budget. Therefore, the accuracy and timeliness of operational-level decision-making in construction projects is of utmost importance. This award supports fundamental research to enhance construction decision-making accuracy by reducing uncertainties through the seamless integration of process-level data into decision-making. This will be achieved by building the theoretical foundation and significantly advancing the current state of construction simulation modeling through enabling real-time interaction with a simulation model as the real project evolves, and communicating the simulation output through a feedback loop to steer the events in the real project. Therefore, results from this research will benefit the U.S. economy and the society since it leads to better decision-making which results in reducing waste, rework, cost, time, and ensures safety. The multi-disciplinary nature of this project will help broaden participation of underrepresented and diverse student groups in integrated research and pedagogical activities, and positively impact engineering education.<br/><br/>The knowledge-based simulation modeling framework in this project enables process-level models to autonomously learn from and adapt to ever-changing and evolving construction systems. Process-level knowledge that serves as the input of such simulation models is obtained from ubiquitous sensory data that describe relationships, interactions, and uncertainty attributes of field processes, and enable the generation and maintenance of more accurate simulation models. In doing so, some scientific barriers are yet to be overcome to realize the full accreditation and application of this framework. The research team will design and test methods that draw from data mining, machine learning, forecasting, and control to fill the existing knowledge gaps in capturing and mining complex data and meta-data from equipment and human crew interactions. The resulting process-level knowledge will be rich enough to describe, model, analyze, and project the uncertainties of construction systems at any point in time and consequently help adjust resource allocations and operational scenarios on the job site."
"1657476","CRII: NeTS: Next Generation Spectrum Measurement Algorithms and Infrastructures","CNS","CRII CISE Research Initiation, Special Projects - CNS","02/15/2017","05/08/2018","Mariya Zheleva","NY","SUNY at Albany","Standard Grant","Alexander Sprintson","01/31/2020","$190,932.00","","mzheleva@albany.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","026Y, 1714","7363, 8228, 9102, 9251","$0.00","The current paradigm of exclusive spectrum allocation is creating artificial scarcity of spectrum resources that has a dramatic impact on network performance and user experience. Underutilized bands provide an opportunity for more efficient, shared spectrum access that has recently brought together policymakers, industry leaders and academic researchers to set an agenda for next-generation spectrum management. A critical enabler of such advances is deep understanding of spectrum use. This award will support research seeking to impact the scientific community, the policy domain and the public, by providing methods and informing system designs for efficient spectrum measurement and characterization, and informing the design of future mobile wireless networks and spectrum policy. Products from this research will be included in the PI's undergraduate and graduate courses, demonstrating the positive policy and societal impact of computer science and promoting the discipline to diverse groups of students.<br/><br/>The goal of this project is to employ fundamental knowledge in signal processing and propagation, machine learning, and large-scale measurement to devise fast and robust algorithms and inform spectrum measurement infrastructure design. Of main research interest, will be the tight-knit mutual dependence between algorithms and infrastructures. The project will achieve this goal in two research tasks: (i) design and development of spectrum measurement algorithms for adaptive spectrum sensing, spectrum data management and spectrum characterization; and (ii) evaluation of spectrum measurement infrastructures that will illuminate the impact of spectrum sensor properties on spectrum characterization quality and will inform the design of mixed sensor cost/mobility measurement infrastructures. This research will bridge the gap between algorithm design and infrastructures. It will introduce the first holistic framework for spectrum measurement and characterization to facilitate next generation spectrum management. The design of adaptive, data-driven algorithms will facilitate rapid and efficient spectrum measurement and characterization. Exploration of the cost and mobility trade-offs in spectrum sensing infrastructure will inform efficient end-to-end infrastructure design that minimizes cost while maximizing the learning outcomes of spectrum sensing."
"1716432","III: Small: Collaborative Research: Comprehensive Heterogeneous Response Regression from Complex Data","IIS","Info Integration & Informatics","09/01/2017","07/27/2017","Fei Wang","NY","Joan and Sanford I. Weill Medical College of Cornell University","Standard Grant","Amarda Shehu","08/31/2020","$249,040.00","","few2001@med.cornell.edu","1300 York Avenue","New York","NY","100654805","6469628290","CSE","7364","7364, 7923","$0.00","Predictive modeling is one fundamental problem in supervised machine learning. Traditional predictive modeling approaches typically built one predictor for each prediction task. However, in many real world problems, one needs to build predictors for multiple inter-correlated tasks simultaneously. For example, in real-time anesthesia decision making, the anesthesia drugs will have impact on multiple indicators of an anesthesia patient, such as anesthesia depth, blood pressures, heart rates, etc. The anesthesiologist needs to consider all those different aspects as well as their intrinsic dependence before s/he can make the decision. The goal of this project is to conduct systematic research on heterogeneous response regression, which builds multiple regression models for heterogeneous responses as well as exploring the relationship among them.<br/><br/>Specifically, the project's heterogeneous response regression framework is based on a tailored latent factor model that captures the relationship among different responses in a low-dimensional space. The response heterogeneity will be captured by utilizing the exponential family distributions and beyond. The model parameters can be learned through regularized maximum likelihood estimation via a majorization-minimization procedure. Moreover, a distributed solution based on the alternating direction method of multipliers can be derived to enhance the scalability. The project will also make the framework more flexible by (1) adding individualized effect terms to capture sample/feature heterogeneity and induce task dependency; and (2) leveraging a functional model to account for time-varying predictors and to make the prediction time-sensitive. The project will demonstrate the effectiveness of its method in disease risk prediction and financial stability modeling."
"1643534","Biological and Physical Drivers of Oxygen Saturation and Net Community Production Variability along the Western Antarctic Peninsula","OPP","ANT Organisms & Ecosystems","06/15/2017","06/07/2017","Nicolas Cassar","NC","Duke University","Standard Grant","Francis Moore","05/31/2021","$356,500.00","","nicolas.cassar@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","GEO","5111","","$0.00","This project seeks to make detailed measurements of the oxygen content of the surface ocean along the Western Antarctic Peninsula.   Detailed maps of changes in net oxygen content will be combined with measurements of the surface water chemistry and phytoplankton distributions.  The project will determine the extent to which on-shore or offshore phytoplankton blooms along the peninsula are likely to lead to different amounts of carbon being exported to the deeper ocean.   The project team members will participate in the development of new learning tools at the Museum of Life and Science. They will also teach secondary school students about aquatic biogeochemistry and climate, drawing directly from the active science supported by this grant.<br/><br/>The project will analyze oxygen in relation to argon that will allow determination of the physical and biological contributions to surface ocean oxygen dynamics.  These assessments will be combined with spatial and temporal distributions of nutrients (iron and macronutrients) and irradiances.  This will allow the investigators to unravel the complex interplay between ice dynamics, iron and physical mixing dynamics as they relate to Net Community Production (NCP) in the region.  NCP measurements will be normalized to Particulate Organic Carbon (POC) and be used to help identify area of ""High Biomass and Low NCP"" and those with ""Low Biomass and High NCP"" as a function of microbial plankton community composition.  The team will use machine learning methods- including decision tree assemblages and genetic programming- to identify plankton groups key to facilitating biological carbon fluxes. Decomposing the oxygen signal along the West Antarctic Peninsula will also help elucidate biotic and abiotic drivers of the O2 saturation to further contextualize the growing inventory of oxygen measurements (e.g. by Argo floats) throughout the global oceans."
"1651995","CAREER:   Gaussian Graphical Models: Theory, Computation, and Applications","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","08/02/2019","Caroline Uhler","MA","Massachusetts Institute of Technology","Continuing Grant","Gabor Szekely","06/30/2022","$233,223.00","","cuhler@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269, 8048","1045","$0.00","Technological advances and the information era allow the collection of massive amounts of data at unprecedented resolution. Making use of this data to gain insight into complex phenomena requires characterizing the relationships among a large number of variables. Graphical models explicitly capture the statistical relationships between the variables of interest in the form of a network. Such a representation, in addition to enhancing interpretability of the model, enables computationally efficient inference. The investigator develops methodology to infer undirected and directed networks between a large number of variables from observational data. This research has broad societal impact, as it affects application domains from weather forecasting to phylogenetics and to personalized medicine. In addition, the PI is one of the initial faculty hires in a new MIT-wide effort in statistics. As such, the PI has major impact on creating new undergraduate and PhD programs in statistics to train the next generation in big data analytics, crucial for taking on challenging roles in this data-rich world.<br/><br/>The goal of this project is to study probabilistic graphical models using an integrated approach that combines ideas from applied algebraic geometry, convex optimization, mathematical statistics, and machine learning, and to apply these models to scientifically important novel problems. The research agenda is structured into three projects. In the first project, the investigator develops methods to infer causal relationships between variables from observational data using the framework of directed Gaussian graphical models combined with tools from optimization and algebraic geometry. The end goal is to apply this new methodology to learn tissue- and person-specific gene regulatory networks from gene expression data such as the Genotype-Tissue Expression (GTEx) project. In the second project, the investigator develops scalable methods for maximum likelihood estimation in Gaussian models with linear constraints on the covariance matrix or its inverse. Such models are important for inference of phylogenetic trees or cellular differentiation trees. The third project is an application of graphical models to weather forecasting; the investigator develops new parametric methods based on Gaussian copulas and also non-parametric methods for the post-processing of numerical weather prediction models that take into account the complicated dependence structure of weather variables in space and time."
"1734605","I-Corps: Transmedia publishing platform and service system","IIP","I-Corps","03/15/2017","03/16/2017","Elena Fedorovskaya","NY","Rochester Institute of Tech","Standard Grant","Pamela McCauley","08/31/2018","$50,000.00","Andrea Hickerson","eafppr@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will benefit news and broad media consumers, as well as the publishing and printing industries. The project augments print publications with digital interactive media content, both curated and created collaboratively by users. Media rich reading experience will enhance interest in reading and news media consumption. The data obtained during the project may lead to new approaches on engaging different audiences with news media consumption. The project?s commercial impact stems from extending the viability of print media communication industry by both improving reader retention and increasing circulation. A larger audience and the ability to closely target a market based on hard data increases revenue from advertising. Corporations and their advertising agencies will benefit by providing more relevant advertising to end users, delivering more impact per dollar spent.<br/><br/>This I-Corps project uses Augmented Reality to integrate multiple information channels including both physical print and digital media. The digital media assets are hosted on a server and accessed by users through an adaptive interface that selectively displays controls based on the available content and user preferences. Scanned publication pages are classified using a machine learning algorithm trained on a document database and further identified using image matching to allow necessary efficiency and precision. The system collects usage data and interaction history to learn about reading habits and media interaction. These data are used for analytics and also as an input for the image matching workflow. The prototype was developed using iterative user-centered design and tested in experiments for preferred functionality and user experience."
"1646208","CPS: Breakthrough: Control Improvisation for Cyber-Physical Systems","CNS","CPS-Cyber-Physical Systems","01/01/2017","09/01/2016","Sanjit Seshia","CA","University of California-Berkeley","Standard Grant","David Corman","12/31/2020","$425,000.00","","sseshia@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7918","7918, 8234","$0.00","Inspired by the manner in which humans improvise in everyday life, this NSF project is creating a theory of algorithmic improvisation for cyber-physical systems design. It is developing a mathematical framework, supported by tools, to address the challenge of designing systems that adapt to uncertainty in their operating environment and to changing requirements. Moreover, this framework has broad relevance to many fields in computer science and engineering. Results from the proposed work are being incorporated into teaching, with a particularly strong impact on courses at UC Berkeley on cyber-physical systems and formal methods, and on undergraduate projects conducted under broader outreach programs at UC Berkeley. Additionally, through collaborations with industry partners, the project is improving the state of the art in verification and control in the cyber-physical systems industry.<br/><br/>Uncertainty in the design process, in the behavior of sub-systems that evolve over time, and in the operating environment remains a challenge for CPS design. There is a need to design automatic controllers that improvise to handle challenging situations as a skilled human would. This project addresses this need with a foundation approach that is developing a theoretically-sound definition of algorithmic improvisation that is also grounded in practice. It is exploring the full range of variations of the problem definition, analyzing their computational complexity, and devising efficient algorithms where shown to be theoretically possible. Additionally, it is developing new applications to verification, in novel algorithms for simulation-driven verification and verification of machine learning components, and to control, using improvisation for randomized robot path planning and for controlled exploration in adaptive, learning-based control. Together, this tight combination of theoretical work and practical applications seeks to break new ground in the science of cyber-physical systems."
"1650538","Planning I/UCRC University of Connecticut: Center for Science of Heterogeneous Additive Printing of 3D Materials (SHAP3D)","IIP","INDUSTRY/UNIV COOP RES CENTERS","02/01/2017","01/25/2017","Anson Ma","CT","University of Connecticut","Standard Grant","Prakash Balan","01/31/2018","$15,000.00","Rainer Hebert","anson.ma@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","ENG","5761","123E, 5761","$0.00","Additive manufacturing technologies are widely used in a variety of industries including consumer products, automotive, medical, aerospace, and machinery. The additive manufacturing industry exceeds $5 billion in 2015 and is expected to top $20 billion within the next five years. It has become an extremely competitive area of research in countries around the world. To ensure US' global leadership in this emerging field originated from the US, academic partners (currently including Georgia Institute of Technology (GT), University of Connecticut (UConn), and University of Massachusetts Lowell (UML)) have come together to create the Center for Science of Heterogeneous Additive Printing of 3D Materials (SHAP3D). SHAP3D will serve the diverse interests of industry, government, and academia by addressing fundamental research challenges to meet the commercial needs of industry for 3D printing of heterogeneous materials. SHAP3D will develop the critical and necessary insight into fundamental processing-structure-property relationships to predict and control the integration of diverse materials for 3D printing. The work of SHAP3D will be critical as the industry adopts 3D printing for product prototyping, tooling, and higher volume manufacturing with three specific economic outcomes. First, the Center will pursue higher performance materials and composites that enable diverse and lighter weight products to minimize total life cycle costs and environmental footprint. Second, in order to minimize processing costs, the Center will explore more optimal and parallel processes to more quickly print products with higher resolution. Third, SHAP3D will investigate interfacial physics and design concepts for integrating dissimilar materials to facilitate multi-functional components/products, broaden the number of 3D printed applications, and increase market size. Active collaboration with industry partners will ensure relevance to education and training of the future workforce to expedite the adoption and integration of 3D printing methods into manufacturing processes. The three institutions will create a scholarship fund specifically for the recruitment of diverse graduate students. A portion of this scholarship fund will be directed to underrepresented students from minority serving institutions, including community colleges. A large number of companies in Connecticut supporting the aerospace and shipbuilding sectors will benefit from the I/UCRC. UConn?s Additive Manufacturing Innovation Center (AMIC) has introduced state-of-the-art additive manufacturing to over 120 companies and government agencies. Faculty members affiliated with this I/UCRC have also participated in many well-established outreach programs targeting students, industry, and the general public. For instance, an additive manufacturing workshop was held at UConn in 2015 in collaboration with non-profit &#8232;Connecticut Youth Forum. This Forum empowers over 700 urban, suburban and rural youth through civil dialogue, service learning, and leadership development activities. With the support of this award, the UConn team will work closely with the Engineering Diversity Program and student-led 3D Printing Club at UConn to initiate new educational outreach programs, especially targeting high schools in inner city Hartford and Willimantic with high minority populations. In addition to offering industry workshops and relevant projects through existing NSF REU and BRIDGE programs, the UConn team will also coordinate with local industry partners and professional organizations to create new senior design projects and co-op/internship opportunities.<br/><br/>The SHAP3D Center will perform research to understand the synthesis, properties, and processing of heterogeneous materials for integration into complex, additively manufactured products.  The work SHAP3D envisions would encompass many different additive printing methods, such as fused deposition modeling (FDM), selective laser sintering (SLS), stereolithography (SLA), poly/ink jet, and other additive approaches. The Center will perform fundamental material modeling and processing research to establish and translate validated materials and processes to students and practitioners. The proposed center will enable: (i) the rational design and creation of new material feedstocks and, (ii) the understanding of material properties, protocols, and design rules that must be characterized and developed to optimize the process and predict the properties of products and parts created from multiple polymer materials (e.g., different constituent materials, fillers/additives, and interfaces). The proposed technical activities at UConn will draw from the existing strengths of the School of Engineering (SoE) and the Institute of Materials Science (IMS). The four thrust areas are: (i) additive manufacturing of soft materials and integration with metallic and/or ceramic materials, (ii) flexible hybrid electronics, (iii) multifunctional composites&#8232; for aerospace and biological applications, and (iv) scalable nano-manufacturing. These areas will be further supported by interdisciplinary faculty teams with deep knowledge in materials science, machinery, computations and simulations, big data processing, and machine learning. Of particular focus is multi-material 3D printing, which further enables the synergistic use of polymers and non-polymers in ways that best suit a specific application, maximizing the broader impact."
"1729205","Collaborative Research:   CI-P: ShapeNet: An Information-Rich 3D Model Repository for Graphics, Vision and Robotics Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2017","06/07/2017","Leonidas Guibas","CA","Stanford University","Standard Grant","Roger Mailler","08/31/2018","$33,333.00","","guibas@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7359","7359","$0.00","The goal of this project is to plan the development of a richly annotated repository of 3D models called ShapeNet that currently exists only in a preliminary form. ShapeNet will include 3-4 million 3D models of everyday objects in 4-5 thousand categories, in a variety of representations. Models in the ShapeNet repository will be annotated with multiple annotation types: geometric (parts, symmetries), semantic (keywords for the shape and its parts), physical (weight, size), and functional (affordances, scene context). The availability of ShapeNet data, capturing the 3D geometry of a significant fraction of object categories in the world, together with associated detailed meta-data and semantic information, will catalyze major developments in graphics, vision and robotics by providing adequate data against which new proposed techniques and methodologies for shape or scene analysis and synthesis can be vetted -- and with which machine learning algorithms can be trained. ShapeNet can be considered an encyclopedia that facilitates the creation of intelligent systems and agents capable of operating autonomously in the world --- because they can have deep knowledge of that world.<br/><br/>While most of the ShapeNet models will be initially found on the Web, the annotations will be obtained through an active learning combination of modest human input (including crowd-sourcing), extensive algorithmic transport, and human verification. During the planning period the effort will focus on mathematical representations of the semantic knowledge associated with 3D models, as well as on a design framework for key algorithms allowing knowledge transport from one model to another. Further challenges to be addressed include the quantification of data quality issues and the specification of all the multimodal (3D, image, language) UIs and APIs needed for users to be able to exploit and search this wealth of data, or to contribute additional models and annotations to it."
"1827925","CDS&E: Collaborative Research: Computational Design of Topological Superconductors and Weyl - Dirac Semimetals","DMR","CONDENSED MATTER & MAT THEORY","07/05/2017","04/13/2018","Ashvin Vishwanath","MA","Harvard University","Continuing Grant","Daryl Hess","10/31/2019","$81,964.00","","avishwanath@g.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1765","7433, 8084","$0.00","NONTECHNICAL SUMMARY<br/>The Division of Materials Research and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports a close interaction of theoretical and computational research to develop novel theoretical and computational methods and tools for calculating and predicting materials properties, and to use them to discover new materials with novel functionalities. The PIs will develop methods that combine a predictive computational method based on density functional theory with methods from the quantum mechanical theories of many interacting particles and methods from computer science. The PIs will focus on the discovery of new states of electrons that are predicted to exist in materials and involve new ways for electrons organize themselves. The organization obeys rules governed by topology, a mathematical theory that focuses on properties of objects that remain unchanged by deformation. While subtle, topological phases are robust being able to survive materials deformations and imperfections. These new topological states include new kinds of insulators, metals and superconductors. The new tools will enable the PIs and the community to predict specific materials with new electronic topological states that may arise in materials such as topological semimetals and superconductors. <br/>This research effort includes developing and disseminating a new software tool, TOP STUDIO, which will enhance and simplify research on material specific studies of new states of matter. Experimentalists, materials scientists and engineers in the US and in other countries will be able to use the user friendly interface of TOP STUDIO to calculate properties of compounds. The software will enable education on topological properties of electrons in solids at advanced undergraduate and graduate levels.  The project will involve and train graduate students and postdocs who will receive a unique interdisciplinary training in computational and theoretical condensed matter physics and materials. Providing well-written objected oriented modern software, using a standardized interface will allow for broader participation of the community in this research area and for educating the next generation.<br/><br/>TECHICAL SUMMARY <br/>The Division of Materials Research and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports development of new computational methods combining robust electronic structure methods with an advanced many-body theory and machine learning algorithms. The main objective of this project is to develop and implement new methods for the search and discovery of advanced quantum materials with novel magnetic, superconducting and transport characteristics that rely on topologically protected states. The search includes materials that are Weyl-Dirac semimetals and topological superconductors. <br/>The research nurtures the close interaction between theory and computation. The computational approach is based on density functional theory, which is able to predict some properties of many materials including metals and semiconductors, combined with dynamical mean field theory, which includes some effects of strong correlation. To tackle the variety of interactions needed to discover various topological phases in real materials new theoretical methods, powerful algorithms, and computer programs will be developed. Linear response theory will be utilized in order to predict full wave vector and frequency dependent interactions controlling topological superconductivity phenomena. Floquet theory will be used to study topological phases induced by time-dependent fields. The resulting software will contribute to the tools used to search, predict, and discover new materials with topologically protected states of electrons.<br/>The new TOP STUDIO software will be created with a user-friendly interface designed to allow materials exploration by non-experts, by materials scientists and engineers and by theoretical solid-state physicists. TOP STUDIO will promote teaching, training and learning with an educational mode, which can be used to teach students about topological states of quantum matter to students using visualization techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1729486","Collaborative Research:   CI-P: ShapeNet: An Information-Rich 3D Model Repository for Graphics, Vision and Robotics Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2017","06/07/2017","Qixing Huang","TX","University of Texas at Austin","Standard Grant","James Donlon","08/31/2020","$33,333.00","","huangqx@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7359","7359","$0.00","The goal of this project is to plan the development of a richly annotated repository of 3D models called ShapeNet that currently exists only in a preliminary form. ShapeNet will include 3-4 million 3D models of everyday objects in 4-5 thousand categories, in a variety of representations. Models in the ShapeNet repository will be annotated with multiple annotation types: geometric (parts, symmetries), semantic (keywords for the shape and its parts), physical (weight, size), and functional (affordances, scene context). The availability of ShapeNet data, capturing the 3D geometry of a significant fraction of object categories in the world, together with associated detailed meta-data and semantic information, will catalyze major developments in graphics, vision and robotics by providing adequate data against which new proposed techniques and methodologies for shape or scene analysis and synthesis can be vetted -- and with which machine learning algorithms can be trained. ShapeNet can be considered an encyclopedia that facilitates the creation of intelligent systems and agents capable of operating autonomously in the world --- because they can have deep knowledge of that world.<br/><br/>While most of the ShapeNet models will be initially found on the Web, the annotations will be obtained through an active learning combination of modest human input (including crowd-sourcing), extensive algorithmic transport, and human verification. During the planning period the effort will focus on mathematical representations of the semantic knowledge associated with 3D models, as well as on a design framework for key algorithms allowing knowledge transport from one model to another. Further challenges to be addressed include the quantification of data quality issues and the specification of all the multimodal (3D, image, language) UIs and APIs needed for users to be able to exploit and search this wealth of data, or to contribute additional models and annotations to it."
"1653315","CAREER: Prediction of multiscale emergent dynamics in decentralized cell populations","CBET","Cellular & Biochem Engineering","03/01/2017","01/26/2017","Neda Bagheri","IL","Northwestern University","Standard Grant","Steven Peretti","08/31/2020","$500,000.00","","nbagheri@uw.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","ENG","1491","1045","$0.00","PI: Bagheri, Neda<br/>Proposal No: 1653315<br/><br/>The proposed work will develop computational approaches to predict dynamics of cell populations. Signaling present within a cell and from cell-to-cell will be mathematically modeled to predict emergence in cellular, tissue and tumor microenvironments, with special emphasis on breast cancerous tumors. Additionally, STEM concepts will be introduced to young and diverse audiences through children?s textbooks with hands-on exercises that highlight the contribution of women and underrepresented minorities to the field.<br/><br/>Advances in technology offer remarkable insights into individual cell signaling and function; their constraints limit investigation of how these cells cooperate within the microenvironment to produce robust emergent cell population dynamics. Computational approaches can be used to fill gaps in knowledge but biological complexity demands increasingly sophisticated frameworks, and our field has yet to develop a fully integrated, multi-scale, multiclass heterogeneous model that can be adapted to countless contexts to predict emergence of cell populations. This project offers such a framework where large-scale dynamics arise from individual autonomous cell decisions through a predictive agent based model. Our model includes intra-and intercellular signaling, heterogeneity of cell types (healthy and cancer cells) and states (e.g., proliferative, quiescent, migratory, and others), metabolism of nutrients, and physical orientation and constraints. It will be one of the first models to integrate these biochemical and physical responses in a single framework to predict emergence in the microenvironment. Given its generalizable and flexible framework, people from all disciplines can find familiarity in emergence, providing invaluable cross-disciplinary opportunities for discussion and research. The accessibility of such a model will also enable advanced principles on complexity and emergence to be woven into educational material. In addition to curriculum development, STEM concepts will be introduced to young and diverse audiences through children?s textbooks (with hands-on exercises) that highlight the contribution of women and underrepresented minorities to the field. This effort will involve the collaboration of students from STEM and non-STEM fields to advance best practices of teaching and learning for youth. By making STEM topics more familiar and less procedural, the next generation of students will be guided with a basic understanding of computer science, machine learning, complexity, and biology. This CAREER proposal supports multi-disciplinary research opportunities to catalyze understanding of complex biological systems and facilitate integration of related findings into accessible stories and demonstrations distributed to broad audiences."
"1648451","Engineering Research Center for Precise Advanced Technologies and Health Systems for Underserved Populations (PATHS-UP)","EEC","ERC-Eng Research Centers, EFRI Research Projects","10/01/2017","07/10/2020","Gerard Cote","TX","Texas A&M Engineering Experiment Station","Cooperative Agreement","Deborah Jackson","09/30/2022","$15,900,000.00","Ashutosh Sabharwal, Jessica Ramella-Roman, Aydogan Ozcan","gcote@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","1480, 7633","113E, 121E, 123E, 124E, 125E, 129E, 130E, 131E, 132E, 1480, 7680","$0.00","Chronic diseases such as diabetes and cardiovascular disease (CVD) are a leading cause of morbidity and mortality. Every 30 seconds one American will be diagnosed with diabetes and another will suffer a coronary event. These diseases are a burden in underserved communities across the US due to higher prevalence and reduced access to care. Overcoming this human and economic burden is a grand challenge. The vision for the NSF-ERC on Precise Advanced Technologies and Health Systems for Underserved Populations (PATHS-UP) is to change the paradigm for the health of underserved populations by developing revolutionary, cost-effective technologies and systems at the point-of-care. Led by Gerard Coté at Texas A&M University in partnership with the University of California at Los Angeles, Rice University, and Florida International University, PATHS-UP brings outstanding expertise to overcome four barriers endemic to POC devices, the need to: be field deployable, have high accuracy, have low complexity, and be affordable. The mission of PATHS-UP is defined by two overarching goals: (1) to engineer transformative, robust, and affordable technologies to improve healthcare access, enhance the quality of service and life, and reduce healthcare costs and (2) to recruit and educate a diverse group of scientists and engineers who will lead the future in developing enabling technologies to improve health in underserved communities.<br/><br/>PATHS-UP will develop two transformative engineered systems to monitor key biomarkers (biochemical, biophysical, and behavioral) of chronic disease: a Lab-in-your-Palm (LiyP) and a Lab-on-a-Wrist (LoaW). The LiyP will be enabled by novel amplification biochips based on nano-engineered single-molecule chain reactions combined with innovative handheld computational imaging and modular spectroscopic instruments. The LoaW will be enabled by unique, ""bar-code like"" biochemical marker implants (grain of rice in size) coupled with a novel wrist-worn spectral imager to visualize the implant through tissue and innovative sensors to monitor biophysical markers (cuffless blood pressure, heart rate). PATHS-UP will also develop innovative algorithms that monitor behavior (diet, medication intake) and predict long-term complications. These enabling technologies are founded on rigorous research in biomaterials, nanoscale systems, sample enrichment, computational imaging, multimodal data integration, and machine learning. Testbeds include one-of-a-kind in vitro phantoms, human subject studies in controlled lab environments, and patients in underserved communities. Developing and integrating these transformational systems into communities requires a multidisciplinary team of engineers, medical doctors, public health experts, industry professionals, and community health leaders. Such broad technical scope and societal outreach go beyond traditional funding sources and require the formation of the PATHS-UP ERC. The team will use participatory design and community engagement to prevent PATHS-UP from merely throwing technologies at these communities, and instead develop technologies that seamlessly integrate into their lives.<br/><br/>Underserved communities in every US state have higher prevalence and less access to equitable healthcare services. Thus, many people in these communities go undiagnosed or are diagnosed late, which can lead to serious consequences. To address this challenge, PATHS-UP will develop advanced technologies to prevent, delay the onset, and manage diabetes and cardiovascular disease. This requires both the development of transformational health technologies and systems and a paradigm shift in how these technologies are integrated into communities. Beyond the obvious societal health impact of the Center?s systems, the students, post-docs, and faculty nurtured by the Center?s intellectual community will also be a significant outcome of PATHS-UP.  The team has a passion to promote meaningful, lasting, engagement with K-College students, especially under-represented minorities and K-12 teachers in our partner underserved communities.  PATHS-UP will provide experiential learning and new engineering/public health curriculum for college students, research experiences for K-12 students and their teachers, and opportunities for participatory design with key stakeholders and community engagement, to promote a rich intellectual environment. The team also has a history of entrepreneurship, having spun off biomedical companies with students, and see building the innovation ecosystem as a vital part of PATHS-UP.<br/>"
"1718853","RI: Small: Collaborative Research: A Topological Analysis of Uncertainly Representation in the Brain","IIS","Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","08/15/2017","08/04/2017","Junzhou Huang","TX","University of Texas at Arlington","Standard Grant","Rebecca Hwa","07/31/2021","$210,000.00","","jzhuang@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7495, 8624","7495, 7923, 8089, 8091","$0.00","Characterizing how brain regions activate, collaborate, and interact in cognition empowers us with advanced approaches to help humans make the right decisions on high stress jobs, prevent drug abuse, and treat neurological disorders. This project will study cognitive control in terms of the uncertainty representation, namely, how brains execute the same cognitive task with different levels of uncertainty. Based on theory and algorithms in topology data analysis, the project will analyze brain functional MRI images using novel topological descriptors, which directly model global interactions between brain regions in a principled manner. These descriptors will be used in novel learning models to discover brain activity patterns that are crucial for uncertainty representation. The outcome of the project will include (1) new knowledge in uncertainty representation, e.g., fine-scale activity patterns and interactions between brain regions correlated to the uncertainty level; (2) new topological analysis tools for brain imaging study. This project will bring research and educational opportunities to graduate and undergraduate students from both computer science and neuroscience. The PIs will also mentor students from underrepresented groups and high school students through the CUNY College Now program.<br/><br/>This project will create new computational topology algorithms to extract rich information from the intrinsic structure of data. Novel machine learning methods will be created in order to leverage the topological structures for not only prediction, but also knowledge discovery. A novel interactive data exploration platform based on topological features will be developed for brain imaging study. These techniques and software will be validated on task-evoked fMRI data to produce quantitative assessments of accuracy and to characterize advantages and limitations of these approaches. Domain experts will validate the quality of the approach in validating scientific hypotheses and data exploration."
"1749864","EAGER: Probabilistic Models and Algorithms","CCF","Algorithmic Foundations","09/15/2017","09/05/2017","Aravind Srinivasan","MD","University of Maryland College Park","Standard Grant","Joseph Maurice Rojas","02/29/2020","$129,000.00","","srin@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7796","7916, 7926","$0.00","The power of randomness in the computational context is one of the key discoveries of computer science. This project studies algorithms for fundamental problems that will further explore and use such power offered by randomness. One example of such a basic problem considered is in facility location: how can we place facilities at a low cost in order to minimize the total commute time for the customers? This classical problem has significant modern applications as well: e.g., in clustering data for machine learning, and in placing services on the Internet cloud. The PI aims to resolve how best this -- and closely-related problems in clustering and facility location -- can be solved via new probabilistic techniques. The PI will also study other such fundamental problems (e.g., in efficient resource allocation) in optimization through randomized algorithms, as well as study how such investigations improve and/or develop broadly-applicable probabilistic techniques.<br/> <br/>This project aims to have broader impact also through human-resource development. A graduate student will be directly involved in almost all this proposed research. The PI has advised two multi-year ``Gemstone"" undergraduate research teams: both teams have had their research published in national conferences. The PI proposes to start advising a new such team around 2018 and continue to expose them to the interplay between algorithms, randomness, and networks. Furthermore, the PI will mentor two high-school students; these students will continue to learn algorithms, randomness and applications from their basics up to cutting-edge research. The research of one of these high-school students, co-mentored by the PI, was invited to compete in the Intel International Science and Engineering Fair in 2017. <br/> <br/>A substantial part of this project will be on developing improved approximation algorithms through (new techniques in) randomization: approximation algorithms are those that are provably efficient and deliver solutions that are within a provable distance from optimal. Two representative examples of problems considered in this regard are in facility location (opening a subset of a given set of facilities in order to minimize the sum of the facility-opening costs and the total commute-time of the customers, as well as variants of this classical problem), and assigning jobs to servers in a general setting, in order to minimize the sum of the completion times of the jobs (weighted by individual priorities of the jobs). Methodologically, this project proposes to develop new techniques to carefully ""round"" infeasible solutions to feasible solutions via randomization, to further understand the power of information-theoretic notions (e.g., when one has a range of choices of probability distribution for the random choices to be made, can choosing a distribution that maximizes the entropy offer additional power?), and the interplay between linear-algebraic and probabilistic arguments. The ""rounding"" approach is flexible and general: e.g., for the facility location and job-assignment problems, it is computationally easy to start with near-optimal but infeasible solutions, and the key open question is how to optimally round to a feasible solution. Advances in such rounding techniques have had numerous consequences in approximation algorithms; a key goal of this project is to contribute to further such advances."
"1645444","Collaborative Research: GEODES: Geoscience Diversity Experiential Simulations","ICER","Integrat & Collab Ed & Rsearch","03/15/2017","03/28/2017","Jason Chen","VA","College of William and Mary","Standard Grant","Brandon Jones","02/29/2020","$196,007.00","","jachen@email.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","GEO","7699","","$0.00","Part I<br/>The Geoscience Diversity Experiential Simulations (GEODES) project tests new research-based methods for providing professional development (PD) in equity and inclusion for geoscientists. The project will create virtual reality role-play simulations designed to teach cohorts of geoscientists to develop and practice skills for recognizing bias and effectively intervening in geoscience-specific scenarios. This new ability to act will also be harnessed in the project?s leadership curriculum by focusing on institutional work to minimize the effects of bias on gatekeeping decisions. Testing the success of these scenarios in changing real-life behavior will contribute to the literature around diversity training for scientists. Evaluation of our comprehensive PD program will inform promising practices for geoscience departments to change attitudes and practice to create a more welcoming climate for all.<br/>Our project provides PD to university faculty to train them as champions for diversity who will also become role models for other professionals. Participants will begin to systematically open up the profession by employing a social-closure lens to minimize bias in key professional gate-keeping roles (e.g., search committees and graduate admissions). Furthermore, our virtual simulation provides a tool that various levels of geoscience leadership might be interested in demonstrating, advertising, and implementing. The proposed new immersive-learning tool will be generalizable to other STEM fields. A combination of machine learning and technology development may allow such active-participation simulations for all faculty, students, and administrators in the near future. The leadership curriculum will systematize the broad literature on institutional change for inclusion, thus creating a coherent, understandable, and general strategy that can be applied to many more institutions<br/><br/>Part II<br/>The Geoscience Diversity Experiential Simulations(GEODES)project draws from social cognitive theory (Bandura, 1986) and social closure theory (Murphy, 1988) to provide professional development (PD) for a cohort of geoscientists to 1) increase their knowledge of bias and social justice issues relevant to the geosciences; 2) engage in bystander intervention training using virtual reality simulations; 3) develop leadership skills to target critical gatekeeping decisions to transform their home institutions; and 4) support their continued development into champions for diversity, equity, and inclusion. These goals are achieved through several phases, beginning with a three-day intensive workshop that teaches and challenges participants to think critically about social justice and develops leadership skills. The workshop will culminate in bystander intervention training using virtual reality role-play scenarios. Participants will continue to meet for a monthly journal club over video conferencing, where issues of  equity, inclusion, and leadership are discussed as a group. Measured outcomes include pre-post changes in attitudes and knowledge regarding diversity, equity, and leadership techniques."
"1742661","NSF Student Travel Grant for 2017 Secure Knowledge Management Workshop (SKM)","CNS","Secure &Trustworthy Cyberspace","06/01/2017","06/06/2017","H. Raghav Rao","TX","University of Texas at San Antonio","Standard Grant","Sara Kiesler","05/31/2018","$12,142.00","Shambhu Upadhyaya, Raghu Santanam, Manish Agrawal, RAM KRISHNAN","hr.rao@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","8060","025Z, 7434, 7556","$0.00","This award supports travel of graduate students to a multi-disciplinary workshop in 2017 with emphasis on topics related to cyber-security. The travel grant will enable career development and learning opportunities. Attending conferences is an important component of graduate school education for future computer security researchers. Students will have the opportunity to discuss leading edge research with world-class computer security researchers, and establish networks, connections, and mentoring relationships that will serve them well during their research careers. <br/><br/>The workshop the graduate students will attend is called the Secure Knowledge Management (SKM) workshop. This workshop deals with topics in machine learning, privacy, trust, risk, and social and economic aspects, and technology domains such as IoT, cloud computing and big data. The requested funds will be used to support student travel awards, and members of underrepresented groups will be especially encouraged to apply. Students will be exposed to the state of the art in cyber-security as presented by experts from both academia and industry. In addition, students will hear about internships and job openings, and have ample opportunity to discuss employment and collaboration with more senior members of the field. The results of the conference, and the awards made possible by sponsors will be made available to the research community."
"1653392","CAREER: SusChEM: Unlocking local solvation environments for energetically efficient hydrogenations with quantum chemistry","CBET","Catalysis","02/01/2017","08/30/2018","John Keith","PA","University of Pittsburgh","Standard Grant","Robert McCabe","01/31/2022","$526,746.00","","jakeith@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","ENG","1401","1045, 8248","$0.00","The project addresses the production of carbon-neutral liquid fuels via electrocatalytic reduction of the greenhouse gas carbon dioxide (CO2) to methanol.  Specifically, the study seeks to improve the efficiency and selectivity of current solvent-based electrochemical processes by advancing understanding of how aqueous electrolytes participate in the overall reaction mechanisms at the atomic scale.  The research will be coupled with educational thrusts that engage students in grades 8-12 in learning about renewable energy catalysis and computational chemistry.<br/><br/>The focus of the study will be to integrate high-level electronic structure theory, molecular dynamics, and machine learning to quantitatively understand how interactions between solvent molecules, salts, and co-solutes (i.e. ""local solvation environments"") regulate fundamental mechanisms of CO2 reduction (CO2R) into fuels.  Four basic scientific questions will be addressed related to CO2R in the presence of aromatic N-heterocycles, here studied in the form of molecules and as nitrogen-doped carbon electrodes.  These are 1) the identification of the most likely chemical functionalities (i.e. Lewis base, Brønsted acid, H-atom donor, hydride donor) that participate in energetically efficient CO2R into methanol; 2) quantitative predictions of the free energy barriers for different CO2 hydrogenation processes in different local solvation environments; 3) refined understanding of the level of computational modeling needed to reliably predict hydrogenation thermodynamics and kinetics in realistic electrochemical environments; and 4) generalized insight into the degree to which local solvation environments can be tuned to enhance the conversion of low-value carbon-containing feedstocks to liquid fuels. Graduate and undergraduate students will develop educational modules that combine concepts in renewable energy and introduce computational chemistry modeling. These modules will then be tested to determine their capacity to engage and excite students in the Pittsburgh Public School District about opportunities in STEM fields."
"1718798","III: Small: Collaborative Research: Comprehensive Heterogeneous Response Regression from Complex Data","IIS","Info Integration & Informatics","09/01/2017","07/27/2017","Kun Chen","CT","University of Connecticut","Standard Grant","Amarda Shehu","08/31/2021","$250,000.00","","kun.chen@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7364","7364, 7923","$0.00","Predictive modeling is one fundamental problem in supervised machine learning. Traditional predictive modeling approaches typically built one predictor for each prediction task. However, in many real world problems, one needs to build predictors for multiple inter-correlated tasks simultaneously. For example, in real-time anesthesia decision making, the anesthesia drugs will have impact on multiple indicators of an anesthesia patient, such as anesthesia depth, blood pressures, heart rates, etc. The anesthesiologist needs to consider all those different aspects as well as their intrinsic dependence before s/he can make the decision. The goal of this project is to conduct systematic research on heterogeneous response regression, which builds multiple regression models for heterogeneous responses as well as exploring the relationship among them.<br/><br/>Specifically, the project's heterogeneous response regression framework is based on a tailored latent factor model that captures the relationship among different responses in a low-dimensional space. The response heterogeneity will be captured by utilizing the exponential family distributions and beyond. The model parameters can be learned through regularized maximum likelihood estimation via a majorization-minimization procedure. Moreover, a distributed solution based on the alternating direction method of multipliers can be derived to enhance the scalability. The project will also make the framework more flexible by (1) adding individualized effect terms to capture sample/feature heterogeneity and induce task dependency; and (2) leveraging a functional model to account for time-varying predictors and to make the prediction time-sensitive. The project will demonstrate the effectiveness of its method in disease risk prediction and financial stability modeling."
"1735095","NRT: Interdisciplinary Training in Complex Networks and Systems","DGE","NSF Research Traineeship (NRT), Project & Program Evaluation","09/01/2017","08/12/2019","Luis Rocha","IN","Indiana University","Standard Grant","Vinod Lohani","08/31/2023","$3,049,174.00","Bernice Pescosolido, Katy Borner, Olaf Sporns, Armando Razo, Selma Sabanovic","rocha@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","EHR","1997, 7261","1331, 1371, 7433, 9179, SMET","$0.00","Understanding complex networked systems is key to solving some of the most vexing problems confronting humankind, from discovering how dynamic brain connections give rise to thoughts and behaviors, to detecting and preventing the spread of misinformation or unhealthy behaviors across a population. Graduate training, however, typically occurs in one of two dimensions: experimental and observational methods in a specific area such as biology and sociology, or in general methodologies such as machine learning and data science.  With more and more students seeking to gain sufficient expertise in mathematical and computational methods on top of domain-specific laboratory and social analysis methodologies, a greater demand for more efficient training is emerging. This National Science Foundation Research Traineeship (NRT) award to Indiana University will address this growing need with an integrated dual PhD program that trains students to be ""bidisciplinary"" in Complex Networks and Systems (CNS) and another discipline of their choosing from the natural and social sciences. It will seamlessly integrate traditional education with interdisciplinary hands-on research in a culture of academic and human diversity. This program will provide unique interdisciplinary training for thirty-four (34) PhD students, including twenty-two (22) funded trainees. The program will provide additional training experience to 40 summer affiliate students and a population of more than 300 participants across the participating PhD programs.<br/><br/>The training program capitalizes on the new Indiana University Network Science Institute (IUNI). The Institute's 165+ faculty members will serve in interdisciplinary PhD program committees to be co-chaired by research mentors from both CNS and the target empirical domain. Project-driven, team-based research at IUNI will seamlessly integrate academic education with interdisciplinary hands-on scientific and industrial research. Trainees will learn to connect the general-purpose, computational expertise of CNS to the deep, domain-specific research methodologies of the natural, behavioral, and social sciences thus bridging the gap between distinct training cultures. They will be a new breed of STEM scientists that escapes the silos of disciplinary training to address the complex problems of the 21st century. Specifically, the four goals of training activity are: 1) provide dual research proficiency; 2) develop collaborative skills via early integration into problem-driven, interdisciplinary research; 3) produce a diverse workforce by recruiting student cohorts from a broad set of disciplines and varied backgrounds to be trained within a team culture; 4) establish a sustainable interdisciplinary training model by enlarging the institutional channels created between informatics and natural and social sciences to other Indiana University departments and institutions. A science-of-science study conducted throughout the NRT project will evaluate the efficacy of interdisciplinary training of the students in this program. This project will develop a flexible dual PhD program and best-practices to allow additional departments at Indiana University to join the program in the future, as well as other institutions to develop similar programs.<br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training. The Traineeship Track is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas, through comprehensive traineeship models that are innovative, evidence-based, and aligned with changing workforce and research needs."
"1840857","RI: Medium: Collaborative Research: Next-Generation Statistical Optimization Methods for Big Data Computing","IIS","Robust Intelligence","09/01/2017","09/19/2018","Han Liu","IL","Northwestern University","Continuing Grant","Rebecca Hwa","12/31/2020","$237,718.00","","hanliu@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7495","7495, 7924","$0.00","This project develops a new generation of optimization methods to address data mining and knowledge discovery challenges in large-scale scientific data analysis. The project is constructed in the context that modern computing architectures are enabling us to fit complex statistical models (Big Models) on large and complex datasets (Big Data).  However, despite significant progress in each subfield of Big Data, Big Model, and modern computing architecture, we are still lacking  powerful optimization  techniques to effectively integrate these key components.<br/><br/>One important bottleneck is that many general-purpose optimization  methods are not specifically designed for statistical learning problems. Even some of them are tailored to utilize specific problem structures, they have not actually incorporated sophisticated statistical thinking into algorithm design and analysis. To tackle this bottleneck, the project extends traditional theory to open new possibilities for nontraditional optimization  problems, such as nonconvex and infinite-dimensional examples.  The project develops deeper theoretical understanding of several challenging  issues in optimization (such as nonconvexity), develops new algorithms that will lead to better practical methods in the big data era, and demonstrates the new methods on challenging bio-informatics problems.<br/><br/>The project is closely related to NSF's mission to promote Big Data research, and will have broad impacts. In the Big Data era, we see an urgent need for powerful optimization methods to handle the increasing complexity of modern datasets.  However, we still lack adequate methods, theory, and computational techniques.  By simultaneously addressing these aspects, this project will deliver novel and useful statistical optimization methods that benefit all relevant scientific areas. The project will deliver easy-to-use software packages which directly help scientists to explore and analyze complex datasets.  Both PIs will also design and develop new classes to teach modern techniques in handling big data optimization problems. All the course materials - including lecture notes, problem sets, source code, solutions and working  examples - will be freely  accessed online.  Moreover, both PIs will write tutorial  papers and disseminate the results of this research through the internet, academic conferences, workshops,  and journals.  Through senior theses and potentially the REU (Research Experiences for Undergraduates) program, the proposed project will also actively include undergraduates and engage under-represented minority groups.<br/><br/>To achieve these goals, this project develops (i) a new research area named statistical optimization, which incorporates sophisticated statistical thinking into modern optimization, and will effectively bridge machine learning, statistics, optimization,  and stochastic analysis; (ii) new theoretical frameworks and computational methods for nonconvex and infinite-dimensional optimization, which will motivate effective optimization methods with theoretical  guarantees that are applicable to a wide variety of prominent statistical models; (iii) new scalable optimization methods, which aim at fully harnessing the horsepower of modern large-scale distributed computing infrastructure.  The project will shed new theoretical light on large-scale optimization, advance practice through novel algorithms and software, and demonstrate the methods on challenging bio-informatics problems."
"1720237","Operator Splitting Methods: Certificates and Second-Order Acceleration","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/07/2017","Wotao Yin","CA","University of California-Los Angeles","Standard Grant","Leland Jameson","06/30/2020","$205,000.00","","wotaoyin@math.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1271","9263","$0.00","This research project is centered on development of improved numerical algorithms for application to large-scale systems that include, for example, signal/image/video reconstruction and processing, bioinformatics, and automated learning or mining of information from very large data sets. Operator splitting is a class of methods that decomposes a difficult problem into simple sub-problems.  Within the past decade, operator splitting methods gained popularity due to the growing demand to handle ever-larger models. For example, signal processing and machine learning applications often have multiple parts that are easy to handle separately but are very challenging when combined. Ideas from operator splitting have led to efficient algorithms for broad classes of objective functions that are used to define the underlying systems. There is still, however, much to be done to handle complex situations. Through further development of operator splitting techniques, this research has the potential to provide efficient and stable approaches to solve a yet wider class of challenging problems. The project also includes educational impact through the development of courses, presentation of seminars, and graduate student training opportunities.<br/><br/>The principal investigator intends to design and implement algorithms that improve the speed and stability of operator splitting methods. This project aims to extend the principle of operator splitting in two ways. First, operator splitting algorithms will be introduced that recognize infeasible and feasible-but-unbounded optimization problems, as well as those that have finite optimal values but unattainable solutions. Such pathological problems are not rare and cripple existing techniques. The new algorithms will address these pathologies and make future solvers more robust. Second, by incorporating second-order information in a novel fashion, the project will address two significant drawbacks of operator splitting algorithms. These are the slow tail convergence, and the sensitivity to severe problem conditions. Techniques to ensure global convergence will be developed. Because operator splitting is a high-level abstraction, the results of the project will apply to a broad range of numerical methods that arise in science and engineering."
"1758807","Supporting US-Based Students to Participate in the 2017 IEEE International Conference on Data Mining (ICDM 2017)","IIS","Info Integration & Informatics","11/15/2017","10/25/2017","Raju Gottumukkala","LA","University of Louisiana at Lafayette","Standard Grant","Maria Zemankova","10/31/2018","$24,000.00","Vijay Raghavan","raju@louisiana.edu","104 E University Ave","Lafayette","LA","705032014","3374825811","CSE","7364","7364, 7556, 9150","$0.00","This award provides travel support for 16 U.S.-based graduate students to participate in the 2017 International Conference on Data Mining (ICDM 2017), held in New Orleans, LA, November 18th - 21st, 2017 (http://www.icdm2017.bigke.org). The conference attracts new and original research, and some of the top data mining researchers from the U.S., and abroad to discuss their latest. The conference covers advancement in research in many topics relevant to data mining that include statistics, machine learning, pattern recognition, databases, data warehouses, data visualization, knowledge-based systems and high-performance computing. The proceedings of the ICDM conference will be distributed through the IEEE Computer Society and will be available through the IEEE Explore Digital Library.<br/> <br/>Besides having a strong technical program, the conference features workshops in emerging topics on data mining, tutorials, panels, demos, and the PhD Forum. The PhD Forum is designed to provide an interactive environment in which PhD students can meet, exchange their ideas and experiences both with peers and with senior researchers from the data mining community. The ICDM organizers recognize the importance of recruiting, engaging and retaining students in data mining research given the importance of this topic in emerging field of data science. The conference participation enables the students to share their original research results with their peers and experts, learn about new algorithms and tools in the field of data mining, and obtain experience with application of data mining for various transdisciplinary problems. Participation of U.S.-based students is critical to developing U.S. competitiveness of future workforce in science and technology."
"1660221","SBIR Phase II:  Enabling Techologies for Energy-Centric Mobile App Design to Extend Mobile Device Battery Life","IIP","SBIR Phase II","03/01/2017","02/15/2019","Abhilash Jindal","IN","Mobile Enerlytics LLC","Standard Grant","Peter Atherton","02/28/2019","$759,998.00","","jindal.abhilash@gmail.com","1281 Win Hentschel Blvd","West Lafayette","IN","479064182","7653378990","ENG","5373","5373, 8032, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will result from it having far-reaching societal, commercial, and technological impact. (1) Initial research by the Principal Investigator on analyzing the energy drain of AngryBirds has demonstrated the severe energy inefficiency of popular mobile apps in today's app market. The importance of this work is heightened by smartphones being an important enabler of Internet access for disadvantaged people in both developed and developing countries, and hence being an important tool in overcoming the ""digital divide"". (2) Commercially, the project will foster a paradigm shift in the mobile app industry ($101B industry in 2020) from the current feature-centric to energy-aware app design. Such a paradigm shift will have a significant, long lasting impact on the app industry. Energy-efficient apps lead to longer battery life, which in turn leads to longer user engagement time, which ultimately translates into millions of dollars of increased mobile revenue as all major businesses are shifting towards mobile. Hence this SBIR project will lead to a marketable product. (3) Technically, the proposed work will extend the performance profiling technology that is foundational to the software industry into the energy dimension, which is critical to the mobile software industry.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project will develop the industry's first app energy management (AEM) solution to help app developers reduce app battery drain, and extend the battery life of billions of smartphones. The research objectives are (1) to develop advanced energy debugging techniques that can automatically identify energy drain opportunities from legitimate energy hotspots; and (2) to develop an SDK-based app energy monitoring system for monitoring app energy drain when running on consumer phones in the open market. These objectives pose significant technical challenges. While similar challenges on performance metrics (such as running time) have been well studied for traditional software, in particular in high-performance computing, in this project the company is expanding them to the energy dimension for the mobile app industry, which has not been attempted before. The company will develop novel machine-learning based solutions to learn, classify, and auto-detect energy optimization opportunities. As a result it expects to develop the first set of solutions to these fundamental challenges in optimizing the energy drain of millions of mobile apps in the app market."
"1656830","A framework for analyzing converging feedforward and cortical-bulbar feedback dynamics in target detection from complex odor scenes","IOS","STATISTICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY, Robust Intelligence, Modulation","08/01/2017","08/13/2019","Dinu Albeanu","NY","Cold Spring Harbor Laboratory","Continuing grant","Edda Thiels","07/31/2021","$921,000.00","","albeanu@cshl.edu","1 BUNGTOWN ROAD","COLD SPRING HARBOR","NY","117244220","5163678307","BIO","1269, 1271, 7334, 7454, 7495, 7714","8007, 8091, 9178, 9179, 9263","$0.00","This project makes use of recent advances in optical imaging and optogenetic strategies to monitor the brain at work. Specifically, the project is focused on understanding the interplay between ascending and descending (feedback) activity patterns in the olfactory system of behaving mice. Here, the investigator does not simply focus on the olfactory sensory module that integrates and transmits information from the nose to the brain but determines how higher brain areas, namely, the olfactory cortex, interact in the recurrent processing loop. This strategy enables the investigator to evaluate how sensory inputs are shaped by internal brain states via feedback. Furthermore, the investigator works at the interface of two approaches by combining cutting-edge experimental approaches--optical imaging and optogenetic strategies-- with novel computational models that give rise to non-mutually exclusive testable predictions. The investigator determines whether these feedforward-feedback loops contribute to attention states, extraction of odor identity, or broadcasting of predictions and error signals related to the incoming odorants. Experimental techniques are complemented, through an international collaboration, with state-of-the-art data analysis that characterizes neuronal population dynamics along high-dimensional trajectories and measures occurrence of activity patterns, characteristic timescales, patterns interaction, and coordination as a function of behavior. Additionally, the project provides opportunities for students and postdoctoral trainees from the USA and Romania to expand their experimental and computational skills through their participation in the international collaboration.<br/><br/>A central goal of systems neuroscience is to describe behaviors in terms of the neuronal circuits that control them. This constitutes a steep challenge in the mammalian brain, because behaviors are thought to rely on widely distributed feedforward, as well as top-down feedback neural representations, which are technically difficult to monitor at large scales and manipulate at cellular resolution. The project builds on recent experimental results from the lead investigator and novel algorithms for odor identification developed by the international collaborator. Specifically, the project probes the fine structure of olfactory perception and tests the central hypothesis that feedback serves one or more of the following three mechanisms: predictive coding, attractor generation, or attention to enhance the discriminability of behaviorally relevant stimuli. The dynamics of: a) cortical-bulbar feedback, and b) olfactory bulb output neurons on which feedback acts indirectly via interneurons are monitored and subsequently modulated with cellular resolution in mice engaged in olfactory discrimination forced-choice tasks and contextual reversal learning tasks. Reversible optogenetic local suppression of cortical feedback in the olfactory bulb is combined with simultaneous two-photon resonant scanning imaging (100 Hz) of hundreds of neurons. To address the proposed feedback roles, specific experimental design is combined with machine learning tools and dynamical systems analysis."
"1650504","Phase II I/UCRC Trustees of Boston University: Center on Biophotonic Sensors and Systems","IIP","IUCRC-Indust-Univ Coop Res Ctr","03/01/2017","03/05/2019","Thomas Bifano","MA","Trustees of Boston University","Continuing grant","Prakash Balan","06/30/2018","$102,243.00","","tgb@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","ENG","5761","116E, 5761, 8038, 9181, 9251","$0.00","CBSS operates at the intersection of life sciences and photonics engineering and focuses on precompetitive research that provides the enabling technologies for advanced methods to detect, sense and identify biological properties, conditions or changes at the molecular, cellular and subcellular level.  These innovations will address solutions in the areas of Disease Diagnosis, Drug Efficacy Testing & Monitoring, Drug Discovery, and Food & Water Safety through long-term partnerships with industry members. During Phase II, research efforts will continue at Boston University (lead institution) and UC Davis (university site) to develop collaborations with faculty across our campuses whose research is relevant to the Center?s mission.  BU?s Creating the Societal Engineer, a concept of a social consciousness and appreciation for how products advance our quality of life while creating jobs and economic opportunity, resonates well with a primary mission of the Center to educate a diverse pipeline of trained scientist and engineers.  Students have an opportunity to directly participate in the technology transfer process and with industry learn to identify applications and define areas of greatest needs, evaluate new technologies and research tools, and pursue research that can be applied to solving real-world challenges and improving healthcare outcomes. As CBSS grows, the Center intends to capitalize on a trend by companies in the life sciences sector to accelerate innovation through collaborative research and university-led innovations, specifically in the area of biophotonic sensing. <br/><br/>The Center's value to the membership is based on the ability to deliver pre-competitive research that can be adapted to align with a member?s product portfolio plans. This high priority research and product directed research typically remains the focus of each individual company, but collectively, the membership values the Center for:<br/>- Adding to the breadth and diversity of a member's research portfolio.<br/>- Working on pre-competitive technologies that could add some benefits in the future.<br/>- Working on alternative approaches that have merit but which a member company has neither, the time or resources to pursue.  This also helps a member better position their existing product/technology approaches.<br/>- Providing exposure to emerging areas of research that may be outside the company?s domain, thus adding to a member?s innovation capability.<br/>The research projects, guided by industry strategic oversight, will lead to breakthroughs in understanding cell/host interactions, new biomarker discovery, point-of-care and interoperation diagnostics, personalized cancer treatment, and many other technologies that will improve the quality of healthcare. The Center?s research is aligned with three overlapping thrust areas: Bio-Imaging, Optical Diagnostics and Analytics, and each of these thrust areas has a component in Systems, Sensors & Devices, and Materials & Biology. The goal of the Analytics research is to embed data retrieval and computing processing power in imaging and diagnostic tools to develop an understanding of biological, genetic, behavioral and environmental data gathered so that healthcare decisions can be made down to the individual level.  At BU recent developments in high-resolution tip-tilt-position deformable mirrors (TPP-DM) combined with computation and machine learning algorithms demonstrate the combinations of bio-imaging and analytics for applications such as imaging of blood and tissue samples or cell sorting."
"1743101","ABI Innovation: EAGER: Towards an optimal experimental design framework with Omics data","DBI","ADVANCES IN BIO INFORMATICS","09/15/2017","09/07/2017","Ilias Tagkopoulos","CA","University of California-Davis","Standard Grant","Peter McCartney","08/31/2020","$300,000.00","","iliast@cs.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","BIO","1165","7916","$0.00","The goal of this project is to create novel methods that help researchers gather and integrate existing data sets in order to better inform the design of future experiments. Ideally new experiments include some replication, fill in gaps and produce new knowledge, but this balance is hard to achieve when existing data sets are in different locations or organized in very different ways.  Thus to achieve the goal, two aims are proposed: 1) develop methods for creating cohesive biological datasets from public experiments such that they are suitable for training computational models; 2) develop methods that indicate what experimental conditions should be used to collect new datasets so that are the most likely to yield important information about the study organism's biological properties, like structure and behavior. Achieving this goal will enable an understanding of important rules of life for organisms more efficiently and economically, by focusing on the experiments that give us the most value for the funds spent.<br/><br/>This exploratory project will focus on data arising from genome-wide transcriptional profiling methods (e.g. microarrays, RNA-Seq), building a computational foundation for later expansion. First, optimal data processing techniques for creating integrated compendia will be assessed, in order to select the best method for building training datasets for machine learning methods. Second, data-driven computational models will be trained on the data compendia and evaluated for success in describing and microbial behavior. Third, given the normalized compendia (in the transcriptomics data space) an optimal experimental design methodology will be prototyped, to recommend the best set of experiments to perform to yield the complete set of data needed to fit and test the biological model. The experimental design methodology will be benchmarked using synthetic data, and then evaluated by exploring the effect of design- recommended combinations of antibiotics and antiseptics (10 in all) on microbial behavior. This will be compared to the outcomes of experiments designed by methods currently used. Success metrics will focus on how quickly the required information in the experimental space is gathered and what level of uncertainty in a model remains after each experiment is completed."
"1841569","CAREER: An Integrated Inferential Framework for Big Data Research and Education","DMS","STATISTICS, Division Co-Funding: CAREER","09/01/2017","08/13/2019","Han Liu","IL","Northwestern University","Continuing Grant","Gabor Szekely","06/30/2021","$347,702.00","","hanliu@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","MPS","1269, 8048","1045","$0.00","This project addresses several fundamental challenges in modern data analysis and aims to create a new research area named Big Data Inference. Currently available literature regarding Big Data research mainly focuses on developing new estimators for complex data. However, most of these estimators are still in lack of systematic inferential methods for uncertainty assessment. This project hopes to bridge this gap by developing new inferential theory for modern estimators unique to Big Data analysis.  The deliverables of this project include easy-to-use software packages, which directly help scientists to explore and analyze complex datasets. The principal investigator is also actively collaborating with many scientists to ensure the more direct impact of this project to the targeted scientific communities.<br/><br/>This project aims to develop novel inferential methods for assessing uncertainty (e.g., constructing confidence intervals or testing hypotheses) of modern statistical procedures unique to Big Data analysis. In particular, it develops innovative statistical inferential tools for a variety of machine learning methods which have not yet been equipped with inferential power. It also provides necessary inferential tools for the next generation of scientists to be competitive in modern data analysis."
"1717532","SHF: Small: Enabling and Analyzing Accuracy-aware Reliable GPU Computing","CCF","Software & Hardware Foundation","08/01/2017","07/07/2017","Adwait Jog","VA","College of William and Mary","Standard Grant","Yuanyuan Yang","07/31/2021","$449,999.00","Evgenia Smirni","ajog@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7798","7923, 7941","$0.00","Graphics Processing Units (GPUs) are becoming the default choice for general-purpose hardware acceleration because of their ability to enable orders of magnitude faster and energy-efficient execution for large-scale high-performance computing applications. Since the majority of such applications executing on large-scale HPC systems are long-running, it is very important that they cope with a variety of hardware- and software-based faults. Many prior works have shown that real HPC systems are vulnerable to soft errors. An absence of essential protection and checkpointing mechanisms can lead to lower scientific productivity, operational efficiency, and even monetary loss. However, these protection mechanisms (e.g., error correction codes) are themselves not free -- they incur very high performance, energy, and area costs. <br/><br/>This project takes a holistic approach to explore the avenues to reduce these protection overheads by taking advantage of the fact that all errors do not lead to an unacceptable loss in the accuracy of application output. Prior results show that GPGPU applications are amenable to such accuracy-aware optimizations. In order to enable these optimizations, this project will address three major research questions: a) What hardware/software support and tools are necessary to determine which instructions are not vulnerable to soft errors, b) Based on this analysis, which hardware component(s) need not be protected and for how long, while not sacrificing application quality beyond the user's quality requirements, and c) What optimizations in terms of resource management and scheduling are necessary to make low-overhead but reliable computation more effective and efficient. These questions will be explored via a variety of GPGPU applications emerging from the areas of high-performance computing (HPC), big-data analytics, machine learning, and graphics. If successful, this project will generate several novel research insights that will play an important role in enabling low-cost reliable GPU computing. The results of this project will be integrated into the existing and new undergraduate and graduate courses on computer architecture and reliability, which will facilitate in training students, including women and students from diverse backgrounds and minority groups. "
"1665212","Next-Generation NMR Crystallography Through Ab Initio Structure Refinement","CHE","Chem Thry, Mdls & Cmptnl Mthds","07/01/2017","08/07/2018","Gregory Beran","CA","University of California-Riverside","Standard Grant","Evelyn Goldfield","06/30/2021","$486,122.00","","gregory.beran@ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","MPS","6881","7433, 8084, 9216, 9263","$0.00","Gregory Beran of the University of California Riverside is supported by an award from the Chemical Theory, Models and Computational Methods program in the Chemistry Division to develop new computational tools that will facilitate the determination of three-dimensional crystal structures via nuclear magnetic resonance (NMR) spectroscopy.  Knowledge of molecular crystal structures is essential in pharmaceuticals and many other areas of chemistry.  Different crystal packing motifs (""polymorphs"") of the same molecule can exhibit vastly different properties, and occurrences of undesirable polymorphs have caused major drug recalls and other serious problems for patients and pharmaceutical manufacturers.  NMR spectroscopy is increasingly used to determine crystal structures and characterize different polymorphs.  Translating the set of peaks present in an NMR spectrum into a three-dimensional crystal structure frequently involves trial-and-error computational modeling of potential structures to identify the structure whose predicted NMR spectrum best matches the experimentally observed one.  This project develops computational tools to circumvent this trial-and-error process and enable direct crystal structure refinement which can automatically solve for the crystal structure that produces the experimentally observed NMR spectrum.  These new tools will significantly improve the utility of NMR for solving crystal structures.  <br/><br/>Achieving these goals requires research advances in three areas. First, new computationally practical electronic structure methods for modeling the non-covalent interactions that govern crystal packing are being developed to enable identification of good initial crystal structures.  Second, because accurate predictions of the NMR chemical shifts for a given structure are essential to discriminating between correct and incorrect structural assignments when comparing predicted and observed spectra, models which predict NMR chemical shifts more reliably than widely used existing techniques are being developed.  Third, to enable direct NMR-driven crystal structure refinement, machine learning models that reproduce how changes in molecular conformation and crystal packing impact the chemical shifts are being developed.  Success will lead to greater ability to engineer molecular crystals with desired properties.  In addition, this award is supporting outreach activities at a local elementary school and production of online tools to educate new users about computational chemistry approaches."
"1800582","Collaborative Research:  Observed and Future Dynamically Downscaled Estimates of Precipitation Associated with Mesoscale Convective Systems","AGS","Climate & Large-Scale Dynamics","08/25/2017","10/23/2017","Vittorio Gensini","IL","Northern Illinois University","Standard Grant","Eric DeWeaver","07/31/2021","$49,537.00","","vgensini@niu.edu","301 Lowden Hall","De Kalb","IL","601152828","8157531581","GEO","5740","026Z","$0.00","A mesoscale convective system (MCS) is a collection of thunderstorms organized on a larger scale than the storms it contains, in which the individual thunderstorms act in concert to generate the atmospheric motion that organizes and sustains the system.  These large storm systems produce extreme weather including hail, floods, and tornados, but they also make an important contribution to water resources over the eastern two thirds of the continental US (CONUS) during the growing season.  This project seeks to understand MCS behavior in an aggregate sense, including the long-term contribution of MCS precipitation to the overall water balance of the CONUS and the importance of year-to-year variability in MCS activity for anomalously wet (flood) or dry (drought) conditions.  <br/><br/>A key tool for conducting the research is the  Weather Services International (WSI) National Operational Weather radar (NOWrad) data set, a 20-year record (currently 1996-2015) created from the National Weather Service radar stations which provide continuous near-total coverage of the CONUS. A primary goal of the project is to develop and apply an automated procedure to detect and track MCSs in the radar data.  The algorithm identifies MCSs as contiguous or semi-contiguous features in radar maps over an area of at least 100km along the system's major axis exceeding a threshold reflectivity value.  MCS tracking is complicated by the the tendency of MCSs to split and merge as they propagate, and the algorithm incorporates a method for identifying mergers and splits. A further issue is that large regions of intense precipitation can occur in frontal cyclones and landfalling hurricanes, and a classification scheme is necessary to distinguish these regions from MCSs.  A machine learning technique to perform this classification is developed using expert judgement to train a random forest classifier (RFC) scheme. Further expert judgement is solicited through a survey which invites the research community to participate in the development and validation of the tracking and classification schemes.  The catalog of MCS events and their characteristics (intensity, duration, structure, etc) is then used to study MCS seasonality, interannual variability, and contribution to CONUS rainfall including floods and droughts.<br/><br/>Further work uses a global climate model (GFDL-CM3) in combination with a regional convection permitting model (WRF-ARW at 4km horizontal resolution) to simulate MCSs over the CONUS under present-day and projected future climate conditions.  The simulations are analyzed according to the tracking and classification schemes developed for the NOWrad data, and the model simulations allow examination of how MCS behavior depends on climatic factors such as tropospheric moisture, soil moisture, atmospheric stability, and large-scale atmospheric circulation.<br/><br/>The work has broader impacts due to the importance of MCS rainfall as a water resource for agriculture and the severe weather hazards related to MCS activity.  The algorithms and datasets produced for the project will be shared with researchers and operational climatologists and hydrologists through an online portal.  In addition, the project supports and trains a graduate student and provides summer  support for an undergraduate, thereby providing for the future scientific workforce in this area."
"1738929","CICI: RSARC: Infrastructure Support for Securing Large-Scale Scientific Workflows","OAC","Cybersecurity Innovation","09/01/2017","01/30/2020","Ping Yang","NY","SUNY at Binghamton","Standard Grant","Robert Beverly","08/31/2021","$999,999.00","Shiyong Lu, Guanhua Yan, Fengwei Zhang","pyang@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","8027","","$0.00","The scientific workflow is an important paradigm for automating and accelerating data processing and sharing in the scientific community. The correctness of scientific discoveries relies on the trustworthiness and reliability of the data processed by scientific workflows and the underlying cyberinfrastructure. Unfortunately, modern scientific workflow systems lack robust infrastructure support for the trustworthy execution of scientific workflows and for the protection of the data processed by such workflows. A scientist or student may forge or alter datasets or computation simply to get papers accepted for publication. A malicious user may also publish forged workflow data on websites, misleading other scientists into investigating and publishing invalid results. This project aims to support a community of engineers and scientists to collaboratively and securely collect, analyze, and share data using scientific workflows.  The success of this project contributes significantly to the national cyberinfrastructure vision of securing the scientific discovery process for a wide range of science and engineering disciplines.<br/><br/>This project develops infrastructure support for secure execution of scientific workflows, detection of anomalous execution flows, and protection of scientific data.  In particular, this project: (1) develops a trusted execution environment for scientific workflows leveraging the Intel Software Guard Extension (SGX) to protect the execution of scientific workflows as well as the data processed by scientific workflows; (2) produces encrypted, tamper-proof, and non-repudiable block-graphs that enable scientists to verify the origin of scientific data and examine how a piece of data was modified and distributed; and (3) develops a machine-learning based anomaly detection technique to detect anomalous execution flows based on logs collected by the underlying cyberinfrastructure."
"1637225","Collaborative Research:  Observed and Future Dynamically Downscaled Estimates of Precipitation Associated with Mesoscale Convective Systems","AGS","Physical & Dynamic Meteorology, Climate & Large-Scale Dynamics, AGS-ATM & Geospace Sciences","09/01/2017","04/09/2019","Walker Ashley","IL","Northern Illinois University","Standard Grant","Eric DeWeaver","08/31/2021","$321,074.00","","washley@niu.edu","301 Lowden Hall","De Kalb","IL","601152828","8157531581","GEO","1525, 5740, 6897","026Z, 4444, 5740, 6897","$0.00","A mesoscale convective system (MCS) is a collection of thunderstorms organized on a larger scale than the storms it contains, in which the individual thunderstorms act in concert to generate the atmospheric motion that organizes and sustains the system.  These large storm systems produce extreme weather including hail, floods, and tornados, but they also make an important contribution to water resources over the eastern two thirds of the continental US (CONUS) during the growing season.  This project seeks to understand MCS behavior in an aggregate sense, including the long-term contribution of MCS precipitation to the overall water balance of the CONUS and the importance of year-to-year variability in MCS activity for anomalously wet (flood) or dry (drought) conditions.  <br/><br/>A key tool for conducting the research is the  Weather Services International (WSI) National Operational Weather radar (NOWrad) data set, a 20-year record (currently 1996-2015) created from the National Weather Service radar stations which provide continuous near-total coverage of the CONUS. A primary goal of the project is to develop and apply an automated procedure to detect and track MCSs in the radar data.  The algorithm identifies MCSs as contiguous or semi-contiguous features in radar maps over an area of at least 100km along the system's major axis exceeding a threshold reflectivity value.  MCS tracking is complicated by the the tendency of MCSs to split and merge as they propagate, and the algorithm incorporates a method for identifying mergers and splits. A further issue is that large regions of intense precipitation can occur in frontal cyclones and landfalling hurricanes, and a classification scheme is necessary to distinguish these regions from MCSs.  A machine learning technique to perform this classification is developed using expert judgement to train a random forest classifier (RFC) scheme. Further expert judgement is solicited through a survey which invites the research community to participate in the development and validation of the tracking and classification schemes.  The catalog of MCS events and their characteristics (intensity, duration, structure, etc) is then used to study MCS seasonality, interannual variability, and contribution to CONUS rainfall including floods and droughts.<br/><br/>Further work uses a global climate model (GFDL-CM3) in combination with a regional convection permitting model (WRF-ARW at 4km horizontal resolution) to simulate MCSs over the CONUS under present-day and projected future climate conditions.  The simulations are analyzed according to the tracking and classification schemes developed for the NOWrad data, and the model simulations allow examination of how MCS behavior depends on climatic factors such as tropospheric moisture, soil moisture, atmospheric stability, and large-scale atmospheric circulation.<br/><br/>The work has broader impacts due to the importance of MCS rainfall as a water resource for agriculture and the severe weather hazards related to MCS activity.  The algorithms and datasets produced for the project will be shared with researchers and operational climatologists and hydrologists through an online portal.  In addition, the project supports and trains a graduate student and provides summer  support for an undergraduate, thereby providing for the future scientific workforce in this area."
"1707405","NeuroNex Innovation Award: A National Resource for Mesoscale and Connectomic Brain Mapping","DBI","PHYSICS OF LIVING SYSTEMS, Cross-BIO Activities","09/01/2017","08/31/2018","Narayanan Kasthuri","IL","University of Chicago","Continuing Grant","Sridhar Raghavachari","02/29/2020","$799,999.00","","bobbykasthuri@anl.gov","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","BIO","7246, 7275","8091","$0.00","Mapping brains at arbitrary scales from the cellular composition of entire brains to reconstructing every synapse is a powerful tool for understanding brain function and dysfunction.  However, the financial, engineering and computational barriers to new brain mapping technologies are far beyond the reach of most laboratories and even universities and institutions. A NeruoHub at a national lab, Argonne National Lab (ANL), could provide widespread access to such technologies. Specifically, the Hub will leverage the brightest hard X-ray source and one of the fastest computers at ANL, to provide mesoscale maps that detail the locations of every cell, their shapes and ultimately their connections with each other in large volumes to entire brains.  The Hub will craft these technologies for brain mapping across diverse species, specifically the nervous system of the octopus.  Enabling comparative studies across brain regions and species like the octopus will provide insights into what is general and what is idiosyncratic, and can reveal novel solutions when applied to species with distinctive perceptual or behavioral capabilities. Finally, the Hub will nurture and develop the relationship between neuroscience and the national lab system. For decades, many other fields of science have benefitted from strong collaborative relationships with the national lab system, leveraging large-scale resources to significantly advance their respective fields. Neuroscience has not. The Hub will establish the first pipeline for providing access and introduction to the broader neuroscience and national lab communities, paving the way for future collaborations.  <br/><br/><br/>The technical approach is to develop sample preparation protocols for staining large volumes to entire octopus brains with heavy metals (e.g. osmium, lead, and uranium), embedding in plastic, and imaging with synchrotron source micro- and nano-x-ray microscopy followed by automated serial section electron microscopy.  Development of new protocols will leverage existing protocols and imaging approaches for mammalian brains while also developing capabilities at ANL for 100nm resolution projection x-ray microscopy. Where possible, reconstructions of neurons and their processes from micro- and nano-X-ray datasets will be validated with automated large volume serial electron microscopy. In tandem existing machine learning algorithms used for tracing neurons and their processes will be scaled onto Argonne High Performance Computers for analyses of X-ray datasets."
"1650531","EAGER:   Mining Heterogeneous Network Constructed from Multiple Data Sources","IIS","Info Integration & Informatics","01/01/2017","09/09/2016","Christopher Yang","PA","Drexel University","Standard Grant","Wei Ding","12/31/2019","$198,662.00","","ccy24@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7364","7364, 7916, 9150","$0.00","Relying on a single data source for knowledge discovery often results in unsatisfactory performance because of the missing patterns involving other potential entities and their relationships.  This is particularly important in healthcare informatics areas such as pharmacovigilance.  Pharmacovigilance is an important healthcare issue due to the impact of the adverse drug reactions.  It complicates patients' medical conditions, increase hospital admissions, and contribute to more morbidity and event death.  In current pharmacovigilance research, most work only consider a single data source for discovering the associations between the two entities, namely drugs and adverse drug reactions.  This project develops a novel framework to integrate multiple data sources, including spontaneous report systems, electronic health records, pharmaceutical databases, scientific literature, and web data, for heterogeneous network mining.  Such a heterogeneous network consists of multiple entities, including drugs, adverse drug reactions, patients, diseases, and symptoms, and various types of relationships among such entities.<br/><br/>This project extends the capability of machine learning, data analytics, and pharmacovigilance by integrating multiple data sources for pharmacovigilance applications.  In particular, the inclusion of patient-centric data on the web creates insights that may not be obtained from traditional data sources mainly contributed by health professionals.  The outcomes of the project include techniques for heterogeneous path mining and structural topological pattern mining on four pharmacovigilance applications, namely adverse drug reaction detection, drug-drug interaction, prescribing cascade, and phenotypic information discovery.  Such techniques can also be extended for drug repositioning and off-label use identification.  The result of this research is beneficial to multiple disciplines including pharmacy, medicine, public health, and computing.  The integrated education plan includes incorporating the research findings into courses offered by the Master of Science program in Health Informatics.  The outreach plan involves organizing workshops, conferences, and seminars to disseminate the research outcomes."
"1725235","Policy as a Private Good: Firm-Lobbyist-Politician Networks in the Legislative Process","SES","Political Science","08/15/2017","05/22/2018","In Song Kim","MA","Massachusetts Institute of Technology","Continuing Grant","Brian Humes","07/31/2020","$244,293.00","","insong@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","1371","","$0.00","General Abstract<br/>This project investigates a key question in political science, ""Who governs?"".  It seeks to identify the set of political actors exert who greater levels of influence over policy-making.  Representation is one of the fundamental pillars of democracy, ensuring citizens the right to petition elected government officials in order to influence policies. Yet, a vast literature has demonstrated that the policy-making process in the U.S. is dominated by individuals with substantial economic resources.  Especially important players include the firms owned by those elites who make campaign contributions, lobby, and make political endorsements to increase their leverage in influencing government policies. Despite the significance of this ""unequal representation,"" empirical studies of political representation have been limited because it is difficult to observe a direct link between political actors and politicians, let alone the true underlying preferences that drive their interactions. The proposed project will investigate the direct representation of specific firms' interests by politicians in the form of highly complex policy outputs. The investigator will develop a theoretical framework to identify the conditions under which firms invest in political networks.  To overcome the empirical challenge in identifying the links between firms and politicians, the investigator will then construct a large database of records of firms' lobbying on congressional bills and their political connections to the legislators who sponsor lobbied bills.  The project will not only provide valuable insight into the influence of interest groups, it will also produce an easily accessible website that will illustrate the important players and their positions in the policy process.  This will be of great value to scholars, and will serve as a valuable teaching resource as well. <br/><br/>Technical Abstract<br/>This project will develop a set of machine learning tools used to analyze political connections. Specifically, the project will combine the methodologies of ideal point estimation widely used in political science with probabilistic models of latent networks originating in computer science. The primary goal of the proposed methodology is to identify the location of firms, lobbyists, and politicians in a common 'ideological' space in which the relative proximity of various participants implies a closer political connection or alignment of interests.  Groups of firms, lobbyists, and politicians who share similar political interests will be clustered together. This methodological framework will allow researchers to identify political networks that are prevalent in various policy dimensions. The open-source software that implements the proposed methodology and the entire database with its web-interface will be made publicly available."
"1715202","III: Small: Novel Statistical Data Analysis Approaches for Mining Human Genetics Datasets","IIS","Info Integration & Informatics","09/01/2017","07/27/2017","Petros Drineas","IN","Purdue University","Standard Grant","Sylvia Spengler","08/31/2021","$499,984.00","Peristera Paschou","pdrineas@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7364","7364, 7923","$0.00","The advent of modern genotyping and sequencing technologies has revolutionized human genetics research, allowing researchers to truly understand how different we are from one another. Large datasets describing the common patterns of human genetic variation may be easily thought of as matrices, with the rows representing individuals and the columns representing loci in the genome that correspond to common polymorphisms. The broader impact of such datasets cannot be overemphasized: they are a key resource for researchers to use to find genes affecting health, disease, and responses to drugs and environmental factors, as well as understanding the evolutionary and biological history of our species. Extracting useful information from such datasets promotes the progress of science and, at the same time, advances national health, prosperity and welfare. This project will bridge the gap between state-of-the-art algorithms for data analysis developed in the theoretical computer science and applied mathematics communities and the application of such algorithms to the analysis of the increasingly larger volume of datasets in the human genetics community.<br/><br/>In the context of this project, first, from an algorithmic perspective, the project team will design and analyze novel algorithms for three prototypical, fundamental research topics that combine linear algebra and randomization, namely sparse Principal Components Analysis, matrix completion, and linear (or kernel) discriminant analysis. All three topics have been widely popular in the theoretical computer science, machine learning, and applied mathematics communities. Yet these research topics have been essentially overlooked by the population genetics community. Second, from a population genetics perspective, the team will apply the developed algorithms to gain novel insights regarding population structure, ancestry informative markers, and natural selection, as well as improve imputation methods and Genome-Wide Association Studies (GWAS) data analysis. All three methods will be evaluated on population genetics datasets that are available to the PIs. The project will train graduate students and will disseminate the results of the research to a broad community of applied mathematicians, theoretical computer scientists, and population geneticists."
"1654589","CAREER:  Nonparametric function estimation: shape constraints, adaptation, inference and beyond","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","06/10/2020","Adityanand Guntuboyina","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","06/30/2022","$333,305.00","","aditya@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 8048","1045","$0.00","Nonparametric statistics is an area of statistics and machine learning that allows one to model and analyze datasets without making strong prior assumptions about the data. Data problems where the techniques of nonparametric statistics are useful come from a wide variety of applied areas including biology, medicine, astronomy, engineering, economics and operations research. In modern complex and large datasets, these methods are especially crucial as they enable the detection of important trends and patterns in the data that may be missed by traditional parametric statistical techniques. However there exist many unresolved issues concerning the theory, methodology and application of nonparametric methods in modern data problems. A systematic study of these issues will be undertaken in this project which will result in (a) an improved understanding (in terms of accuracy and uncertainty quantification) of many existing methods, and (b) novel methods and computational algorithms that will be useful to applied practitioners in the scientific areas mentioned above. Most of the proposed projects are collaborative and involve researchers from a diverse set of universities. The project also contains a well-developed plan of educational activities which will have a major impact on the education and training of undergraduate and graduate students at UC Berkeley in statistical research. In particular, many of the educational activities of the project are aimed towards undergraduate students, a group that is often given less importance at large research universities.<br/><br/>Concretely, a wide range of nonparametric models will be studied, covering both regression and density estimation. In situations where empirically attractive estimators exist, an elaborate theoretical study is proposed focusing on their adaptive risk properties. In other situations, estimators and efficient computational algorithms are proposed together with an analysis of their accuracy. Important practical problems of inference and uncertainty quantification are also addressed. The specific regression problems that are investigated in this project include (a) multivariate convex regression, univariate trend filtering and additive shape constrained regression where adaptive risk properties of the natural estimators will be established, (b) multivariate trend filtering and quasi-convex regression where new estimators are provided along with efficient computational algorithms, and (c) global and pointwise inference in shape constrained estimation where uncertainty quantification will be addressed. In density estimation, the problems investigated include: (a) log-concave and mixture density estimation where maximum likelihood estimators will be studied, (b) distributionally robust optimization and nongaussian component analysis where novel methodology will be proposed based on shape-constrained density estimation, and (c) robust approaches to shape-constrained inference where new procedures will be developed."
"1651838","CAREER:Matrix Products: Algorithms and Applications","CCF","Special Projects - CCF, Algorithmic Foundations","03/15/2017","06/16/2020","Virginia Williams","MA","Massachusetts Institute of Technology","Continuing Grant","Tracy Kimbrel","02/28/2022","$332,211.00","","virgi@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","2878, 7796","1045, 7926, 9251","$0.00","Methods for multiplying matrices are routinely used to approach computational problems from a huge variety of applications: finding good routes in networks, pattern detection in networks, simulating motion in computer graphics and animation, protein and RNA structure prediction in biochemistry, questions in quantum mechanics, machine learning, electronics, scientific computing, and anywhere linear systems of equations need to be solved. The ability to multiply large matrices faster would have tangible impact on the world. For the past fifty years, computer scientists have been developing a rich mathematical theory of matrix multiplication algorithms;  still, it is not clear exactly how fast matrices can be multiplied, nor what the best algorithms would even look like.  The main goal of the PI is to deepen and extend the theory of matrix multiplication, and to search for faster algorithms for the problem.<br/> <br/>The most studied version of matrix multiplication is when the matrix entries come from an underlying ring such as the integers (Z), and the ""plus"" and ""times"" operations are addition and multiplication over Z. The algorithmic progress on ring matrix multiplication is a prime example of algorithmic ingenuity. For decades the trivial approach was deemed optimal until deep theory led to significant and surprising improvements. The theoretical study of ring matrix multiplication algorithms aims to pinpoint the exponent ""omega"" of matrix multiplication, considered to be the main measure of progress on the problem. The number omega is the smallest real number for which there is an algorithm that multiplies two square matrices of dimension n using n^(omega+o(1)) operations (additions and multiplications of numbers). Since the output has size n^2, omega is at least 2; the most recent bound omega < 2.373 was obtained by the PI. The PI aims to investigate new approaches to improving the bound on omega and related parameters, with a long-term goal of designing a fast and practical algorithm.<br/> <br/>The impressive improvements above only apply to ring matrix multiplication. However, in many applications, different, potentially more complex matrix products are needed. For instance, in computing shortest paths in a network, one relies on the so called distance product of real matrices for which the ""plus"" operation is minimum and the ""times"" operation is addition. The matrix product is no longer over a ring, but rather over a semiring. Non-ring matrix products are not as well understood as ring matrix multiplication; some, such as the distance product, don't even seem to admit much faster algorithms than the brute-force algorithm that follows from their definition. The second major goal of the PI is to study a large variety of non-ring matrix products, develop algorithms for them, and broaden and strengthen their applications.<br/> <br/>This project has several educational goals. These include mentoring undergraduate and graduate students, the development of new courses directly related to the described topics, and incorporating these topics into existing core algorithms courses. The lectures and project materials will be available on the course website for the general public. The PI is wholeheartedly committed to diversity. The PI has experience in recruiting and mentoring both undergraduate and graduate minority students, and will continue to take an active role in seeking and recruiting students from diverse cultures and backgrounds."
"1661386","ABI Development: Collaborative Research: The first open access digital archive for high fidelity 3D data on morphological phenomes","DBI","IIBR: Infrastructure Innovatio, ADVANCES IN BIO INFORMATICS","09/01/2017","09/05/2019","Douglas Boyer","NC","Duke University","Continuing Grant","Peter McCartney","08/31/2021","$1,410,460.00","Gregg Gunnell, Sayan Mukherjee, Timothy McGeary","doug.boyer@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","BIO","084Y, 1165","1165","$0.00","People and societies thrive best when they understand how the social and physical dynamics of their environment work, allowing them to respond appropriately.  Natural scientists have built our understanding of the physical world. The scientific understanding they built has contributed to the development of technologies and practices that benefit human economies. For example, genetic sequencing of DNA enables deeper understanding of biological organisms; the consequences for human health, food production, understanding of evolutionary adaptation, etc. have been revolutionary and are still unfolding. The DNA sequence is the blueprint for an organism's anatomical structure (morphology) and function, but images capturing morphology are now much less prevalent than genetic data. Museums and researchers have been creating 3D digital images of natural history collections, and there are extensive 3D image data sets for some model organisms, but these data are mostly in closed collections, and generally unavailable or very difficult to access. This project aims to provide infrastructure to increase the accessibility of anatomical information, with a focus on 3D images. The resource will create the first open access, web-enabled image archive accepting and serving high-resolution, 3D scans of all organisms, called MorphoSource. Standardized descriptive tags will allow scientists to use this database to easily combine genetic and anatomical datasets for the first time, supporting the formulation of novel research questions. MorphoSource will link to other databases (such as iDigBio [www.idigbio.org]) that aggregate information on museum specimens from around the world. Having a shared common resource will change the culture among researchers and museums, making collaborations between physically distant experts more feasible, but it will also open the linked research collections of museums to anyone with Internet access anywhere in the world.  Large data sets are prerequisites for many statistical and machine learning methods, so the resource will enable innovations in computational image analysis methods, fostering new types of collaborations that advance field-wide scientific understanding. The resource will track data use, enhancing reproducibility and also providing an objective metric of the value of individual data elements. Open access to the data linked through MorphoSource will enable anyone with Internet access to see the detailed anatomical evidence for theories like evolution.  Pilot work has shown that teachers and students eagerly consume this newly available information, with numbers already in the thousands.  Positive results of this access include (1) providing a more intuitive type of raw data (compared to DNA sequence) for showing the public why some conclusions about evolutionary relationships were reached, (2) providing an 'interest metric' for the value of natural history museums and the collections they hold, (3) increasing the community of people (including citizen scientists) who have access to the data required to make important discoveries by studying biological variation. <br/><br/>The specific plan for creating the repository for 3D data on all organisms is as follows. The primary goal is to restructure and improve a proof of concept database called MorphoSource. The restructuring will allow MorphoSource to meet the needs of a growing community of researchers and educators through massive upscaling, and to implement a novel approach for economically preserving data for the long term. To accomplish this, the MorphoSource server will be rebuilt to use the Fedora digital asset management architecture, which has been developed by library scientists to serve emerging needs related to the archiving and sharing of digital data. As part of this architecture upgrade, the data hosted on MorphoSource will be given an additional layer of protection through managing asynchronous copies in DuraCloud, a digital data preservation platform that leverages Amazon cloud. This restructuring will allow the MorphoSource server architecture to be integrated with the Duke University Libraries repository infrastructure. MorphoSource will also be able to invite institutional communities to be consortium partners in support of data storage and to enact data preservation techniques that guarantee integrity and readability for the foreseeable future. Additional tools will (1) allow for rapid, automated ingestion of dozens to hundreds of datasets at once, (2) link MorphoSource with major biodiversity archives, and (3) provide in-browser visualization of 3D series of image slices, such as those generated by CT and MRI scanners.  The plan includes ingesting thousands of high quality legacy CT datasets from published studies, enabling their reuse, increasing the repeatability of studies. The project leaders plan to directly work with and design tools for K-12 educators and students to help them benefit from this resource. These datasets and educational tools will be available to researchers and the public through the updated MorphoSource website, available at www.morphosource.org."
"1741022","BIGDATA: F: Collaborative Research: Foundations of Responsible Data Management","IIS","Big Data Science &Engineering","09/01/2017","08/21/2017","H. Jagadish","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sylvia Spengler","08/31/2021","$385,000.00","","jag@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8083","7433, 8083","$0.00","Big Data technology promises to improve people's lives, accelerate scientific discovery and innovation, and bring about positive societal change. Yet, if not used responsibly, this same technology can reinforce inequity, limit accountability and infringe on the privacy of individuals: irreproducible results can influence global economic policy; algorithmic changes in search engines can sway elections and incite violence; models based on biased data can legitimize and amplify discrimination in the criminal justice system; algorithmic hiring practices can silently reinforce diversity issues and potentially violate the law; privacy and security violations can erode the trust of users and expose companies to legal and financial consequences. The focus of this project is on using Big Data technology responsibly -- in accordance with ethical and moral norms, and legal and policy considerations. This project establishes a foundational new role for data management technology, in which managing the responsible use of data across the lifecycle becomes a core system requirement. The broader goal of this project is to help usher in a new phase of data science, in which the technology considers not only the accuracy of the model but also ensures that the data on which it depends respect the relevant laws, societal norms, and impacts on humans. <br/><br/>This project defines properties of responsible data management, which include fairness (and the related concepts of representativeness and diversity), transparency (and accountability), and data protection. It complements what is done in the data mining and machine learning communities, where the focus is on analyzing fairness, accountability and transparency of the final step in the data analysis lifecycle, and considers the problems that can be introduced upstream from data analysis: during dataset selection, cleaning, pre-processing, integration, and sharing. This project develops conceptual frameworks and algorithmic techniques that support fairness, transparency and data protection properties through all stages of the data usage lifecycle: beginning with data discovery and acquisition, through cleaning, integration, querying, and ultimately analysis. The contributions are structured along three aims. Aim 1 considers responsible dataset discovery, profiling, and integration. Aim 2 considers responsible query processing and develops a general framework for declarative specification, checking and enforcement of fairness, representativeness and diversity. Aim 3 incorporates data protection into the lifecycle, develops techniques to facilitate sharing of sensitive data, and considers the tradeoffs between privacy and transparency. This project is poised to establish a multidisciplinary research agenda around responsible data management as a critical factor in enabling fairness, accountability and transparency in decision-making and prediction systems. Additional information about the project is available at DataResponsibly.com."
"1704790","SHF: Medium: Collaborative Research: Testing in the Era of Approximation","CCF","Software & Hardware Foundation","09/01/2017","08/31/2017","Milos Gligoric","TX","University of Texas at Austin","Standard Grant","Sol Greenspan","08/31/2021","$450,000.00","Sarfraz Khurshid","gligoric@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7924, 7944","$0.00","Many computations, such as image processing, machine learning, and engineering simulations are inherently approximate -- they trade off quality of results for better performance. However, approximation also introduces new challenges when reasoning about program behaviors and finding bugs. At present, testing in this area requires more principled and effective approaches. Simultaneously, approximation itself provides an effective new basis for innovations in the well-trodden field of testing, thereby making testing more efficient and valuable. The project will develop a bi-directional integration of testing and automated approximation, new approach for developing and optimizing an increasingly important class of programs. The results will be embodied in open source tool sets and rigorously evaluated using open-source and proprietary applications. New educational and course materials will be developed for courses on compilers, program analysis and software engineering.<br/><br/>More concretely, the project will develop a set of techniques and tools for testing approximate programs, including a test specification language and techniques for automated migration of existing tests to the new language, techniques for dynamic approximate-program analysis, and techniques for optimal approximation discovery.  Moreover, the project will develop approximate computing techniques to improve the performance of regression testing and mutation testing."
"1745382","EAGER:   SSDIM:   Leveraging Point Processes and Mean Field Games Theory for Simulating Data on Interdependent Critical Infrastructures","CMMI","Special Initiatives, ","09/01/2017","11/29/2018","Hongyuan Zha","GA","Georgia Tech Research Corporation","Standard Grant","Walter Peacock","08/31/2020","$200,000.00","Xiaojing Ye","zha@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1642, Q231","036E, 041E, 042E, 1057, 7916","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) project addresses modeling and inference problems in order to improve understanding of  interactions and interdependencies within interdependent critical infrastructures (ICIs). The key application areas are financial services, healthcare systems, communication technologies. This work will result in novel machine learning methodologies to generate data on infrastructure interdependencies. Findings will be widely disseminated in scholarly fora, with accompanying efforts in graduate-level training. Data and computer software produced in this project will be made publicly available via online data repositories. <br/><br/>This project includes the development of new generative models and algorithms to simulate and synthesize extensive interdependent CI data for comprehensive study. This research focuses on modeling and simulation of interdependent critical infrastructure (ICI) data by leveraging point process models and mean field games (MFG) theory. In particular, multivariate Hawkes processes are used to model interactions and interdependencies of behaviors in a variety of domains. Additionally, an MFG framework is employed to capture the implicit optimization strategies that individuals perform, along with the cost functions that drive those strategies. This work addresses both mechanistic and human aspects of the ICIs, captured in point process models and their evolution. This work  advances the theory and computational methods for generative methods and algorithms for quantitative understanding and rigorous analysis of ICIs."
"1717854","SHF: Small: Collaborative Research: Automated Numerical Solver EnviRonment (ANSER)","CCF","Software & Hardware Foundation","08/15/2017","08/04/2017","Elizabeth Jessup","CO","University of Colorado at Boulder","Standard Grant","Almadena Chtchelkanova","07/31/2020","$225,000.00","","jessup@cs.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7798","7923, 7942, 9102","$0.00","The computational science community is tackling ever larger and more complex applications. The solution of the underlying mathematics problems requires using high-end parallel computing resources effectively, and delivering performance without degrading productivity is critical for the success of scientific computing. Converting mathematics from algorithms to high-quality implementations, however, is a difficult process, whether an application is developed from scratch or by leveraging existing software libraries. Modern numerical packages provide numerous solutions with widely varying performance. Selecting among these possibilities requires expertise in numerical computation, mathematical software, compilers, and computer architecture, but even such broad knowledge does not guarantee the selection of the best-performing method for a particular problem. In response to these challenges, ANSER (Automated Numerical Solver EnviRonment) automates the selection and configuration of algorithms such as sparse linear solvers, eigensolvers, and graph methods in the context of large-scale scientific and engineering applications. The overall approach is generalizable to any situation involving multiple solutions whose performance varies with input problem properties.  ANSER increases developer productivity and promotes effective use of modern parallel architectures to solve large-scale scientific and engineering problems. This work also impacts the training of the next-generation scientific workforce by involving graduate and undergraduate students in this model-guided development of high-performance software. <br/> <br/>ANSER, the Automated Numerical Solver EnviRonment, is an open-source web-based platform that supports the development of both scientific applications and high-performance libraries.  It selects, configures and, in some cases, generates implementations of high-performance numerical algorithms.  ANSER defines a methodology for automating the process of identifying problem features, creating performance models (based on combining analytical and machine learning approaches), and employing them in creating and configuring numerical software. ANSER initially targets widely used numerical packages for nonlinear partial differential equations and solution of eigenvalue problems, but it is designed to be extensible to other types of numerical methods, such as graph computations and n-body simulations. In addition to traditional dissemination methods (open-source software releases and publications), ANSER integrates semantic analysis of scientific computing literature to discover numerical methods similar to those provided by the target libraries and to identify and connect with our users. ANSER provides multiple interfaces to support different types of users, including students, computational scientists, and numerical library developers."
"1719205","CIF:Small:Collaborative Research:Distributed Fog Computing for Non-Convex Big-Data Analytics","CCF","Comm & Information Foundations","09/01/2017","06/28/2017","Gesualdo Scutari","IN","Purdue University","Standard Grant","Phillip Regalia","08/31/2020","$270,000.00","","gscutari@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7935","$0.00","In our data-deluge era, massive chunks of information, perpetually collected by pervasive sensors, are communicated and processed by distributed computational architectures. To address emergent big-data computational issues, this project embarks on an ambitious multidisciplinary research effort that aims at advancing the state-of-the-art in-network/distributed big-data processing via a general algorithmic framework for data analytics over massively distributed data sets. The proposed algorithmic framework enables fully distributed and parallel big-data analytics, for a variety of heterogeneous data sets over a wide range of computational architectures. The developed research directions are beneficial also to domains far beyond big-data analytics, such as signal processing, machine learning, next-generation wireless communications, smart-city and smart-grid networks. Research results are distributed through archival publications, courses, undergraduate research opportunities, tutorials and conference presentations.<br/><br/>The developed scheme relies on a novel convexification/decomposition technique which accommodates a rich class of non-convex, unstructured and stochastic optimization tasks with non-separable objective functions. Algorithms are designed for settings where data are distributed across a large number of multi-core computational nodes, within a network of arbitrary topology with (possibly) time-varying and even random links. This new class of algorithms addresses shortcomings of current (non-parallel and non-distributed) convexification techniques via (i) full control of the degree of parallelism and distribution of the computation/signaling among processors/network nodes, and (ii) by offering a plethora of convex approximants, regularization terms, step-size rules, and communication protocols. Designed for time-varying or even random network topologies, the advocated framework demonstrates also another desirable attribute for distributed computations: resiliency to (random) network failures."
"1718796","CIF:Small:Collaborative Research:Distributed Fog Computing for Non-Convex Big-Data Analytics","CCF","Comm & Information Foundations","09/01/2017","06/28/2017","Konstantinos Slavakis","NY","SUNY at Buffalo","Standard Grant","Phillip Regalia","08/31/2020","$200,000.00","","kslavaki@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7797","7923, 7935","$0.00","In our data-deluge era, massive chunks of information, perpetually collected by pervasive sensors, are communicated and processed by distributed computational architectures. To address emergent big-data computational issues, this project embarks on an ambitious multidisciplinary research effort that aims at advancing the state-of-the-art in-network/distributed big-data processing via a general algorithmic framework for data analytics over massively distributed data sets. The proposed algorithmic framework enables fully distributed and parallel big-data analytics, for a variety of heterogeneous data sets over a wide range of computational architectures. The developed research directions are beneficial also to domains far beyond big-data analytics, such as signal processing, machine learning, next-generation wireless communications, smart-city and smart-grid networks. Research results are distributed through archival publications, courses, undergraduate research opportunities, tutorials and conference presentations.<br/><br/>The developed scheme relies on a novel convexification/decomposition technique which accommodates a rich class of non-convex, unstructured and stochastic optimization tasks with non-separable objective functions. Algorithms are designed for settings where data are distributed across a large number of multi-core computational nodes, within a network of arbitrary topology with (possibly) time-varying and even random links. This new class of algorithms addresses shortcomings of current (non-parallel and non-distributed) convexification techniques via (i) full control of the degree of parallelism and distribution of the computation/signaling among processors/network nodes, and (ii) by offering a plethora of convex approximants, regularization terms, step-size rules, and communication protocols. Designed for time-varying or even random network topologies, the advocated framework demonstrates also another desirable attribute for distributed computations: resiliency to (random) network failures."
"1657162","CRII: CIF: Models, Theories and Algorithms for Timeliness Optimization in Information-update Systems","CCF","CRII CISE Research Initiation, Comm & Information Foundations","02/15/2017","05/08/2018","Bo Ji","PA","Temple University","Standard Grant","Phillip Regalia","01/31/2021","$188,557.00","","boji@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","026Y, 7797","7797, 7935, 8228, 9251","$0.00","The last two decades have witnessed significant advances in the development of theoretical foundations and control mechanisms for network resource allocation. These newly developed theories and mechanisms have substantially improved network performance in terms of throughput and delay. However, optimizing throughput and delay is insufficient for networked systems that require real-time information update. The state-of-the-art theoretical foundations need to be largely expanded to integrate timeliness of information into the design of network control mechanisms. The research on timeliness optimization is still at its nascent stage. New theoretical results and practical solutions coming out of this project are expected to have a significant impact not only on information theory and networking community, but also on databases and machine learning community. This project will focus on providing research experiences to undergraduate and K-12 students, recruiting and advising underrepresented students, and engaging in curriculum development activities. <br/><br/>The goal of this research is to develop new models, theories, and algorithms for optimizing timeliness performance in information-update systems. A recently proposed metric called age-of-information or simply ""age"", will be employed as a key metric to study timeliness performance. First, this research investigates the impact of channel coding on timeliness of information transmitted over a lossy channel. Second, this research studies the problem of age minimization under a bounded staleness constraint in a new setting where information can be partitioned into multiple disjoint units with partial updates. Finally, this research introduces a new Pull model where the destination sends queries to the sources to pull information of interest and proposes using replication schemes to optimize timeliness performance."
"1652257","CAREER:   New Methods for Central Streaming Problems","CCF","Algorithmic Foundations","02/01/2017","02/11/2020","Vladimir Braverman","MD","Johns Hopkins University","Continuing Grant","A. Funda Ergun","01/31/2022","$435,758.00","","vova@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7796","1045, 7926","$0.00","The streaming model is a powerful model of computation that has made a significant impact on computer science over the past decade. Recent developments demonstrate the critical need for streaming methods in numerous applications such as networking, machine learning, astronomy and statistical inference. The project will develop new streaming and sketching algorithms that will be applicable in the aforementioned areas. The project will support undergraduate research and engage students in working on cutting-edge theoretical problems. The project will promote STEM education by collaborating with Independence School Local 1 (IHS), a public charter high school in Baltimore city, where minority students constitute about 60 percent of the student body. This project will help to organize (I) a workshop for first generation students and (II) an annual Sublinear Algorithms Workshop at Johns Hopkins University. The project will promote core education and will introduce new advanced courses and seminars that will convey the principles of algorithms to non-theory students.<br/><br/>In 1996, Alon, Matias and Szegedy published a fundamental paper on streaming algorithms. The paper introduced the problem of approximating frequency moments in the streaming model and asked the open question, ?What other frequency-based functions can be approximated on streams?? Since 1996 the research on data streams has resulted in great progress. Despite this progress, our understanding of many fundamental streaming problems is far from being complete. The main technical objective of this project is to develop new algorithms that will resolve central problems and overcome existing barriers of streaming methods. The specific goals are the following: (1) Answer the main open question of Alon, Matias and Szegedy and obtain a zero-one law for all frequency-based functions. (2) Discover the relation between the sliding window model and the unbounded model. Extend this knowledge to the decay and distributed models. (3) Design new sampling methods for data streams. Extend the sampling methods for the sliding window model to decay models, improve the weighted and distributed sampling."
"1737918","ATD:  Collaborative Research:  Multivariate Quantiles for Rapid Spatio-Temporal Threat Detection","DMS","ATD-Algorithms for Threat Dete","09/01/2017","08/25/2017","Singdhansu Chatterjee","MN","University of Minnesota-Twin Cities","Standard Grant","Leland Jameson","08/31/2020","$100,000.00","","chatt019@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","046Y","6877","$0.00","Different kinds of data on societal attributes, observed from multiple sources, at multiple locations, and at different points in time, will be studied in this project. The geometrical properties of such data will be analyzed to quantify and characterize normal patterns in the data, which will then be leveraged to identify sudden departures from normal patterns within societies. Methodology for understanding normal patterns in the data and rapidly detecting change in one or more aspects of the data will be devised in this project. Data from different locations around the world will be analyzed and used to formulate strategies for risk mitigation and emergency responses.<br/><br/>The geometric properties of high-dimensional spatio-temporal data will be studied in this project to construct a multi-dimensional extremity indicator. This indicator and other statistical and machine learning techniques will be used for rapid spatio-temporal change detection, under a variety of technical conditions and frameworks. Such changes may be towards specific known directions, or generic departures from normal patterns. Methods for detecting changes in extremes and tails of multivariate probability distributions will likewise be developed as part of this project. Social, economic, and supply chain logistics data will then be studied to develop policy and rapid response strategies using data-driven techniques."
"1740802","I-Corps: Internet of Things for Condition-Based Maintenance of Medical Assets","IIP","I-Corps","06/01/2017","04/20/2017","Ekundayo Shittu","DC","George Washington University","Standard Grant","Nancy Kamei","03/31/2019","$50,000.00","","eshittu@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to improve safety and minimize equipment maintenance costs. In healthcare, equipment malfunctions are a major patient safety issue because of direct patient care impact, and also due to equipment maintenance costs in general. The technology developed here monitors the health of biomedical equipment asset with the purpose of predicting, in real-time, biomedical equipment asset failure in order to reduce the rate of equipment failure, thereby, improving safety and reducing costs. This project will equip healthcare and military facilities with a tool to better manage their assets, eradicate equipment downtime, reduce maintenance cost and eliminate unanticipated failures. The added benefit of the commercial potential of this project comes from the opportunity for Original Equipment Manufacturers (OEMs) of equipment assets to improve their technologies with the condition-based maintenance strategy that this innovation offers. <br/><br/>This I-Corps project seeks to develop a low-cost, power-efficient, wireless, mesh-networked family of microprocessors, sensors and software designed to give real-time reporting on equipment status and offer condition-based maintenance schedules. The instrumentation innovation is the combination of different sensors to capture periodic data on pressure, temperature, humidity, position, current and vibration on an equipment asset for condition-based maintenance of the asset. This project will be based on the Internet of Things (IoT) in the connection and management of different assets in a healthcare facility or military materiel supply chain. This project will develop a template for the integration of mesh radio frequency identification (RFID) of emitted but encrypted radio frequency (RF) signals in conformance to IEEE 802.15.4 standards over the 2.4 GHz industrial, scientific, and medical (ISM) radio band. The device will be used in conjunction with the cloud where data will be processed in a wireless configuration. Coupled with the development of this module is a dynamic data analytic procedure that incorporates novel machine learning statistical routines for maintenance forecasts."
"1660071","SBIR Phase II:  SSD In-Situ Processing","IIP","SBIR Phase II","03/15/2017","08/27/2018","Vladimir Alves","CA","NGD Systems, Inc.","Standard Grant","Peter Atherton","02/29/2020","$1,398,973.00","","vladimir.alves@ngdsystems.com","7545 Irvine Center Drive","Irvine","CA","926182932","9495106327","ENG","5373","165E, 169E, 5373, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be to fundamentally change what a storage device can do, and give storage a third capability that is not addressed by existing storage technology - the ability to actually process user data.  For the computation to take place, only the computational request and the resulting data need to transfer over the storage interface, reducing interface traffic and the required power. The advent of Big Data and the increasing use of Hyperscale Server technology have resulted in the creation of an additional storage tier that is different from traditional enterprise storage. This new tier requires significantly larger capacity yet lower cost, lower operating power, and yet must still exhibit enterprise level reliability. This combination of characteristics cannot be serviced by existing technologies, and execution with large data sets typical of Big Data results in inefficient solutions. The information being stored represents the large, unstructured data mined by today's companies for key information and trends that help dictate corporate direction, advertising, and monetization. Future applications include machine learning for video analytics, genome sequencing and enabling Fog Storage and Fog Computing, among others.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project explores the Big Data paradigm shift where processing capability is pushed as close to the data as possible. The In-Situ processing technology pushes this concept to the absolute limit, by putting the computational capability directly into the storage itself and eliminating the need to move data to main memory before processing. The technology innovation begins with a solid foundation of an enterprise SSD tailored for the needs of modern Data Centers. Key technology that will be added to support these capabilities include hardware-assisted quality of service control, low-cost 3D-TLC and QLC NAND Flash enablement through the use of advanced ECC, and a proprietary elastic Flash Translation Layer to support extremely large capacity drives. The final element added to this foundation will be the ability to perform computation directly on the data with the addition of specialized In-Situ processing aided by hardware accelerators."
"1812380","SBE: Small: An Analysis of the Relationship Between Cyberaggression and Self-Disclosure among Diverse Youths","SES","Secure &Trustworthy Cyberspace","09/28/2017","02/13/2018","April Edwards","IL","Elmhurst College","Standard Grant","Sara Kiesler","08/31/2019","$191,849.00","","e2unlimitedtech@gmail.com","190 Prospect","Elmhurst","IL","601263296","6306176470","SBE","8060","7923, 8239, 9102, 9178, 9229, 9251, SMET","$0.00","Youths of the digital age live parallel lives online and in the real world, frequently disclosing personal information to cyberfriends and strangers, regardless of race, class or gender. Race and gender do make a difference, however, when these online disclosures lead to acts of cyberaggression. The PIs' previous work revealed that some youths are resistant to cyberaggression and that there are differences in perceptions of cyberbullying among youths from different cultural and racial backgrounds.  This research aims to explore the relationship between youths' self-disclosures, cultural backgrounds, and their perceptions of cyberaggression.<br/><br/>The PIs conduct a longitudinal, interdisciplinary study that builds upon their ongoing cyberaggression pattern recognition research by: 1) using surveys and focus groups to test and refine their theories about self-disclosure, perception, cultural difference, and cyberaggression communication patterns, 2) using machine learning to develop detection and response technologies for use in applications designed to protect youths, 3) using focus groups to evaluate the applications, and 4) making the data collected from this project available to the research community. This work is important to understand the role of self-disclosure in cybervictmization among youths, and provides the theoretical groundwork for the development of effective response strategies that can be employed by youths when they are attacked online. The data from this study will provide a rich source of material for other researchers in both computer science and in the social and behavior sciences."
"1724263","CRCNS Research Proposal: Collaborative Research: Data-driven approaches for restoring naturalistic motor functions using functional neural stimulation","IIS","Special Initiatives, CRCNS-Computation Neuroscience, Robust Intelligence","09/01/2017","04/17/2020","V John Mathews","OR","Oregon State University","Standard Grant","Kenneth Whang","08/31/2021","$421,057.00","","mathews@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","1642, 7327, 7495","7327, 8089, 9251","$0.00","Individuals with lower limb paralysis retain the ability to plan and initiate gait within the central nervous system but, due to conditions such as spinal cord injury or stroke, are unable to transmit those directives to the muscles of the lower limb. This research project uses a biologically-inspired, data-driven approach to address this deficiency. Specifically, the research team is developing and evaluating machine learning methods and electrical stimulation of nerves to control movements of the lower limbs. The goal is to develop and evaluate methods to restore natural, coordinated, and graceful gait in an animal model of paralysis. This activity is a first step toward providing benefit to the paralyzed community by creating pathways toward the development and commercialization of functional gait restoration systems that evoke more natural, controlled movement of paralyzed limbs. In addition, the project offers unique opportunities to train engineering students in the performance of pre-clinical studies, placing these future researchers at the forefront of engineering technology and medical research. <br/><br/>This research project aims to develop and evaluate methods to restore natural, coordinated, and graceful gait in an animal model of paralysis using a synergistic collaboration of a multi-disciplinary and multi-university team of investigators and a combination of innovative modeling and algorithm development supported by a series of experiments. Specifically, the project aims to achieve the following sub-goals involving application of data-driven algorithms in a series of experiments that are designed to evoke increasingly complex movements over the course of the proposed work: (a) Develop, characterize, and evaluate advanced controllers of joint angle and joint torque production of a single joint in only a single direction, to allow comparison of the data-driven model to earlier, classical controls methods; (b) Develop, characterize, and evaluate advanced controllers of joint angle and joint torque production of a single joint in both directions, to elucidate methods used by the advanced controller's solution to the under-constrained problem of agonist-antagonist muscle pair control; (c) Develop, characterize, and evaluate advanced controllers of joint angle and joint torque production of multiple joints in both directions, to elucidate methods used by the advanced controller's solution to the competing-goals problem of biarticular muscle control; and, (d) Recreate natural, coordinated, and graceful gait by use of the advanced controllers arising from the first three goals, and demonstrate this result on a treadmill platform."
"1717997","III: Small: Improving Technical Paper Database Search through Math-Aware Search Engines","IIS","Info Integration & Informatics","12/01/2017","12/01/2017","Richard Zanibbi","NY","Rochester Institute of Tech","Standard Grant","Wei-Shinn Ku","11/30/2020","$498,928.00","Anurag Agarwal","rlaz@cs.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7364","7364, 7923","$0.00","Today's search engines make use of sophisticated techniques for searching based upon words, but are not able to make nuanced use of mathematical notation.  This project aims to allow scientists, engineers, mathematicians, and students to locate technical information using words, mathematical notation, or some of each.  For example, a mathematician studying graph theory could use these new capabilities to find related applications in physics, ecology, and social network analysis, despite any differences in the notation and terminology used in those disciplines. Given a large collection of technical documents, we will apply machine learning techniques to construct associations between the formulae and words used to explain mathematical ideas, and determine how to translate automatically between those two forms of expression.  These associations and translations can then be used by students who write what they are looking for using words, with the search engine finding documents that express those same ideas, even if only in mathematical notation.  These new math-aware search engines will accelerate innovation by allowing searchers to discover information both across technical disciplines and, by using mathematical notation as a pivot, even across human languages.<br/><br/>To accomplish these goals, the project will develop novel scalable techniques for indexing and retrieval of mathematical content in technical documents. These methods will accommodate a broad range of notational conventions, formats, and encodings.  New context-based methods for inferring associations between formulae and related text will be used to build rich and flexible models of content equivalence.  These equivalence models will be used in new ranking algorithms that integrate results found using words or using mathematical notation into a single ranked list.  Open-source reference implementations will be shared publicly, and new test collections created to evaluate these implementations will be shared with other researchers.  To gain experience with the use of these new capabilities, the project will add math-aware search to the CiteSeerX digital library of scientific literature. CiteSeerX is an open Web service that can be used to compare alternative retrieval methods in actual use.  For further information see the project Web page: https://www.cs.rit.edu/~dprl/math-aware-search.html."
"1741607","EAGER:  Collaborative:  Leveraging High-Density Internet Peering Hubs to Mitigate Large-Scale DDoS Attacks","CNS","Secure &Trustworthy Cyberspace","08/15/2017","08/07/2017","Maria Konte","GA","Georgia Tech Research Corporation","Standard Grant","Nina Amla","07/31/2020","$120,152.00","","mkonte@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8060","025Z, 7434, 7916, 9102","$0.00","Large-scale distributed denial of service (DDoS) attacks pose an imminent threat to the availability of critical Internet-based operations, as demonstrated by recent incidents that brought down a number of highly popular web services such as Twitter, Spotify and Reddit. While several solutions to counter DDoS attacks have been proposed by both industry and academia, most of the solutions that are currently deployed on the Internet - such as traffic scrubbing - tend to detect and mitigate DDoS attacks close to the victim edge network, once the attack has already caused damage. Creating systems for early DDoS attack detection and mitigation that can be deployed at the core of the Internet has the potential to significantly improve Internet security and reliability. <br/><br/>This project investigates innovative machine learning-based DDoS attack detection and mitigation solutions that can be deployed at the core of the Internet, within Internet eXchange Points (IXPs). IXPs are high-density peering hubs that provide infrastructure used by autonomous systems (ASes) to interconnect, and are therefore well positioned to observe significant fractions of global Internet traffic. The project leverages IXP-based traffic monitoring to develop advanced traffic analysis and classification methods for efficient, automated early detection and mitigation of DDoS attacks. The researchers aim to first investigate methods for defending against distributed reflective DoS (DRDoS) attacks, which rely on spoofed IP traffic to amplify the attacker's available bandwidth, and to then expand the investigation to volumetric DDoS attacks that do not rely on spoofed traffic. As part of the project, the researchers aim to develop collaborations with IXPs and Internet operators around the world, to facilitate research on DDoS defenses and increase opportunities for high-impact technology transfer."
"1718582","III: Small: Non-Invasive Real-Time Analytics in Database Systems using Holistic Query Compilation","IIS","Info Integration & Informatics","08/01/2017","09/14/2017","Andrew Pavlo","PA","Carnegie-Mellon University","Continuing Grant","Sylvia Spengler","07/31/2020","$499,774.00","Todd Mowry","pavlo@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7364, 7923","$0.00","There are two major trends in modern data processing applications that make them distinct from applications in previous decades. The first is that they are noted for their continuously changing data sets. This could come from transactions updating the database or from upstream sources. The second is that they want to analyze the latest obtained data as quickly as possible. Data has immense value as soon as it is created, but that value diminishes over time. Therefore, it is imperative that the queries access the newest data generated in order for their results to have the most impact. The ability to ask complex questions about data as soon as it enters in the database is useful in many application domains, including real-time monitoring systems (e.g., is an incoming packet from a potential attacker?) and financial services (e.g., is this new credit card purchase fraudulent?). But current systems contain architecture remnants of legacy database management systems (DBMSs) that prevent them from taking advantage of newer hardware support for parallel optimizations. This limits the types of queries that an application executes on a DBMS that targets data as soon as it arrives. In turn, this adds additional cost to deploying a database application in terms of both hardware and administration overhead. Thus, the goal of this project is to investigate using query compilation to allow non-invasive analytical operations that are more complex than what is practical in today's DBMSs. Such query compilation techniques are beneficial to a wide array of data processing systems. The results of this study will allow organizations to deploy DBMSs that are able to handle applications with larger data sets and more complex workloads with fewer resources (e.g., hardware, personnel, energy).<br/><br/>Modern data-intensive applications seek to obtain new insights in real-time by analyzing a combination of historical data sets alongside recently collected data. To support such workloads, database management systems (DBMSs) need to support complex analytical queries over diverse data sets. The ever decreasing cost of DRAM is allowing a greater number of these applications to be memory-resident. As such, in-memory DBMSs will be used for most analytical and machine learning applications in the future. But there are remnants of how legacy disk-oriented DBMSs process queries that still exist in newer in-memory DBMSs that inhibit the kind of high-performance query execution over large data sets that this project targets. Thus, the goal of this project is to overcome this barrier through a new holistic approach to query compilation that integrates it comprehensively throughout the DBMS, and which builds upon (and adapts) recent advances in ""just-in-time"" (JIT) compilation technology and heterogeneous hardware resources. Using compilation to optimize many different aspects of the DBMS's architecture is important to support future ""Big Data"" applications that need to ingest large amounts of new data while simultaneously executing complex analytical workloads in near real-time."
"1718160","CSR: SMALL: Virtualized Accelerators for Scalable, Composable Architectures","CNS","CSR-Computer Systems Research","10/01/2017","07/14/2017","David Brooks","MA","Harvard University","Standard Grant","Matt Mutka","09/30/2020","$450,000.00","Gu-Yeon Wei, Ryan Adams","dbrooks@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7354","7923","$0.00","This project seeks to develop fundamental technologies to enable the next-generation of computing devices that will power future ubiquitous computing devices such as smartphones, self-driving cars, and autonomous robots. The project develops novel tools and techniques at both the hardware and software layers of computer systems. This project will also train new graduate engineers in architecting complex computing systems, modern software and hardware design methodologies, and cutting edge machine learning techniques. All of these skillsets are in broad demand in US industry but have been underrepresented in STEM education.<br/><br/>Heterogeneous architectures comprising general purpose processors, graphics processors, and hardware accelerators designed for specific computing tasks have been widely adopted in today's computing systems for both edge and cloud devices. Specialized computing blocks provide tremendous benefits in energy efficiency. However, a major challenge in the design of such systems is the loss of generality and flexibility that has limited their adoption to a small set of application domains that do not often change. Increased flexibility could be unlocked if accelerators were built from smaller dynamically composable blocks, but existing approaches are difficult to program and scale poorly. This project proposes a design flow to generate a templated System-on-Chip (Soc) with a composable accelerator system that can be physically instantiated for a range of computing platforms. Through a virtualization layer, collections of physical hardware blocks are exposed to software as virtual accelerators. To efficiently search the large design space of the SoC, new design space exploration techniques are under investigation."
"1702760","SaTC: CORE: Medium: Developing for Differential Privacy with Formal Methods and Counterexamples","CNS","Secure &Trustworthy Cyberspace","07/01/2017","05/31/2017","Daniel Kifer","PA","Pennsylvania State Univ University Park","Standard Grant","Sol Greenspan","06/30/2021","$1,200,000.00","Danfeng Zhang","duk17@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8060","025Z, 7434, 7924","$0.00","Differential privacy is an elegant formulation of conditions an algorithm must meet in order to avoid leaking personal information contained in its inputs. It is becoming mainstream in many research communities and has been deployed in practice in the private sector and some government agencies. Accompanying this growth is an unfortunate, but expected, difficulty: many algorithms that are claimed to be differentially private are incorrect. This phenomenon affects both new-comers and seasoned veterans of the differential privacy field because of the difficulty and subtlety of developing new differentially private algorithms.<br/><br/>This proposal outlines a research plan for the development of a system called DevDP, whose purpose is to enable novice and expert users to develop prototypes and explore differentially private algorithms. In particular, the project will develop program analysis tools and theory that leverage both programming language and machine learning technology to aid the development of correct differentially private programs by automating much of the verification and reasoning about errors. As part of broader impacts, DevDP also has the potential to help educate students and less-technical members of the scientific community by providing interactive software tools. A solid understanding of differential privacy will become crucial as it makes its way into public policy."
"1663938","PREEVENTS Track 2: Collaborative Research: Developing a Framework for Seamless Prediction of Sub-Seasonal to Seasonal Extreme Precipitation Events in the United States","ICER","PREEVENTS - Prediction of and","08/01/2017","07/20/2018","Heather Lazrus","CO","University Corporation For Atmospheric Res","Continuing Grant","Justin Lawrence","07/31/2022","$87,245.00","","hlazrus@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","GEO","034Y","","$0.00","PREEVENTS Track 2: Collaborative Research: Developing a Framework for Seamless Prediction of Sub-Seasonal to Seasonal Extreme Precipitation Events in the United States<br/><br/>Extreme precipitation is a natural hazard that poses risks to life, society, and the economy. Impacts include mortality and morbidity from fast-moving water, contaminated water supplies, and waterborne diseases as well as dam failures, power and transportation disruption, severe erosion, and damage to both natural and agro-ecosystems. These impacts span several sectors including water resource management, energy, infrastructure, transportation, health and safety, and agriculture. However, on the timeframe required by many decision makers for planning, preparing, and resilience-building ""subseasonal to seasonal (S2S; 14 to 90 days)"" forecasts have poor skill and thus adequate tools for prediction do not exist. Additionally, societal resilience to these events cannot be increased without established, two-way communication pathways between researchers, forecasters, and local or regional decision makers. Therefore, the goal of this project is to enhance scientific understanding of S2S extreme precipitation events, improve their prediction, and increase communication between research and stakeholder communities with regard to such events. The overarching results will be the development of predictive models that have the potential to reduce mortality, morbidity, and damages caused by S2S extreme precipitation events and broadening participation in science by including federal, tribal, and local stakeholders. Three targeted user communities; water resource managers, emergency managers, and tribal environmental professionals will engage throughout the project duration via workshops. The co-production of knowledge will steer the science to focus on useful characteristics that matter most to the people who use and rely on predictions, thus contributing to knowledge-sharing and improving the capability to predict what is meaningful.<br/><br/>This project will enhance fundamental understanding of the large-scale dynamics and forcing of S2S extreme precipitation events in the U.S. and improve capability to model and predict such events. This project assembles an expert team of scientists and stakeholders to narrow the prediction gap of S2S extreme precipitation events by answering four scientifically and societally relevant research questions: 1) What are the synoptic patterns associated with, and characteristics of, S2S extreme precipitation events in the contiguous U.S.? 2) Do large-scale modes of climate variability modulate these events? If so, how? 3) How predictable are S2S extreme precipitation events across temporal scales? and 4) How do we create an informative prediction of S2S extreme precipitation events optimized for policymaking and planning? To answer these questions, this project will for the first time, combine observations with novel machine-learning techniques, high-resolution radar composites, dynamical climate models (the National Multi-Model Ensemble and the Coupled Model Intercomparison Project phase 5), and workshops that engage stakeholders in the co-production of knowledge. This project will identify the fundamental weather and climate processes that are tied to S2S extreme precipitation events across the U.S. from scales as small as individual storms to those as large as ocean basins. The prediction skill for S2S extreme precipitation events will be improved through an increased mechanistic understanding of historical events and a quantitative evaluation of model performance for simulating these events and their characteristic patterns. The statistical and co-production frameworks developed in this project will have the flexibility to be applied across meteorological extremes and timescales, in other global regions, with future climate model simulations, and with other stakeholder communities to reduce the impact of and increase resilience to extreme meteorological events."
"1660093","SBIR Phase II:  Predicting Musculoskeletal Injury Risk of Material Handling Workers with Novel Wearable Devices","IIP","SBIR Phase II","04/01/2017","12/18/2019","Haytham Elhawary","NY","One Million Metrics Corp","Standard Grant","Rick Schwerdtfeger","12/31/2019","$1,417,997.00","","haytham@1mmcorp.com","450 West 33rd Street","New York","NY","100010000","6174808200","ENG","5373","116E, 152E, 165E, 169E, 5373, 8034, 8043, 8240, 9231","$0.00","This Small Business Innovation Research (SBIR) Phase II project has the objective of demonstrating that discrete, belt mounted internet-connected wearable devices used by industrial workers can detect high risk lifting activities, promote safe lifting practices and behavior change, and predict the risk of musculoskeletal injuries due to unsafe lifting. Each year over 600,000 workers suffer a musculoskeletal injury due to lifting related activities, which cost US companies over $15bn annually. Worker injuries affect employee morale, absenteeism, productivity loss and employee turnover, all of which are challenges to the efficient running of a company and are a unnecessary cause of human suffering. By developing a wearable device that can detect high risk lifting activity and provide immediate feedback to workers, safer lifting practices can be promoted and a reduction in the number of unsafe lifts registered, leading to a reduction in injuries.<br/><br/>The project includes three main technical objectives: i) the development of machine learning algorithms to detect lifting events from sensor data, and to measure risk related metrics associated to those lifting events. When a lift is considered high risk, real-time feedback will be provided to the worker; ii) the deployment of the device in an industrial setting at several customer sites for 12 months, with the number of high risk lifts performed by workers quantified over time to measure the ability of the system to drive behavior change in the workforce; and ii) the development of a model that can predict the likelihood of musculoskeletal injures based on the risk metrics measured. It is expected that the outcomes of the project demonstrate a significant reduction in the risk of suffering musculoskeletal injuries, paving the way for a clear return on investment value proposition for the industrial companies and their insurance carriers who are potential customers."
"1735756","Measuring R&D knowledge diffusion through large databases","SMA","SciSIP-Sci of Sci Innov Policy","07/01/2017","08/28/2018","Christina Freyman","CA","SRI International","Continuing Grant","Cassidy Sugimoto","06/30/2019","$298,277.00","","christina.freyman@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","SBE","7626","7626","$0.00","This research increases understanding of the impact of federal research funding on economic outputs by developing a new data source that enables new analyses of science, engineering, and innovation. The project will improve the measurement of R&D knowledge diffusion by collecting and linking three types of public data: federal government grants and contracts, U.S. patents, and licensing agreements.  This project assists the federal government in communicating the impact of public R&D investments to policy makers, stakeholders and citizens. By understanding the factors that lead to commercial licensing of federal government research, the federal government can invest its funds more strategically and increase the productivity of R&D investment, accelerate innovation and increase the United States? economic competitiveness.  In addition, this project directly addresses the goal to create a federal government that is responsive and accountable to its citizens.<br/> <br/>The project builds a data platform on an extensible, common data schema shared by all data types, including patents, publications, and awards. Collecting and combining these data will create a unique, novel, big data set. The government interest section of a U.S. patent will be used to link patents and federal government investments, which can be found in grant databases and the federal procurement data system. Patent licensing is opaque and not generally available; however, reports of technology licensing agreements are filed with the U.S. Securities and Exchange Commission, providing a rich public dataset on technology development. This project will use machine learning to extract these mentions, building a licensing agreement database. Specifically, this database will then be combined with the limited information available in the USPTO's licensing database, and linked to types of federal investment. This approach represents a new way to measure R&D impact by tracing knowledge flows through multiple sources and points in time. The project will demonstrate the viability of collecting licensing data on a large scale and linking the data back to the supporting federal government investments. The resulting data set will be made publicly available."
"1739097","Collaborative Research: CDS&E-MSS: Local Approximation for Large Scale Spatial Modeling","DMS","CDS&E-MSS","06/01/2017","03/10/2017","Benjamin Haaland","UT","University of Utah","Standard Grant","Christopher Stark","08/31/2019","$75,000.00","","benhaaland@hotmail.com","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","MPS","8069","8083, 9263","$0.00","Computer simulation is growing as a means of studying complex dynamics in applied science.  Once a tool exclusive to industrial engineering and computational physics, it is increasingly common in biology, chemistry, and economics.  Gone are the days when equilibrium dynamics are appropriate and cute systems of equations can be solved by hand.  Computer experiments are becoming more diverse, they are becoming more complex and they are growing in size thanks to modern supercomputing.  We need a new vanguard of modeling tools that can cope with the needs of modern computer experiments, particularly their increasing size (big data) and rapidly evolving and refining nature as models become more sophisticated, and supercomputing environments approach the exa-scale.  This funded research targets extensions and applications of a new breed of flexible and fast response surface methods, the so-called local approximate Gaussian process (laGP).  Our motivating applications come primarily from problems in computer experiments and uncertainty quantification, and ideas are borrowed from -- and will represent an important extension to -- the related literatures of geo-statistics and machine learning.  The over-arching goal is a modernization of the response surface and surrogate modeling toolkit to better serve future applications across applied science.<br/><br/>Gaussian process (GP) models are popular in spatial modeling contexts, like geostatistics or computer experiments, where response surfaces are reasonably smooth but little else can be assumed. GP models provide accurate predictors, but increasingly impose computational bottlenecks: large dense matrix decompositions impede efforts to keep pace with modern trends in data acquisition. A scramble is on for fast approximations. Two common themes are sparsity, allowing fast matrix decompositions, and supercomputing, allowing distributed calculation. But these inroads are at capacity. Rapidly expanding mobile device networks, high-resolution satellite imagery (and GPS), and supercomputer simulation generate data of ever-increasing size. This funded research centers on local approximate GP (laGP) models as a means of enabling the powerful GP spatial modeling framework to address modern big data problems. Initial implementations show promise, expanding data size capabilities by several orders of magnitude. However much work remains to ensure that laGP methods can supplant conventional GPs in diverse spatial modeling contexts. Here we propose several methodological enhancements, many involving shortcuts that have provably minimal impact on laGP performance. We are motivated by two big data computer model emulation applications: one involving satellite positioning and another on solar power generation. Yet we are mindful that for our efforts to have impact, the wider spatial modeling context must always be kept in view."
"1714022","EAPSI: Implementing and Analyzing 3-Dimensional Variants on Digital Neuropsychological Exams","OISE","EAPSI","07/01/2017","06/21/2017","Raniero Lara-Garduno","TX","Lara-Garduno            Raniero        A","Fellowship Award","Anne Emig","06/30/2018","$5,400.00","","","","College Station","TX","778458784","","O/D","7316","5921, 5978, 7316","$0.00","The field of clinical neuropsychology has placed increasing emphasis in early detection of cognitively degenerative diseases such as Alzheimer's and Parkinson's. Existing research in digitizing the diagnosis process aims to help specialists bring their services to rural areas where access to neuropsychologists may be limited. This research intends to further explore new kinds of clinical neuropsychology exams designed to enrich collected data for computational analysis. The researcher will create a 3-dimensional variant of the existing popular Trail Making Test in neuropsychology. Existing normative data from paper-and-pencil Trail Making Tests show clear correlations between age and average test completion time among cognitively healthy users. The project will explore whether similar correlations can be found in this proposed variation among a wide distribution of age ranges. This project will be conducted at the University of Tokyo under the mentorship of Dr. Takeo Igarashi, one of the field's leading researchers in 3D computer graphics and Human-Computer Interaction.<br/><br/>The test will be constructed for use in touch-enabled tablet devices, where patients will rotate a sphere comprised of labeled boxes and tap the next correct box in pre-determined sequences. The data points of most interest reside in the sphere manipulation behaviors as well as finger gesture data. Sphere manipulation would be captured by creating an invisible trail placed directly at the center of the screen's viewport. As the sphere rotates, the changing coordinates of the viewport as a function of time are stored, effectively recording where the patient was observing the sphere at any time. These coordinates will then be mapped to a 2D plane as treated as a digitized sketch, allowing machine learning techniques typically used in sketch recognition to be used in this context, with the intent to help classify the user's behavior in sphere manipulation per their cognitive function.<br/><br/>This award, under the East Asia and Pacific Summer Institutes program, supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science."
"1729743","DMREF/Collaborative Research: A Data-Centric Approach for Accelerating the Design of Future Nanostructured Polymers and Composites Systems","CMMI","POLYMERS, DMREF","09/01/2017","08/04/2017","L. Brinson","IL","Northwestern University","Standard Grant","Mary Toney","03/31/2018","$809,379.00","Wei Chen","cate.brinson@duke.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","ENG","1773, 8292","085E, 7237, 8021, 8400, 9102","$0.00","Polymer nanocomposites are highly tailorable materials that, with careful design, can achieve superior properties not available with existing materials. Most polymer nanocomposites are developed using an Edisonian (trial and error) process, severely limiting the capacity to optimize performance and increasing time to implementation. The solution is a data-driven design approach. As an example, this Designing Materials to Revolutionize and Engineer our Future (DMREF) project will design new material systems that simultaneously optimize for dielectric response and mechanical durability, a combination currently not achievable but necessary for high voltage electrical transmission and conversion. These new materials will have a significant economic impact on society because they will enable higher efficiency generation and transmission of electricity.  More broadly, this new design approach will result in new nanostructured polymer material systems that will impact a wide range of industries such as energy, consumer electronics, and manufacturing.  To ensure broad access to this work, the data, tools and models developed will be integrated and shared through an open data resource, NanoMine. The team will interact with the scientific community to create an integrated virtual organization of designers and researchers to test and improve the models. Educational components will reach undergraduate and graduate communities via interdisciplinary cluster programs at the two institutions, and provide undergraduate research opportunities and web based instructional modules and workshops.<br/><br/>The research is based on a central research hypothesis that using a data-driven approach, grounded in physics, allows integration of models that bridge length scales from angstroms to millimeters to predict dielectric and mechanical properties to enable the design and optimization of new materials. Data, algorithms and models will be integrated into the new and growing nanocomposite data resource NanoMine to address challenges in data-driven material design. This research will result in advancements in three areas. First, integrating a broad set of literature data and targeted experiments with multiscale methods will enable the development of interphase models to predict local polymer properties near interfaces considered critical for modeling polymer composites. Second, a hybrid approach utilizing machine-learning to bridge length scales between physics-based modeling domains will be used to create meaningful multiscale processing-structure-property relationship work flows. And, third, a Bayesian inference approach will utilize the knowledge contained in a dataset as a prior probability distribution and guide 'on-demand' computer simulations and physical experiments to accelerate the search of optimal material designs. Case studies will demonstrate the data-centric approach to accelerate the development of next-generation nanostructured polymers with predictable and optimized combinations of properties."
"1741047","BIGDATA: F: Collaborative Research: Foundations of Responsible Data Management","IIS","Secure &Trustworthy Cyberspace, Big Data Science &Engineering","09/01/2017","08/21/2017","Julia Stoyanovich","PA","Drexel University","Standard Grant","Sylvia Spengler","04/30/2019","$484,888.00","","stoyanovich@nyu.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8060, 8083","025Z, 7433, 7434, 8083, 9102","$0.00","Big Data technology promises to improve people's lives, accelerate scientific discovery and innovation, and bring about positive societal change. Yet, if not used responsibly, this same technology can reinforce inequity, limit accountability and infringe on the privacy of individuals: irreproducible results can influence global economic policy; algorithmic changes in search engines can sway elections and incite violence; models based on biased data can legitimize and amplify discrimination in the criminal justice system; algorithmic hiring practices can silently reinforce diversity issues and potentially violate the law; privacy and security violations can erode the trust of users and expose companies to legal and financial consequences. The focus of this project is on using Big Data technology responsibly -- in accordance with ethical and moral norms, and legal and policy considerations. This project establishes a foundational new role for data management technology, in which managing the responsible use of data across the lifecycle becomes a core system requirement. The broader goal of this project is to help usher in a new phase of data science, in which the technology considers not only the accuracy of the model but also ensures that the data on which it depends respect the relevant laws, societal norms, and impacts on humans. <br/><br/>This project defines properties of responsible data management, which include fairness (and the related concepts of representativeness and diversity), transparency (and accountability), and data protection. It complements what is done in the data mining and machine learning communities, where the focus is on analyzing fairness, accountability and transparency of the final step in the data analysis lifecycle, and considers the problems that can be introduced upstream from data analysis: during dataset selection, cleaning, pre-processing, integration, and sharing. This project develops conceptual frameworks and algorithmic techniques that support fairness, transparency and data protection properties through all stages of the data usage lifecycle: beginning with data discovery and acquisition, through cleaning, integration, querying, and ultimately analysis. The contributions are structured along three aims. Aim 1 considers responsible dataset discovery, profiling, and integration. Aim 2 considers responsible query processing and develops a general framework for declarative specification, checking and enforcement of fairness, representativeness and diversity. Aim 3 incorporates data protection into the lifecycle, develops techniques to facilitate sharing of sensitive data, and considers the tradeoffs between privacy and transparency. This project is poised to establish a multidisciplinary research agenda around responsible data management as a critical factor in enabling fairness, accountability and transparency in decision-making and prediction systems. Additional information about the project is available at DataResponsibly.com."
"1661132","ABI Development: Collaborative Research: The first open access digital archive for high fidelity 3D data on morphological phenomes","DBI","ADVANCES IN BIO INFORMATICS","09/01/2017","09/06/2019","Timothy Ryan","PA","Pennsylvania State Univ University Park","Continuing grant","Peter McCartney","08/31/2020","$505,997.00","Annmarie Ward, Kathleen Hill","tmr21@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","BIO","1165","","$0.00","People and societies thrive best when they understand how the social and physical dynamics of their environment work, allowing them to respond appropriately.  Natural scientists have built our understanding of the physical world. The scientific understanding they built has contributed to the development of technologies and practices that benefit human economies. For example, genetic sequencing of DNA enables deeper understanding of biological organisms; the consequences for human health, food production, understanding of evolutionary adaptation, etc. have been revolutionary and are still unfolding. The DNA sequence is the blueprint for an organism's anatomical structure (morphology) and function, but images capturing morphology are now much less prevalent than genetic data. Museums and researchers have been creating 3D digital images of natural history collections, and there are extensive 3D image data sets for some model organisms, but these data are mostly in closed collections, and generally unavailable or very difficult to access. This project aims to provide infrastructure to increase the accessibility of anatomical information, with a focus on 3D images. The resource will create the first open access, web-enabled image archive accepting and serving high-resolution, 3D scans of all organisms, called MorphoSource. Standardized descriptive tags will allow scientists to use this database to easily combine genetic and anatomical datasets for the first time, supporting the formulation of novel research questions. MorphoSource will link to other databases (such as iDigBio [www.idigbio.org]) that aggregate information on museum specimens from around the world. Having a shared common resource will change the culture among researchers and museums, making collaborations between physically distant experts more feasible, but it will also open the linked research collections of museums to anyone with Internet access anywhere in the world.  Large data sets are prerequisites for many statistical and machine learning methods, so the resource will enable innovations in computational image analysis methods, fostering new types of collaborations that advance field-wide scientific understanding. The resource will track data use, enhancing reproducibility and also providing an objective metric of the value of individual data elements. Open access to the data linked through MorphoSource will enable anyone with Internet access to see the detailed anatomical evidence for theories like evolution.  Pilot work has shown that teachers and students eagerly consume this newly available information, with numbers already in the thousands.  Positive results of this access include (1) providing a more intuitive type of raw data (compared to DNA sequence) for showing the public why some conclusions about evolutionary relationships were reached, (2) providing an 'interest metric' for the value of natural history museums and the collections they hold, (3) increasing the community of people (including citizen scientists) who have access to the data required to make important discoveries by studying biological variation. <br/><br/>The specific plan for creating the repository for 3D data on all organisms is as follows. The primary goal is to restructure and improve a proof of concept database called MorphoSource. The restructuring will allow MorphoSource to meet the needs of a growing community of researchers and educators through massive upscaling, and to implement a novel approach for economically preserving data for the long term. To accomplish this, the MorphoSource server will be rebuilt to use the Fedora digital asset management architecture, which has been developed by library scientists to serve emerging needs related to the archiving and sharing of digital data. As part of this architecture upgrade, the data hosted on MorphoSource will be given an additional layer of protection through managing asynchronous copies in DuraCloud, a digital data preservation platform that leverages Amazon cloud. This restructuring will allow the MorphoSource server architecture to be integrated with the Duke University Libraries repository infrastructure. MorphoSource will also be able to invite institutional communities to be consortium partners in support of data storage and to enact data preservation techniques that guarantee integrity and readability for the foreseeable future. Additional tools will (1) allow for rapid, automated ingestion of dozens to hundreds of datasets at once, (2) link MorphoSource with major biodiversity archives, and (3) provide in-browser visualization of 3D series of image slices, such as those generated by CT and MRI scanners.  The plan includes ingesting thousands of high quality legacy CT datasets from published studies, enabling their reuse, increasing the repeatability of studies. The project leaders plan to directly work with and design tools for K-12 educators and students to help them benefit from this resource. These datasets and educational tools will be available to researchers and the public through the updated MorphoSource website, available at www.morphosource.org."
"1702694","WiFiUS: Fault-Tolerant Cognitive IoT Systems Using Sensors of Limited Field-of-View: Fundamental Limits and Practical Strategies","CNS","Special Projects - CNS, Special Projects - CCF","04/01/2017","04/03/2017","Pulkit Grover","PA","Carnegie-Mellon University","Standard Grant","Monisha Ghosh","03/31/2019","$299,115.00","","pgrover@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1714, 2878","7363, 8229","$0.00","Improving reliability and efficiency of Internet of Things (IoT) is necessary to harness their full potential. IoT systems will sense, communicate, and process data to produce reliable decisions and inferences. The investigators study three ways in which each IoT device will be limited: limited ""field of view"" of each sensor; severe energy limitations; unreliability of sensing and communication. This research contributes towards progress of science by obtaining and analyzing novel cross-layer IoT sensing, communication, and computing strategies that outperform classical strategies by several orders of magnitude in energy efficiency and reliability. For validation, the researchers examine improvements on practically relevant problems of health monitoring through neuro-interfaces and distributed camera inference. The research involves several graduate and undergraduate courses, with emphasis on engaging female and minority students.<br/><br/>Severe energy constraints provide a compelling incentive for cross-layer designs of ""cognitive"" IoT systems to jointly sense, compress, communicate, and compute. However, naive cross-layer designs can reduce tolerance to system faults, e.g., sensing/ communication/computation errors. The investigators pursue a systematic understanding, utilizing it to obtain novel cross-layer designs of cognitive IoT strategies that gracefully trade off fault tolerance and efficiency through novel error-correction techniques compatible with limited fields of view. They benchmark this trade off against novel information-theoretic fundamental limits. The obtained techniques increase the robustness of algorithms that use, as a first step, linear projections on distributedly-sensed large dimensional data. Focusing on algorithms for the widely applicable k-Nearest Neighbors (k-NN) problem, the researchers obtain improved algorithms and analyze performance-efficiency-robustness tradeoffs, bridging information theory, statistics, and machine learning."
"1721578","SBIR Phase I:  Reconstructing Consistently Detailed City-Scale Environments From Incomplete 2D and 3D Data","IIP","SBIR Phase I","07/01/2017","06/28/2017","Christopher Mitchell","NY","Geopipe, Inc.","Standard Grant","Peter Atherton","08/31/2018","$224,009.00","","christopher@geopi.pe","16 West 22nd Street 6th Floor","New York","NY","100105969","9176861961","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to make it cheaper and faster for architects, urban planners, and real-estate developers (APDs), as well as many others, to work with detailed models of the real world. Designers in APD fields must visualize and render their projects in the context of the real world. Pictures, videos, 3D printing, and even virtual reality inform the design process and facilitate communication with lay customers and stakeholders. These applications require consistently detailed models of the built world, and this project will automate the generation of these models. We estimate that APDs spend at least $80M annually creating these models by hand; and that at least $300M more is spent on such models for simulations, special effects, and video game design. By algorithmically generating virtual models without human intervention, the significant cost (in time and money) of manual creation will be saved, freeing design professionals to do work they want to be doing.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will advance the state of the art in reconstructing highly detailed models of the world for diverse commercial applications. The first hurdle is solving the problem of reconstructing surfaces representing the boundaries of real-world solids (buildings) from noisy point cloud data. While surface reconstruction is well-studied in a variety of contexts, it remains an open problem in general, as successful algorithms must be informed by priors on the intended datasets. Using a data-driven approach to segment and classify input point clouds will facilitate the application of different reconstruction techniques to different objects (e.g. trees or buildings). The second hurdle is development of a machine learning algorithm which handles the dual problems of procedural modeling and inverse procedural modeling from a single statistical model, enabling visually realistic predictions about the details of a given building, even when that information is not available from source data (which may be of inconsistent quality across a large geographic area)."
"1745746","Workshop on Objective Bayes Methodology","DMS","STATISTICS","08/01/2017","08/01/2017","Peter Mueller","TX","University of Texas at Austin","Standard Grant","Gabor Szekely","07/31/2018","$10,000.00","","pmueller@math.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","MPS","1269","7556","$0.00","This award supports participation in the four-day ""Objective Bayes 2017 Workshop"" held at the University of Texas at Austin, TX on December 10-13, 2017. The workshop will consist of one day of tutorials for graduate students and new researchers, three days of scientific sessions, and a poster session, all centering on major recent developments in statistical inference methods that can be used automatically, that is, methodology that does not require subjective input other than a stylized probabilistic description of how the data arises.<br/><br/>The Objective Bayes workshop (""O'Bayes"") is one of the longest running and preeminent meetings in Bayesian statistics, addressing a wide range of topics including robust, default Bayesian analysis, reproducibility, variable selection, big data, and nonparametric Bayesian methods.  Objective Bayes methods encompass research in very diverse areas, from biostatistics, to machine learning, asymptotics, spatial inference, and computation. The workshop will serve to foster interaction of researchers in these diverse areas with an aim of initiating novel ideas and research directions. For more details about the conference please see the conference homepage https://sites.google.com/site/obayes2017/."
"1703504","Collaborative Research: SusChEM: Unlocking the fundamental mechanisms that underlie selectivity in oleochemical producing enzymes","CBET","Cellular & Biochem Engineering","09/01/2017","08/21/2017","Brian Pfleger","WI","University of Wisconsin-Madison","Standard Grant","Steven Peretti","08/31/2021","$300,000.00","","pfleger@engr.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","1491","1757, 8248","$0.00","Engineering, mathematics, biochemistry, and biology are combined to develop strategies for converting renewable resources into higher-value chemicals and materials. Some of these products have never been produced biologically, so organisms need to be modified to make this possible. There is also a great need for computational tools that can predict beneficial modifications to cellular proteins, thereby guiding these efforts. This research project is developing novel proteins that are active in the sustainable production of fats and oils. If successful, this work has the potential to displace unsustainable and environmentally unfriendly processes that contribute to the deforestation of tropical regions. Research experiences for undergraduate students and outreach efforts directed at K-12 students and the public through the SCIENCountErs program are being provided and serve to enhance the development of a highly knowledgeable STEM workforce.  The efforts are enhancing our understanding of the techniques, capabilities, and limitations of protein design. <br/><br/>In this collaborative research project, researchers are integrating computational and experimental methods to develop software that will be used to design novel enzymes involved in oleochemical metabolism. This protein-design infrastructure is integrating recent developments in molecular modeling and machine learning and developing design rules for tailoring chain length specificity for three oleochemical-synthesizing enzymes. These novel enzymes are being deployed in microbial biocatalysts and evaluated for their ability to produce oleochemicals from renewable resources. Upon conclusion, this work will generate a deeper understanding of selectivity in oleochemical metabolism, novel enzymes and microbial biocatalysts for producing specialty oleochemicals, and enzyme-design capabilities that could be widely applied in synthetic biology applications."
"1637244","Collaborative Research:  Observed and Future Dynamically Downscaled Estimates of Precipitation Associated with Mesoscale Convective Systems","AGS","Climate & Large-Scale Dynamics","09/01/2017","08/17/2017","Russ Schumacher","CO","Colorado State University","Standard Grant","Eric DeWeaver","08/31/2021","$86,467.00","","russ.schumacher@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","GEO","5740","026Z","$0.00","A mesoscale convective system (MCS) is a collection of thunderstorms organized on a larger scale than the storms it contains, in which the individual thunderstorms act in concert to generate the atmospheric motion that organizes and sustains the system.  These large storm systems produce extreme weather including hail, floods, and tornados, but they also make an important contribution to water resources over the eastern two thirds of the continental US (CONUS) during the growing season.  This project seeks to understand MCS behavior in an aggregate sense, including the long-term contribution of MCS precipitation to the overall water balance of the CONUS and the importance of year-to-year variability in MCS activity for anomalously wet (flood) or dry (drought) conditions.  <br/><br/>A key tool for conducting the research is the  Weather Services International (WSI) National Operational Weather radar (NOWrad) data set, a 20-year record (currently 1996-2015) created from the National Weather Service radar stations which provide continuous near-total coverage of the CONUS. A primary goal of the project is to develop and apply an automated procedure to detect and track MCSs in the radar data.  The algorithm identifies MCSs as contiguous or semi-contiguous features in radar maps over an area of at least 100km along the system's major axis exceeding a threshold reflectivity value.  MCS tracking is complicated by the the tendency of MCSs to split and merge as they propagate, and the algorithm incorporates a method for identifying mergers and splits. A further issue is that large regions of intense precipitation can occur in frontal cyclones and landfalling hurricanes, and a classification scheme is necessary to distinguish these regions from MCSs.  A machine learning technique to perform this classification is developed using expert judgement to train a random forest classifier (RFC) scheme. Further expert judgement is solicited through a survey which invites the research community to participate in the development and validation of the tracking and classification schemes.  The catalog of MCS events and their characteristics (intensity, duration, structure, etc) is then used to study MCS seasonality, interannual variability, and contribution to CONUS rainfall including floods and droughts.<br/><br/>Further work uses a global climate model (GFDL-CM3) in combination with a regional convection permitting model (WRF-ARW at 4km horizontal resolution) to simulate MCSs over the CONUS under present-day and projected future climate conditions.  The simulations are analyzed according to the tracking and classification schemes developed for the NOWrad data, and the model simulations allow examination of how MCS behavior depends on climatic factors such as tropospheric moisture, soil moisture, atmospheric stability, and large-scale atmospheric circulation.<br/><br/>The work has broader impacts due to the importance of MCS rainfall as a water resource for agriculture and the severe weather hazards related to MCS activity.  The algorithms and datasets produced for the project will be shared with researchers and operational climatologists and hydrologists through an online portal.  In addition, the project supports and trains a graduate student and provides summer  support for an undergraduate, thereby providing for the future scientific workforce in this area."
"1653118","CAREER: Integrated Design of Intelligent Structures with Tailored Distributed Damping","CMMI","EDSE-Engineering Design and Sy, CAREER: FACULTY EARLY CAR DEV, ESD-Eng & Systems Design, Special Initiatives","05/01/2017","07/24/2020","James Allison","IL","University of Illinois at Urbana-Champaign","Standard Grant","Kathryn Jablokow","04/30/2022","$618,000.00","","jtalliso@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","ENG","072Y, 1045, 1464, 1642","067E, 068E, 073E, 091Z, 1045, 116E, 9178, 9231, 9251","$0.00","This Faculty Early Career Development (CAREER) Program research project aims to investigate a fundamentally new approach for designing intelligent structures for vibration and motion control, create new numerical design strategies, and to generate a foundational understanding for how best to design this new class of intelligent structures. Existing intelligent structures use integrated sensors and actuators, such as piezoelectric materials, distributed across the surface or interior of a flexible elastic material to control dynamic behavior. Here embedded viscoelastic materials (VEMs) are introduced to overcome current performance limitations. Incorporating VEMs is challenging because 1) engineers cannot rely on past experience to design this unprecedented system, and 2) accurate VEM models are computationally expensive. Here new integrated design optimization strategies will be used to accelerate generation of design knowledge for this new type of intelligent structure, and to reduce computational expense. Numerical and physical experiments will center on application to precision pointing for space-based telescopes. More precise pointing has the potential to enhance significantly scientific data gathering, including search for exoplanets. These advances also have potential to advance other domains where ultra-quiet structural stability or precision motion control is critical (e.g., manufacturing, robotics, and defense). Education and outreach components of this CAREER project involve the creation of unique hands-on activities that allow K-12 and undergraduate students to experience the value of design automation tools. These are enhanced by collaborations with the ""Girls do Science"" program at the Orpheum Children's Museum and a CubeSat project with NASA's Jet Propulsion Laboratory.<br/><br/>Intelligent structures (IS) have been studied extensively, but primarily for active damping as opposed to motion control, and have largely avoided incorporation of spatially distributed VEMs for damping. Inclusion of VEMs and extension to motion control could help achieve new performance levels, but introduces a profound design challenge as no design history, validated design guidelines, or expert intuition exist. Current IS technology utilizes spatially distributed control actuation to tailor dynamic behavior. Recent work has combined control tailoring with distributed geometric elastic substructure design. A remaining obstacle is control of high-order structural modes in practical implementations, and is addressed here via strategically distributed VEMs to damp high-order modes passively. VEM distribution can be varied spatially to synergize with elastic material and control distribution. This enhances design flexibility, but this adds a new level of complexity. A new concept for integrated dynamic system design optimization where surrogate models of the state space derivative function are constructed and improved adaptively is planned. This capitalizes on the intrinsic properties of dynamic systems for numerical efficiency. VEMs can be modeled accurately, but with great computational expense, using high-order ordinary differential equation (ODE) approximations. The new derivative function surrogate modeling (DFSM) strategy is posited to produce high-accuracy VEM modeling using efficient low-order ODEs with state-dependent parameters. DFSM adjusts parameter mappings adaptively to improve accuracy, which supports efficient low-order computation. After thorough study of DFSM for damped IS design, DFSM will then be used for rapid design exploration and systematic generation of design data. Machine learning strategies will then be used to identify patterns and relationships within this data. An iterative inductive process will be used to identify possible design guidelines from these results, such as preferred distributed shape relationships or sensor/actuator/VEM placement guidelines. Additional numerical design studies will be used to validate/refine design guidelines."
"1741026","BIGDATA: IA: Collaborative Research:  Detecting Financial Market Manipulation: An Integrated Data- and Model-Driven Approach","IIS","Big Data Science &Engineering","09/01/2017","12/14/2018","David Byrd","GA","Georgia Tech Research Corporation","Standard Grant","Sara Kiesler","08/31/2021","$330,000.00","","dave@imtc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8083","7433, 8083","$0.00","Financial stock market manipulators can profit illegally by misleading investors about market conditions. For example, in several recent incidents, manipulators successfully spoofed markets by inserting orders that deceived investors about supply or demand for the security. This kind of behavior has increased with the prevalence of algorithmic trading. It imposes substantial harm to the economy, by reducing the efficiency of capital allocation, and more seriously, threatening to compromise the integrity and stability of financial markets. Spoofing is difficult to detect because the underlying actions have legitimate purposes as well as nefarious ones. This project will apply innovative approaches to improve detection and deterrence of market manipulation.<br/><br/>The project will integrate data-driven methods, including calibration of detectors with normal background activity and extraction of manipulation signatures from enhanced time series, with model-based techniques for characterizing manipulation strategies based on strategic analysis of market microstructure. The key idea is to use simulation and optimization to generate successful manipulation strategies for trading models calibrated from available market data streams. These strategies will then be injected into the trading models, to produce enhanced data streams that include labeled manipulation activity. Having labeled activity enables the application of machine learning techniques to extract signatures of spoofing activity, which can be used to construct surveillance and audit algorithms. Methods produced in this project in conjunction with guidance on market design and regulation policy can contribute to reducing the threat from increasingly capable market manipulators."
"1703274","Collaborative Research: SusChEM: Unlocking the fundamental mechanisms that underlie selectivity in oleochemical producing enzymes","CBET","Cellular & Biochem Engineering","09/01/2017","08/21/2017","Costas Maranas","PA","Pennsylvania State Univ University Park","Standard Grant","Steven Peretti","08/31/2021","$300,000.00","","costas@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","1491","1757, 8248","$0.00","Engineering, mathematics, biochemistry, and biology are combined to develop strategies for converting renewable resources into higher-value chemicals and materials. Some of these products have never been produced biologically, so organisms need to be modified to make this possible. There is also a great need for computational tools that can predict beneficial modifications to cellular proteins, thereby guiding these efforts. This research project is developing novel proteins that are active in the sustainable production of fats and oils. If successful, this work has the potential to displace unsustainable and environmentally unfriendly processes that contribute to the deforestation of tropical regions. Research experiences for undergraduate students and outreach efforts directed at K-12 students and the public through the SCIENCountErs program are being provided and serve to enhance the development of a highly knowledgeable STEM workforce.  The efforts are enhancing our understanding of the techniques, capabilities, and limitations of protein design. <br/><br/>In this collaborative research project, researchers are integrating computational and experimental methods to develop software that will be used to design novel enzymes involved in oleochemical metabolism. This protein-design infrastructure is integrating recent developments in molecular modeling and machine learning and developing design rules for tailoring chain length specificity for three oleochemical-synthesizing enzymes. These novel enzymes are being deployed in microbial biocatalysts and evaluated for their ability to produce oleochemicals from renewable resources. Upon conclusion, this work will generate a deeper understanding of selectivity in oleochemical metabolism, novel enzymes and microbial biocatalysts for producing specialty oleochemicals, and enzyme-design capabilities that could be widely applied in synthetic biology applications."
"1721024","Geometric and Topological Modeling and Computation of Biomolecular Structure, Function, and Dynamics","DMS","Molecular Biophysics, MSPA-INTERDISCIPLINARY, CDS&E-MSS","07/01/2017","06/14/2017","Guowei Wei","MI","Michigan State University","Standard Grant","Christopher Stark","06/30/2020","$375,000.00","Yiying Tong","wei@math.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","1144, 7454, 8069","7465, 8007, 9263","$0.00","A major feature of the biological science in the 21st century is its transition from  qualitative and descriptive  to quantitative and analytical.  Experimental exploration of self-organizing biomolecular systems, such as viruses, molecular motors and proteins in Alzheimer's disease, has been a dominating driving force in scientific discovery and innovation in the past few decades. Unfortunately, quantitative understanding of biomolecular structure, function, and dynamics severely lags behind the pace of the experimental progress.  Fundamental challenges that hinder the current quantitative understanding of biomolecular systems are their tremendous complexity and excessively large number of  degrees of freedom.  Most biological processes occur in water, which constitutes 65-90 percent human cell mass.  An average human protein  has about 5500 atoms, which, together with its surrounding water molecules, involve about 100,000 degrees of freedom. The dimensionality increases dramatically for subcellular organelles and multiprotein complexes. The real-time structure optimization, dynamic simulation, and function prediction of molecular motors and/or viruses in human cells are intractable with full-atom models at present. A crucial question is how to reduce the number of degrees of freedom, while retaining the fundamental physics in complex biological systems. This project addresses grand challenges in the structure, function, and dynamics of self-organizing biomolecular systems due to exceptionally massive data sets. These challenges are tackled through the introduction of a new mathematical models, together with advanced computational methods to deal with excessively large biomolecular data sets. This proposal offers innovative  approaches to an important area in massive data analysis, dimensionality reduction, computational mathematics and mathematical modeling.<br/><br/>The project addresses the aforementioned challenges by a number of geometric and topological approaches. First, a  multiscale framework is proposed to  reduce the dimensionality and number of degrees of freedom by a macroscopic continuum description of the aquatic environment, and a microscopic discrete description of biomolecules. Additionally, adaptive coarse-grained approach based on persistently stable manifolds is introduced to further reduce the dimensionality of excessively large biomolecular systems. A total free energy functional is introduced to bring the macroscopic surface tension and microscopic potential interactions on an equal footing. The differential geometry theory of surfaces is utilized to describe the interface between macroscopic and microscopic domains. Potential driven geometric flows are constructed to minimize the total free energy functional.  Furthermore, evolutionary topology and total curvature are introduced to analyze the topology-function relationship of biomolecules. Frenet frames are utilized to characterize the local geometry and associated stable manifolds in dynamical data of biomolecular systems. Machine learning algorithms are proposed to extract stable manifolds.  Finally, perturbation strategy is introduced to explore the persistence of stable manifolds, which provides the assurance for the reliability of the coarse grained model. In addition to promising and extensive preliminary results illustrating the power of this approach, extensive validation and application have been proposed to ensure that the proposed methodology yields robust and powerful tools for biomolecular structure optimization, function prediction  and dynamical simulation.<br/><br/>This project is funded by the Division of Mathematical Sciences with cofounding from the Division of Molecular and Cellular Biosciences."
"1717282","Global patterns, predictors, and their dynamical consequences in zoonotic diseases of mammals","DEB","Ecology of Infectious Diseases, Cross-BIO Activities, ","09/01/2017","05/09/2019","Barbara Han","NY","Institute of Ecosystem Studies","Standard Grant","Katharina Dittmar","08/31/2022","$2,025,612.00","John Drake, Suzanne O'Regan","hanb@caryinstitute.org","2801 SHARON TPKE","MILLBROOK","NY","125450129","8456777600","BIO","7242, 7275, Q266","019Z, 7242, 9178, 9251","$0.00","Around the world, outbreaks of novel infectious diseases are increasing in frequency.  The majority of infectious diseases emerging in humans originate from animal hosts.  There are now thousands of scientific studies investigating animal-borne (zoonotic) disease threats, but to date there are few methods to predict in advance where new infectious disease threats are most likely to arise.  In this project, researchers seek to understand why some animals harbor many more human pathogens compared to others; what kinds of pathogens are most likely to pose unforeseen disease threats to humans; and what kinds of environments may see large disease outbreak events in the future.  To do this, researchers will analyze large databases on mammals, their pathogens, and environmental conditions to describe what features best predict animal-borne infectious diseases in humans. Researchers will also examine the theoretical consequences of these predictions using mathematical models, to understand how disease patterns are most likely to change over time. Instead of reacting to new diseases after they have emerged, the predictions and methodological advances developed in this project will immediately benefit society by informing us how future disease threats may be preempted.<br/><br/>By combining machine learning and mathematical modeling with growing volumes of data on infectious diseases in wildlife and humans, this project will: (1) Identify intrinsic traits of hosts and pathogens and general environmental features that best predict patterns of zoonotic infection; (2) Identify particular mammal species and pathogenic agents likely to be undiscovered sources of future zoonoses; (3) Investigate the host, pathogen, and environmental covariates that distinguish infection from zoonotic disease in humans, and that combine to predict outbreak size; (4) Fit mathematical models (theory) to empirically observed patterns to derive potential underlying processes; (5) Investigate paradigmatic eco-epidemiological mechanisms through which empirically observed traits of hosts, pathogens, and changing environments influence transmission processes and disease dynamics."
"1701304","Doctoral Dissertation Research:  The Cultural Meaning of Entrepreneurship","SES","Sociology","05/01/2017","05/01/2017","Mark Granovetter","CA","Stanford University","Standard Grant","Toby Parcel","04/30/2020","$12,000.00","Zhiwen Zhao","mgranovetter@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","SBE","1331","1331, 9179","$0.00","The Cultural Meaning of Entrepreneurship<br/><br/>Abstract: <br/><br/>This project will adopt a comparative lens to uncover and contrast what it means to be an entrepreneur in China and the U.S., the two largest economies in the world at different economic development stages with drastically different cultural orientations to entrepreneurship, and to investigate how this meaning has evolved over time.  Situated in economic sociology and cultural sociology, this dissertation will contribute to the literature of entrepreneurship by directing attention to the under-studied role that culture plays.  It will also enhance understanding of the contemporary meaning of work as well as the globalization of capitalism. There are few issues that are more crucial than understanding the meaning of work, given that we spend most of our awake time pursuing one type of work or another.  Understanding the fundamental question of what work means to people will inform public discourses on job motivation, satisfaction, and work-life balance.  Furthermore, this study will have practical implications for public policies aiming to encourage innovation and economic development.   <br/><br/>Led by the enormous success of Silicon Valley, entrepreneurial fervor in the 1980's has spread across countries and cultures. Chief among them is the recent phenomenal rise of entrepreneurship in China.  In 2015 alone, venture capitalists poured a record $37 billion into Chinese startups, making China second in the global league (Ernst and Young 2016). Such rapidity of the rise of entrepreneurship is puzzling.  Unlike Protestant ethics, for more than a millennium, the Chinese traditional culture valued intellectual pursuits rather than market opportunistic behaviors.  Business people were perceived as low status, and capitalists were even punished under Mao.  It was only after Deng's open and reform policy in the late 70's that China gradually relaxed its constraint on private businesses, and it was not until 2000 that China emerged as a global hotbed for entrepreneurship.  Given this traditionally unfavorable if not outright hostile environment, how did entrepreneurship grow to today's vibrancy? Particularly, how did the cultural environment change, and how do entrepreneurs in China make sense of their identity and their role in the society?  How are these understandings different from or similar to those in the U.S.?  Using interviews and participant observations, this project seeks to uncover the current cultural meaning of entrepreneurship.  This dissertation will also take advantage of cutting-edge machine learning techniques and topic modeling to analyze large media text corpora in order to examine how the meaning of entrepreneurship has evolved over time."
"1821431","SHF: Small: Collaborative Research: Coupling Computation and Communication in FPGA-Enhanced Clouds and Clusters","CCF","Special Projects - CCF, Software & Hardware Foundation","10/01/2017","05/22/2020","Anthony Skjellum","TN","University of Tennessee Chattanooga","Standard Grant","Almadena Chtchelkanova","05/31/2021","$218,192.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","2878, 7798","7923, 7924, 7942, 9251","$0.00","The introduction of Field Programmable Gate Arrays (FPGAs) to accelerate clusters of servers in datacenters and clouds provides a great, immediate opportunity to leverage a new technology in high-end computing. With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore?s Law world. Since the hardware adapts to the application higher efficiency can be achieved, and since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Large-scale communication can consequently proceed with both higher bandwidth, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. The proposed design allows for useful processing while data is in flight in the network resulting in reduced software overhead in parallel middleware and reduced network congestion. The key tenets of the research are to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially software overhead. <br/>The research project, FC5 (an FPGA framework for coupling communication and computation in clouds and clusters) has several thrusts. First, hardware support for FC5 and investigation of methods of configurability in FC5 to reduce communication latency and support computing in the network are studied. A second outcome is a prototype version of the Open MPI open source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations. Third, proof-of-concept versions of multiple FC5 software models, including direct hardware access, a transparent MPI-in-OpenCL, and an API-based mechanism that exposes essential functionality. Finally, because FC5 is evolving rapidly with major new announcements expected imminently, continued refinement is essential. At least two model applications, Molecular Dynamics and Map-Reduce, will be used as test cases. <br/>With the continued consolidation of computing services into the cloud, the potential broader impact is to increase both the scale and availability of parallel applications. The broad range of uses of cloud and cluster computing for commercial, government, and academic applications means that acceleration offered will have a widespread impact applicable across many sectors. The growing acceptance of high performance computing in industry (e.g., fast machine learning) is one particular potential commercial sector that will be enhanced by this project."
"1725657","SPX: Enabling Scalable Synchronizations for General Purpose GPUs","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/08/2017","Jun Yang","PA","University of Pittsburgh","Standard Grant","Marilyn McClure","09/30/2021","$850,000.00","Rami Melhem","juy9@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","042Y","026Z","$0.00","Today?s GPUs have been successfully used for general purpose processing including high-performance computing, machine learning, graph analytics, to name a few applications. However, general purpose computing has different complexity and characteristics from those of graphics processing. A crucial characteristic is the need for coordination and synchronization among parallel threads, which occurs more often in general purpose applications than in graphics rendering. However, unlike common CPU designs, current GPU designs are missing full-fledged support for synchronization primitives that are convenient for implementing transparent scalability and achieving high performance for many parallel applications.<br/><br/>This project develops truly effective, efficient, easy-to-use and easy-to-implement synchronization mechanisms for GPGPU while overcoming major obstacles pertaining to the GPU architecture. The first thrust will identify GPGPU synchronization primitives that are deemed sufficient to handle a large set of applications with static and dynamic dependencies. The second thrust will provide solutions and methodologies on how to use the new primitives in the most efficient way to suit the characteristics of applications. The third thrust addresses the main challenges in implementing those primitives in efficiency, possible context switching overhead, and memory capacity limitations. The success in this project will help fuel the spread of accelerator architectures for high-performance computing, cloud and mobile systems. It will increase efficiency for improved performance and reduced energy/power consumption, leading to a greener IT industry. Trained students will become future taskforce to continue the revolution of extending computing into every realm of science, life, commerce and culture."
"1741190","BIGDATA: IA: Collaborative Research:  Detecting Financial Market Manipulation: An Integrated Data- and Model-Driven Approach","IIS","Big Data Science &Engineering","09/01/2017","04/12/2019","Michael Wellman","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sara Kiesler","08/31/2021","$686,000.00","Michael Barr, Uday Rajan","wellman@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8083","7433, 8083, 9251","$0.00","Financial stock market manipulators can profit illegally by misleading investors about market conditions. For example, in several recent incidents, manipulators successfully spoofed markets by inserting orders that deceived investors about supply or demand for the security. This kind of behavior has increased with the prevalence of algorithmic trading. It imposes substantial harm to the economy, by reducing the efficiency of capital allocation, and more seriously, threatening to compromise the integrity and stability of financial markets. Spoofing is difficult to detect because the underlying actions have legitimate purposes as well as nefarious ones. This project will apply innovative approaches to improve detection and deterrence of market manipulation.<br/><br/>The project will integrate data-driven methods, including calibration of detectors with normal background activity and extraction of manipulation signatures from enhanced time series, with model-based techniques for characterizing manipulation strategies based on strategic analysis of market microstructure. The key idea is to use simulation and optimization to generate successful manipulation strategies for trading models calibrated from available market data streams. These strategies will then be injected into the trading models, to produce enhanced data streams that include labeled manipulation activity. Having labeled activity enables the application of machine learning techniques to extract signatures of spoofing activity, which can be used to construct surveillance and audit algorithms. Methods produced in this project in conjunction with guidance on market design and regulation policy can contribute to reducing the threat from increasingly capable market manipulators."
"1722913","SCH: INT: Personalized Proactive Diabetes Self-Management and Social Networking for American Indians","IIS","Smart and Connected Health, EPSCoR Co-Funding","09/01/2017","08/29/2019","Juan Li","ND","North Dakota State University Fargo","Standard Grant","Soo-Siang Lim","08/31/2021","$988,114.00","Jun Kong, Siobhan Wescott, Donald Warne","j.li@ndsu.edu","Dept 4000 - PO Box 6050","FARGO","ND","581086050","7012318045","CSE","8018, 9150","8018, 8062, 9150","$0.00","The epidemic of diabetes in American Indian (AI) communities is a serious public health challenge. As daily diabetes care is primarily handled by the patients and their families, the effectiveness of diabetes control is largely impacted by self-care strategies and behaviors. This project proposes a proactive diabetes self-care mobile platform based on the unique socio-economic, cultural, and geographical status of AI patients living in a reservation community in the upper Midwest. The mobile platform connects AI diabetic patients to their medical devices, healthcare team and similar patients, and offers personalized prediction, recommendation, and social networking regarding diabetes care. It transforms diabetes management from the traditional reactive and hospital-centered care to preventive, proactive, evidence-based, and person-centered care. This project will also provide valuable research opportunities for both graduate and undergraduate students, especially from underrepresented populations.<br/><br/><br/>The goal of the project is to develop an integrated, accessible, cost-effective solution for improved diabetes self-management and social networking for AI patients. Considering the quasi-ubiquitous use of cellphones in most AI communities, a cellphone-based platform is proposed to provide smart and personalized service. A biocultural ontology is developed to model the unique characteristics of AI patients. Based on a user's ontological profile, through ontology-based reasoning and machine learning approaches, personalized prediction and recommendation are delivered to the AI patient. Semantics-based social matching is applied as a technical and intellectual framework for connecting AI patients with similarities to form online peer support communities. Personalized self-managed trust and privacy mechanisms are developed to establish secure user relationships. To improve system usability, a multimodal interface is implemented to deliver information with appropriate input and output modalities to a user based on the user's physical and social context."
"1743847","RAPID:  Emotion Regulation, Attitudes, and the Consequences for Political Behavior in a Polarized Political Environment","SES","Political Science","06/01/2017","05/07/2020","Cherie Maestas","NC","University of North Carolina at Charlotte","Standard Grant","Brian Humes","05/31/2021","$198,205.00","Sara Levens","cmaestas@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","SBE","1371","7914, 9179","$0.00","Overview and Broader Impacts<br/>Transitions of power in democracies from one party to another create unique political contexts.   The prospect of substantial and sometimes unexpected changes in economic, social, or foreign policy stimulates heated debate and strong emotions in the public. This evolving context allows for scholars to assess how emotion regulation shapes political behavior and attitudes. The activation of emotion in the face of unexpected events may manifest in a number of forms: posting on social media, attending town hall meetings, contacting elected officials, signing petitions, and/or participating in social movements. This knowledge helps to explain when and why emotions fuel broad-scale political participation and competition. The data generated from this project enables the investigators to assess theories about the influence of emotion regulation on political participation, i.e. how different types of emotion regulation strategies lead to different types of political behavior.  The project will also provide valuable data for the social science community more broadly.  In particular, the study will produce the first multi-wave panel survey tracking how individual attitudes and behavior change over time in response to political events during the first year of a party transition in the United States. The survey is the first in political science to study how emotion regulation habits vary across individuals in society and considers whether, over time, these habits lead to more consensus or greater polarization. The study utilizes established practices from psychology to measure emotion regulation habits in political contexts. The project also trains graduate and undergraduate students in machine learning, automated text coding, and text-analytics of open-ended survey responses. These are emerging as critical skills in the field of survey research as computing power opens new opportunities for understanding public attitudes through more free-form, naturalistic responses compared to traditional surveys. <br/><br/>Scientific Merit<br/>This study contributes to a growing body of research exploring the importance of socio-political emotions in fueling public attitudes and behavior. We develop new theory to explain how individual-level emotion regulation habits a) moderate the affective processing of information and b) influence expressive and social political behavior. Pilot tests indicate individuals adept at regulating their emotions through reappraising emotion-provoking stimuli were more likely to become politically active compared to those who use suppressive or avoidance regulation habits. Reappraisers are more likely to transmit their views to others, thus serving as a source of social contagion. This project uses a three-wave panel survey design to conduct a broader test of how emotion regulation shapes issue engagement, changes in perceptions of political responsibility, and changes in levels and forms of political participation. We draw our panel sample from a pool of respondents to a large online national survey conducted just prior to the 2016 elections to obtain pre-election baseline opinions on key policy topics like healthcare, the environment, tax reform, and military use. Subsequent waves of the panel study are timed to survey respondents during critical moments of policy change, or will follow an unexpected extraordinary event, should one arise. The design permits the investigators to observe individual-level change in the use of emotion regulation habits over time, and to test whether different emotion regulation habits make individuals more prone to reexamine their beliefs. If that is the case, these individuals could potentially change beliefs, or reject new information and maintain rigidity in attitudes. The research also allows the investigators to examine how emotion regulation influences or hinders political activism in response to emotionally provocative moments in society."
"1659871","REU Site: Sensor, Signal and Information Processing Devices and Algorithms","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2017","01/17/2017","Andreas Spanias","AZ","Arizona State University","Standard Grant","Joseph Maurice Rojas","12/31/2021","$299,923.00","Jennifer Blain Christen","spanias@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","1139","9250","$0.00","This three year REU site will recruit and train nine undergraduate students each summer and engage them in research endeavors on the design of sensors including student training in mathematical methods for extracting information from sensor systems.  The investigators along with a team of faculty advisors will supervise a series of multidisciplinary projects in the design of integrated sensor systems.   In addition to the planned projects, the faculty leaders of this program will organize a series of industry collaborative training activities for the students.  This REU features multidisciplinary synergies across different research labs that provide access to unique sensor and algorithm technology. The program also includes crosscutting modules, and workshops in public speaking, policy, standards, ethics, patents, SBIR planning, and outreach.   Annual REU workshops will train students to communicate with stakeholders who can help establish new standards. An evaluation unit will assess REU goals annually using feedback from academia, industry and stakeholders.  The program engages minority colleges to broaden participation and enhance recruitment. <br/><br/>The REU will address STEM problems associated with sensor applications in internet of things, health monitoring and security.  Specific objectives of the REU site are to:  a) introduce students to general research practices by immersing them in government and industry research activities, b) engage students in integrated design of sensor devices and relevant information processing methods, c) motivate students to pursue research careers,   d) provide cross cutting skills in presentation, developing patents, entrepreneurship, and building awareness on social implications, policies and standards.  The REU projects are designed to introduce students to an array of sensor device design technologies that emphasize low power circuits, flexible electronics, MEMS, and embedded systems.  During the same period, projects will train REU students to interpret data from sensors by studying and programming machine learning algorithms, sensor fusion methods, and techniques to interpret big data sets."
"1737443","SCC-IRG Track 2: Community on Multimodality: Participatory Action, Service, and Support (COMPASS)","ECCS","S&CC: Smart & Connected Commun","09/01/2017","05/28/2020","Daphney-Stavrou Zois","NY","SUNY at Albany","Standard Grant","Anthony Kuh","08/31/2022","$1,328,060.00","Charalampos Chelmis, Wonhyung Lee","dzois@albany.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","ENG","033Y","042Z","$0.00","Those in need of help often do not know how to locate or access service providers. Likewise, service-providing agencies often work in silos. The lack of communication also applies to volunteers; people do not know who to help and how they can be resourceful. Response becomes even more problematic when a problem demands the coordination of service providers, volunteers, and government structures, and after business hours, when the communication channels that can aid people in need become sparse. This project will (i) simplify the discovery and use of services, (ii) enable two-way communication between stakeholders (e.g., residents and service providers), (iii) deploy resources more efficiently, and (iv) assist stakeholders in assessing and promoting the wellbeing of their communities. The end result will be directly applicable to communities across the US, and has the potential to be influential beyond the human and physical services domain. It will advance computer science, electrical and computer engineering, and social and behavioral sciences to collectively address the challenges associated with this problem, and will create educational opportunities to encourage students to cross disciplinary boundaries. Women and underrepresented groups will be encouraged to participate in this project by collaborating with community-based organizations, and via programs at the University at Albany.<br/><br/>This multidisciplinary project takes a community-wide approach that will integrate people and data with analytics and engineering using social and behavioral sciences to maximize the efficiency of delivering human and physical services, and also improve the sense of connectedness of residents with service providers and government structures. The project will study the limitations of extant services, which will in turn inform the development of novel decision-making mechanisms with sufficient behavioral realism. The outcome will be an integrated ""one-stop"" service of services. This project will (i) develop new data mining methods for uncovering complex interdependencies within a dynamic sociotechnical system, (ii) devise novel information processing, machine learning, and control methods to dynamically optimize delivery of human and physical services under uncertainty with humans in the decision-making loop, and (iii) shed light on the ability of communities to integrate emerging technologies to become more connected in human interactions."
"1637212","Collaborative Research:  Observed and Future Dynamically Downscaled Estimates of Precipitation Associated with Mesoscale Convective Systems","AGS","Climate & Large-Scale Dynamics","09/01/2017","08/17/2017","Vittorio Gensini","IL","Community College District 502","Standard Grant","Eric DeWeaver","11/30/2017","$49,537.00","","vgensini@niu.edu","425 Fawell Blvd.","Glen Ellyn","IL","601376708","6309424611","GEO","5740","026Z","$0.00","A mesoscale convective system (MCS) is a collection of thunderstorms organized on a larger scale than the storms it contains, in which the individual thunderstorms act in concert to generate the atmospheric motion that organizes and sustains the system.  These large storm systems produce extreme weather including hail, floods, and tornados, but they also make an important contribution to water resources over the eastern two thirds of the continental US (CONUS) during the growing season.  This project seeks to understand MCS behavior in an aggregate sense, including the long-term contribution of MCS precipitation to the overall water balance of the CONUS and the importance of year-to-year variability in MCS activity for anomalously wet (flood) or dry (drought) conditions.  <br/><br/>A key tool for conducting the research is the  Weather Services International (WSI) National Operational Weather radar (NOWrad) data set, a 20-year record (currently 1996-2015) created from the National Weather Service radar stations which provide continuous near-total coverage of the CONUS. A primary goal of the project is to develop and apply an automated procedure to detect and track MCSs in the radar data.  The algorithm identifies MCSs as contiguous or semi-contiguous features in radar maps over an area of at least 100km along the system's major axis exceeding a threshold reflectivity value.  MCS tracking is complicated by the the tendency of MCSs to split and merge as they propagate, and the algorithm incorporates a method for identifying mergers and splits. A further issue is that large regions of intense precipitation can occur in frontal cyclones and landfalling hurricanes, and a classification scheme is necessary to distinguish these regions from MCSs.  A machine learning technique to perform this classification is developed using expert judgement to train a random forest classifier (RFC) scheme. Further expert judgement is solicited through a survey which invites the research community to participate in the development and validation of the tracking and classification schemes.  The catalog of MCS events and their characteristics (intensity, duration, structure, etc) is then used to study MCS seasonality, interannual variability, and contribution to CONUS rainfall including floods and droughts.<br/><br/>Further work uses a global climate model (GFDL-CM3) in combination with a regional convection permitting model (WRF-ARW at 4km horizontal resolution) to simulate MCSs over the CONUS under present-day and projected future climate conditions.  The simulations are analyzed according to the tracking and classification schemes developed for the NOWrad data, and the model simulations allow examination of how MCS behavior depends on climatic factors such as tropospheric moisture, soil moisture, atmospheric stability, and large-scale atmospheric circulation.<br/><br/>The work has broader impacts due to the importance of MCS rainfall as a water resource for agriculture and the severe weather hazards related to MCS activity.  The algorithms and datasets produced for the project will be shared with researchers and operational climatologists and hydrologists through an online portal.  In addition, the project supports and trains a graduate student and provides summer  support for an undergraduate, thereby providing for the future scientific workforce in this area."
"1702426","MSB-FRA: Testing abiotic drivers of activity, abundance, and diversity of ground-dwelling arthropod communities at a continental scale","DEB","MacroSysBIO & NEON-Enabled Sci","08/15/2017","05/20/2020","Michael Kaspari","OK","University of Oklahoma Norman Campus","Standard Grant","Elizabeth Blood","07/31/2021","$1,223,398.00","Cameron Siler, Michael Weiser, Katie Marshall, Matthew Miller","MKASPARI@OU.EDU","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","BIO","7959","9150, 9178, 9251","$0.00","Insects are among the most abundant and ecologically important animals in the biosphere. They include serious crop pests and invasive species that cause millions in damage. A key goal of this project is to understand how and when the number, and activity of insects change as one moves from place to place across the U.S., and why those numbers fluctuate from year to year.  Such an understanding can help predict insect pest outbreaks, the spread of invasive species, and changes in an ecosystems ability to provide food and fiber and conserve soil nutrients. This project will use insect samples collected by the National Ecological Observatory Network (NEON) at 47 locations spanning the U.S.'s major ecosystems to determine how abundance, activity, and diversity of soil insects vary across the U.S.  The project will use environmental barcoding (analyzing insect DNA from the samples' preservative) and image analysis techniques to  train computer algorithms to count and identify insects preserved in the samples. Together, these technologies will automate and streamline NEON's monitoring network, providing the first such nationwide dataset on abundance, activity, and diversity of the U.S.'s soil insects. In doing so they will serve a variety of stakeholders: ecologists testing and refining models that predict future insect communities; land managers who want to know the likelihood of a pest eruption; conservation biologists and urban planners hoping to anticipate spread of invasive ants and beetles.<br/> <br/>A key aim of this project is to quantify and predict how Earth's great abiotic drivers--temperature, precipitation, and biogeochemistry--govern how ecological communities of soil insects vary from place to place. Community data at continental extents vastly underrepresent the terrestrial arthropods in part due to the immense effort required to count, size, and identify taxa ranging from mites to ants to beetles to spiders. Yet the few existing arthropod datasets suggest that as one travels from deserts to rainforests, terrestrial arthropod communities vary by orders of magnitudes in abundance (the number of individuals), size (mass per individual), activity (the rate at which individuals do work on the system), and diversity (the number of species/forms). Combined, these four variables help predict how arthropods regulate ecosystem processes like decomposition, herbivory, and seed dispersal. This knowledge gap will be filled by the analysis of samples from the NEON pitfall network (arrays of traps, sunk in the soil, that capture and store biweekly samples of arthropods in ethanol). It will develop two complementary methods to do so. Environmental Bar Coding samples and identifies pitfall taxa from extracts of ethanol. Image Analysis uses machine learning to count, size, and classify arthropods in a sample. Pitfall samples containing key orders of Earth's arthropods will be analyzed from NEON's 47 sites, and Environmental Bar Coding and Image Analysis pipelines that count, size, and identify taxa from these samples will be developed, tested, honed, and validated, and then used to analyze two years of NEON samples."
"1818574","DMREF/Collaborative Research: A Data-Centric Approach for Accelerating the Design of Future Nanostructured Polymers and Composites Systems","CMMI","POLYMERS, DMREF","10/07/2017","04/06/2020","L. Brinson","NC","Duke University","Standard Grant","Alexis Lewis","08/31/2021","$941,269.00","","cate.brinson@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","ENG","1773, 8292","085E, 116E, 7237, 8021, 8400, 9102, 9178, 9231, 9251","$0.00","Polymer nanocomposites are highly tailorable materials that, with careful design, can achieve superior properties not available with existing materials. Most polymer nanocomposites are developed using an Edisonian (trial and error) process, severely limiting the capacity to optimize performance and increasing time to implementation. The solution is a data-driven design approach. As an example, this Designing Materials to Revolutionize and Engineer our Future (DMREF) project will design new material systems that simultaneously optimize for dielectric response and mechanical durability, a combination currently not achievable but necessary for high voltage electrical transmission and conversion. These new materials will have a significant economic impact on society because they will enable higher efficiency generation and transmission of electricity.  More broadly, this new design approach will result in new nanostructured polymer material systems that will impact a wide range of industries such as energy, consumer electronics, and manufacturing.  To ensure broad access to this work, the data, tools and models developed will be integrated and shared through an open data resource, NanoMine. The team will interact with the scientific community to create an integrated virtual organization of designers and researchers to test and improve the models. Educational components will reach undergraduate and graduate communities via interdisciplinary cluster programs at the two institutions, and provide undergraduate research opportunities and web based instructional modules and workshops.<br/><br/>The research is based on a central research hypothesis that using a data-driven approach, grounded in physics, allows integration of models that bridge length scales from angstroms to millimeters to predict dielectric and mechanical properties to enable the design and optimization of new materials. Data, algorithms and models will be integrated into the new and growing nanocomposite data resource NanoMine to address challenges in data-driven material design. This research will result in advancements in three areas. First, integrating a broad set of literature data and targeted experiments with multiscale methods will enable the development of interphase models to predict local polymer properties near interfaces considered critical for modeling polymer composites. Second, a hybrid approach utilizing machine-learning to bridge length scales between physics-based modeling domains will be used to create meaningful multiscale processing-structure-property relationship work flows. And, third, a Bayesian inference approach will utilize the knowledge contained in a dataset as a prior probability distribution and guide 'on-demand' computer simulations and physical experiments to accelerate the search of optimal material designs. Case studies will demonstrate the data-centric approach to accelerate the development of next-generation nanostructured polymers with predictable and optimized combinations of properties."
"1703637","SHF: Medium: Collaborative Research: Testing in the Era of Approximation","CCF","Software & Hardware Foundation","09/01/2017","08/31/2017","Sasa Misailovic","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sol Greenspan","08/31/2020","$250,000.00","","misailo@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7924, 7944","$0.00","Many computations, such as image processing, machine learning, and engineering simulations are inherently approximate -- they trade off quality of results for better performance. However, approximation also introduces new challenges when reasoning about program behaviors and finding bugs. At present, testing in this area requires more principled and effective approaches. Simultaneously, approximation itself provides an effective new basis for innovations in the well-trodden field of testing, thereby making testing more efficient and valuable. The project will develop a bi-directional integration of testing and automated approximation, new approach for developing and optimizing an increasingly important class of programs. The results will be embodied in open source tool sets and rigorously evaluated using open-source and proprietary applications. New educational and course materials will be developed for courses on compilers, program analysis and software engineering.<br/><br/>More concretely, the project will develop a set of techniques and tools for testing approximate programs, including a test specification language and techniques for automated migration of existing tests to the new language, techniques for dynamic approximate-program analysis, and techniques for optimal approximation discovery.  Moreover, the project will develop approximate computing techniques to improve the performance of regression testing and mutation testing."
"1745300","EAGER: SSDIM: Multiscale Methods for Generating Infrastructure Networks","CMMI","Special Initiatives, ","09/01/2017","08/25/2017","Ilya Safro","SC","Clemson University","Standard Grant","Walter Peacock","08/31/2020","$200,000.00","Patrick Rosopa, Kalyan Piratla","isafro@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","ENG","1642, Q231","036E, 041E, 042E, 1057, 7916, 9150","$0.00","The ultimate goal of this EArly-concept Grant for Exploratory Research (EAGER) project is to develop a crossdomain multiscale network and interdependent critical infrastructure (ICI) generator that captures many features of real networks and incorporates an arbitrarily large or small degree of stochasticity. Starting from samples of known or hypothesized networks, the generator will synthesize ensembles of networks and ICIs that will preserve, on average, a diverse set of topological and physical design properties at multiple scales of its structure. These properties will include several measures of centrality, assortativity, path lengths, clustering, flows, required physical capacities, and modularity. This approach introduces an unbiased variability across the ensemble in many of these properties at multiple scales which creates a desired realism of the synthesized system. The models will include human factor components incorporated in both topological and physical designs. A toolbox of algorithms and heuristics for generating synthetic networks of infrastructures and ICIs will be developed, and generated benchmarks will be disseminated. <br/><br/>Outcomes of this work will facilitate such tasks as simulation, policy testing and decision making for ICIs enabled by fundamental advancement in network generation algorithms. The toolbox will be designed using a modular approach that will allow it to evolve and to be applied in other domains. The interdisciplinary team of investigators comprising expertise in computer science, behavioral science, civil engineering, and network science and public health will ideally support the goal of realistic synthetic data generation. Perspectives and methodological approaches from network analysis, big data systems, machine learning, water networks, and organizational sciences will be brought to bear to develop a toolkit of methods that include a combination of network analytics, optimization, and statistical analysis techniques. The resulting products will include developed algorithms and generated synthetic datasets disseminated for a broad scientific community."
"1718820","AF: Small: Algorithms for Solving Real-Life Instances of Optimization and Clustering Problems","CCF","Algorithmic Foundations","08/15/2017","08/09/2017","Yury Makarychev","IL","Toyota Technological Institute at Chicago","Standard Grant","Tracy Kimbrel","07/31/2021","$449,986.00","","yury@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926","$0.00","The project aims to develop efficient algorithms for computational problems that arise in business, engineering, and science. The project will have an impact on theoretical computer science (TCS) by improving our understanding of the nature of real-life instances and designing better algorithms -- with provable performance guarantees -- for them. Additionally, the results will be relevant to researchers in other areas of computer science, including machine learning and optimization. In particular, this project will provide researchers with new practical algorithms. Through its wide-reaching results, the project will strengthen the connection between TCS and other areas of computer science. Further, the project will be of interest to researchers in mathematics, mathematical physics, and statistics, in part because one of the key models considered in this proposal has been introduced and studied in these fields.<br/><br/>The PI will collaborate on this project with graduate and undergraduate students at the Toyota Technological Institute at Chicago (TTIC) and the University of Chicago. Additionally, he will invite PhD students from other universities to work on the project during summer months. The PI will incorporate the topic of this proposal in his graduate courses. Specifically, he will include introductory material on the topic in his graduate Algorithms course and more advanced material in his course on metric geometry in computer science.<br/><br/>Many problems that arise in business, engineering, and science are very hard in the worst case: for them, there are no ""universal"" efficient algorithms -- i.e., algorithms that solve all possible instances in reasonable (polynomial) time. However, real-life instances are usually considerably simpler than the most difficult ones. The goal of this project is to identify what makes real-life instances computationally tractable and to design efficient algorithms for solving them. These algorithms will solve many real-life instances of computational problems by exploiting their structural properties (at the same time, the algorithms may fail to solve the most difficult instances, which, however, almost never appear in practice).<br/><br/>In order to design efficient algorithms -- with provable performance guarantees -- for real-life instances of computational problems, one has to define a formal model for real-life instances. This proposal will explore existing generative and descriptive models for real-life instances of clustering and optimization problems, including semi-random stochastic block models and models based on different stability assumptions. The project will also develop new, more advanced models. The PI will design new algorithms for these models and analyze existing heuristics."
"1717883","SHF: Small: Collaborative Research: Automated Numerical Solver EnviRonment (ANSER)","CCF","Software & Hardware Foundation","08/15/2017","08/04/2017","Boyana Norris","OR","University of Oregon Eugene","Standard Grant","Almadena Chtchelkanova","07/31/2021","$224,999.00","","norris@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","7798","7923, 7942, 9102","$0.00","The computational science community is tackling ever larger and more complex applications. The solution of the underlying mathematics problems requires using high-end parallel computing resources effectively, and delivering performance without degrading productivity is critical for the success of scientific computing. Converting mathematics from algorithms to high-quality implementations, however, is a difficult process, whether an application is developed from scratch or by leveraging existing software libraries. Modern numerical packages provide numerous solutions with widely varying performance. Selecting among these possibilities requires expertise in numerical computation, mathematical software, compilers, and computer architecture, but even such broad knowledge does not guarantee the selection of the best-performing method for a particular problem. In response to these challenges, ANSER (Automated Numerical Solver EnviRonment) automates the selection and configuration of algorithms such as sparse linear solvers, eigensolvers, and graph methods in the context of large-scale scientific and engineering applications. The overall approach is generalizable to any situation involving multiple solutions whose performance varies with input problem properties.  ANSER increases developer productivity and promotes effective use of modern parallel architectures to solve large-scale scientific and engineering problems. This work also impacts the training of the next-generation scientific workforce by involving graduate and undergraduate students in this model-guided development of high-performance software. <br/> <br/>ANSER, the Automated Numerical Solver EnviRonment, is an open-source web-based platform that supports the development of both scientific applications and high-performance libraries.  It selects, configures and, in some cases, generates implementations of high-performance numerical algorithms.  ANSER defines a methodology for automating the process of identifying problem features, creating performance models (based on combining analytical and machine learning approaches), and employing them in creating and configuring numerical software. ANSER initially targets widely used numerical packages for nonlinear partial differential equations and solution of eigenvalue problems, but it is designed to be extensible to other types of numerical methods, such as graph computations and n-body simulations. In addition to traditional dissemination methods (open-source software releases and publications), ANSER integrates semantic analysis of scientific computing literature to discover numerical methods similar to those provided by the target libraries and to identify and connect with our users. ANSER provides multiple interfaces to support different types of users, including students, computational scientists, and numerical library developers."
"1718698","CIF: Small: High-Dimensional Analysis of Stochastic Iterative Algorithms for Signal Estimation","CCF","Comm & Information Foundations","07/01/2017","07/03/2017","Yue Lu","MA","Harvard University","Standard Grant","Phillip Regalia","06/30/2020","$515,560.00","","yuelu@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7797","7923, 7935, 9251","$0.00","Optimization lies at the heart of modern signal and information processing. In recent years, the soaring quantity of information that is being acquired and becoming available makes computational and algorithmic issues increasingly important. This project contributes to an understanding of the fundamental limits of various stochastic optimization algorithms when dealing with high-dimensional data. Since such algorithms are the workhorse in many estimation, inference, and machine learning tasks, this research is well-posed to make significant and broad impact on many applications. Examples include real-time or low-latency medical image reconstructions, distributed computation on power grids, and the training of artificial neural networks for image understanding.<br/><br/>In this project, the PI studies a family of efficient stochastic iterative algorithms for solving large-scale convex and nonconvex optimization problems that arise in various signal estimation tasks. The broad goal in this project is to analyze the exact dynamics of these stochastic iterative algorithms in the high-dimensional limit. This asymptotic analysis provides a complete characterization of the typical behavior of the algorithms. The theoretical investigation draws upon techniques from the statistical physics of mean-field interactive particle systems, the weak convergence theory of stochastic processes, signal processing, information theory, and optimization. The theoretical analysis can be used to clarify the effectiveness of such stochastic methods for large-scale optimization and to establish their fundamental performance bounds. The insights obtained from the analysis can also be used to guide the design of new scalable algorithms to achieve optimal trade-offs between estimation accuracy, sample complexity, and computational complexity."
"1659142","CC* Network Design: Network Infrastructure to Support Computational Biology, Computational Science, and Computing Across the Curriculum","OAC","Campus Cyberinfrastructure","07/01/2017","12/21/2016","Bruce Maxwell","ME","Colby College","Standard Grant","Kevin Thompson","06/30/2020","$380,246.00","David Angelini, Charles Wray, Andrea Tilden, Bruce Segee","bmaxwell@colby.edu","4000 Mayflower Hill","Waterville","ME","049018840","2078594342","CSE","8080","9150","$0.00","The research partnership of Colby College, the University of Maine, and the Jackson Laboratory are building a dedicated research network to support projects in computational biology, computational physics and astrophysics, computational chemistry, and machine learning, The network is also expanding the capabilities available to faculty in the classroom, enabling more complex and modern classroom and laboratory activities.<br/><br/>The project provides 10Gb network connectivity to all academic buildings at Colby College to give all faculty access to state-of-the-art computing and data facilities, which is critical given the trends in all academic disciplines towards greater use of digital and computing resources. The project also creates a secure dedicated 10Gb research network connecting the Advanced Computing Group resources at the University of Maine and the large genomic databases at the Jackson Laboratory with researchers in variety of disciplines at Colby College.<br/><br/>The new research network provides the capability for researchers at Colby to obtain fast access to gigabyte and terabyte scale data sets from Jackson Laboratory or other sources and make use of advanced computing resources at the University of Maine with a high bandwidth and high availability connection. Using this infrastructure, researchers are able to execute analyses and simulations, and professors are able to assign projects and exercises in courses, that were not previously possible with the old infrastructure."
"1736196","HBCU-RISE: Bridging Quantitative Science with Biological Research: Jumpstarting Computational Systems Biology Research at PVAMU","HRD","Centers for Rsch Excell in S&T","09/01/2017","06/23/2017","Lijun Qian","TX","Prairie View A & M University","Standard Grant","Victor Santiago","08/31/2021","$1,000,000.00","Seungchan Kim, Pamela Obiomon, Xiangfang Li","liqian@pvamu.edu","P.O. Box 519","Prairie View","TX","774460519","9362611689","EHR","9131","9131","$0.00","The Historically Black Colleges and Universities Research Infrastructure for Science and Engineering (HBCU-RISE) activity within the Centers of Research Excellence in Science and Technology (CREST) program supports the development of research capabilities at HBCUs that offer doctoral degrees in science and engineering disciplines. HBCU-RISE projects have a direct connection to the long-term plans of the host department(s) and the institutional mission, and plans for expanding institutional research capacity as well as increasing the production of doctoral students in science and engineering. With support from the National Science Foundation, Prairie View A&M University (PVAMU) aims to provide innovative solutions to more effective and efficient drug development by bridging quantitative research with biomedical science. The project aims to 1) jumpstart computational biology research to stimulate students' interest and enhance the PhD program in Electrical Engineering, 2) improve student enrollment and retention, and 3) attract more minority students to pursue graduate study, especially doctoral degrees. This project is aligned with the mission of the institution and the goals of the Electrical and Computer Engineering (ECE) Department. The proposed activities will support the ECE department in building a strong research program in computational biology, thus achieving the goals of enhancing the PhD program in the ECE department and broadening participation in computational biology at PVAMU. The proposed project will greatly improve African American involvement in cutting edge research that is extremely valuable to the nation. <br/><br/>The aim of this project is to study and analyze the dynamic evolution of drug/cell interactions using biomedical big data, including both public domain data and dynamic time series data from systematic drug perturbations experiments. Innovative image processing, machine learning, dynamic modeling and control techniques are proposed to help understand the genetic regulation of cancer cells and the mechanism of action of molecularly targeted agents on gene regulation. Specifically, combining the information from robust image feature extraction using advanced image processing techniques (Thrust 1) with candidate drug targets and the identification of drug treatments identified using a novel network-based computational tool, Evaluation of Differential DependencY (EDDY; Thrust 2). Dynamic modeling and analysis of drug response in critical biological pathways will be carried out in Thrust 3. Equipped with the knowledge extracted from biomedical big data obtained in Thrust 2 and a predictive preclinical model that reveal how biological regulatory networks react when perturbed from time series data in Thrust 3, novel therapeutic interventions will be designed in Thrust 4 using advanced control theory. Findings from this study will provide innovative solutions to more effective and efficient drug development by bridging quantitative research with biomedical science. This project will be conducted in collaboration with the TEES-AgriLife Center for Bioinformatics and Genomics Systems Engineering (CBGSE) at Texas A&M University and the Translational Genomic Research Institute (TGen). The knowledge gained from this project will be disseminated broadly to a community of scientists and engineers."
"1736091","I-Corps: Electroencephalography based assessment of cognitive function","IIP","I-Corps","04/01/2017","03/29/2017","Andre Fenton","NY","New York University","Standard Grant","Steven Konsek","09/30/2017","$50,000.00","","afenton@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project arises from objective cognitive assessment aided by electroencephalography (EEG) technology. Stress, anxiety, irritability, lack of focus, emotional reactivity and problems with relationships amongst coworkers all impact productivity. Stress alone is responsible for up substantial healthcare costs in the U.S. Similarly, good mental health is associated with improved creativity, inspiration and mental clarity, improved relationships and reduced anxiety, improved collaboration and increased employee loyalty. This project aims to explore the commercial potential of including objective cognitive assessment as part of healthcare and wellness programs. Users would be monitored for signs of psychological distress and if detected, an intervention such as therapy, coaching or training would be suggested in order to minimize the impact of distress in its early stages. Objective cognitive assessment can have an impact on the overall productivity of employees and is likely to increase the mental capital of the organization.<br/><br/>This I-Corps project is based on electroencephalography (EEG) aided assessment of cognitive function and builds on decades of cognitive evaluation research, recent developments of biomedical signal acquisition and processing, and expertise in statistical analyses, data science, and machine learning. The proposed technology expands prior developments of both research and clinical devices for recording brain activity in diverse settings including emergency rooms, sports fields, military and aerospace. The technology also directly builds on experience with quantifying cognitive state of a person in a wide range of settings such as exercise, cognitive processing, and communication. Furthermore, it builds on a decade long efforts in cognitive assessment in animal models of epilepsy, schizophrenia, intellectual disability and autism."
"1738093","EAGER: SC2: A Simple, Robust, and Flexible Framework for Collaborative Spectrum Sharing","CNS","Information Technology Researc","04/01/2017","03/24/2017","Mihail Sichitiu","NC","North Carolina State University","Standard Grant","Thyagarajan Nandagopal","03/31/2019","$100,000.00","Ismail Guvenc","mlsichit@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","1640","7363, 7916","$0.00","Wireless communications are affecting deeply the daily lives of the overwhelming majority of people on this planet. The insatiable demand for network capacity has motivated an entirely different approach on spectrum utilization. The work in this project will enable new collaborative spectrum sharing approaches that will impact the lives of anybody relying directly or indirectly on wireless communications. <br/><br/>The main objective of this project is to research, develop, and implement technical solution concepts for collaborative spectrum sharing and to test their performance in head-to-head scrimmages with other competing approaches. The key research areas focus on agile, reconfigurable radios, RF environmental understanding, network wide contextualization, reasoning and collaboration with other collaborating, and yet competing, radio networks. The project will address key building blocks to create a collaborative intelligent radio using simple but robust techniques building on machine learning principles."
"1723612","I-Corps: Internet of Things Monitoring System","IIP","I-Corps","01/15/2017","01/13/2017","Eugene Wu","NY","Columbia University","Standard Grant","Steven Konsek","06/30/2017","$50,000.00","","ew2493@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will result in a better understanding of the need for internet connected devices and how the collected data can influence specific customer needs. If designed for firearms, this will be the first major, objective firearms data collection technology. For private security and insurance firms, this could reduce liability and increase cost savings on premiums. Among law enforcement, the data could usher in a new age of policing and firearms handling, building trust within communities while keeping officers safe. Researching the data could generate new methods of visualizing large amounts of data that could be applied to other data-centric fields and technologies. Both the data analytics and the firearms industry are multi-billion dollar markets and if implemented effectively, this will drive technological advancements in a rapidly growing market.<br/><br/>This I-Corps project will explore the possible applications of a device composed of a series of ruggedized sensors, built into the grips of a firearm, dedicated to providing real time firearms activity monitoring, including firearm location, orientation, and discharge monitoring. This is an install and forget device, independent of the firing mechanism (does not prevent discharges), that collects objective data on firearms usage and orientation. In turn, the data collected has a host of possible applications among security forces, ranging from augmenting critical first response systems to minimizing response times and improving situational awareness, to machine learning in automating radio transmissions and predictive firearm maintenance. Inventory control and firearms accountability are also possibilities with this potentially life-saving technology."
"1639683","EarthCube Data Infrastructure: Intelligent Databases and Analysis Tools for Geospace Data","ICER","AGS-ATM & Geospace Sciences, EarthCube","08/15/2017","08/21/2017","Alexander Kosovichev","NJ","New Jersey Institute of Technology","Standard Grant","Eva Zanzerkia","07/31/2020","$500,000.00","Gelu Nita, Vincent Oria","alexander.g.kosovichev@njit.edu","University Heights","Newark","NJ","071021982","9735965275","GEO","6897, 8074","026Z, 7433","$0.00","Solar activity and variability are among the key factors determining the state of the Earth?s atmosphere, global trends and climate changes. Explosive events in the form of high-energy radiation and mass ejections cause geomagnetic storms in the ionosphere and magnetosphere, affecting biological systems, disrupting power grids, and communications.  For understanding and predicting the complex and evolving Earth system, it is critical to investigate its coupling to the space environment and to solar variability. To facilitate interdisciplinary research on solar influences, PI and the team will develop a unique data environment that will integrate new and archived satellite and ground-based observational data. The integrated data environment will allow researchers to efficiently access solar and geospace data and use them for studying fundamental problems of solar activity and variability and their impacts on Earth systems, as well as for developing new predictive capabilities. The innovative interdisciplinary approach for building an intelligent integrated database, developed in collaboration between heliophysicists and computer scientists, will contribute to knowledge discovery in the EarthCube and associated fields. The proposed activities will facilitate the transfer of innovative data analysis, data visualization, and data-driven modeling techniques to in-class teaching at the undergraduate and graduate levels and to other fields of research that may benefit from a similar framework.<br/><br/>The primary goal is to develop tools for data access and analysis that can be easily used by the Geoscience community for studying and modeling various components of the coupled Earth system. The project will develop innovative tools to extract and analyze the available observational and modeling data in order to enable new physics-based and machine-learning approaches for understanding and predicting solar activity and its influence on the geospace and Earth systems. The geospace data are abundant: several terabytes of solar and space observations are obtained every day. Finding the relevant information from numerous spacecraft and ground-based data archives and using it is a paramount, and currently a difficult task. <br/><br/>The scope of the project is to develop and evaluate data integration tools to meet common data access and discovery needs for two types of Heliophysics data: 1) long-term synoptic activity and variability, and 2) extreme geoeffective solar events. The project will integrate existing data resources, such as the Heliophysics Knowledge Database (HEK), Solar Dynamics Observatory Joint Science Operations Center (SDO JSOC), Virtual Solar Observatory (VSO), Heliophysics Integrated Observatory (HELIO), and others.<br/> <br/>The methodology consists in the development of a data integration infrastructure and access methods capable of 1) automatic search and identification of image patterns and event data records produced by space and ground-based observatories, 2) automatic association of parallel multi-wavelength/multi-instrument database entries with unique pattern or event identifiers, 3) automatic retrieval of such data records and pipeline processing for the purpose of annotating each pattern or event according to a predefined set of physical parameters inferable from complimentary data sources, and 4) generation of a pattern or catalog and associated user-friendly graphical interface tools that are capable to provide fast search, quick preview, and automatic data retrieval capabilities"
"1705092","CSR: Medium: Optimal Control of Approximate Computing Systems","CNS","CSR-Computer Systems Research","10/01/2017","08/22/2017","Keshav Pingali","TX","University of Texas at Austin","Standard Grant","Matt Mutka","09/30/2021","$599,175.00","Donald Fussell","pingali@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7354","","$0.00","Computing is increasingly constrained by energy both at the low end in portable devices like cell phones and at the high end in large-scale data centers. Therefore, reducing the energy expended in computation is one of the most important problems facing Computer Science today. By taking computational shortcuts such as not executing certain portions of the program or executing them with lower accuracy, it is possible in many applications, for instance video processing, to reduce energy requirements without substantially affecting the quality of the output. The goal of the Capri project is to build a system that can optimize the energy consumption of a program in these ways while guaranteeing that the output quality stays within some bound specified by the programmer.<br/><br/>There are two major problems that must be solved: (i) building models to characterize the output quality and energy behavior of a program, and (ii) using these models to solve a constrained optimization problem to determine how approximation can be used. Capri will use state-of-the-art machine learning techniques to build program models for output quality and energy behavior, and will employ modern non-linear optimization algorithms to solve the constrained optimization problem. The project will produce scalable open-loop and closed-loop control systems for optimizing applications for energy efficiency. The project includes presenting tutorials on information-efficient computing at conferences like Principles and Practice of Parallel Programming (PPoPP) and High-Performance Computer Architecture (HPCA), and incorporate this material into classes and make it publicly available. All data and outputs produced by the Capri project will be made available at this website: http://iss.ices.utexas.edu/."
"1729783","DMREF: Collaborative Research on High throughput Exploration of Sequence Space of Peptide Polymers that Exhibit Aqueous Demixing Phase Behavior","DMR","DMREF","10/01/2017","08/18/2017","Rohit Pappu","MO","Washington University","Standard Grant","John Schlueter","09/30/2021","$266,212.00","","pappu@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","8292","054Z, 7433, 8004, 8400, 8990, 9150","$0.00","Non-technical Description: Proteins are incredibly adaptive molecules. For example, when exposed to a stimulus, such as an environmental change in temperature, pH or light, some proteins undergo conformational (shape) changes that can lead to useful material properties, such as a phase transition, turning from a solid to liquid, or a liquid to solid. Understanding and predicting how these changes occur on the molecular level could lead to the creation of an entire class of soft materials that can respond to environmental cues. This research will develop and apply computer algorithms to quickly go through large amounts of protein sequence data to predict as yet undiscovered temperature-sensitive peptide sequences. These sequences can then be subjected to computer-based modeling to predict conformational changes that ensue from a phase transition. Finally, experiments will be conducted to predict and determine the 2D and 3D materials architectures that can be created by combining stimulus-responsive peptide sequences. If successful, these methods could create a toolkit for the efficient design and fabrication of a large variety of materials with custom-designed properties.<br/><br/>Technical Description: Stimulus responsiveness is a striking feature of proteins in Nature, whereby responses to chemical stimuli such as ligand binding, phosphorylation, and methylation, and physical stimuli such as changes in temperature, pH, light, and salt concentration lead to sharp conformational or phase transitions. Unlike proteins, which encode diverse responses to numerous stimuli by richly sampling amino acid sequence space, current bioinspired designs of repetitive polypeptides have focused on a tiny fraction of the vast conceivable expanse of sequence space. The primary goal of the proposed research is thus to develop generalized materials design rules, by combining experiments, fast and accurate physics-based computer simulations, and data science, to accelerate the discovery and development of a potentially huge class of thermally-responsive polypeptide materials by a systematic exploration of sequence space. This research will ""for the first time"" provide a complete atomistic understanding of the determinants of the lower critical solution temperature (LCST) and upper critical solution temperature (UCST) phase behavior, enable de novo molecular design of LCST and UCST peptide polymers and identify rules on how to combine them to create hierarchically-ordered, nanostructured polypeptide materials that exhibit unique morphologies that can be tuned as a function of their stimulus responsiveness. These materials could serve as nanostructured scaffolds and templates and enable a broad range of biocatalytic, bioelectronic, or assay devices. The PIs also plan to release the PIMMS modeling package, a set of tools for performing lattice-based simulations of polymers. PIMMS will provide support for the machine learning algorithms that enable the design of responsive protein-based polymers. The PIMMS codebase will be released as open source. A user community will be coalesced around the language by ensuring that interested researchers are able to contribute modules to or implement application-specific algorithms within the codebase. This is expected to allow a wider growth of the project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1740996","BIGDATA: F: Collaborative Research: Foundations of Responsible Data Management","IIS","Big Data Science &Engineering","09/01/2017","08/21/2017","Bill Howe","WA","University of Washington","Standard Grant","Sylvia Spengler","08/31/2021","$364,975.00","","billhowe@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8083","7433, 8083","$0.00","Big Data technology promises to improve people's lives, accelerate scientific discovery and innovation, and bring about positive societal change. Yet, if not used responsibly, this same technology can reinforce inequity, limit accountability and infringe on the privacy of individuals: irreproducible results can influence global economic policy; algorithmic changes in search engines can sway elections and incite violence; models based on biased data can legitimize and amplify discrimination in the criminal justice system; algorithmic hiring practices can silently reinforce diversity issues and potentially violate the law; privacy and security violations can erode the trust of users and expose companies to legal and financial consequences. The focus of this project is on using Big Data technology responsibly -- in accordance with ethical and moral norms, and legal and policy considerations. This project establishes a foundational new role for data management technology, in which managing the responsible use of data across the lifecycle becomes a core system requirement. The broader goal of this project is to help usher in a new phase of data science, in which the technology considers not only the accuracy of the model but also ensures that the data on which it depends respect the relevant laws, societal norms, and impacts on humans. <br/><br/>This project defines properties of responsible data management, which include fairness (and the related concepts of representativeness and diversity), transparency (and accountability), and data protection. It complements what is done in the data mining and machine learning communities, where the focus is on analyzing fairness, accountability and transparency of the final step in the data analysis lifecycle, and considers the problems that can be introduced upstream from data analysis: during dataset selection, cleaning, pre-processing, integration, and sharing. This project develops conceptual frameworks and algorithmic techniques that support fairness, transparency and data protection properties through all stages of the data usage lifecycle: beginning with data discovery and acquisition, through cleaning, integration, querying, and ultimately analysis. The contributions are structured along three aims. Aim 1 considers responsible dataset discovery, profiling, and integration. Aim 2 considers responsible query processing and develops a general framework for declarative specification, checking and enforcement of fairness, representativeness and diversity. Aim 3 incorporates data protection into the lifecycle, develops techniques to facilitate sharing of sensitive data, and considers the tradeoffs between privacy and transparency. This project is poised to establish a multidisciplinary research agenda around responsible data management as a critical factor in enabling fairness, accountability and transparency in decision-making and prediction systems. Additional information about the project is available at DataResponsibly.com."
"1741254","BIGDATA: F: Collaborative Research: Foundations of Responsible Data Management","IIS","Big Data Science &Engineering","09/01/2017","08/21/2017","Gerome Miklau","MA","University of Massachusetts Amherst","Standard Grant","Sylvia Spengler","08/31/2021","$365,000.00","","miklau@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8083","7433, 8083","$0.00","Big Data technology promises to improve people's lives, accelerate scientific discovery and innovation, and bring about positive societal change. Yet, if not used responsibly, this same technology can reinforce inequity, limit accountability and infringe on the privacy of individuals: irreproducible results can influence global economic policy; algorithmic changes in search engines can sway elections and incite violence; models based on biased data can legitimize and amplify discrimination in the criminal justice system; algorithmic hiring practices can silently reinforce diversity issues and potentially violate the law; privacy and security violations can erode the trust of users and expose companies to legal and financial consequences. The focus of this project is on using Big Data technology responsibly -- in accordance with ethical and moral norms, and legal and policy considerations. This project establishes a foundational new role for data management technology, in which managing the responsible use of data across the lifecycle becomes a core system requirement. The broader goal of this project is to help usher in a new phase of data science, in which the technology considers not only the accuracy of the model but also ensures that the data on which it depends respect the relevant laws, societal norms, and impacts on humans. <br/><br/>This project defines properties of responsible data management, which include fairness (and the related concepts of representativeness and diversity), transparency (and accountability), and data protection. It complements what is done in the data mining and machine learning communities, where the focus is on analyzing fairness, accountability and transparency of the final step in the data analysis lifecycle, and considers the problems that can be introduced upstream from data analysis: during dataset selection, cleaning, pre-processing, integration, and sharing. This project develops conceptual frameworks and algorithmic techniques that support fairness, transparency and data protection properties through all stages of the data usage lifecycle: beginning with data discovery and acquisition, through cleaning, integration, querying, and ultimately analysis. The contributions are structured along three aims. Aim 1 considers responsible dataset discovery, profiling, and integration. Aim 2 considers responsible query processing and develops a general framework for declarative specification, checking and enforcement of fairness, representativeness and diversity. Aim 3 incorporates data protection into the lifecycle, develops techniques to facilitate sharing of sensitive data, and considers the tradeoffs between privacy and transparency. This project is poised to establish a multidisciplinary research agenda around responsible data management as a critical factor in enabling fairness, accountability and transparency in decision-making and prediction systems. Additional information about the project is available at DataResponsibly.com."
"1737813","Collaborative Research:  Improving Constraints on Tropical Climate Feedbacks with Inverse Modeling of the Stable Isotopic Composition of Atmospheric Water Vapor","AGS","Climate & Large-Scale Dynamics","08/15/2017","08/11/2017","Robert Field","NY","Columbia University","Standard Grant","Eric DeWeaver","07/31/2020","$41,388.00","","rf2426@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","GEO","5740","","$0.00","The relative humidity (RH) of air in the subtropics, meaning RH in the belts of subsiding air found on either side of the equator, is an important factor in determining the behavior of subtropical clouds and their effects on climate.  Air in the subtropics generally enters the region in the upper troposphere after ascending in the deep convective clouds found in the convergence zones near the equator.  The RH of the air is largely determined by the coldest temperature it encounters during in-cloud ascent, as condensation dries the air to the saturation moisture value given by its temperature (lower moisture content for colder air).  But other factors also influence the RH of the subsiding subtropical air, in particular the air can be moistened by mixing with air from more humid levels closer to the surface.  The same RH can be achieved either by a relatively warm last saturation temperature with little mixing or a relatively cold last saturation temperature followed by greater mixing,  The two pathways to the same RH can have different implications for cloud feedbacks and RH change in a warming climate.<br/> <br/>However, these two pathways can be distinguished by examining the relative concentrations of heavier isotopes of water vapor, water vapor in which one of the hydrogen atoms is replace by deuterium or the oxygen 16 atom is replace by oxygen 18.  Roughly speaking, the heavier forms of water vapor evaporate more sluggishly and condense more readily than ordinary H2O, an effect which depends on the temperature at which the evaporation or condensation takes place.  Thus, heavy isotopes contain important clues to understanding the processes which set subtropical RH and relate it to subtropical clouds and their climate feedbacks.<br/> <br/>With this motivation the PIs examine the isotopic concentration of water vapor in the subtropical mid-troposphere using satellite and ground-based observations as well as climate model simulations.  Ground-based observations include measurements taken by the lead PI on the Chajnantor Plateau in Chile (see AGS-1158582).  Much of the work is performed using an inverse modeling technique in which an optimal set of parameters (including last saturation temperature and vertical mixing, among others) is determined using a machine learning algorithm that mimics natural selection.  The inverse technique is advantageous in that it is computationally inexpensive and can be applied equally well to both observations and model output.<br/> <br/>The work has societal relevance as it can lead to a better understanding of the role of subtropical clouds in climate change, a central issue in efforts to anticipate the amount of warming caused by greenhouse gas increases.  In addition, the project includes an extensive education and outreach effort through the New Mexico Museum of Natural History and Science in Albuquerque.  The museum serves a large Hispanic and Native American population including both inner city and rural communities.  The effort engages elementary school students through a summer camp program, middle and high school students through a ""junior docent"" summer program, and middle and high school teachers through a professional development workshop.  In addition, the project provides support and training for a graduate student, thereby providing for the future workforce in this research area."
"1703782","SaTC: CORE: Medium: Collaborative: Privacy-Aware Trustworthy Control as a Service for the Internet of Things (IoT)","CNS","Secure &Trustworthy Cyberspace","09/01/2017","07/06/2017","Saman Aliari Zonouz","NJ","Rutgers University New Brunswick","Standard Grant","Phillip Regalia","08/31/2021","$341,371.00","","saman.zonouz@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8060","025Z, 7434, 7924","$0.00","The Internet of Things (IoT) includes a variety of devices such as smart appliances, cars, and other physical systems that are deeply embedded in our everyday lives and that are at risk from new kinds of threats to security and privacy from hackers or state actors. In IoT systems, sensors are used to probe the physical state of the system (e.g., temperature in a building or rotational speed of a wheel of a car) and then software control systems use algorithms to determine appropriate adjustments to the system (e.g., run the air conditioning for 5 minutes or apply the brakes). The project is focused on protecting  those control systems and algorithms to ensure security and privacy for users.<br/><br/>The research addresses trustworthy and privacy-aware control architectures for IoT through mechanisms drawn from control, cryptography, software, and hardware. These include: (i) A framework for formally reasoning about safety and privacy properties of control software in conjunction with dynamical models of the physical world and associated sensing and actuation channels; (ii) Lightweight domain-specific mechanisms, for policing flow of information through software applications, while leveraging the semantics of machine learning and control algorithms, physics of the system, and numerical properties; (iii) Enforcing desired safety and information leakage properties via a combination of principled sensor data perturbation, control algorithms optimized for efficient computation over encrypted data, and a hardware-supported trusted computing base tailored to protecting sensed data and control algorithm parameters; (iv) A resilient control and timing infrastructure that protects against attacks on timing information through a hybrid use of edge and cloud resources and physical models. The success of the mechanisms is being assessed on experimental testbeds for smart home, industrial automation and smart vehicles, but have broader applicability to many other IoT applications. The project team is also creating a new graduate class on IoT security and developing educational material on IoT security for high-schoolers through the  Los Angeles Computing Circle initiative at the University of California - Los Angeles."
"1700884","Asymptotics for Rational Points","DMS","ALGEBRA,NUMBER THEORY,AND COM","06/01/2017","06/17/2019","Jordan Ellenberg","WI","University of Wisconsin-Madison","Continuing Grant","Michelle Manes","05/31/2021","$360,000.00","","ellenber@math.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1264","","$0.00","The research scope of this grant lies on the active interface between number theory and geometry.  Geometry is perhaps the oldest part of mathematics, and number theory, the study of equations and their solutions in whole numbers, is hardly younger.  Yet it is only in the very recent history of mathematics that researchers have understood just how interrelated these subjects are and how much they have to offer each other.  One example is the ""cap set problem,"" related to the popular card game Set.  In the game, one asks:  how many cards is it possible to have on the table with no legal play?  It turns out that this problem has to do with the geometry of points and lines in 4-dimensional space, with equations among numbers and their last base-3 digit, and the relation between these.  In 2016 the PI was part of a major breakthrough on this old problem, and his proposed research will continue investigating the new ideas that led to progress as well as other projects mixing number theory and geometry.<br/> <br/>The proposal covers several areas in number theory, algebraic geometry, topology, combinatorics, and applied math, in collaboration with a wide group of fellow researchers, including graduate students.  One of the central questions of arithmetic statistics is: how many number fields are there of discriminant at most X? More particularly: how many of these have Galois group G for a specified subgroup G of a symmetric group S_n? A famous conjecture of Malle proposes a description for the asymptotic behavior of this count as X grows.   Many of the major themes in contemporary number theory (e.g. Bhargava's work on counting quartic and quintic extensions, progress on Cohen-Lenstra conjectures) concern cases of this conjecture. In previous work, the PI showed that the Cohen-Lenstra conjecture over the function field F_q(t) could be approached by the methods of algebraic topology, using Grothendieck's theory of etale cohomology as the bridge between the two subjects. Now the PI proposes to prove the upper bound in the Malle conjecture in the case K = F_q(t), again using a combination of topological and arithmetic methods, but now with input from the theory of quantum shuffle algebras.   In another project, the PI proposes to investigate the analogy between Malle's conjectures and the Batyrev-Manin conjectures, which study the asymptotics for rational points with bounded height on algebraic varieties. The height is a natural notion of complexity of an algebraic point just as the discriminant is for a number field.  Here, the technical bridge is the theory of algebraic stacks; the PI will develop a theory of rational points of bounded height on Deligne-Mumford stacks, which first of all requires defining the height of a point on a stack. In particular, the discriminant of a number field is the height (in the novel sense) of a point on the classifying stack of a finite group. The PI will formulate a generalized Batyrev-Manin conjecture for stacks, which specializes to both Malle's conjecture and the Batyrev-Manin conjecture. The PI will also investigate properties of the new definition: for instance, the PI will aim to prove that the Faltings height is actually height on the moduli stack of abelian varieties in this sense.  The PI also proposes problems in additive combinatorics, the homology of FI-modules, and the geometry of machine learning."
"1712642","Deterministic Sampling through Energy Minimization","DMS","STATISTICS","07/01/2017","05/14/2019","Roshan Joseph","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","06/30/2020","$199,999.00","","roshan@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","1269","7433, 8083","$0.00","This project aims at developing optimal deterministic methods for statistical sampling / statistical observations, in contrast to commonly-used random sampling methods such as Monte Carlo (MC) and Markov Chain Monte Carlo (MCMC). The MC/MCMC methods have revolutionized statistics, allowing statisticians to model and solve complex and high-dimensional problems that would have been intractable using conventional techniques. One drawback of these methods is that very many observations or data samples are needed due to the slow convergence rate inherent in random sampling. This becomes an issue when the sampling is expensive. The deterministic method under study in this project attempts to overcome this problem by sampling points more intelligently, so that the same information provided by a random sample can be obtained with fewer deterministic samples. This can significantly cut down the cost of sampling and subsequent computations. The method under development has applications in many fields, such as uncertainty quantification, computer experiments, and machine learning.<br/><br/>The project aims to provide deterministic samples obtained through the minimization of certain energies. The goal is to use carefully developed optimization techniques to reduce the number of expensive evaluations of a probability distribution, thereby reducing the overall computational cost. Furthermore, the deterministic sample provides a much better representative set of points for the distribution, which can further reduce the cost of subsequent computations involving integrals. Compared to the existing Quasi-Monte Carlo methods, which are mostly developed for sampling from the uniform hypercube, the methods under study are much more general and can be used to directly sample from any probability distribution. Two methods for deterministic sampling will be investigated. The first method, known as minimum energy designs, is useful when the probability density is expensive to evaluate. The second method, known as support points, is useful when the integrand is expensive but sampling from the probability density is easy. The minimum energy design possesses an important property: its empirical distribution asymptotically converges to the target distribution. This is a property not shared by some of the competing representative point sets in the literature, such as principal points. On the other hand, support points are obtained by minimizing an energy distance which is used for goodness-of-fit testing. In this light, support points can be viewed as point sets that optimally compact a continuous probability distribution. The project focuses on developing efficient optimization methods for these energy functions using as few function evaluations as possible, and improving the distributional properties of the point sets so that they can be used in problems where MC/MCMC methods are computationally impracticable."
"1651570","CAREER: A Runtime for Fast Data Analysis on Modern Hardware","CNS","CSR-Computer Systems Research","04/15/2017","03/20/2020","Matei Zaharia","CA","Stanford University","Continuing Grant","Marilyn McClure","03/31/2022","$467,196.00","","matei@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7354","1045","$0.00","The computer revolution that continuously transformed our society throughout the past 60 years happened because every year computer processors reliably became faster. Unfortunately, this trend has stopped.  New processors can no longer easily be made faster. Instead, new computer hardware uses parallelism or specialized components to achieve performance, which has made it much harder to build high-performance applications since most existing data processing systems run 10-100x slower than they could even on current processors and will have even more trouble on emerging hardware. To drive advances in information processing, computer systems that automatically map applications to emerging hardware are needed. This is a challenging intellectual problem.<br/><br/>This project proposes ""Weld"", a run-time for data-intensive parallel computation on modern hardware.  The project includes 2 main research thrusts:<br/>   <br/>     *An intermediate language (IL) for data-intensive computation that can capture common data-intensive<br/>applications but is easy to optimize for parallel hardware.  This language enables mapping workloads to diverse hardware like CPUs and GPUs. <br/>     *A runtime API that lets Weld dynamically optimize across different libraries used in the same program.   This API will allow Weld to perform complex optimizations like loop blocking across parallel libraries, unlocking speedups not yet possible.<br/><br/>Success of this project will result in the creation of software that automatically maps existing key data intensive applications (e.g., data analytics, machine learning and search) to emerging hardware devices and achieves a 10-100x speedup over current applications. Beyond producing new technology, this project will train the next generation of engineers in high performance processing, online teaching resources, and research mentoring for undergraduate and graduate students. Together, education and new technology may make industrial, scientific, and government users of big data 10-100x more productive and enable the next generation of knowledge-driven systems."
"1659748","REU Site: UNCC Crime Analytics - Research Experiences for Undergraduates","SMA","RSCH EXPER FOR UNDERGRAD SITES","03/15/2017","03/07/2017","Michael Turner","NC","University of North Carolina at Charlotte","Standard Grant","Josie S. Welkom","02/28/2021","$306,744.00","Matthew Phillips, Beth Bjerregaard","mgturner@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","SBE","1139","9250","$0.00","This project is funded from the Research Experiences for Undergraduates (REU) Sites program in the SBE Directorate. It has both scientific and societal benefits, and it integrates research and education. The Crime Analytics REU is a ten-week summer program designed to help students develop research skills and technical abilities in the multidisciplinary fields of data science and analytics, and to prepare them for graduate study or employment in a data-saturated economy following graduation.  Specifically, the program seeks to:  1) aggressively recruit a diverse subset of undergraduate students to actively participate in innovative research projects that apply analytic tools to make decisions about crime; (2) expose students to foundational analytic methods early in their educational careers in hopes of encouraging their involvement in data science and an analytics-based field of study; (3) expose and engage students to research opportunities that demand communication and cooperation across experts in traditional disciplinary boundaries; (4) enhance students' educational experiences by exposing them to workshops, seminars, and social activities that enrich their professional development and inspire them to pursue graduate school; and (5) foster shared interdisciplinary communication skills by offering opportunities to disseminate research findings via an annual undergraduate research symposium, annual professional conferences, and peer-reviewed journal outlets.<br/><br/>The Crime Analytics-REU site is modeled around a team-based pedagogical concept in which students will be selected and assigned to interdisciplinary research teams that will use cutting-edge data technologies to explore problems related to crime and criminal justice. Students will be exposed to data science skills that include working with large data sets, data mining and manipulation, machine learning, crime mapping, quantitative modeling, and data visualization. Students will also be taken to visit leaders in the data industry, such as SAS, Google, Apple, and the Charlotte-Mecklenburg Police Department's Crime Analysis lab. The research projects will culminate with the preparation and submission of a research manuscript for publication in an academic journal, and a public presentation of their research. Students will also participate in social and cultural enrichment activities as well as workshops designed to prepare them for graduate school and or professional employment in the data analytics field."
"1744471","EAGER: Exploring the Feasibility of Software Testing Techniques to Evaluate Fairness Algorithms in Software Systems","CNS","Software & Hardware Foundation, Secure &Trustworthy Cyberspace","09/01/2017","06/30/2017","Yuriy Brun","MA","University of Massachusetts Amherst","Standard Grant","Sol Greenspan","08/31/2018","$131,230.00","Alexandra Meliou","brun@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7798, 8060","025Z, 7434, 7916, 7944","$0.00","Today, software is making more automated decisions with societal impact. For example, software determines who gets a loan or gets hired, computes risk-assessment scores that help decide who goes to jail and who is set free, and aids in diagnosing and treating medical patients. The increased role of software in such decisions makes software fairness a critical property. As more societal functions operate in cyberspace, the importance of software fairness increases. This project evaluates the feasibility of using software testing technology to identify behaviors whose outputs are more favorable for certain inputs. Using testing in such a manner is aimed at capturing causal relationships between characteristics of the software inputs and the software behavior.  The approach is novel compared to typical machine learning classification techniques that analyze data but do not test the behavior of application software. The central idea is to identify causal relationships between software inputs and the way the software behaves, e.g., its outputs.<br/> <br/>Software testing enables conducting causal experiments consisting of running the software with nearly identical inputs that vary only in a key characteristic under test. Variations in that characteristic that affect behavior provide evidence of a causal relationship. Measuring such causal relationships requires test suites that focus on small variability in a key set of characteristics under test, while existing testing techniques focus on large variability that leads to greater coverage. As a result, existing techniques are ill-suited for measuring causal relationships and new technology is necessary. The result is the ability to test software for new properties for which no testing procedures existed."
"1758430","CRII: SCH: Characterizing, Modeling and Evaluating Brain Dynamics","IIS","CRII CISE Research Initiation, Smart and Connected Health, IntgStrat Undst Neurl&Cogn Sys","06/15/2017","09/15/2017","Ruogu Fang","FL","University of Florida","Standard Grant","Wendy Nilsen","04/30/2019","$157,504.00","","ruogu.fang@bme.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","026Y, 8018, 8624","7364, 8018, 8089, 8091, 8228, 9251","$0.00","Brain dynamics, which reflects the healthy or pathological states of the brain with quantifiable, reproducible, and indicative dynamics values, remains the least understood and studied area of brain science despite its intrinsic and critical importance to the brain. Unlike other brain information such as the structural and sequential dimensions that have all been extensively studied with models and methods successfully developed, the 5th dimension, dynamics, has only very recently started receiving systematic analysis from the research community. The state-of-the-art models suffer from several fundamental limitations that have critically inhibited the accuracy and reliability of the dynamic parameters' computation. First, dynamic parameters are derived from each voxel of the brain spatially independently, and thus miss the fundamental spatial information since the brain is ?connected?. Second, current models rely solely on single-patient data to estimate the dynamic parameters without exploiting the big medical data consisting of billions of patients with similar diseases. <br/><br/>This project aims to develop a framework for data-driven brain dynamics characterization, modeling and evaluation that includes the new concept of a 5th dimension - brain dynamics - to complement the structural 4-D brain for a complete picture. The project studies how dynamic computing of the brain as a distinct problem from the image reconstruction and de-noising of convention models, and analyzes the impact of different models for the dynamics analysis. A data-driven, scalable framework will be developed to depict the functionality and dynamics of the brain. This framework enables full utilization of 4-D brain spatio-temporal data and big medical data, resulting in accurate estimations of the dynamics of the brain that are not reflected in the voxel-independent models and the single patient models. The model and framework will be evaluated on both simulated and real dual-dose computed tomography perfusion image data and then compared with the state-of-the-art methods for brain dynamics computation by leveraging collaborations with Florida International University Herbert Wertheim College of Medicine, NewYork-Presbyterian Hospital / Weill Cornell Medical College (WCMC) and Northwell School of Medicine at Hofstra University. The proposed research will significantly advance the state-of-the-art in quantifying and analyzing brain structure and dynamics, and the interplay between the two for brain disease diagnosis, including both the acute and chronic diseases. This unified approach brings together fields of Computer Science, Bioengineering, Cognitive Neuroscience and Neuroradiology to create a framework for precisely measuring and analyzing the 5th dimension - brain dynamics - integrated with the 4-D brain with three dimensions from spatial data and one dimension from temporal data. Results from the project will be incorporated into graduate-level multi-disciplinary courses in machine learning, computational neuroscience and medical image analysis. This project will open up several new research directions in the domain of brain analysis, and will educate and nurture young researchers, advance the involvement of underrepresented minorities in computer science research, and equip them with new insights, models and tools for developing future research in brain dynamics in a minority serving university."
"1659334","Nonlinear Factor and Latent Variable Models","SES","Economics, Methodology, Measuremt & Stats","06/01/2017","03/23/2017","Susanne Schennach","RI","Brown University","Standard Grant","Cheryl Eavey","05/31/2020","$229,997.00","","smschenn@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","SBE","1320, 1333","","$0.00","This research project will create new nonlinear dimension-reducing data analysis methods. The methods will be designed to extract information regarding a few specific unobserved variables of interest from large amounts of observed but potentially error-contaminated data. This type of nonlinear approach is inherently challenging due to the risk that the data alone may not uniquely identify the features of the variables of interest unless carefully motivated assumptions are made and a detailed formal analysis is performed. The new nonlinear methods will generalize current linear dimension-reducing statistical analysis methods that are widely used in statistical surveys, economics, medical imaging, data compression, machine learning, and internet search engines. Developing efficient nonlinear extensions of these methods will advance data analysis capabilities in these fields considerably by enabling researchers to uncover intricate nonlinear relationships that are currently masked through the lenses of linear approaches. A graduate student will obtain valuable research education and training by playing an important role in the development and implementation of the new methods. Software developed from this project will be made publicly available.<br/><br/>The idea that the information contained in a large number of even imperfectly measured variables can be summarized by a small number of variables (the ""factors"" or ""principal components"") has been widely adopted in economics and statistics and is gathering even more attention with the increasing prevalence of ""big data."" Some of the methods to be developed can be seen as a unification and generalization of widely used classes of techniques, such as linear latent factor models, multiway array decomposition, and nonclassical measurement error models. Other methods to be developed generalize the widely used linear principal component analysis to nonlinear settings. The main questions addressed in this work are: What reasonable assumptions ensure that the distribution of the many observed variables uniquely determine the distribution of the specific unobserved variables of interest?  Can efficient numerical algorithms be devised to find this unique mapping between observed and unobserved distributions? Do the new methods reduce to existing methods in special cases? The approaches used to answer these questions draw from the fields of optimal transport, entropy maximization, operator theory, and measure theory."
"1728792","Collaborative Research: Mass Media and Representative Democracy","SES","Political Science","08/01/2017","07/18/2018","Stuart Soroka","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Brian Humes","07/31/2021","$151,051.00","","ssoroka@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","SBE","1371","1371","$0.00","General Abstract<br/>This project examines the role of mass media in the functioning of representative democratic government.  It focuses on the frequency, accuracy, and clarity of policy cues in media content.  It also explores how the public uses these cues to inform their preferences for policy in the US.  The investigators begin with three important propositions.  First, a responsive democratic public requires only basic levels of knowledge about policy and policy change. Second,the necessary information is (in some domains) readily available in media content.  Third, citizens pick up on these cues and adjust their policy preferences accordingly. This project tests these propositions, leveraging differences in media coverage across policy areas drawing upon multiple news outlets in different regions of the country to understand variation in the opinion-policy link.  The investigators hypothesize that there are areas where media coverage provides a reasonably accurate view of policy, and others in which media content is lacking. The accuracy of public perceptions of, and preferences for, policy should benefit or suffer accordingly.  This project provides a unique, ""big"" data-driven investigation into how (and when, and why) representative democracy works. <br/><br/>Technical Abstract<br/>The centerpiece of this project is an automated content-analytic dataset comprised millions of news stories and television news transcripts on US public policy, across a range of spending areas, over the past 35 years.  News content is drawn from full-text databases (primarily Lexis-Nexis) and analyzed using both dictionary and machine-learning approaches.  The reliability and validity of the method is tested through human coding. These content-analytic data are examined alongside existing budgetary time series and opinion polling.  Analyses offer the first large-scale exploration into the kinds of policy information that do, and do not, appear in media coverage. This will allow the team to assess how the policy preferences of individuals might change depending on the policy-relevant media content they consume.  As part of the assessment of this influence, the team will provide a direct comparison of media coverage of policy and actual policy across eight policy areas.  Results provide a new and unique view into the role that media play in representative democracy, particularly in public responsiveness to policy, across issue areas and over time."
"1704074","III: Medium: High-Performance Factorization Tools for Constrained and Hidden Tensor Models","IIS","Info Integration & Informatics","09/01/2017","07/14/2020","George Karypis","MN","University of Minnesota-Twin Cities","Continuing Grant","Sylvia Spengler","08/31/2021","$1,517,014.00","Nikolaos Sidiropoulos","karypis@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7364","7364, 7924","$0.00","Tensors generalize matrices to higher dimensions (called modes) and are designed to model multi-way data. Tensor factorization algorithms analyze such multi-way data to uncover relations between the different modes that can be used to both gain insights and to predict unknown aspects of the underlying system/process. For example, medical diagnosis and treatment records can be modeled via a four-mode tensor whose modes correspond to patients, physicians, diagnosis, and treatments and its factorization can provide insights on the co-occurrence of medical conditions, treatment approaches, any treatment differences based on the physician, and identify potential instances of medical fraud. This project's research is designed to address current limitations of tensor analysis by developing new theory and algorithms and high-performance scalable parallel formulations of the various computational kernels used by these algorithms, and a flexible open source software toolkit that can be used to perform constrained and hidden tensor factorization of very large and sparse multi-way datasets. The success of this project will allow researchers to leverage the power of multi-way ``Big Data'' analysis to solve various problems in diverse application domains such as healthcare, medical imaging, cybersecurity, social and behavioral sciences, and e-commerce. At the same time, the project will provide data science training to the students involved by combining cutting-edge data and signal analytics, data mining, and high-performance computing.<br/><br/>Constrained matrix and tensor factorization techniques are widely used for dimensionality reduction, clustering, and estimation in machine learning, signal processing, and many other walks of science and engineering. Unconstrained matrix and tensor factorization algorithms are relatively mature, but constrained counterparts are lagging in terms of speed, scalability, and flexibility. In many applications (e.g., medical imaging and recommender systems), instead of observing the actual entries of a tensor, we observe a limited number of linear combinations (e.g., partial sums) of these entries and need to identify the tensor's latent factors from these measurements. Being able to directly identify the latent factors from linear measurements, which we refer to as hidden tensor factorization, has important advantages in terms of complexity, memory footprint, and the ability to handle very large data sets. Developing open source high-performance parallel tools for constrained and hidden tensor factorization in both shared- and distributed-memory systems will significantly enhance the ability to analyze very large multi-way data. The research will evolve along two synergistic thrusts. First, it will develop new theory and algorithms for constrained and hidden tensor factorization by (i) building fast first-order (FFO) and fast stochastic first-order (FSFO) constrained tensor decomposition algorithms that strike favorable trade-offs between simplicity, scalability, and speed of convergence, and (ii) tackling important identifiability and algorithmic issues related to hidden tensor factorization. Second, it will undertake a multi-pronged effort towards developing high-performance parallel formulations for the computational kernels used in constrained and unconstrained tensor and hidden tensor factorization and develop a high-performance tensor factorization software toolbox. The release of the high-performance tensor factorization toolbox will enable researchers and practitioners to scale up not only the size of data but also the variety of constraints and types of data they can analyze. The research will involve students that will be trained in data science, combining cutting-edge signal and data analytics, data mining, and high-performance computing."
"1715200","CHS: Small: Recommender and Decision-Making Systems Seeking Compatible Sets from Multiple Collections under Varying Constraints","IIS","HCC-Human-Centered Computing","10/01/2017","05/15/2018","Lucy Dunne","MN","University of Minnesota-Twin Cities","Standard Grant","William Bainbridge","09/30/2020","$507,523.00","","ldunne@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7367","7367, 7923, 9178, 9251","$0.00","Recommender systems that assist in decision-making are increasingly used in a number of information settings.  Most research in this area focuses on algorithms that predict how much a user will prefer a given item based on their own and others' past preferences.  Such research tends to treat the question as a machine learning, statistical, or optimization problem; however, this abstracts away important aspects of making recommender systems useful to people, including attending to the decision-making processes and contexts being supported.  This project will develop recommender systems that explicitly attend to these concerns using a smart home as the driving domain.  The methods developed for reasoning in this context can inform similar decision problems in other domains, such as managing supplies and logistics and accessing information. <br/>        <br/>This project will develop solutions to a number of important but under-addressed challenges for recommender systems, including (1) recommending groups of related items that work well together, rather than individual items; (2) recommending and re-recommending items primarily from an existing set of items rather than focusing on single-use consumption of new items; and (3) considering important factors about both the physical and social environment that affect decision making.  The project will explore these issues in the context of ensemble selection decisions in a smart home.  The system will be grounded in case-based reasoning; this will mitigate the cold start problem that plagues ratings-based collaborative filtering algorithms and fits well with the planned attribute-based representations of items, preferences, and context.  To do this, the team will use 3D scanners and rendering software to create personalized models of items, and both experts and crowdworkers will then assess the model outputs, accounting for social and physical contexts.  This will generate a case library that, along with rulesets for combining items, can be used to generate recommendations for both sets of items and individual items that fit well with already-chosen ones.  The system will be based on an existing prototype, adapted to run on a tablet to support remote data collection.  The deployment includes a setup process where the team will attach RFID tags to items and photograph them for later analysis and addition of attributes; a three-month period in which users' choices and context will be logged but no recommendations are made, and a six-month period where recommendations are offered.  The system will be evaluated in terms of the usefulness of the recommendations, their ability to suggest novel and liked combinations, and their effect on the use and recombination of users? existing items. Through deploying prototype systems in homes, the work will also give insight into designing smart home applications such as activity and medication monitoring that involve object tracking."
"1728558","Collaborative Research: Mass Media and Representative Democracy","SES","Political Science","08/01/2017","07/18/2018","Christopher Wlezien","TX","University of Texas at Austin","Continuing Grant","Brian Humes","07/31/2021","$88,248.00","","wlezien@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","SBE","1371","","$0.00","General Abstract<br/>This project examines the role of mass media in the functioning of representative democratic government.  It focuses on the frequency, accuracy, and clarity of policy cues in media content.  It also explores how the public uses these cues to inform their preferences for policy in the US.  The investigators begin with three important propositions.  First, a responsive democratic public requires only basic levels of knowledge about policy and policy change. Second,the necessary information is (in some domains) readily available in media content.  Third, citizens pick up on these cues and adjust their policy preferences accordingly. This project tests these propositions, leveraging differences in media coverage across policy areas drawing upon multiple news outlets in different regions of the country to understand variation in the opinion-policy link.  The investigators hypothesize that there are areas where media coverage provides a reasonably accurate view of policy, and others in which media content is lacking. The accuracy of public perceptions of, and preferences for, policy should benefit or suffer accordingly.  This project provides a unique, ""big"" data-driven investigation into how (and when, and why) representative democracy works. <br/><br/>Technical Abstract<br/>The centerpiece of this project is an automated content-analytic dataset comprised millions of news stories and television news transcripts on US public policy, across a range of spending areas, over the past 35 years.  News content is drawn from full-text databases (primarily Lexis-Nexis) and analyzed using both dictionary and machine-learning approaches.  The reliability and validity of the method is tested through human coding. These content-analytic data are examined alongside existing budgetary time series and opinion polling.  Analyses offer the first large-scale exploration into the kinds of policy information that do, and do not, appear in media coverage. This will allow the team to assess how the policy preferences of individuals might change depending on the policy-relevant media content they consume.  As part of the assessment of this influence, the team will provide a direct comparison of media coverage of policy and actual policy across eight policy areas.  Results provide a new and unique view into the role that media play in representative democracy, particularly in public responsiveness to policy, across issue areas and over time."
"1740833","TRIPODS: From Foundations to Practice of Data Science and Back","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC","09/01/2017","09/04/2019","John Wright","NY","Columbia University","Continuing Grant","A. Funda Ergun","08/31/2020","$1,500,000.00","Qiang Du, David Blei, Daniel Hsu, Alexandr Andoni","jw2966@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","041Y, 1253","047Z, 062Z","$0.00","In recent decades, scientific and technological fields have experienced ""data moments"" as researchers recognized the potential of drawing new types of inferences by applying techniques from computational statistics and machine learning to ever-growing datasets. At the same time, everyday life is increasingly saturated with products of data analysis: search engines, recommendation systems, autonomous vehicles, etc. These developments raise fundamental methodological questions, including how to collect and pre- pare data for analysis, and how to transform statistical inferences into effective action and new statistical inquiries. To address these questions, it is necessary to develop theoretical foundations for the practice of data science, and to provide practitioners with sound and practically relevant methodological training. The Columbia TRIPODS Institute pursues these goals through an integrated program of research in data science foundations, curriculum development, and center-building activities. The research program seeks to provide theoretical understanding of practical heuristics, develop modular and well-structured toolkits of computational primitives for data science, and to support the entirety of the data science cycle, from data collection and annotation, to the assessment of the analysis product. <br/><br/>The Institute pursues programs of research, education and center-building aimed at articulating theoretical foundations for data science. Its activities aim to have a major impact in shaping this emerging field. The research directions include understanding tractable classes of optimization problems, developing primitives that support efficient computation on data, and developing methodological foundations for interactive protocols in data science. These directions address challenging problems at the interface between theory and practice, the solutions of which require ideas spanning mathematics, statistics, and computing. The educational activities articulate model curricula in data science at the MS/professional and PhD levels, including interdisciplinary courses aimed at building a common language for a new generation of scientists and engineers. Center building activities are organized around cross-disciplinary themes and structured to encourage interaction across disciplines and to develop a common methodological community in Foundations of Data Science. These research and educational activities-including workshops, summer schools, distinguished lecture series, long-term visits, and outreach- help to further define and disseminate a common language for foundational research and education, and to increase diverse participation in data science. Located within the Center for Foundations of Data Science in the Data Science Institute at Columbia University, the Institute is at the center of the Northeast Big Data Hub. This position supports expansion of activities within Columbia, and also with other Big Data Hub members, TRIPODS Institutes, and research/industry organizations.  Funds for the project come from CISE Computing and Communications Foundations, CISE Information Technology Research, MPS Division of Mathematical Sciences, and MPS Office of Multidisciplinary Activities."
"1738293","Design of Motion-Artifact Robust Electronic Tattoos and Software Reconfiguration Methodologies for Bio-impedance Sensing","CNS","Special Projects - CNS, CSR-Computer Systems Research","09/01/2017","05/29/2020","Roozbeh Jafari","TX","Texas A&M Engineering Experiment Station","Standard Grant","Erik Brunvand","08/31/2020","$316,000.00","Deji Akinwande, Nanshu Lu","rjafari@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","1714, 7354","7354, 7916, 9251","$0.00","Electronic-tattoos (e-tattoos) are ultra-thin, ultra-soft sensors and electronics that can noninvasively adhere to human skin like a temporary transfer tattoo. Compared to the state-of-the-art wearable electronics, e-tattoos offer several exceptional characteristics. First, they conform to the skin and create a tight contact with the human body enabling robust signal measurements. Second, they may allow the skin to breathe eliminating the adverse effect of traditional adhesive patches. Lastly, they do not constrain natural skin motion hence present high degrees of comfort for the user. In other words, the user may ""put it on and forget about it"". E-tattoos are poised to enable new opportunities for the next generation of ubiquitous, unobtrusive and cost-effective health and wellness monitoring impacting the national health, bringing personalized care the individuals need to their homes. A strategic education and outreach effort focuses on broadening the participation of underrepresented groups in science and engineering via a year-long undergraduate research experience with enhanced graduate school preparation in partnership with Texas A & M University EnMed program and University of Texas-Austin NASCENT NSF Engineering Research Center. <br/><br/>Emerging bio-impedance sensing offers new paradigms to capture a number of important physiological signals including heart rate, respiration rate and blood pressure, all around the human wrist. As the most dynamic body part, the wrist is under constant movement. The major challenge in bio-impedance sensing is the negative effects of motion artifacts that corrupt the data, degrade the signal fidelity, and prevent decision making with sufficient confidence. Our project leverages ultra-thin and ultra-soft e-tattoos for bio-impedance sensing on the wrist because e-tattoos enable the most intimate but noninvasive coupling for the electrodes and the human skin, even under severe skin deformation. Our project also explores software reconfiguration methodologies and machine learning techniques to further address the challenges. In particular, we investigate: 1) design and development of an array of submicron-thick, skin-conformable graphene electrode tattoos for the first time, and 2) novel reconfiguration techniques that would eliminate or reduce the noise associated with motion artifacts and enhance the signal fidelity."
"1729420","DMREF: Collaborative Research: Transforming Electrocatalysis using Rational Design of Two Dimensional Materials","CBET","DMREF","09/01/2017","07/23/2020","Amin Salehi-Khojin","IL","University of Illinois at Chicago","Standard Grant","Robert McCabe","08/31/2021","$1,164,114.00","Robert Klie","salehikh@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","ENG","8292","8396, 8398, 8400, 9263","$0.00","The project will employ an integrated theoretical and experimental approach to rapidly discover highly efficient catalysts, based on two-dimensional (2D) materials in contact with an ionic liquid, for a variety of electrochemical reactions of importance for sustainable energy generation, chemicals manufacturing, environmental remediation, and energy storage.  Electrocatalysis - as employed in energy storage and conversion devices such as advanced batteries, fuel cells, photovoltaics, and chemical electrolyzers - is becoming an increasingly important alternative to conventional thermal catalysis, but needs further improvements in efficiency, cost reduction, and chemical selectivity for wide-scale commercial implementation.  The project will address those needs by combining first-principles density functional theory calculations, including solvent interaction effects and theory-guided machine learning, to identify new ionic-liquid electrolytes and low-cost 2D materials capable of displacing existing thermal processes with electrocatalytic processes utilizing renewable and/or sustainable resources.  Educational and outreach components of the project will focus on preparing both graduate and undergraduate students for the workforce needed to realize advanced energy technologies.  Emphasis at both universities will be placed on the recruitment of minority and underrepresented student populations through existing programs including the Minority Engineering Recruitment and Retention Program (MERRP) at the University of Illinois-Chicago, and the Office of Undergraduate Research at Washington University.  The theoretical and experimental dataset on 2D materials generated in the study, along with relevant computational codes, will be disseminated to the broader research community through on-line repositories.<br/><br/>Two-dimensional transition metal dichalcogenides (TMDCs) in contact with ionic liquid (IL) electrolytes will be used as the starting materials offering a new paradigm for electrocatalysis based on materials with low work function, significant overlap of the d-band partial density of states with the Fermi energy, and an electrolyte 'solvent' that protects rather than poisons the catalytic sites.  Novel material combinations and structures will be predicted using computational tools and then synthesized using chemical vapor deposition, chemical vapor transport and colloidal chemistry. Atomic and electronic structure will be characterized using in-situ aberration-corrected scanning transmission electron microscopy (STEM).  The information obtained from high-resolution STEM, including high-angle annular dark-field (HAADF) and annular bright-field (ABF) imaging, as well as electron energy loss spectroscopy (EELS) and energy dispersive X-ray spectroscopy (XEDS), will be used to confirm successful synthesis of the desired structures and to create starting configurations for the first-principles modeling efforts.  Both ex-situ and in-situ electrochemical experiments will be conducted to measure the activity and selectivity of the synthesized materials.  In particular, the study will utilize a novel graphene liquid cell, developed by one of the investigators, that enables atomic-resolution imaging and spectroscopy in a liquid environment. Mechanistic studies of the electrocatalytic reactions and transport measurements will be made utilizing in-situ differential electrochemical mass spectrometry (DEMS) together with a traditional silicon nitride based electrochemical stage for STEM characterization under operando conditions.  Taken together, the advanced synthesis, characterization, and evaluation techniques, coupled with efficient computational search methods, will accelerate the discovery of 2D material-based-catalysts with superior activity and selectivity for various electrochemical reactions including the oxygen reduction reaction (important in fuel cell technology), and the hydrogen evolution reaction (important in water electrolysis)."
"1730661","CI-SUSTAIN: Sustainable Tools for Analysis and Research on Darknet Unsolicited Traffic (STARDUST).","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2017","05/29/2020","Alberto Dainotti","CA","University of California-San Diego","Standard Grant","Ann Von Lehmen","09/30/2021","$780,459.00","Alistair King","aldainotti@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1714, 7359","7359, 9251","$0.00","The Internet is a vast enterprise carrying information to people and devices all over the world.  The University of California San Diego Network Telescope (UCSD-NT) is a unique monitoring system which, for over a decade, has enabled global visibility into Internet behavior that few other data sources can offer.  It has provided relevant data used in a broad set of sub-disciplines in Computer & Information Science & Engineering (CISE) and beyond, from network and systems security and stability, to machine learning and big data processing techniques, and, most recently, for studies of cyberwarfare.  The STARDUST project will help sustain operation and extend the applicability of the UCSD-NT infrastructure.  The stabilized and enhanced infrastructure capabilities will serve a diverse range of academic researchers, the vast majority of whom have no access to any other source of global Internet traffic data.  In addition, the proposed enhancements will support invaluable hands-on experience in operationally relevant network security and traffic analysis research for a wide audience of computer science students.<br/><br/>STARDUST aims at maintaining continued operation of the UCSD-NT infrastructure while maximizing its utility to researchers from various disciplines.  The scope of work includes upgrading and modernizing the current infrastructure to handle the predicted growth in traffic, leveraging virtualization and NSF-funded High-Performance Computing (HPC) platforms at the San Diego Supercomputer Center for computational data analysis, and introducing meta-data semantics to simplify many tasks researchers typically want to do with UCSD-NT data.  The proposed modifications will leave researchers more time (and available HPC resources) to focus on their specific scientific questions.  Moreover, the project will forge an interdisciplinary collaboration between researchers from the field of computer networks and HPC scientists and engineers to experiment with novel approaches for research on live traffic analysis.  Project results will contribute to advancing knowledge in diverse CISE disciplines, e.g., facilitating the development of efficient strategies for early detection and mitigation of cyber attacks, supporting macroscopic Internet performance and reliability assessments, and opening a new domain for the application of live streaming big data analysis techniques."
"1734627","NRI: FND: Bioinspired Design and Shared Autonomy for Underwater Robots with Soft Limbs","IIS","NRI-National Robotics Initiati","08/15/2017","04/13/2018","Yigit Menguc","OR","Oregon State University","Standard Grant","David Miller","07/31/2021","$694,583.00","Camille Palmer, Geoffrey Hollinger","yigit.menguc@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","8013","8086, 9251","$0.00","Currently, delicate and challenging underwater manipulation tasks are performed only by human divers when it is safe for them to enter the environment. Robots can help address this issue by distancing human operators from dangerous environments, while still leveraging their skills in planning complex manipulation tasks. Rigid robotic manipulators, however, are typically not suitable for delicate manipulation tasks.  To address this limitation, this project explores the design and control of soft robotic arms inspired by the octopus.  Soft robots can perform delicate tasks of scientific interest in underwater environments, such as retrieving biological samples or delicate artifacts, without harming them.  Such soft robots can also be safer when operating alongside humans. The results of the research will be integrated into hands-on courses as part of the Oregon State University PhD in Robotics and into K-12 outreach via the OSU Robotics Club and Oregon FIRST Robotics competitions.<br/><br/>The objective of this research is to establish a framework for underwater manipulation, combining shared autonomy between human operators and robots with mechanically-directed soft actuation and sensing. The proposed work will examine new actuator morphologies, alternative fabrications techniques, and the use of stretchable integrated liquid metal sensors. To control the soft grippers, this project develops a planning and control interface that utilizes machine learning techniques to leverage human operators' skills at quickly identifying stable grasps. The physical attributes of the soft grippers will be designed in tandem with algorithms, which will provide improved understanding of underwater interaction and shared autonomy.  Dexterity and compliance of the soft manipulators will be evaluated for large contact-area, multi-point gripping, which is particularly advantageous for grasping delicate objects underwater.  Testing will be done in a benchtop underwater test bed, using kinematic motion capture and interaction forces to evaluate performance.<br/>"
"1657199","CRII: SaTC: ExHume: An Empirical Approach to Program Analysis for Security","CNS","CRII CISE Research Initiation","02/01/2017","01/27/2017","Brendan Dolan-Gavitt","NY","New York University","Standard Grant","Sol Greenspan","01/31/2021","$174,952.00","","bd52@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","026Y","025Z, 8228","$0.00","As software controls an ever-increasing number of devices that perform critical tasks, their security and robustness are of paramount importance to society. In addition, malicious software and ""greyware"" that violates the privacy of users causes billions in damage every year and erodes public confidence in computing systems. However, despite decades of work on techniques to analyze software with automated techniques, it remains a difficult and largely manual task. The ExHume project aims to address this issue by collecting detailed information about both normal and erroneous behaviors of a large corpus of real-world software. This behavioral information will then be used to inform a new generation of empirically guided program analysis techniques, with the goal of significantly improving software vulnerability discovery and reverse engineering techniques.<br/><br/>The technical approach taken by ExHume is to capture in vivo traces of software using low-overhead record and replay techniques. This corpus, which will consist of normal and buggy executions of benign and malicious software, can then be mined to extract a wide variety of detailed, dynamic information about the runtime properties of the software under test. Specifically, the data gathered will be used to create (a) new heuristics for guiding program exploration, which can be used for symbolic execution and guided fuzz testing; (b) methods for identifying previously unseen implementations of known functionality and algorithms; and (c) a set of domain-specific dynamic taint analyses that can aid in explicating the features used by image processing and machine learning algorithms."
"1663840","PREEVENTS Track 2: Collaborative Research: Developing a Framework for Seamless Prediction of Sub-Seasonal to Seasonal Extreme Precipitation Events in the United States","ICER","PREEVENTS - Prediction of and","08/01/2017","08/08/2018","Elinor Martin","OK","University of Oklahoma Norman Campus","Continuing Grant","Justin Lawrence","07/31/2022","$1,842,563.00","Michael Richman, Renee McPherson, Cameron Homeyer, Jason Furtado","elinor.martin@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","GEO","034Y","9150","$0.00","PREEVENTS Track 2: Collaborative Research: Developing a Framework for Seamless Prediction of Sub-Seasonal to Seasonal Extreme Precipitation Events in the United States<br/><br/>Extreme precipitation is a natural hazard that poses risks to life, society, and the economy. Impacts include mortality and morbidity from fast-moving water, contaminated water supplies, and waterborne diseases as well as dam failures, power and transportation disruption, severe erosion, and damage to both natural and agro-ecosystems. These impacts span several sectors including water resource management, energy, infrastructure, transportation, health and safety, and agriculture. However, on the timeframe required by many decision makers for planning, preparing, and resilience-building ""subseasonal to seasonal (S2S; 14 to 90 days)"" forecasts have poor skill and thus adequate tools for prediction do not exist. Additionally, societal resilience to these events cannot be increased without established, two-way communication pathways between researchers, forecasters, and local or regional decision makers. Therefore, the goal of this project is to enhance scientific understanding of S2S extreme precipitation events, improve their prediction, and increase communication between research and stakeholder communities with regard to such events. The overarching results will be the development of predictive models that have the potential to reduce mortality, morbidity, and damages caused by S2S extreme precipitation events and broadening participation in science by including federal, tribal, and local stakeholders. Three targeted user communities; water resource managers, emergency managers, and tribal environmental professionals will engage throughout the project duration via workshops. The co-production of knowledge will steer the science to focus on useful characteristics that matter most to the people who use and rely on predictions, thus contributing to knowledge-sharing and improving the capability to predict what is meaningful.<br/><br/>This project will enhance fundamental understanding of the large-scale dynamics and forcing of S2S extreme precipitation events in the U.S. and improve capability to model and predict such events. This project assembles an expert team of scientists and stakeholders to narrow the prediction gap of S2S extreme precipitation events by answering four scientifically and societally relevant research questions: 1) What are the synoptic patterns associated with, and characteristics of, S2S extreme precipitation events in the contiguous U.S.? 2) Do large-scale modes of climate variability modulate these events? If so, how? 3) How predictable are S2S extreme precipitation events across temporal scales? and 4) How do we create an informative prediction of S2S extreme precipitation events optimized for policymaking and planning? To answer these questions, this project will for the first time, combine observations with novel machine-learning techniques, high-resolution radar composites, dynamical climate models (the National Multi-Model Ensemble and the Coupled Model Intercomparison Project phase 5), and workshops that engage stakeholders in the co-production of knowledge. This project will identify the fundamental weather and climate processes that are tied to S2S extreme precipitation events across the U.S. from scales as small as individual storms to those as large as ocean basins. The prediction skill for S2S extreme precipitation events will be improved through an increased mechanistic understanding of historical events and a quantitative evaluation of model performance for simulating these events and their characteristic patterns. The statistical and co-production frameworks developed in this project will have the flexibility to be applied across meteorological extremes and timescales, in other global regions, with future climate model simulations, and with other stakeholder communities to reduce the impact of and increase resilience to extreme meteorological events."
"1738541","STTR Phase II:  Automated system for creating custom three-dimensional radiofrequency ablation lesion geometries in post-lumpectomy margin ablation breast cancer treatment","IIP","STTR Phase II","09/15/2017","08/22/2019","Robert Rioux","IL","Innoblative Designs","Standard Grant","Alastair Monk","02/29/2020","$680,659.00","Alan Sahakian","bob@innoblative.com","4660 N Ravenswood Avenue","Chicago","IL","606404510","4252102646","ENG","1591","1591, 169E, 5345, 8042","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase II project will focus on the design of the first completely automated radio frequency ablation (RFA) system for breast tissue. Of the  approximately 200,000 breast cancer patients in the US who elect lumpectomy annually, over 20% must undergo re-operation due to lack of clear margins. Furthermore, many breast cancer patients who are treated with lumpectomy require expensive adjuvant radiation therapy that can last up to 7 weeks causing time away from work and family. Fortunately, performing intraoperative RFA of the lumpectomy cavity has been shown to decrease the need for re-operations and radiation therapy by sterilizing tumor margins. However, no RFA devices are designed for post-lumpectomy breast cavities or breast tissue. The RFA system being developed in this project will automate post-lumpectomy RFA, accurately and precisely delivering energy based on local tissue properties and desired ablation depth in three dimensions thus customizing treatment for each patient. Eventual commercialization of this system could provide early stage breast cancer patients new treatment options that improve quality of life, reduce burdens of care, and costs while providing breast cancer recurrence control.   <br/><br/>The proposed project aims to develop a system (control unit and device) for optimal post-lumpectomy RFA. The proposed device is designed to mechanically fit the post-lumpectomy cavity for near-perfect tissue contact. The control unit includes advanced algorithms that utilize machine learning to create a three-dimensional ablation status map of each margin (ablation vs in-ablated) for controlling ablation. The combined system allows the surgeon to customize and monitor the three-dimensional ablation profile and automates therapy delivery to ensure accurate, precise ablation results. The objectives propose gathering device requirements, identifying critical tasks of an automated RFA procedure, and improving the system algorithm by collecting training data in cadaveric and prophylactic mastectomy specimens. The intellectual merits proposed are: (1) an automated ablation system implemented on an embedded microprocessor and co?]processor FPGA capable of ablation shape estimation and control; (2) a system that demonstrates clinical relevance through successful ablation in human tissue and surgeon usability."
"1829826","Network Comparison, a Cornerstone of the Foundations of Network Science","DMS","CDS&E-MSS","10/01/2017","07/13/2020","Laurent Hebert-Dufresne","VT","University of Vermont & State Agricultural College","Standard Grant","Christopher Stark","08/31/2021","$122,120.00","","lhebertd@uvm.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","MPS","8069","8083, 9150, 9263","$0.00","""Big data"" increasingly means big networks, because such data either directly concerns relational structures as in human and animal mobility patterns, gene interaction networks, or is influenced by an underlying relational structure as in epidemiological studies, urban studies, and cultural diffusion. Most applications of networks rely crucially on comparing networks, for example to detect changes in one network across time, to categorize or classify multiple networks of similar types, or to build analogies across fields by comparing networks of different origins. The question of how to compare two networks in a principled way, without relying on the ad hoc choice of statistics used by many current comparison methods, is key to the foundations of network science. This project will bring to bear new ideas from mathematics, computer science, and statistical physics on the problem of principled, structural comparison of networks. Through pre-existing collaborations, the PIs will leverage these new comparison methods to address questions in several different areas, for example, about: how food webs change across gradients like latitude, altitude, and temperature,morphological growth patterns of bacterial colonies, the evolution of human culture and communities, and links between socio-economic indicators and epidemiology.<br/><br/><br/>Our project will develop new rigorous and principled methods of comparing the structure of complex networks. The methods to be pursued aim to get away from single-scale summary statistics; to break new ground, we must think in terms of structural distance rather than statistical inference. In combination with tools from machine learning, such structural comparison methods are an important step towards defining the ""space of real-world networks"", which could serve as a more rigorous basis for a theory of complex networks. These methods have four advantageous features: (1) they systematically consider multiple scales of network organization, (2) they do not depend on an identification of the nodes of the two networks beforehand, (3) they can compare networks of different sizes, and (4) they are not dependent on any particular generative model of network growth. Very few, if any, of the existing network comparison methods have all of these features, and those that do exist have not been extensively developed. These features enable many new applications in a range of areas, including ecology, microbiology, cultural evolution, and epidemiology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1712706","New Methodology and Theory for Optimal Treatment Regimes with Applications to Precision Medicine","DMS","STATISTICS","07/01/2017","01/15/2020","Charles Doss","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2021","$176,555.00","","cdoss@stat.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1269","","$0.00","The problem of finding the optimal treatment regime, or a series of sequential treatment regimes, based on individual characteristics has important applications in areas such as precision medicine, government policies, and active labor market interventions.  Depending on the application, a treatment can represent a drug, a device, a program, a policy, an intervention, or a strategy.  Stimulated by the advancements in fields such as genomics and medical imaging, the last decade has witnessed exciting and remarkable progress in personalized medicine, ranging from treatments for breast cancer to treatments for major depressive disorders.  The success of precision medicine depends on the development of accurate and reliable statistical and machine learning tools for estimating the optimal treatment regime given the data collected from randomized experiments or observational studies.  This project will develop novel methodology, theory, and algorithms with the potential to significantly advance the state of the art in statistical estimation and inference for optimal treatment regimes. <br/> <br/>The proposed research will significantly enhance the availability of statistical methodology and theory for static or dynamic optimal treatment regimes estimation.  A systematic framework for estimating optimal treatment regimes using a new quantile criterion for a variety of scenarios will be developed.    The research will focus on both one-stage (static) treatment regimes and dynamic treatment regimes, the latter allowing for treatments to vary with time.  In addition, the research will address completely observed responses and randomly censored responses (e.g., survival times), randomized trials and observational studies, and doubly robust estimation. The framework will also be extended to alternative criteria such as Gini's mean difference. This project will significantly advance the theoretical foundations of a large class of robust estimators of optimal treatment regimes.  Furthermore, it addresses the challenging and important problem of developing new methodology and algorithms to identity important variables for optimal treatment regime estimation in the high-dimensional setting.  The investigator will develop software packages and make them freely available to the research community.  Students from minority groups will be especially encouraged to participate in the proposed project."
"1729787","DMREF: Collaborative Research: Transforming Electrocatalysis using Rational Design of Two Dimensional Materials","CBET","DMREF","09/01/2017","07/31/2017","Rohan Mishra","MO","Washington University","Standard Grant","Robert McCabe","08/31/2021","$361,177.00","","rmishra@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","ENG","8292","8396, 8398, 8400, 9150, 9263","$0.00","The project will employ an integrated theoretical and experimental approach to rapidly discover highly efficient catalysts, based on two-dimensional (2D) materials in contact with an ionic liquid, for a variety of electrochemical reactions of importance for sustainable energy generation, chemicals manufacturing, environmental remediation, and energy storage.  Electrocatalysis - as employed in energy storage and conversion devices such as advanced batteries, fuel cells, photovoltaics, and chemical electrolyzers - is becoming an increasingly important alternative to conventional thermal catalysis, but needs further improvements in efficiency, cost reduction, and chemical selectivity for wide-scale commercial implementation.  The project will address those needs by combining first-principles density functional theory calculations, including solvent interaction effects and theory-guided machine learning, to identify new ionic-liquid electrolytes and low-cost 2D materials capable of displacing existing thermal processes with electrocatalytic processes utilizing renewable and/or sustainable resources.  Educational and outreach components of the project will focus on preparing both graduate and undergraduate students for the workforce needed to realize advanced energy technologies.  Emphasis at both universities will be placed on the recruitment of minority and underrepresented student populations through existing programs including the Minority Engineering Recruitment and Retention Program (MERRP) at the University of Illinois-Chicago, and the Office of Undergraduate Research at Washington University.  The theoretical and experimental dataset on 2D materials generated in the study, along with relevant computational codes, will be disseminated to the broader research community through on-line repositories.<br/><br/>Two-dimensional transition metal dichalcogenides (TMDCs) in contact with ionic liquid (IL) electrolytes will be used as the starting materials offering a new paradigm for electrocatalysis based on materials with low work function, significant overlap of the d-band partial density of states with the Fermi energy, and an electrolyte 'solvent' that protects rather than poisons the catalytic sites.  Novel material combinations and structures will be predicted using computational tools and then synthesized using chemical vapor deposition, chemical vapor transport and colloidal chemistry. Atomic and electronic structure will be characterized using in-situ aberration-corrected scanning transmission electron microscopy (STEM).  The information obtained from high-resolution STEM, including high-angle annular dark-field (HAADF) and annular bright-field (ABF) imaging, as well as electron energy loss spectroscopy (EELS) and energy dispersive X-ray spectroscopy (XEDS), will be used to confirm successful synthesis of the desired structures and to create starting configurations for the first-principles modeling efforts.  Both ex-situ and in-situ electrochemical experiments will be conducted to measure the activity and selectivity of the synthesized materials.  In particular, the study will utilize a novel graphene liquid cell, developed by one of the investigators, that enables atomic-resolution imaging and spectroscopy in a liquid environment. Mechanistic studies of the electrocatalytic reactions and transport measurements will be made utilizing in-situ differential electrochemical mass spectrometry (DEMS) together with a traditional silicon nitride based electrochemical stage for STEM characterization under operando conditions.  Taken together, the advanced synthesis, characterization, and evaluation techniques, coupled with efficient computational search methods, will accelerate the discovery of 2D material-based-catalysts with superior activity and selectivity for various electrochemical reactions including the oxygen reduction reaction (important in fuel cell technology), and the hydrogen evolution reaction (important in water electrolysis)."
"1657315","CRII: CHS: Customized Navigation for Older Adults with Vision Loss","IIS","CRII CISE Research Initiation","03/01/2017","03/01/2017","Shiri Azenkot","NY","Cornell University","Standard Grant","Ephraim Glinert","02/29/2020","$175,000.00","","shiri.azenkot@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","026Y","7367, 8228","$0.00","For a person with low vision, interacting with a smartphone to access information that is critical to their independence and safety, while possible, is slow and tedious.  The ultimate goal of this research is to remedy that deficiency by creating technology that enables these users to access information quickly and easily, both at home and on the go.  As a concrete and challenging first step, the work will focus on systems to enable seniors with low vision to plan and execute urban navigation tasks.  If successful, members of this large and growing target community will be empowered to perform a key activity of daily living with more confidence and independence.  Project outcomes will advance the state of the art in accessibility research more broadly, by laying the foundation for a new paradigm of accessible interaction via low-cost mainstream devices.<br/><br/>To these ends, the research will involve a number of phases: (1) documenting the navigation and mobility patterns and challenges facing members of the target user community; (2) the design and implementation of a desktop system that produces ""optimal"" routes based on feature accessibility along a given route; (3) the design and implementation of a smartphone navigation application that will provide accessible guidance including accessible maps; and (4) evaluation of the smartphone application in the field with target users.   Achieving these goals will require an interdisciplinary effort that draws on expertise in a diverse set of fields, including accessibility, interaction techniques, information processing, and machine learning."
"1712037","Modeling Spin Configurations and Ranking","DMS","STATISTICS","07/01/2017","05/17/2017","Sumit Mukherjee","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2021","$197,045.00","","sm3949@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","This project considers the modeling of ranking and categorical data that are strongly dependent in nature. Typical examples of dependent ranking data arise when one ranks students in a class based on exam scores, or when people rank movies on a website. Typical examples of dependent categorical data arise when one studies how the behavioral patterns of people in a social network (say smoking or voting preference) are influenced by their friends. Both types of data have become increasingly common. This project studies modeling schemes for such data aimed at capturing its dependence, which is not assumed to be well understood. The research develops a rigorous understanding of the behavior of these models, which will aid in statistical inferences from data sets of these types.<br/><br/>For the ranking work, the project will study the Mallows model of rankings and its generalizations. Most existing research concentrates on one particular such model, namely the Mallows model with Kendall's tau. This project instead puts forth a general framework based on permutation limit theory to understand the behavior of Mallows models and permutation models in general. For modeling dependent or categorical data, the focus is on Ising models, which originated in statistical physics and have received significant recent attention in statistics and machine learning. The main goal of the research is to develop inference for Ising model parameters that are not computationally prohibitive."
"1738075","Collaborative Research:  Improving Constraints on Tropical Climate Feedbacks with Inverse Modeling of the Stable Isotopic Composition of Atmospheric Water Vapor","AGS","Climate & Large-Scale Dynamics","08/15/2017","08/11/2017","Joseph Galewsky","NM","University of New Mexico","Standard Grant","Eric DeWeaver","07/31/2021","$284,246.00","","galewsky@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","GEO","5740","9150","$0.00","The relative humidity (RH) of air in the subtropics, meaning RH in the belts of subsiding air found on either side of the equator, is an important factor in determining the behavior of subtropical clouds and their effects on climate.  Air in the subtropics generally enters the region in the upper troposphere after ascending in the deep convective clouds found in the convergence zones near the equator.  The RH of the air is largely determined by the coldest temperature it encounters during in-cloud ascent, as condensation dries the air to the saturation moisture value given by its temperature (lower moisture content for colder air).  But other factors also influence the RH of the subsiding subtropical air, in particular the air can be moistened by mixing with air from more humid levels closer to the surface.  The same RH can be achieved either by a relatively warm last saturation temperature with little mixing or a relatively cold last saturation temperature followed by greater mixing,  The two pathways to the same RH can have different implications for cloud feedbacks and RH change in a warming climate.<br/> <br/>However, these two pathways can be distinguished by examining the relative concentrations of heavier isotopes of water vapor, water vapor in which one of the hydrogen atoms is replace by deuterium or the oxygen 16 atom is replace by oxygen 18.  Roughly speaking, the heavier forms of water vapor evaporate more sluggishly and condense more readily than ordinary H2O, an effect which depends on the temperature at which the evaporation or condensation takes place.  Thus, heavy isotopes contain important clues to understanding the processes which set subtropical RH and relate it to subtropical clouds and their climate feedbacks.<br/> <br/>With this motivation the PIs examine the isotopic concentration of water vapor in the subtropical mid-troposphere using satellite and ground-based observations as well as climate model simulations.  Ground-based observations include measurements taken by the lead PI on the Chajnantor Plateau in Chile (see AGS-1158582).  Much of the work is performed using an inverse modeling technique in which an optimal set of parameters (including last saturation temperature and vertical mixing, among others) is determined using a machine learning algorithm that mimics natural selection.  The inverse technique is advantageous in that it is computationally inexpensive and can be applied equally well to both observations and model output.<br/> <br/>The work has societal relevance as it can lead to a better understanding of the role of subtropical clouds in climate change, a central issue in efforts to anticipate the amount of warming caused by greenhouse gas increases.  In addition, the project includes an extensive education and outreach effort through the New Mexico Museum of Natural History and Science in Albuquerque.  The museum serves a large Hispanic and Native American population including both inner city and rural communities.  The effort engages elementary school students through a summer camp program, middle and high school students through a ""junior docent"" summer program, and middle and high school teachers through a professional development workshop.  In addition, the project provides support and training for a graduate student, thereby providing for the future workforce in this research area."
"1740895","EAGER: USBRCCR: Improving Network Security at the Network Edge","CNS","Secure &Trustworthy Cyberspace","09/01/2017","06/21/2017","Donald Towsley","MA","University of Massachusetts Amherst","Standard Grant","Phillip Regalia","08/31/2021","$299,995.00","Phillipa Gill","towsley@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8060","022Z, 025Z, 7434, 7916","$0.00","Recent years have seen the Internet playing an increasingly critical role in our daily lives with home networks hosting PCs, tablets, mobile devices along with more specialized devices such as smart televisions, thermostats, and other Internet-of-Things (IoT) devices. While these devices offer users an array of services and conveniences, they come at the cost of introducing security vulnerabilities into the home network. Thus users are confronted with the dual challenges of securing their networks and devices against malicious software (malware) and botnets that may perform distributed denial of service attacks on commercial and public websites and of maintaining the privacy of increasingly personal flows of data through IoT devices.<br/><br/>This project takes a multifaceted look at the problem of securing home networks in the face of these challenges. Specifically, it includes a partnership with a Brazilian Internet Service Provider giving access to data from thousands of home network connections. This allows the creation of a baseline of network behavior against which to identify malicious behavior due to malware or compromised devices. Second, the project will develop behavior models of typical use of IoT in the wild. This will allow a better understanding of how sensitive and personal information can leak from IoT devices to IoT providers. The baseline and the IoT behavior models will lead to new methods for identifying the presence of anomalous/malicious behavior as well as leakage of privacy information.  The research conducted in this project provides significant benefits to society.  First, the results will allow users to enhance the security of their home networks and better protect personal and sensitive information.  Second, the project will provide substantial opportunities for students to develop software and research skills along with cybersecurity skills.<br/><br/>This project tackles the problem of securing modern home networks. The approach to this problem will be analytical and empirical. The project will consist of:<br/>(i) Development of techniques based on statistical analysis and machine learning that rely on data gathered in home networks to detect and classify malicious network activities. These techniques will focus on malicious activities both within and outside home networks.<br/>(ii) Fingerprinting of home network traffic to enable detection of compromised devices and characterization of the behavior of such devices even when flows are encrypted. <br/>(iii) Development of tools that will help users control access to their data."
"1659502","REU Site : Research Experiences in Computational Science, Engineering, and Mathematics (RECSEM)","OAC","RSCH EXPER FOR UNDERGRAD SITES","05/15/2017","05/09/2017","Kwai Wong","TN","University of Tennessee Knoxville","Standard Grant","Alan Sussman","12/31/2020","$359,426.00","Stanimire Tomov","kwong@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1139","9102, 9250","$0.00","The Research Experiences in Computational Science, Engineering, and Mathematics (RECSEM) REU site program at the University of Tennessee (UTK) directs a group of ten undergraduate students to explore the emergent interdisciplinary computational science models and techniques via a number of cohesive compute and data intensive applications. The RECSEM program complements the growing importance of computational sciences in many advanced degree programs and provides scientific understanding and discovery to undergraduates with an intellectual focus on research projects using high performance computing (HPC).  This program aims to deliver a real-world research experience to the students by partnering with teams of scientists engaged in scientific computing research at the National Institutes of Computational Sciences (NICS), the Innovative Computing Laboratory (ICL), and the Joint Institute for Computational Sciences (JICS) at UTK and Oak Ridge National Laboratory.   Additional international students supported by PI's partner universities in Hong Kong also participate in this program. Together the students of RECSEM work collaboratively to achieve their research tasks, and at the same time share a unique opportunity for trading academic experiences, scientific ideas, and cultural social activities in this ten week long program.  Thus, this REU site program supports the NSF's mission to promote the progress of science and to advance the national prosperity.<br/><br/>This program is organized around a synergetic theme of ideas and practices those are common to many scientific applications.  The research projects are categorized in three interrelated areas of research interests: engineering applications, numerical mathematics, and linear algebraic software and tools.  The scope of work for these projects put emphasis on conducting software development, model implementation, and design and evaluation of numerical experiments under the guidance of a team of experts in each scientific domain.  Projects include computation in multi-scale materials science and biomechanics applications, simulation of traffic flow phenomena, implementation of high order parallel numerical schemes, and processing of images with different techniques and algorithms of machine learning and data analytics.  The students would have opportunities to perform large-scale scientific simulations on HPC clusters as well as world-class state-of-the-art supercomputers equipped with the latest hardware technologies provided by the Extreme Science and Engineering Discovery Environment (XSEDE) organization.  These latest computing units include graphical processing units (GPUs), multicore processors and the Intel Xeon Phi processors (MIC). This program is organized in four major stages: HPC training, research formulation, project action, and scientific reporting.  These stages aim to gradually assist the students towards finishing their research projects in time with appropriate level of motivation and guidance."
"1723085","Beyond With-replacement Sampling for Large-Scale Data Analysis and Optimization","DMS","CDS&E-MSS","07/15/2017","06/11/2019","MERT GURBUZBALABAN","NJ","Rutgers University Newark","Continuing Grant","Christopher Stark","12/31/2020","$125,001.00","","mg1366@rutgers.edu","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","MPS","8069","8083, 9263","$0.00","Advances in sensing and processing technologies, communication capabilities and smart devices have enabled deployment of systems where a massive amount of data is collected to make decisions. Many key problems of interest for analyzing and processing big data result in large-scale optimization problems.  For a core, very widely used optimization method, which is efficient for such problems where the data points are sampled and processed in a sequential manner, there is a large gap between the theory and practice of this method. This project is about filling this gap by providing novel performance guarantees relevant to practical problems as well as developing novel and faster variants of the optimization method. The methods and techniques developed under the scope of this project will contribute to the efficiency and mathematical foundations of optimization algorithms targeted for big data challenges, contributing to more efficient decision making for a wide variety of large-scale data analysis problems. <br/><br/>Incremental gradient (IG) is the core, very widely used optimization method mentioned above and subsumes popular optimization methods in data analysis and machine learning practice such as stochastic gradient descent, randomized coordinate descent and Kaczmarz methods. Various performance guarantees for IG are available if data points are sampled with replacement in an independent identically distributed (i.i.d.) manner. However, these are not helpful in practical scenarios: In practice, data is often sampled in a non-i.i.d fashion without-replacement instead, as the resulting convergence is typically much faster. A first goal in this project is to study and quantify this discrepancy over an interesting class of regression problems, which has been a key open problem. Several techniques and methods are proposed for obtaining asymptotic and non-asymptotic theoretical guarantees for without-replacement sampling schemes. A second goal is to develop fast algorithms with convergence guarantees that go beyond the limitations of the i.i.d. sampling. For this purpose, a new framework for studying several alternative sampling schemes and their performance is developed. Using this framework, novel sampling schemes based on weighted without-replacement sampling and cyclic sampling that can adapt to the dataset and improve upon the performance of the traditional i.i.d. sampling in terms of limiting accuracy are developed."
"1717154","AF:   Small:   Collaborative Research:   Distributed Quasi-Newton Methods for Nonsmooth Optimization","CCF","Algorithmic Foundations","09/01/2017","08/30/2017","Petros Voulgaris","IL","University of Illinois at Urbana-Champaign","Standard Grant","Joseph Maurice Rojas","08/31/2020","$137,000.00","","pvoulgaris@unr.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7923, 7933","$0.00","Optimization, which finds the inputs to a mathematical function that produce the minimum output, is a workhorse algorithm behind many of the advances in smart devices or applications in the cloud. As data gets larger and more distributed, new ideas are needed to maintain the speed and accuracy of optimization. Operator splitting, which expresses the function to minimize as the sum of two convex functions, one of which is smooth and the other non-differentiable, is an idea that has produced to new first-order optimization methods.  This project explores operator splitting with second-order optimization methods, which have faster convergence to the minimum.  The focus is on large, distributed, and streaming data sets, so that the resulting general-purpose numerical solvers and embedded systems implementations can support optimization in cyberphysical systems and the Internet-of-Things.   The project has as priority the active engagement and training of students and researchers, with specific emphasis on the inclusion of women and under-represented minority groups. This project not only involves collaboration across three top-tier American universities, but also with European research institute, KU Leuven. <br/><br/>In specific, this research project seeks to interpret existing methods for structured convex optimization (such as the celebrated ADMM algorithm) as gradient methods applied to specific functions arising from the original problem formulation, and  interpret of operator-splitting techniques as fixed point iterations for appropriately selected operators.  A key theoretical foundation is the introduction of new envelope functions (smooth upper approximations possessing the same sets of solutions) that can be used as merit functions for variable-metric backtracking line-search. To conclude, a principal focus of the project is to design distributed asynchronous methods applicable to large-scale multi-agent cyberphysical systems that involve big data and impose stringent real-time constraints for decision-making. In this purview, the goal is to deliver methods that will outperform current state-of-the-art in terms of (a) speed of computations, (b) scalability with big data sizes, (c) robustness to various types of uncertainty, and, most topically, (d) distributed asynchronous implementation over networks in real-time. The merits will be illustrated in the context of applications in signal processing, control, machine learning and robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822191","SHF: Medium: Collaborative Research: Next-Generation Message Passing for Parallel Programming: Resiliency, Time-to-Solution, Performance-Portability, Scalability, and QoS","CCF","Software & Hardware Foundation","10/01/2017","06/02/2020","Anthony Skjellum","TN","University of Tennessee Chattanooga","Continuing Grant","Almadena Chtchelkanova","05/31/2021","$555,740.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","7798","7924, 7942, 9150, 9251","$0.00","Parallel programming based on MPI is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics.  Emerging supercomputer systems will have more faults and MPI needs to be able to workaround such faults to be appropriate to these emerging situations, rather than causing an entire application to fail.  Collaborative, transformative message passing research for High Performance Computing (HPC) critical to performance-portable parallel programming in new and forthcoming scalable systems (with a strategy of ""best practice-first, standardization-later"") is being reduced to practice. A substantial subset of the Message Passing Interface (MPI-3/4) application programmer interface is being made fault tolerant through extensions with weak collective transactions that synchronize between parallel tasks. <br/><br/>This research studies  the novel model that localizes faults, provides tunable fault-free overhead, allows for multiple kinds of faults, enables hierarchical recovery, and is data-parallel relevant.  Fault modeling of underlying networks is being studied. Application developers control the granularity and fault-free overhead in this effort. Performance and scalability results of the middleware prototype are being demonstrated principally through compact applications that relate to real use cases of practical and academic interest. The impact of this work ranges from users of the largest supercomputers in government labs to practical clusters that have long-running, time-critical applications, and to space-based and other parallel processing in ""hostile"" environments where faults occur more frequently than in past years.  The project is producing usable free software that will be widely shared in the community as well as guidance on how better parallel programs can be written in academia, industry, and government.  The project also provides guidelines for how to update existing or legacy programs to use the new capabilities that are being reduced to practice."
"1715458","SaTC: CORE: Small: Collaborative: GOALI: Detecting and Reconstructing Network Anomalies and Intrusions in Heavy Duty Vehicles","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","08/01/2017","06/16/2020","Indrakshi Ray","CO","Colorado State University","Standard Grant","Phillip Regalia","07/31/2021","$291,701.00","","iray@cs.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","1714, 8060","025Z, 065Z, 7434, 7923, 9102, 9178, 9251","$0.00","Heavy vehicles (e.g., trucks and busses) are a critical element of U.S. and worldwide logistics, often carrying cargo of high value or high risk (e.g., explosive liquids and gasses).  Heavy vehicles often have hundreds of Electronic Control Units (ECUs) that communicate over an internal network to carry commands (such as ""engage the brakes"") or share sensor data (such as the temperature of pressurized cargo unit carrying petroleum). ECUs with access to the communication network can send any message they want.  If the network or an ECU is compromised by an attack, the truck or a cargo container safety mechanism could malfunction. This project is gathering data from operational trucks to better understand communication among components of heavy vehicles and developing techniques to detect attacks in this environment.<br/><br/>The project is working to accomplish three main objectives: (1) Collect representative Controller Area Network (CAN) bus data from operational heavy vehicles, (2) Develop detection systems that can distinguish anomalous CAN bus network traffic, and (3) Test and verify the detection systems to reduce the number of false positives. The team is developing a log algebra to efficiently assess live CAN traffic using embedded devices with limited resources. Data is being gathered from truck traffic during highway operation, enabling the application of machine learning algorithms for anomaly detection. The team is evaluating the effectiveness of their intrusion detection techniques in their heavy vehicle testbed, using synthetic attacks against testbed ECUs and real-world CAN traffic data."
"1652694","CAREER: Designing synthesizable, ligand-protected bimetallic nanoparticles and modernizing engineering curriculum through computational nanoscience","CBET","Proc Sys, Reac Eng & Mol Therm","03/01/2017","02/28/2017","Ioannis Bourmpakis","PA","University of Pittsburgh","Standard Grant","Raymond Adomaitis","02/28/2022","$500,001.00","","gmpourmp@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","ENG","1403","1045","$0.00","The goal of this project is to develop a novel open-access computational framework for predicting the growth mechanisms and morphologies of ligand-protected metal nanoparticles (NPs). With NPs impacting numerous fields of science and technology, from energy to medicine to the environment, there is a critical need to determine the growth mechanisms of ligand-protected metal NPs and predict NP morphologies that can be synthesized in the laboratory. Although metal nanoparticles (NPs) of different sizes and shapes can be synthesized by colloidal chemistry methods,  advances towards controlling NP morphology have been based largely on trial and error experimentation, which is often tedious and costly. The proposed computational framework will employ novel first-principles-based structure-property relationships accounting for structure sensitivity and metal composition. The integration of research and education efforts will focus on modernizing the traditional Chemical Thermodynamics course by introducing animation modules based on cutting-edge nanotechnology examples.  Outreach activities are planned through a nanoscale-inspired interactive computer game to engage high school students, including underrepresented minorities, into pursuing STEM careers and increase awareness about the importance of the field of nanotechnology.<br/><br/>The proposed research project will combine Density Functional Theory methods with Monte Carlo and Molecular Dynamics simulations, Machine Learning, and scientific computing to develop a novel, open-access computational framework, applicable to the design of ligand-protected NPs. This framework will generate a library of crystal structures and electronic properties of thermodynamically stable, thiolate-protected, Au-based bimetallic NPs, across a range of heterometals and particle morphologies, all under realistic experimental conditions. The proposed work aims to advance current theories on NP stabilization, which are based on simplified, electron counting rules. The proposed computational framework will enable rational design of ligand-protected NPs.  It will also elucidate NP growth steps that are experimentally intractable, thus accelerating nanomaterials discovery.  The research findings will be made available online for experimental verification."
"1665333","To the limits of the density matrix renormalization group in quantum chemistry, and beyond","CHE","Chem Thry, Mdls & Cmptnl Mthds","08/01/2017","08/16/2017","Garnet Chan","CA","California Institute of Technology","Continuing Grant","Evelyn Goldfield","07/31/2021","$450,000.00","","garnetc@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","MPS","6881","7433, 8084, 9216, 9263","$0.00","Garnet Chan at the California Institute of Technology is supported by an award from the Chemical Theory, Models and Computational Methods (CTMC) Program in the Chemistry Division to develop new computational techniques that will allow scientists to address the inner workings of the biological systems that help convert nitrogen in the air into the chemical forms that plants and animals use. In addition, he is pursuing a fundamental reformulation of quantum mechanics that potentially will enable the quantum mechanics of any physical system to be efficiently simulated on computers. The outreach activities focus on the recruitment of underrepresented undergraduates to pursue summer research with the view towards addressing the pipeline issue in the STEM disciplines.<br/><br/>In more detail, the computational techniques being developed in the first part of the research focus on extending density matrix renormalization group (DMRG) techniques to confidently treat strongly correlated electronic structure as found in systems such as the FeMo cofactor (FeMoCo) of nitrogenase. These techniques include improved DMRG active space methods; time-dependent perturbation theory techniques; and time-dependent spectral methods. In the second part of the research he is pursuing an intensive development program to bring the full generality of tensor network algorithms, across the threshold of models, to ab initio quantum chemistry. This work pursues connections between tensor networks and modern machine learning algorithms."
"1743846","RAPID:  Emotion Regulation, Attitudes, and the Consequences for Political Behavior in a Polarized Political Environment","SES","Political Science","06/01/2017","07/02/2019","Lonna Rae Atkeson","NM","University of New Mexico","Standard Grant","Brian Humes","05/31/2020","$31,364.00","","atkeson@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","SBE","1371","7914","$0.00","Overview and Broader Impacts<br/>Transitions of power in democracies from one party to another create unique political contexts.   The prospect of substantial and sometimes unexpected changes in economic, social, or foreign policy stimulates heated debate and strong emotions in the public. This evolving context allows for scholars to assess how emotion regulation shapes political behavior and attitudes. The activation of emotion in the face of unexpected events may manifest in a number of forms: posting on social media, attending town hall meetings, contacting elected officials, signing petitions, and/or participating in social movements. This knowledge helps to explain when and why emotions fuel broad-scale political participation and competition. The data generated from this project enables the investigators to assess theories about the influence of emotion regulation on political participation, i.e. how different types of emotion regulation strategies lead to different types of political behavior.  The project will also provide valuable data for the social science community more broadly.  In particular, the study will produce the first multi-wave panel survey tracking how individual attitudes and behavior change over time in response to political events during the first year of a party transition in the United States. The survey is the first in political science to study how emotion regulation habits vary across individuals in society and considers whether, over time, these habits lead to more consensus or greater polarization. The study utilizes established practices from psychology to measure emotion regulation habits in political contexts. The project also trains graduate and undergraduate students in machine learning, automated text coding, and text-analytics of open-ended survey responses. These are emerging as critical skills in the field of survey research as computing power opens new opportunities for understanding public attitudes through more free-form, naturalistic responses compared to traditional surveys. <br/><br/>Scientific Merit<br/>This study contributes to a growing body of research exploring the importance of socio-political emotions in fueling public attitudes and behavior. We develop new theory to explain how individual-level emotion regulation habits a) moderate the affective processing of information and b) influence expressive and social political behavior. Pilot tests indicate individuals adept at regulating their emotions through reappraising emotion-provoking stimuli were more likely to become politically active compared to those who use suppressive or avoidance regulation habits. Reappraisers are more likely to transmit their views to others, thus serving as a source of social contagion. This project uses a three-wave panel survey design to conduct a broader test of how emotion regulation shapes issue engagement, changes in perceptions of political responsibility, and changes in levels and forms of political participation. We draw our panel sample from a pool of respondents to a large online national survey conducted just prior to the 2016 elections to obtain pre-election baseline opinions on key policy topics like healthcare, the environment, tax reform, and military use. Subsequent waves of the panel study are timed to survey respondents during critical moments of policy change, or will follow an unexpected extraordinary event, should one arise. The design permits the investigators to observe individual-level change in the use of emotion regulation habits over time, and to test whether different emotion regulation habits make individuals more prone to reexamine their beliefs. If that is the case, these individuals could potentially change beliefs, or reject new information and maintain rigidity in attitudes. The research also allows the investigators to examine how emotion regulation influences or hinders political activism in response to emotionally provocative moments in society."
"1718071","SaTC: CORE: Small: A Privacy-Preserving Meta-Data Analysis Framework for Cyber Abuse Research - Foundations, Tools and Algorithms","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/01/2017","06/16/2020","Sriram Chellappan","FL","University of South Florida","Standard Grant","Sara Kiesler","08/31/2021","$538,333.00","Nathan Fisk","sriramc@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","1714, 8060","025Z, 7434, 7923, 9178, 9251","$0.00","This project develops and deploys a mobile communications metadata analysis platform, designed to predict incidents of youth harassment or abuse, drawing on participatory methods to drive research design decisions and maintain youth privacy. By analyzing communication patterns - rather than content - in collaboration with youth participants, this project makes innovative contributions by facilitating a contextualized understanding of youth abuse and harassment. In turn, this analysis supports the development of new, privacy-enhancing tools which allow for early intervention in instances of abuse and distress. Starting from the hypothesis that abused or distressed youth display observably different patterns of communication, the data collected allow researchers to visualize and analyze the everyday communication networks of youth in new ways, providing new opportunities to understand the structures and conditions of youth sociality and forms of abuse while simultaneously developing new predictive models and techniques.<br/><br/>In particular, this project 1) designs new methods grounded in participatory design that allows for a better understanding of the ways in which youth conceptualize privacy, adult supervision and surveillance online; 2) develops a highly scalable and practically deployable mobile platform for performing large scale metadata and survey analysis, taking the privacy needs of end users as a core design principle as identified from the first outcome; 3) ensures that the platform guarantees full anonymity to participants, while also allowing for longitudinal and flexible survey administration, metadata collection, and seamless scaling; 4) extracts novel features in the spatial and temporal domain from communication metadata logs (from texts and calls) that associate with cyber harassment and abuse; 5) leverages the features extracted to design context aware machine learning algorithms to quickly detect abnormal patterns from metadata logs. The project trains a number of graduate and undergraduate student in inter-disciplinary research. Long term impact of this project scale well beyond youth populations, and the project more generally moves towards the development of a fully anonymous, confidential, needs-based social scientific research platform."
"1703438","Combined molecular simulation and experimental study to discover, predict and control enzyme immobilization in polymeric nanoparticles","CBET","Nanoscale Interactions Program","09/01/2017","06/10/2020","W. James Pfaendtner","WA","University of Washington","Standard Grant","Nora Savage","02/28/2021","$395,688.00","Elizabeth Nance","jpfaendt@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","ENG","1179","7237, 7974, 9251","$0.00","Enzymes, protein-based biological catalysts, have enormous potential to revolutionize the way we transform chemicals to useful products, treat disease, or detoxify environmental contaminants.  However, the use of enzymes in practice is limited by their tendency to lose their activity through a variety of mechanisms. One common strategy to improve enzyme robustness is to immobilize and encapsulate it in a polymer, a large molecule composed of many repeated subunits. This strategy holds great promise, but the polymer properties, encapsulation technique, and enzyme/polymer interactions all play important roles in determining the success of a particular encapsulation strategy. This research project aims to compare and contrast computer simulations with experimental results to help develop an efficient means of predicting beneficial encapsulation strategies. Working with students through the UW College of Engineering Math Academy, the researchers are providing many opportunities for enriching the training of graduate students, improving education, and engaging undergraduates from a local community college (Bellevue College) in research. <br/><br/>To date, successful enzyme encapsulations have been discovered largely via extensive trial and error experimentation and serendipity. This project, a combined study of molecular scale simulations and experimental measurement of enzyme activity and release in various matrices, seeks to build a rational design framework to discover, predict, and control the essential governing driving forces at the enzyme/polymer interface. Specifically, this project is demonstrating a comprehensive strategy, using fast molecular dynamics simulations paired with statistical machine learning, to identify sequence level descriptors of strong and weak binding of enzymes encapsulated in polymer nanoparticles. Complementary experiments are being performed that study enzyme loading, release and activity under a wide range of conditions. The ability to rationally design such interactions would be a transformative advance that could be applied to hydrogels, inorganic surfaces, or other types of enzyme/polymer systems. The strong interplay between the experiments and simulations will ensure that the results are accurate and potentially help to identify areas of improvement for molecular dynamics force fields and high throughput experimental design."
"1711702","Sensor aided millimeter wave communication for connected vehicles","ECCS","CCSS-Comms Circuits & Sens Sys","08/01/2017","06/12/2017","Robert Heath","TX","University of Texas at Austin","Standard Grant","Jenshan Lin","07/31/2021","$360,000.00","","rheath@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","ENG","7564","153E","$0.00","Automotive and aerial vehicles are being equipped with more sensors to enhance automated driving/flying as well as to perform general sensing tasks. Despite increasing data rates from cameras, LIDAR, and RADAR, along with higher levels of onboard computation, such vehicles do not have the means to share such high rate sensor data to enhance their situational awareness or improve cooperation. Millimeter-wave communication is one solution for high data rate vehicular communications. Achieving the highest rates with millimeter-wave, though, requires frequent link reconfiguration in mobile environments. This research is aimed at using the information derived from sensors that operate in some band other than the communication band, to help configure the communication link. The emphasis is placed on theoretical development and experimental validation of frameworks of using sensor information to permit link reconfiguration in mobile environments. Broader impacts include teaching, mentoring, participation of underrepresented groups, community outreach through demos and videos, and technology transfer through collaborations with industry partners. <br/><br/>This research project addresses the fundamentals of communication using side information derived from various sensing techniques. The project leverages mathematical tools from array signal processing and machine learning and will focus on the applications related to beam configuration. On the theoretical side, this project develops mathematical tools for understanding the frequency of beam realignment, a framework for relating radar and communication paths, and fundamental limits for sensor-aided beam alignment in millimeter-wave communication. On the practical side, this project tests critical hypothesis about statistical correlations between sensor measurements and communication link performance in a series of testbeds. The testbeds support low-cost radar derived from WiFi signals and higher cost automotive radars, coupled with custom developed millimeter-wave communication link capability. Outcomes of this research include (i) an understanding of how much overhead can be reduced by using information derived from potentially very different communication bands, and (ii) rapidly reconfigurable and robust millimeter-wave communication links."
"1651101","CAREER: Development of Fundamental Relationships Between Surface Structure, Composition, Stability, and Activity of Oxide Electrocatalysts in Aqueous Environments","DMR","CONDENSED MATTER & MAT THEORY","02/01/2017","01/19/2017","Alexie Kolpak","MA","Massachusetts Institute of Technology","Continuing Grant","Daryl Hess","01/31/2022","$191,000.00","","kolpak@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1765","026Z, 054Z, 1045, 7433, 8084, 8249, 8396, 8604","$0.00","NONTECHNICAL SUMMARY <br/>This award supports theoretical and computational research with the aim to enable the design of abundant and environmentally benign materials to be utilized as catalysts for splitting water molecules into their constituent hydrogen and oxygen component elements, and for capturing carbon in various forms to render it environmentally benign. A catalyst is a material that can accelerate the rate of a specific chemical reaction without being consumed in the reaction. The PI will use a suite of computational tools to investigate and design novel catalysts. In the process, the PI will also develop new computational approaches that are aimed to enable accurate prediction of these multi-component catalytic systems at a higher level of realism than currently possible starting from the constituent atoms up to the macroscopic scale. Success will enable new ways to design energy storage and conversion technologies. These new tools may also have application to the design of a wide range of other renewable energy technologies such as solar cells, fuel cells, and thermoelectric materials that can convert heat to electricity, as well as of advanced optics and electronics technologies. The computational approaches and the data generated as a result of this research will be made freely available providing new opportunities for online education and for researchers and industry to accelerate the pathway from materials design to device implementation. The research will provide a platform for educational component activities including research opportunities for students, course enrichment through use of data, and special efforts to engage and mentor both female and disabled students in various contexts pursuing careers in science and engineering.<br/><br/><br/>TECHNICAL SUMMARY<br/>This award supports theoretical and computational research and education to advance toward the capability to design materials with desired properties using computation with a focus on materials surfaces and their application as catalysts. The design of active, stable, earth-abundant catalysts for the oxygen evolution reaction and the oxygen reduction reaction would enable effective development of electrochemical energy storage and conversion technologies such as electrolyzers, fuel cells, and metal-air batteries. Similarly, new earth-abundant catalyst materials with high stability, high activity, and high selectivity are required to enable technologies based on aqueous electrochemical carbon dioxide reduction reactions, which can produce hydrogen, methane, methanol, and potentially longer-chain hydrocarbons for use as fuels or specialty chemicals. Catalyst activity is largely governed by surface properties, and the ability to predict and understand, structure-function relationships at realistic catalytic interfaces lies beyond current approaches. This presents a significant challenge to designing such materials.<br/><br/>In this project, the PI will address this key challenge by developing a new approach that aims for rapid and accurate prediction of atomic and electronic structure at realistic catalyst-solvent interfaces. The approach, based on a combination of first-principles density functional theory computations, machine learning algorithms, classical molecular dynamics and Monte Carlo simulations, and electrochemical principles will enable the study of environment-structure-property relationships in nanostructured materials in the presence of explicit water molecules, as well as the development of fundamental predictive models to guide the design of new materials systems with tailored properties. These capabilities will be demonstrated in the context of investigating the oxygen evolution reaction, the oxygen reduction reaction, and carbon dioxide reduction reactions on candidate materials from the promising class of transition metal oxides.<br/>  <br/>This research has the potential to lead to the design of novel water splitting and carbon dioxide reduction catalysts; the development of fundamental insights into oxide interface chemistry; and the dissemination of new computational tools that will enable detailed study and prediction of complex, realistic interface structures with quantum mechanical accuracy. Both the physical insights and the new tools will be directly applicable to the design of tailored materials systems for other catalytic reactions, as well as for a wide variety of other applications, such as photovoltaics, fuel cells and batteries, thermoelectrics, and nanoscale composite materials, in which interfaces play an important role. In addition, the proposed methodology could lead to a new paradigm in high-throughput computational screening of materials systems by enabling materials selection with respect to properties that are directly related to materials incorporation into realistic device geometries.  Computational approaches and the data generated as a result of this research will be made freely available online, providing new opportunities for online education and for accelerating the pathway from materials design to application. The research will provide a platform for educational component activities including research opportunities for students, course enrichment through use of data, and special efforts to engage and mentor both female and disabled students in various contexts pursuing careers in science and engineering."
"1717213","SaTC: CORE: Small: Massively Scalable Secure Computation Infrastructure Using FPGAs","CNS","Secure &Trustworthy Cyberspace","09/01/2017","07/21/2017","Stratis Ioannidis","MA","Northeastern University","Standard Grant","Sandip Kundu","08/31/2021","$499,999.00","Miriam Leeser","IOANNIDIS@ECE.NEU.EDU","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8060","025Z, 7434, 7923","$0.00","The statistical analysis of behavioral data collected through clinical trials, surveys, and experimentation, has a long history in academic disciplines like medicine, sociology, and behavioral economics. The privacy risks inherent in such studies are often at odds with the tremendous societal benefits resulting from sharing data among researchers and practitioners. Mining behavioral data at scale is also a ubiquitous practice among Internet companies, giving rise to significant privacy concerns. As the potential benefits to society are enormous, harnessing this data for the better good while protecting privacy is one of the grand challenges faced by our society today. This project addresses this challenge by bringing Secure Function Evaluation (SFE) of practical, real-life data mining and machine learning algorithms into the realm of practicality, through the development of a highly parallel, efficient, scalable computation platform for secure computation operating at a massive scale. <br/> <br/>The project develops a Massively Scalable Secure computation Infrastructure using FPGAs (MaSSIF), accelerating secure computations over a cluster of FPGAs and leveraging benefits of both hardware acceleration and multi-device parallelism. MaSSIF significantly differs from previous implementations of SFE in that it is the first to accelerate secure computation primitives: specifically, Garbled Circuits (GC) with FPGAs on such a massively parallel scale. The algorithms considered are (a) computationally intensive, (b) non-trivial to parallelize under SFE, and (c) of considerable practical importance.  MaSSIF advances the state of the art both through novel SFE algorithms, as well as in the design and optimization of accelerated, scalable systems for SFE."
"1719160","SHF:SMALL: MECAR: Memory-Centric Architecture to Bridge the Gap Between Computing and Memory","CCF","Software & Hardware Foundation","07/15/2017","07/07/2017","Yuan Xie","CA","University of California-Santa Barbara","Standard Grant","Yuanyuan Yang","06/30/2021","$450,000.00","","yuanxie@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798","7923, 7941","$0.00","Traditional computer systems usually follow the so-called classic Von Neumann architecture, with separated processing units (such as CPUs and GPUs) to do computing and memory units for data storage.  The increasing gap between the computing of processor and the memory has created the memory wall problem in which the memory subsystem is becoming the bottleneck of the entire computing system. As technology scales, data movement between the processing units (PUs) and the memory is becoming one of the most critical performance and energy bottlenecks in various computer systems, ranging from cloud servers to end-user devices. As we enter the era of big data, many emerging data-intensive workloads become pervasive and mandate very high bandwidth and heavy data movement between the computing units and the memory.  The fundamental goal of this project is to advance the trend of bridging the gap between computing and memory, with an application-driven approach. <br/><br/>By leveraging the PI's prior extensive research on 3D-stacked memory and non-volatile memory architecture, the PI proposes to focus on (1) designing memory-centric processing unit (PU) architecture with massive GB on-chip/on-package memory integrated with computing units; (2) investigating new processing-in-memory(PIM) memory architecture designs with both DRAM and emerging NVM; (3) and co-design and co-optimization of both memory-centric PU architecture and NDC/PIM memory architecture, with the emerging data-intensive applications such as neural computing and graph analytics as application driver to guide the architecture optimization. The success of this research will have enormous economic and social benefits as broader impact. The research will provide the design guidelines for enabling future computing systems beyond the state-of-the-art, ranging from high performance exascale computing to low power mobile systems. Consequently, it will enhance nearly every digital device available today from consumer to enterprise electronics. It can also spawn new applications involving the computation on the exascale of data, e.g. data mining, machine learning, bio-informatics, etc. It is expected that this project will serve as a catalyst to accelerate the adoption of data-intensive and memory-centric technologies in future computer systems and applications from architecture and system design perspectives. The PI has extensive industrial ties with summer internships planned, which will be invaluable for broadening the knowledge and skills of the students. The PI will also strive to educate a broad audience on the emerging technologies through regular and online classes. Publication/lecture notes will be released on public websites to promote the broader dissemination of scientific knowledge."
"1715409","SaTC: CORE: Small: Collaborative: GOALI: Detecting and Reconstructing Network Anomalies and Intrusions in Heavy Duty Vehicles","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","08/01/2017","07/22/2017","Jeremy Daily","OK","University of Tulsa","Standard Grant","Phillip Regalia","11/30/2019","$256,000.00","Urban Jonson","Jeremy.Daily@colostate.edu","800 S. Tucker Drive","Tulsa","OK","741049700","9186312192","CSE","1714, 8060","025Z, 065Z, 7434, 7923, 9150, 9178, 9251","$0.00","Heavy vehicles (e.g., trucks and busses) are a critical element of U.S. and worldwide logistics, often carrying cargo of high value or high risk (e.g., explosive liquids and gasses).  Heavy vehicles often have hundreds of Electronic Control Units (ECUs) that communicate over an internal network to carry commands (such as ""engage the brakes"") or share sensor data (such as the temperature of pressurized cargo unit carrying petroleum). ECUs with access to the communication network can send any message they want.  If the network or an ECU is compromised by an attack, the truck or a cargo container safety mechanism could malfunction. This project is gathering data from operational trucks to better understand communication among components of heavy vehicles and developing techniques to detect attacks in this environment.<br/><br/>The project is working to accomplish three main objectives: (1) Collect representative Controller Area Network (CAN) bus data from operational heavy vehicles, (2) Develop detection systems that can distinguish anomalous CAN bus network traffic, and (3) Test and verify the detection systems to reduce the number of false positives. The team is developing a log algebra to efficiently assess live CAN traffic using embedded devices with limited resources. Data is being gathered from truck traffic during highway operation, enabling the application of machine learning algorithms for anomaly detection. The team is evaluating the effectiveness of their intrusion detection techniques in their heavy vehicle testbed, using synthetic attacks against testbed ECUs and real-world CAN traffic data."
"1737865","ATD:  Collaborative Research:  Multivariate Quantiles for Rapid Spatio-Temporal Threat Detection","DMS","Geography and Spatial Sciences","09/01/2017","08/25/2017","Benjamin Bagozzi","DE","University of Delaware","Standard Grant","Leland Jameson","08/31/2021","$84,989.00","","bagozzib@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","MPS","1352","6877, 9150","$0.00","Different kinds of data on societal attributes, observed from multiple sources, at multiple locations, and at different points in time, will be studied in this project. The geometrical properties of such data will be analyzed to quantify and characterize normal patterns in the data, which will then be leveraged to identify sudden departures from normal patterns within societies. Methodology for understanding normal patterns in the data and rapidly detecting change in one or more aspects of the data will be devised in this project. Data from different locations around the world will be analyzed and used to formulate strategies for risk mitigation and emergency responses.<br/><br/>The geometric properties of high-dimensional spatio-temporal data will be studied in this project to construct a multi-dimensional extremity indicator. This indicator and other statistical and machine learning techniques will be used for rapid spatio-temporal change detection, under a variety of technical conditions and frameworks. Such changes may be towards specific known directions, or generic departures from normal patterns. Methods for detecting changes in extremes and tails of multivariate probability distributions will likewise be developed as part of this project. Social, economic, and supply chain logistics data will then be studied to develop policy and rapid response strategies using data-driven techniques."
"1651855","CAREER: A multi-scale, data-driven model of 3D cell motility","MCB","Cellular Dynamics and Function, Systems and Synthetic Biology","01/01/2017","02/21/2020","Stephanie Fraley","CA","University of California-San Diego","Continuing Grant","Steve Clouse","12/31/2021","$966,713.00","","sifraley@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","BIO","1114, 8011","1045, 7465","$0.00","CAREER: A multi-scale, data-driven model of 3D cell motility<br/><br/>This project aims to study how cells migrate in spaces similar to the body by developing new techniques to measure cell movements in three-dimensions (3D). This research will generate understanding of physiologically relevant cell migration behaviors, which are responsible for orchestrating the formation of multicellular organisms. Because cells interact directly and constantly with their surroundings to create movement, migration is context dependent. However, cell motility has historically been studied in two-dimensional (2D) environments. The Principal Investigator (PI) discovered that cells display different migration behaviors in 3D environments that are more physiological. This project will address the fundamental question: How do 3D environmental cues give rise to distinct migration behaviors? The PI will employ a new process for creating 3D environments, a new integrated microscopic imaging technique and advanced computational biology approaches to develop a predictive, quantitative model of how cells move in three dimensions through a synthetic extracellular matrix (ECM). Such a model has the potential to advance our basic understanding of cell motility, which is a critical aspect of eukaryotic development.  The project will additionally provide research training opportunities for graduate students and experience-based educational projects on multi-scale systems analysis and career education for high school and undergraduate students. <br/><br/>In 3D spaces like bodily tissues, it is generally accepted that the coordination of four local processes (adhesion, cytoskeletal polymerization, contractility, and matrix remodeling) dictate global motility behaviors. Yet, no quantitative framework has been developed to link these localized processes to global cell behaviors. Such a framework would represent a generalized platform for both studying and engineering cell motility, where any signal input (chemical exciter/inhibitor, physical cue, etc.) could be quantified by its effect on these processes to predict the resulting motility behavior output. Historically, motility models have been formulated using a bottom-up approach, where theoretical physical and kinetic equations are fitted to observations. These approaches have advanced the understanding of local processes of motility on 2D planar surfaces. However, mathematical, computational, and experimental limitations have prevented their full extension to physiologically relevant 3D cell motility. The top-down approach used in this project overcomes current limitations to immediately address a fundamental question: How can subcellular molecular processes be connected with the geometry and translocation of the whole cell in a 3D ECM? The goal of this project is to integrate knowledge across three scales of cell motility, extracellular, sub-cellular, and cellular, by taking a data-driven systems approach. In Aim 1, the PI will develop a microfluidic device that will enable highly controlled, systematic variation of ECM matrix properties and high-throughput analysis of their regulatory role in cell migration. Aim 2 will combine into a single platform a time-lapse cell tracking assay and quantitative molecular imaging techniques, which the PI previously developed for measuring the four core motility processes within cells in a 3D ECM. This platform will allow the analysis of single-cell motility at an integrated systems level for the first time. In Aim 3, the PI will apply machine learning and statistical modeling techniques to the single-cell data generated in Aim 2 to develop a predictive framework for emergent 3D cell motility."
"1737915","ATD:   Collaborative Research:  Multivariate Quantiles for Rapid Spatio-Temporal Threat Detection","DMS","ATD-Algorithms for Threat Dete, ","09/01/2017","09/05/2018","Ujjal Mukherjee","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Leland Jameson","08/31/2021","$24,999.00","","ukm@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","046Y, Q218","6877","$0.00","Different kinds of data on societal attributes, observed from multiple sources, at multiple locations, and at different points in time, will be studied in this project. The geometrical properties of such data will be analyzed to quantify and characterize normal patterns in the data, which will then be leveraged to identify sudden departures from normal patterns within societies. Methodology for understanding normal patterns in the data and rapidly detecting change in one or more aspects of the data will be devised in this project. Data from different locations around the world will be analyzed and used to formulate strategies for risk mitigation and emergency responses.<br/><br/>The geometric properties of high-dimensional spatio-temporal data will be studied in this project to construct a multi-dimensional extremity indicator. This indicator and other statistical and machine learning techniques will be used for rapid spatio-temporal change detection, under a variety of technical conditions and frameworks. Such changes may be towards specific known directions, or generic departures from normal patterns. Methods for detecting changes in extremes and tails of multivariate probability distributions will likewise be developed as part of this project. Social, economic, and supply chain logistics data will then be studied to develop policy and rapid response strategies using data-driven techniques."
"1660447","Coastal Erosion Vulnerabilities, Monsoon Dynamics, and Human Adaptive Response","BCS","Geography and Spatial Sciences","09/01/2017","09/06/2017","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Standard Grant","Scott Freundschuh","02/28/2022","$405,868.00","Bimal Paul, Walter Curtis, Munshi Rahman","tomc3@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","SBE","1352","1352, 9179, 9186, 9189, 9278","$0.00","This project will investigate how coastal erosion in low elevation environments is linked to seasonal precipitation patterns, how humans perceive their vulnerability to erosion risk, and how humans secure livelihoods in the face of erosion.  Rural populations in coastal zones are dependent on land resources to support agriculture and on marine resources to support fishery-based activities.  Coastal erosion has potential to threaten economic and geopolitical stabilities due to the permanent nature of lost land resources that deprives population of land-based resources and access to marine resources.  Variation in the location and timing of erosion events produces uncertainty for populations who must negotiate erosion threats without the benefit of consistent science-informed information regarding risk and vulnerability. This project contributes to a better understanding of the intersection of coastal environmental change, lowland coastal populations, atmospheric science, and economic development.  Results will inform coastal processes, as well as human vulnerability and resilience, and will also make methodological advances in quantitative geospatial analysis.  Students will be trained in STEM science and project elements will be used to enhance modules of existing courses at two universities.  Project results will be disseminated in journals and other outlets, presented to government agencies and NGOs, and shared with local stakeholders.  The project will become registered with the Resilience Connections Network, an international virtual space for the interaction between global and local leaders on resilience science and practitioners. <br/><br/>The study site for this project is the delta of a large river that is the outlet for water discharge driven by monsoon precipitation in a large watershed with a shoreline particularly vulnerable to erosion.  The region has high rates of annual erosion whereby shorelines may retreat over 100 meters per year causing livelihood disruption and displacement of farming and fishing households.  The project has three objectives.  The first objective characterizes time-space patterns of shoreline erosion and monsoon dynamics at multiple scales.  Understanding erosion rates at annual scales is particularly important to characterize vulnerability because households need to forecast and respond over relatively short time horizons that are not resolved at coarser decadal scales.  The second objective assesses predictive relationships between monsoon precipitation and erosion dynamics with the hypothesis that time-space patterns of rainfall can help predict erosion behavior.  A third objective assesses social vulnerability, risk perception and how resource endowments and adaptive behavior may promote resilience.  These objectives will be addressed using a mixed-methods approach that combines geospatial analysis using earth observation data, statistical and machine-learning predictive modelling, and quantitative and qualitative primary social data collected in two villages.  Investigators will produce an approximately 50-year time series record of shoreline change that will used in concert with monthly precipitation data to develop an annual predictive model of erosion risk.  Field research will be done to collect social data from local populations in terms of their risk perception, vulnerability, and adaption behaviors by conducting a household survey, focus groups, and key informant interviews.  Although the project will focus on the delta region of the Ganges-Brahmaputra-Meghna river system in Bangladesh, the research will provide new insights for dealing with coastal erosion in densely populated coastal zones in other regions, including the United States."
"1656877","CRII: SHF: Toward Sustainable Software for Science - Implementing and Assessing Systematic Testing Approaches for Scientific Software","CCF","CRII CISE Research Initiation, CI REUSE, EPSCoR Co-Funding","03/01/2017","03/20/2017","Upulee Kanewala","MT","Montana State University","Standard Grant","Sol Greenspan","02/28/2021","$157,006.00","","upulee.kanewala@cs.montana.edu","309 MONTANA HALL","BOZEMAN","MT","597172470","4069942381","CSE","026Y, 6892, 9150","7798, 7944, 8228, 9150","$0.00","Scientific software is widely used in science and engineering. In<br/>addition, results obtained from scientific software are used as evidence in<br/>research publications. Despite the critical usage of such software, many<br/>studies have pointed out a lack of systematic testing of scientific software.<br/>As a result, subtle program errors can remain undetected. There are numerous<br/>reports of subtle faults in scientific software causing losses of billions of<br/>dollars and the withdrawal of scientific publications. This research aims to develop<br/>automated techniques for test oracle creation, test case selection, and develop<br/>methods for test oracle prioritization targeting scientific software. The intellectual<br/>merits of this research are the following: (1) It advances the understanding of<br/>the scientific software development process and investigates methods to<br/>incorporate systematic software testing activities into scientific software<br/>development without interfering with the scientific inquiry, (2) It forges new<br/>approaches to develop automated test oracles for programs that produce complex<br/>outputs and for programs that produce outputs that are previously unknown, (3)<br/>It develops new metrics to measure the effectiveness of partial test oracles<br/>and uses them for test oracle prioritization, (4) It extends the boundaries of<br/>current test case selection to effectively work with partial or approximate<br/>test oracles. The project's broader significance and importance are (1)<br/>produces a publicly available, easy to use testing tool that can be incorporated<br/>into the scientific software development culture such that the testing<br/>activities will not interfere with ?doing science,? (2) recruits Native Americans<br/>and women into computer science research, (3) develops a new higher level<br/>undergraduate course titled ?Software development methods for Scientists?<br/>targeting senior level undergraduate students in non-CS disciplines.<br/><br/>This project develops METtester: an automated testing framework that can effectively<br/>test scientific software. This testing framework analyzes the source code of<br/>the program under test and utilizes machine learning techniques in order to<br/>identify suitable test oracles called metamorphic relations (MRs). Then, it automatically<br/>generates effective test cases to conduct automated testing based on the<br/>identified MRs using a mutation based approach. After that, it creates a<br/>prioritized order of MRs to be used with testing in order to identify faults as<br/>early as possible during the testing process. Finally, METtester conducts<br/>testing on the program under test using the prioritized order of MRs with the generated<br/>test cases."
"1717391","AF:  Small:  Collaborative Research:  Distributed Quasi-Newton Methods for Nonsmooth Optimization","CCF","Algorithmic Foundations","09/01/2017","08/30/2017","Angelia Nedich","AZ","Arizona State University","Standard Grant","Tracy Kimbrel","08/31/2020","$199,840.00","","Angelia.Nedich@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7796","7923, 7933, 9102","$0.00","Optimization, which finds the inputs to a mathematical function that produce the minimum output, is a workhorse algorithm behind many of the advances in smart devices or applications in the cloud. As data gets larger and more distributed, new ideas are needed to maintain the speed and accuracy of optimization. Operator splitting, which expresses the function to minimize as the sum of two convex functions, one of which is smooth and the other non-differentiable, is an idea that has produced to new first-order optimization methods.  This project explores operator splitting with second-order optimization methods, which have faster convergence to the minimum.  The focus is on large, distributed, and streaming data sets, so that the resulting general-purpose numerical solvers and embedded systems implementations can support optimization in cyberphysical systems and the Internet-of-Things.   The project has as priority the active engagement and training of students and researchers, with specific emphasis on the inclusion of women and under-represented minority groups. This project not only involves collaboration across three top-tier American universities, but also with European research institute, KU Leuven. <br/><br/>In specific, this research project seeks to interpret existing methods for structured convex optimization (such as the celebrated ADMM algorithm) as gradient methods applied to specific functions arising from the original problem formulation, and  interpret of operator-splitting techniques as fixed point iterations for appropriately selected operators.  A key theoretical foundation is the introduction of new envelope functions (smooth upper approximations possessing the same sets of solutions) that can be used as merit functions for variable-metric backtracking line-search. To conclude, a principal focus of the project is to design distributed asynchronous methods applicable to large-scale multi-agent cyberphysical systems that involve big data and impose stringent real-time constraints for decision-making. In this purview, the goal is to deliver methods that will outperform current state-of-the-art in terms of (a) speed of computations, (b) scalability with big data sizes, (c) robustness to various types of uncertainty, and, most topically, (d) distributed asynchronous implementation over networks in real-time. The merits will be illustrated in the context of applications in signal processing, control, machine learning and robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717207","AF:  Small:  Collaborative Research:  Distributed Quasi-Newton Methods for Nonsmooth Optimization","CCF","Algorithmic Foundations","09/01/2017","03/22/2019","Nikolaos Freris","NY","New York University","Standard Grant","Tracy Kimbrel","08/31/2020","$163,160.00","Nasir Memon","nf47@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7796","7923, 7933","$0.00","Optimization, which finds the inputs to a mathematical function that produce the minimum output, is a workhorse algorithm behind many of the advances in smart devices or applications in the cloud. As data gets larger and more distributed, new ideas are needed to maintain the speed and accuracy of optimization. Operator splitting, which expresses the function to minimize as the sum of two convex functions, one of which is smooth and the other non-differentiable, is an idea that has produced to new first-order optimization methods.  This project explores operator splitting with second-order optimization methods, which have faster convergence to the minimum.  The focus is on large, distributed, and streaming data sets, so that the resulting general-purpose numerical solvers and embedded systems implementations can support optimization in cyberphysical systems and the Internet-of-Things.   The project has as priority the active engagement and training of students and researchers, with specific emphasis on the inclusion of women and under-represented minority groups. This project not only involves collaboration across three top-tier American universities, but also with European research institute, KU Leuven. <br/><br/>In specific, this research project seeks to interpret existing methods for structured convex optimization (such as the celebrated ADMM algorithm) as gradient methods applied to specific functions arising from the original problem formulation, and  interpret of operator-splitting techniques as fixed point iterations for appropriately selected operators.  A key theoretical foundation is the introduction of new envelope functions (smooth upper approximations possessing the same sets of solutions) that can be used as merit functions for variable-metric backtracking line-search. To conclude, a principal focus of the project is to design distributed asynchronous methods applicable to large-scale multi-agent cyberphysical systems that involve big data and impose stringent real-time constraints for decision-making. In this purview, the goal is to deliver methods that will outperform current state-of-the-art in terms of (a) speed of computations, (b) scalability with big data sizes, (c) robustness to various types of uncertainty, and, most topically, (d) distributed asynchronous implementation over networks in real-time. The merits will be illustrated in the context of applications in signal processing, control, machine learning and robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1729452","DMREF/Collaborative Research: A Data-Centric Approach for Accelerating the Design of Future Nanostructured Polymers and Composites Systems","CMMI","POLYMERS, DMREF","09/01/2017","06/29/2018","Linda Schadler","NY","Rensselaer Polytechnic Institute","Standard Grant","Alexis Lewis","08/31/2021","$795,623.00","Curt Breneman, Ravishankar Sundararaman, Linda Schadler","linda.schadler@uvm.edu","110 8TH ST","Troy","NY","121803522","5182766000","ENG","1773, 8292","085E, 116E, 7237, 8021, 8400, 9102, 9178, 9231, 9251","$0.00","Polymer nanocomposites are highly tailorable materials that, with careful design, can achieve superior properties not available with existing materials. Most polymer nanocomposites are developed using an Edisonian (trial and error) process, severely limiting the capacity to optimize performance and increasing time to implementation. The solution is a data-driven design approach. As an example, this Designing Materials to Revolutionize and Engineer our Future (DMREF) project will design new material systems that simultaneously optimize for dielectric response and mechanical durability, a combination currently not achievable but necessary for high voltage electrical transmission and conversion. These new materials will have a significant economic impact on society because they will enable higher efficiency generation and transmission of electricity.  More broadly, this new design approach will result in new nanostructured polymer material systems that will impact a wide range of industries such as energy, consumer electronics, and manufacturing.  To ensure broad access to this work, the data, tools and models developed will be integrated and shared through an open data resource, NanoMine. The team will interact with the scientific community to create an integrated virtual organization of designers and researchers to test and improve the models. Educational components will reach undergraduate and graduate communities via interdisciplinary cluster programs at the two institutions, and provide undergraduate research opportunities and web based instructional modules and workshops.<br/><br/>The research is based on a central research hypothesis that using a data-driven approach, grounded in physics, allows integration of models that bridge length scales from angstroms to millimeters to predict dielectric and mechanical properties to enable the design and optimization of new materials. Data, algorithms and models will be integrated into the new and growing nanocomposite data resource NanoMine to address challenges in data-driven material design. This research will result in advancements in three areas. First, integrating a broad set of literature data and targeted experiments with multiscale methods will enable the development of interphase models to predict local polymer properties near interfaces considered critical for modeling polymer composites. Second, a hybrid approach utilizing machine-learning to bridge length scales between physics-based modeling domains will be used to create meaningful multiscale processing-structure-property relationship work flows. And, third, a Bayesian inference approach will utilize the knowledge contained in a dataset as a prior probability distribution and guide 'on-demand' computer simulations and physical experiments to accelerate the search of optimal material designs. Case studies will demonstrate the data-centric approach to accelerate the development of next-generation nanostructured polymers with predictable and optimized combinations of properties."
"1719017","CIF: Small: Info-Clustering: An Information-Theoretic Framework for Data Clustering","CCF","Comm & Information Foundations","07/01/2017","06/22/2017","Tie Liu","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","06/30/2021","$439,315.00","","tieliu@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7797","7923, 7935","$0.00","Clustering refers to a procedure that groups similar objects together while separating dissimilar ones apart. This simple idea has a wide range of applications in different areas of scientific research. From the mathematical viewpoint, the problem of clustering is quite unique in that it attempts to discover unknown patterns of data without a clear knowledge of the ground truth. Instead of jumping to a specific algorithm or a dataset (which is a common practice in the literature), this research aims to lay a rigorous theoretical ground, upon which many meaningful and practical implementations can be developed subsequently. This research is accompanied by the investigator's continuing effort in curriculum development, involving undergraduate and graduate students in research, and broadening the participation of women and underrepresented minorities in engineering.<br/><br/>To achieve the aforementioned goal, the investigator plans to take an information-theoretic view of the data clustering problem by modeling each object to be clustered as a piece of information. A key advantage of this information-theoretic view is that now the similarity among multiple objects can be naturally measured by the amount of shared information. This is precisely where information theory, with the accumulation of over 70 years of active research, can be most useful. The main agendas of this research are to understand: 1) what clustering algorithms can be derived from the proposed info-clustering framework by leveraging the large body of literature on multivariate dependency modeling including graphical models and parameter families; 2) whether the proposed info-clustering framework can be leveraged to make some progress on the long-standing open problem of subset feature selection in statistics and machine learning."
"1744819","I-Corps: Crowdsource sensing of earthquakes","IIP","I-Corps","07/01/2017","01/22/2020","Richard Allen","CA","University of California-Berkeley","Standard Grant","Andre Marshall","12/31/2020","$50,000.00","","rallen@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project it to reduce the impact of future earthquakes.  This smartphone-based crowdsource sensing application provides earthquake alerts and shaking hazard data. The vision is to provide earthquake early warning around the globe, and collect data on building performance.  Warnings can be used to slow and stop trains reducing the likelihood of derailment, to isolate hazardous chemicals and machinery to reduce damage, to move data to safe storage, and to signal individuals to move to a safe place.  Shaking data recoded in buildings can also be used to assess the performance of a building during an earthquake.  Recorded shaking in smaller non-damaging quakes can be used to predict building performance in larger events.  The data recorded by application enabled phones can potentially be used by structural engineers to assess buildings, and can be used by the insurance industry to set premiums.<br/><br/>This I-Corps project develops a global smartphone-based seismic network.  The technology uses machine learning techniques to distinguish between earthquakes and other sources of shaking as recorded on personal/private smartphones, and is embedded in the application that has been downloaded by over 250,000 users around the world.  When a phone detects an earthquake, it notifies a central server.  The server confirms the earthquake based on multiple phone detection, and estimates the size and location.  An alert could then be issued to users.  At the same time, the phone records the ground shaking during the earthquake.  This timeseries data can be used for a range of purposes including fundamental research on earthquake processes, and characterization of buildings."
"1741608","EAGER:  Collaborative:  Leveraging High-Density Internet Peering Hubs to Mitigate Large-Scale DDoS Attacks","CNS","Secure &Trustworthy Cyberspace","08/15/2017","08/07/2017","Roberto Perdisci","GA","University of Georgia Research Foundation Inc","Standard Grant","Nina Amla","07/31/2021","$179,818.00","","perdisci@cs.uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","CSE","8060","025Z, 7434, 7916","$0.00","Large-scale distributed denial of service (DDoS) attacks pose an imminent threat to the availability of critical Internet-based operations, as demonstrated by recent incidents that brought down a number of highly popular web services such as Twitter, Spotify and Reddit. While several solutions to counter DDoS attacks have been proposed by both industry and academia, most of the solutions that are currently deployed on the Internet - such as traffic scrubbing - tend to detect and mitigate DDoS attacks close to the victim edge network, once the attack has already caused damage. Creating systems for early DDoS attack detection and mitigation that can be deployed at the core of the Internet has the potential to significantly improve Internet security and reliability. <br/><br/>This project investigates innovative machine learning-based DDoS attack detection and mitigation solutions that can be deployed at the core of the Internet, within Internet eXchange Points (IXPs). IXPs are high-density peering hubs that provide infrastructure used by autonomous systems (ASes) to interconnect, and are therefore well positioned to observe significant fractions of global Internet traffic. The project leverages IXP-based traffic monitoring to develop advanced traffic analysis and classification methods for efficient, automated early detection and mitigation of DDoS attacks. The researchers aim to first investigate methods for defending against distributed reflective DoS (DRDoS) attacks, which rely on spoofed IP traffic to amplify the attacker's available bandwidth, and to then expand the investigation to volumetric DDoS attacks that do not rely on spoofed traffic. As part of the project, the researchers aim to develop collaborations with IXPs and Internet operators around the world, to facilitate research on DDoS defenses and increase opportunities for high-impact technology transfer."
"1729671","DMREF: Collaborative Research: High throughput Exploration of Sequence Space of Peptide Polymers that Exhibit Aqueous Demixing Phase Behavior","DMR","DMREF","10/01/2017","08/18/2017","Ashutosh Chilkoti","NC","Duke University","Standard Grant","John Schlueter","09/30/2021","$1,183,788.00","Stefan Zauscher","chilkoti@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","MPS","8292","054Z, 7433, 8004, 8400, 8990","$0.00","Non-technical Description: Proteins are incredibly adaptive molecules. For example, when exposed to a stimulus, such as an environmental change in temperature, pH or light, some proteins undergo conformational (shape) changes that can lead to useful material properties, such as a phase transition, turning from a solid to liquid, or a liquid to solid. Understanding and predicting how these changes occur on the molecular level could lead to the creation of an entire class of soft materials that can respond to environmental cues. This research will develop and apply computer algorithms to quickly go through large amounts of protein sequence data to predict as yet undiscovered temperature-sensitive peptide sequences. These sequences can then be subjected to computer-based modeling to predict conformational changes that ensue from a phase transition. Finally, experiments will be conducted to predict and determine the 2D and 3D materials architectures that can be created by combining stimulus-responsive peptide sequences. If successful, these methods could create a toolkit for the efficient design and fabrication of a large variety of materials with custom-designed properties.<br/><br/>Technical Description: Stimulus responsiveness is a striking feature of proteins in Nature, whereby responses to chemical stimuli such as ligand binding, phosphorylation, and methylation, and physical stimuli such as changes in temperature, pH, light, and salt concentration lead to sharp conformational or phase transitions. Unlike proteins, which encode diverse responses to numerous stimuli by richly sampling amino acid sequence space, current bioinspired designs of repetitive polypeptides have focused on a tiny fraction of the vast conceivable expanse of sequence space. The primary goal of the proposed research is thus to develop generalized materials design rules, by combining experiments, fast and accurate physics-based computer simulations, and data science, to accelerate the discovery and development of a potentially huge class of thermally-responsive polypeptide materials by a systematic exploration of sequence space. This research will ""for the first time"" provide a complete atomistic understanding of the determinants of the lower critical solution temperature (LCST) and upper critical solution temperature (UCST) phase behavior, enable de novo molecular design of LCST and UCST peptide polymers and identify rules on how to combine them to create hierarchically-ordered, nanostructured polypeptide materials that exhibit unique morphologies that can be tuned as a function of their stimulus responsiveness. These materials could serve as nanostructured scaffolds and templates and enable a broad range of biocatalytic, bioelectronic, or assay devices. The PIs also plan to release the PIMMS modeling package, a set of tools for performing lattice-based simulations of polymers. PIMMS will provide support for the machine learning algorithms that enable the design of responsive protein-based polymers. The PIMMS codebase will be released as open source. A user community will be coalesced around the language by ensuring that interested researchers are able to contribute modules to or implement application-specific algorithms within the codebase. This is expected to allow a wider growth of the project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award."
"1803530","Collaborative Research: Academic hiring networks and scientific productivity across disciplines","SMA","APPLIED MATHEMATICS, SciSIP-Sci of Sci Innov Policy, EPSCoR Co-Funding","10/01/2017","12/14/2018","Daniel Larremore","CO","University of Colorado at Boulder","Standard Grant","Cassidy Sugimoto","08/31/2020","$134,329.00","","daniel.larremore@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","SBE","1266, 7626, 9150","4444, 7626, 8050, 8251, 9251","$0.00","Advances in science come from the collective and linked efforts of thousands of researchers working within and across disciplines. This project creates rigorous models of the composition, dynamics, and network structure of the United States? scientific workforce across heterogeneous independent institutions with different strengths and emphases.  The systematic influence on these institutional and individual characteristics on scientific advances across disciplines is investigated. The results of this project will generate new insights into the composition of the scientific workforce and scientific productivity across fields. In addition, this project trains new graduate and undergraduate students in cutting-edge computational and statistical research techniques, and will develop and disseminate new large-scale open data sets on the composition of the United States? scientific workforce and provide new software for collecting structured data automatically from open unstructured sources.<br/><br/>This project uses state-of-the-art computational and statistical techniques from network science, machine learning, and social modeling to create a new technology platform for automatically and systematically collecting high-quality structured data on the composition, dynamics, and output of the scientific workforce.  These data will be combined with social survey results of individual researchers and with rigorous network methods to model the relationship between workforce composition, productivity, and observable differences at the individual and institutional levels within and between scientific fields. Mathematical models of the short- and long-term evolution of workforce in order to evaluate the likely outcomes of certain types of interventions and policies are developed."
"1714508","CSR: Small: Enabling In-Network Computation for Datacenter Applications","CNS","CSR-Computer Systems Research","09/01/2017","07/28/2017","Arvind Krishnamurthy","WA","University of Washington","Standard Grant","Marilyn McClure","08/31/2020","$475,000.00","","arvind@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7354","7923","$0.00","The emergence of programmable network devices, such as reconfigurable switches and customizable network accelerators, along with the increasing traffic of data centers, motivate the use of in-network computation. Today's latest reconfigurable switches support configurable per-packet processing, including customizable packet headers, customizable packet processing, and the ability to maintain state inside the switch. Given this hardware trend, this project seeks to offload computing operations onto intermediate networking devices for a broad range of application services ranging from distributed storage to big data analytics and distributed machine learning, thus optimizing the operations of data center applications. <br/><br/>The project's primary goal is to build a programming framework to enable in-network computing using programmable networking hardware. In designing and implementing this framework, the project addresses the following research questions: First, the project tackles how to best integrate reconfigurable switches and network accelerators into a data center network, as they both have limitations in terms of the operations that they can execute. Second, a key project goal is to identify what is a simple and yet powerful programming application program interface (API) for these programmable devices to support a broad class of applications. Third, another key challenge addressed is how to keep state and computations on these devices consistent with that of application servers, and how to co-design data center applications to take advantage of the performance benefits enabled by this paradigm. <br/><br/>This project seeks to improve the efficiency of network-intensive data center applications that are used by literally billions of people around the globe on a daily basis. By improving their efficiency, one can dramatically reduce the cost of data center services as well as make it much cheaper for new public services to be developed. Collaborators at various switch vendors are equal partners in this effort, providing access to new technologies as well as assisting in technology transfer to the industry.  The project will integrate undergraduate students as researchers, and material from the project will be incorporated into both undergraduate and graduate courses.<br/><br/>A key project goal is to publicly release developed software and enable a rich set of high-performance data center applications. All software will be made public as soon as they are developed, hosted via GitHub at https://github.com/arvindkrish/incbricks and accessible from the project website at the University of Washington. A mirrored version of this repository will be maintained at the University for at least five years."
"1705135","SaTC: CORE: Medium: Collaborative: Privacy-Aware Trustworthy Control as a Service for the Internet of Things (IoT)","CNS","Secure &Trustworthy Cyberspace","09/01/2017","07/06/2017","Mani Srivastava","CA","University of California-Los Angeles","Standard Grant","Phillip Regalia","08/31/2021","$850,000.00","Paulo Tabuada","mbs@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","8060","025Z, 7434, 7924","$0.00","The Internet of Things (IoT) includes a variety of devices such as smart appliances, cars, and other physical systems that are deeply embedded in our everyday lives and that are at risk from new kinds of threats to security and privacy from hackers or state actors. In IoT systems, sensors are used to probe the physical state of the system (e.g., temperature in a building or rotational speed of a wheel of a car) and then software control systems use algorithms to determine appropriate adjustments to the system (e.g., run the air conditioning for 5 minutes or apply the brakes). The project is focused on protecting  those control systems and algorithms to ensure security and privacy for users.<br/><br/>The research addresses trustworthy and privacy-aware control architectures for IoT through mechanisms drawn from control, cryptography, software, and hardware. These include: (i) A framework for formally reasoning about safety and privacy properties of control software in conjunction with dynamical models of the physical world and associated sensing and actuation channels; (ii) Lightweight domain-specific mechanisms, for policing flow of information through software applications, while leveraging the semantics of machine learning and control algorithms, physics of the system, and numerical properties; (iii) Enforcing desired safety and information leakage properties via a combination of principled sensor data perturbation, control algorithms optimized for efficient computation over encrypted data, and a hardware-supported trusted computing base tailored to protecting sensed data and control algorithm parameters; (iv) A resilient control and timing infrastructure that protects against attacks on timing information through a hybrid use of edge and cloud resources and physical models. The success of the mechanisms is being assessed on experimental testbeds for smart home, industrial automation and smart vehicles, but have broader applicability to many other IoT applications. The project team is also creating a new graduate class on IoT security and developing educational material on IoT security for high-schoolers through the  Los Angeles Computing Circle initiative at the University of California - Los Angeles."
"1651938","CAREER:  Measurement, Analysis, and Novel Applications of Blockchains","CNS","Secure &Trustworthy Cyberspace","02/15/2017","09/14/2019","Arvind Narayanan","NJ","Princeton University","Continuing Grant","Sara Kiesler","01/31/2022","$444,851.00","","arvindn@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8060","025Z, 1045, 7434","$0.00","This research seeks to develop the science of blockchains. A blockchain is a distributed append-only database containing cryptographically linked records. The technology is the basis of cryptocurrencies (e.g., bitcoin). Principled methods for analyzing blockchain data will be developed in order to answer key questions about the economics, security, privacy, and anonymity of cryptocurrencies. The project envisions blockchains as the pillar of a new breed of decentralized applications and Internet security mechanisms. The project will develop a new open-source blockchain analysis tool, BlockSci, and use it to answer research questions including: How should we model cryptocurrency participants? How well do decentralized markets work, and how do they fail? How private are cryptocurrencies in practice? Can blockchains serve as the foundation for a secure, decentralized Internet? The research outputs of the project will be integrated into Princeton's Massive Online Open Course on cryptocurrencies, student projects, and a new course on the economics of security. The project will inform the cryptocurrency industry and developer community through conferences and release of open-source software.<br/><br/>This project will combine computer science research on cryptocurrencies with cutting-edge techniques from game theory and mechanism design. The project could validate the applicability of powerful machine learning methods, including spectral graph analysis, to the rich data in blockchains. The system design and deployment component will contribute to important ongoing debates on the different approaches to scaling of big data analytics techniques. The PI will continue his engagement with policy makers and regulators to ensure that government agencies are informed by the results of this project."
"1829403","RI: Small: Parallel Methods for Large-Scale Probabilistic Inference","IIS","Robust Intelligence","09/01/2017","04/17/2018","Ryan Adams","NJ","Princeton University","Continuing Grant","Rebecca Hwa","08/31/2020","$433,499.00","","rpa@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7495","7495, 7923","$0.00","We are undergoing a revolution in data.  We have grown accustomed to constant upheaval in computing -- quicker processors, bigger storage and faster networks -- but this century presents the new challenge of almost unlimited access to raw data.  Whether from sensor networks, social computing, or high-throughput cell biology, we face a deluge of data about our world.  Scientists, engineers, policymakers, and industrialists need to use these enormous floods of data to make better decisions.  This research project is about providing foundations for tools to achieve these goals. Simple models give only coarse understanding.  The world is sophisticated and dynamic, providing rich information.  Furthermore, representation of uncertainty is critical to discovering patterns in complex data.  Not only are many natural processes intrinsically random, but our knowledge is always limited.  The calculus of probability allows us to represent this uncertainty and design algorithms to act effectively in an unpredictable world. The gold standard for probabilistic analysis is Markov chain Monte Carlo (MCMC), a way to identify hypotheses about the unobserved structure of the world that are consistent with observed data.  It is a powerful and principled way to perform data analysis, but traditional MCMC methods do not map well onto modern computing environments.  MCMC is a sequential procedure that cannot generally take advantage of the parallelism offered by multi-core desktops and laptops, cloud computing, and graphical processing units.  This research will develop new methods for MCMC that are provably correct, but that take advantage of large-scale parallel computing. There are a variety of broader impacts of this work.  In addition to the core technical contributions, the project engages in deep scientific collaborations.  New photovoltaic materials will lead to better solar cells and more sustainable energy production.  New techniques for uncovering genetic regulatory mechanisms will lead to better understanding of disease. Quantitative models of mouse activity will give insight into the neural basis of behavior and provide a deeper understanding of brain disorders.  <br/><br/>From a technical point of view, this work pursues two complementary approaches to large-scale Bayesian data analysis with MCMC: 1) a novel general-purpose framework for sharing of information between parallel Markov chains for faster mixing, and 2) a new computational concept for speculative parallelization of individual Markov chains. These theoretical and practical explorations, combined with the release of associated open source software, will yield more robust and scalable probabilistic modeling. It will develop provably-correct foundations and efficient new algorithms for parallelization of Markov transition operators for posterior simulation.  These operators will be used in three collaborations that are representative of the methodological demands for large-scale statistical inference: 1) predicting the efficiencies of novel organic photovoltaic materials, 2) discovering new genetic regulatory mechanisms, and 3) quantitative neuroscientific models for mouse behavior. While this proposal focuses on the generalizable technical challenges of these problems, these collaborations provide compelling examples of how machine learning can be broadly transformative.<br/><br/>Finally, the project includes a significant outreach component, engaging with local middle schoolers, and involving underrepresented minorities in summer research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830864","From Approximate to Exact Designs with Applications to Big Data","DMS","STATISTICS","08/01/2017","03/02/2018","Wei Zheng","TN","University of Tennessee Knoxville","Standard Grant","Gabor Szekely","05/31/2020","$101,570.00","","wzheng9@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1269","7433, 8083","$0.00","Design of experiments is an integral part of the scientific process in many areas of research with a direct impact on society, such as the biological sciences, the health sciences, the social sciences, engineering, marketing, and education.  A well-chosen design facilitates the collection of data that, at a minimum cost, maximizes the information for the scientific questions of interest.  Many scientific studies allow for repeated use of conceptual units, so that developing tools for optimal design for these problems has great potential impact.  Particularly in the realm of big data, there is much room for improvement of existing methods for design of experiments, and the tools and concepts under development in this research project have potential to lead to significant gain of information without increasing computational cost.  Results from the project will be made available to researchers in other areas through easy-to-use software that implements the algorithms to be developed. Graduate students will be trained to become researchers in design of experiments. <br/><br/>This project aims to result in a major leap forward in understanding and knowledge of optimal design of experiments. Recent work in the field has had a significant impact on the advancement of optimal crossover designs and designs for interference models for arbitrarily given covariance structures and design size configurations. However, these results have for the most part been limited to approximate designs for relatively simple models. While these results are arguably important in their own right, this project will extend methods and tools to achieve the ultimate goal of deriving exact designs for a wider spectrum of practical models. The results will be a much needed addition to our collective design toolbox. Most importantly, this project will go beyond the territory of design and apply the tools and ideas from design of experiments to subsampling problems emerging in big data with both statistical and machine learning methods under consideration. Preliminary results indicate that this is an opportune time to make these challenging but critical steps."
"1724345","CRCNS Research Proposal: Collaborative Research: Data-driven approaches for restoring naturalistic motor functions using functional neural stimulation","IIS","Special Initiatives, CRCNS-Computation Neuroscience","09/01/2017","08/23/2017","David Warren","UT","University of Utah","Standard Grant","Kenneth Whang","08/31/2021","$394,943.00","","David.Warren@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","1642, 7327","7327, 8089","$0.00","Individuals with lower limb paralysis retain the ability to plan and initiate gait within the central nervous system but, due to conditions such as spinal cord injury or stroke, are unable to transmit those directives to the muscles of the lower limb. This research project uses a biologically-inspired, data-driven approach to address this deficiency. Specifically, the research team is developing and evaluating machine learning methods and electrical stimulation of nerves to control movements of the lower limbs. The goal is to develop and evaluate methods to restore natural, coordinated, and graceful gait in an animal model of paralysis. This activity is a first step toward providing benefit to the paralyzed community by creating pathways toward the development and commercialization of functional gait restoration systems that evoke more natural, controlled movement of paralyzed limbs. In addition, the project offers unique opportunities to train engineering students in the performance of pre-clinical studies, placing these future researchers at the forefront of engineering technology and medical research. <br/><br/>This research project aims to develop and evaluate methods to restore natural, coordinated, and graceful gait in an animal model of paralysis using a synergistic collaboration of a multi-disciplinary and multi-university team of investigators and a combination of innovative modeling and algorithm development supported by a series of experiments. Specifically, the project aims to achieve the following sub-goals involving application of data-driven algorithms in a series of experiments that are designed to evoke increasingly complex movements over the course of the proposed work: (a) Develop, characterize, and evaluate advanced controllers of joint angle and joint torque production of a single joint in only a single direction, to allow comparison of the data-driven model to earlier, classical controls methods; (b) Develop, characterize, and evaluate advanced controllers of joint angle and joint torque production of a single joint in both directions, to elucidate methods used by the advanced controller's solution to the under-constrained problem of agonist-antagonist muscle pair control; (c) Develop, characterize, and evaluate advanced controllers of joint angle and joint torque production of multiple joints in both directions, to elucidate methods used by the advanced controller's solution to the competing-goals problem of biarticular muscle control; and, (d) Recreate natural, coordinated, and graceful gait by use of the advanced controllers arising from the first three goals, and demonstrate this result on a treadmill platform."
"1749103","CAREER: Stochastic processes and embeddings on networks","DMS","PROBABILITY, Division Co-Funding: CAREER","07/01/2017","08/17/2018","Allan Sly","NJ","Princeton University","Continuing Grant","Tomek Bartoszynski","07/31/2020","$325,486.00","","allansly@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","MPS","1263, 8048","1045","$0.00","Random processes on networks play a major role in a wide range of areas including mathematical physics, machine learning, and theoretical computer science. One such question we will address involves analysing algorithms for detecting sub-communities within a social network by investigating the graph of friendship relations. The proposal addresses questions from combinatorics and computer science about when random combinatorial and computational problems have solutions, such as when there is a colouring in a random graph.  Ideas from statistical physics give predictions for these thresholds which we will mathematically prove.  It also studies questions of how long it takes random processes on networks, such as the Glauber dynamics Markov chain, to reach their equilibrium distribution and how this depends on their initial starting position.  Such processes are used as algorithms for sampling high dimensional distributions.  The proposal focuses on development of mathematical theory with a view to better understanding these problems.  Finally the proposal will support the development of new graduate courses in discrete probability and stochastic process on random graphs as well as providing research opportunities for graduate and undergraduate researchers.<br/><br/>The main theme of this proposal is the development of new theory and applications across a range of stochastic processes on networks. One aspect involves studying phase transitions of Gibbs measures on random graphs, particularly random constraint satisfaction problems.  Here we hope to establish conjectures from statistical physics for a range of models such as the chromatic and independence numbers on random graphs.  A second theme is the development of new tools for establishing rough isometries and other geometric notions of closeness between random metric spaces.  In particular we will consider whether independent copies of Poisson processes, percolation clusters and SLE curves are rough isometries or quasi-symmetries.  Finally the proposal will consider Markov random fields such as the Ising model on lattices.  At high temperatures it will consider the question of universality of the cutoff phenomena as well as the effect of different initial conditions on the mixing time.  At low temperatures it will pursue a better understanding of Ising interfaces in order to establish rapid mixing results."
"1747316","EAGER: University and Corporate Research: Substitutes or Complements?","SMA","SciSIP-Sci of Sci Innov Policy, SCIENCE RESOURCES STATISTICS","07/15/2017","07/10/2017","Ashish Arora","MA","National Bureau of Economic Research Inc","Standard Grant","Cassidy Sugimoto","06/30/2020","$224,142.00","Sharon Belenzon","ashish.arora@duke.edu","1050 Massachusetts Avenue","Cambridge","MA","021385398","6178683900","SBE","7626, 8800","7626, 7916","$0.00","Even as they focus on innovation as a source of growth, American corporations have steadily move away from in-house research. This is not because research is not relevant for invention. To the contrary, inventive activity relies on research more than ever before. One explanation might be the growing size and relevance of university research and an emerging division of innovative labor in which firms and universities specialize in different sets of activities that innovation requires.  If university research substitutes for corporate research, expansion of relevant university research will reduce corporate research.  However, firms differ not only in the pool of relevant knowledge, but also in the type of inventive activity they perform. Specifically, firms that are producing more novel inventions, and inventions that rely more on recent scientific advances, may have greater incentives to invest in internal research because the scientific problems they face are likely to be less generic and hence less likely to be solved by university scientists.  The broader value of the project consists of significantly expanding the data infrastructure for the science of science policy. Specifically, this project provides firm level measures on the use and production of scientific and engineering knowledge, and on the novelty of the inventive activity of the firm. The information will be useful to policy makers, industry practitioners, scholars, and anyone interested in the role of science and engineering knowledge in innovation.<br/><br/>This project leverages advanced machine learning techniques to explore this question. It matches 20 million scientific publications to 6 million patents for the period 1980-2016 to develop measures of the relevance of science and engineering research to inventive activity. It uses this measure to quantify the relevance of university research for 4000 R&D performing publicly traded American headquartered corporations. It then analyzes how variations in the flow of relevant university research is related to in-house research by a corporation.  A second part of the project develops a measure of the degree of novelty of the patents filed by US headquartered corporations in order to analyze how firms with more novel inventions respond to changes in the flow of relevant university research. These firm level data are available to the broad public, through an easy-to-use web platform."
"1652669","CAREER: Synergistic Cross-IoT N-Way Sensing using Wireless Traffic in the Edge","CNS","Networking Technology and Syst","09/01/2017","09/04/2019","Ting Zhu","MD","University of Maryland Baltimore County","Continuing Grant","Alexander Sprintson","08/31/2022","$292,796.00","","zt@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7363","1045","$0.00","Internet-of-Things (IoT) devices have been increasingly used in people's daily life for applications such as personal health monitoring, global positioning, power grids management, supply chain control, and smart home automation. These IoT devices are expected to be widely deployed in environments ranging from residential houses to highly dynamic urban surroundings. With the increasing number of IoT devices, this CAREER project aims at improving the existing sensing performance by leveraging various sensing capabilities from different IoT devices. The success of this research can fully unleash the potential of increasing number of IoT devices and significantly improve people's daily life. The research results are likely to inspire novel theoretical and systematic studies that open up new areas of research in Internet-of-Things. The broader impact of this CAREER project is amplified by (i) improving curriculum development with enhanced course projects; (ii) raising interest in technology among K-12 students and under-represented minority groups through open houses; (iii) supporting talented female and minority PhD students to successfully accomplish their doctoral studies; and (iv) disseminating research results through high-profile tutorials and open-source sites.<br/><br/>The main goal of this CAREER project is to enable IoT devices to conduct accurate, efficient, and scalable N-way sensing. This project will deliver a networked system with modularized structure that accommodates various solutions for cross-IoT sensing, wireless communication, machine learning, and data processing in the edge. New designs will be implemented with a combination of different IoT hardware platforms and this project will deliver reusable system designs, algorithms, and protocol libraries to the research community. In addition to academic papers and technical reports, this CAREER project is expected to contribute to the system development in broader application communities across multiple disciplines, such as human behavior study, sleep monitoring, and augmented reality."
"1702008","DISSERTATION RESEARCH:  Biomass estimation and uncertainty analysis: Integrating Bayesian modeling and small-footprint waveform LiDAR data","DEB","MacroSysBIO & NEON-Enabled Sci","06/01/2017","03/02/2017","Sorin Popescu","TX","Texas A&M AgriLife Research","Standard Grant","Elizabeth Blood","05/31/2018","$19,213.00","Tan Zhou","s-popescu@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454375","9798626777","BIO","7959","","$0.00","Remote sensing is used to characterize and map the spatial variation of Earth's vegetation and its changes over time. A relatively new airborne laser sensor called LiDAR (Light Detection And Ranging) has been used to accurately characterize the three-dimensional vegetation structure and to measure plant biomass and carbon content. With increased LiDAR data availability, there is a need to develop open source tools for processing these data effectively to obtain the vegetation and terrain information. With current methods, substantial variation and uncertainty remain in estimating the amount and type of vegetation. Support from this Doctoral Dissertation Improvement Grant (DDIG) will be used to develop open source tools to process LiDAR data from the National Ecological Observatory Network (NEON) and to explore their potential for use in identifying tree species, estimating vegetation amounts, and determining the statistical uncertainty in the analysis. The methods developed from this project will facilitate the creation of 3D-vegetation structure and enable the understanding of ecosystem patterns and processes. The quantification of errors and uncertainties of vegetation structure and amount are also conducive to designing effective plans for sustainable forest management and providing accurate inputs for biogeochemical models to inform science-based policy.<br/><br/>This research will be accomplished through the following steps: (1) Develop algorithms and open source tools for LiDAR data processing, (2) Segment individual trees and identify tree species with raw LiDAR data alone using Random forests and Bayesian machine learning method, (3) Build different models (step-wise, Random forests, and hierarchical Bayesian models) to estimate forest biomass and carbon stocks using LiDAR metrics and variables derived from point clouds, and (4) Quantify the uncertainty of estimations in different processing stages based on different approaches and model parameters. The algorithms and processing methodologies developed in this proposal will provide open source tools for LiDAR waveform processing and enhance the use and value of NEON data. Moreover, the products of this research will assist forest managers to better manage precious natural resources and make more informed decisions."
"1745824","Workshop:  The subterranean macroscope: sensor networks for understanding, modeling, and managing soil processes (University of Chicago-Hyde Park, Illinois - October 2017)","CBET","Plant Genome Research Project, Special Initiatives, EFRI Research Projects","09/01/2017","08/02/2017","Supratik Guha","IL","University of Chicago","Standard Grant","James Jones","08/31/2018","$99,887.00","Charles Rice","guha@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","ENG","1329, 1642, 7633","004Z, 7556","$0.00","Science has a poor understanding of one of the most important components of life on earth: the physical, biological and chemical nature of the soil in the subsurface region of the earth that typically reaches to 0.6 to three meters below the surface. Soils provide food, fiber and fresh water, make major contributions to energy and climate sustainability, and help maintain biodiversity and the overall protection of ecosystem. Scientists lack an accurate understanding and predictive models for soil and for the plants that grow in it, because it has been difficult to create high resolution experimental maps of the biological and geochemical properties of the soil over large tracts of land over time. However, the emergence of sensor networks, better sensing using nanotechnology, and data analytics has the potential soon to bring such high resolution experimental soil mapping capabilities within reach. The goal of this workshop is to bring together the engineering, computer science, and soil and plant science communities to develop a vision for underground wireless sensor networks that will enable the creation of such subterranean soil maps, followed by using these data to develop the next generation of soil and plant models. Besides furthering the boundaries of fundamental science, these networks would have a major impact on food security and the way we manage the environment and ecosystems. <br/><br/>Scientists have a poor understanding of the physical, chemical and biological transformations and cycling of soil in the vadose zone and its influences on plant science and food security. Lacking adequate soil data at high spatial and temporal length scales, current models are inadequate. However, revolutionary advances in sensors and nanotechnology, sensor networks, communications and microelectronics technologies, and data analytics are poised to enable scalable and affordable subterranean sensing networks that, in turn, can potentially revolutionize soil, plant, and ecosystem sciences. Such advances would have a major impact on the environment, food security and its management. Building on recent successes across various science and engineering fields, this workshop brings together researchers in soil science (including experts in the biological, chemical and physical nature of soil), dynamic soil modeling expertise, plant sciences, sensor networks, microelectronics and wireless researchers, and machine learning/data analytics. The goal of the workshop is to create a vision and framework for how such a subterranean sensor network could be built across different geographical scales, with sensors that will generate dense, useful data that will inform soil science, plant science and modeling efforts. These efforts, in turn would lead to the next level of understanding of the physical, chemical and biological nature of soil, and its impact on plant science and food security. The discussion will be framed by three key topics that build upon NSF?s ?Big Ideas?: (1) Convergence, bringing together the aforementioned multidisciplinary communities; (2) Big Data, the efficient transmission, curation, and analysis of dense, in situ soil data over time; and (3) Understanding the Rules of Life, predicting phenotype from genotypes in diverse environments through advances in measuring soil environmental conditions that interact with genotype (G x E interactions). The workshop also considers how technology and data can best interface with computational models to inform real-world decisions. Longer-term impacts of the workshop may include facilitating the use of unprecedented levels of spatial and temporal, multi-modal data that can revolutionize our understanding of soils, developing more accurate soil and plant models than are available today, and enabling a profound impact on environmental sustainability, agricultural yields, climate models, water and agricultural management, and global food security."
"1721483","SBIR Phase I:  A tool for estimating objective outcome measures in clinical speech applications","IIP","SBIR Phase I","07/01/2017","07/10/2017","Harry Schmitt","AZ","Aural Analytics","Standard Grant","Nancy Kamei","06/30/2018","$225,000.00","","harry.schmitt@auralanalytics.com","975 S. MYRTLE AVE. COOR 3472","tempe","AZ","852870102","4803297528","ENG","5371","5371, 8018, 8032, 8042, 8089","$0.00","The broader impact/commercial potential of the Small Business Innovation Research (SBIR) Phase I project is to make uniformly high quality practice by Speech-Language Pathologists (SLPs) accessible to non-ambulatory and underserved rural populations. Because the inability to engage in spoken communication is among the most debilitating of all human conditions, poor access to high quality speech-language pathology services is an important societal issue. It creates significant health disparities affecting not only the quality of peoples' lives but also the economics of their communities. Because speech-language pathology treatment is highly behavioral and intensive, any proposed solution must improve both the performance and productivity of individual SLPs. To provide the necessary scalability required for a nation-wide problem, any implementation must include networked mobile tools as well as a strong algorithmic foundation that automatically generates objective, reliable, and sensitive outcome measures. The project has the potential for significant commercial impact. There are currently 160K SLPs in the US and that number expected to exceed 240K by 2024. Following a software-as-a-service model where clinicians freely download software to their mobile device but need active a subscription to access outcome ratings, projects to a total clinical market of between $20M and $100M.<br/><br/>The proposed project addresses the strong demand for the development of dependent objective measures of pathological speech that exists in a variety of clinical settings. Currently, perceptual (subjective) ratings of speech conducted by clinicians are the gold standard for evaluating speech improvement or decline. While it is acknowledged that subjective ratings have poor validity and reliability, objective measures have not been readily available. This project takes a novel intellectual approach to the problem by building machine-learning algorithms that objectively model experts' subjective ratings, but with levels of reliability that far exceed that of clinicians. The computational engine takes as an input a set of speech samples from a speaker and automatically evaluates the speech along clinically-standard perceptual dimensions. This yields outputs that are immediately clinical interpretable, thereby exceeding the value of norm-based objective outcomes. The project goals are, (i) to refine and extend existing technology for the evaluation of pathological speech; and (ii) to rigorously evaluate its performance and utility using beta testing. Statistical analyses will determine whether the model outperforms the human ratings with respect to reliability and sensitivity, as has been the case in pilot tests."
"1744604","EAGER: Biomimetic wireless system design for IoT networks: from sensors to brain controlled applications","ECCS","CCSS-Comms Circuits & Sens Sys","08/15/2017","07/20/2017","Tongtong Li","MI","Michigan State University","Standard Grant","Lawrence Goldberg","07/31/2019","$150,000.00","","tongli@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","ENG","7564","153E, 7916","$0.00","The Internet of Things (IoT), which networks versatile devices for information exchange, remote sensing, monitoring and control, is finding promising applications in nearly every field. Motivated by the human circulation system, this project introduces innovative methodologies that can regulate the design and analysis of future IoT networks, and provide more reliable human-device interface, including brain-controlled applications. Potentially, it can make significant contributions to the establishment of an ideal human-device platform for Smart Home, Smart Grid, health monitoring, national security, e-commerce, as well as many future applications that can benefit from fast and reliable human-device communications. Moreover, by integrating the technological advances resulting from this project into the curriculum development and outreach activities at Michigan State University, significant impacts are expected from this project on training a highly-skilled and diverse workforce in the areas of wireless communications and IoT networking. <br/><br/>This project aims to develop a unified framework for network modeling and characterization, and to develop innovative techniques for IoT network design, management and performance evaluation, so as to enable the future human-device user interface. More specifically, this project plans to: 1) develop a unified framework to characterize the convergence of network centric management and ad hoc flexibility. The new framework includes all the existing networks as special cases, and makes quantitative network performance evaluation more tractable and systematic; 2) develop innovative network design and performance analysis methodologies based on the unified framework. Optimal topology design will be provided for throughput maximization. Stability, delay and efficiency analysis will be conducted to provide benchmarks on network performance evaluation.  Diversity enhancement and dynamic routing protocol design will be carried out to reinforce network security and reliability; 3) develop innovative machine learning based overlapping user grouping techniques for massive multi-input multi-output systems. These new techniques can greatly increase the system capacity and ensure full coverage for all the IoT devices; and 4) apply the advanced tools in communications to perform multi-level computational brain analysis, and to achieve more accurate brain signal extraction. Multi-level brain analysis will result in better understanding on brain functions, dysfunctions and brain processing capacity. Accurate brain signal extraction lays the foundation for the development of reliable interface between the human mind and brain-controlled devices, which will be new members in the IoT family.  Transformative research in this project includes new IoT network design, management and performance evaluation techniques, as well as the new user interface between human mind and brain-controlled devices."
"1714060","EAPSI: Multimodal Interaction Algorithm for Human-Robot Interaction with Biologically-Inspired Robots","OISE","EAPSI","06/01/2017","05/04/2017","Joseph Campbell","AZ","Campbell                Joseph","Fellowship Award","Anne Emig","05/31/2018","$5,400.00","","","","Chandler","AZ","852266097","","O/D","7316","5921, 5978, 7316","$0.00","In order for robots to engage in physical interactions with humans, their behavior needs to be intrinsically safe. Current state-of-the-art algorithms do so by maintaining an awareness of humans with single-modality data models, for example limb positions derived from a camera. However, approaches using only a single input modality typically neglect other potentially critical sources of information. In addition, they are prone to noise, failures, or line-sight problems, since no redundant data sources are available. This project will address this issue with the development of a machine learning algorithm which performs human intention inference and determines an appropriate robot response for human-robot interaction using multi-modal data sources, such as camera, accelerometer, shoe-based pressure sensor, and electromyography data. This research will be conducted in collaboration with Dr. Shuhei Ikemoto and Dr. Koh Hosoda at Osaka University, who specialize in biologically-inspired robotics and human-robot interaction. This project benefits from Dr. Ikemoto's and Dr. Hosoda's invaluable expertise, as well as access to a human-inspired, pneumatically-actuated robot located at Osaka University which will enable unique human-robot interaction scenarios that are not currently possible at the researcher's institution.<br/><br/>Interaction Primitives are a state-of-the-art framework for modeling the interaction that takes place between a robot and a human during collaborative, physical activities. However, this existing framework is designed to work in low dimensional space with a single data modality for each agent. When introducing additional modalities in this project, the dimensionality of the input data will increase rapidly. At the same time, the number of training samples will remain constant, since that is physically constrained by the number of demonstrations that can be performed. To resolve this issue, this project will develop an extension of Interaction Primitives capable of performing density estimation with high dimension, low sample size data sets with the goal of producing safer, and more accurate interactions.<br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science."
"1742031","Network Optimization of Functional Connectivity in Neuroimaging for Differential Diagnoses of Brain Diseases","CMMI","SERVICE ENTERPRISE SYSTEMS","02/01/2017","04/25/2017","Wanpracha Chaovalitwongse","AR","University of Arkansas","Standard Grant","Georgia-Ann Klutke","08/31/2018","$45,205.00","","artchao@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","ENG","1787","076E, 078E, 116E, 8023, 9147, 9178, 9231, 9251, MANU","$0.00","The objective of this award is to develop a computational framework for identifying the critical network topology of brain connectivity in neuroimaging data, specifically functional magnetic resonance imaging (fMRI). In this framework, network optimization modeling and mathematical programming algorithms will be employed to characterize connectivity patterns in fMRI data from different brain regions. Machine learning techniques will be employed to construct a pattern recognition model used to detect biomarkers and predict the brain disease conditions (i.e., abnormals vs. controls). An information-theoretic approach will be used to select the most informative brain regions to improve the generalizability and to increase the accuracy of the diagnosis prediction model.<br/><br/>If successful, the results of this research will lead to improvements in efficiency and efficacy of brain functional connectivity modeling and new developments of optimization methods for handling large-scale spatio-temporal data. The developed computational framework will be extremely useful for neuroscientists and neurologists to identify abnormal functional connectivity in the brain and to gain a greater understanding of the brain function. The framework will be employed and tested as a novel biomarker for differential diagnoses of brain disorders. Alzheimer?s disease (AD), autism spectrum disorder (ASD), and Parkinson?s disease (PD) will be the case points in this project to test if our computational framework is a sensitive enough tool to detect alterations in brain connectivity associated with brain disorders. Accurate diagnosis can substantially extend a patient?s lifespan and some treatments have different outcomes at different disease stages. Additionally, the developed computational framework can be applied to other real-life large-scale spatio-temporal data that arise in other research areas such as manufacturing, medicine, bioinformatics, neuroscience, finance, and geosciences."
"1724891","MRI: Acquisition of High Performance Computing Infrastructure to Support Computational and Data-Enabled Science and Engineering","OAC","Major Research Instrumentation, Information Technology Researc","10/01/2017","09/14/2017","Thomas Furlani","NY","SUNY at Buffalo","Standard Grant","Stefan Robila","09/30/2018","$999,900.00","E. Bruce Pitman, Abani Patra, Jochen Autschbach, Varun Chandola","furlani@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1189, 1640","1189","$0.00","This project will acquire high-performance data analytic and computing infrastructure to support the University at Buffalo's (UB) new cross-disciplinary PhD program in Computational and Data-Enabled Science and Engineering (CDSE), as well as student/faculty research at the interface of data and computing.   Specifically, the proposed resource will enable a broad range of cross-disciplinary research throughout UB, involving 25 affiliated faculty members from across 15 departments. The integration of the resource in the CDSE program is particularly timely given the growing importance that big data, data analytics and simulation based engineering and science (SBES) have on society, science and the economy, as well as the critical need to ensure the development of an adequate workforce with the skills necessary to keep the United States at the forefront of the information age. <br/><br/>In particular, the acquired infrastructure will support research and education in a wide range of areas, such as:  the development and application of quantum theoretical methods for the simulation of spectroscopic, optical, and magnetic properties of molecular systems; financial market microstructure and asset pricing; machine learning and data mining methodologies for big data; geographic information systems; modeling volcanic eruptions and volcanic ash plumes and ash clouds that rise into the atmosphere; the integration of novel methodologies for large scale computational and data sciences to solve problem ranging from mass flows to biomechanics; regulatory genomics, gene regulation, genome annotation; quantitative network neuroscience; fast and accurate exploration of large biological and engineering data collections; and pharmacokinetic/pharmacodynamics modeling and vertical integration of big-data for target identification.  <br/><br/>Furthermore, the acquired infrastructure will provide CDSE graduates with computational, mathematical and statistical skills that provide both depth and breadth across the areas of data sciences, applied and numerical mathematics and high performance and data intensive computing. By facilitating the training of these ""next generation"" data analysts and computational scientists, who will graduate and take jobs throughout the Nation, the project will have an impact far beyond the boundaries of UB in terms of advancing knowledge and understanding across a broad spectrum of fields, including finance, engineering, materials science, geology, medicine, and economics."
"1707668","Improving Data Quality of Advanced LIGO Gravitational-Wave Searches","PHY","LIGO RESEARCH SUPPORT","08/01/2017","06/26/2018","Marco Cavaglia","MS","University of Mississippi","Continuing grant","Pedro Marronetti","02/28/2019","$240,000.00","","cavagliam@gmail.com","100 BARR HALL","UNIVERSITY","MS","386771848","6629157482","MPS","1252","9150","$0.00","This award focuses on a specific task which is mission critical for the success of gravitational-wave astrophysics in the next few years: the improvement of data quality collected by the LIGO Interferometer Gravitational-wave Observatory (LIGO) detectors in future observing runs. Research will focus on (1) using existing techniques to identify and remove non-astrophysical noise in the data stream, and (2) developing new methods to build predictive models for detector noise. Broader impacts on the development of gravitational-wave astrophysics will consist in improving LIGO's search pipelines and the performance of the detectors. Educational and public outreach initiatives will strengthen programs aimed at yielding knowledgeable teachers with enough physics content to effectively teach physics courses in school. New initiatives to promote science among diverse segments of the population will be developed through collaborations with educators in other disciplines.<br/><br/>Removing non-astrophysical artifacts from gravitational-wave data is crucial for reducing instrumental noise non-stationarity, extending the detector network duty cycle, and increasing the statistical significance of gravitational-wave candidate events. Improvements in these areas, in turn, boost parameter estimation of the gravitational-wave detections and enable refined astrophysical interpretations of the signals. Personnel funded under this award will analyze data from LIGO detector output and auxiliary sensors with the goal to isolate and identify sources of noise affecting LIGO's gravitational-wave searches. Results from these investigations will be fed back to LIGO Laboratory commissioners and instrumentation researchers to assist in the mitigation of instrumental and environmental disturbances. At the same time, Mississippi students and researchers will develop new, fast, reliable and accurate methods to model instrumental noise in interferometric gravitational-wave detectors. Machine learning-based algorithms, such as genetic programming, will be used to build predictive models to uncover the origin of non-astrophysical noise in the detectors."
"1710670","A Data-centric Approach to Turbulence Simulation","CBET","CDS&E","09/01/2017","08/21/2017","Kenneth Jansen","CO","University of Colorado at Boulder","Standard Grant","Ron Joslin","08/31/2020","$549,990.00","Philippe Spalart, John Evans","jansenke@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","ENG","8084","8084","$0.00","Turbulence or turbulent flow is commonly observed in many everyday phenomena, such as airflow over a vehicle, billowing clouds, rising smoke or fast-flowing waters.  Understanding and accurately modeling turbulent flow is important for design in applications as diverse as transportation (planes, cars, etc.), energy generation (wind turbines or more traditional power plants), and manufacturing.  Turbulence evolves through interaction of a broad range of length and time scales, posing significant challenges to engineers who model turbulent fluid flow using computer simulations. High fidelity computational models, which resolve turbulent motions, can require several months of time on the largest supercomputer to simulate even a relatively simple flow. By contrast, low fidelity models that attempt to predict only the net effects of the turbulence can give results on complicated flows in seconds but the prediction has low accuracy for many important flows.  To effectively use simulations of turbulence for design applications, engineers typically require flow solutions for thousands of problem-defining parameter combinations. The need for a large number of cases typically necessitates the use of lower fidelity models to keep cost and time feasible. This proposal involves using the more detailed information from a select, small number of higher fidelity simulations together with a full set of inexpensive low fidelity simulations to collectively achieve a substantially more accurate prediction for the thousands of parameter variations needed.  The scale-resolving simulations proposed lend themselves to animations of fluid structures. While a full understanding is beyond students at the K-12 levels, simulations that show macro- and small-scale fluid structures are expected to be highly motivating to science, technology, engineering, and mathematics (STEM) areas. Therefore, visualization results from this research are being packaged for direct outreach activities in coordination with the Broadening Opportunity through Leadership and Diversity<br/>(BOLD) Center at the University of Colorado Boulder.<br/><br/>This research project involves three main objectives: 1) using data-driven machine learning from scale-resolving simulations to improve scale-modeling closures, 2) developing multi-fidelity models that fuse data from O(1000) low-cost simulations and O(20) higher-cost simulations to achieve an accuracy approaching that of the higher-cost simulations for all O(1000) parameter combinations at substantially reduced cost, and 3) combining these two approaches. These developments are being demonstrated on flows that are currently known to be beyond the predictive capacity of the low-cost models. The focus is on adverse pressure gradient flows, including those that lead to flow separation. The simulations performed in this project will advance the state-of-the-art in adverse pressure gradient flow simulation. Specifically, simulations are being executed at momentum Reynolds numbers of 6,000 for direct numerical simulation, 30,000 for wall-resolved large eddy simulation, and 150,000 for wall-modeled large eddy simulation. The data from all simulations is being be made available to the science and engineering community to enable further analysis and fundamental insight."
"1733835","Leveraging Auxiliary Information on Marginal Distributions in Multiple Imputation for Survey Nonresponse","SES","Methodology, Measuremt & Stats, SCIENCE RESOURCES STATISTICS","09/01/2017","08/16/2017","Jerome Reiter","NC","Duke University","Standard Grant","Cheryl Eavey","08/31/2020","$300,000.00","D. Sunshine Hillygus","jerry@stat.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","SBE","1333, 8800","","$0.00","This research project will develop methods and practical tools for leveraging the information from auxiliary data sources, such as administrative records and databases gathered by private-sector data aggregators, to adjust for nonresponse in surveys.  Modern surveys have seen steep declines in response rates.  These declines threaten the validity of secondary analyses based on those incomplete data.  Government agencies and survey organizations are under increasing budgetary pressures, however, and the result is fewer resources available for extensive nonresponse follow-up activities. In this environment, government agencies and survey organizations need new options for handling missing data.  This project will provide such options, enhancing the ability of data producers to create high-quality public use datasets that account for missing data.  The project will benefit data users, including scholars who use survey data and those interested in methods for evaluating and correcting for biases due to nonresponse.  An open-source package will be developed and made widely available via the Comprehensive R Archive Network. This package will enable agencies and other users to take advantage of the methodological advances.  The project will train two Ph.D. students from underrepresented groups, one in statistical science and one in political science. The project also will engage two undergraduate students in a data science summer research experience.<br/><br/>The methodological developments to be addressed in this project will focus on the following question: How can survey organizations take advantage of information about the marginal distributions of survey variables that are available in auxiliary data sources when adjusting for nonresponse?  The project will develop methods that enable users to posit distinct specifications of missing data mechanisms for different blocks of values. The project also will develop multiple imputation routines based on machine learning techniques to handle imputation with auxiliary information in databases with large numbers of variables.  The multiple imputation framework is leveraged to propagate uncertainty not only from the missing data, but also from population-based auxiliary marginal information with potentially non-trivial uncertainty.  The project will fuse features of Bayesian modeling and classical survey-weighted estimation to ensure imputations account for complex survey designs.  The methodology will be illustrated on an application examining voter turnout among subgroups of the population in the Current Population Survey (CPS).  The application will use population-based auxiliary data from government election statistics available in the United States Elections Project and voter files available from Catalist, a leading national vendor of voter registration data. The information in the auxiliary margins will be used to adjust the CPS data for nonresponse with a more reasonable set of assumptions than previous analyses of voter turnout based on the CPS.  The CPS voter turnout application will inform scholars and policy makers about inequalities in electoral participation and provides insights about possible policy alternatives for improving voter turnout."
"1704800","SaTC: CORE: Medium: Implicit One-handed Mobile User Authentication by Induced Thumb Biometrics on Touch-screen Handheld Devices","CNS","Secure &Trustworthy Cyberspace","08/01/2017","07/19/2017","Lina Zhou","MD","University of Maryland Baltimore County","Standard Grant","Dan Cosley","04/30/2019","$718,185.00","Dongsong Zhang, Jianwei Lai","lzhou8@uncc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","8060","025Z, 7434, 7924, 9102","$0.00","People often store private and sensitive data on their mobile devices, and the security of these devices is essential. This project advances and develops a new process for verifying a user's legitimate right to access a mobile device. Existing research has not made this process very usable for many people who lack dexterity or the use of both hands. This research aims to design and develop a method for one-handed authentication on a touch-screen mobile handheld device. The objective is to improve both security and usability of authentication. The proposed methods also will detect unauthorized access to a mobile device in a continuous manner, even if the password is stolen. The interdisciplinary nature of this work will promote teaching, training, and education in mobile security and privacy, human-computer interaction, mobile accessibility, machine learning, and behavioral science. The researchers will actively engage students at both graduate and undergraduate levels in their research activities, and make a strong effort to engage women and underrepresented minorities.<br/><br/>The project will support one-handed mobile authentication on a touch-screen mobile handheld device by inducing thumb biometrics and by enabling one-handed text entry based on thumb strokes. This project will advance authentication research and practice by: (1) laying the groundwork for one-handed authentication in support of both point-of-entry and implicit continuous authentication; (2) introducing a new venue for improving the security of one-handed authentication by inducing and fusing thumb biometrics from user interactions with a touch-screen mobile device; (3) creating new design principles for improving the usability of mobile authentication; and (4) addressing accessibility challenges for users with situational or visual impairments via the support of keypress-less text entry on a mobile touch screen. This project will lend itself to a new solution that can address the common security-usability tradeoff of mobile authentication methods."
"1748444","Constrained Statistical Estimation and Inference: Theory, Algorithms and Applications","DMS","STATISTICS","07/01/2017","07/26/2017","John Lafferty","CT","Yale University","Standard Grant","Gabor Szekely","07/31/2018","$144,973.00","","john.lafferty@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","MPS","1269","","$0.00","This project lies at the boundary of statistics and machine learning. The underlying theme is to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The project will explore theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications. The overall goal of the research is to develop theory and tools that can help scientists to conduct more effective data analysis.<br/><br/>Many statistical methods are purely ""data driven"" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central. The project will develop minimax theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. Other constraints to be studied include shape restrictions such as convexity and monotonicity for high dimensional data. The project will also investigate the incorporation of physical constraints through the use of PDEs and models of physical dynamics and mechanics, focusing on both algorithms and theoretical bounds."
"1726895","Workshop:  Disrupting Illicit Supply Networks: New Applications of Operations Research and Data Analytics to End Modern Slavery; Austin, Texas, and Washington, DC","CMMI","OE Operations Engineering, LAW AND SOCIAL SCIENCES","04/01/2017","03/30/2017","Noel Busch-Armendariz","TX","University of Texas at Austin","Standard Grant","Irina Dolinskaya","03/31/2018","$99,829.00","Matt Kammer-Kerwick","nbusch@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","ENG","006Y, 1372","073E, 1372, 5514, 7556","$0.00","This award provides support for a workshop to convene an interdisciplinary team of scholars to identify promising research directions for applications of operations research (OR) and data analytics aimed at the disruption of illicit supply networks such as human trafficking.    The United States Department of State considers human trafficking a form of modern-day slavery and broadly defines it to be when a person is deceived or coerced in situations of prostitution, forced labor, or domestic servitude.  Human trafficking is a key example of illicit networks that operate in and to the detriment of society.  Other examples include arms trafficking, drug trafficking, animal trafficking, and human smuggling. By bringing together scholars from disparate disciplines, the goal of the workshop is to identify new research approaches and breakthrough strategies for disrupting the illicit networks commonly found in human trafficking.<br/><br/>The workshop, which will consist of two meetings, the first to be held at The University of Texas at Austin and the second to be held in Washington, DC, will enable scholars from operations research, management science, analytics, machine learning, and data science to exchange ideas and outline a potential research agenda for the development of disruptive interventions against illicit networks.  The agenda developed at this workshop will help move understanding of such illicit systems from descriptive characterization and predictive estimation toward improved dynamic operational control.  OR and data analytics are fields ideally suited to bring this perspective to the study of illicit networks.  Few studies have approached this problem from a dynamic systems theoretical perspective that allows the social justice challenge to be represented as a mathematical system that can be analyzed in terms of decision variables to help guide, control, and constrain behavioral dynamics toward desired goals.  Solutions to remediate the effect of illicit networks are inherently interdisciplinary, typically involving the fields of criminal justice, social work, social science, economics, healthcare, and law.  Such systems are dynamic and exploit and victimize members of the community. What is more, they involve both legal and illicit activities at the same time, which often obscures criminal activity from law enforcement.  These systems can be hierarchical, nonstationary networks of interconnected activities and participants that involve intersectional decision making by perpetrators, victims, and/or bystanders.  Highly common among such systems is a paucity of data due in large part to the hidden aspects of the crime and the partial observability of the population of interest.  The workshop aims include examining the structure and nature of illicit networks within an analytic and modeling framework; exploring the form and complexity of viable, real-world solutions using OR methodologies; assessing the characteristics and amount of data needed to model and analyze the problem; proposing a research agenda to guide the efforts of interdisciplinary teams of scholars to develop methods and solutions; and initiating and facilitating ongoing interactions among workshop attendees and their research collaborators, including junior investigators and graduate students."
"1707943","Dynamics, aging and universality in complex systems","DMS","PROBABILITY","04/01/2017","01/04/2017","Paul Bourgade","NY","New York University","Standard Grant","Tomek Bartoszynski","03/31/2018","$49,000.00","Antonio Auffinger, Ivan Corwin","bourgade@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","MPS","1263","7556","$0.00","This award supports the conference ""Dynamics, Aging and Universality in Complex Systems"" that will be held from the 19th-23rd of June, 2017, at the Courant Institute (NYU) in New York City. The conference has 30 confirmed speakers in very active and fundamental areas at the intersection of probability, analysis, and mathematical physics. The main aim in the rigorous study of complex disordered systems is to understand how random microscopic systems display predictable and statistically universal collective macroscopic behaviors. Many such systems evolve to stationary states, but do so in very long time scales (e.g., exponential in system size). Dynamics and aging in these complex systems seeks to understand how their complicated energy landscapes governs their pre-stationary behavior. There are many different approaches in this area that will be represented at the conference, including spin-glasses, stochastic partial differential equations (PDEs), large deviation theory, and random matrix theory. This gathering will lead to cross-fertilization between these areas as well as inspire and inform a new generation of researchers. This event will attract a large and diverse spectrum of participants, including many earlier career researchers, women and under-represented minorities.<br/><br/>Many of the complex systems which will be discussed in this conference aim to describe real world phenomena and results proved for the mathematical models provide predictions applicable to the real systems. At a simpler level, think, for instance, of how the central limit theorem for sums of independent random variables predicts the Gaussian statistics which drives a large part of statistics in the real world. Besides physical applications, complex systems (e.g., spin glasses, growth processes and random matrices) have found many applications in computer science, machine learning, data science, bioinformatics, chemistry, and even areas like ecology and earth science.<br/><br/>The website for this conference is: http://cims.nyu.edu/conferences/gba60/"
"1722068","SBIR Phase I:  Automated Security for the DevOps World","IIP","SMALL BUSINESS PHASE I","07/01/2017","06/28/2017","Andrew Davidson","CA","Tala Security Inc","Standard Grant","Peter Atherton","03/31/2018","$225,000.00","","drew@talasecurity.io","1185 Adler Ct","Fremont","CA","945364027","4086558382","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to provide robust, automated security protections to enterprise web applications. With the growing threat of attacks at the web application layer, security is a top of mind concern for both the developers and users of web applications. At the same time, defending against these attacks is a resource-intensive, error-prone process that typically is performed manually by specialized web security experts. The project will use application-aware analysis and an innovative information model to automatically build and deploy web security policies on enterprise servers, freeing the enterprise to focus on their core business and reduce the overhead of internal security teams. The project will also use a hybrid static/dynamic analysis to automatically craft server-side and client-side policies using modern web protection mechanisms to give users of web applications confidence that their interactions with a web application will be secure. This approach leverages cutting-edge techniques from program analysis and machine learning, and will advance the state of the art in automated security policy generation.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will explore ways in which automated security policies enact meaningful protection of sensitive resources. Protecting web applications is a challenging research problem in that automated approaches are either biased towards significant false positive rates, in which benign behavior is flagged as malicious, or significant false negative rates, in which malicious activity is not detected. This project aims to resolve this bias using application aware whitelists: the project automatically explores an application's behavior dynamically, and will also statically build a model of that application's purpose. The project is expected to yield two major components: a server-side protection module to isolate and harden sensitive resources on an enterprise deployment, and a client-side protection module to enforce fine-grained policies for users of the enterprise's web applications. The project will involve developing, analyzing, and improving both of these components using large-scale web applications and will integrate the experience and challenges of enterprises with leading web applications."
"1648907","STTR Phase I:  Advanced and Remote Screening, Monitoring and Rehabilitation of Cognitive Health","IIP","STTR PHASE I","01/01/2017","12/16/2016","Debraj De","MO","Smart Health Beacons, LLC","Standard Grant","Jesus Soriano Molla","12/31/2017","$225,000.00","Sajal Das","debraj.de1@gmail.com","900 Innovation Drive","Rolla","MO","654012985","6782279033","ENG","1505","1505, 8018, 8032, 8042, 8089, 8091, 9150","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is to improve and automate cognitive health screening mechanisms used in hospital facilities, by designing and validating a smart chair and a smart wearable based health data-centric novel solution. U.S. population has more than 16 million people (this is rapidly growing further) living with cognitive impairment. There is alarming fact that people with cognitive impairment report more than three times as many hospital stays as individuals with other health conditions. These are driving development of improved cognitive health assessment solutions, that will detect subtle signs of cognitive decline early in daily life. Motivated by these markets in demand, this project is designed to develop a system for advanced and remote screening and monitoring of cognitive health, and also enabling gamified user interaction for cognitive rehabilitation. The developed technology has large potential to help elderly people prone to levels of dementia (from mild cognitive impairment to Alzheimer's disease), and slowly rehabilitating patients dealing with cognitive decline (stroke survivors and cancer patients under treatment). The designed platform and cognitive scoring algorithms developed in this project will be tested and validated with real patient study at hospital facility.<br/><br/><br/>The proposed project is a research, development and clinical studies effort to design and validate an advanced and remote screening, monitoring and rehabilitation solution for cognitive health. Such cognitive health management system will be achieved by technological innovation in multi-modal sensing and machine learning based behavioral data analytics from a smart chair and a custom wearable device (used separately), along with cognitive psychology driven gamified interactions and interventions. The smart chair system will capture patient non-verbal response and stress patterns, when answering to cognitive questionnaire. The smart wearable system (used with a few custom Bluetooth beacons in proximity) will analyze normalcy, forgetfulness, confusion in patient, while performing daily life cognitive tasks (such as baking a cake or cookie) in home-like kitchen facility in hospital. The main research objective includes two novel cognitive health scoring mechanism from user response with chair based cognitive questionnaire, and wearable based cognitive task performance quality. It will also design user interactions and interventions for cognitive rehabilitation and behavior reinforcement. This is a very impactful project with team's direct ongoing collaboration with hospital and neurologist, and access to large number of patients with range of cognitive disabilities (in accordance with IRB and HIPAA compliance)."
