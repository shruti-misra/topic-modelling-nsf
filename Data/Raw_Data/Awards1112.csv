"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1054215","CAREER: Computation and Approximation in Structured Learning","IIS","Robust Intelligence","06/01/2011","06/03/2011","Ben Taskar","PA","University of Pennsylvania","Standard Grant","Todd Leen","05/31/2013","$500,000.00","","taskar@gmail.com","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7495","1045, 1187, 7495","$0.00","Machine learning is transforming the way many fields make sense of data, from engineering and science to medicine and business. Machine learning has vastly improved speech recognition, machine translation, robotic navigation and many other prediction tasks. A crucial goal of machine learning is automating intelligent processing of information: this project will focus on automatically describing videos by detecting objects, people, actions and interactions between them, and parsing documents by extracting entities, events and relationships between them.  All these prediction tasks require more than just true-false or multiple-choice answers, but have an exponential number of possible answers to consider. Breaking these joint predictions up into independent decisions (for example, translating each word on its own, recognizing a phoneme at a time, detecting each object separately) ignores critical correlations and leads to poor accuracy.<br/><br/>Structured models, such as grammars and graphical models, can capture strong dependencies but at considerable computational costs. The barrier to improving accuracy in such structured prediction problems is the prohibitive cost of inference.  Structured prediction problems present a fundamental trade-off between approximation error and inference error due to computational constraints as we consider models of increasing complexity. This trade-off is poorly understood but is constantly encountered in machine learning applications.<br/><br/>The primary outcome of this project will be a framework for addressing very large scale structured prediction using a novel coarse-to-fine architecture. This architecture will enable explicit, data-driven control of the approximation/computation trade-off.  It promises to drastically advance state-of-the-art accuracy in computer vision and natural language applications and greatly enhance search and organization of documents, images, and video. The PI's plan includes an active role in the machine learning community, disseminating results through tutorials, code and data and organizing workshops."
"1054630","CAREER: Online Education as a Vehicle for Human Computation","IIS","HCC-Human-Centered Computing","03/15/2011","03/16/2011","Luis von Ahn","PA","Carnegie-Mellon University","Standard Grant","William Bainbridge","02/29/2016","$482,052.00","","biglou@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","1045, 1187, 7367","$0.00","Human computation is a growing research area that studies how to harness the combined power of humans and computers to solve problems that would be impossible for either to solve alone. The goal of this project is to introduce online education as a new vehicle and incentive mechanism for human computation. The central hypothesis is that problems that are difficult for computers can be transformed into tasks that are also educational, so that students solve the problems at the same time as they learn. With millions of people learning online, education could provide a powerful motivator for participation in distributed human computation. This project will demonstrate that education allows significantly more complex problems to be attacked with human computation than has been possible with previous paradigms for human computation. The project will also explore whether human computation can be a motivator for education. <br/><br/>The hypothesis will be tested on a new large-scale system called Duolingo, a free language-learning site in which students will solve problems that computers cannot yet solve. The site will present students with many types of activities, each exercising a different aspect of the foreign language while simultaneously channeling the students to perform a different task that artificial intelligence cannot yet accomplish. Some of the tasks that students will perform include: language translation, audio transcription, and image tagging. <br/><br/>Broader impacts.  The project will provide a free language-learning site expected to help millions of users learn a foreign language.  It will also provide large quantities of useful data in many languages to train more accurate machine learning algorithms for language translation, voice recognition, and computer vision.  Duolingo will also serve as a platform for performing large-scale experiments on how people learn languages online. In addition, undergraduate and graduate classes will be improved and developed using this research."
"1046589","SBIR Phase I: Serious Gaming Platform for Mastering the Physician-Patient Diagnostic Interview","IIP","SMALL BUSINESS PHASE I","01/01/2011","12/15/2010","David Baker","WV","IntelligentSimulations LLC","Standard Grant","Glenn H. Larsen","06/30/2011","$149,335.00","","vicbaker01@gmail.com","1022 Laurelwood Drive","Morgantown","WV","265088900","3042918977","ENG","5371","1658, 5371, 9216, HPCC, 9150","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims to implement a new strategy for teaching medical students to interact with and relate to patients. Present teaching methods are labor intensive for instructors and students and limited to fixed settings. The project team will develop a serious gaming software platform that captures the human element by simulating patient responses (level of trust, topic comfort, and sense of urgency) through the use of ?intelligent, emoting avatars? (virtual patients) represented through artificial intelligence (AI) and dynamic video. The software platform replaces the passive experience of dialog tree based character interaction with the active experience of dynamic conversation. The training platform will have elements of virtual reality (interactive, immersive, and experiential) and be capable of supporting libraries of virtual patients. The innovation will enable applications to provide a new level of emotional experience for the users by achieving an emotional attachment and sense of bonding with intelligent virtual characters, capable of dynamic conversation and emotional responses based on concepts involving level of trust, topic comfort, and sense of urgency. The product will enable the student to practice and learn at their own computer on their own schedule within a ?learning environment? that is extraordinarily rich and realistic.<br/><br/>The broader impact/commercial potential of this project will change the way medical students learn in the medical school environment. Medical school teaching methods are costly, labor intensive for instructors and students and limited to fixed settings. The project team will develop a serious gaming software platform that relies on artificial intelligence and makes use of the interactive, immersive, and experiential elements of virtual reality that have made video games so successful. They will adapt these elements to serious gaming to create an instructional technology to teach medical students to interact with and relate to patients. The student will interact with a virtual patient by entering keyboard based dialog or by using speech recognition technology. The product will enable the student to practice and learn at their own computer on their own schedule. The essentially passive experience of dialog tree based character interaction is replaced with the active experience of dynamic conversation. This technology is intended to address the problems posed by present expensive medical school teaching methods which are labor intensive for instructors and students. The technology will offer advances over traditional on-line and class-room instruction materials such as video presentations, PowerPoint and flat written formats. Over the long term, this entirely scalable technology has the potential to fundamentally enrich the educational environment for medical school teaching with a correspondingly enormous commercial profitability and societal impact."
"1145358","SIG-011: International Workshop on Stochastic Image Grammars","IIS","Robust Intelligence","10/01/2011","07/23/2011","Sinisa Todorovic","OR","Oregon State University","Standard Grant","Jie Yang","09/30/2012","$5,000.00","","sinisa@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","7495, 7556","$0.00","This travel grant supports two US participants to attend the International Workshop on Stochastic Image Grammars (SIG-11). The notion of stochastic image grammars encompasses hierarchical representations of objects and events occurring in images and video, and their associated learning and inference algorithms. The virtue of image grammars lies in their expressive power to represent an exponentially large number of object and event configurations by using a relatively much smaller vocabulary, and a few compositional rules. <br/><br/>Statistics, machine learning, natural language processing, and cognitive psychology experience a resurgence of stochastic grammars. In computer vision, however, this momentum seems to be present only in the area of 2D object recognition. The main objective of the workshop is to promote interdisciplinary research among these traditionally separate scientific disciplines toward grammar-based formulations of a wider range of vision problems, beyond object recognition, such as, e.g., 3D structure from motion, and activity recognition. The workshop is also aimed at reducing the apparent disconnect between research groups working on image grammars, by addressing the need for a unified theoretical framework. To this end, SIG-11 provides a forum for sharing research experiences in grammars between the vision community and the keynote speakers who are experts in cognitive psychology, neuroscience, and natural language processing. Solicited peer-reviewed papers are expected to be published in the proceedings of the 13th International Conference on Computer Vision."
"1128436","Collaborative: Gesture Recognition Challenge","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2011","07/21/2011","Isabelle Guyon","CA","Clopinet","Standard Grant","Paul Werbos","08/31/2012","$49,800.00","","guyon@clopinet.com","955 Creston Road","Berkeley","CA","947081501","5105246211","ENG","7607","1653, 9102","$0.00","The objective of this research is both to advance the field of video data processing (more particularly gesture recognition) and to illustrate the power of deep learning architectures and transfer learning. The approach is to organize a challenge culminating in a life evaluation at the site of a conference.<br/>Intellectual merit: Much of the recent research in Adaptive and Intelligent Systems (AIS) has sacrificed the grand goal of designing systems ever approaching human intelligence for solving data mining tasks of practical interest with more immediate reward. This project gives an opportunity to deep learning architectures inspired by neural networks to demonstrate their ability to address more complex problems requiring to transfer knowledge from task to task (transfer learning), leveraging the availability of video data not directly related to the target task of gesture recognition. The participants will also be involved in a data exchange to grow an unprecedented large and diverse database of gestures. <br/>Broader Impact: Challenges have proved to be a great stimulus of research. For a long lasting impact, the challenge platform and the data and software repositories will remain open beyond the term of the NSF funded project. The educational components of the project include engaging students in the contest, providing material directly usable in teaching curricula, and demonstrating gesture recognition to high school students to expose them to computer vision research and sign language communication. Our connections with the deaf community will allow us to gear the product of this research to advance assistive technology."
"1114833","RI: Small: Temporal and Spatiotemporal Processing in Recurrent Neural Networks with Unsupervised Learning","IIS","Robust Intelligence","09/01/2011","09/09/2011","Dean Buonomano","CA","University of California-Los Angeles","Standard Grant","Kenneth Whang","08/31/2014","$249,616.00","","dbuono@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7923","$0.00","The brain's ability to perform complex forms of pattern recognition, such as speech discrimination, far exceeds that of the best computer programs. One of the strengths of human pattern recognition is its seamless processing of the temporal structure and temporal features of stimuli. For example, the phrase ""he gave her cat food"" can convey two different meanings depending on whether the speaker emphasizes the pause between ""her"" and ""cat,"" or ""cat"" and ""food."" Attempts to emulate the brain's ability to discriminate such patterns using artificial neural networks have had only limited success. These models, however, have traditionally not captured how the brain processes temporal information. Indeed most of these models have treated time as equivalent to a spatial dimension, in essence assuming that the same input is buffered and played at different delays. Similarly, more traditional approaches to pattern recognition, which generally rely on discrete time bins, also do not capture how the brain processes temporal information. The goal of the current research is to use a framework, referred to as state-dependent networks or reservoir computing, to simulate the brain's ability to process both the spatial and temporal features of stimuli. A critical component of this framework is that temporal information is automatically encoded in the state of the network as a result of the interaction between incoming stimuli and internal states of recurrent networks.<br/><br/>This project will develop a general model of spatiotemporal pattern recognition focusing on speech discrimination. The model will incorporate plasticity, a critical characteristic of the brain that has eluded previous state-dependent network models. Plasticity is a cardinal feature of the brain's computational power. For example, in the context of speech recognition, even at the age of 6 months, the brains of babies are tuned to recognize sounds of their native language. This ability is an example of experience-dependent cortical plasticity and it relies in part on synaptic plasticity and cortical reorganization. Incorporating synaptic plasticity into recurrent networks has proven to be a very challenging problem as a result of the inherent nonlinear and feedback dynamics of recurrent networks. The current project will use a novel unsupervised form of synaptic plasticity--based on empirically observed forms of plasticity referred to as homeostatic synaptic plasticity--to endow state-dependent networks with the ability to adapt and self-tune to the stimulus set the network is exposed to. This project interfaces recent advances in theoretical neuroscience and novel approaches in machine learning. The results will help develop artificial neural networks that capture the brain's ability to process temporal information and reorganize in response to experience."
"1117591","RI: Small: Ensemble Methods for Structured Prediction","IIS","Robust Intelligence","08/01/2011","07/29/2011","Mehryar Mohri","NY","New York University","Standard Grant","Weng-keen Wong","07/31/2016","$407,074.00","","mohri@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7495","7923","$0.00","Ensemble methods are general techniques in machine learning for combining several hypotheses to create a more accurate predictor.  In the batch learning setting, techniques such as bagging, boosting, stacking, error-correction techniques, Bayesian averaging, or other averaging schemes are common instances of these methods.  These methods often significantly improve performance in practice and often benefit from favorable learning guarantees, typically in terms of the margins of the training samples. <br/><br/>However, ensemble methods and their theory have been developed primarily for the common binary classification problem, or standard regression tasks where the target labels are real numbers and thus have no structure. These techniques do not readily apply to structured prediction problems such as pronunciation modeling, speech recognition, parsing, machine translation, or image processing. The objective of this proposal is to create the theoretical foundation, large-scale algorithms, and practical techniques for devising effective ensembles of structured prediction techniques. The benefits of these algorithms are likely to be at least as significant as those resulting from ensemble techniques in binary classification.<br/><br/>Our solutions will be crucial to a broad set of applications and will be made widely accessible through open-source software programs. These software and open-source programs will make the use of our learning algorithms accessible to a broad community of researchers and engineers.  More broadly, our techniques will benefit the society through the discovery of significantly more accurate solutions to a variety of important problems including speech recognition, speech synthesis, and machine translation."
"1008747","Emerging Research - Empirical Research-- Measuring the Impact of 0nline Discourse in Undergraduate STEM Courses: Semi-automatic Assessment of Large Discussion Board Corpora","DRL","REAL","09/01/2011","09/16/2015","Jihie Kim","CA","University of Southern California","Standard Grant","Finbarr Sloane","11/30/2015","$327,546.00","Erin Shaw, Gisele Ragusa","jihie@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","EHR","7625","7625, 9177, 9178, 9251, SMET","$0.00","The investigators seek to explore the extent to which the quantity and quality of student participation in course discussion boards (a.k.a. online asynchronous discussions (OAD)) is associated with retention in or dropping out from undergraduate computer science and industrial engineering majors.   This study represents a planning and pilot study using data from discussion board enhanced STEM courses at the University of Southern California.   The ultimate goal of the investigators' intended future research will be to produce knowledge usable in making more effective use of this learning technology.<br/><br/>The methods include computational analysis of the discussion text, a survey questionnaire, and information gathered from the registrar's office.  Machine learning/natural process learning techniques will be used to process the data. The investigators will use generalized linear modeling and Multivariate Analysis of Covariance techniques in their analysis. Additionally, the investigators seek to determine differences in participation by demographic characteristics and language/technical backgrounds.<br/><br/>Discussion boards are now commonplace in undergraduate STEM learning environments, yet a solid base of research on their use and on user behaviors does not exist.  This research is intended to begin to fill that gap.  And, given many instructional reforms depend upon them, it is vital that we understand these factors better in order to improve instruction and learning."
"1116541","RI: Small: Addressing Visual Analogy Problems on the Raven's Intelligence Test","IIS","ROBUST INTELLIGENCE","08/01/2011","07/30/2012","Ashok Goel","GA","Georgia Tech Research Corporation","Continuing grant","Hector Munoz-Avila","07/31/2015","$450,000.00","","ashok.goel@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","7923","$0.00","This proposal aims to create purely image-based reasoning methods for solving visual analogy problems, particularly so-called Raven's Progressive Matrices (RPM) problems. The project draws on recent results from the study of human cognition as well computer science and mathematics. Raven's Progressive Matrices consist wholly of visual analogy problems in which a matrix of geometric figures is presented with one entry missing, and the correct missing entry must be selected from a set of answer choices. Recent analysis of RPM data suggests that although in general the performance of individuals with autism on most intelligence tests is significantly inferior to that of typically developing individuals, on the Raven's test the performance of the two groups is comparable. This data is consistent with the ""Thinking in Pictures"" hypothesis that has been proposed as a potential, partial cognitive explanation of autism. In both artificial intelligence and psychology, current theories of solving RPM problems first convert the visual inputs into verbal representations and then process the verbal representations. In contrast, this project explores the hypothesis that many RPM problems can be solved using only visual representations, without extracting any verbal representations from the input images. This project will develop and analyze computational techniques for addressing RPM problems with only visual representations. <br/><br/>In particular, this project will develop a novel algorithm based on affine transformations for addressing RPM problems as well as a second algorithm that makes use of fractal encodings. With both approaches -- affine and fractal -- the project seeks to achieve human-level performance on RPM in terms of percentages of problems solved correctly. The two algorithms will also be tested on the ""odd-man-out"" corpus that contains thousands of visual analogy problems. The project will formally characterize the set of visual analogy problems for which the affine and fractal algorithms are applicable, analyze the computational properties of the algorithms, construct proofs of their correctness for specific classes of problems, and compare the errors made by the two algorithms with those made by two groups of humans -- typically developing individuals and individuals with autism. The project will parameterize the visual algorithms to detect the settings under which the patterns of errors made by an algorithm on RPM problems most closely match the error patterns of the two human groupings. <br/><br/>Autism is an important problem of growing social concern. While the thinking-in-pictures hypothesis has long been a significant insight into cognition in autism, and empirical evidence -- both behavioral and neuroimaging -- in its favor is increasing, there have been no computational models for it. The proposed research would help provide a computational form to this hypothesis and may help establish a disposition towards visual thinking with autism. RPM is considered one of the core tests of intelligence, and although there have been several suggestions about the visuospatial nature of RPM problems, all current computational models addressing such visual analogy problems use sequential processing on propositional representations of the input images. The algorithms from this project that rely on visual representations for RPM could provide new insights into intelligence testing. Lastly, while fractal encodings have been used in computer graphics for generating images and in computer vision for texture analysis in image processing, this project's use of fractal encodings for visual analogies on intelligence tests will contribute to knowledge of fractal computing."
"1122504","DIP: Teaching Writing and Argumentation with AI-Supported Diagramming and Peer Review","IIS","NATIONAL SMETE DIGITAL LIBRARY, Cyberlearn & Future Learn Tech","09/01/2011","04/09/2012","Kevin Ashley","PA","University of Pittsburgh","Standard Grant","Tatiana Korelsky","08/31/2017","$1,390,735.00","Diane Litman, Christian Schunn","ashley@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7444, 8020","7444, 8045, 8842, 9251","$0.00","The PIs are investigating the design of intelligent tutoring systems (ITSs) that are aimed at learning in unstructured domains. Such systems are not able to do as much automatically as ITSs working in traditionally narrow and well-structured domains, but rather they need to share responsibilities for scaffolding learning with a teacher and/or peers. In the work proposed, the three PIs, who share expertise in automated natural language understanding, intelligent tutoring systems, machine learning, argumentation (especially in law), complex problem solving, and engineering education, are integrating intelligent tutoring, data mining, machine learning, and language processing to design a socio-technical system (people and machines working together) that helps undergraduates and law students write better argumentative essays. The work of helping learners derive an argument is shared by the computer and peers, as is the work of helping peer reviewers review the writing of others and the work of learners to turn their argument diagrams into well-written documents. Research questions address the roles computers might take on in promoting writing and the technology that enables that, how to distribute scaffolding between an intelligent machine and human agents, how to promote better writing (especially the relationship between diagramming and writing), and how to promote learning through peer review of the writing of others. <br/><br/>This project is bringing together outstanding researchers from a variety of different disciplines -- artificial intelligence, law education, engineering and science education, and cognitive psychology -- to address an education issue of national concern -- writing, especially writing that makes and substantiates a point -- and to explore ways of extending intelligent tutoring systems beyond fact-based domains. It fulfills all aims of the Cyberlearning program -- to imagine, design, and learn how to best design and use the next generation of learning technologies, to address learning issues of national importance, and to contribute to understanding of how people learn."
"1065632","CIF: Medium: Collaborative Research: Information Theory and Statistical Inference from Large-Alphabet Data","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2011","08/08/2011","Narayana Santhanam","HI","University of Hawaii","Standard Grant","Richard Brown","07/31/2016","$369,620.00","","nsanthan@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","CSE","7797","7924, 7935, 9150","$0.00","Statistical analysis is key to many challenging applications such as text classification, speech recognition, and DNA analysis. However, often the amount of data available is comparable or even smaller than the set of symbols (alphabet) constituting the data. Unfortunately, not much is known about optimal inference in this so-called large-alphabet domain. Recently, several promising approaches have been developed by different scientific communities, including Bayesian nonparametrics in statistics and machine learning, universal compression in information theory, and the theory of graph limits in mathematics and computer science.<br/><br/>The investigators study the problem drawing from these multiple perspectives, but with a particular focus on developing the information theoretic approach. The research studies analytical properties of the ""pattern maximum likelihood'' estimator, which performs well in practice but is not understood theoretically, and also explores computational speedups. Moreover, it attempts to delineate which problem classes are better handled by Bayesian nonparametric techniques and which by the pattern approach, and explores links between these approaches. The investigators use the resulting theory for automatic document classification, allowing for more automation in storing, retrieving, and analyzing data. Furthermore, the investigators use the theory to study genetic variations, whose link with disease diagnosis is a crucial step in the systematic quantification of biology that is playing an increasingly important role in medical advancement. The research also brings new courses to the classroom, with a special outreach effort to involve women and under-represented minorities, including through the Native Hawaiian Science and Engineering Mentorship Program."
"1065494","CIF: Medium: Collaborative Research: Information Theory and Statistical Inference from Large-Alphabet Data","CCF","Comm & Information Foundations","08/01/2011","08/08/2011","Mokshay Madiman","CT","Yale University","Standard Grant","Phillip Regalia","10/31/2013","$400,471.00","","madiman@udel.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7797","7924, 7935, 9150","$0.00","Statistical analysis is key to many challenging applications such as text classification, speech recognition, and DNA analysis. However, often the amount of data available is comparable or even smaller than the set of symbols (alphabet) constituting the data. Unfortunately, not much is known about optimal inference in this so-called large-alphabet domain. Recently, several promising approaches have been developed by different scientific communities, including Bayesian nonparametrics in statistics and machine learning, universal compression in information theory, and the theory of graph limits in mathematics and computer science.<br/><br/>The investigators study the problem drawing from these multiple perspectives, but with a particular focus on developing the information theoretic approach. The research studies analytical properties of the ""pattern maximum likelihood'' estimator, which performs well in practice but is not understood theoretically, and also explores computational speedups. Moreover, it attempts to delineate which problem classes are better handled by Bayesian nonparametric techniques and which by the pattern approach, and explores links between these approaches. The investigators use the resulting theory for automatic document classification, allowing for more automation in storing, retrieving, and analyzing data. Furthermore, the investigators use the theory to study genetic variations, whose link with disease diagnosis is a crucial step in the systematic quantification of biology that is playing an increasingly important role in medical advancement. The research also brings new courses to the classroom, with a special outreach effort to involve women and under-represented minorities, including through the Native Hawaiian Science and Engineering Mentorship Program."
"1065622","CIF: Medium: Collaborative Research: Information Theory and Statistical Inference from Large-Alphabet Data","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2011","08/08/2011","Alon Orlitsky","CA","University of California-San Diego","Standard Grant","John Cozzens","07/31/2016","$418,565.00","","alon@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7924, 7935, 9150","$0.00","Statistical analysis is key to many challenging applications such as text classification, speech recognition, and DNA analysis. However, often the amount of data available is comparable or even smaller than the set of symbols (alphabet) constituting the data. Unfortunately, not much is known about optimal inference in this so-called large-alphabet domain. Recently, several promising approaches have been developed by different scientific communities, including Bayesian nonparametrics in statistics and machine learning, universal compression in information theory, and the theory of graph limits in mathematics and computer science.<br/><br/>The investigators study the problem drawing from these multiple perspectives, but with a particular focus on developing the information theoretic approach. The research studies analytical properties of the ""pattern maximum likelihood'' estimator, which performs well in practice but is not understood theoretically, and also explores computational speedups. Moreover, it attempts to delineate which problem classes are better handled by Bayesian nonparametric techniques and which by the pattern approach, and explores links between these approaches. The investigators use the resulting theory for automatic document classification, allowing for more automation in storing, retrieving, and analyzing data. Furthermore, the investigators use the theory to study genetic variations, whose link with disease diagnosis is a crucial step in the systematic quantification of biology that is playing an increasingly important role in medical advancement. The research also brings new courses to the classroom, with a special outreach effort to involve women and under-represented minorities, including through the Native Hawaiian Science and Engineering Mentorship Program."
"1202141","CAREER: Enabling Community-Scale Modeling of Human Behavior and its Application to Healthcare","IIS","ROBUST INTELLIGENCE","10/01/2011","06/13/2014","Tanzeem Choudhury","NY","Cornell University","Continuing grant","Hector Munoz-Avila","02/29/2016","$440,125.00","","tanzeem.choudhury@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7495","1045, 1187, 7495, 9102, 9150, 9215, 9251, HPCC","$0.00","Research supported by this award is developing community-based methods for sensing, recognizing, and interpreting human activities from body-worn sensors. Specifically, this research is<br/><br/>1) developing systems that learn new classes of activity with minimal human supervision, where the system queries a human user for additional information on an activity being learned, but only when such queries are informationally necessary and behaviorally unobtrusive,<br/><br/>2) developing the paradigm of community-guided learning, which leverages people's social ties and behavioral similarities, in order to define an efficient scheme for sharing various aspects of the underlying activity classes across many individuals, and<br/><br/>3) evaluating the new community-guided learning methods by using them to learn about (a) social isolation and functional independence among elderly persons, and (b) social interaction among high-functioning autistic children.<br/><br/>Speaking generally, the research is advancing machine learning and artificial intelligence, especially in the areas of semi-supervised, active, and relational learning. Beyond these basic scientific contributions, the resulting research has the potential to transform community health assessment by collecting fine-grained clinically-relevant information continuously, cheaply, and unobtrusively, over long periods of time. This research also opens up many opportunities for education and outreach, in part because it is pushing machine learning and artificial intelligence into social and societally-important realms, promising to attract groups, notably women, who are under-represented in computer science.<br/>"
"1147641","EAGER: IIS: RI: Learning in Continuous and High Dimensional Action Spaces","IIS","ROBUST INTELLIGENCE","09/01/2011","08/05/2011","Ronald Parr","NC","Duke University","Standard Grant","Todd Leen","08/31/2013","$149,996.00","","parr@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495","7916","$0.00","The proposed research is in the general area of ""planning under uncertainty.""  This topic addresses the problem of choosing good actions in situations where actions do not have deterministic outcomes.  Applications for this general framework include but are not limited to robotic control, medical decision making, and business optimization.  The overarching mathematical framework for this problem is that of decision theory or Markov decision processes, topics that are studied in a wide range of fields, including engineering, economics, operations research and, more recently, artificial intelligence.  <br/><br/>Recent technical efforts in this area have sought to address large problems by combining successful statistical and machine learning techniques with decision-theoretic reasoning.  The underlying insight behind these efforts is that machine learning can generalize across similar states of the world, thereby allowing algorithms to propose good actions for new states of the world without explicitly considering every possible state or outcome, as was required by classical approaches.<br/><br/>The combination of classical decision theoretic methods and machine learning has shown great promise for large state spaces, but one aspect that has been under-explored is large action spaces.  Large action spaces arise naturally from a fine discretization of a continuous action space or from a large set of discrete choices, such as assignments of firefighters to regions on a map.  One way to address the general challenge would be to group actions into sets and use machine learning methods to predict which set is preferred.  By doing this multiple times over carefully arranged partitions of the action space, it should possible to achieve an exponential reduction in the effort required to select the best action.<br/><br/>Potential applications of this research include robotic control, power grid management, and forest/fire management strategies."
"1116384","HCC: Small: Building Audio Interfaces with Crowdsourced Concept Maps and Active Transfer Learning","IIS","HCC-Human-Centered Computing","09/01/2011","08/16/2011","Bryan Pardo","IL","Northwestern University","Standard Grant","Ephraim Glinert","08/31/2016","$499,804.00","Darren Gergle","pardo@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7367","7367, 7923","$0.00","The United States is a world-leader in software and in multimedia content (e.g. music, film). To remain so, we must continually raise the bar in both software and media production. Software tools for media production (e.g. the audio production suite Protools) often have complex interfaces, conceptualized in ways that makes it difficult for any but the most expert to realize the power of these tools. Complex interfaces and steep learning curves can discourage creative people from doing their best work with such tools. Here, we focus on audio production tools. We propose a user-centered approach to remove the great disconnect between existing audio production tools and the conceptual frameworks within which many people work, both expert musicians and the broader public. The tools we develop will automatically adapt to the user's conceptual framework, rather than forcing the user to adapt to the tools. Where appropriate, the tools will speed and enhance their adaptation using active learning informed by interaction with previous users (transfer learning). The tools will also automatically build a crowdsourced audio concept map. This will help provide facilities for computer-aided, directed learning, so that tool users can expand their conceptual frameworks and abilities. By letting people manipulate audio on their own terms and enhancing their knowledge of such tools with directed learning, we expect to transform the interaction experience, making the computer a device that supports and enhances creativity, rather than an obstacle.<br/><br/>This work will have a number of broader impacts. The tools developed will be directly usable by practicing musicians and will also facilitate learning and creativity for the general public. These techniques will also be applicable to personalization of hearing aids and new diagnostic systems for audiologists. Our approach to tool personalization is core work in human-computer interaction and should generalize to other creative activities (e.g. image manipulation). Resulting advances in active and transfer learning will be of great value to machine learning researchers. Finding the relationships between quantifiable parameters of audio and the language and metaphors used by practicing musicians to describe sound is central to this work. This is of great interest to cognitive scientists, linguists, artificial intelligence researchers, and engineers. Concept maps for audio terms should also prove useful for machine translation. Broad application of techniques to map human descriptive terms on to machine-manipulable parameters will change expectations for both artists and scientists. Artists will be able to explore new lines of creativity that currently require significant investments of time in vastly disparate fields (e.g. signal processing and painting). This has the potential to transform information science and lead to new cognitive models of creativity, forming the basis for new approaches to education and research in both technology and in art."
"1054419","CAREER: Computationally Generated Biomarkers","IIS","Info Integration & Informatics","02/01/2011","05/17/2013","Zeeshan Syed","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Sylvia Spengler","05/31/2013","$87,538.00","","zhs@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7364","1045, 7364","$0.00","The focus of the proposed research is to develop a computational framework that allows for the systematic exploration of the growing human ""physiome"" for clues to predict and prevent major diseases. This research is motivated by the observation that despite recent progress in medicine, the disease burden of many important clinical conditions remains unacceptably high because of a failure to promptly match patients to treatments that are appropriate to their current condition or their individual risk. This situation is true of many different areas of medicine: in cardiology, over 300,000 deaths take place each year due to fatal arrhythmias; in psychiatry, 34% of patients with bipolar disorder have an interval longer than 10 years before they first receive a diagnosis; in the setting of intensive care, there are over 650,00 cases of unanticipated sepsis each year. These are only some examples, where the therapies to significantly improve patient outcomes and reduce healthcare costs exist, but are often not applied in a timely fashion due to the absence of adequate information-based tools to guide their use.<br/><br/>There is a critical need in this setting for novel biomarkers to guide decision-making. Particularly striking is the lack of good biomarkers that exploit recent advances in acquiring large physiological datasets continuously from patients over long periods. The existing practice of discovering disease markers in such data is highly dependent on human input and subjective abilities; inappropriate for large datasets and subtle markers; and unable to generalize for different systems and diseases. The goal of this research is to bridge this gap, through computational methods for the structured discovery of novel, highly-discriminative risk markers from terabytes and even potentially petabytes of physiological data.<br/><br/>Inspired by the translational impact of computational biology in extracting valuable insights from large volumes of genomic and proteomic data, this research endeavors to lay the foundations of a complementary body of research focused around a vision of ""computational physiomics"". The PI proposes a computational framework where large volumes of waveform data are first abstracted into a uniform string representation, and the resulting physiological text is then studied for characteristics associated with risk. The abstraction of physiological signals into text creates the opportunity to study these signals in a fundamentally different manner from earlier efforts (and to thereby discover new insights). As part of this work, the PI will address the challenges associated with transforming the many different kinds of physiological time-series signals (e.g., quasi-periodic, aperiodic, non-uniformly sampled, multi-channel) into symbolic sequences, registering these symbols across patients, formulating problems relating to risk stratification in the context of textual data, and developing algorithms to efficiently and accurately solve these problem statements. In addition, through extensive collaborations with clinical colleagues, he will rigorously evaluate the clinical utility of the research on real-world datasets drawn from different high-impact clinical applications.<br/><br/>Early investigation of the ideas that form the basis of the proposed work have shown great promise for cardiovascular applications. Clinical studies in two separate cohorts with nearly 6,000 patients show that the computational framework enables the discovery of risk markers from ECG signals that identify patients at an 8-9 fold increased risk of death within three months of a heart attack, and moreover, that this information is independent of other generally accepted risk variables (e.g., demographics, comorbidities, imaging results, biomarkers, other ECG variables etc.). The research should enable continued progress in the case of cardiovascular disease, as well as similar progress for other focus applications (e.g., psychiatry, critical care, neurology, and obstetrics) through clinical collaborations. In addition to impact in these specific cases, the research also lays the foundation of a broader body of research (i.e., the vision of ""computational physiomics"") that can transform medical data analysis by including large volumes of continuous physiological signals in a rubric that today can only handle discrete data. This research represents a central piece in this context that connects advances in continuous patient monitoring to advances in classification methods. While the research is motivated by clinical applications, the computational questions addressed by the work and the techniques will also advance existing work on time-series prediction and on sequential data mining and machine learning more generally. The challenge of extracting insights from large volumes of time-series data is increasingly important across many different disciplines. The research promises to significantly advance both the broad goal of time-series analysis, as well as individual sub-problems in the areas of motif discovery, long-term signal comparison, anomaly detection, characterizing complexity, and identifying structure in apparently noisy signals.<br/><br/>This research will help to establish a strong inter-disciplinary program in computer science and medicine at the University of Michigan. This program will be inherently translational, and have a significant education component that provides graduate and undergraduate students with coursework and research opportunities exposing them to real-world medical problems, complex and large clinical datasets, computational methods, and the design of experiments. The PI will use the methods and materials related to this proposal both in the development of new courses (such as the biomedical machine learning course that the PI has introduced at Michigan), and to enrich existing courses in algorithms and data structures (which the PI teaches) and artificial intelligence and machine learning (which are taught by other faculty in the CSE department). Replacing some of the traditional applications covered in these courses by applications of clear importance to human health should engender a sense of excitement among students as they understand first-hand the role computation can have in improving the human condition. Developing, implementing, and evaluating our computational framework will also generate several undergraduate research positions that will allow students to experience and learn about multi-disciplinary research.  The PI is also committed to making the datasets that form the basis of the proposed research available to the broader research and educational communities for use in a de-identified manner. This will enable researchers who do not have established clinical partners to enter the research area and educators to construct laboratory activities, and facilitate the uniform assessment of risk stratification algorithms on a common set of signals."
"1151951","IIS: III: Workshop on Discovery Informatics","IIS","Info Integration & Informatics","09/01/2011","08/29/2011","Yolanda Gil","CA","University of Southern California","Standard Grant","Sylvia Spengler","08/31/2012","$90,000.00","","gil@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7364","7364, 7556","$0.00","The workshop aims to identify the research challenges and opportunities for transforming the scientific discovery process through advances in computing and information sciences in general, and intelligent systems in particular. It seeks to define a research agenda in Discovery Informatics. The workshop is organized around three themes: (1) efficient experimentation and discovery processes, (2) practical issues in building and refining predictive models from scientific data, and (3) social computing for science.<br/><br/>The participants include experts and visionaries in the areas of knowledge representation and inference, machine learning and data mining, experiment design and planning, information integration, computational models of discovery, collaborative technologies, robotics, social networks, visualization, and representative application (science) domains.  <br/><br/>Research in Discovery Informatics is expected to integrate advances in multiple subdisciplines of artificial intelligence and cognitive science to develop the next generation informatics driven exploratory apparatus for scientific discovery. The resulting formal frameworks and computational tools have the potential to not only accelerate discovery but enable new modes of discovery by providing the tools that empower scientists to reach across disciplinary boundaries. Such tools can also contribute to enhanced modes of teaching and learning in science, technology, engineering, and mathematics (STEM) disciplines.<br/><br/>The results of the workshop (including the workshop report) will be freely disseminated to the larger scientific research and educational community."
"1123617","DIP: Collaborative Research: A Personalized Cyberlearning System Based on Cognitive Science","IIS","Cyberlearn & Future Learn Tech","09/01/2011","09/07/2011","Elizabeth Marsh","NC","Duke University","Standard Grant","John Cherniavsky","08/31/2016","$354,850.00","","emarsh@psych.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8020","8045, 8842","$0.00","Investigators from Rice University and Duke University will build a Personalized Cyberlearning System, designed around three principles from cognitive science (retrieval practice, spacing, and enhanced feedback), that leverages advances in machine learning and makes use of an existing instructional content material and problem set database aimed at undergraduate engineering students. The system will use artificial intelligence methods to optimize practice and feedback for students. Research will seek to advance knowledge, in a real-world setting, about a range of issues concerning how feedback facilitates learning, how individual differences come in to play, as well as those more specifically aimed at the development of the learning technology system itself.<br/><br/>The project is important as part of the effort to harness the vast quantities of information on the web to personalize instruction for a wide range of learners. Moreover, the development of such cyberlearning technologies holds promise for opening up STEM education for motivated self-learners while also allowing access to a large volume of material for a range of students who might not otherwise have it."
"1124535","DIP: Collaborative Research: A Personalized Cyberlearning System Based on Cognitive Science","IIS","NATIONAL SMETE DIGITAL LIBRARY, Cyberlearn & Future Learn Tech","09/01/2011","05/20/2014","Richard Baraniuk","TX","William Marsh Rice University","Standard Grant","christopher hoadley","08/31/2015","$610,150.00","Paul Padley, Moshe Vardi, Don Johnson, C. Sidney Burrus","richb@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7444, 8020","8045, 8842, 9251","$0.00","Investigators from Rice University and Duke University will build a Personalized Cyberlearning System, designed around three principles from cognitive science (retrieval practice, spacing, and enhanced feedback), that leverages advances in machine learning and makes use of an existing instructional content material and problem set database aimed at undergraduate engineering students. The system will use artificial intelligence methods to optimize practice and feedback for students. Research will seek to advance knowledge, in a real-world setting, about a range of issues concerning how feedback facilitates learning, how individual differences come in to play, as well as those more specifically aimed at the development of the learning technology system itself.<br/><br/>The project is important as part of the effort to harness the vast quantities of information on the web to personalize instruction for a wide range of learners. Moreover, the development of such cyberlearning technologies holds promise for opening up STEM education for motivated self-learners while also allowing access to a large volume of material for a range of students who might not otherwise have it."
"1110970","SoCS: Studying the Computability of Emotions by Harnessing Massive Online Social Data","IIS","HCC-Human-Centered Computing, SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2011","06/13/2017","Michelle Newman","PA","Pennsylvania State Univ University Park","Continuing Grant","William Bainbridge","12/31/2017","$784,821.00","Jia Li, Reginald Adams, Michelle Newman","mgn1@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7367, 7953","7367, 7953, 9251","$0.00","The emergence of massive human-rated and commented visual data has opened avenues for exploring fundamental questions in artificial intelligence beyond the horizon.  This project tackles the challenge of automatically inferring visual aesthetics and emotions and inventing new systems that assist creative and decision-making activities of the general public.  An interdisciplinary team, with expertise in visual modeling, data mining, psychology, and computational sciences will build tools to distill information from a combination of visual, textual, and numerical data.  Visual features, selected based on published literature and consultation with domain experts, will be extracted for discriminating types of emotions.  The resulting systems can select and rank visual information based on aesthetics and emotions.<br/><br/>Intellectual Merits: This project will allow computer scientists to gain understanding of next-generation computerized visual aesthetics and emotion assessment systems.  The complex inter-relationship among content, context, and subjectivity in aesthetics and emotion assessment makes the corresponding learning problems especially challenging, which is likely to trigger innovation in machine learning and statistical modeling. Such capabilities will fundamentally change the way visual information is analyzed, processed, and managed.  The project will advance our understanding of the computability of emotions, and lead to new applications that can be used in a variety of settings.<br/><br/>Broader Impacts: The research will have a transformative impact in the fields of information retrieval, human-computer interaction, information processing, consumer electronics, and design. The technology can also be used to refine multimedia content that serves as education resources.  The project will disseminate research findings, generate new software implementations and collected datasets, and provide online services that can be used by researchers, educators, and industry. Education efforts include developing an interdisciplinary curriculum, training cross-disciplinary scientists, and involving underrepresented groups in research."
"1116826","NeTS: Small: Networking over Random Fields: A Statistical Model for Cognitive Radio Networks","CNS","Special Projects - CNS, Networking Technology and Syst","08/01/2011","02/06/2012","Husheng Li","TN","University of Tennessee Knoxville","Standard Grant","Min Song","07/31/2014","$302,685.00","","husheng@eecs.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1714, 7363","7363, 7923, 9150, 9251","$0.00","Experiments have demonstrated the temporal and spatial correlations of spectrum availability, which are of key importance in the design and analysis of cognitive radio networks. Motivated by the observation, this research applies the theory of random fields, which describes the behavior of multiple correlated random variables, to model the spectrum availabilities in time and space domains. For global spectrum activity, a homogeneous random field like Ising model is used to model the spatial correlation and analyze the performance. For local spectrum activities, Bayesian networks are used to describe the causality in spectrum and statistically infer the future spectrum situations. Furthermore, the model of controlled random fields is employed to design the networking protocols in cognitive radio networks. A low-cost spectrum sensor is designed to collect the real spectrum measurement in multiple locations simultaneously. The research promotes the understanding of frequency spectrum activities and enhances the design and analysis of the next generation cognitive radio networks. The research involves aspects of wireless communications, networking, artificial intelligence and imaging processing; thus the inter-disciplinary essence of the research also lends itself to cross-disciplinary education. Novel courses will be devised, which involve the topics of cognitive radio networks, machine learning and image processing. This project also expects to attract traditionally underrepresented groups, as well as outreach high school students."
"1118055","RI: Small: Large-Scale Machine Learning for Connectomics","IIS","Robust Intelligence","09/01/2011","08/30/2011","Pieter Abbeel","CA","University of California-Berkeley","Standard Grant","Kenneth Whang","08/31/2015","$450,000.00","","pabbeel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","7923","$0.00","Large-scale image data analysis has in recent years become a key bottleneck in natural science research, particularly in the field of neuroscience. Technological advances in automated data acquisition have enabled the collection of terabyte and petabyte-size datasets.  Extracting the rich information contained in these datasets manually would require an inordinate amount of human labor; reconstructing the neural connectivity in a complete fruitfly brain or cortical column of a mouse from electron microscopy data, key tasks of interest, would require ten thousand years of human labor using current state-of-the-art manual and semi-automated approaches. Improved automated image analysis tools are likely to be directly useful to the neuroscience community, enabling large-scale dense reconstruction of neural circuits from microscopy data, in which the morphology of every neuronal process is traced and all chemical synaptic connections between cells are identified, thereby mapping the complete ""wiring diagram"" of the circuit contained in the neural tissue. Such reconstructions have the potential to fundamentally impact the understanding of neural circuits by enabling competing models of brain architecture to finally be rigorously verified or falsified experimentally.<br/><br/>The large size of the datasets, the need for high accuracy to avoid incorrect scientific conclusions being drawn about the data, and the need for well-calibrated confidence measures in order to limit the time that must be spent manually verifying the output of algorithms, are all substantial challenges not well-addressed by existing segmentation methods.  The investigators propose to (i) Develop efficient algorithms for convolutional locality-sensitive hashing, a novel generalization of locality-sensitive hashing techniques to the highly applicable setting of dense overlapping patches from a larger data volume. (ii) Develop efficient algorithms for the overlapping patch and convolutional variants of sparse coding designed to scale to very large datasets, filter sizes and numbers of filters. The proposed convolutional locality-sensitive hashing approach will be employed to enable this. (iii) Develop algorithms that leverage (i) and (ii) to segment electron microscopy data, and compare empirically to existing segmentation methods. All of the proposed methods are highly scalable to executions on large compute clusters in order to handle large training and test datasets. Furthermore, since the proposed methods allow explicit representation of the data, they are expected to be better calibrated than parametric methods such as the existing neural network-based methods for segmentation of electron microscopy data that currently achieve the best accuracy."
"1054319","CAREER:  Flexible Learning for Natural Language Processing","IIS","Robust Intelligence","02/01/2011","03/02/2015","Noah Smith","PA","Carnegie-Mellon University","Continuing grant","Tatiana Korelsky","01/31/2016","$565,812.00","","nasmith@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","1045, 1187, 7495, 7945, 9251","$0.00","Statistical learning is now central to natural language processing<br/>(NLP).  Bridging the gap between learning and linguistic<br/>representation requires going beyond learning parameters.  This CAREER<br/>project addresses three challenging, unresolved questions:<br/><br/>1. Given recent advances in learning the parameters of linguistic<br/>models and in approximate inference, how can the process of feature<br/>design be automated?<br/><br/>2. Given that NLP tasks are often defined without recourse to real<br/>applications and that a specific annotated dataset is unlikely to<br/>fulfill the needs of multiple NLP projects, can learning frameworks be<br/>extended to perform automatic task refinement, simplifying a<br/>linguistic analysis task to obtain more consistent, more precise, or<br/>faster performance?<br/><br/>3. Can computational models of language take into account the non-text<br/>context in which our linguistic data are embedded?  Building on recent<br/>success in social text analysis and text-driven forecasting, this<br/>CAREER project seeks to exploit context to refine models of linguistic<br/>structure while enabling advances in this application area.<br/><br/>This basic research supports advances in a wide range of language<br/>engineering applications and discrete data analysis.  In addition to<br/>core research advances, this CAREER project contributes a new<br/>publicly-available parser that models the most consistently learnable<br/>elements of syntactic struture.  Educational activities include a new<br/>project-based on text-driven forecasting within the PI's undergraduate<br/>NLP course and a new undergraduate course in machine learning. It<br/>supports involvement by the PI in outreach activities to high school<br/>students and to a wider range of students at CMU by exposing aspects<br/>of his research in non-CS classrooms."
"1137289","ICML 2011 Workshop on Learning from Clinical Free Text","IIS","Information Technology Researc, Info Integration & Informatics, Robust Intelligence","06/15/2011","06/08/2011","Zeeshan Syed","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sylvia Spengler","05/31/2012","$6,774.00","","zhs@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1640, 7364, 7495","7364, 7495, 7556, 8018","$0.00","The increasing availability of large quantities of clinical data in digitized form in the course of routine patient care, presents unprecedented opportunities as well as challenges in improving the quality and reducing the cost of clinical care through the use of Electronic Medical Record (EMR) systems. Because it is impossible to anticipate and precisely identify/define all of the relevant information that would be useful in every clinical setting, and the need for the definitions of key information elements to change in response to changes in medical knowledge (e.g., evidence-based treatment guidelines), despite the increasing emphasis on collecting information in structured fields of EMRs, a substantial fraction of key information continues to be available as unstructured (i.e., free) text. Hence, there is a growing interest in identifying novel learning approaches that can be used to adapt strategies for information extraction from free text in such settings. <br/><br/>Zeeshan Syed and collaborators plan to organize a multi-disciplinary Workshop on Learning from Clinical Free Text to bring together researchers from machine learning, computational linguistics, and medical informatics, who share an interest in problems and applications of learning from unstructured clinical text. The workshop to be held on July 2, 2011 at Bellevue, Washington, USA,  in conjunction with the International Conference on Machine Learning (ICML), which is the premier international forum for researchers and practitioners from academia, industry, and government for sharing the latest advances in machine learning.<br/><br/>Scientific Merits: The workshop seeks to bridge the gap between the theory of machine learning, natural language processing, and the applications and needs of the healthcare community and to promote fruitful interdisciplinary collaborations. The workshop seeks to cover a range of topics of interest to academic as well as industrial participants through a program consisting of presentations by invited speakers from machine learning, computational linguistics. and medical informatics, and by authors of extended abstracts solicited from the broader research community. A panel discussion will help identify important problems, applications, and synergies across the research in and practice of machine learning, computational linguistics, and medical informatics. The workshop will connect established researchers  with graduate students and early career researchers and academics. <br/><br/>Broader Impacts: These activities will collectively facilitate the the infusion of the latest results and tools from the machine learning and computational linguistics, and text mining communities into Health Informatics, catalyze the establishment of an interdisciplinary community of researchers focused on advancing machine learning to meet the needs of information extraction from free text medical records, and help integrate a diverse group of graduate students and early career researchers into the Health Informatics community."
"1116782","RI: Small: A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing","IIS","Robust Intelligence","09/01/2011","07/28/2014","Martha Palmer","CO","University of Colorado at Boulder","Continuing grant","Tatiana Korelsky","08/31/2015","$300,000.00","","mpalmer@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7495","7923","$0.00","This project uses statistical models and human judgment to determine dynamic, probabilistic representations of extensible usages of words; these representations are suitable for incorporation into VerbNet, a lexical resource widely used in the Natural Language Processing (NLP) community. Existing lexical resources reflect a binary notion of usages as grammatical or not. However, in actual language use, forms vary in acceptability; moreover, the process of coercion extends words beyond their standard usages. For example, a strictly intransitive action verb such as 'sneeze' may be used as in 'She sneezed the foam off the cappuccino', expressing manner of motion.  This research has a two-pronged approach involving extensive use of machine learning and a fundamental shift in the development and use of VerbNet. Specifically, the research develops probabilistic methods for: (1) analyzing usages of verbs in large corpora and incorporating the resulting probabilistic information into VerbNet classes; and (2) representing information about the likelihood of potential constructional coercions and the productivity of such extensions.  These developments use the Hierarchical Bayesian Model of Parisien and Stevenson, which are an ideal framework for marrying probabilistic reasoning about complex, real-world data within the hierarchically-organized VerbNet lexicon. In addition to statistical models, the representations are also informed by human judgments with respect to the use of such constructions. Thus, this research enriches the current symbolic verb representations in VerbNet with probabilistic distributional information, which becomes salient through the influence of construction grammar. <br/><br/>Encoding verb knowledge probabilistically provides the necessary flexibility to represent extensional constructions and support their appropriate interpretation by NLP systems.  This is especially useful for interpretation in new domains and genres, leading to advances in NLP technologies, such as question answering and machine translation, thus improving information access. Additionally, insights into statistical properties of constructions gained through this research are valuable for psycholinguistic models of language acquisition and second language learning."
"1144111","EAGER: Exploring Multimedia Information Networks","IIS","Info Integration & Informatics","08/01/2011","07/23/2011","Thomas Huang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Maria Zemankova","07/31/2013","$199,360.00","Heng Ji","huang@ifp.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7364","7364, 7916, 7923","$0.00","This project is built on the interdisciplinary collaboration between multimedia and natural language processing (NLP) researchers. The goal of this exploratory research is to provide effective methods for organizing,  searching, mining and reasoning with web-scale multimedia. The approach is based on formulation of a structured multimedia database, called Multimedia Information Networks (MINets) which enables new searching and mining paradigms such as keyword query of unlabeled image/video databases, query expansion in content-based image retrieval (CBIR), and measuring similarity between different modalities such as an image and a piece of text). In addition, MINets framework is expected to provide the ability to perform robust inference (e.g., recognizing objects and activities in images or videos) in the presence of noise and uncertainty. In its simplest form, a MINet is a graph where nodes are either concepts (text) or data (such as images), and links are ontological/semantic relationships between concepts, attachment of images to concepts, and visual similarity measures between images.  Construction of an experimental MINets framework involves crawling the Web for a particular domain, gathering images with associated text and exploiting natural language processing, computer vision and mining techniques in establishing the concepts, associated images, interconnecting links and an ontology that supports inference. Validation against current web search engines and CBIR techniques is expected to provide a proof-of-concept for the novel MINets framework.<br/><br/>This interdisciplinary exploratory project is expected to yield general theoretical and algorithmic MINets framework that will provide new searching, mining and reasoning capabilities for multimedia data. It will help to define new research areas in effective utilization of multimedia information sources for cross-media and cross-conceptual knowledge discovery and analysis, large-scale annotation, information fusion and inference. Project results, including open source software, annotated corpora, scoring metrics will be disseminated via project Website (https://netfiles.uiuc.edu/qi4/www/MINets.htm). This project will provide research opportunities for graduate and undergraduate students."
"1145152","EAGER: Combining Knowledge with Data for Generalizable and Robust Visual Learning","IIS","Robust Intelligence","10/01/2011","06/11/2015","Qiang Ji","NY","Rensselaer Polytechnic Institute","Standard Grant","Jie Yang","09/30/2016","$226,421.00","","qji@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","7495","7495, 7916, 9251","$0.00","Computer vision has made tremendous progress in the past decades, partially enabled   by the advanced machine learning techniques.  But compared with human perception, computer vision remains primitive.  One contributing factor for this is the data-driven nature of the current learning algorithms and their inability to incorporate any related knowledge.  The data-driven methods tend to be database-specific and cannot generalize well to unseen data. This project addresses this issue through the introduction of a knowledge-augmented statistical learning framework.   Within this framework, knowledge and data can be systematically exploited, captured, and are principally integrated to jointly train a vision algorithm.  Developing such a framework, however, is challenging since the domain knowledge often exists in different and diverse formats, typically inaccessible to the data-driven statistical machine learning methods.  To overcome this challenge, the research team systematically converts domain knowledge into either the constraints on the model or into pseudo-data, whereby they can be incorporated into the statistical learning methods.  The project includes systematic identification of knowledge from different sources and concrete mechanisms to capture the knowledge and to convert them into formats easily accessible to the automatic machine learning methods.  The project also involves demonstrating the effectiveness of the proposed framework for certain computer vision problems.<br/><br/>The project provides the training for graduate and undergraduate students, and the research results are disseminated through publications and organization of the related workshops."
"1145291","EAGER: Human Computation: Integrating the Crowd and the Machine","IIS","Info Integration & Informatics, HCC-Human-Centered Computing","08/01/2011","07/26/2011","Albert Lin","CA","University of California-San Diego","Standard Grant","Ephraim Glinert","07/31/2013","$66,002.00","Falko Kuester, Gert Lanckriet","a5lin@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7364, 7367","7364, 7367, 7916","$0.00","Because both information and connectivity are more available today than ever before thanks to digital technologies, questions can now be addressed by enlisting massive human demographics to supplement the limitations of computer computation.  This is especially relevant in the case of visual analytics, where human intuition remains far superior to existing computer object recognition algorithms.  While algorithms are limited by pre-labeling requirements, humans can perceive subtle variations and nuances to identify and classify unexpected objects.  These tasks, however, are often too massive in scale for a single human to accomplish.  Distributing this task over a massive network not only succeeds in categorizing data, but generates massive quantities of human quantifiers (training data) to potentially teach computer vision algorithms to mimic human perception in order to distinguish the normal from the abnormal.<br/><br/>This exploratory project will combine collective human visual perception with machine learning and object recognition, through a study of 1.25 million crowd-sourced inputs provided by over 6,000 volunteers labeling satellite imagery in a search for anomalies in northern Mongolia.  These data, collected from June 2010 to the present via an online platform developed by the PI in collaboration with National Geographic Digital Media, afford an ideal ""case study"" environment to investigate the nature of crowd generated data and methods that distill the wide variability of human input into computational algorithms.  The online participants, excited by the potential of discovering the tomb of Genghis Khan, examined massive amounts of ultra-high resolution multispectral satellite imagery to label loosely defined anomalies into various categories. Trends that emerged from the massive volume of labels represent a collective human perspective on what the images contain.  A team led by the PI traveled to Mongolia to ground-truth areas of high user input convergence.  The resulting ground-truthed anomalies provide a unique opportunity to both accurately measure the quality of human/automated analysis and to investigate the effect of supplementing noisy crowd-sourced data sets with small pools of absolute data in machine learning.  In the current project the PI will develop a framework for applying and evaluating the following three research phases designed to study the nature of large scale human generated data for integration into supervised learning algorithms:<br/><br/>1. Consensus Clustering - Tag evaluation mechanisms based upon the volume and consistency of neighboring tags and the ability of the individuals creating those tags.  Unsupervised methods for ""merging"" labels will also be applied for extended anomalies such as roads and rivers.<br/><br/>2. Feature Vector Extraction - Both the type of features (e.g., color, luminance, edges and gradients, scale, orientation, etc.) and the extent of the neighborhoods (e.g., local, wide and global) required to detect anomalies are unknown a priori.  Thus, the aim is to determine sufficiently diverse features to  capture all relevant cues within the image.<br/><br/>3. Machine Learning - Dominant features representative of, and excluded from, pixel groups of given categories will be determined from the results of Phase 2 above.<br/><br/>Broader Impacts:  In this exploratory study the PI will lay the foundation for extracting new machine/human collaborative opportunities from the resource of the crowd.  Understanding the bonds between human and computer intelligence will have a profound impact on many branches of science.  Thus, concepts developed in this effort may ultimately prove transformative by affording migration of crowd-sourcing from a project-based tool for distributed analytics into a portal bridging collective human perception and machine learning."
"1111107","SOCS: Socially Intelligent Computing for Coding of Qualitative Data","IIS","HCC-Human-Centered Computing, VIRTUAL ORGANIZATIONS, SOCIAL-COMPUTATIONAL SYSTEMS","09/01/2011","05/15/2014","Kevin Crowston","NY","Syracuse University","Continuing Grant","Ephraim Glinert","08/31/2015","$779,831.00","Nancy McCracken","crowston@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7367, 7642, 7953","7367, 7642, 7953, 9251","$0.00","This project will develop and evaluate an innovative research tool, based on Natural Language Processing (NLP) and Machine Learning (ML), to support qualitative social science research, specifically content analysis. Content analysis is a qualitative research technique for finding evidence of concepts of theoretical interest using text rather than numbers as its raw data.  The process of identifying and labeling significant features in text is referred to as ""coding,"" and the result of such an analysis is a text annotated with codes for the concepts exhibited. This technique has become increasingly popular and more applicable as the volume of available ""born-digital"" text has exploded.  However, the reliance on manual analysis of the text limits the scale and scope of content analysis research.<br/><br/>In this project, the problem of coding qualitative data is conceptualized as an information extraction problem amenable to automation using NLP. However, rather than seeking to automate the process, the technologies will be used in a supporting role, creating a human-computer partnership. ML will be used to induce NLP rules from examples of coded text, avoiding the need to develop rules manually. To reduce the amount of training data needed from the human participants, an active learning process will be employed, in which a few hand-coded examples are used to create an initial model that can be further evolved through interaction with the user. These approaches will be combined in a prototype tool to support qualitative content analysis. As a demonstration and test of the tool, it will be applied to current and novel studies of cyber-infrastructure-supported distributed groups, specifically free/libre open source software development teams, and then to a broad range of social science research problems. This broad usage will also provide a test of the generalizability of a socio-computational approach to this problem.<br/><br/>The intellectual merit of the research is four-fold.  First, the proposal seeks to develop a novel socio-computational system that supports a human-computer partnership through the integration of information extraction and active learning. Second, a validation study will apply the tool to a diverse set of codes, providing evidence of the generality and limits of a socio-computational approach.  Third, the demonstration studies using the tool will contribute to research on distributed groups.  Finally, the project addresses a fundamental methodological problem in the broad domain of qualitative research, namely dealing with large quantities of unstructured qualitative data, by applying innovative computer-support.  By avoiding the need for hand-written rules and reducing the required amount of hand-annotated training data, this partnership will make practical the use of a system for coding large quantities of qualitative data in various domains.<br/><br/>The project has numerous broader impacts. It will benefit society by providing useful infrastructure for research in the form of a content analysis tool for scientific research and in for the form of corpora of annotated data for use in future Natural Language Processing research. The demonstration studies will provide generalizable knowledge to improve the effectiveness of distributed groups, an increasingly important mode of organization. Finally, the project contributes to the education and training, of women and minority group members in particular."
"1148895","EAGER:  Automatic Reconstruction of Typed Input from Compromising Reflections","IIS","","08/01/2011","08/26/2011","Jan-Michael Frahm","NC","University of North Carolina at Chapel Hill","Standard Grant","Jie Yang","07/31/2014","$151,749.00","Fabian Monrose","jmf@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","K576","7916","$0.00","This project explores computer vision techniques aimed at exploiting compromising reflections associated with data input in mobile electronic devices such as smart phones.  The ubiquity of these personal communication devices and their growing roles in data manipulation tasks, make unintended visual emanations an exploitable liability to data security. Nevertheless, there is still a gap in understanding of both the limitations of these techniques as well as the availability of effective mitigation mechanisms. It is the goal of this work to contribute to filling this conceptual gap.<br/><br/>The study builds upon recent state of the art techniques for automatic reconstruction of typed input from compromising reflections, comprising of robust keystroke event detection and classification mechanisms coupled to natural language processing modules. Such paradigm is both effective and amenable to low cost implementation in commodity devices. Based on these new developments, threat scenarios are no longer restricted to controlled scenarios using specialized equipment, but rather consist of highly flexible and possibly impromptu attacks. The project develops advanced cross-platform data input transcription prototypes used within a threat validation framework. This framework provides a characterization of both threat scenario operational limitations (e.g., imaging resolution, scene illumination, computational requirements) as well as the performance characteristics (e.g., robustness, accuracy) of the different vulnerability exploitation mechanisms. Moreover, the results of the analysis of diverse threat scenarios are being used to identify and develop appropriate mitigation mechanisms when possible."
"1111176","Computational Facilitation of Online Deliberation in Complex Policymaking.","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","10/01/2011","04/05/2012","Cynthia Farina","NY","Cornell University","Standard Grant","William Bainbridge","09/30/2015","$766,000.00","Claire Cardie, Daniel Cosley","crf7@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7953","7953, 9251","$0.00","This research seeks to develop socio-computational approaches to improve citizen participation in rulemaking, using a pilot online participation platform, RegulationRoom.  Rulemaking, the process through which federal regulatory agencies make new regulations, is an unusual method of federal decision-making because it formally incorporates a period of peer knowledge creation, the ""notice-and-comment"" process. Agencies proposing new rules must seek input from stakeholders, experts, and the general public, and consider all criticisms, questions, new data, and alternative ideas received. Yet only a limited segment of individuals and groups participate, and these participants engage in adversarial position-taking rather than collaborative deliberation. Neither of these shortcomings has been remedied by putting the process online.  It is reasonable to hypothesize that broader, better public engagement requires purposefully designing e-rulemaking systems to provide norm enculturation and deliberation priming.  This project will explore strategies for each, drawing on expertise in law, conflict resolution, natural language processing (NLP), machine learning, and recommender systems. <br/><br/>This research provides unique contributions in at least four areas: (a) advancing the state-of-the art in NLP by developing discourse analysis techniques to facilitate deliberative dialogue and collaborative knowledge production; (b) advancing recommender system and online community research to support mentoring activities and engagement with alternate points of view; (c) advancing the field of conflict resolution by adapting face-to-face moderation techniques to the online environment; and (d) extending legal understanding of how to increase transparency and participation by supporting broader, better public participation in rulemaking and other complex policymaking domains. It will also generate annotated datasets of comment and deliberation quality that will be released to other researchers. The results will extend the techniques and assessment measures available to moderators of online group discussion and provide new computational tools for improving the quality of individual contributions and for interpreting and synthesizing information during the knowledge production process. <br/><br/>Learning how to design effective Rulemaking 2.0 systems will strengthen the democratic process by expanding the range of individuals and groups who understand, engage with, and meaningfully contribute to federal policymaking. In addition, better policy outcomes should result from socio-computational techniques that enable meaningful participation by important but traditionally absent stakeholder groups, such as small business owners. State and local governments that use public comment processes will benefit similarly from this work as will non-governmental groups trying to increase effective participation in complex collaborative content creation online."
"1059221","Collaborative Research:  CI-ADDO-EN:  Development of Publicly Available,  Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2011","08/20/2011","Benjamin Bahan","DC","Gallaudet University","Standard Grant","Ephraim Glinert","07/31/2016","$92,257.00","Christian Vogler","benjamin.bahan@gallaudet.edu","800 Florida Avenue, NE","Washington","DC","200023660","2026515497","CSE","7359","7359","$0.00","The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br/> <br/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br/><br/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL."
"1059281","Collaborative Research:  CI-ADDO-EN:  Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2011","07/27/2011","Dimitris Metaxas","NJ","Rutgers University New Brunswick","Standard Grant","Ephraim Glinert","07/31/2014","$97,908.00","","dnm@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7359","7359","$0.00","The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br/> <br/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br/><br/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL."
"1059218","Collaborative Research:  CI-ADDO-EN:  Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2011","06/14/2016","Carol Neidle","MA","Trustees of Boston University","Standard Grant","Ephraim Glinert","07/31/2017","$368,205.00","Stan Sclaroff","carol@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7359","7359","$0.00","The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br/> <br/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br/><br/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL."
"1059235","Collaborative Research: CI-ADDO-EN: Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2011","03/24/2014","Vassilis Athitsos","TX","University of Texas at Arlington","Standard Grant","Ephraim Glinert","07/31/2015","$98,630.00","","athitsos@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","1714, 7359","7359, 9251","$0.00","The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br/> <br/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br/><br/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL."
"1144564","EAGER: Detecting and Tracking Cyber Bullying on the Social Web","IIS","Info Integration & Informatics","09/01/2011","05/04/2012","Yi Zhang","CA","University of California-Santa Cruz","Standard Grant","Maria Zemankova","08/31/2013","$163,332.00","","yiz@soe.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7364","7364, 7916, 9251","$0.00","As many children are connected to their peers and spend a significant amount of time on social web sites (Facebook, Twitter, etc.), cyber bullying in social media is becoming a severe problem that can lead to serious social, psychological, and health effects.  To alleviate the problem, intervention from adults (teachers, parents, law enforcement, social web site moderators, etc.) is the key. However, many victims do not report bullying to adults, and bullies can use aliases and act anonymously, and thus they are difficult to identify.  The goal of this exploratory project is to eliminate or at least reduce these problems by developing an intelligent system to automatically detect and track cyber bullies on the social web.<br/><br/>This project explores a solution to cyber bullying based on a combination of machine learning, natural language processing, information filtering, and recommendation and social network modeling techniques. The expected results of the project include: (1) algorithm(s) that can detect cyber bullies and bullying messages automatically; (2) piloting results that suggest what prediction accuracy to expect; (3) a preliminary social web bullying detection prototype system and (4) the first labeled social cyber-bullying data set for testing of the prototype and future research in this direction. The project has high risk, as whether such a system can be developed is an untested idea and the task is challenging, largely due to the diversity of bully behaviors, ambiguity, and the special language used by bullies in social media. <br/><br/>Results from this research project are expected to create a foundation for future larger-scale projects investigating the cyber-bullying problem in social media, which will eventually make the social interaction much safer for hundreds of millions of children and beyond. Teaching, training and learning will be promoted directly for the graduate students who serve as research assistants and programmers, and several undergraduates will contribute to the programming. The impact will be strengthened by UCSC's ethnic and cultural diversity, its proximity to Silicon Valley, and the PI's industry collaborations. Results of this research, including data generated, publications, and demo software will be available via the project web site (http://users.soe.ucsc.edu/~yiz/bullying/)."
"1128296","Collaborative: Gesture Recognition Challenge","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2011","07/13/2012","Vassilis Athitsos","TX","University of Texas at Arlington","Standard Grant","Paul Werbos","08/31/2012","$55,920.00","","athitsos@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","ENG","7607","1653, 9251","$0.00","The objective of this research is both to advance the field of video data processing (more particularly gesture recognition) and to illustrate the power of deep learning architectures and transfer learning. The approach is to organize a challenge culminating in a life evaluation at the site of a conference.<br/>Intellectual merit: Much of the recent research in Adaptive and Intelligent Systems (AIS) has sacrificed the grand goal of designing systems ever approaching human intelligence for solving data mining tasks of practical interest with more immediate reward. This project gives an opportunity to deep learning architectures inspired by neural networks to demonstrate their ability to address more complex problems requiring to transfer knowledge from task to task (transfer learning), leveraging the availability of video data not directly related to the target task of gesture recognition. The participants will also be involved in a data exchange to grow an unprecedented large and diverse database of gestures. <br/>Broader Impact: Challenges have proved to be a great stimulus of research. For a long lasting impact, the challenge platform and the data and software repositories will remain open beyond the term of the NSF funded project. The educational components of the project include engaging students in the contest, providing material directly usable in teaching curricula, and demonstrating gesture recognition to high school students to expose them to computer vision research and sign language communication. Our connections with the deaf community will allow us to gear the product of this research to advance assistive technology."
"1065390","RI: Medium: Collaborative Research: Semantically Discriminative : Guiding Mid-Level Representations for Visual Object Recognition with External Knowledge","IIS","ROBUST INTELLIGENCE","08/01/2011","09/26/2013","Kristen Grauman","TX","University of Texas at Austin","Continuing grant","Jie Yang","07/31/2017","$498,994.00","","grauman@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495","7495, 7924","$0.00","This project explores (semi-)automatic ways to create ""semantically discriminative"" mid-level cues for visual object categorization, by introducing external knowledge of object properties into the statistical learning procedures that learn to distinguish them.  In particular, the PIs investigate four key ideas: (1) exploiting taxonomies over object categories to inform feature selection algorithms such that they home in on the most abstract description for a given granularity of label predictions; (2) leveraging inter-object relationships conveyed by the same taxonomies to guide context learning, so that it captures more than simple data-driven co-occurrences; (3) exploring the utility of visual attributes drawn from natural language, both as auxiliary learning problems to bias models for object categorization, as well as ordinal properties that must be teased out using non-traditional human supervision strategies; (4) mining attributes that are both distinctive and human-nameable, moving beyond manually constructed semantics.<br/><br/>The project entails original contributions in both computer vision and machine learning, and is an integral step towards semantically-grounded object categorization.  Whereas mainstream approaches reduce human knowledge to mere category labels on exemplars, this work leverages semantically rich knowledge more deeply and earlier in the learning pipeline. The approach results in vision systems that are less prone to overfit incidental visual patterns, and representations that are readily extendible to novel visual learning tasks.  Beyond the research community, the work has broader impact through inter-disciplinary training of graduate and undergraduate students, and outreach to pre-college educators and students through workshops and summer camps encouraging young students to pursue science and engineering."
"1115493","III: Small: Collaborative Research: Using Large-Scale Image Data for Online Social Media Analysis","IIS","Info Integration & Informatics","08/01/2011","05/07/2013","Fei-Fei Li","CA","Stanford University","Standard Grant","Sylvia Spengler","07/31/2014","$311,797.00","","feifeili@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7364","7364, 7923, 9251","$0.00","Understanding and analyzing the way our world is connected is a critical but new challenge in today's world, thanks to the technological advances of personal computers, mobile devices, as well as local and global Internet connections.  Most current methods in the area of social media analysis, inference and understanding are based on textual data. However, the image data makes an increasingly large proportion of data in social media. Hence, there is an urgent need for tools that can effectively use image data to extract important information to infer patterns and activities of people, communities and society at large. <br/><br/>This project combines advances in computer vision, machine learning, and social networks in novel ways for understanding and analyzing large-scale social media data.  The proposal brings together computer vision and machine learning research in novel ways to develop new methods for analyzing large-scale social media data. It pursues 4 inter-related aims: (i) Establishing a large-scale visual concept ontology and structures for the web-image world via crowdsourcing, taxonomy induction, and nonparametric learning methods; (ii) Understanding  activity in social networks by analyzing image contents in the context of social media in large-scale and with connectivity; (iii) Inferring the structure of social networks and communities from image contents and activity of individuals in social networks; (iv) Discovering and analyzing  dynamic social media trends.  <br/><br/>Anticipated products of this research include new tools for analysis and modeling of socially generated content, with special emphasis on image data. The resulting methods provide potentially useful insights that characterize users, communities and societies, in a broad range of applications.  The project offers enhanced research-based advanced training opportunities for graduate as well as undergraduate students and involves development of new courses on related topics at both Stanford University and Carnegie Mellon University."
"1065243","RI: Medium: Collaborative Research: Semantically Discriminative: Guiding Mid-Level Representations for Visual Object Recognition with External Knowledge","IIS","ROBUST INTELLIGENCE","08/01/2011","07/13/2017","Fei Sha","CA","University of Southern California","Continuing grant","Jie Yang","12/31/2017","$491,289.00","","feisha@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","7495, 7924","$0.00","This project explores (semi-)automatic ways to create ""semantically discriminative"" mid-level cues for visual object categorization, by introducing external knowledge of object properties into the statistical learning procedures that learn to distinguish them.  In particular, the PIs investigate four key ideas: (1) exploiting taxonomies over object categories to inform feature selection algorithms such that they home in on the most abstract description for a given granularity of label predictions; (2) leveraging inter-object relationships conveyed by the same taxonomies to guide context learning, so that it captures more than simple data-driven co-occurrences; (3) exploring the utility of visual attributes drawn from natural language, both as auxiliary learning problems to bias models for object categorization, as well as ordinal properties that must be teased out using non-traditional human supervision strategies; (4) mining attributes that are both distinctive and human-nameable, moving beyond manually constructed semantics.<br/><br/>The project entails original contributions in both computer vision and machine learning, and is an integral step towards semantically-grounded object categorization.  Whereas mainstream approaches reduce human knowledge to mere category labels on exemplars, this work leverages semantically rich knowledge more deeply and earlier in the learning pipeline. The approach results in vision systems that are less prone to overfit incidental visual patterns, and representations that are readily extendible to novel visual learning tasks.  Beyond the research community, the work has broader impact through inter-disciplinary training of graduate and undergraduate students, and outreach to pre-college educators and students through workshops and summer camps encouraging young students to pursue science and engineering."
"1115313","III: Small: Collaborative Research: Using Large-Scale Image Data for Online Social Media Analysis","IIS","Info Integration & Informatics","08/01/2011","07/23/2011","Eric Xing","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","07/31/2014","$204,202.00","","epxing@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7923","$0.00","Understanding and analyzing the way our world is connected is a critical but new challenge in today's world, thanks to the technological advances of personal computers, mobile devices, as well as local and global Internet connections.  Most current methods in the area of social media analysis, inference and understanding are based on textual data. However, the image data makes an increasingly large proportion of data in social media. Hence, there is an urgent need for tools that can effectively use image data to extract important information to infer patterns and activities of people, communities and society at large. <br/><br/>This project combines advances in computer vision, machine learning, and social networks in novel ways for understanding and analyzing large-scale social media data.  The proposal brings together computer vision and machine learning research in novel ways to develop new methods for analyzing large-scale social media data. It pursues 4 inter-related aims: (i) Establishing a large-scale visual concept ontology and structures for the web-image world via crowdsourcing, taxonomy induction, and nonparametric learning methods; (ii) Understanding  activity in social networks by analyzing image contents in the context of social media in large-scale and with connectivity; (iii) Inferring the structure of social networks and communities from image contents and activity of individuals in social networks; (iv) Discovering and analyzing  dynamic social media trends.  <br/><br/>Anticipated products of this research include new tools for analysis and modeling of socially generated content, with special emphasis on image data. The resulting methods provide potentially useful insights that characterize users, communities and societies, in a broad range of applications.  The project offers enhanced research-based advanced training opportunities for graduate as well as undergraduate students and involves development of new courses on related topics at both Stanford University and Carnegie Mellon University."
"1065336","SHB: Medium: Assistive Cloudlet-Based Mobile Computing for the Cognitively Impaired","IIS","Information Technology Researc, Smart and Connected Health","09/01/2011","06/11/2014","Mahadev Satyanarayanan","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","08/31/2015","$1,271,272.00","Daniel Siewiorek, Martial Hebert","satya@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1640, 8018","7924, 8018, 9251","$0.00","This project advances computer science by producing scientific insights, algorithms, system designs, implementation techniques, and experimental validations at the intersection of three major subdisciplines. These subdisciplines are (1) computer systems (including mobile computing, operating systems, and wireless networking), (2) vision technologies (including computer vision and machine learning), and (3) human-computer interaction (including activity inferencing, distraction reduction, and context awareness). These will be integrated to create cognitive assistive systems that can function ""in the wild"" with sufficient functionality, performance and usability to be valuable at any time and place to provide help for the cognitively impaired.<br/><br/>From a societal perspective, this research has the potential to improve the quality of lives of individuals whose cognitive capabilities have declined due to natural aging, illness or traumatic injuries (estimated at 20 million Americans). In addition, cognitive support can assure safe use and compliance with instructions in rehabilitation and management of chronic illness. From an educational viewpoint, this research offers many unique opportunities to train graduate and undergraduate students on how to approach problems from a broad multi-interdisciplinary perspective. In close partnership with industry, this research has the potential to impact mobile computing by empowering resource-poor mobile devices to run interactive, compute-intensive applications at any time and place. While this proposal focuses on applying this new capability to the problem of cognitive assistance, it can also address important needs of the general population. Further, the resulting cloudlet architecture has the potential to transform arenas as diverse as business, engineering, health care, and defense."
"1062351","Collaborative Research: ABI Innovation : Computational and Informatics Tools for Supporting Collaborative Wildlife Monitoring and Research","DBI","ADVANCES IN BIO INFORMATICS","06/01/2011","03/12/2012","Thomas Huang","IL","University of Illinois at Urbana-Champaign","Continuing grant","Anne Maglia","05/31/2015","$443,405.00","","huang@ifp.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","BIO","1165","1165, 9178, 9179","$0.00","The University of Missouri and the University of Illinois at Urbana-Champaign are awarded collaborative grants to develop advanced computational and informatics tools that will support wildlife data collection, analysis, and management at large scales.  Project objectives include investigation of 1) advanced computer vision methods for detecting and tracking animals in dynamic and cluttered environments; 2) adaptive classification, machine learning, and information fusion methods for recognizing animal species and individual ID; and 3) data summarization and database management schemes to support collaborative wildlife research.  The performance of these computational and informatics tools will be evaluated using existing camera trap datasets and field studies in terms of their potential to support collaborative wildlife research.  <br/><br/>This project will broadly advance the state-of-the-art in computer vision, wildlife monitoring, ecology, and conservation research.  It will provide new methods and tools for automated processing and mining of massive wildlife monitoring data at large scales.  This will allow individual or coordinated networks of wildlife researchers to analyze and manage camera-trap data with minimum effort and compare and share data between research groups across different geographical regions.  Collaborative wildlife monitoring and tracking at large geographical and time scales will help us understand the complex dynamics of wildlife systems, evaluate the impact of human actions and environmental changes on wildlife species, and answer many important wildlife, ecological, and conservation research questions.  The database will be hosted by Smithsonian.  This will provide exciting interdisciplinary opportunities for mentoring graduate students and involving K-12 and undergraduate students into professionally guided research.  Software and results of this project will be available from the website http://videonet.ece.missouri.edu."
"1116583","RI: Small: Clustering, Classification and Alignment of Time Series for Human Sensing","IIS","Robust Intelligence","09/01/2011","05/15/2014","Fernando De la Torre","PA","Carnegie-Mellon University","Standard Grant","Todd Leen","08/31/2015","$458,000.00","","ftorre@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923, 9251","$0.00","Time series analysis is central to the study of computer vision, signal processing, computer graphics, machine learning, and social sciences, among other fields.  This project entails original contributions towards algorithms for unsupervised pattern discovery  and temporal alignment of time series, and its applications to model human motion. In particular, the PI proposes three new methods: (1) a discriminative temporal clustering that factorizes a set of time series into segments belonging to one of k temporal clusters, (2) a method for discovering the set of most discriminative segments between two sets of time series, and (3) an unsupervised algorithm for temporally aligning multi-modal time series. The PI proposes an energy minimization framework to encompass these three problems.  This framework should provide researchers with a thorough understanding of a large number of existing time series techniques, and it may serve as a tool for dealing with other problems in time series as they arise.<br/><br/>Enabling computers to understand human behavior has the potential to revolutionize many areas that benefit society such as clinical diagnosis, human computer interaction, and social robotics. Advances in time series to model human actions and events from sensory data have been critical to the success of systems that can recognize and characterize human behavior. However, most existing algorithms have been supervised in nature. Supervised learning typically requires large amounts of human annotation, that is typically labor intensive and is difficult to standardize across coders. In this proposal the PI explores the use of unsupervised learning techniques for aligning and discovering patterns in time series of human motion that have been captured with accelerometers, video or motion capture technologies. The PI will show how the proposed algorithms outperform state-of-the-art techniques in several human sensing tasks such as temporal alignment of human motion, temporal clustering of human activities from video, learning motion primitives, and joint segmentation and classification of human behavior.  In the educational aspect, the PI will continue to provide support to the Carnegie Science Center to demonstrate human sensing technologies, as well as incorporate a large number of undergraduates in his research. The research source code will be made available to the scientific community."
"1062354","COLLABORATIVE RESEARCH: ABI Innovation: Computational and Informatics Tools for Supporting Collaborative Wildlife Monitoring and Research","DBI","ADVANCES IN BIO INFORMATICS","06/01/2011","06/05/2013","Zhihai He","MO","University of Missouri-Columbia","Continuing grant","Anne Maglia","05/31/2015","$842,455.00","Tony Han, Joshua Millspaugh","hezhi@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","BIO","1165","1165, 9178, 9179","$0.00","The University of Missouri and the University of Illinois at Urbana-Champaign are awarded collaborative grants to develop advanced computational and informatics tools that will support wildlife data collection, analysis, and management at large scales.  Project objectives include investigation of 1) advanced computer vision methods for detecting and tracking animals in dynamic and cluttered environments; 2) adaptive classification, machine learning, and information fusion methods for recognizing animal species and individual ID; and 3) data summarization and database management schemes to support collaborative wildlife research.  The performance of these computational and informatics tools will be evaluated using existing camera trap datasets and field studies in terms of their potential to support collaborative wildlife research.  <br/><br/>This project will broadly advance the state-of-the-art in computer vision, wildlife monitoring, ecology, and conservation research.  It will provide new methods and tools for automated processing and mining of massive wildlife monitoring data at large scales.  This will allow individual or coordinated networks of wildlife researchers to analyze and manage camera-trap data with minimum effort and compare and share data between research groups across different geographical regions.  Collaborative wildlife monitoring and tracking at large geographical and time scales will help us understand the complex dynamics of wildlife systems, evaluate the impact of human actions and environmental changes on wildlife species, and answer many important wildlife, ecological, and conservation research questions.  The database will be hosted by Smithsonian.  This will provide exciting interdisciplinary opportunities for mentoring graduate students and involving K-12 and undergraduate students into professionally guided research.  Software and results of this project will be available from the website http://videonet.ece.missouri.edu."
"1063169","REU Site in Computer Systems","IIS","RSCH EXPER FOR UNDERGRAD SITES","02/01/2011","01/27/2011","Zachary Dodds","CA","Harvey Mudd College","Standard Grant","Maria Zemankova","01/31/2014","$367,461.00","Melissa O'Neill","dodds@cs.hmc.edu","301 Platt Boulevard","CLAREMONT","CA","917115901","9096218121","CSE","1139","9250","$0.00","The Harvey Mudd College REU brings 10 undergraduates to Claremont in order to engage them in research and encourage graduate study in computer science. The program provides a microcosm of the graduate experience through a set of four projects under a broadly-scoped systems umbrella. The first develops algorithms for estimating cophylogenetic trees based on biological host/parasite data. Students' algorithms have been encapsulated in a software package named Jane, currently in use by several biological research groups. The second effort investigates approaches for automatically creating - and helping humans create - jazz. Its Impro-visor software has thousands of users and supports both the jazz and computational music communities. The third project creates efficient algorithms for memory management within garbage-collected computer languages such as Java, and the fourth project tests machine-learning and computer-vision-based algorithms that improve the state-of-the-art of performance of low-cost robot platforms, such as the iRobot Create.<br/><br/>In addition to the contributions of these four projects within each of their domains of specialization, the REU's environment emphasizes the benefits of pursuing CS at the graduate level for participants who had not previously considered graduate work. Challenging research problems prompt students' guided development of research skills: investigation, presentation, and publication. Introductions by advisors transition into student-led talks and culminate with publications and conference experiences during or after the summer.  Although proud of the academic contributions of the REU participants, the program's most important impact lies in its cultivation of the next generation of CS researchers."
"1059436","II-EN: UCI Irvine Sensorium","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","05/01/2011","05/12/2015","Sharad Mehrotra","CA","University of California-Irvine","Standard Grant","Sylvia Spengler","12/31/2015","$373,000.00","Ramesh Jain, Nalini Venkatasubramanian, Michael Carey","sharad@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7359","7359","$0.00","Dr. Sharad Mehrotra and an interdisciplinary team of collaborators at the University of California at Irvine (UCI) will develop I-sensorium, to serve as a ""living laboratory"" to support research in several related areas of cyber-physical systems: including theoretical foundations and underlying principles of building sentient systems; engineering, software, and systems level challenges; and novel application contexts where such sentient systems can be used. I-Sensorium will serve as a testbed for exploring and testing novel sentient technologies. I-Sensorium augments Responsphere, an existing UCI crisis response test-bed with state-of-the-art storage and computational infrastructure to acquire and process continuous streams of sensory data from Responsphere sensors. The I-Sensorium software will leverage multiple ongoing research projects at UCI on large-scale, heterogeneous sensor databases, sensor middleware platforms for querying and analysis of heterogeneous sensor data, and an event representation system for multi-media data, to provide  a high level programming environment for the I-Sensorium allowing a broad group of researchers to participate and benefit from its creation. <br/><br/>The development of I-Sensorium offers opportunities for research-based education and training in many aspects of cyber-physical systems. The infrastructure as well as  the sensor data acquisition enabled by it presents research challenges and opportunities in  sensor data management, machine learning, computer vision, event modeling, among others. Interactions with industrial partners and government agencies are expected to lead to advances in the applications of cyber-physical systems in real-world settings. Education and outreach efforts  are aimed at broadening the participation of women and members of underrepresented groups in computer science and engineering. Further information about this project can be found at: http://www.i-sensorium.org"
"1117015","RI: Small: Automatic Speech Recognition of Unconventionally-Vocalized Speech","IIS","Robust Intelligence","09/01/2011","04/08/2013","Daniel Ellis","NY","Columbia University","Continuing Grant","Tatiana Korelsky","08/31/2016","$457,999.00","","dpwe@ee.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7495","7495, 7923, 9251","$0.00","Despite great advances in computer recognition of conventional speech, automatic recognizers have enormous trouble with speech that deviates from speech norms in pitch range, speaking style, and timing.<br/>Unconventional speech includes the ""motherese"" used to speak to young children, certain kinds of dysarthric speech, and singing.  Current speech recognizers draw their power from statistical models of very large collections of real speech, but the corollary of this power is that speech that differs from this norm cannot be handled nearly so well.<br/><br/>The goal of this project is to create a speech recognition system able to handle a broad range of non-canonical speaking and voicing styles. As a motivating basis, we will target the transcription of singing.<br/>Sung speech poses a number of significant challenges with implications in broader speech scenarios: In comparison with conventional speech, the speech timing is highly distorted; the pitch level, range, and dynamics are very different; and there are frequently simultaneous sound sources (i.e., accompanying instruments) whose signals must be distinguished from the voice.<br/><br/>The approach is to make a best-effort separation of the voice, e.g., by closely filtering the predominant pitch in a mixed signal.  This candidate voice is then transformed and normalized to resemble conventional speech: The pitch harmonics are interpolated to achieve a more pitch-invariant spectrum, and the time axis is warped to achieve a more uniform rate of change (eliding over sustained, unchanging sounds).  Then, a conventional speech recognizer is adapted to recognize this normalized speech.  To train the recognizer for the target domain, a substantial collection of music audio is manually aligned with phoneme-level transcriptions of the singing.  This corpus will be freely available to other researchers in music and non-canonical speech.<br/><br/>This work will develop techniques to make current speech recognition applicable to a much broader range of speech material and speakers."
"1115963","Interior-point algorithms for conic optimization with sparse matrix cone constraints","DMS","COMPUTATIONAL MATHEMATICS","09/01/2011","06/06/2011","Lieven Vandenberghe","CA","University of California-Los Angeles","Standard Grant","Junping Wang","08/31/2015","$303,100.00","","vandenbe@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1271","9263","$0.00","Conic optimization is an extension of linear programming in which the <br/>componentwise vector inequalities are replaced by inequalities with <br/>respect to nonpolyhedral convex cones.  The conic optimization model is  <br/>widely used in the recent literature on convex optimization and provides<br/>an elegant framework for extending interior-point algorithms from linear <br/>programming to convex optimization.  It is also the basis of popular <br/>modeling systems for convex optimization.  <br/>The research on algorithms for conic optimization has mainly focused on  <br/>three types of inequalities, associated with the nonnegative orthant, <br/>the second-order cone, and the positive semidefinite cone.  <br/>This restriction is motivated by symmetry properties that can be exploited <br/>to formulate symmetric primal-dual interior-point algorithms.<br/>However, large gaps in linear algebra complexity exist between the <br/>three types of conic constraints, and this can lead to inefficiencies when <br/>convex optimization problems are converted to the standard conic format.  <br/>This study considers approaches to improve the efficiency of conic optimization <br/>solvers by considering a larger class of conic constraints, <br/>defined by chordal sparse matrix cones, i.e., cones of positive <br/>semidefinite matrices with a given chordal sparsity pattern, <br/>and the associated dual cones of chordal sparse matrices that <br/>have a positive semidefinite completion.  These cones include as special <br/>cases the three standard cones, but also several interesting non-self-dual <br/>cones.  Moreover non-chordal sparsity patterns can often be efficiently <br/>embedded in chordal patterns and, as a consequence, sparse semidefinite <br/>programs can be solved as non-symmetric cone programs involving <br/>lower-dimensional cones than the positive semidefinite cone used in  <br/>semidefinite programming methods.  The choice for chordal matrix cones is  <br/>further motivated by the existence of fast algorithms for evaluating the <br/>associated barrier functions and their derivatives.<br/>The investigator and his collaborators study nonsymmetric <br/>interior-point algorithms for sparse matrix cones, building on techniques <br/>developed for large-scale sparse matrix computations, in particular, <br/>multifrontal and supernodal factorization algorithms and parallel sparse <br/>matrix algorithms.<br/><br/>A wide variety of practical problems in engineering and science can be  <br/>formulated as nonlinear convex optimization problems, and solved using <br/>algorithms developed over the last few decades.  <br/>The success of these techniques has created a demand for robust and <br/>efficient algorithms for very large convex optimization problems, <br/>especially for applications in machine learning, computer vision, <br/>electronic design automation, sensor networks, and combinatorial <br/>optimization.  The problem sizes that arise in these fields often <br/>exceed the capabilities of general-purpose solvers.  <br/>The work of the prinicipal investigator with his collaborators considers approaches to improve the scalability of interior-point<br/>algorithms, an important class of convex optimization algorithms.<br/>Freely available high-quality software implementations of the techniques developed in the<br/>project are a product of the research."
"1052653","RUI: Effects of visual phonetic similarity on audiovisual spoken word recognition","BCS","LINGUISTICS","07/15/2011","06/03/2014","Lorin Lachs","CA","California State University-Fresno Foundation","Standard Grant","Joan Maling","06/30/2015","$107,922.00","","llachs@csufresno.edu","5241 N. Maple Ave","Fresno","CA","937400001","5592780840","SBE","1311","1311, 9229, SMET","$0.00","Spoken word recognition is defined as the process by which acoustic patterns are matched to meanings in the ""mental lexicon"" -- the memory repository of the approximately 85000 words known by an adult language speaker. Previous research has demonstrated that the number of lexical entries acoustically similar to a given word influences the ease with which that word is recognized. However, speech recognition is not solely an acoustic phenomenon; watching someone speak also provides additional information about the content of an utterance. Although there is presently a solid understanding of the role of acoustic similarity in spoken word recognition, less is known about visual similarity, and very little is known about the interaction of acoustic and visual similarity when both sources of information are available.  This may be because visual similarity is particularly difficult to measure. One problem is that speech units that are acoustically different can be visually identical. Words that are visually identical are said to comprise a Lexical Equivalence Class (LEC). Previous research has shown that the number of words residing in an LEC affects the ease with which those words are lipread. Similarly, the size of an LEC affects the extent to which visual information enhances the recognition of auditory speech. However, the makeup of an LEC is to some extent dependent on the speaker. The proposed research will accomplish two goals: first, a publicly accessible computational tool will be built to facilitate computational and experimental investigations of visual lexical similarity. Second, several behavioral experiments will further elucidate the role that visual similarity plays in spoken word recognition. This project will advance our understanding of the basic mechanisms involved in spoken word recognition. Such knowledge will be useful for clinicians working with deaf or hearing-impaired populations, and engineers working on problems in automatic speech recognition."
"1059266","CI-P Seeing speech: A community resource for analysis of multi-modal language data","CNS","Linguistics, Perception, Action & Cognition, CCRI-CISE Cmnty Rsrch Infrstrc","03/01/2011","02/18/2011","Diana Archangeli","AZ","University of Arizona","Standard Grant","Tatiana Korelsky","02/28/2013","$100,000.00","Ian Fasel","dba@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","1311, 7252, 7359","7359","$0.00","Producing human speech requires the exquisite timing of multiple interacting modalities: movements of the lips, tongue, velum, and vocal folds are all precisely coordinated to give each sound its unique properties. Understanding how these different modalities interact is important to basic linguistic and cognitive science as well as to applied research areas, such as automatic speech recognition, speech therapy, and second language learning. This workshop will bring together experts in speech database construction, speech recognition, and articulatory research to explore the feasibility and desirability of developing software and a database to study the interaction of the speech articulators and how their coordination relates to the sounds produced. The workshop brings these three communities together to address (i) requirements of software for extracting and analyzing articulatory data in conjunction with the acoustic signal, and (ii) properties of the database to be constructed, namely: which utterances should be recorded; how to synchronize capture of the multiple modalities; markup, annotation, and storage of the data.<br/><br/><br/>The goal of the project is to develop both software for multi-modal speech analysis and a database of synchronized multi-modal speech recordings. A critical first step is to assess which features of the database and software are needed to maximize the long-term value to the scientific community. This workshop brings together researchers from diverse backgrounds in human language and computer sciences to examine these issues, so that the database and software may facilitate studies of oral tract articulation across many disciplines, e.g. to understand the diversity of human language sounds, language acquisition and endangered languages, explore speech deficits, teach foreign language pronunciation or oral language to the profoundly deaf, improve speech recognition and synthesis software, understand how musicians shape sounds while playing wind instruments, etc."
"1124479","Acoustic variability and perception of children's speech","BCS","LINGUISTICS","09/01/2011","08/22/2011","Peter Assmann","TX","University of Texas at Dallas","Standard Grant","William J. Badecker","02/28/2017","$283,023.00","","assmann@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","SBE","1311","1311","$0.00","No two voices are exactly alike, and speech sounds can vary dramatically when produced by different individuals.  A substantial component of this variability stems from anatomical differences between speakers that reflect their age and sex. Although these factors complicate the relationship between acoustic cues and phonetic properties of speech, they provide information from which the listener can determine the age, sex and size of the speaker, referred to as indexical properties.<br/><br/>The aim of the proposed research is to investigate the relationship between indexical and phonetic properties in children's speech through four linked projects. Project 1 involves the construction and acoustic analysis of a vowel database from children ranging in age from 5 through 18 years. The database will provide the materials for experiments investigating the perceptual consequences of age-related changes in speech. In Project 2, natural and modified versions of the recordings will be used to examine the cues that distinguish male from female voices at different ages. Project 3 will investigate the perception of speaker age in children's voices and evaluate the effectiveness of vocal age conversion using synthesis techniques based on models of vocal tract scaling. Project 4 investigates the link between vowel identification and indexical properties, requiring listeners to provide vowel identification responses together with judgments of the perceived sex and age of the speaker. Pattern recognition models will be implemented using acoustic measurements from the database to model the statistical relationships between the acoustic properties of children's speech as a function of age and sex, and to predict listeners' responses in the perceptual experiments.<br/><br/>This research will provide valuable information on speech development and the processes by which listeners extract linguistic and indexical information from children's speech. The findings could provide useful information for automatic speech recognition systems applied to children's speech, reveal effective strategies for synthesizing children's voices, and serve as normative data in clinical studies of disordered speech."
"1116530","RI: Small: Robust Auditory Object Recognition with Spike Sequence Coding and the State-Dependent Dynamics of Cortical Networks","IIS","ROBUST INTELLIGENCE","09/01/2011","08/22/2011","Dezhe Jin","PA","Pennsylvania State Univ University Park","Standard Grant","Kenneth Whang","12/31/2014","$296,470.00","","djin@phys.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7495","7923","$0.00","Recognizing speech or other auditory objects in adverse environments -- e.g. with noise, reverberation, and multiple speakers -- is essential for human and animal communication.  Current speech recognition technologies work well in high signal-to-noise conditions, but perform orders of magnitude below human performance in adverse conditions. Converging evidence from neuroscience suggests that auditory information is encoded in sparse and precisely timed spikes of sub-cortical neurons. However, the extent to which codes based on spike timing might underlie the robustness of human auditory object recognition has not yet been fully investigated. This project bridges this gap by devising a biologically inspired computational model of auditory processing at the cortical level and extracting computational principles that are essential for the model to achieve robust auditory object recognition.<br/><br/>The approach is to transform sounds into the spike sequences generated by feature-detecting thalamic auditory neurons, and to integrate these spikes spatially and temporally using the state-dependent dynamics of cortical neurons with active dendrites. In the proposed model, an auditory object first evokes sequential spiking of thalamic neurons that have been trained to detect useful features. Then, through feed-forward excitation and inhibition from the thalamus, and lateral excitation and inhibition from the cortical neurons, the state of the cortical network evolves, leading to temporal integration. Recognition of the auditory object is signaled when the cortical neurons reach a specific network state. The computational model is constrained by experimental results on the properties of cortical neurons, the organization principles of cortical networks, and the activity-dependent plasticity rules of the network structures. The project aims both to design feature detectors that can robustly represent auditory objects with spatiotemporal spike sequences, and to build a cortical network model that can recognize specific auditory objects using state transitions driven by the thalamic inputs, with neuron dynamics that can be compared with those observed in the auditory cortex. The recognition performance of the computational model will be evaluated and improved with auditory tasks designed to compare different approaches to speech recognition."
"1056409","CAREER: Representations of phonetic reduction and dialect variation in speech production and perception","BCS","LINGUISTICS","07/15/2011","03/13/2015","Cynthia Clopper","OH","Ohio State University","Continuing grant","William J. Badecker","06/30/2017","$407,955.00","","clopper.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","SBE","1311","1045, 1187, 1311, 9251","$0.00","Human speech is inherently highly variable: no two utterances of the same word produced by the same talker are acoustically identical. The sources of this variability include properties of individual words, such as how often they occur in spoken language; properties of the context in which a word is uttered, such as the predictability of a word given the previous word; properties of the conversational setting, such as formality; and properties of the talker, such as gender or regional background. Most previous research in this area has focused on one or two sources of variation at a time. However, recent advances in statistical analysis make possible the examination in this study of many variables simultaneously. The two specific hypotheses that will be tested are that the variation due to linguistic and social factors is highly systematic and that this systematicity can facilitate speech processing. Thus, the first goal of this project is to determine how much of the variability in speech can be accounted for by these kinds of linguistic and social factors. To achieve this goal, speech produced by native speakers of American English will be recorded and analyzed in a series of production experiments that manipulate different combinations of these sources of variability. The second goal of the project is to determine how these sources of variability affect the perception and representation of the linguistic and social information that is carried by speech. To achieve this goal, word recognition, word processing, and dialect identification tasks using the recordings collected in the production experiments will be conducted. In addition to substantially contributing to our understanding of variation in speech production and perception, the results of this project have the potential to inform research on language acquisition, language processing in autism spectrum disorders, and speech technology, including automatic speech recognition."
"1125164","Phonetic Characteristics of Epenthetic Vowels in Palestinian Arabic","BCS","Linguistics","08/15/2011","08/20/2012","Nancy Hall","CA","California State University-Long Beach Foundation","Continuing grant","William Badecker","07/31/2015","$202,802.00","","nhall2@csulb.edu","6300 State Univ. Dr.","Long Beach","CA","908154670","5629858051","SBE","1311","1311, 9229","$0.00","This project explores the phonetics of several sub-dialects of Arabic spoken in Israel. The main focus is on the acoustic properties of vowels and the ways that native speakers perceive them. A distinctive feature of Arabic is that in casual speech, a great many words can optionally be pronounced with extra vowels: for example, the Arabic word for ""girl"" can be pronounced as either ""bint"" or ""binit"". There is evidence that the optional vowels are different from regular vowels, but the details of these differences have not been investigated in depth. The project consists of data collection, elicitation, and experiments designed to identify the nature of these vowels, their acoustic properties, and how speakers perceive them.<br/><br/>Phonetics experiments will be carried out at several Israeli universities, with Arabic-speaking university students as participants. The project will train U.S. graduate students in acoustic speech analysis, experiment design and phonetic fieldwork.<br/><br/>The project will have both practical and scientific implications. A better understanding of the phonetics of Arabic may help advance the state of the art in automatic speech recognition, speech to speech translation, speaker identification, as well as the teaching of colloquial Arabic. Scientifically, the optional vowels have posed difficulty for theories of language due to certain puzzling properties, for instance that they never appear in stressed syllables. A better understanding of the acoustics of these vowels can help resolve such puzzles. More generally, the project will contribute to our understanding of the mental and physical processes underlying human speech production and perception."
"1116076","RI: Small: Emotional Speech Production: Analysis, Modeling and Synthesis","IIS","ROBUST INTELLIGENCE","08/01/2011","07/28/2015","Sungbok Lee","CA","University of Southern California","Standard Grant","Tatiana D. Korelsky","07/31/2016","$450,000.00","Shrikanth Narayanan","sungbokl@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","7923","$0.00","Encoding of emotions in speech is achieved by vocal modulations that require an intricate control of human voicing and vocal tract articulation. The aim of this research is to identify and model articulatory processes for emotional speech production based on advanced speech production data acquisition technologies including Electromagnetic articulography (EMA) and a real-time Magnetic resonance imaging (rt-MRI). The research focuses on directly measuring and modeling articulatory kinematics and their interplay with prosodic modulation of pitch, loudness and segmental durations in speech emotion expression in order to understand the emotional speech production strategies across emotion types as well as across speakers. The validity of the emotional speech production models is verified by using a software articulatory synthesizer in an analysis-by-synthesis fashion. Theoretical implications of the findings are interpreted in relation to the Hyper and Hypo theory and the Converter/Distributor (C/D) model of speech production.<br/><br/>Detailed knowledge on the effects of emotion on the human speech articulatory and prosodic patterning has transformative potential in developing improved speech processing technologies for emotional speech recognition and synthesis that are critical for the development of natural and robust man-machine interfaces. This goal also critically includes informing quantitative assessment of expressive speech to characterize atypical or distressed vocal behavior in diverse populations, for instance, children with Autism Spectrum Disorder (ASD). Finally, a natural by-product of this research effort is the unique articulatory database that will be shared freely with the community for further expansion of the knowledge of human speech production."
"1133625","Improving Sound Perception with an Advanced Intracochlear Electrode Array and Integrated Insertion Platform","CBET","Disability & Rehab Engineering","10/01/2011","05/05/2017","Pamela Bhatti","GA","Georgia Tech Research Corporation","Standard Grant","Michele Grimm","09/30/2017","$311,999.00","","pamela.bhatti@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","5342","010E","$0.00","PI: Bhatti, Pamela<br/>Proposal Number: 1133625<br/><br/>The research objective of this proposal is to test the hypothesis that intracochlear electrical stimulation with a polymeric high-density electrode array enables a more focused and selective activation of auditory neurons when compared with contemporary cochlear electrode arrays. To achieve the research objective we must develop a method for integrating a flexible high-density array with a silicone insertion platform (IP), and test the integrated device mechanically, electrically, in-vitro, and eventually in-vivo. We divide the proposed research into two phases. In Phase I we will focus on using an IP that has been validated in humans and (a) microfabricate and integrate a thin-film array with the IP, (b) using a 3D human cochlear model, validate the integrated array mechanically through bend tests and insertion studies, and electrically by impedance measurement, and (c) compare the efficacy of intracochlear electrical stimulation of the high-density array with a contemporary array in the cat model. In Phase II we leverage the methods established in Phase I to develop an array specifically sized for the cat model by integrating a thin-film array with a molded silicone insertion platform. <br/>INTELLECTUAL MERIT  Over 180,000 individuals use cochlear implants worldwide and many achieve a high level of speech recognition. But there remains a group of patients achieving poor speech recognition and difficulty understanding speech in noisy environments. One potential means to overcome this challenge is to provide an enhanced coupling between the electrical to neural interface with a high-density array. Implementing such an array is impossible with contemporary fabrication methods where arrays are constructed by hand from bundles of wire encased in silicone. There simply is not enough room in the cochlea to scale up this method. This proposal is the first effort at integrating a thin-film based array with an IP. This is a transformative approach as it leverages the fine feature resolution (1m) offered by microfabricated thin-films and combines such arrays with an IP commanding the same mechanical flexibility, dimensions and features of contemporary arrays validated in humans. Furthermore, the introduction of integrated high-density arrays sized for the cat model will enable fundamental studies exploring and comparing novel electric fields shaping strategies, such as current steering and current focusing, employed to optimize the electrical-to-neural interface.<br/>BROADER IMPACTS  Integrated thin-film arrays open a host of possibilities to further improve patient performance with cochlear implants. Recent studies have illustrated the benefit of combined electrical (high frequency) and acoustic (low frequency) stimulation for improving speech perception, especially in noisy environments. An important component is a short (10mm) array inserted into the base of the cochlea that preserves any remaining low frequency neural elements by minimizing trauma. By decoupling the mechanical design of the platform from the array, our approach enables continued development of less traumatic platforms while retaining the high-density electrode configuration.  Furthermore, integrating a thin-film array with an insertion platform may enable such arrays to approach other structures such as stimulation of vestibular nerve fibers for a vestibular (balance) prosthesis and potentially deep brain stimulation to mitigate Parkinson's disease, epilepsy and depression.  To broaden the participation of underrepresented groups, the PI maintains a strategy for integrating her research with outreach, mentoring, and teaching to engage students across the K-graduate continuum. This includes ""Science Nights"" at the Fernbank Science Center (Atlanta, GA) and codeveloping science modules with local middle/high schoolteachers. She guides undergraduate and graduate researchers in her BioSystems Interface Lab, mentors minority students, and is creating a graduate-level Hybrid Biosystems course addressing vestibular, cochlear and cardiac biosystems. The PI has developed a LabVIEW simulation of a cochlear implant signal processor as an IEEE Real World Engineering Project supporting free open-licensed educational materials thereby enabling her to reach an international student/teacher population. And finally, the PI is partnering with the local cochlear implant community to create undergraduate and graduate research opportunities for deaf students in her lab."
"1118610","SBIR Phase II:  ASL Literacy Support System","IIP","SBIR Phase II, International Research Collab","09/15/2011","07/28/2015","Corinne Vinopol","md","Institute for Disabilities Research and Training, Inc.","Standard Grant","Glenn H. Larsen","07/31/2015","$903,049.00","","corinne@idrt.com","11323 Amherst Avenue","silver spring","md","209024695","3019424326","ENG","5373, 7298","010E, 115E, 164E, 167E, 169E, 5373, 5976, 7218, 7385, 8031, 8033","$0.00","This Small Business Innovation Research Phase II project will result in a robust assistive technology, cost-wise accessible to deaf individuals and their families/service providers, as well as businesses, which functions as:<br/><br/>(1) An instructional tool to improve the literacy of deaf children and adults, and <br/>(2) A real-time translation device (i.e., between American Sign Language and English).<br/><br/>The technology will accommodate a variety of input and output options:<br/>Input: (1) typing, (2) scanning, (3) screen text transfer, (4) sensor-enabled glove (the AcceleGlove?), (5) 3-D camera, (6) speech recognition; and<br/>Output: (1) text, (2) sign graphics, (3) sign video clips, (4) speech.<br/><br/>The Instant ASL Communication System, as it is called, has two access modes: DVD, Web or local server-based access. This hardware/software system also will enable the user to edit, print, select appropriate signs when more than one match the English word and vice versa, ?hide? signs when support is not wanted, retrieve sign graphics/videos through an index, and generate flashcards and sign/word matching worksheets. The product will include a translation lexicon of 24,000 English words/phrases and 8,000 signs. Many deaf children are challenged by reading since this process largely depends on auditory understanding. Teachers of the deaf frequently reinterpret text into ASL or enhance it with signs to render it more comprehensible to their students. Research has shown that incorporation of signs with text provides a multimodal approach to the development of early literacy skills that utilizes multiple intelligences and learning styles. <br/><br/>The broader impact/commercial potential of this project is largely reflected in its effect on the Deaf community and those who interact with them. ASL is a visual/gestural language distinct from English. Many deaf people who rely on sign language do not have good facility with English. Because English is an auditory mediated language that depends upon phonological code, reading achievement scores of deaf children usually fall far short of those found among hearing children of comparable abilities. An interesting aspect of the low reading skill levels displayed by deaf students is that while they may not understand a sentence in print, they may understand it perfectly presented in ASL. This product will be tremendously useful to teachers, business personnel, speech/language pathologists, etc. who have a need to support understanding of English text with ASL signs for purposes of literacy improvement, curriculum enhancement, or communication. This product will enable English users to type, scan text, or paste screens text and have output in text with ASL graphics and/or video support. Inversely, deaf users will be able to sign to it and obtain English text and audio output. As a server-based translation service, there will be considerable impact on the ability of deaf individuals to be integrated into society at large for employment, education, and social purposes. Improvements to the AcceleGlove? will have implications to other fields of R&D, such as robotics, telemedicine, virtual reality, and defense. The gesture library will have utility to other gesture capture strategies (e.g., camera-based)."
"1117439","CIF: Small: RUI: Fixation-Driven Contour Integration of Natural Images for Early Visual Processing","CCF","Comm & Information Foundations","08/01/2011","07/16/2011","Toshiro Kubota","PA","Susquehanna University","Standard Grant","John Cozzens","07/31/2015","$170,619.00","","kubota@susqu.edu","514 University Ave","Selinsgrove","PA","178701164","5703724571","CSE","7797","7923, 7936","$0.00","A reliable system for automated image understanding can have immense impacts on many applications including image search, video surveillance, autonomous vehicles, and robotics, to name a few. However, the progress of the technology has been slow compared to text understanding and speech recognition. The problem can be attributed to lacking ways of breaking an image into a set of meaningful components analogous to words in text and speech processing. This research will develop an algorithm to partition a digital image into such meaningful components efficiently and effectively. To reach the goal, the investigators focus on discontinuities in color and brightness often called ""edges"" and study algorithmic ways to group them and delineate objects found in the image. Undergraduate students will actively participate in interdisciplinary research involving computer science, mathematics, statistics, psychology and biology. The PI will also develop an interdisciplinary course that integrates cognitive, neuro, and computer sciences. The resulting source code, software tools, data, and visual materials will be made publicly available to promote STEM education.<br/><br/>More specifically, the study centers on two recent innovations developed by the investigators: semi-group smoothing with a matrix of linear filters, and successive partitioning of a graph with increasing complexity. The former is an affine commutative linear operator that is shown to be effective in extracting high-curvature points while robust against aliasing. It will be used to smoothen, partition, and characterize contour fragments. The latter generates a set of closed contours surrounding a focal point successively from a simple shape to more complex ones. Furthermore, the study investigates new context sensitive perceptual saliency metrics that quantify the importance of edges based on their surroundings."
"1052819","Doctoral Dissertation Research: Energy measures in the stop VOICING contrast","BCS","Linguistics","09/01/2011","08/15/2011","Thomas Purnell","WI","University of Wisconsin-Madison","Standard Grant","Joan Maling","08/31/2013","$11,600.00","Blake Rodgers","tcpurnell@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","SBE","1311","1311, 9179, SMET","$0.00","This dissertation research seeks to better understand how listening is informed by the act of moving our vocal apparatus as well as the acoustic and perceptual implications of crosslinguistic articulatory differences. A number of factors are known to contribute to our ability to make use of the acoustic signal such as articulatory strength, trading relations, and acoustic energy in contrasts. The fundamental theoretical question addressed here is whether articulation per se has direct implications for abstract gestures as distinctive features. The project builds on earlier work investigating the role of energy characteristics. Acoustic data will be collected for a range of native speakers of such typologically distinct languages as English, Dutch, German and French. <br/><br/>Tongue position and air pressure data for plosive sounds at the beginning and end of a syllable will be synchronized with the acoustic recordings. A comprehensive data set for these languages will be used to, first, establish percepts and the boundary conditions within which a perception study will be conducted, and, second, increase our understanding of the articulation-acoustic relation across the speech chain. The relative spectral energy levels in lower harmonics will be measured and evaluated, along with their changing characteristics as a speaker makes a vowel after or before one of these consonants. These spectral energy change characteristics have been found to be more correlated with voicing contrasts than the traditional measures of voice onset time, vowel length, frequency changes, etc. across contexts and across language types. Information gained from the acoustic and articulatory data collection will be used to generate tokens to be tested in a subsequent set of perception experiments. The work has implications for automatic speech recognition technology."
"1116051","HCC: Small: MobileAccessibility: Bridge to the World for Blind, Low-Vision, and Deaf-Blind People","IIS","HCC-Human-Centered Computing","08/01/2011","06/27/2013","Richard Ladner","WA","University of Washington","Continuing Grant","Ephraim Glinert","07/31/2015","$516,000.00","Jeffrey Bigham","ladner@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7367","7367, 7923, 9251","$0.00","More than 160 million blind, low-vision, and deaf-blind people worldwide have not realized the full potential of the mobile revolution.  People in these groups often use special-purpose portable devices to solve specific accessibility problems, such as obtaining product information from bar codes, finding location information via GPS, and accessing printed text using optical character recognition (OCR).  Unfortunately, devices targeted at these groups are specialized for one or few functions, usually not networked, and expensive.  Devices also target one disability, thereby preventing a deaf-blind person from, for instance, using a device designed for a low-vision person.  Blind, low-vision, and deaf-blind people who can afford it must carry multiple devices with varying interfaces.  This is despite the fact that many mainstream mobile devices already have the necessary sensors, such as a camera, microphone, GPS locator, accelerometer, and compass, to provide all of these functions on one device.  MobileAccessibility is the PI's approach to providing useful mobile accessible functionality to blind, low-vision, and deaf-blind users.  This approach leverages a smart phone's sensors, multi-modal output, and access to remote services to reduce the cost of existing accessibility solutions and enable completely new ones to be created.  Some key user interaction problems for these groups of users that will be addressed in this project include: (i) how can a blind, low-vision, or deaf-blind person effectively use the camera on a smart phone to achieve an accessibility goal, (ii) how can enlarged presentations be effectively navigated by a low-vision person on the small screen of a smart phone, (iii) how can vibration be effectively used to convey information to a blind or deaf-blind person, (iv) how can valuable network services be best utilized by these communities, (v) how can the knowledge of one person about their environment be effectively captured, stored, and used among these communities.  The user-centered design of these applications will involve blind, low-vision, and deaf-blind people throughout their development.  Prototype applications to provide context to the research questions will be built for all three groups. Input will use speech recognition, the touch screen, and the keyboard.  Output will be audio for blind users, enlargement for low-vision users, and vibration and tethering to Braille devices for deaf-blind and blind users.  The resulting interfaces will be evaluated both in the lab and in the field.  There will a focus on identifying common interaction techniques that can be employed by multiple applications.<br/><br/>Broader Impacts:  This research represents a new paradigm in mobile assistive technologies where a single programmable device can serve a multitude of accessibility needs.  Rather than using separate devices for different needs, accessibility solutions can be downloaded to a single device.  The research challenge is to design, build, and evaluate novel accessibility solutions in this new paradigm.  A mobile phone that can accomplish multiple accessibility tasks has the potential to provide the target communities with more independence than they have currently.  Furthermore, the MobileAccessibility solution has the potential to be inexpensive and more sustainable than current accessibility solutions.  Qualified students with disabilities will be recruited as researchers, giving them a chance to participate in work directly affecting them. New project-oriented curricula based on MobileAccessibility will be created."
"1127650","EAAI-11: The Second Symposium on Educational Advances in Artificial Intelligence","IIS","ROBUST INTELLIGENCE","03/01/2011","02/16/2011","Kiri Wagstaff","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Edwina L. Rissland","02/29/2012","$17,000.00","Marie desJardins","Kiri.Wagstaff@jpl.nasa.gov","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495","$0.00","This award supports participants to EAAI-11, The Second Symposium on Educational Advances in Artificial Intelligence. EAAI-11 will be collocated with the Twenty-fifth AAAI Conference on Artificial Intelligence (AAAI-11), to be held August 7-11, in San Francisco. EAAI-11 provides a venue for researchers and educators to discuss pedagogical issues and share resources related to teaching artificial intelligence (AI) and using AI in education across a variety of curricular levels (K-12 through postgraduate training), with a natural emphasis on undergraduate and graduate teaching and learning. The EAAI symposium will seek and disseminate contributions, such as model assignments, syllabi, projects, and ready-to-adopt materials for teaching AI, address how to more effectively teach AI in multi-disciplinary contexts involving subjects like biology or economics, and how themes from AI can be used generally to enhance education and engagement of students in Computer Science and STEM disciplines. EAAI will include a session ""How AI and AI education can improve CS education and outreach."""
"1058428","SBIR Phase II:  Multimodal Semantic Video Retrieval and Summarization","IIP","SBIR Phase II","04/01/2011","12/18/2012","Wael Abd-Almageed","MD","Video Semantics LLC","Standard Grant","Glenn H. Larsen","03/31/2013","$705,999.00","","wamageed@videosemantics.com","3565 A2 Ellicott Mills Dr","Ellicott City","MD","210430000","3013186427","ENG","5373","116E, 164E, 167E, 5373, 6850, 9139, 9231, 9251, HPCC","$0.00","This Small Business Innovation Research Phase II project will develop contextual video segmentation and automatic tagging technology and software. In long video streams that contain one or more topics, the software automatically discovers the beginnings and ends of Contextually-Coherent Video Segments in each video. Moreover, Video Semantics' technology automatically assigns textual tags to each segment such that these tags describe the topic discussed in that segment. The tags assigned make all parts of the video easily searchable.  Large video producers currently depend on manually segmenting their content into small segments and assigning textual tags to these segments in order to make them searchable. A short advertisement is then inserted before each segment. This manual segmentation and tagging process represents a significant pain point for content producers because it is labor intensive and not cost effective. Meanwhile, government agencies, which continuously monitor video content depend on speech recognition to spot specified keywords. This approach inflicts two pain points: (i) analysts have to deal with large number of false detections because the context in which the keyword occurs might be irrelevant, and (ii) if the keyword occurs in an important context, analysts still need to scroll back and forth into the video to find the beginning of the relevant segment.  <br/><br/>Video Semantics' technology and products have the potential to efficiently address significant market needs. In addition to the commercial applications, the proposed technology will enable media monitoring agencies to perform their tasks more efficiently saving valuable analyst time and resources. Moreover, because Video Semantics? technology is language-independent, media monitoring agencies will be able to monitor more content in foreign languages without the need to develop language-specific technologies. The company will employ an indirect sales strategy via partnerships with software companies that develop media monitoring solutions and metadata generation tools. The company has identified its first customer and is working with them to integrate the contextual segmentation and tagging technology with their current media monitoring solutions."
"1106480","AAAI 2011 Spring Symposium on Artificial Intelligence and Sustainable Design","IIS","ROBUST INTELLIGENCE","04/01/2011","03/25/2011","Douglas Fisher","TN","Vanderbilt University","Standard Grant","Tatiana D. Korelsky","03/31/2014","$30,000.00","","douglas.h.fisher@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","7495","7495","$0.00","Imperatives for environmental and societal sustainability are challenging designers to consider factors that had been previously given little attention. The Association for the Advancement of Artificial Intelligence (AAAI) Spring Symposium on Artificial Intelligence and Sustainable Design, held March 21-23, 2011 at Stanford University, focuses on the challenges of sustainable design and the role that artificial intelligence (AI) plays in understanding and achieving sustainability. The presumption is that the increased complexity of design necessitated by a desire for very long-term planet sustainability, to include cradle-to-cradle design, requires application of and advances in AI. NSF funds are supporting travel, subsistence and registration of symposium participants. The symposium and follow-up discussion will elaborate current and future research directions at the intersection of AI, design, and sustainability. We anticipate that the AAAI Symposium proceedings and the nascent AI and Sustainable Design community will be of interest outside of AI and computing. Sustainable design is a topic of importance to society; bringing this topic to the attention of AI researchers has the potential for advances in sustainable design that go beyond current computational and design approaches."
"1046036","SBIR Phase I:  Artificial Intelligence Software to Tutor Literary Braille to the Blind and Visually Impaired","IIP","SMALL BUSINESS PHASE I, REAL","01/01/2011","12/15/2010","Benny Johnson","PA","Quantum Simulations Incorporated","Standard Grant","Glenn H. Larsen","06/30/2011","$150,000.00","","johnson@quantumsimulations.com","5275 SARDIS RD","MURRYSVILLE","PA","156689536","7247338603","ENG","5371, 7625","110E, 5371, 6856, 9177, SMET","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims to focuses on developing the first artificial intelligence software to tutor literary Braille to blind/visually impaired students. Braille is the primary medium for written communication for the blind and there has been a dramatic decline in Braille literacy, negatively impacting academic performance, ability to navigate the everyday world and employment opportunities. The ability to bring proven effective AI technology to the table, will make a meaningful difference in providing equitable education opportunities to all students, as this project speaks directly to issues of basic literacy. Initially targeted for K-12 students, the majority of which are mainstreamed students served by the itinerant teacher of the visually impaired (TVI), the tutor will be web-based, enabling anyone to receive expert support on demand during study at school and home. Importantly, the tutor will operate using standard accessibility technology already in use. Because the tutor is supplemental to existing curricula and integrates directly with existing lessons, it will not require teachers/TVIs to change lessons, teaching materials, or schedules. In addition to improving learning outcomes for students, this project will also include support for teachers/TVIs responsible for instruction.<br/><br/>The broader/commercial impact of this project will be the potential for Braille education software based on artificial intelligence, delivered just-in-time through the Internet. The anticipated impact is that students achieve literacy and are able to perform at a higher level (e.g. academics, daily living, employment) resulting in improved quality of life and increased societal contributions. To have an impact, the product must be affordable, effective for a heterogeneous population in diverse learning environments, easy to use and easily accessed at convenient times and locations in informal and formal educational settings. In SBIR research supported by NIH, Quantum has successfully created AI-based educational software that is accessible to the blind (in chemistry). Furthermore, Quantum has successfully patented and commercialized unique AI technologies in chemistry and accounting using a business-to-business licensing model that provides educational companies with first-to-market and strong sustainable advantages. This model engages the entire spectrum of educational vendors, offering breakthrough technology that permits increased market share for customers and rapid dissemination to end users. For this project, Quantum will partner with organizations with established channels, who distribute the software as an online service, such as the American Printing House for the Blind, a partner on this project.<br/><br/>"
"1138325","CRPA: Communicating Avatars: Artificial Intelligence + Computer Graphics = Innovative Science","DRL","AISL","10/01/2011","09/25/2011","Avelino Gonzalez","FL","The University of Central Florida Board of Trustees","Standard Grant","Paul Jennings","09/30/2014","$150,000.00","Ronald DeMara","gonzalez@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","EHR","7259","9177, SMET","$0.00","This CRPA project will produce a human-like avatar exhibit for the Orlando Science Center that will verbally communicate with middle and high school grade visitors, engaging them in the subjects of computer science, artificial intelligence, and engineering. Human-like characteristics include features to match the demographics of the Center's clientele and verbal communication in the English language. In addition to discussing how avatars are developed and how artificial intelligence works, the avatar image will answer questions from the visitors on selected topics, including subjects from the media models of Avatar and  IBM's Watson event on Jeopardy. <br/><br/>Considerable planning and research has gone into this project to make sure that the avatar is life-like and can engage in realistic dialog. The avatar images will resemble real individuals who have diverse demographic characteristics in order to enhance the human-computer interface. The system is designed to deal with background noise and antagonistic visitors. Evaluation at all levels (front-end, formative, and summative) will make the exhibit most effective and facilitate the goals of the project which are to inform the target audience on STEM subjects.<br/><br/>The desire to have electronic analogs of humans has been a goal for half of a century. This project builds on prior research in this area and is one of the most sophisticated contemporary models in the field. It is anticipated that this work may contribute to future applications in education and assistance for individuals with disabilities. Moreover, engagement with the avatar may ignite curiosity among young visitors and stimulate interest in science careers."
"1138158","Student Travel Support for the 15th International Conference in Artificial Intelligence in Education (AIED 2011)","IIS","Cyberlearn & Future Learn Tech","06/01/2011","05/25/2011","Gautam Biswas","TN","Vanderbilt University","Standard Grant","Janet L. Kolodner","05/31/2012","$20,000.00","","gautam.biswas@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","8020","7556, 8045, 9150","$0.00","This proposal seeks funding to support the travel of advanced doctoral students so that they can participate in the 15th International Conference in Artificial Intelligence in Education and in the Young Researchers Track (YRT) that is part of that conference. The Young Researchers Track provides a forum for Ph.D. students to present and discuss their work with mentors from outside of their home institutions and to meet peers with similar interests. Mentoring happens in two ways -- as part of the Young Researchers Track, where they will be encouraged and offered feedback by three mentors, and through an individual mentoring program where each doctoral student will be matched with a mentor with expertise in the student's research area. Mentors will be senior AI and ED researchers.<br/><br/>Participants in the Young Researchers Track are members of the next generation of cyberlearning researchers. Participation in the proposed program will supplement their education at their home institutions and help prepare them to be leaders in transforming education through the use of learning technologies."
"1047441","SBIR Phase I:  Commercial Scale-Up Of An Intelligent Modular Robot Platform iMobot for Research and Education","IIP","SMALL BUSINESS PHASE I","01/01/2011","12/09/2010","Graham Ryland","CA","Barobo, Inc.","Standard Grant","Glenn H. Larsen","06/30/2011","$150,000.00","","gryland@barobo.com","813 Harbor Blvd, Suite 335","West Sacramento","CA","956912201","9167158840","ENG","5371","1658, 5371, 9216, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project will study the feasibility for commercialization of an intelligent reconfigurable modular robot system called iMobot, which was originally developed at the University of California, Davis. Nowadays, robotics has grown beyond automation to encompass intelligent systems that are self-reliant, reconfigurable, mobile, intelligent, and aware of their environment. iMobot has four degrees of freedom capable of full mobility and assembly into clusters. Because of its flexibility, modularity, and reconfigurability, iMobot will be an ideal platform for many research and teaching programs in colleges and universities. For example, it allows researchers to study artificial intelligence, swarm technology, robot collaboration, mobile networking, and programming for reconfigurability. iMobot is designed with open architecture. Each module has a processor capable of supporting sensor fusion, gait simulation, and runs an open source embedded Linux operating system. Users can customize software and accessories for their specific needs. Proposed product feasibility research includes adaptable connectivity between modules, intelligent plug-and-play sensors, a robust and lightweight chassis, and re-configurability. In this proposed project, a professional design team will re-design and build a commercial quality prototype of iMobot for manufacturing in a large quantity. <br/><br/>The broader impact/commercial potential of this project is that the proposed project will be one of the first attempts to scale up an intelligent reconfigurable modular robot for commercial deployment. The iMobot could be used for university research and teaching, K-12 STEM education, and life-saving rescue and search operations in the first responder system. With a standardized hardware base using an open architecture, users will be able to more widely share their work with each other, and to create a valuable open educational resource. Robotics is an interdisciplinary field. The unique full mobility and reconfigurability of iMobot are very appealing to college and K-12 students. The robot can be used alone or in collaboration with others, making it a flexible and scalable educational tool. By introducing students to interesting robotic projects with affordable hardware platforms, which involve a variety of math, physics, information technology, and engineering principles, we can excite their imagination and give them confidence to pursue STEM careers, especially for underrepresented and economically disadvantaged groups."
"1068871","IGERT: Training Program in Wireless Intelligent Sensor Networks (WISeNet)","DGE","IGERT FULL PROPOSALS","09/01/2011","08/13/2015","Silvia Ferrari","NC","Duke University","Continuing Grant","Laura Regassa","08/31/2018","$3,126,326.00","Ronald Parr, Pankaj Agarwal, Gabriel Katul, Silvia Ferrari, John Albertson","ferrari@cornell.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","EHR","1335","110E, 1335, 9179, SMET","$0.00","This Integrative Graduate Education and Research Traineeship (IGERT) award supports the development of an interdisciplinary graduate training program in Wireless Intelligent Sensor Networks (WISeNet) at Duke University.  Recent advancements in wireless sensors are transforming many areas of science and engineering, spanning ecology, geosciences, robotics, and artificial intelligence.  Intellectual Merit: This program provides students with an interdisciplinary training that will prepare them to conduct research on novel intelligent methods and algorithms for wireless sensor networks.  The thematic basis is the development of intelligent sensors that process, store, and learn from data so as to improve their ability to gather information over time. <br/><br/>Broader impacts include unprecedented observation of environmental and ecological processes, and more effective and reliable use of sensors for defense and national security.  In collaboration with an Advisory Board of external and international collaborators, WISeNet faculty and students will rapidly transfer research findings into the curriculum, and develop integrated computational tools that directly support the WISeNet research goals.  The program will partner with Duke?s REU program and Graduate School to enhance diversity.  Students will receive a WISeNet graduate certificate that includes required laboratory and field experiments; new cross-disciplinary courses; and simulation, visualization, and virtual reality projects.  The program will develop a common research and educational experience for U.S. Ph.D. scientists and engineers working in the areas of sensor networks, environmental modeling and prediction, and computational intelligence, and provide students with an academic environment and placement opportunities where they can flourish at a global scale.<br/><br/>IGERT is an NSF-wide program intended to meet the challenges of educating U.S. Ph.D. scientists and engineers with the interdisciplinary background, deep knowledge in a chosen discipline, and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing innovative new models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries."
"1041707","Spatial Intelligence and Learning Center (SILC)","SMA","Geography and Spatial Sciences, SCIENCE OF LEARN CTRS- CENTERS, REAL, Science Across Virtual Instits","10/01/2011","09/15/2017","Nora Newcombe","PA","Temple University","Cooperative Agreement","Soo-Siang Lim","09/30/2018","$18,306,816.00","Susan Goldin-Meadow, Susan Levine, Dedre Gentner, Larry Hedges","newcombe@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","SBE","1352, 7278, 7625, 8077","5912, 5936, 5946, 5948, 7278, 8058, 9177","$0.00","The Spatial Intelligence and Learning Center (SILC) was established in the fall of 2006 as one of three second-cohort Science of Learning Centers. SILC's purpose is to develop the new science of spatial learning and to use this knowledge to transform STEM educational practice. Spatial learning is the acquisition of spatial knowledge and skills, and the use of spatial knowledge and skills to facilitate learning in both spatial and non-spatial domains. It provides the foundation for a wide range of reasoning skills in STEM-based activities, from solving mathematical problems to engineering new products to understanding graphical depictions of complex systems. Previous research shows that spatial skills are a strong predictor of entry into STEM disciplines in college and into STEM careers, that substantial improvement of spatial learning is possible, and that this improvement matters to STEM success. SILC has brought together researchers from multiple lines of work on spatial cognition and education and from a variety of traditional disciplines (e. g., cognitive science, psychology, artificial intelligence, linguistics, education, STEM disciplines), integrating them to achieve new insights. SILC researchers are developing a set of powerful tools for spatial learning, honing them into effective, deployable educational techniques and practices for STEM learning, including advanced technology (e.g., intelligent educational software), effective curriculum units (e.g., in elementary school mathematics), engaging activities (e.g., in children's museums), and spatial assessment instruments (e.g., testing children's spatial skills, testing adults' STEM-relevant spatial skills). Several of the insights, tools and products from SILC's initial funding period already hold transformational potential for spatial learning. Research and translational activities in the second and last funding period, from 2011-2016, will continue the investment in the science of spatial learning, in order to allow the fulfillment of this promise."
"1124651","Collaborative Research:   CDI Type-1: A Computer Framework for Modeling Complex Pattern Formation","EF","CDI TYPE I","09/01/2011","09/12/2011","Michael Levin","MA","Tufts University","Standard Grant","James O. Deshler","08/31/2014","$380,000.00","","michael.levin@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","BIO","7750","7721, 7722","$0.00","The mechanisms living systems use to establish and maintain complex 3-dimensional shapes during embryonic development are poorly understood even though molecular and cell biologists have generated mountains of data about genes and their effects on organisms. Fundamental advances in controlling biological form are stymied by the difficulty of obtaining shape information through the analysis of gene networks such that it is currently difficult or impossible for scientists to generate testable models of shape based on experimental results from current biological research. These investigators will apply state-of-the-art computational science and artificial intelligence to create a novel suite of computational tools that will fundamentally integrate numerous areas of biology and engineering to promote research into the mechanisms used by organisms for establishing and maintaining their 3-dimensional shape. This ""Bioinformatics of Shape"" project will integrate experimental data, a new mathematical language, a system for storing and mining data, a modeling environment within which rule sets for regulatory mechanisms can be simulated on computers, and an artificial intelligence module that will help scientists discover and test novel ideas about how shape is generated through genetics. The benefits to society of this new kind of collaboration between computer scientists and biologists include the translation of molecular and cell biological data into a new level of understanding that could have implications for regenerative medicine, adaptive and self-repairing devices for robotics and other engineering applications. The work will provide unique training opportunities for students, establish a proof-of-principle for new educational tools at the boundary between artificial intelligence and biology, and facilitate data to knowledge production in a number of fields, such as developmental biology, evolutionary biology, and the engineering of complex adaptive systems."
"1124665","Collaborative Research: CDI Type-1: A Computer Framework for Modeling Complex Pattern Formation","EF","CDI TYPE I, EPSCoR Co-Funding","09/01/2011","09/12/2011","Jeffrey Habig","ID","Boise State University","Standard Grant","James O. Deshler","08/31/2015","$300,000.00","Timothy Andersen","jeffreyhabig@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","BIO","7750, 9150","7721, 7722, 9179","$0.00","The mechanisms living systems use to establish and maintain complex 3-dimensional shapes during embryonic development are poorly understood even though molecular and cell biologists have generated mountains of data about genes and their effects on organisms. Fundamental advances in controlling biological form are stymied by the difficulty of obtaining shape information through the analysis of gene networks such that it is currently difficult or impossible for scientists to generate testable models of shape based on experimental results from current biological research. These investigators will apply state-of-the-art computational science and artificial intelligence to create a novel suite of computational tools that will fundamentally integrate numerous areas of biology and engineering to promote research into the mechanisms used by organisms for establishing and maintaining their 3-dimensional shape. This ""Bioinformatics of Shape"" project will integrate experimental data, a new mathematical language, a system for storing and mining data, a modeling environment within which rule sets for regulatory mechanisms can be simulated on computers, and an artificial intelligence module that will help scientists discover and test novel ideas about how shape is generated through genetics. The benefits to society of this new kind of collaboration between computer scientists and biologists include the translation of molecular and cell biological data into a new level of understanding that could have implications for regenerative medicine, adaptive and self-repairing devices for robotics and other engineering applications. The work will provide unique training opportunities for students, establish a proof-of-principle for new educational tools at the boundary between artificial intelligence and biology, and facilitate data to knowledge production in a number of fields, such as developmental biology, evolutionary biology, and the engineering of complex adaptive systems."
"1107011","IJCAI 2011 Doctoral Consortium and International Experience","IIS","COLLABORATIVE RESEARCH, ROBUST INTELLIGENCE","02/01/2011","01/20/2011","Judith Goldsmith","KY","University of Kentucky Research Foundation","Standard Grant","Jie Yang","01/31/2012","$90,000.00","","goldsmit@cs.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7298, 7495","5914, 5979, 7484, 9150","$0.00","The project funds two related activities for US-based graduate students: (1) attendance at the doctoral consortium at the International Joint Conference on Artificial Intelligence (IJCAI), where students discuss their research with senior members of the community, network with students from around the world, and discuss issues related to ethics in computing research; (2) extended research visits to European research groups, where students are exposed to new research paradigms and approaches within artificial intelligence and robotics, and build bridges between the European host and their own research group for long term cooperation. <br/> <br/>The goals of the project are to give students a clearer idea of what ethical dilemmas they may encounter as computer scientists; meet other students and professionals; have the opportunity to present their work to others in related research subfields; receive feedback from a senior member of their research community.   Students who participate in the extended visits also do research with a new research group and/or begin a new research project; work with European colleagues; live, albeit briefly, in a foreign country; report their work at the doctoral consortium. <br/><br/>The outcomes of the project are assessed with pre- and post-surveys administered to the students and the European hosts. Participation in the project is expected to help graduate students become members of the international community of scholars,  and develop into the global scientists that are needed for the future. These cooperations will advance science and industry in all countries."
"1125978","AAAI/SIGART 2011 Doctoral Consortium","IIS","ROBUST INTELLIGENCE","07/01/2011","02/28/2011","Bradley Clement","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Edwina L. Rissland","06/30/2012","$20,340.00","Elizabeth Sklar","bclement@jpl.nasa.gov","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495","$0.00","This award supports participation of doctoral students in the Doctoral Consortium of the 25th AAAI Conference on Artificial Intelligence (AAAI-11) to be held on August 7-11, 2011 in San Francisco, CA. The Doctoral Consortium (DC) will be held on August 7-8. This award supports travel stipends for 13 US student participants and one-on-one mentoring lunches, a group dinner, and a poster session for all participants. The Doctoral Consortium will extend over two days and will include participant presentations, panel discussions, feedback to the participants from mentors, informal discussions (over lunch and during breaks), a group dinner for students and mentors, and a poster session. Each participant will give a 20-minute presentation that will be followed by 20 minutes of discussion led by an assigned mentor to provide feedback on the research and the presentation itself. To help the participants make the transition from being students to embarking on a research career, there will be a presentation on guidelines for developing a successful research program. In addition, there will be panel discussions addressing issues such as research strategies, job search, publishing, and establishing research funding."
"1049719","EAGER:  Self-Assembly of Complex Systems","CCF","BIO COMPUTING","01/01/2011","08/24/2010","Russell Deaton","AR","University of Arkansas","Standard Grant","Mitra Basu","01/31/2013","$200,000.00","","rjdeaton@memphis.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","7946","7916, 9150, 9218, HPCC","$0.00","Abstract for EAGER: Self-Assembly of Complex Systems <br/>Intellectual Merit:  <br/>Self-assembly is a model for how individual components arrange themselves through local interactions to form organized structures. It originated as a model for construction of nanotechnology. The theory of complex systems describes many phenomena in nature, from human intelligence and evolving communities of organisms to human social networks, economies, and cultures. In these systems, complexity emerges in ways that is not immediately obvious from an understanding of the component parts and the relationships between them. In this project, self-assembly will be investigated as a mechanism for creation of complex systems. Self-assembly can be shown to be equivalent to other models of complex systems. In addition, it can be programmed like a computer. A goal of this project is to develop efficient ways to program self-assembly to produce interesting complex systems, which have practical applications. As a beginning to this, we have developed a mapping of self-assembly onto graphs that enables us to use an efficient algorithm to determine the system that is constructed. Thus, self-assembly should be able to generate complex systems and to provide efficient and realistic simulation of those types of systems. In the project, the self-assembly algorithms will be applied to automatic content generation for games, in which the self-assembly automatically creates situations and non-player characters with which players of the game interact. The conjecture is that this will provide more dynamic and realistic game environments, and moreover, will be an interesting test-bed for investigation of the relationship between self-assembly and complex systems.<br/>Broader Impacts:  <br/>This research integrates ideas from chemistry, physics, biology, and computer science to relate self-assembly to complex systems, and to produce potentially transformative tools that will not only improve understanding of complex systems, but also form the basis for innovative complex systems in a variety of application domains. These include nanotechnology, artificial intelligence, art, literature, and computer games. There are many natural phenomena (i.e. human intelligence, living systems) for which traditional symbolic models of computation are only able to capture a part of their essential capabilities and characteristics.  Human language is an example. This research conceivably could result in software that is able to produce target systems that capture some of the capability, adaptability, and complexity that is observed in nature.  If successful, the project could result in a new paradigm for realistic and complex behavior through computer programs, and would potentially impact not only nanotechnology, but also applications that require automatic generation of realistic content. Moreover, our models of self-assembly can generate this content in tractable ways. In addition, under the direction of the investigator, graduate and undergraduate students will work together in a team on this project, and will be educated in the unique multidisciplinary approach that has been proposed."
"1058925","Collaborative Research: CI-ADDO-NEW: StarExec: Cross-Community Infrastructure for Logic Solving","CNS","SOFTWARE & HARDWARE FOUNDATION","09/01/2011","08/15/2011","Geoffrey Sutcliffe","FL","University of Miami","Standard Grant","Nina Amla","08/31/2016","$150,295.00","","geoff@cs.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","7798","7359, 7944","$0.00","Logic solvers are software programs that can solve complex logical<br/>formulas fully automatically. Problems in many areas of Computer<br/>Science, such as artificial intelligence, program analysis, security,<br/>hardware verification, and cyber-physical systems can be faithfully<br/>translated into logical formulas. Those formulas can then be solved<br/>fully automatically by logic solvers. Over the past two decades, at<br/>least ten different logic-solving communities have emerged, based on<br/>different logical languages and solving techniques. These communities<br/>have independently been building computing infrastructure to aid<br/>development and evaluation of their solvers: libraries of benchmark<br/>formulas, cluster-backed web services, annual competitions, and more.<br/>Such infrastructure also provides an important access point for<br/>users, who can find all the solvers at one site, or even run solvers<br/>on the infrastructure cluster to test their relative capabilities.<br/>The goal of this research is to build a single piece of shared<br/>computing infrastructure called StarExec, which will be used by many<br/>different logic solving communities. StarExec will provide improved<br/>services for established logic-solving communities, and lower the<br/>entry barrier for new and emerging communities.<br/><br/>The StarExec infrastructure will consist of a custom web service<br/>interfacing to a medium-sized compute cluster. This open-source<br/>service will allow multiple logic-solving communities to host<br/>benchmark libraries, run jobs comparing different solvers, and host<br/>competitions. StarExec will leverage economies of scale to provide<br/>more sophisticated services than is feasible for most individual<br/>logic-solving communities. A very important goal of StarExec is not<br/>just to collocate different logic-solving communities, but to unite<br/>them. To this end, the StarExec team will develop formal<br/>specifications of both the syntax and proof-theoretic semantics of<br/>different communities' logical languages. This will be done using a<br/>meta-language called LFSC (""Logical Framework with Side Conditions""),<br/>developed in previous NSF-funded research. Translation of formulas<br/>between compatible fragments of different logics will be implemented,<br/>which will will enable a greater degree of integration between solver<br/>communities than was previously possible. For example, it will be<br/>possible for solvers in one community to be run on benchmarks from<br/>another. This integration will also aid users of logic solvers, who<br/>will have a greater variety of options, all in a common framework, for<br/>solving their problems. The broader impact of the StarExec project<br/>will be to accelerate the development, adoption, and convergence of<br/>different logic-solving technologies. This will enable faster<br/>progress in nationally important application areas such as artificial<br/>intelligence, verification, security, and cyber-physical systems,<br/>which increasingly depend on high-performance logic solvers."
"1058748","Collaborative Research: CI-ADDO-NEW: StarExec: Cross-Community Infrastructure for Logic Solving","CNS","Information Technology Researc, CCRI-CISE Cmnty Rsrch Infrstrc, TRUSTWORTHY COMPUTING, Algorithmic Foundations, Software & Hardware Foundation","09/01/2011","08/31/2015","Aaron Stump","IA","University of Iowa","Standard Grant","Nina Amla","08/31/2017","$1,959,838.00","Cesare Tinelli","aaron-stump@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","1640, 7359, 7795, 7796, 7798","7359, 7795, 7918, 7934, 7944","$0.00","Logic solvers are software programs that can solve complex logical<br/>formulas fully automatically.  Problems in many areas of Computer<br/>Science, such as artificial intelligence, program analysis, security,<br/>hardware verification, and cyber-physical systems can be<br/>translated into logical formulas.  Those formulas can then be solved<br/>fully automatically by logic solvers. Over the past two decades, at<br/>least ten different logic-solving communities have emerged, based on<br/>different logical languages and solving techniques.  These communities<br/>have independently been building computing infrastructure to aid<br/>development and evaluation of their solvers: libraries of benchmark<br/>formulas, cluster-backed web services, annual competitions, and more.<br/>Such infrastructure also provides an important access point for<br/>users, who can find all the solvers at one site, or even run solvers<br/>on the infrastructure cluster to test their relative capabilities.<br/><br/>The goal of this research is to build a single piece of shared<br/>computing infrastructure called StarExec, which will be used by <br/>different logic solving communities.  StarExec will provide improved<br/>services for established logic-solving communities, and lower the<br/>entry barrier for new and emerging communities.<br/><br/>The StarExec infrastructure will consist of a custom web service<br/>interfacing to a medium-sized compute cluster.  This open-source<br/>service will allow multiple logic-solving communities to host<br/>benchmark libraries, run jobs comparing different solvers, and host<br/>competitions.  StarExec will leverage economies of scale to provide<br/>more sophisticated services than is feasible for most individual<br/>logic-solving communities.  A very important goal of StarExec is not<br/>just to collocate different logic-solving communities, but to unite<br/>them.  To this end, the StarExec team will develop formal<br/>specifications of both the syntax and proof-theoretic semantics of<br/>different communities' logical languages.  This will be done using a<br/>meta-language called LFSC (""Logical Framework with Side Conditions""),<br/>developed in previous NSF-funded research.  Translation of formulas<br/>between compatible fragments of different logics will be implemented,<br/>which will will enable a greater degree of integration between solver<br/>communities than was previously possible.  For example, it will be<br/>possible for solvers in one community to be run on benchmarks from<br/>another.  This integration will also aid users of logic solvers, who<br/>will have a greater variety of options, all in a common framework, for<br/>solving their problems.  The broader impact of the StarExec project<br/>will be to accelerate the development, adoption, and convergence of<br/>different logic-solving technologies.  This will enable faster<br/>progress in nationally important application areas such as artificial<br/>intelligence, verification, security, and cyber-physical systems,<br/>which increasingly depend on high-performance logic solvers."
"1216253","HCC: EAGER:  Authoring Game AIs by Demonstration for Real-Time Strategy Games","IIS","HCC-Human-Centered Computing","09/01/2011","01/31/2012","Ashwin Ram","CA","Palo Alto Research Center Incorporated","Standard Grant","William Bainbridge","12/31/2012","$142,921.00","","ashwin.ram@parc.com","3333 Coyote Hill Road","Palo Alto","CA","943041314","6508124070","CSE","7367","7367, 7916, 9251","$0.00","This research will explore novel ""authoring by demonstration"" techniques for real-time strategy (RTS) games. Creating rich artificial intelligence (AI) behavior sets for complex computer games requires significant engineering effort. Developers need to anticipate all imaginable circumstances that the AI may encounter within the game world. The resulting AI is often static and results in predictable behaviors, detracting from the player experience. In addition, it is difficult for average players to create AI behaviors, without significant expertise in both AI and scripting. Modeling human-like goals and behaviors required for multiplayer games with semi-autonomous avatars adds additional complexity. This potentially transformative project will develop novel learning techniques that allow users to create intelligent behaviors simply by demonstrating them. The research will be done within the domain of RTS games, as these domains pose significant challenges that must be tackled in order to scale up the learning techniques to real-world tasks.<br/><br/>Case-based planners, hierarchical task network planners, or industry-standard behavior-tree execution engines require a library of base behaviors or methods in order to generate complete plans, which traditionally are coded by hand. The project will investigate ways to automate the process of generating such behavior libraries based on novel methods for learning strategic plans from user demonstrations. The techniques will be evaluated in the context of a case-based planning system for RTS games. RTS games are complex and involve strategic decision-making, multi-agent coordination, real-time interaction, and partially-observable environments. These properties pose significant challenges to existing AI methods for planning and learning. This research will make fundamental scientific contributions to learning, case-based reasoning, and AI for real-time strategic domains, addressing key problems in goal recognition, plan learning, and authoring support. <br/><br/>This research will enable game designers and other non-programmers to create the behavior sets for RTS games without requiring programming knowledge. This capability has two main consequences: first, it allows game developers to create games with less effort, and second it will enable a new genre of games where players would be able to create their own AIs as part of the game play. Additionally, as RTS games are essentially domain-specific simulations, the research will support authoring of behavior sets for domains such as simulation environments for training, real-time robotic control, organizational modeling for business decision-making, or sophisticated market simulations for economics strategy or public policy. The educational impact of the project is twofold. First, the project will constitute an important advance towards easy authoring of training simulators for educational applications that require environment with complex AI behaviors. This will enable development of new educational technologies with simulators or virtual worlds. Second, the project will involve undergraduate and graduate students in all phases of the work."
"1135374","Supporting Students Attending the User Modeling, Adaptation and Personalization 2011 Conference","IIS","HCC-Human-Centered Computing","04/15/2011","04/14/2011","Peter Brusilovsky","PA","University of Pittsburgh","Standard Grant","Ephraim Glinert","03/31/2012","$15,396.00","","peterb@mail.sis.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7367","7484, 7556","$0.00","This is funding to support travel by 6-8 students currently enrolled in PhD programs in the United States to present their accepted papers and posters and/or to take part in the Doctoral Consortium at the 2011 International Conference on User Modeling, Adaptation, and Personalization (UMAP) to be held in Girona, Spain, on July 11-15, 2011.   UMAP, the premier user modeling conference in the world, was formed in 2009 as a merger of the long-running and successful biennial conference series on User Modeling (UM, 1986-2007) and the Adaptive Hypermedia and Adaptive Web-Based Systems (AH, 2000-2008); the former provided a forum where academic and industrial researchers from the many fields involved in user modeling research (artificial intelligence, human-computer interaction, education, linguistics, psychology, and information science) could exchange their complementary insights on user modeling issues, while the latter provided a forum for dissemination of adaptive technology for hypermedia and other web-based systems.  User modeling has been found to significantly enhance the effectiveness and usability of software systems in a variety of areas.  A user model is an explicit representation of properties of a particular user; a system that constructs and consults user models can adapt diverse aspects of its performance to individual users.  Applications for user modeling range from electronic commerce and intelligent learning environments to health care and assistive technologies.  Relevant platforms for user modeling include mobile and wearable systems, and smart environments, as well as individual desktop systems, groupware, adaptive hypermedia, and other web-based systems.   More information about the conference is available at http://www.umap2011.org. <br/><br/>The UMAP 2011 Doctoral Consortium will provide a unique opportunity for PhD students partway through their dissertation research to receive valuable feedback from top researchers in the field.  The event will be held in a special session that is open to all students, not only those accepted to present, as well as to regular conference attendees, and which is designed so as to provide a great educational opportunity for all attending students, a unique event where a community as a whole can engage in a discussion with students about emerging topics, expected rigor, evaluation approaches, etc.  Each student participant will be allotted 15 minutes in which to present his/her work (including a short demo if appropriate), with an additional 15 minutes allocated to questions and for discussion.  Both during the question/discussion period and in subsequent informal interactions, committee members and other conference participants will provide constructive comments on the student's work and attempt to address any aspects of the work on which s/he has requested advice.  Student papers will be published in the main proceedings of the conference.  <br/><br/>Broader Impacts:  Bringing young and creative researchers to UMAP 2011 will help advance an important and socially valuable research field.  NSF funding will significantly impact the careers of the next generation of User Modeling researchers, by enabling a number of them to take part in an important event they would otherwise have to miss.  The students will have an opportunity to gain wider exposure in the community for their innovative work, and to obtain feedback and guidance from senior members of the research community.  Participation will help foster a sense of community among these young researchers, by allowing them to create a social network both among themselves and with senior investigators at a critical stage in their professional development.  In allocating NSF funds to participants, the organizers will give preference to students who are placed at a disadvantage due to the conference's location in Europe; they will strive for diversity among the selected students across a number of dimensions (gender, racial, ethnic, disabilities, institutional, etc.), and they will also take special steps to promote participation from institutions with relatively large numbers of students from under-represented groups."
"1117956","RI: Small: Towards Practical Tractability in Constraint Processing","IIS","Robust Intelligence","08/01/2011","01/23/2015","Berthe Choueiry","NE","University of Nebraska-Lincoln","Standard Grant","Hector Munoz-Avila","07/31/2015","$435,564.00","","choueiry@cse.unl.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","7495","7495, 7923, 9150, 9251","$0.00","Many problems in artificial intelligence, engineering, and management can be advantageously modeled and solved as Constraint Satisfaction Problems, but scalability remains in practice a major obstacle to the successful deployment of Constraint Solvers. The goal of this project is to design algorithms and strategies that enable computers to overcome, in practice, the scalability barrier. To this end, the proposed research targets the two fundamental mechanisms in Constraint Processing that are the most promising for breaking the complexity barrier, namely, enforcing consistency and detecting and breaking symmetry. This project aims to design new algorithms for those two mechanisms and develop strategies for intertwining them. <br/><br/>The tractability of a problem is guaranteed by a relationship between a structural parameter of the problem and its level of consistency. This project aims to design algorithms that enforce the needed level of consistency without adversely modifying the structure of the problem. Symmetries can be detected and exploited to dramatically reduce the cost of problem solving.  This project aims to (1) design algorithms for detecting symmetries, and (2) develop approximation strategies for exploiting them without sacrificing the soundness and completeness of problem solving. A key observation is that algorithms for enforcing consistency and those for locally detecting symmetry are based on the same atomic operations. Furthermore, they seem to be effective under complementary operating conditions. This project further aims to design strategies for intertwining the operation of the two types of algorithms so that the application of the one type enables and facilitates that of the other type, in order to yield new opportunities to control the combinatorial explosion. The approach will be validated on applications of practical importance and extended to address similar combinatorial problems in other areas of Computer Science such as Databases and Software Engineering.<br/><br/>The proposed activities contribute to the progress of the research on two fundamental aspects of Constraint Processing. From a practical perspective, this research directly benefits many combinatorial problems of practical importance. From a scientific standpoint, this project will identify connections and build new bridges with other areas of Computer Science, such as Databases and Software Engineering. The insight gained from these investigations will be used to improve the scope and content of introductory and advanced courses on Constraint Processing.  The opportunities and research avenues will be heavily exploited to involve undergraduate students in research and to give them experience in using the project's insights on problems that they find engaging (e.g., Sudoku) and for understanding the operation of algorithms in Computer Science courses; the goal is to motivate students to conduct research in Constraint Processing and to transfer results from this field to other students and researchers in Computer Science, Mathematics, Engineering, to entice high-school students to study Computer Science, and in addition, to explain to the general public some of the fundamental mechanisms at the heart of complex problem solving."
"1110885","Advanced Study Institute on Global Healthcare Grand Challenges and Opportunities, Antalya, Turkey, July 15-30, 2011","CBET","Engineering of Biomed Systems","05/01/2011","04/18/2011","Metin Akay","TX","University of Houston","Standard Grant","Kaiming Ye","04/30/2013","$50,000.00","","makay58@gmail.com","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","ENG","5345","004E, 7237","$0.00","1110885, Akay<br/><br/>This award will fund the travel expenses of 10 graduate students and post-docs and 25 faculty to participate at the 2nd Advanced Study Institute on Global Healthcare Grand Challenges and Opportunities cosponsored by National Academy of Engineering (NAE), the department of Biomedical Engineering at the University of Houston and the Akdeniz University will be held on July 15-30, 2011, Antalya, Turkey.<br/><br/>The National Academy of Engineering announced the 14 Grand Challenges for Engineering at the annual meeting of the American Association for the Advancement of Science. The committee also praised biomedical and biological engineering as the research field to fulfill the promise of personalized medicine. Included among these 14 challenges were Reverse?Engineering the Brain, Engineer better Medicines and Advance Health Informatics.<br/><br/>An important way of exploiting such information would be through the development of methods that allow doctors to forecast the benefits and side effects of potential treatments or cures.<br/>""Reverse-Engineering"" the Brain, is an emerging discipline that helps us to understand how the brain works and treats several diseases. It furthermore helps us to develop computerized artificial intelligence. Advanced computer intelligence, in turn, should enable automated diagnosis and prescriptions for treatment. Computerized catalogs of health information will enhance the medical system's ability to track the spread of disease and analyze the comparative effectiveness of different approaches to prevention and therapy. Finally, engineering new medicines will help us fight the growing danger of attacks from novel disease-causing agents. For instance, certain deadly bacteria have repeatedly evolved new properties, conferring resistance against even the most powerful antibiotics. New viruses arise with the power to kill and spread more rapidly than disease-prevention systems are designed to counteract.<br/><br/>Intellectual Merits: The main objective of the Advanced Summer Institute on Global Healthcare<br/>-- Challenges and Opportunities is to highlight and discuss these emerging grand challenges, mainly focused on the latest advances in the areas of science, engineering, technology and medicine. The institute provides a unique environment to discuss the emerging research areas, challenges and opportunities which lead to very fruitful discussions.<br/><br/>Broader Impacts: It exposes the attendees with biology and medicine backgrounds to the latest developments in these emerging enabling technologies. It is also helpful to those with engineering and science background who are interested in doing research in bionanoscience and nanomedicine, neuroscience and engineering since the advanced institute provides exceptional insights into the fundamental challenges in biology and medici"
"1143734","EAGER:  Decision Support System for Reasoning with Preferences","CCF","Software & Hardware Foundation","08/01/2011","07/14/2011","Samik Basu","IA","Iowa State University","Standard Grant","Nina Amla","07/31/2013","$111,393.00","Robyn Lutz","sbasu@cs.iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7798","7916, 7944, 9150","$0.00","Automated decision support systems help users make informed and intelligent choices over a set of alternatives, taking into account user preferences and trade-offs among multiple system attributes. In the software engineering domain decision support systems are used to help in evaluating alternative design, technical and managerial choices in terms of quantitative preferences and trade-offs. Preferences over alternatives are evaluated either by directly soliciting from the stakeholders a measure of the perceived utility/value of each attribute, or by quantifying such utility/value based on past experience and expertise. In most practical settings, however, preferences over attributes cannot all be quantified. On the other hand, considering preferences only qualitatively (specifying them as simple relative orderings between alternatives) is also not practical. To overcome these limitations, the proposed research focuses on developing a new paradigm for decision support systems, where preferences are specified both in qualitative and quantitative terms.<br/><br/>The main thrust of this work will be to: (a) develop robust formalisms for representing and reasoning with quantitative and qualitative preferences in an unified fashion, (b) investigate application-domain specific extensions to the formalisms, and (c) identify implementation strategies for practical application of the decision support system as a preference analyzer. The anticipated results will help realize application-specific robust decision support systems in multiple domains, including product-line engineering, safety-critical system development, and goal-oriented requirements engineering, by enabling improved automated reasoning about preferences.  This work will contribute to research-based training of a postdoctoral scholar and a graduate student in techniques that cut across software engineering, formal methods and artificial intelligence. Research results will be disseminated through publications in journals and conferences."
"1142510","Collaborative Research: EAGER: Network for Science, Engineering, Arts and Design (NSEAD)","IIS","HCC-Human-Centered Computing","08/01/2011","07/14/2011","Carol LaFayette","TX","Texas A&M Research Foundation","Standard Grant","Kevin Crowston","07/31/2013","$190,050.00","","lurleen@viz.tamu.edu","400 Harvey Mitchell Parkway, S","College Station","TX","778454375","9798626777","CSE","7367","7367, 7916","$0.00","The Network to support Science, Engineering, Arts and Design (NSEAD) will support transformative research and pedagogy that are only possible through the combined expertise of diverse knowledge domains and disciplines.  For example, as physicists and engineers developed new imaging techniques, visual artists experimented with the new expressive potentials they enabled, often influencing development of the technologies. Visual artists and musicians have created and continue to create computer languages and algorithms while pushing technologies for composing and recording in fields of software engineering, artificial intelligence, graphics and visualization. Students who are involved in the arts have higher math, verbal, and composite SAT scores than students who are not involved in the arts. (Vaughn and Winner, 2000).   There is a growing movement by higher education academic institutions in the United States to integrate the Arts and STEM disciplines to educate the whole student while leveraging creative cognitive skills for solving complex problems in science and technology disciplines.  And finally, diverse ecosystem of academic programs in pre-K to gray formal and informal STEM learning; scientific research conferences; exhibitions, and cultural institution programs continues to emerge as new information technologies, creativity support and social networking tools become pervasive in our society. This project envisions a network that addresses fundamental challenges including the need to align academic pedagogies with 21st century thinking skills; to promote diversity of perspectives, approaches, and people in the creative information technology economy; and to benchmark best practices that create critical thinkers and leaders for the ever-changing technology-driven job market. The development of such a network will provide a platform to disseminate and generate public dialogue about the intellectual, cultural, and economic potential of intersections of science, technology and creativity. <br/><br/>NSEAD will be a platform to support the burgeoning research community of Computer Scientists, Engineers, Artists and Designers engaged in integrative research and pedagogy across these disciplines. NSEAD will provide a bridge for academic institutions, non-profit organizations, industry liaisons, and resource providers to collaborate, share best practices in research and pedagogy, and build stronger affinities. It will serve as a junction for elements such as: 1) research community development;  2) collaboration and project matchmaking opportunities; 3) skills expertise referrals; 4) inter-institutional collaborations; 5) forums to share best practices in pre-K to gray STEM learning and creative enrichment; and 6) strategies for network leadership and resource sustainability."
"1132362","Workshop: International Conference on Computational Creativity: Broadening Participation","IIS","HCC-Human-Centered Computing","05/01/2011","04/11/2011","Dan Ventura","UT","Brigham Young University","Standard Grant","Ephraim Glinert","04/30/2013","$20,000.00","","ventura@cs.byu.edu","A-285 ASB","Provo","UT","846021231","8014223360","CSE","7367","7556, 9150","$0.00","Computational creativity is a young, highly interdisciplinary field incorporating insights and results from cognitive science, artificial intelligence, design, human computing interaction, mathematics, art and linguistics among others.  As a result, there is still a significant communication gap amongst contributors to the field.  The International Conference on Computational Creativity seeks to bridge this gap by creating a unified forum for discussion and exchange of ideas, and the workshop session is designed to foster broad participation and act as a forum for new participants in the conversation.  The award will facilitate this participation by providing travel support for participants that would like to join the conversations who will not present peer reviewed papers, but who would be benefited by exposure to established computational creativity researchers.  In addition, it will further this cooperation by funding mentoring activities that pair established researchers with workshop participants."
"1142505","Collaborative Research: EAGER: Network for Science, Engineering, Arts and Design (NSEAD)","IIS","HCC-Human-Centered Computing","08/01/2011","07/14/2011","Gunalan Nadarajan","MD","Maryland Institute College of Art","Standard Grant","William Bainbridge","07/31/2012","$45,618.00","","guna@mica.edu","1300 Mount Royal Avenue","Baltimore","MD","212174134","3016699200","CSE","7367","7367, 7916","$0.00","The Network to support Science, Engineering, Arts and Design (NSEAD) will support transformative research and pedagogy that are only possible through the combined expertise of diverse knowledge domains and disciplines.  For example, as physicists and engineers developed new imaging techniques, visual artists experimented with the new expressive potentials they enabled, often influencing development of the technologies. Visual artists and musicians have created and continue to create computer languages and algorithms while pushing technologies for composing and recording in fields of software engineering, artificial intelligence, graphics and visualization. Students who are involved in the arts have higher math, verbal, and composite SAT scores than students who are not involved in the arts. (Vaughn and Winner, 2000).   There is a growing movement by higher education academic institutions in the United States to integrate the Arts and STEM disciplines to educate the whole student while leveraging creative cognitive skills for solving complex problems in science and technology disciplines.  And finally, diverse ecosystem of academic programs in pre-K to gray formal and informal STEM learning; scientific research conferences; exhibitions, and cultural institution programs continues to emerge as new information technologies, creativity support and social networking tools become pervasive in our society. This project envisions a network that addresses fundamental challenges including the need to align academic pedagogies with 21st century thinking skills; to promote diversity of perspectives, approaches, and people in the creative information technology economy; and to benchmark best practices that create critical thinkers and leaders for the ever-changing technology-driven job market. The development of such a network will provide a platform to disseminate and generate public dialogue about the intellectual, cultural, and economic potential of intersections of science, technology and creativity. <br/><br/>NSEAD will be a platform to support the burgeoning research community of Computer Scientists, Engineers, Artists and Designers engaged in integrative research and pedagogy across these disciplines. NSEAD will provide a bridge for academic institutions, non-profit organizations, industry liaisons, and resource providers to collaborate, share best practices in research and pedagogy, and build stronger affinities. It will serve as a junction for elements such as: 1) research community development;  2) collaboration and project matchmaking opportunities; 3) skills expertise referrals; 4) inter-institutional collaborations; 5) forums to share best practices in pre-K to gray STEM learning and creative enrichment; and 6) strategies for network leadership and resource sustainability."
"1142663","Collaborative Research: EAGER: Network for Science, Engineering, Arts and Design (NSEAD)","IIS","HCC-Human-Centered Computing","08/01/2011","07/14/2011","Carol Strohecker","NC","Winston-Salem State University","Standard Grant","Kevin Crowston","07/31/2012","$43,493.00","","cs@CenterforDesignInnovation.org","601 S Martin Luther King Jr Dr","Winston Salem","NC","271100003","3367503019","CSE","7367","7367, 7916","$0.00","The Network to support Science, Engineering, Arts and Design (NSEAD) will support transformative research and pedagogy that are only possible through the combined expertise of diverse knowledge domains and disciplines.  For example, as physicists and engineers developed new imaging techniques, visual artists experimented with the new expressive potentials they enabled, often influencing development of the technologies. Visual artists and musicians have created and continue to create computer languages and algorithms while pushing technologies for composing and recording in fields of software engineering, artificial intelligence, graphics and visualization. Students who are involved in the arts have higher math, verbal, and composite SAT scores than students who are not involved in the arts. (Vaughn and Winner, 2000).   There is a growing movement by higher education academic institutions in the United States to integrate the Arts and STEM disciplines to educate the whole student while leveraging creative cognitive skills for solving complex problems in science and technology disciplines.  And finally, diverse ecosystem of academic programs in pre-K to gray formal and informal STEM learning; scientific research conferences; exhibitions, and cultural institution programs continues to emerge as new information technologies, creativity support and social networking tools become pervasive in our society. This project envisions a network that addresses fundamental challenges including the need to align academic pedagogies with 21st century thinking skills; to promote diversity of perspectives, approaches, and people in the creative information technology economy; and to benchmark best practices that create critical thinkers and leaders for the ever-changing technology-driven job market. The development of such a network will provide a platform to disseminate and generate public dialogue about the intellectual, cultural, and economic potential of intersections of science, technology and creativity. <br/><br/>NSEAD will be a platform to support the burgeoning research community of Computer Scientists, Engineers, Artists and Designers engaged in integrative research and pedagogy across these disciplines. NSEAD will provide a bridge for academic institutions, non-profit organizations, industry liaisons, and resource providers to collaborate, share best practices in research and pedagogy, and build stronger affinities. It will serve as a junction for elements such as: 1) research community development;  2) collaboration and project matchmaking opportunities; 3) skills expertise referrals; 4) inter-institutional collaborations; 5) forums to share best practices in pre-K to gray STEM learning and creative enrichment; and 6) strategies for network leadership and resource sustainability."
"1143713","EAGER: Shared Visual Common Ground in Human-Robot Interaction for Small Unmanned Aerial Systems","IIS","HCC-Human-Centered Computing","08/01/2011","03/27/2012","Robin Murphy","TX","Texas A&M Engineering Experiment Station","Standard Grant","Ephraim Glinert","07/31/2014","$316,000.00","Bob McKee","robin.r.murphy@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7367","7367, 7916, 9251","$0.00","This project will create a computational theory of visual common ground, allowing users to give directives to a robot (or other team members) and receive confirmation or constraints through visual communication over a shared visual display. The motivating example is an urban search and rescue (US&R) professional tapping, sketching, and annotating on an iPad in order to direct a small unmanned aerial system (sUAS) without training. Previous work in human-robot interaction with common ground has been limited to natural language, but recent work has shown that having all team members see the robot's eye view in unmanned ground robots significantly improved performance and situation awareness.  The proposed work populate the computational theory using the Shared Roles Model to represent the inputs (directives, notations), outputs (display viewpoint, form, size, location, content, etc.),  and transformations (visual communication engine).  The computational theory will be prototyped, refined, and tested by US&R practitioners flying realistic sUAS missions at Texas A&M's Disaster City.<br/><br/>Intellectual merit: The project will create a computational theory of visual common ground that will enable two-way human-robot interaction using visual communication mechanisms such as tapping, sketching, and annotation on shared visual displays on mobile devices such as iPads, smartphones, and tablet PCs. The results will advance the fields of human-robot interaction, artificial intelligence, and cognitive science. <br/><br/>Broader impacts:  The results could revolutionize how people use mobile devices to interact with robots (and with each other) using naturalistic visual mechanisms, bypassing extensive training. The project will actively recruit women, Hispanics, and persons with disabilities to participate through REU programs. An open source visual communication toolkit for HRI researchers will be produced. The results will improve robots for public safety, remote medicine, and telecommuting, and could also immediately help save lives through incorporation into Texas Task Force 1."
"1111047","Using Gaze Cues to Build Partner Models for Collaborative Behavior","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2011","07/26/2012","Gregory Zelinsky","NY","SUNY at Stony Brook","Continuing grant","William Bainbridge","07/31/2015","$749,999.00","Susan Brennan, Dimitrios Samaras","gregory.zelinsky@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7953","7953","$0.00","The ability to interact remotely over the Internet is redefining the nature of collaboration.  Many collaborative activities require coordinating attention and action with another person moment-by-moment; without the benefit of being physically present with another person these sorts of collaborations are difficult to conduct efficiently.  This project explores using human eye gaze to create partner models for mediating time-critical collaborative activities.  A partner model is a dynamically learned description of what a partner is trying to do - for example, what someone may be looking for, or what they consider to be relevant within a task. <br/><br/>Intellectual Merit: Many tasks and events are implicit or poorly defined, requiring that partner models be learned from evidence unfolding as part of a person's ongoing behavior.  Eye trackers will be used to determine the task-relevant objects that a person chooses to look at (and not look at); through analysis of these gaze patterns and the properties of the objects, human and computer partners will learn a model of what this person is attempting to do.  Various tasks will be explored, such as searching for a new and/or ambiguous moving target specified only by incomplete semantic descriptions, or monitoring a complex dynamic environment for unusual events, defined by atypical target movements and relationships between people and objects.  The findings will advance the fields of human-computer interaction, psycholinguistics, artificial intelligence, object and event detection by humans and computers, and multimodal human communication.<br/><br/>Broader Impacts:  The results of the project will facilitate the development of new tools that can help people with their tasks, by, for example, finding and highlighting objects in a scene that match the viewer's goals and helping the viewer track moving targets.  The results will also lead to new tools for remote collaboration, with the goal being to make coordination at a distance as efficient as face-to-face interaction.  The tools and techniques from the project are expected to benefit a variety of applications, including the development of assistive technologies for people with communication impairments and the creation of better security screening procedures.  The project will provide training and research experiences for Stony Brook University's racially, ethnically, and economically diverse students, including women and others underrepresented in science and engineering."
"1129177","AAAI 2011 Robotics Program","IIS","HCC-Human-Centered Computing, Robust Intelligence","07/01/2011","03/29/2011","Andrea Thomaz","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Richard Voyles","06/30/2012","$20,000.00","","athomaz@ece.utexas.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7367, 7495","7367, 7495, 7556","$0.00","The 2011 AAAI Robotics Exhibition and Workshop (San Francisco, CA August 7-11, 2011) continues a focus on key research problems in manipulation and learning through challenges in: (1) humanoid robotics, (2) learning by demonstration, and (3) samll-scale manipulation (robot chess).  The teams selected for each challenge define research problems and repeatable experiments in areas that drive autonomous assistance in both military and domestic needs.  The workshop enhances the challenge goals by (1) creating awareness in the larger AI community of available software tools, and (2) crafts a roadmap for development platforms that are more accessible to the general computer science research community.<br/><br/>A significant number of hands-on exhibits complement the workshop discussions and panel.  Here, research teams showcase working demonstrations that support the challenge themes of learning, teaming and manipulation.  Exhibits are on display for 2 full days during the AAAI Conference, providing an excellent opportunity to engage a broad technical audience.  The exhibits are open to the general public to raise awareness of the state-of-the-art in robotics.  Students from local schools and summer camps visit.  Access to the exhibits provides an opportunity for students and leaders to learn how robotics and AI play important roles in society."
"1117684","III:  Small:  Compression-Aware Algorithms for Massive Datasets","IIS","Info Integration & Informatics","07/01/2011","07/17/2012","Gabriel Robins","VA","University of Virginia Main Campus","Continuing grant","Sylvia Spengler","06/30/2016","$499,984.00","Abhi Shelat","robins@cs.virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7364","7923","$0.00","As many application domains continue to generate data at exponentially increasing rates, much of the data that is gathered is stored in a compressed format. However, very few classic data processing algorithms have been updated to handle compressed data. This project aims to address this gap by developing (i) algorithms for massive data sets that can directly operate on compressed data; and (ii) compression schemes that are aware of the algorithms that would  operate on the data. <br/><br/>In many settings, algorithms that manipulate very large composite objects while interacting only with their succinct descriptions can substantially reduce the time and memory requirements relative to their counterparts that have to work with uncompressed representations of the same data. These performance gains are realized by leveraging highly repetitive or parametrically specified input structures, to enable algorithms to manipulate very large composite objects while interacting only with their compressed descriptions. Anticipated results of the project include new geometric algorithms that solve problems such as convex hull, Voronoi diagrams, nearest points and earth-mover distances when the inputs are in compressed format; new graph algorithms that compute minimum spanning trees, shortest paths, and network flows on compressed input graphs; and new compression-aware data structures that support efficient storing, querying and processing of compressed data. All the algorithmic contributions will be validated with experiments on real and synthetic massive data sets. The resulting algorithms are likely to find application in many different domains including networks, genomics, databases, computer graphics, artificial intelligence, geographic information systems, integrated circuit design, and computer-aided engineering. <br/><br/>Broader Impacts: Compression-aware data processing algorithms and algorithm-aware data compression schemes have applications across a wide range of tasks that involve processing of massive data sets consisting of large data objects (e.g., images, sequences, graphs). The formulations, algorithms, codes, and theories <br/>that will be developed and disseminated by this project are likely to contribute to the development of efficient and practical algorithms and data structures that could impact the way in which organizations collect, store, process, such data. The project offers enhanced research based training opportunities for students in an area of considerable theoretical as well as practical significance. Additional information about the project can be found at: http://www.cs.virginia.edu/robins"
"1117801","HCC: Small: Manipulating Perceptions of Robot Agency","IIS","HCC-Human-Centered Computing","09/01/2011","08/05/2011","Brian Scassellati","CT","Yale University","Standard Grant","Ephraim Glinert","08/31/2014","$500,000.00","","brian.scassellati@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7367","7367, 7923","$0.00","Robots are increasingly becoming a part of daily human interactions: They vacuum floors, deliver medicine in hospitals, and provide company for elderly and disabled individuals. This project examines one aspect of people's interactions with these robots: how intentional and self-reflective the robot seems to be. Because the perceived agency of a robot affects many dimensions of people's interactions with that robot, it is important to understand how features of robot design, such as its behavior and cognitive abilities, affect perceptions of agency.  This question is addressed through a series of laboratory experiments that manipulate behavior and cognitive abilities and measure the degree of agency attributed to socially interactive robots.<br/><br/>Intellectual merit: The project will lead to new measures of perceived robot agency and new knowledge about how people collaborate with robots. The results will inform how engineers construct robots, how artificial intelligence researchers conceptualize behavioral architectures, and how designers craft interactions to produce robots that engage people in simple ways.<br/><br/>Broader impacts: The project will provide a new quantitative measurement of agency that can be used in human-robot interaction and related disciplines and new information that can inform how agency is modeled in the design of human-robot interactions, especially in situations where recognition of agency is a primary factor.  The outcomes will be used to improve socially assistive robotics for children with social deficits. The project will also enhance interdisciplinary research offerings for graduate and undergraduate students at the investigators' institution."
"1120558","RUI: MPS-BIO: Collaborative Research: Design and Construction of Second-Generation Bacterial Computers","MCB","Cross-BIO Activities, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY","09/15/2011","09/09/2011","Todd Eckdahl","MO","Missouri Western State University","Standard Grant","Susanne von Bodman","08/31/2013","$200,001.00","Jeffrey Poet","eckdahl@missouriwestern.edu","4525 Downs Drive","St. Joseph","MO","645072246","8162714364","BIO","7275, 7334, 7454","1228, 8007, 9178, 9183, 9229","$0.00","Bacteria can be modified, through synthetic biology approaches, to function as biological computers. They can be programmed with DNA sequences, interact naturally with other cells and the environment, power themselves from food sources, and respond to directed evolution. Biological computing is not yet widespread, and this project will develop second-generation bacterial computers capable of addressing mathematical problems in graph theory, probability, theory of computation, and artificial intelligence.  The project will identify problems that are scalable and generalizable, develop a system of classifying the bacterial computational complexity of mathematical problems, and construct mathematical models and computer simulations to assess the design feasibility of new bacterial computer programming languages. Proven and novel bacterial computing mechanisms will be used to program bacterial computers to solve the selected mathematical problems. <br/><br/>Broader Impacts.  This project will provide multidisciplinary education and research training for undergraduate students (including members of groups under-represented in science) at the interface of mathematics, synthetic biology and computer science. Students will pose research questions, develop testable hypotheses, collect and analyze data, and communicate results through publications and at scientific meetings. <br/><br/>This project is supported jointly by the Networks and Regulation and Mathematical Biology Programs."
"0938402","Individual:  Fueling the STEM Pipeline by Mentoring Across the Age Span","DUE","PAESMEM Pres Awrds Excell Ment","08/15/2011","05/15/2015","Maja Mataric","CA","University of Southern California","Standard Grant","Martha James","07/31/2016","$10,000.00","","mataric@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","EHR","1593","9178, SMET","$0.00","Dr. Maja Mataric received her Ph.D. in Computer Science and Artificial Intelligence from MIT in 1994. She is Professor of Computer Science and  Neuroscience, and Pediatrics Director of the Center for Robotics and Embedded Systems, Co-Director of the Robotics Research Lab, and Senior Associate Dean for Research at the Viterbi School of Engineering at the University of Southern California. Dr. Mataric has established a mentoring philosophy that encompasses the need to encourage students continuously as they progress in their educations. Her stated philosophy is that mentoring must be viewed as a pipeline process, in which role models and training opportunities are provided from as early as possible and the pipeline is continually fueled. The pipeline mentoring program is based on the established literature about critical times for capturing interest and recruiting women and underrepresented students into STEM areas. To build the pipeline, the comprehensive spectrum of mentoring activities performed to date span: K-12 STEM outreach and teacher training, undergraduate student mentoring toward placement in graduate programs, graduate and postdoctoral student mentoring and placement in academic positions, peer mentoring of female faculty, mentoring of junior faculty in engineering, and developing a culture of mentoring at USC. Dr. Mataric's mentoring programs have resulted in new courses and programs at the K-12 level that have trained generations of teachers and students and continue to recruit generations of inner-city at-risk students into STEM topics. Approaches have yielded the recruitment of underrepresented groups at each level, from all-girls elementary school teams winning robotics contests at the state level, to outstanding placement of Ph.D. students and postdoctoral fellows in academic positions, to outstanding outcomes in female faculty mentoring leading toward nationally competitive research grants, to developing novel mentoring programs with impact across the entire university. The programs have resulted in the placement of Ph.D. students in minority serving universities and of Ph.D. students from underrepresented groups (women and African Americans) in top research universities in the US and world-wide. The programs have also established a role-modeling and networking pipeline between the K-12 inner city institutions, USC undergraduates, Ph.D. students, and faculty. Dr. Mataric's mentoring programs have effectively aided in the recruitment and retention of women faculty in engineering at USC and have had a significant impact on institutional-wide cultural change."
"1131404","GOALI: Study of Advance Rate of Hard Rock Tunnel Boring Machines (TBMs) and the Impacts of Ground Conditions and Machine Specifications","CMMI","GRANT OPP FOR ACAD LIA W/INDUS, GEOMECHANICS & GEOMATERIALS","09/01/2011","08/15/2011","Jamal Rostami","PA","Pennsylvania State Univ University Park","Standard Grant","Richard Fragaszy","08/31/2015","$200,000.00","W. D. Rogstad, Lok Home","rostami@mines.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","1504, 1634","036E, 037E, 1057, 1504, 172E, CVIS","$0.00","The objective of this Grant Opportunity for Academic Liaison with Industry (GOALI) project is to develop a model for estimation of Tunnel Boring Machine (TBM) utilization and advance rate based on the machine specifications and ground conditions.  Accurate estimation of the penetration, utilization, and daily advance rate of hard rock TBM has been a challenge due to the complexity of the machine rock interaction and the influence of operational and management issues on machine production.  Having a reasonably accurate estimate is crucial in justification of the project, as well as planning and cost estimation.  The errors in the estimates have caused many technical and legal problems and have engaged much of resources in construction claims.  A quick review of the literature shows that much of research has focused on the estimation of rate of penetration (ROP) of certain machine types in a given geology.  Yet, the estimation of machine utilization and analysis of downtime has not been treated in a systematic way, although downtime is the largest proportion of time spent in any tunneling operation involving TBMs.  The limited amount of research in this area is outdated and does not reflect the advances in machine manufacturing techniques.   The controlling parameters for TBM utilization and advance rate include the geological setting, machine type and specifications, operational parameters, the machine backup system and auxiliary equipment, and finally site management.  This study will look at the case histories of recent TBM application to evaluate the impact of various geological parameters on machine performance.  An existing TBM field performance database will be updated with additional data for statistical analysis and seeking new relationships between controlling parameters.  The study will also develop activity based models of the tunneling operation and will establish correlation between time required to perform each activity and ground conditions to allow for more accurate estimate of machine utilization and advance rate.  Also, the feasibility of using artificial intelligence methods for estimation of the machine performance will be evaluated based on the available data to complement the proposed models.  This project will be performed with the participation and contributions by the Robbins Company, the largest manufacturer of tunnel boring machines in the US (and one of the largest in the world), and Frontier Kemper, a leading tunneling contractor in North America.  Both companies will assist the research project by providing field data and expertise to expand the database of TBM field performance and realistic activity time models.  The proposed work will improve the accuracy of the existing performance prediction models and offer means to achieve more efficient operation.  <br/><br/>With a more reliable estimation of TBM advance rate, more accurate cost estimation for hard rock tunneling can be achieved and many of unnecessary construction claims can be avoided.  This study can also lead to an objective evaluation of machine backup system and impact of various components on machine utilization, which can lead to a systematic evaluation of ground conditions for selection of proper machine and backup system to avoid long delays in tunneling operation.  Overall, the result of this study leads to more efficient and cost effective tunneling with reduced delays, improved safety, and prospects for better risk management for tunnel construction using TBM.  The main beneficiary of the study will be the general public through the cost savings on the construction of critically needed civil infrastructure upgrades such as water, sewer, rail, subway, and road tunnels."
"1133316","The Development of a Hexacopter With a Dexterous Manipulator","IIP","IUCRC-Indust-Univ Coop Res Ctr","08/15/2011","05/10/2012","Carlotta Berry","IN","Rose-Hulman Institute of Technology","Standard Grant","Rathindra DasGupta","07/31/2013","$0.00","","berry123@rose-hulman.edu","5500 Wabash Avenue","Terre Haute","IN","478033920","8128778972","ENG","5761","5761, 8042","$0.00","IIP 1133316 <br/>Rose-Hulman Institute of Technology; Carlotta Berry <br/><br/>The purpose of this unsolicited proposal is to create a collaborative effort between Rose-Hulman Institute of Technology (RHIT) and the NSF Industry/University Cooperative Research Center for Safety, Security and Rescue Research Center (SSR-RC) at the University of Denver (DU). This work seeks to capitalize on the faculty, undergraduate and graduate student resources at this primarily undergraduate engineering institution, RHIT, and the research equipment and resources available at DU. The SSR-RC provides integrative robotics and artificial intelligence solutions in robotics for activities conducted by emergency response personnel. <br/><br/>The intellectual merits of this work lie in the fact that the utility of an unmanned air vehicle (UAV) is extensive in military and civil applications. The design of a robust UAV with vertical take-off and landing will allow for surveillance and maneuverability in narrow spaces. In addition, UAVs rarely include a manipulator; thus, the inclusion of one through this work will allow for more flexibility with respect to the UAV interacting with its environment. The funding of this project will illustrate a practical integration of research and education at two very different types of institutions. <br/><br/>The broader impacts of the proposed work are primarily related to the inclusion of faculty, undergraduate and graduate students at RHIT into the high-level research available through the laboratory at DU. The relationship is expected to enhance the infrastructure for research and education at both of these schools. If the hexacopter proves to be a viable resource, then it will affect populations far beyond academia. In addition, since the students will need to consider the novice end user in their design, it will require them to have an awareness of societal needs and capabilities."
"1120578","RUI: MPS-BIO: Collaborative Research: Design and Construction of Second-Generation Bacterial Computers","MCB","Cross-BIO Activities, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY, Systems and Synthetic Biology","09/15/2011","02/27/2013","A. Malcolm Campbell","NC","Davidson College","Standard Grant","Susanne von Bodman","08/31/2014","$206,559.00","Laurie Heyer","macampbell@davidson.edu","Box 7149","Davidson","NC","280357149","7048942644","BIO","7275, 7334, 7454, 8011","1228, 7744, 8007, 9178, 9183, 9229","$0.00","Bacteria can be modified, through synthetic biology approaches, to function as biological computers. They can be programmed with DNA sequences, interact naturally with other cells and the environment, power themselves from food sources, and respond to directed evolution. Biological computing is not yet widespread, and this project will develop second-generation bacterial computers capable of addressing mathematical problems in graph theory, probability, theory of computation, and artificial intelligence.  The project will identify problems that are scalable and generalizable, develop a system of classifying the bacterial computational complexity of mathematical problems, and construct mathematical models and computer simulations to assess the design feasibility of new bacterial computer programming languages. Proven and novel bacterial computing mechanisms will be used to program bacterial computers to solve the selected mathematical problems. <br/><br/>Broader Impacts.  This project will provide multidisciplinary education and research training for undergraduate students (including members of groups under-represented in science) at the interface of mathematics, synthetic biology and computer science. Students will pose research questions, develop testable hypotheses, collect and analyze data, and communicate results through publications and at scientific meetings. <br/><br/>This project is supported jointly by the Networks and Regulation and Mathematical Biology Programs."
"1158862","RII: Enhancing Alabama's Research Capacity in Nano/Bio Science and Sensors","EPS","EPSCoR Research Infrastructure","09/01/2011","07/28/2016","Mahesh Hosur","AL","Tuskegee University","Cooperative Agreement","Uma Venkateswaran","08/31/2017","$11,332,243.00","","mahesh.hosur@tamuk.edu","1200 W Montgomery Road","Tuskegee Institute","AL","360881923","3347278233","O/D","7217","0000, 1769, 7217, 9150, OTHR","$0.00","Abstract<br/><br/>Proposal Number:  EPS-0814103<br/><br/>Proposal Title: RII: Enhancing Alabama?s Research Capacity in Nano/Bio Science and Sensors  <br/><br/>Institution:   Alabama A&M University<br/><br/>Goals: This Research Infrastructure Improvement award will facilitate the creation of a statewide partnership among Alabama core research institutions to enhance R&D competitiveness in the emerging area of nano/bio science and molecular sensors. This partnership is designed to foster collaborative research, and to stimulate multidisciplinary education through four centers. Each center is designed to support and train new faculty, staff, and students in key nano- and biotechnology innovation areas aligned with the state?s economic development priorities outlined in the governor?s ?Plan 2010?.<br/><br/>Project Major Foci: The Tuskegee University led center will develop and characterize new  biodegradable nanostructured materials; the Auburn University led center will use organismal models to identify mechanisms of adaptation to natural and man-made environmental challenges; the University of Alabama in Birmingham led center will develop new optical and molecular sensing technologies; and the Alabama A&M University led center will use nanopatterning and nanofabrication techniques to control structure-property relationships in order to tailor materials properties for specific applications. An additional role of this center will be the integration of the findings of each center to catalyze innovation based economic development. <br/><br/>Intellectual Merit<br/>The centers will ensure reciprocal transfer of information and technology between the life sciences and engineering to catalyze novel research in both fields. Biological discovery will elucidate structural and functional principles in living systems that can serve as templates for pioneering the design and synthesis of nanomaterials. Likewise, advances in nanoscale detection, quantification, and nanoengineering will open new frontiers in the life sciences. <br/><br/>Broader Impacts<br/>The centers will establish statewide infrastructure that will foster collaboration, build partnerships, develop future STEM-enabled workforce, improve scientific literacy, and develop new economic opportunities. The centers research will likely have a major impact on the environment, homeland security, and industrial process control.<br/><br/>"
"1136452","Workshop on Mechanical Engineering Design Knowledge Modeling; Washington, DC; 28 August 2011","CMMI","ESD-Eng & Systems Design","08/01/2011","07/20/2011","David Rosen","GA","Georgia Tech Research Corporation","Standard Grant","Paul Collopy","07/31/2013","$47,314.00","Joshua Summers","david.rosen@me.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1464","067E, 068E, 073E, 7556","$0.00","The objective of this award will be to conduct a one day workshop with the aim of clearly defining a research program for capturing, representing, and modeling mechanical engineering design knowledge that the research, academic, and industrial community can pursue in both the near and long term.  A formalized language of mechanical engineering is envisioned that will allow engineers to communicate more precisely between each other and with computers. The resulting formalized ME design knowledge and language will be encoded and implemented in an open-knowledge repository.  Such a knowledge repository could capture the disparate and representationally diverse ME knowledge of typical undergraduate students providing numerous benefits, including: 1) a standard for ME knowledge; 2) a knowledge base to support engineering design; 3) a knowledge base for computer-aided tutoring systems; and 4) a key component of the ME research and education infrastructure. A convergence of artificial intelligence, engineering informatics, description logics, and the semantic web with mechanical engineering design research will be the enabling factor to realize the vision of this research proposal and workshop.   <br/><br/>If successful, the work may impact US economic and national security since a ME knowledge repository could increase the value of US graduates who have expertise beyond just the application of engineering science principles. Successful definition of a research agenda toward an ME knowledge repository has the potential for transformational outcomes. Increased awareness in the research community of high-impact research areas will increase interest in knowledge modeling. Establishing an ME knowledge repository as part of the educational infrastructure enables myriad methods and tools. The knowledge repository also has the potential to be a valuable resource for ME education research, as well as ME research in general. The networking function of the workshop will establish new relationships between individuals spanning widely across the field. This opportunity will provide a stimulus for new interactions and collaboration to advance the state of the art in engineering knowledge modeling. An understanding should emerge of the potential for this work to impact critical areas of engineering education, engineering information modeling, knowledge modeling, and the semantic web.  The results will be disseminated through presentations, conference papers, and journal articles."
"1056046","CAREER: Oxidative Stress - Responsive Shape Memory Vascular Patch","CBET","Engineering of Biomed Systems","09/01/2011","04/11/2012","Hak-Joon Sung","TN","Vanderbilt University","Standard Grant","Michele Grimm","08/31/2016","$455,999.00","","hj72sung@gmail.com","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","ENG","5345","004E, 1045, 1187, 137E, 7218, 7237, 9102, 9150","$0.00","Sung<br/>1056046<br/><br/>The PI will program two ""smart"" functions (i.e., oxidative stress-responsive and shape memory functions) into biomaterials by employing shape memory polymers crosslinked with peptide sequences that degrade in response to injury-mediated overproduction of reactive oxygen species. The ""smart"" functions of these biomaterials will be evaluated as a form of an injectable vascular patch whose shape, size, and thickness can be tuned to custom-fit even small blood vessels. Stimuli-sensitive materials change their structure and shape in response to changes in environment, such as heat, light, moisture or magnetic field. In the proposed project, oxidative stress is selected as an external stimulus because overproduction of reactive oxygen species is a universal mark of damaged organs, tissues, and cells. Reactive oxygen species-degradable peptides that can crosslink polymers when making patch scaffolds will be used to generate the oxidative stress-responsive function. Shape memory polymers in a combinatorial format, x% crosslinkable unit -co-y%non-crosslinkable unit, where x and y% indicate the molar ratio, will be used to tune the ability to memorize  temporary shapes and regain their original shape after exposure to body temperature (37 C).  The basic polymer type proposed for this study has shown excellent vascular compatibility in previous studies. Furthermore, the polymer-peptide complex will be fabricated asvascular patch scaffolds aimed at repairing small blood vessels (e.g., cerebral hemorrhage and stroke). The patch scaffold surface will be coated with antibodies against vascular cell adhesion molecule-1 to generate a ""suture-free sealing effect"" by mimicking inflammatory cell adhesion onto injured endothelium.  The ""smart"" functions and their subsequent effects on vascular healing will be evaluated in a bioreactor system that mimics the vascular environment. This is a very challenging research task due to the interdisciplinary nature of the conceptual and technical approaches. <br/><br/>Intellectual Merit: The proposed research will advance the state of the art in methods and techniques for applications of ""smart"" biomaterials to develop therapeutic inventions. Some of the innovative expected results include: 1) providing a stepping stone to develop the next generation of biomaterials that enable  artificial intelligence-like work flow (i.e., navigating, sensing, and fixing); 2) incorporating biological molecules into a shape memory material function; 3) advancing ""smart"" material functions to cope with complex biological signaling; 4) a new therapeutic approach to regeneration of injured small blood vessels and a therapy of further pathogenesis, such as cerebral hemorrhage and stroke; and 5) generating a new tool box for minimally-invasive surgery and for design of scaffolds with customizable  size, shape, and thickness. The proposed research will have a far reaching impact in terms of real applications of the research developed, as well as the broad spectrum of research areas. Achieving the goals of this project requires a deep understanding of material design and fabrication, as well as biomedical applications. <br/><br/>Broader Impact: Direct outcomes of this research will influence a large cross-section of the engineering and biomedical community and will stimulate education towards multidisciplinary subjects. Successful translation of the research to real world applications has the potential to revolutionize biomaterials and the regenerative medicine industry. Broader impacts also result from a range of education and dissemination activities, including 1) integrating research projects with coursework, outreach, and training through the existing courses and programs; 2) teaching abroad to expand outreach; and 3) creating a web-based vibrant youth community comprised of students in the  US and S. Korea. Though the focus is biomaterials and tissue engineering, the basic elements will appeal to students from all areas of science and engineering. The PI will use established assessment tools to evaluate and adjust his pedagogical methods in these various integrative learning environments. He will also disseminate his new pedagogical methods for use by other instructors."
"1068026","I/UCRC CGI: Collaborative Research - I/UCRC for Identification Technology Research","CNS","GOALI-Grnt Opp Acad Lia wIndus, IUCRC-Indust-Univ Coop Res Ctr, ","03/15/2011","05/27/2015","Judee Burgoon","AZ","University of Arizona","Continuing Grant","Dmitri Perkins","02/28/2017","$391,141.00","Jay Nunamaker","jburgoon@cmi.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","1504, 5761, L528","0000, 1049, 116E, 122E, 170E, 5761, 8038, 8039, 8046, 9178, 9251, OTHR, SMET","$0.00","The purpose of this proposal is to start a new I/UCRC ""Identification Technology Research (CITeR)"" with a focus on technologies of human measurement, identity, and intent that rest on firm pillars of trust, privacy, security, and reliability. The lead of the proposed Center will be Clarkson (CU) with site locations at the West Virginia University (WVU) and the University of Arizona (UA). <br/><br/>Technologies to measure, monitor, identify humans and human intent are needed for a broad range of commercial and security applications. The use of technology to both improve performance (e.g. as minimizing false alarms for a given desired positive detection rate) or improving user interface (e.g. speeding lines through airports) will have a strong benefit to society. The proposed center will seek advanced theoretical and analytical techniques for developing algorithms and assessing performance of tools and systems that for human and intent identification. Such tools are needed for a variety of applications, and the current state of the art provides significant room for advancement. <br/><br/>The proposed center builds upon the past success of an existing center that has been in operation since 2001. West Virginia University and University of Arizona have been the key players in CITeR since its inception, while others such as Michigan State have made notable contributions in areas such as image processing. In this proposal, Clarkson University as the lead institution adds complementary Center capabilities for measurements and signal processing to identity humans and human intent, and expands the center in an important way. <br/><br/>The proposed research plan represents an appropriate set of projects for comprehensively addressing needs in this area. The investigators are well-qualified to carry out the research, and many have a significant, well-respected research track record in their fields. <br/><br/>CITeR will focus on research, education, and technology dissemination. CITeR's leadership with Clarkson to develop the Diversity 2050 Initiative program is an active program to identify and address educational needs of next-generation professionals in the identification technology arena, and is likely to lead to a broad-reaching and positive impact to higher education in these areas. The proposal lays out a clear plan for engaging the other CiTER sites in this effort. The proposed and re-organized CITeR will extend its research core while building on the solid foundation of the past. The proposed program builds on an existing center that has had great success since its inception in 2001, with more than 80 projects that have involved 22 faculty and 130 students."
"1102382","Robust Nonlinear Adaptive Control of Unmanned Aerial Vehicles Using Neural Networks","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2011","08/10/2011","Subodh Bhandari","CA","Cal Poly Pomona Foundation, Inc.","Standard Grant","Radhakisan Baheti","07/31/2016","$360,000.00","Amar Raheja, Norali Pernalete, Salomon Oldak, Donald Edberg","sbhandari@cpp.edu","3801 West Temple, Bldg 55","Pomona","CA","917682557","9098692948","ENG","7607","1653","$0.00","The objective of this research is to develop and experimentally validate robust nonlinear adaptive controllers for unmanned aerial vehicles (UAVs) using neural networks. The approach is to use both pre-trained and on-line neural networks based on multilayer perceptrons. The concept of dynamic nonlinear damping will be applied to modify the adaptation law to increase the robustness in the presence of unmodeled dynamics. A stable weight adjustment rule for the on-line neural network will be derived using a Lyapunov stability theorem. <br/><br/>Intellectual Merit<br/>This research, through the use of adaptive controllers, will increase the robustness of UAV operations. The lack of robustness has limited the use of UAVs, despite their potential to replace manned vehicles for many types of missions. Also, as the level of UAV autonomy increases, it is important to design controllers that can work in the entire flight envelope. This research will involve an interdisciplinary team of faculty members from the Engineering and Computer Science Departments at Cal Poly Pomona. An existing fleet of UAVs and complimentary resources will be used.<br/> <br/>Broader Impact<br/>Increased UAV robustness will have potential to extend and increase the use of UAVs for rescue missions, border patrol, scientific research, etc., which will have a significant positive impact on society. This research will involve a number of undergraduate and underrepresented students. This research will also be helpful in curriculum improvement. Moreover, this project will help enhance the research capability of the University, a Hispanic Serving and Designated Undergraduate Institution. Research results will be disseminated to the Intelligent Systems community via conference presentations and publication in journals."
"1053233","CAREER: A Neurophotonic Platform for Causal Brain Analysis","CBET","BIOPHOTONICS, IMAGING &SENSING","09/01/2011","12/15/2010","Edward Boyden","MA","Massachusetts Institute of Technology","Standard Grant","Leon Esterowitz","08/31/2016","$400,000.00","","esb@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","7236","005E, 014E, 1045, 1187","$0.00","1053233<br/>Boyden<br/><br/>Tools that enable the perturbation of specific cellular processes in a temporally-precise manner are critical in the field of neuroscience for determining when and how such processes contribute to neural computations, behaviors, and pathologies. Without such tools, mechanistic understandings of how neurons and neural networks function are sometimes tentative, limiting not only basic science but also clinical progress, as the core mechanisms that contribute to normal function and disease states, and that might enable potential therapies, can remain obscure. To open up the ability to test the causal role of defined neurons in emergent brain functions, the PI has recently pioneered a set of molecular tools that, when genetically expressed in specific neuron classes within the brain, enable those neurons to be electrically activated and silenced in response to specific colors of light. These molecules are opsins, light-driven membrane proteins from nature that, when illuminated, transport charge from one side of the cellular membrane to the other. Since neurons are electrically excitable cells, expression of these genes in neurons and illumination of the resultant transgenic neurons can effect their electrical activation or silencing. Over the last few years, this lab at MIT has distributed these ""optogenetic"" reagents to ~300 research labs around the world, enabling these groups to study the causal role of specific cell types in brain functions. Despite their broad impact, these tools are chiefly useful for analyzing neural circuits at the level of seeing how specific cells causally affect behavior and neural dynamics; they do not enable detailed analysis of the contribution of computational processes within neurons, mediated by specific ion channels and receptors, to neural network operation. Accordingly, the PI proposes to engineer a new generation of molecular reagents and hardware to enable the study of the causal roles of receptors and ion channels in neural computations and behaviors."
"1105444","RUI: Nonlinear and Neural Dynamics in Josephson Networks","DMR","CONDENSED MATTER PHYSICS","08/15/2011","06/10/2013","Kenneth Segall","NY","Colgate University","Continuing Grant","Paul Sokol","07/31/2014","$262,459.00","","ksegall@mail.colgate.edu","13 Oak Drive","Hamilton","NY","133461398","3152287457","MPS","1710","7203, 7574, 7969, 9161, 9178, 9229, AMPP, SMET","$0.00","****Technical abstract****<br/> <br/>This individual investigator award supports an experimental and computational study of nonlinear dynamics in networks of superconducting Josephson junctions.  Josephson junctions are examples of nonlinear systems which can be fabricated with adjustable parameters, measured in a straightforward fashion, and easily scaled to large network sizes.  In addition, a large Josephson junction circuit measured over a long time contains dynamics which would essentially be impossible to calculate on a computer, but which can be observed with electrical measurements.  This project will take a multi-faceted approach to studying the collective, emergent behavior of Josephson junction networks.  First, it will follow previous work in the field on soliton-like modes called fluxons and localized modes called discrete breathers.  Next, studies will be performed on the synchronization of a system of disordered oscillators.  Finally, a circuit of Josephson junctions designed to accurately model the time-dependent voltage of a biological neuron will be fabricated and tested.  This has a longer-term goal of studying the emergent behavior of a large, coupled neural network.  <br/> <br/>****Non-technical abstract****<br/> <br/>An important aspect of physics today is the effort to understand how the fundamental laws of nature result in complex behavior.  For example, consider a system of gas molecules.  Simple laws of force and momentum govern their collisions.  With only a few molecules, the system is simple and uninteresting.  With a large number of molecules, however, the system can organize itself into something complex, like a tornado.  This new behavior comes about not because of a change in the fundamental laws, but rather a change in the number of constituents, in this case gas molecules, of the system.  In this research, a simple electrical circuit element known as a Josephson junction is studied.  Josephson junctions are made from superconducting metals and work at very low temperatures.  Past experiments have looked at the behavior of a single Josephson junction and found it capable of interesting electrical behavior.  However, circuits composed of large numbers of Josephson junctions have yet to be fully studied.  Just like the case of gas molecules, new collective behaviors result when the number of constituents is increased.  This project will look at several of these new behaviors.  One of these, like a tornado, is a swirl of electrical current.  Another is a collective voltage oscillation, a back and forth motion like pendulums swinging together.  A final behavior is voltage spiking, similar to the on-off firing of a biological neuron.  With this last behavior, a longer term goal is to build circuits which would emulate collective behaviors in the human brain, where large numbers of neurons are connected together.  This project incorporates undergraduate students as the primary researchers, preparing them for technical careers in the sciences."
"1153487","RI: SMALL: Statistical Linguistic Typology","IIS","ROBUST INTELLIGENCE","07/01/2011","09/06/2011","Hal Daume","MD","University of Maryland College Park","Continuing grant","Tatiana D. Korelsky","07/31/2013","$365,970.00","","hal@umiacs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7923, 9215, HPCC","$0.00","This project considers the unification of two view of language: that<br/>from natural language processing and that from linguistic typology.<br/>Our view is that typological information is both useful for solving<br/>real-world natural language processing thats and automatically<br/>derivable from language data. This research first explores how to use<br/>typological knowledge to improve performance on problems such as<br/>dependency parsing and machine translation for low density langauges.<br/>Intuitively, our statistical models waste time exploring a hypothesis<br/>space that is too big: the space of realistic grammars is much smaller<br/>than the space of all grammars. The second part of this research<br/>considers the automatic acquisition and boostrapping of typological<br/>knowledge from raw text. The outcome of this research is: (a)<br/>improved statistical models for hard natural language processing<br/>problems; and (b) a larger library of typological universals that have<br/>been derived automatically from data. Our outcomes are empirically<br/>evaluated on the raw language processing tasks and in terms of the<br/>quality of the universal implications mined from data, but comparing<br/>them with known repositories of universals. <br/><br/>Our results will impact the fields of natural language processing and linguistics. From the research side, this research will find applications in a wider variety of problems than the ones we intend to study; in particular, the use of linguistic universals in natural language processing technology<br/>will fundamentally change the way multilinguality is addressed in this<br/>field. From a linguistics perspective, the goal of this project is to<br/>shed new light on linguistic universals. This should impact not only<br/>the area of typology, but also the study and preservation of<br/>endangered languages. By automatically identifying typological<br/>features and implications from data, the process of documenting<br/>endangered languages could be made more efficient: leading to a<br/>smaller loss of knowledge of these languages."
"1102038","Collaborative Research: Wind Power - Neural Network Control, Multidisciplinary Integration, and Advanced Simulation","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/01/2011","07/27/2011","Shuhui Li","AL","University of Alabama Tuscaloosa","Standard Grant","Paul Werbos","07/31/2014","$179,965.00","","sli@eng.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","ENG","7607","155E, 1653, 9150","$0.00","The objective of this research is to develop intelligent vector control technology, intelligent wind power extraction and management strategy, and advanced simulation and testing mechanisms by using  Adaptive Dynamic Programming neural control mechanisms.  This will improve the efficiency and reliability of wind energy conversion systems and enhance wind power integration into the electric utility system for the 20% wind penetration vision of the United States in 2030.  <br/><br/>INTELLECTUAL MERITS <br/>The intellectual merits of this research include development of: 1) Adaptive Dynamic Programming vector control technology to overcome the deficiency of conventional vector control technology, 2) intelligent wind power extraction and management techniques to improve wind power production efficiency and reliability, 3) intelligent control integration strategy under practical system constraints, 4) advanced simulation and testing mechanisms, and 5) a curriculum in Intelligent Sustainable Energy Systems to enhance student capability in multidisciplinary fields.<br/><br/>BROADER IMPACTS<br/>The broader impacts of this research are significant. The rapid development and increased complexities of sustainable energy systems make it increasingly urgent to develop intelligent and cyber-enabled technologies for research and education of sustainable energy system field. This research will enhance optimal and intelligent technology for future smart and sustainable energy systems and increase the participation of an EPSCOR state in advanced scientific research. The research should have direct or indirect impacts to the following mission areas of the United States: reduction in imported energy, reduction of energy-related emissions, improvements in energy efficiency, and a technological lead for the United States in advanced energy techniques."
"1102159","Collaborative Research: Wind Power - Neural Network Control, Multidisciplinary Integration, and Advanced Simulation","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/01/2011","07/27/2011","Donald Wunsch","MO","Missouri University of Science and Technology","Standard Grant","Paul Werbos","07/31/2014","$179,999.00","","dwunsch@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","ENG","7607","155E, 1653","$0.00","The objective of this research is to develop intelligent vector control technology, intelligent wind power extraction and management strategy, and advanced simulation and testing mechanisms by using  Adaptive Dynamic Programming neural control mechanisms.  This will improve the efficiency and reliability of wind energy conversion systems and enhance wind power integration into the electric utility system for the 20% wind penetration vision of the United States in 2030.  <br/><br/>INTELLECTUAL MERITS <br/>The intellectual merits of this research include development of: 1) Adaptive Dynamic Programming vector control technology to overcome the deficiency of conventional vector control technology, 2) intelligent wind power extraction and management techniques to improve wind power production efficiency and reliability, 3) intelligent control integration strategy under practical system constraints, 4) advanced simulation and testing mechanisms, and 5) a curriculum in Intelligent Sustainable Energy Systems to enhance student capability in multidisciplinary fields.<br/><br/>BROADER IMPACTS<br/>The broader impacts of this research are significant. The rapid development and increased complexities of sustainable energy systems make it increasingly urgent to develop intelligent and cyber-enabled technologies for research and education of sustainable energy system field. This research will enhance optimal and intelligent technology for future smart and sustainable energy systems and increase the participation of an EPSCOR state in advanced scientific research. The research should have direct or indirect impacts to the following mission areas of the United States: reduction in imported energy, reduction of energy-related emissions, improvements in energy efficiency, and a technological lead for the United States in advanced energy techniques."
"1064326","Collaborative Research: Quantifying Feedbacks Affecting High Altitude Climate Change","AGS","CLIMATE & LARGE-SCALE DYNAMICS","07/15/2011","04/08/2013","James Miller","NJ","Rutgers University New Brunswick","Continuing grant","Anjuli S. Bamzai","06/30/2014","$300,356.00","Ming Xu","miller@marine.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","GEO","5740","0000, OTHR","$0.00","In many mountain regions there is evidence that temperatures are changing at different rates than the global average. Three questions arise: Are temperatures in mountain regions increasing faster than the global average? Within mountain regions are warming rates dependent on elevation? And if the answers to the above are yes, why do such differences occur?  Several different feedbacks can contribute, including those related to snow-albedo, atmospheric water vapor, cloud cover, and cloud properties. These feedbacks are difficult to quantify because the relationship between two climate variables is invariably interconnected with other variables as well. Also, the sparsity of observations in high-altitude regions exacerbates this difficulty.<br/><br/>This project will combine surface-based and satellite observations with climate model simulations and a neural network analysis scheme to (1) quantify some of the principal relationships that contribute to feedbacks on temperature in high altitude regions, and (2) investigate how these relationships and feedbacks might change through the 21st century in response to increasing atmospheric greenhouse gases. The focus will be on the Tibetan Plateau and the Rocky Mountains in southwestern Colorado. The neural network analysis calculates partial derivatives between pairs of climate variables (e.g., downward longwave radiation and cloud cover) so that the strength of the various links in a feedback loop can be determined. <br/><br/>Broader impacts of this work include: (1) The neural network can be applied in other regions and can enable researchers to quantify important feedbacks in the climate system and analyze non-linear processes; (2) By combining surface-based and satellite observations, a new spatially and temporally expanded observational data base will be available to the research community; (3) A better understanding of climate change in mountain regions will benefit the public by improving management practices that affect the future of water resources, agriculture, tourism, and ecosystems in high altitude regions; (4) A high-school teacher will be supported to work with the investigators to help develop and implement podcasts on mountains and climate change; (5) There will be training for a postdoctoral fellow and undergraduates; and (6) Educational materials will be developed in collaboration with the Mountain Studies Institute in Colorado."
"1055889","CAREER: Simultaneous imaging of photoreceptor and post-photoreceptor responses in the retina","CBET","BIOPHOTONICS, IMAGING &SENSING, EPSCoR Co-Funding","08/01/2011","01/28/2011","Xincheng Yao","AL","University of Alabama at Birmingham","Standard Grant","Leon Esterowitz","04/30/2015","$400,000.00","","xcy@uic.edu","AB 1170","Birmingham","AL","352940001","2059345266","ENG","7236, 9150","005E, 014E, 1045, 1187, 9150","$0.00","1055889<br/>Yao<br/><br/>The research objective of this proposal is to investigate stimulus-evoked intrinsic optical signals (IOSs) associated with photoreceptor and post-photoreceptor neural responses in the complex retinal neural network. This project will start with investigating fast IOSs in frog retinal slices. A retinal slice preparation exposes a cross-section of the retinal layers, and thus allow simultaneous recording of fast IOSs from the photoreceptors and post-photoreceptor neurons, with feasibility of concurrent electrophysiological recording of targeted retinal cells. This phase will test two hypotheses: 1) Fast IOSs correlated with early photoreceptor activities to light stimulation occurs immediately after the stimulus delivery; while fast IOSs associated with second- and third-order post-photoreceptor neurons may have a time delay relative to the stimulus. 2) Fast IOSs can be used to image oscillatory and spike neural activities generated by post-photoreceptor neurons. The second phase of this project is to explore simultaneous optical coherence tomography (OCT) imaging of photoreceptors and post-photoreceptor responses in the intact eye. In order to achieve this objective, an acousto-optic deflector (AOD) based optical coherence tomography (OCT) is proposed for pursuing vibration- and inertia-free optical dissection of retinal neural activities."
"1058291","Testing the Criticality Hypothesis in Local Cortical Circuits","PHY","PHYSICS OF LIVING SYSTEMS","09/15/2011","07/11/2013","John Beggs","IN","Indiana University","Continuing Grant","Krastan Blagoev","08/31/2015","$360,000.00","","jmbeggs@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","MPS","7246","9183","$0.00","How do cortical neurons collectively interact to process information? A new hypothesis, supported by recent multielectrode recordings in brain tissue samples and in the intact brain, suggests that local cortical networks self-organize to operate near a critical point. At this point, interactions across all spatial and temporal scales can occur, and avalanches of neural activity are generated at all sizes, resulting in approximately power law distributions. Very generic neural network models suggest that information processing will be optimal at this point. This ""criticality hypothesis"" has significant implications for computation in cortical circuits, and ultimately, for human health. Despite the potential importance of this hypothesis, it has remained untested in local cortical networks consisting of 100 or more synaptically connected neurons. Previous work has relied on local field potential recordings, which do not reveal the number or location of individual neurons generating signals. Previous work also has relied on relatively few (~60-100) broadly-spaced recording sites, which are insufficient to adequately sample a population of synaptically connected neurons. Here, the PI will record from 512 closely spaced electrodes, allowing us to monitor the spiking behavior of hundreds of neurons, many of which are expected to share synaptic connections. The tests the PI will perform of the criticality hypothesis will allow him, for the first time, to determine if networks of cortical neurons are critical or not. <br/><br/>Scientific education and outreach is another vital component of this proposal. The PI will engage thousands of children from economically disadvantaged areas in the development and use of an interactive exhibit for WonderLab, a popular local science museum for K-middle school children. This exhibit is based on electroencephalogram (EEG) recordings and will display children's brain waves, allowing them to modify these waves by biofeedback. This exhibit will be placed in WonderLab museum, which serves 78,000 visitors each year. WonderLab has been named by Parent's magazine as one of the ""Top 25 Science Centers in the United States."""
"1064281","Collaborative Research: Quantifying Feedbacks Affecting High Altitude Climate Change","AGS","CLIMATE & LARGE-SCALE DYNAMICS, AGS","07/15/2011","04/08/2013","Catherine Naud","NY","Columbia University","Continuing grant","Anjuli S. Bamzai","06/30/2014","$269,405.00","Yonghua Chen","cn2140@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","GEO","5740, 6897","0000, 4444, 5740, 6897, OTHR","$0.00","In many mountain regions there is evidence that temperatures are changing at different rates than the global average. Three questions arise: Are temperatures in mountain regions increasing faster than the global average? Within mountain regions are warming rates dependent on elevation? And if the answers to the above are yes, why do such differences occur?  Several different feedbacks can contribute, including those related to snow-albedo, atmospheric water vapor, cloud cover, and cloud properties. These feedbacks are difficult to quantify because the relationship between two climate variables is invariably interconnected with other variables as well. Also, the sparsity of observations in high-altitude regions exacerbates this difficulty.<br/><br/>This project will combine surface-based and satellite observations with climate model simulations and a neural network analysis scheme to (1) quantify some of the principal relationships that contribute to feedbacks on temperature in high altitude regions, and (2) investigate how these relationships and feedbacks might change through the 21st century in response to increasing atmospheric greenhouse gases. The focus will be on the Tibetan Plateau and the Rocky Mountains in southwestern Colorado. The neural network analysis calculates partial derivatives between pairs of climate variables (e.g., downward longwave radiation and cloud cover) so that the strength of the various links in a feedback loop can be determined. <br/><br/>Broader impacts of this work include: (1) The neural network can be applied in other regions and can enable researchers to quantify important feedbacks in the climate system and analyze non-linear processes; (2) By combining surface-based and satellite observations, a new spatially and temporally expanded observational data base will be available to the research community; (3) A better understanding of climate change in mountain regions will benefit the public by improving management practices that affect the future of water resources, agriculture, tourism, and ecosystems in high altitude regions; (4) A high-school teacher will be supported to work with the investigators to help develop and implement podcasts on mountains and climate change; (5) There will be training for a postdoctoral fellow and undergraduates; and (6) Educational materials will be developed in collaboration with the Mountain Studies Institute in Colorado."
"1134736","Wireless Internet Center for Advanced Technology (WICAT)","IIP","INDUSTRY/UNIV COOP RES CENTERS","08/01/2011","06/24/2014","Barry Horowitz","VA","University of Virginia Main Campus","Continuing grant","Shashank Priya","06/30/2014","$108,625.00","Stephen Patek","bh8e@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","ENG","5761","1049, 116E, 122E, 5761, 8039, 9178, 9251, SMET","$0.00","IIP-1134736 <br/>University of Virginia <br/>Horowitz <br/><br/>This is a proposal to renew the University of Virginia's participation in the Wireless Internet Center for Advance Technology (WICAT). The multi-university center consists of New York Polytechnic, Virginia Tech, Auburn University, and the University of Texas-Austin. WICAT, with Polytechnic as the lead institution, has approximately 25 companies and 8 federal and state agencies as industry associates. The Center research addresses cooperative communications and networking; extending battery life of portable terminals; and wireless applications and associated information delivery. Finding new and better ways to meet the demands on wireless networks will continue to be the goal of research being carried out in WICAT overall. <br/><br/>The UVA WICAT research activity addresses adaptive wireless system design patterns and corresponding technology developments that serve to enable improved designs. The proposed research effort will make important contributions to the future development of adaptive wireless surveillance systems, wireless networking systems, and telehealth systems. The concurrent engineering approach of carrying out component technology research that is based on needs that emerge from system application research provides an important basis for directing and managing the proposed efforts. Based on historical work, both industry and the broader wireless research community find UVAs coupling of system needs to technology needs to be a useful basis for both directing research and evaluating the benefits of research outcomes. <br/><br/>The system-oriented contributions of UVA to WICAT will continue to provide a significant differentiating value to the overall mission of the Center. Multi-university collaborations will continue to provide the other WICAT sites with an expanded set of data and results derived from system-level activities to complement their component technology results. The research collaborations will continue to provide new opportunities to achieve greater diversity by creating student interactions across the research institutions. The educational goals of the center have supported the department's new curriculum which, in turn, serves the industry members well. The site has also supported outreach activities through summer camps. The diversity UVA brings to the center's research agenda has broader impacts on industry and the community, as well."
"1115966","RI: Small: Scalable Algorithms for Learning to Recover Logical Form from Natural Language","IIS","ROBUST INTELLIGENCE","08/01/2011","08/10/2011","Luke Zettlemoyer","WA","University of Washington","Standard Grant","Tatiana D. Korelsky","07/31/2014","$300,000.00","","lsz@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7923","$0.00","A key aim in Natural Language Processing is to robustly map from natural language sentences to formal representations of their underlying meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations. The goal of this project is to develop models and learning algorithms for recovering lexical structure, in the context mapping sentences to logical form. This work is inspired by linguistic theories of the lexicon, but directly motivated by the limitations observed in current, state-of-the-art learning algorithms.<br/><br/>The central hypothesis is that a new probabilistic learning approach for lexical generalization can simultaneous achieve the goals of (1) language-independent learning, (2) robustness when analyzing natural, unedited text, and (3) requiring reduced data annotation effort, in a computationally efficient manner that will scale to large learning problems. The approach under development induces a Combinatory Categorial Grammar (CCG), that is modified to replace the traditional, explicit list of lexical items in the lexicon with a distribution over lexical items that allows for significant generalization in the construction of possible syntactic and semantic structures for given input words. Modifying the CCG lexicon in this manner greatly increases the potential to generalize from the available training data without sacrificing the scalability that comes from working within an established grammar formalism for which efficient learning and parsing algorithms have been developed. This work will have impact at the algorithmic level and through applications, including advanced natural language interfaces to databases for non-technical users."
"1138394","Constraining Tropical Low Cloud Feedbacks Using Observations of the Fast Cloud Response","AGS","CLIMATE & LARGE-SCALE DYNAMICS","11/01/2011","08/31/2011","Robert Pincus","CO","University of Colorado at Boulder","Standard Grant","Eric DeWeaver","10/31/2015","$397,045.00","Frank Evans","robert.pincus@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","GEO","5740","0000, OTHR","$0.00","Changes in the properties of low clouds in the tropics and subtropics in response to global warming have been identified as a key cause of the large uncertainty in global warming projections from climate models.  In some models warming produces more prevalent or more reflective low clouds, thus reflecting more sunlight back to space and partially mitigating the warming effects of greenhouse gases (a negative cloud feedback to the imposed warming), while in other models the clouds become less reflective or less prevalent and enhanced sunlight at the surface exacerbates global warming (a positive cloud feedback).  This research attempts to produce observationally-constrained estimates of the true low cloud feedback using a combination of satellite observations, reanalysis products, and climate model outputs.  The method first uses a neural network to establish empirical relationships between low cloud reflectivity and other cloud properties (determined from satellite observations) and large-scale variables including sea surface temperature and atmospheric temperature and water vapor. These relationships are established using the fast (timescales of hours to a day) response of clouds to their environment.  Once these relationships are established, cloud feedback will estimated by applying the empirical relationships represented by the neural network in combination with changes in the large-scale variables in the climate models which occur as a result of simulated global warming.  In addition to estimates of the low cloud feedback to global warming, the method will provide uncertainty bounds for those estimates, and can be used to diagnose errors in model parameterizations of low cloud properties.<br/><br/>The work has broader impacts due to the key role of tropical low cloud feedback in generating the large uncertainty in model projections of climate change.  A reliable, observationally-based estimate of the low cloud feedback could help to reduce this uncertainty and provide better information to decision makers regarding the likely extent and physical consequences of greenhouse-gas induced global warming."
"1143389","EAGER: Brain Responses to Visual Stimuli in Sharks Using Functional Magnetic Resonance Imaging (FMRI)","DBI","ADVANCES IN BIO INFORMATICS","09/15/2011","08/04/2011","Lawrence Frank","CA","University of California-San Diego","Standard Grant","Anne Maglia","08/31/2014","$250,000.00","","lfrank@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","BIO","1165","1165, 7916","$0.00","The University of California at San Diego is awarded a grant to develop methods for performing functional magnetic resonance imaging (FMRI) experiments in living sharks in order to study their brains as a complete neural network and provide information on how vertebrate neural systems have both structurally and functionally evolved. Understanding this organization and its relationship to shark behavior and ecology has major implications for our understanding of the evolution of vertebrate nervous systems, and the relationship between form and function in the vertebrate brain. In recent years, great advances in human neuroscience have been made using FMRI, where localized changes in brain activity are detected within a sequence of MR images, thus allowing brain activation as a function of time to be measured. In conjunction with advanced methods for non-invasive, high-resolution MRI for quantitating form in chondrichthyan brains, the goal of this project is to develop FMRI capabilities that facilitate the study of the relationship between form and function in these fishes. However, while FMRI in humans is now routine, performing it in sharks requires the development and refinement of both software and hardware to collect reliable, high quality anatomical and functional MRI data in a living partially submerged aquatic specimen. Hence our key objective is to develop these technological advances, and demonstrate the capability of performing FMRI experiments in the shark nervous system. <br/><br/><br/>This project will provide a unique set of methods for studying the relationships between form and function in the shark nervous system, as well as other aquatic model systems, and thus has broad implications for many researchers for whom the link between morphology and function is of great importance, but difficult to explore simultaneously in a well-controlled experimental platform. This project also promotes interdisciplinary collaboration and training in imaging, computation, and data analysis for the morphology, physiology, and marine biology communities. The outcomes of this award including methods, equipment design, and data analysis software will be disseminated to the research community through the Digital Fish Library website (http://www.digitalfishlibrary.org)."
"1053856","CAREER:  Bayesian Models for Lexicalized Grammars","IIS","ROBUST INTELLIGENCE","02/01/2011","01/12/2017","Julia Hockenmaier","IL","University of Illinois at Urbana-Champaign","Continuing grant","Tatiana Korelsky","01/31/2018","$500,001.00","","juliahmr@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7495","1045, 1187, 7495","$0.00","Natural language processing (NLP) is a key technology for the digital age. At the core of most NLP systems is a parser, a program which identifies the grammatical structure of sentences. Parsing is an essential prerequisite for language understanding. But despite significant progress in recent decades, accurate wide-coverage parsing for any genre or language remains an unsolved problem. The broader impact of this CAREER project will be to advance the state of art in NLP technology through the development of more accurate statistical parsing models.  <br/><br/>Since language is highly ambiguous, parsers require a statistical model which assigns the highest probability to the correct structure of each sentence.  The accuracy of current parsers is limited by the amount of available training data on which their models can be trained, and by the amount of information the models take into account. This CAREER project aims to advance parsing by developing novel methods of indirect supervision to overcome the lack of labeled training data, as well as new kinds of models which incorporate information about the prior linguistic context in which sentences appear. It employs Bayesian techniques, which give robust estimates and allow rich parametrization, and applies them to lexicalized grammars, which provide a compact representation of the syntactic properties of a language.<br/>This CAREER project will also train graduate students in natural language processing and develop materials that can be used to teach middle and high school students about NLP  and to inspire them to pursue an education in computer science."
"1102435","ACL-HLT 2011 Student Session","IIS","Robust Intelligence","01/01/2011","12/09/2010","Thamar Solorio","AL","University of Alabama at Birmingham","Standard Grant","Tatiana Korelsky","12/31/2011","$16,200.00","","thamar.solorio@gmail.com","AB 1170","Birmingham","AL","352940001","2059345266","CSE","7495","7495, 9150","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing. The ACL's annual conference is the major international conference in this field. This project is to subsidize travel, conference, and housing expenses of students selected to participate in the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies Student Session. This session is part of the ACL HLT conference to be held on June 19-24, 2011 in Portland, Oregon, USA. The Student Session includes two separate categories of submissions: thesis proposals and research papers. The goal is to provide a forum for students at different stages in their research. <br/><br/>The Student Session provides a valuable opportunity for the next generation of natural language processing researchers to enter the computational linguistics community. It allows the best students in the field to take their first important step toward becoming professional computational linguists by receiving critical feedback on their work from external experts, and by making contacts with other students and senior researchers in their field. The students who are involved in running and selecting papers for the session also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The Student Session contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing research community."
"1108788","RUI: Stability and Interaction of Coherent Structures in Lattice Differential Equations","DMS","APPLIED MATHEMATICS","08/01/2011","07/25/2011","Aaron Hoffman","MA","Franklin W. Olin College of Engineering","Standard Grant","Victor Roytburd","07/31/2015","$94,800.00","","aaron.hoffman@olin.edu","1000 Olin Way","Needham","MA","024921200","7812922426","MPS","1266","9229","$0.00","Much effort in the theory of spatially extended dynamical systems has been devoted to the mechanisms, such as traveling waves, by which activity (or energy or mass) is transported from one region of the spatial domain to another.  Professor Hoffman will study the stability and interaction of coherent structures, i.e. exponentially localized traveling waves and pulses, in lattice differential equations (LDE).  This includes (i) multidimensional stability and interaction of planar fronts in discrete reaction diffusion equations; (ii) collision properties of solitary waves in Hamiltonian lattices such as the Fermi-Pasta-Ulam lattice; (iii) strong interaction of coherent structures in dissipative systems such as annihilation of pulses in the discrete Fitzhugh-Nagumo equation; (iv) Stability of pushed fronts in unidirectional lattice differential equations such as those obtained from upwind discretization of advection-reaction equations or in cellular neural network models.  One advantage of working in the lattice setting is that existence and uniqueness is well-known, hence one can immediately turn to more detailed questions concerning the dynamics.  At the same time, the interaction between the continuous temporal dynamics and the discrete spatial structure can give rise to interesting and subtle phenomenon, such as fronts facing rational directions behaving differently from fronts facing irrational directions.<br/><br/>Nonlinear lattice differential equations are typically too complex to solve explicitly in the sense of writing down a formula in terms of known functions.  However, the methods of dynamical systems can be used to obtain less detailed information and this is often sufficient for the purposes of scientific inquiry.  For example, consider the following questions: As a crystal grows, what shape will it approach? As an alloy cools, how will its phase boundaries evolve? Given two signals propagating along the same fiber, how will they interact? At what rate will an invasive species encroach upon native habitat? The Principal Investigator is concerned with developing mathematical techniques to answer these and similar questions for idealized physical models that admit spatially discrete coherent structures.  Undergraduate students will participate in this research."
"1065013","HCC: Medium: Collaborative Research: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing","IIS","HCC-Human-Centered Computing","07/01/2011","06/12/2013","Carol Neidle","MA","Trustees of Boston University","Continuing Grant","Ephraim Glinert","06/30/2016","$385,957.00","","carol@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7367","7367, 7924","$0.00","American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf.  To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing.  Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement.  How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar?  How should the onsets, offsets, and transitions of these movements be produced?  How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible?  To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production.  The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties.  Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers.   The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing.  Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance.  The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).<br/><br/>Broader Impacts:  This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy.  Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision.  The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL.  The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora.  As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research."
"1064965","HCC: Medium: Collaborative Research: Generating  Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing","IIS","HCC-Human-Centered Computing","07/01/2011","07/25/2014","Dimitris Metaxas","NJ","Rutgers University New Brunswick","Continuing Grant","Ephraim Glinert","06/30/2016","$469,996.00","","dnm@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7367","7367, 7924","$0.00","American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf.  To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing.  Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement.  How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar?  How should the onsets, offsets, and transitions of these movements be produced?  How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible?  To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production.  The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties.  Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers.   The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing.  Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance.  The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).<br/><br/>Broader Impacts:  This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy.  Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision.  The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL.  The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora.  As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research."
"1065009","HCC: Medium: Collaborative Research: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing","IIS","HCC-Human-Centered Computing, IIS Special Projects","07/01/2011","05/08/2013","Matt Huenerfauth","NY","CUNY Queens College","Continuing Grant","Ephraim Glinert","12/31/2014","$359,005.00","","matt.huenerfauth@rit.edu","65 30 Kissena Blvd","Flushing","NY","113671575","7189975400","CSE","7367, 7484","7367, 7924, 9251","$0.00","American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf.  To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing.  Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement.  How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar?  How should the onsets, offsets, and transitions of these movements be produced?  How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible?  To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production.  The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties.  Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers.   The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing.  Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance.  The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).<br/><br/>Broader Impacts:  This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy.  Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision.  The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL.  The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora.  As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research."
"1065114","CSR: Medium: Collaborative Research: Programming parallel in-memory data-center applications with Piccolo","CNS","CSR-Computer Systems Research","07/01/2011","07/05/2013","M. Frans Kaashoek","MA","Massachusetts Institute of Technology","Continuing grant","Marilyn McClure","06/30/2015","$330,171.00","","kaashoek@lcs.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7354","7354, 7924","$0.00","There is a rising demand to scale application performance by distributing <br/>computation across many machines in a data-center. It is difficult to write <br/>efficient and robust parallel programs in the data-center setting because <br/>programmers need to worry about reducing communication overhead while handling <br/>possible machine failures. <br/><br/>This project investigates a new data-centric parallel programming <br/>model, called Piccolo, that can simplify the construction of in-memory <br/>data-center applications such as PageRank, neural network training etc. <br/><br/>In-memory applications can hold all their intermediate states in the aggregate <br/>memory of many machines and benefit from sharing these intermediate states <br/>between machines during computation. Traditionally, these applications <br/>have been built using low-level communication-centric primitives such as MPI, <br/>resulting in significant programming complexity. The recently popular <br/>MapReduce and Dryad also do not fit well with these applications <br/>because their data flow programming model lacks support for shared states. <br/><br/>Unlike data flow models, Piccolo explicitly supports the sharing of mutable, <br/>distributed states via a key/value table interface. Piccolo makes sharing <br/>efficient by optimizing for locality of access to shared tables and <br/>automatically resolving write-write conflicts using user-defined accumulation <br/>functions. As a result, Piccolo is easy to program for, enables applications <br/>that do not fit into MapReduce, and achieves good scalable performance."
"1065169","CSR: Medium: Collaborative Research: Programming parallel in-memory data-center applications with Piccolo","CNS","Computer Systems Research (CSR","07/01/2011","06/25/2013","Jinyang Li","NY","New York University","Continuing grant","M. Mimi McClure","06/30/2015","$523,290.00","","jinyang@cs.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7354","7924, 9102","$0.00","There is a rising demand to scale application performance by distributing<br/>computation across many machines in a data-center. It is difficult to write<br/>efficient and robust parallel programs in the data-center setting because <br/>programmers need to worry about reducing communication overhead while handling <br/>possible machine failures.  <br/><br/>This project investigates a new data-centric parallel programming<br/>model, called Piccolo, that can simplify the construction of in-memory<br/>data-center applications such as PageRank, neural network training etc. <br/><br/>In-memory applications can hold all their intermediate states in the aggregate<br/>memory of many machines and benefit from sharing these intermediate states<br/>between machines during computation.  Traditionally, these applications<br/>have been built using low-level communication-centric primitives such as MPI,<br/>resulting in significant programming complexity. The recently popular <br/>MapReduce and Dryad also do not fit well with these applications<br/>because their data flow programming model lacks support for shared states.<br/><br/>Unlike data flow models, Piccolo explicitly supports the sharing of mutable,<br/>distributed states via a key/value table interface.  Piccolo makes sharing<br/>efficient by optimizing for locality of access to shared tables and<br/>automatically resolving write-write conflicts using user-defined accumulation<br/>functions.  As a result, Piccolo is easy to program for, enables applications<br/>that do not fit into MapReduce, and achieves good scalable performance."
"1134296","Adaptive Closed-loop Control of  Deep Brain Stimulation for Movement Disorders","CBET","Engineering of Biomed Systems","08/15/2011","07/29/2013","Daniela Tuninetti","IL","University of Illinois at Chicago","Continuing Grant","Friedrich Srienc","12/31/2015","$330,001.00","Daniel Graupe","danielat@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","ENG","5345","004E, 137E, 9102","$0.00","1134296<br/>Tuninetti<br/><br/>Deep Brain Stimulation (DBS) provides remarkable therapeutic benefits for otherwise drug-resistant degenerative neurological disorders, such as Parkinson's disease and Essential Tremor, for which no cure exists at present. DBS uses surgically implanted electrodes to deliver high frequency electrical stimulation to the area of the brain that controls motor functions. The stimulation blocks the abnormal nerve signals that cause disease symptoms, such as tremor, but its underlying mechanisms are unclear. Today's DBS systems operate open-loop, i.e., the physician sets DBS parameters by looking at the patient's reaction to stimulation and chooses the combination that reduced symptoms the most. Stimulation is provided continuously and its parameters remain constant over time until the next visit to the physician.<br/><br/>This interdisciplinary research integrates the forefront of electrical engineering, mathematics, and neuroscience principles into the development of models and methods to the response of the area of the brain that controls movement to DBS. It proposes a concrete design of the next generation of DBS systems via adaptive and predictive closed-loop control in an on-off fashion, where on and off times of stimulation are determined/adapted in real-time with the patient's condition. Adaptation of the stimulation parameters to each patient's condition at any given time will: a) diminish brain over-stimulation, thus reducing the damage to healthy neurons and delaying the development of a possible intolerance to DBS, b) lower power consumption, thus prolonging DBS battery life and reducing the risks and costs related to surgeries for battery replacement, and c) reduce DBS side effects on other cognitive functions, such as speech, thus further improving patients' quality of life besides better motor functions control. This will yield improved and personalized health-care at reduced risks and costs.<br/><br/>This research has three main thrusts: 1) Modeling the dynamics of the area in the brain that controls movement by using signals measured from the patient's brain so as to predict the effect of the DBS stimulation parameters; 2) Designing a closed-loop DBS control where brain signals are integrated with signals from the patient?s tremor affected limbs, such as measured by noninvasive Surface ElectroMyoGraphy (sEMG), so as to obtain a more complete picture of the patient?s pathological state. sEMG signal parameters are continuously monitored to predict the re-emergence of the tremor once DBS is stopped and serve as input to the controller, together with the neuronal activity; 3) Prototyping in software the second generation of DBS systems by implementing  low-complexity and energy-efficient algorithms for real-time predictive closed-loop control of DBS.<br/><br/>Although this research focuses on degenerative movement disorders, the discoveries have far reaching implications on the treatment of a number of neurological conditions, such as severe depression, epilepsy, obsessive compulsive disorder, and chronic pain, which have recently been considered for DBS-type treatments. The transformative approach of this proposed research, based on the real-time monitoring of the brain activity, enables DBS stimuli adaptation for those diseases that do not present continuous and/or visible symptoms such as tremor; such adaptation is impossible with any current open-loop technology."
"1065270","RI: Medium: Collaborative Research: Learning Representations of Language for Domain Adaptation","IIS","Robust Intelligence","04/01/2011","02/20/2014","Douglas Downey","IL","Northwestern University","Continuing Grant","Tatiana Korelsky","03/31/2016","$200,000.00","","ddowney@eecs.northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7495","7495, 7924","$0.00","Supervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts.  A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and for the inability to generalize to previously unseen words. <br/><br/>This project is the first to systematically investigate representation-learning as a technique for improving performance on domain adaptation.  It explores latent-variable language models ? including Factorial Hidden Markov Models, dependency parsing models, and deep architectures ? as techniques for extracting novel features from text.  The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier.  The project also explores novel procedures for training a language model, which incorporate Web-scale ngram statistics as substitutes for standard statistics used in unsupervised training.<br/><br/>Language users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology.  By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality.  For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts.  By involving the diverse student bodies at Temple University and Philadelphia-area high schools, the project helps to broaden participation in computer science research by underrepresented groups."
"1058886","Retrieved Context Models of Episodic Memory","BCS","PERCEPTION, ACTION & COGNITION","10/01/2011","09/26/2011","Michael Kahana","PA","University of Pennsylvania","Standard Grant","Betty Tuller","09/30/2014","$449,283.00","","kahana@psych.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","SBE","7252","7752, OTHR","$0.00","This research seeks to illuminate the mechanisms underlying human episodic memory through both computational modeling and experimental studies. Episodic memory is the ability to link the information that we experience with its temporal and situational context. The ability to do so places us within our memories, making them autobiographical. Failures of episodic memory are a hallmark of normal aging and neurodegenerative disease. The first aim of the empirical studies is to assess the influence of prior knowledge and memories of past events on people's ability to encode and retrieve newly learned information. A second aim is to examine how repetition influences memory at a mechanistic level and to explain why repetitions are most beneficial for memories that are widely distributed in time.  In addressing both aims, the investigators  will use and assess the context maintenance and retrieval model, using neural network models of how temporal context is represented in memory, how it evolves through experience, and how it interacts with semantic context and source context in the formation and retrieval of associative information.<br/><br/>Advancing the understanding of human learning and memory has implications for the diagnosis and eventual treatment of disease-related memory impairments such as Alzheimer's disease and other dementias. The work may also impact instructional technology and educational theory and practice."
"1065397","RI: Medium: Collaborative Research: Learning Representations of Language for Domain Adaptation","IIS","ROBUST INTELLIGENCE","04/01/2011","02/20/2014","Alexander Yates","PA","Temple University","Continuing grant","Tatiana D. Korelsky","03/31/2016","$705,982.00","Yuhong Guo","yates@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7495","7924, 9251","$0.00","Supervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts.  A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and for the inability to generalize to previously unseen words. <br/><br/>This project is the first to systematically investigate representation-learning as a technique for improving performance on domain adaptation.  It explores latent-variable language models ? including Factorial Hidden Markov Models, dependency parsing models, and deep architectures ? as techniques for extracting novel features from text.  The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier.  The project also explores novel procedures for training a language model, which incorporate Web-scale ngram statistics as substitutes for standard statistics used in unsupervised training.<br/><br/>Language users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology.  By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality.  For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts.  By involving the diverse student bodies at Temple University and Philadelphia-area high schools, the project helps to broaden participation in computer science research by underrepresented groups."
"1034471","Empowering the visually impaired by understanding links between tactility and properties of surfaces","CBET","Disability & Rehab Engineering","01/01/2011","08/25/2010","Christian Schwartz","TX","Texas A&M Engineering Experiment Station","Standard Grant","Ted A. Conway","12/31/2012","$250,983.00","","cris1@iastate.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","ENG","5342","010E","$0.00","In blind or visually impaired (BVI) individuals, a primary means of delivering information is through tactile channels such as Braille and raised-line illustrations. However, these systems present complications in translating non-tactile information to these forms, and also result in cumbersome and space-consuming materials for instruction and communication of technical and artistic ideas. This is a considerable hindrance to the ability of BVI persons to fully participate in engineering and the sciences, which rely heavily on the presentation and manipulation of graphical information during instruction and peer discussion. A revolutionary improvement in technology is required to address this issue. Currently, there is a lack of fundamental science to address how surfaces can be manipulated to convey tactile information. Thus, there is a critical need to better understand the causative relationships between surface properties (mechanical, textural, thermal, chemical) and the resulting tactile attributes of surfaces (texture, softness, abrasiveness, etc.), as well as a need to train engineers to objectively consider tactility and other sensory attributes during design. Addressing these needs will have a transformative effect not only on the materials science-based understanding of human interaction with surfaces, but also on the paradigms of education and professional occupation of the visually impaired. The objectives of this proposed work are<br/>to test the hypotheses that: 1. Surface tactility can be described quantitatively by a collection of objective tactile descriptors and associated scores, 2. Objective causal relationships exist that relate tactile attributes to engineering-based surface properties, 3. Tactility can be used with a problem-based learning approach to educate engineers to address sensory<br/>design goals, as well as attract BVI students to study engineering and science in college.<br/>This work will employ a thorough experimental approach. Quantitative Descriptive Analysis (QDA) will be used with human evaluators to meticulously identify and quantify the individual tactile descriptors of both textile and solid polymer surfaces. Surfaces will also be analyzed by a number of methods including tribological testing, dynamic mechanical analysis, and surface topography measurement. Statistical and neural network techniques will be employed to identify and investigate relationships between the tactile descriptors and the surface property values. The research tasks will be closely integrated with the educational activities of this work through a number of innovative mechanisms.<br/>Intellectual Merit. This work is novel and transformative because it will be the first broad engineering based approach to understanding how to directly control and optimize the tactile feel of a variety of surfaces and thus produce insights into efficient means of conveying tactile information. This knowledge will also foster new fields of research that will bridge the gap between engineering and neuroscience. Control of tactility will revolutionize paradigms in such fields as haptic displays and textiles, but more importantly will open new doors into the possibilities for educational tools for BVI students as well as technologies to facilitate greater participation of these persons in engineering and science. There is a unique synergy with the biotribology and polymers background of the PI and the resources of his institution that maximize the probability of successful completion of this work.<br/>Broader Impacts. These results will have far-reaching impact on the fundamental understanding of the materials science based origins of tactility. The work will also serve as an instructional platform to expose students to design experiences that incorporate sensory assessment of engineered products. A problem based pedagogical approach, termed iSENSE, will be incorporated into graduate and capstone design courses. A goal of iSENSE is to enable engineering students to transcend discipline paradigms in order to address real-world design challenges that involve sensory assessment. iSENSE also targets the recruitment of middle- and high school BVI students to engineering and science, by their participation in PI-led enrichment courses that will incorporate tactility into engineering design inspired activities. Undergraduate engineering students will work on design projects to develop instructional technology to help facilitate the learning of the BVI students during these enrichment courses."
"1111765","RI: Large: Collaborative Research: 3D Structure and Motion in Dynamic Natural Scenes","IIS","COLLABORATIVE RESEARCH, ROBUST INTELLIGENCE","09/01/2011","01/24/2012","Bruno Olshausen","CA","University of California-Berkeley","Standard Grant","Kenneth Whang","08/31/2016","$704,999.00","","baolshausen@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7298, 7495","5936, 5979, 7495, 7925","$0.00","How does a vision system recover the 3-dimensional structure of the world -- such as the layout of the environment, surface shape, or object motion -- from the dynamic 2-dimensional images received by the sensors in a camera, or the retinas in our eyes?  This problem is fundamental to both computer and biological vision.  Computer vision has developed a variety of algorithms for estimating specific aspects of a scene such as the 3-dimensional positions of points whose correspondence over time can be established, but obtaining complete and robust scene representations for complex natural scenes and viewing conditions remains a challenge.  Biological vision systems have evolved impressive capabilities that suggest they have detailed and robust representations of the 3-dimensional world, but the neural representations that subserve this are poorly understood and neurophysiological studies thus far have provided little insight into the computational process.  This project will pursue an interdisciplinary approach by attempting the understand the universal principles that lie at the heart of 3-dimensional scene analysis.<br/><br/>Specifically, the project will  1) develop a novel class of computational models that recover and represent 3-dimensional scene information, 2) collect high quality video and range data of dynamic natural scenes under a variety of controlled motion conditions, and 3) test the perceptual implications of these models in psychophysical experiments.  The computational models will utilize non-linear decomposition - i.e., the ability to explain complex, time-varying images in terms of the non-linear interaction of multiple factors, such as the interaction between observer motion, the 3-dimensional scene layout, and surface patterns.  Importantly, the components of these models will be adapted to the statistics of natural motion patterns that arise from observer motion through natural scenes and movement around points of fixation.<br/><br/>The project is a collaboration between three laboratories that have played a leading role in developing theoretical models of natural image statistics, visual neural representations, and perceptual processes.  The investigators seek to combine their efforts to develop new models, data sets, and characterizations of 3-dimensional natural scene structure that go beyond previous studies of natural image statistics, and that can be tested in neurophysiological and psychophysical experiments.  This project has the potential to bring about fundamental advances in neuroscience, visual perception, and computer vision by developing new classes of models that robustly infer representations of the 3-dimensional natural environment.  It will create a set of high quality databases that will be made available to help other investigators study these issues.  It will also open up new possibilities for generating realistic stimuli that can guide novel investigations of neural representation and processing."
"1111654","RI: Large: Collaborative Research: 3D Structure and Motion in Dynamic Natural Scenes","IIS","Robust Intelligence","09/01/2011","07/31/2014","Michael Lewicki","OH","Case Western Reserve University","Continuing Grant","Kenneth Whang","08/31/2017","$815,889.00","","michael.lewicki@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","7495","7925","$0.00","How does a vision system recover the 3-dimensional structure of the world -- such as the layout of the environment, surface shape, or object motion -- from the dynamic 2-dimensional images received by the sensors in a camera, or the retinas in our eyes?  This problem is fundamental to both computer and biological vision.  Computer vision has developed a variety of algorithms for estimating specific aspects of a scene such as the 3-dimensional positions of points whose correspondence over time can be established, but obtaining complete and robust scene representations for complex natural scenes and viewing conditions remains a challenge.  Biological vision systems have evolved impressive capabilities that suggest they have detailed and robust representations of the 3-dimensional world, but the neural representations that subserve this are poorly understood and neurophysiological studies thus far have provided little insight into the computational process.  This project will pursue an interdisciplinary approach by attempting the understand the universal principles that lie at the heart of 3-dimensional scene analysis.<br/><br/>Specifically, the project will  1) develop a novel class of computational models that recover and represent 3-dimensional scene information, 2) collect high quality video and range data of dynamic natural scenes under a variety of controlled motion conditions, and 3) test the perceptual implications of these models in psychophysical experiments.  The computational models will utilize non-linear decomposition - i.e., the ability to explain complex, time-varying images in terms of the non-linear interaction of multiple factors, such as the interaction between observer motion, the 3-dimensional scene layout, and surface patterns.  Importantly, the components of these models will be adapted to the statistics of natural motion patterns that arise from observer motion through natural scenes and movement around points of fixation.<br/><br/>The project is a collaboration between three laboratories that have played a leading role in developing theoretical models of natural image statistics, visual neural representations, and perceptual processes.  The investigators seek to combine their efforts to develop new models, data sets, and characterizations of 3-dimensional natural scene structure that go beyond previous studies of natural image statistics, and that can be tested in neurophysiological and psychophysical experiments.  This project has the potential to bring about fundamental advances in neuroscience, visual perception, and computer vision by developing new classes of models that robustly infer representations of the 3-dimensional natural environment.  It will create a set of high quality databases that will be made available to help other investigators study these issues.  It will also open up new possibilities for generating realistic stimuli that can guide novel investigations of neural representation and processing."
"1111328","RI: Large: Collaborative Research: 3D Structure and Motion in Dynamic Natural Scenes","IIS","ROBUST INTELLIGENCE","09/01/2011","08/26/2011","Wilson Geisler","TX","University of Texas at Austin","Standard Grant","Kenneth C. Whang","08/31/2016","$302,048.00","","w.geisler@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495","7925","$0.00","How does a vision system recover the 3-dimensional structure of the world -- such as the layout of the environment, surface shape, or object motion -- from the dynamic 2-dimensional images received by the sensors in a camera, or the retinas in our eyes?  This problem is fundamental to both computer and biological vision.  Computer vision has developed a variety of algorithms for estimating specific aspects of a scene such as the 3-dimensional positions of points whose correspondence over time can be established, but obtaining complete and robust scene representations for complex natural scenes and viewing conditions remains a challenge.  Biological vision systems have evolved impressive capabilities that suggest they have detailed and robust representations of the 3-dimensional world, but the neural representations that subserve this are poorly understood and neurophysiological studies thus far have provided little insight into the computational process.  This project will pursue an interdisciplinary approach by attempting the understand the universal principles that lie at the heart of 3-dimensional scene analysis.<br/><br/>Specifically, the project will  1) develop a novel class of computational models that recover and represent 3-dimensional scene information, 2) collect high quality video and range data of dynamic natural scenes under a variety of controlled motion conditions, and 3) test the perceptual implications of these models in psychophysical experiments.  The computational models will utilize non-linear decomposition - i.e., the ability to explain complex, time-varying images in terms of the non-linear interaction of multiple factors, such as the interaction between observer motion, the 3-dimensional scene layout, and surface patterns.  Importantly, the components of these models will be adapted to the statistics of natural motion patterns that arise from observer motion through natural scenes and movement around points of fixation.<br/><br/>The project is a collaboration between three laboratories that have played a leading role in developing theoretical models of natural image statistics, visual neural representations, and perceptual processes.  The investigators seek to combine their efforts to develop new models, data sets, and characterizations of 3-dimensional natural scene structure that go beyond previous studies of natural image statistics, and that can be tested in neurophysiological and psychophysical experiments.  This project has the potential to bring about fundamental advances in neuroscience, visual perception, and computer vision by developing new classes of models that robustly infer representations of the 3-dimensional natural environment.  It will create a set of high quality databases that will be made available to help other investigators study these issues.  It will also open up new possibilities for generating realistic stimuli that can guide novel investigations of neural representation and processing."
"1111544","SHB: Large: Collaborative Research: Companionbots for Proactive Therapeutic Dialog on Depression","IIS","INFORMATION TECHNOLOGY RESEARC","09/01/2011","09/26/2013","Wayne Ward","CO","Boulder Language Technologies","Standard Grant","Sylvia J. Spengler","08/31/2015","$282,389.00","Daniel Bolanos","wward@bltek.com","2690 Center Green Ct S Ste 200","Boulder","CO","803015406","3035799605","CSE","1640","1640, 7925, 8018","$0.00","This collaborative research investigates a new class of dialog-based, home robotic healthcare assistants to facilitate a new level of in-home, real-time care to elderly and depressed patients, providing lower total costs and higher quality of life. An emotive, physical avatar, called a companionbot, which possesses the ability to engage humans in a way that is unobtrusive and suspends disbelief will be built in this project. The companionbot will be an integration of human language technology, vision, other sensory processing and emotive robotic technology to proactively recognize and dialog with isolated and elderly patients suffering from depression. The companionbot will utilize proactive or companionable dialog based on the context with users suffering from depression. This will require the first multimodal integration of a user model, environment model, and temporal processing with spoken dialog understanding and generation to produce dynamic dialog and emotive interaction, beyond the traditional scripted dialog and emotion. Object recognition, facial expression recognition, and human activity recognition will augment natural language processing to provide current and historical context important to dynamic dialog. <br/><br/>A team of skilled researchers, assembled from the University of Colorado Boulder, University of Denver, CU Anschutz Medical Campus, and Boulder Language Technologies, will work together to achieve the project goals. The investigators will use the companionbots as a tool to run clinical trials to monitor and dialog with their partners to detect signs of physical and emotional deterioration. The companionbots can then notify remote caregivers, as necessary, provide warnings, reminders, life coaching and therapeutic dialog, extending independence and quality of life, and even saving lives. The other benefits of such a system include continuous, annotated data to improve doctor-patient interaction and analysis, real-time monitoring of mental state for remote healthcare providers and, ultimately, real-time intervention as part of a comprehensive treatment strategy.<br/><br/>In addition, this research will promote both STEM practice and research education at the graduate and the undergraduate levels of the affiliated institutions. The companionbots are ideal for teaching the next generation of engineers and scientists in critical emerging technologies, as they permit either a deep focus on specific topics or an interdisciplinary perspective while providing a simple high-level interface to manage everything else. Furthermore, the project will develop related educational material to support others and will provide public outreach to K-12 classes in the area."
"1143703","EAGER: Constructing, Indexing, and Searching Super-Enriched Document Representations in the Cloud","IIS","Info Integration & Informatics, ","09/01/2011","02/23/2012","Eduard Hovy","CA","University of Southern California","Standard Grant","Sylvia Spengler","12/31/2012","$250,000.00","","hovy@cs.cmu.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7364, K155","170E, 7364, 7916","$0.00","There are billions of new digital documents created around the world every day. Examples include emails, blog posts, legal documents, and news articles. To enable effective information management, many of these documents are processed by information retrieval systems, such as desktop search tools or Web search engines. Most existing technologies represent documents digitally. To a computer, these representations are nothing more than a sequence of bits, completely devoid of any explicit meaning. Since most modern search engines utilize such basic representations, they often fail to properly account for the meaning of the words found in the documents, thereby diminishing the quality of their results. Despite the importance of this fundamental problem, there have been surprisingly few attempts to build, and subsequently search, document representations that encode the deeply rich meaning of text, especially for data sets that contain millions or billions of text documents.<br/><br/>This research investigates how to automatically construct, index, and search next-generation super-enriched document representations. The approach relies on the careful integration of traditional text representations with natural language processing-based sources (e.g., named entities, synonyms, and paraphrases), rich knowledge sources (e.g., Wikipedia and Freebase), contextual sources, and other value-added sources of content. Constructing such representations for large document collections requires computationally intensive batch processing to mine, aggregate, and join data across disparate sources. To overcome these challenges, a scalable, massively distributed cloud computing solution is adopted. The resulting enriched document representations can be effectively applied to a wide variety of information retrieval, natural language processing, and data mining tasks."
"1065228","RI: Medium: Collaborative Research: Teaching Computers to Follow Verbal Instructions","IIS","Robust Intelligence","09/01/2011","04/08/2013","Marie desJardins","MD","University of Maryland Baltimore County","Standard Grant","Weng-keen Wong","08/31/2016","$311,743.00","","mariedj@cs.umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7495","7495, 7924, 9251","$0.00","The goal of this research is to develop techniques that will permit a computer or robot to learn from examples to carry out multipart tasks specified in natural language on behalf of a user.  It will study each of these components in isolation, but a significant focus will be on integrating them into a coherent system.  The project will also leverage this technology to provide an entry point to educate non- or pre-computer science students about the capabilities and utility of computers as tools.<br/><br/>Our approach uses three main subcomponents, each of which requires innovative research to solve its portion of the overall problem.  In addition, the integrated architecture is a novel contribution of this work.  The three components are (1) recognizing intention from observed behavior using extensions of inverse reinforcement learning, (2) translating instructions to task specifications using novel techniques in the area of natural language processing, and (3) creating generalized task specifications to match user intentions using probabilistic methods for creating and managing abstractions.<br/><br/>The goal of the work is develop technology for an improved ability for human users to interact with intelligent agents, the incorporation of novel AI research insights and activities into education and outreach activities, and the development of resources for the AI educator community.  In addition to permitting intelligent agents to be developed and trained in the future for a broad range of complex application domains, the interactive agents that we will develop will be used for outreach and student learning."
"1049682","Detecting Local Earthquakes in a Noisy Continental Margin Environment","OCE","Marine Geology and Geophysics","02/15/2011","02/08/2011","Anne Trehu","OR","Oregon State University","Standard Grant","Bilal U. Haq","01/31/2013","$103,966.00","","trehu@coas.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","GEO","1620","0000, 1620, OTHR","$0.00","Assessing earthquake risk due to seismicity along the Cascadia margin from northern California to southern British Columbia is a matter of great public interest. Studies of regional seismicity recorded by arrays of seismographs are a primary tool for this purpose, but to date such studies have been largely limited to onshore arrays. In the upcoming Cascadia project, onshore instrumentation will be complemented by deployments of 60 or more Ocean Bottom Seismographs (OBS) off the Cascadia coast for several years. A modest deployment of OBSs off the Oregon coast in 2007-2009 has demonstrated the great difficulty of separating relevant seismic events in OBS data from impulsive signals of probable biological origin. This project seeks to develop computer automated methods for separating seismic signals from extraneous signals in the OBS data, particularly for instruments located in shallow water near the coast. The project has a number of broader impacts, but by far the most significant is the very high societal relevance of developing these techniques for studies of seismicity and seismic hazards in general, and for the Cascadia project in particular."
"1111568","SHB: Large: Collaborative Research: Companionbots for Proactive Therapeutic Dialog on Depression","IIS","Information Technology Researc, Smart and Connected Health","09/01/2011","05/15/2014","Mohammad Mahoor","CO","University of Denver","Standard Grant","Sylvia Spengler","08/31/2017","$468,122.00","","mmahoor@du.edu","2199 S. University Blvd.","Denver","CO","802104711","3038712000","CSE","1640, 8018","1640, 7925, 8018, 9251","$0.00","This collaborative research investigates a new class of dialog-based, home robotic healthcare assistants to facilitate a new level of in-home, real-time care to elderly and depressed patients, providing lower total costs and higher quality of life. An emotive, physical avatar, called a companionbot, which possesses the ability to engage humans in a way that is unobtrusive and suspends disbelief will be built in this project. The companionbot will be an integration of human language technology, vision, other sensory processing and emotive robotic technology to proactively recognize and dialog with isolated and elderly patients suffering from depression. The companionbot will utilize proactive or companionable dialog based on the context with users suffering from depression. This will require the first multimodal integration of a user model, environment model, and temporal processing with spoken dialog understanding and generation to produce dynamic dialog and emotive interaction, beyond the traditional scripted dialog and emotion. Object recognition, facial expression recognition, and human activity recognition will augment natural language processing to provide current and historical context important to dynamic dialog. <br/><br/>A team of skilled researchers, assembled from the University of Colorado Boulder, University of Denver, CU Anschutz Medical Campus, and Boulder Language Technologies, will work together to achieve the project goals. The investigators will use the companionbots as a tool to run clinical trials to monitor and dialog with their partners to detect signs of physical and emotional deterioration. The companionbots can then notify remote caregivers, as necessary, provide warnings, reminders, life coaching and therapeutic dialog, extending independence and quality of life, and even saving lives. The other benefits of such a system include continuous, annotated data to improve doctor-patient interaction and analysis, real-time monitoring of mental state for remote healthcare providers and, ultimately, real-time intervention as part of a comprehensive treatment strategy.<br/><br/>In addition, this research will promote both STEM practice and research education at the graduate and the undergraduate levels of the affiliated institutions. The companionbots are ideal for teaching the next generation of engineers and scientists in critical emerging technologies, as they permit either a deep focus on specific topics or an interdisciplinary perspective while providing a simple high-level interface to manage everything else. Furthermore, the project will develop related educational material to support others and will provide public outreach to K-12 classes in the area."
"1111953","SHB: Large: Collaborative Research: Companionbots for Proactive Therapeutic Dialog on Depression","IIS","INFORMATION TECHNOLOGY RESEARC, Smart and Connected Health","09/01/2011","09/02/2011","Rodney Nielsen","CO","University of Colorado at Boulder","Standard Grant","Sylvia J. Spengler","12/31/2012","$1,245,407.00","","rodney.nielsen@unt.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1640, 8018","1640, 7925, 8018","$0.00","This collaborative research investigates a new class of dialog-based, home robotic healthcare assistants to facilitate a new level of in-home, real-time care to elderly and depressed patients, providing lower total costs and higher quality of life. An emotive, physical avatar, called a companionbot, which possesses the ability to engage humans in a way that is unobtrusive and suspends disbelief will be built in this project. The companionbot will be an integration of human language technology, vision, other sensory processing and emotive robotic technology to proactively recognize and dialog with isolated and elderly patients suffering from depression. The companionbot will utilize proactive or companionable dialog based on the context with users suffering from depression. This will require the first multimodal integration of a user model, environment model, and temporal processing with spoken dialog understanding and generation to produce dynamic dialog and emotive interaction, beyond the traditional scripted dialog and emotion. Object recognition, facial expression recognition, and human activity recognition will augment natural language processing to provide current and historical context important to dynamic dialog. <br/><br/>A team of skilled researchers, assembled from the University of Colorado Boulder, University of Denver, CU Anschutz Medical Campus, and Boulder Language Technologies, will work together to achieve the project goals. The investigators will use the companionbots as a tool to run clinical trials to monitor and dialog with their partners to detect signs of physical and emotional deterioration. The companionbots can then notify remote caregivers, as necessary, provide warnings, reminders, life coaching and therapeutic dialog, extending independence and quality of life, and even saving lives. The other benefits of such a system include continuous, annotated data to improve doctor-patient interaction and analysis, real-time monitoring of mental state for remote healthcare providers and, ultimately, real-time intervention as part of a comprehensive treatment strategy.<br/><br/>In addition, this research will promote both STEM practice and research education at the graduate and the undergraduate levels of the affiliated institutions. The companionbots are ideal for teaching the next generation of engineers and scientists in critical emerging technologies, as they permit either a deep focus on specific topics or an interdisciplinary perspective while providing a simple high-level interface to manage everything else. Furthermore, the project will develop related educational material to support others and will provide public outreach to K-12 classes in the area."
"1126707","MRI: Acquisition of a Biopotential Measurement System","ECCS","MAJOR RESEARCH INSTRUMENTATION","10/01/2011","09/20/2011","Yih-Choung Yu","PA","Lafayette College","Standard Grant","eyad abed","09/30/2014","$142,309.00","Lisa Gabel, Luis Schettino","yuy@lafayette.edu","High Street","Easton","PA","180421768","6103305029","ENG","1189","1189, 8028","$0.00","Research Objectives and Approaches: The objective of this research is to bridge engineering and natural sciences in an inter-disciplinary collaboration, which will enhance course instruction and research projects across disciplines. The approach is to acquire a state-of-art biopotential instrumentation system that allows the PIs to enhance their collaboration in more advanced research projects with students.<br/><br/>Intellectual Merit: The intellectual merit of the proposal is centered on enhancing a facility that will foster the interaction between the faculty and students in science and engineering to plan and conduct research. This interaction will allow its members to work as a team of scientists and engineers across disciplines, and to interpret and communicate research findings in both peer-reviewed journal publications and at national conferences.<br/><br/>Broader Impacts: The broader impacts will be to effectively integrate education and research; enhance scientific and technical understanding, and engage in research that can benefit society. The outcome of this research is aimed at developing new devices and/or new treatments to improve the quality of life of individuals suffering from neurological disorders. This collaboration is being conducted by under-represented minorities at different stages of professional development, who have demonstrated a track record of engaging under-represented minority students in research in the natural sciences and engineering. This equipment will be utilized to continue improving the research projects from their successful collaborations and allow students to conduct research and independent study projects. Through a data-sharing plan, peer-institutions will be able to access the data collected with this equipment for other projects."
"1115719","RI: Small: Debugging Machine Visual Recognition via Humans in the Loop","IIS","ROBUST INTELLIGENCE","09/01/2011","08/26/2011","Devi Parikh","IL","Toyota Technological Institute at Chicago","Standard Grant","Jie Yang","06/30/2013","$150,000.00","","parikh@vt.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7495","7923","$0.00","The problem of visual recognition is fundamental towards the goal of automatic image understanding. While a large number of efforts have been made in the computer vision community, machine performance at these tasks remains significantly inferior to human ability. <br/>The overarching goal of this project is to leverage the best known visual recognition system - the human visual recognition system. This project employs a ""Human Debugging"" paradigm to replace various components of a machine vision pipeline with human subjects, and examines the resultant effect on recognition performance. Meaningful comparisons provide valuable insights and pinpoint aspects of the machine vision pipeline that are performance bottlenecks and require future research efforts. Specifically, the project considers the problems of image classification and object detection, and explores the roles of local and global information, as well part-detection, spatial modeling and contextual reasoning (including non-maximal suppression) for these problems respectively. <br/>This project touches on a wide range of problems in visual recognition including object recognition, scene recognition and object detection. This novel paradigm of identifying weak links in computational models via humans in the loop is also applicable to other vision problems, as well as other sub-fields in AI. By sharing all collected data and results, and through organized conferences and workshops, this project will initiate and fuel a dialogue with the research community about leveraging humans to advance computer vision. More broadly, this work encourages the involvement of young women and undergraduate students in computer science research."
"1062404","COLLABORATIVE RESEARCH: ABI Development:  Ontology-enabled reasoning across phenotypes from evolution and model organisms","DBI","ADVANCES IN BIO INFORMATICS","07/01/2011","12/14/2015","Todd Vision","NC","University of North Carolina at Chapel Hill","Continuing Grant","Peter McCartney","06/30/2017","$1,305,787.00","Hilmar Lapp","tjv@bio.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","BIO","1165","1165, 1228, 9178, 9179","$0.00","Collaborative grants are awarded to the University of South Dakota and the University of North Carolina to develop ontology-driven tools for machine reasoning over large volumes of phenotype data.   Human-readable descriptions of ""phenotypic"" properties such as anatomy and behavior are not well-suited to computational analysis.  Yet, in evolutionary biology, genetics and development, computational assistance is necessary to discover patterns within the enormous volumes of descriptive phenotype data that are being reported in the literature and in online databases.  Ontologies are structured, controlled vocabularies that can be applied to collections of descriptive data to permit logical reasoning to be used. Using the evolutionary transition from fins to limbs as a test system, this project will develop ontologically-aware software that allows users to discover similar sets of phenotypes for different taxa or mutant genes within large and diverse datasets. A fast semantic similarity engine will be developed to allow searches for evolutionary transitions and mutant genes characterized by similar phenotypic profiles.  An ontological framework for reasoning over homology will be developed to allow rigorous reasoning over evolutionary diverse lineages.  Natural language processing tools will be developed to improve upon the efficiency of mining phenotype data from the literature and improving data consistency. This suite of tools will be tested on a large number of skeletal phenotypes from diverse fossil and modern vertebrates.  Taxonomic and anatomical ontologies for vertebrates will be augmented and hypotheses of anatomical homology formally encoded.  The ontologies and software tools, together with phenotypes extracted from the vertebrate systematic literature, will be integrated in the knowledgebase with genetic and phenotype data from three vertebrate model organisms: zebrafish (Danio rerio),  African clawed frog (Xenopus laevis), and mouse (Mus musculus).  The knowledge base will be exposed to generic reasoners using semantic web standards.  The system will be validated by its success in retrieving candidate genes for the well-studied vertebrate fin-limb transition and other major events in skeletal evolution.  <br/><br/>The evolutionary breadth of the test data requires the development of a rigorous framework for reasoning over hypotheses of homology.  Another goal is to develop and evaluate natural language processing tools for efficiently capturing ontological descriptions of phenotype from the descriptions available in the published literature.  The suite of tools will be validated by recovering developmental genetic pathways that underlie the evolutionary transition from fin to limb in vertebrates, and refined by iterative testing with domain bioinformaticians on the project and biologists from the broader user community.   <br/><br/><br/>A broad community of users will participate through the lifecycle of this project in the development of community standards and resources for the interoperability and computability of phenotypic knowledge. This will be achieved through workshops, usability testing sessions, and coordination with key research networks.  Stakeholder ownership will be enhanced by rapid and open release of a variety of products that we anticipate to be of immediate and enduring value to the greater biology community, including tools for streamlining data curation and performing large-scale semantic similarity searches, high quality vertebrate taxonomy and anatomy ontologies, and standards for reasoning over homology.   We will provide a unique training environment for students, postdocs and summer interns, including Native Americans through outreach at the University of South Dakota and minority and female students though a collaboration with Project Exploration at the University of Chicago.  Project progress and outcomes will be disseminated through both traditional and online outlets for scholarly communication (including blog posts and mailing lists); the primary web presence will be at https://www.phenoscape.org/wiki/."
"1062542","Collaborative research: ABI Development: Ontology-enabled reasoning across phenotypes from evolution and model organisms","DBI","ADVANCES IN BIO INFORMATICS, Unallocated Program Costs","07/01/2011","01/26/2018","Wasila Dahdul","SD","University of South Dakota Main Campus","Continuing Grant","Jennifer Weller","06/30/2018","$1,947,267.00","Paul Sereno, Monte Westerfield, David Blackburn, Wasila Dahdul","wdahdul@uci.edu","414 E CLARK ST","vermillion","SD","570692307","6056775370","BIO","1165, 9199","1165, 1228, 7433, 9150, 9178, 9179","$0.00","Collaborative grants are awarded to the University of South Dakota and the University of North Carolina to develop ontology-driven tools for machine reasoning over large volumes of phenotype data.   Human-readable descriptions of ""phenotypic"" properties such as anatomy and behavior are not well-suited to computational analysis.  Yet, in evolutionary biology, genetics and development, computational assistance is necessary to discover patterns within the enormous volumes of descriptive phenotype data that are being reported in the literature and in online databases.  Ontologies are structured, controlled vocabularies that can be applied to collections of descriptive data to permit logical reasoning to be used. Using the evolutionary transition from fins to limbs as a test system, this project will develop ontologically-aware software that allows users to discover similar sets of phenotypes for different taxa or mutant genes within large and diverse datasets. A fast semantic similarity engine will be developed to allow searches for evolutionary transitions and mutant genes characterized by similar phenotypic profiles.  An ontological framework for reasoning over homology will be developed to allow rigorous reasoning over evolutionary diverse lineages.  Natural language processing tools will be developed to improve upon the efficiency of mining phenotype data from the literature and improving data consistency. This suite of tools will be tested on a large number of skeletal phenotypes from diverse fossil and modern vertebrates.  Taxonomic and anatomical ontologies for vertebrates will be augmented and hypotheses of anatomical homology formally encoded.  The ontologies and software tools, together with phenotypes extracted from the vertebrate systematic literature, will be integrated in the knowledgebase with genetic and phenotype data from three vertebrate model organisms: zebrafish (Danio rerio),  African clawed frog (Xenopus laevis), and mouse (Mus musculus).  The knowledge base will be exposed to generic reasoners using semantic web standards.  The system will be validated by its success in retrieving candidate genes for the well-studied vertebrate fin-limb transition and other major events in skeletal evolution.  <br/><br/>The evolutionary breadth of the test data requires the development of a rigorous framework for reasoning over hypotheses of homology.  Another goal is to develop and evaluate natural language processing tools for efficiently capturing ontological descriptions of phenotype from the descriptions available in the published literature.  The suite of tools will be validated by recovering developmental genetic pathways that underlie the evolutionary transition from fin to limb in vertebrates, and refined by iterative testing with domain bioinformaticians on the project and biologists from the broader user community.   <br/><br/><br/>A broad community of users will participate through the lifecycle of this project in the development of community standards and resources for the interoperability and computability of phenotypic knowledge. This will be achieved through workshops, usability testing sessions, and coordination with key research networks.  Stakeholder ownership will be enhanced by rapid and open release of a variety of products that we anticipate to be of immediate and enduring value to the greater biology community, including tools for streamlining data curation and performing large-scale semantic similarity searches, high quality vertebrate taxonomy and anatomy ontologies, and standards for reasoning over homology.   We will provide a unique training environment for students, postdocs and summer interns, including Native Americans through outreach at the University of South Dakota and minority and female students though a collaboration with Project Exploration at the University of Chicago.  Project progress and outcomes will be disseminated through both traditional and online outlets for scholarly communication (including blog posts and mailing lists); the primary web presence will be at https://www.phenoscape.org/wiki/."
"1139844","A Workshop on Restructuring Adjectives in WordNet","IIS","ROBUST INTELLIGENCE","09/01/2011","06/20/2011","Christiane Fellbaum","NJ","Princeton University","Standard Grant","Tatiana Korelsky","08/31/2012","$20,000.00","","fellbaum@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7495","7495, 7556","$0.00","The workshop gathers developers and users of lexical resources, corpus and computational linguists and researchers in natural language processing to discuss a targeted restructuring of the adjectives in the lexical database WordNet.  Specific proposals for replacing a subset of the current clustering of adjectives around antonyms with ordered scales reflecting the relative intensity of dimensional adjectives, such as ""big"", ""huge"" and ""gigantic"", are presented along with preliminary work demonstrating the feasibility of corpus-based construction of scales by means of lexical-semantic patterns and their potential benefits for NLP.  Discussion topics include (1) the principal benefits of encoding scalar properties for applications including word sense disambiguation, textual entailment and language pedagogy; (2) suitable corpora for extracting data for scale construction; (3) limitations of the recently-developed AdjScales method and alternative or complementary methods for extracting scalar properties; and (4) modeling of scalar adjectives in WordNet.  Participants evaluate the proposed restructuring of adjectives for its feasibility, value and relevance to their own work and its potential for future research and applications.  A report including the presentations, discussions and recommendations of the group will be prepared and freely disseminated via the WordNet website. <br/><br/>The directions for targeted future developments of the widely used WordNet database as spelled out and agreed upon by representatives from a broad expert community assure significant consequences for research and applications in language technology and pedagogy.  For a post-doctoral fellow and a graduate student the workshop provides a unique opportunity to interact with experts in the field."
"1065195","RI: Medium: Collaborative Research: Teaching Computers to Follow Verbal Instructions","IIS","ROBUST INTELLIGENCE","09/01/2011","08/30/2011","Michael Littman","NJ","Rutgers University New Brunswick","Standard Grant","Todd Leen","05/31/2014","$703,922.00","Smaranda Muresan","mlittman@cs.brown.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7495","7924","$0.00","The goal of this research is to develop techniques that will permit a computer or robot to learn from examples to carry out multipart tasks specified in natural language on behalf of a user.  It will study each of these components in isolation, but a significant focus will be on integrating them into a coherent system.  The project will also leverage this technology to provide an entry point to educate non- or pre-computer science students about the capabilities and utility of computers as tools.<br/><br/>Our approach uses three main subcomponents, each of which requires innovative research to solve its portion of the overall problem.  In addition, the integrated architecture is a novel contribution of this work.  The three components are (1) recognizing intention from observed behavior using extensions of inverse reinforcement learning, (2) translating instructions to task specifications using novel techniques in the area of natural language processing, and (3) creating generalized task specifications to match user intentions using probabilistic methods for creating and managing abstractions.<br/><br/>The goal of the work is develop technology for an improved ability for human users to interact with intelligent agents, the incorporation of novel AI research insights and activities into education and outreach activities, and the development of resources for the AI educator community.  In addition to permitting intelligent agents to be developed and trained in the future for a broad range of complex application domains, the interactive agents that we will develop will be used for outreach and student learning."
"1136370","Group Travel Grant for the Doctoral Consortium at the IEEE Conference on Computer Vision and Pattern Recognition","IIS","ROBUST INTELLIGENCE","04/15/2011","04/15/2011","Walter Scheirer","CO","University of Colorado at Colorado Springs","Standard Grant","Jie Yang","03/31/2012","$14,525.00","","wscheire@nd.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","7495","7495, 7556","$0.00","This travel grant supports students who are near graduation, or have recently graduated, to take part in a doctoral consortium at the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). The doctoral consortium highlights the work of these up and coming researchers, and pairs each student with a senior member of the computer vision community who serves as their mentor. The mentorship process provides each student with valuable feedback on their research, as well as meaningful career advice as the students move on to the next phase of their professional development. The doctoral consortium event aims to have representation from a diverse group of participants (in terms of gender, ethnic background, academic institution and geographic location). The travel grant ensures participation from a broad range of institutions across the country and gives visibility to a diverse population of students."
"1117381","AF: Small: Applied Algorithims: Tech Transfer from the Algorithims Toolbox II","CCF","ALGORITHMIC FOUNDATIONS","07/01/2011","06/08/2011","David Karger","MA","Massachusetts Institute of Technology","Standard Grant","Tracy J. Kimbrel","09/30/2015","$400,000.00","","karger@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7796, 7923, 7926","$0.00","Over the past decades, the field of algorithms has developed a toolbox of theoretical techniques that let computer systems to do more with less---to solve harder problems more quickly, using less memory, less communication with other computers, and less of a human user's assistance.  One measure of algorithms' research success has been the large number of implemented systems whose designs have been impacted by contributions from the algorithms community.  There are many more such successes waiting in the wings for individuals or groups who can draw the connection between an existing algorithmic technique and an existing applied problem.  Often, the biggest challenge is recognizing the connection between the practitioner's problem statement and the proper algorithmic solution techniques.  This research addresses the process of ``technology transfer'' from the algorithmic toolbox to other computer science domains.<br/><br/>The investigator is working closely with practitioners in various areas of computer science to identify computational problems whose efficient solution would advance their research agendas, dig through the theory toolbox to find techniques that, properly adapted, can be used to efficiently solve those problems, and assist in such adaptation.  Domains being addressed include natural language processing, detection of influence pathways in biological networks, traffic route planning that accounts for uncertain delays, network coding for efficient use of communication bandwidth, and efficient use of crowdsourced computation.  But rather than being driven by a particular problem domain, the investigator is interested in the overall process for applying theoretical work in algorithms to problems in the practical domain, and is always seeking new applied problems that can benefit from this approach.<br/><br/>Successful completion of the proposed work will contribute advancement to many different branches of computer science.  The contributions to other branches of computer science will, in turn, allow them to achieve their goals of broad impact on society.  The investigator also hopes to increase the general sense of connection between theoreticians and practitioners, yielding increased collaborations and successful applications of algorithms to theory beyond those made directly during this project.  The project will contribute to research training by continuing to employ large numbers of students with attention given to gender diversity."
"1059353","CI: ADDO-EN:  Significant Enhancement of the Exisitng Penn Discourse Treebank","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2011","03/15/2011","Aravind Joshi","PA","University of Pennsylvania","Standard Grant","Tatiana Korelsky","05/31/2013","$100,000.00","","joshi@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7359","7359","$0.00","Building large scale annotated resources is crucial for basic and applied research in Natural Language Processing (NLP). Our major long term goal of this project is to make very substantial extensions to an existing unique resource, the Penn Discourse Treebank (PDTB), developed under prior NSF support, augmenting it with a variety of new annotations as well as refining earlier annotations. Our proposed work involves conducting some new annotations and some pilot experiments to confirm the strategies for augmentation. A further goal is to bring together a cross section of potential users of this resource, first to acquaint them with the potential of this resource as well as to get their feedback for guiding further augmentations. Applications of PDTB for the task of summarization have already been made. Future applications are in the areas of information extraction, question-generation, and machine translation among others. On the theoretical side, our resource will prove useful in increased theoretical understanding of discourse structure of language."
"1153822","Learning Reading Strategies for Science Texts in a Gaming Environment: iSTART vs iTG","IIS","HCC-Human-Centered Computing, IIS Special Projects, TRUSTWORTHY COMPUTING","08/16/2011","08/30/2011","Danielle McNamara","AZ","Arizona State University","Continuing Grant","Ephraim Glinert","04/30/2012","$87,599.00","","dsmcnamara1@gmail.com","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7367, 7484, 7795","1707, 7367, 9215, HPCC","$0.00","It is well known that many students find science text challenging to comprehend. Students' reading ability is partly to blame. Reading problems become most apparent when the student is faced with a challenging text for which they have knowledge deficits. Science text, in particular, has many technical terms that are difficult to ground in everyday experience. Thus, there is a need for reading comprehension interventions to improve science comprehension. This project compares the effectiveness of two types of tutoring environments in improving high-school students' ability to understand challenging science text. Both environments contain the same pedagogical content, but present it differently: as a lesson or as a game. The first environment, developed and tested over the past 5 years, is an automated reading strategies tutor called iSTART (Interactive Strategy Training for Active Reading and Thinking) that uses animated pedagogical agents to deliver interactive instruction on self-explanation and reading strategies (comprehension monitoring, paraphrasing, generating inferences). Instruction occurs in three stages with each stage requiring increased interaction on the part of the learner. Results across a wide range of studies indicate that iSTART is highly effective in improving students' ability to understand challenging science text. While effective, iSTART can be somewhat unappealing to an average high-school student in extended practice situations. While students need extended practice to master the strategies, iSTART becomes monotonous over time. To increase students' engagement, an alternative version of iSTART practice will be developed that allows students to practice iSTART strategies in a game environment. iSTART-The Game (iTG) will present the same reading strategy practice to students, but will incorporate game-based principles to enhance engagement. This project examines whether a gaming environment for learning strategies for science text comprehension more effectively sustains students' attention and engagement during training, and thereby results in improved acquisition and mastery of these strategies. In the first year of funding, an automated reading strategy tutoring system that is framed in a gaming environment will be developed. In Year 2, pilot studies will be conducted to refine the system. In Year 3, practice using the strategies will be compared in three conditions: iTG, iSTART, and a control condition. Students will practice the strategies over a period of five additional sessions after the initial training. This experiment will provide information on the potential value of iTG over time and specifically, whether iTG engages students over repeated practice sessions. The potential interactive effects of individual differences such as prior science knowledge, reading skill, and motivational levels will also be examined.<br/><br/>This research will contribute to better understanding of engagement as a factor in learning gains. It is predicted by the investigators that many students who do not find the standard tutoring environment sufficiently engaging will significantly benefit from the alternative approach. Most importantly, this instructional intervention should be especially valuable for those students most at risk due to having lower ability and interest in science. This research will accomplish the goal of creating a test-bed learning environment intended to improve reader engagement and advance our understanding of the potential instructional gains from such environments. The project will contribute to our understanding of the relationship between game features and engagement. It will provide students with tools that help them more effectively meet the challenges of learning from difficult science texts. In addition, this research will contribute to our understanding of the roles that cognition and emotion play in fostering learning, the specific processes involved with learning from science texts, and the complex interplay of factors such as reading strategies, knowledge, reading skill, interest, and motivation."
"1115680","CGV: Small: Inverse Light Transport Under Femto-Photography and Transient Imaging","IIS","GRAPHICS & VISUALIZATION","08/01/2011","07/20/2011","Ramesh Raskar","MA","Massachusetts Institute of Technology","Standard Grant","Ephraim P. Glinert","07/31/2014","$499,999.00","","raskar@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7453","7453, 7923","$0.00","CGV: Small: Inverse Light Transport under Femto-Photography and Transient Imaging<br/>Raskar, Ramesh, Massachusetts Institute of Technology<br/><br/>How can you photograph objects beyond the line of sight? How can you recover bidirectional reflectance of materials from a single viewpoint? These seemingly impossible tasks are possible by considering the finite speed of light and using a new type of computational photography called, Femto-Photography. New advances in ultra-fast imaging provide tremendous new opportunities in modeling, representing and synthesizing light transport in computer graphics and computer vision. Research in computational photography and scene understanding will benefit by analyzing the transient response of the scene to extremely short duration active illumination. Traditional imaging uses steady-state response where the global illumination has reached an equilibrium state. The investigators are developing a new theoretical framework for transient light transport and are addressing inverse problems using time-resolved imaging. The investigators have recently developed the first physical demonstration of hidden geometry recovery.<br/><br/>The research aims to develop a new branch of computational imaging by developing a mathematical framework for studying higher dimensional light transport that exploits time-resolved imaging. This research brings ultra-fast imaging in the realm of computer graphics/vision and computational photography. The finely sampled time-dimension provides a range of research directions for modeling and measuring geometry and photometry of scenes that were previously considered beyond the reach of traditional machine vision. The techniques for time-resolved imaging exploit multiplexing, sparsity-exploiting reconstructions, state-space formulation, system identification methods and parameterized reflectance models in novel ways. Overall, the research pushes the boundaries of light transport based methods by an extra (time) dimension and hopes to show that forward and inverse problems in 5D light transport can inspire the next generation of imaging hardware and algorithms."
"1131883","US-German Collaboration:  Towards a Neural Theory of 3D Shape Perception","IIS","PERCEPTION, ACTION & COGNITION, COLLABORATIVE RESEARCH, CRCNS, ROBUST INTELLIGENCE","11/01/2011","07/24/2013","Steven Zucker","CT","Yale University","Continuing grant","Kenneth C. Whang","10/31/2015","$460,004.00","","steven.zucker@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7252, 7298, 7327, 7495","5936, 5979, 7327","$0.00","How the brain estimates the 3D shape of objects in our surroundings remains one of the most significant challenges in visual neuroscience. The information provided by the retina is fundamentally ambiguous, because many different combinations of 3D shape, illumination and surface reflectance are consistent with any given image.  Despite this ambiguity, the visual system is extremely adept at estimating 3D shape across a wide range of viewing conditions, something that no extant machine vision system can do. The long-term goal of the project is to develop a computational model in neural terms to explain how 3D shape is estimated in the primate visual system. It will build upon the responses of cells early in visual cortex (V1) and develop models of how they can be organized into mid-level configurations that specify 3D shape properties.  Importantly, the project will also measure human perception of 3D shape in a series of psychophysical experiments designed to test specific predictions, bringing together the complementary expertise of Roland W. Fleming (Giessen University: human perception, psychophysics) and Steven W. Zucker (Yale University: computational vision, computational neuroscience). The results should provide a deeper understanding of visual circuit properties in the ventral processing stream; they should provide models for 3D computer vision and graphics; and they may pave the way for the development of rehabilitation strategies for patients with visual deficits.<br/><br/>The basic approach starts with populations of neurons tuned to different orientations and seeks to understand how these provide basic information about local shape properties according to the principles of differential geometry.  Specifically, when 3D surfaces are projected onto the retina, the distorted gradients of shading and texture lead to highly structured patterns of local image orientation, or orientation fields, which can be inferred via circuits involving long-range horizontal connections.  The investigators seek to derive formal models showing how these networks can be organized to infer 3D surface properties. The specific approach is involves four stages: (i) modeling how the visual system obtains clean and reliable orientation fields from the outputs of model V1 cells through lateral interactions and feedback; (ii) establishing how local measurements are grouped into specific ""mid-level"" configurations to support the recovery of 3D shape properties (modeling V2 to V4); (iii) modeling how these low- and mid-level 2D measurements can be mapped into representations of 3D shape properties (V4 to IT); and (iv) modeling how grouping and global constraints can convert these shape estimates into global shape reconstructions (again V4 to IT). Targeted psychophysical experiments will complement all of the modeling and test specific predictions from it. The resulting stimuli will support next generation neurophysiological experiments. Although the above stages define a working strategy, dependencies among these stages should also provide a model of the feedforward/feedback projections that link different areas of cortex. The ultimate goal is a model that can correctly predict the errors, the successes, and the limits of human shape perception. <br/><br/>This project is jointly funded by Collaborative Research in Computational Neuroscience and the Office of International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF)."
"1215812","CAREER: Object Recognition with Hierarchical Models","IIS","Robust Intelligence","09/01/2011","12/09/2011","Pedro Felzenszwalb","RI","Brown University","Continuing grant","Jie Yang","02/28/2014","$166,503.00","","pff@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","1045, 1187, 7495, 9215, HPCC","$0.00","Abstract<br/><br/>Title: CAREER: Object Recognition with Hierarchical Models<br/><br/>PI: Pedro Felzenszwalb<br/><br/>Institution: University of Chicago<br/><br/>CAREER: Object Recognition with Hierarchical Models<br/><br/>Object recognition is one of the most important problems in computer vision.  While researchers have worked on this problem for over thirty  years, vision systems are still unable to recognize many common  objects in cluttered images.  The PI proposes to address this problem  by developing new hierarchical models and efficient search algorithms  for recognition.<br/><br/>Hierarchical models represent objects using parts which are themselves  defined in terms of subparts.  Moreover, the subparts may be  recursively defined in terms of smaller components.  This hierarchical  organization can efficiently encode important relationships among the  components that make up an object.  Another important property of  hierarchical models is that components can be shared among different  object models.  This is useful for being able to quickly recognize  which of many possible objects are present in an image.  It is also  important for learning models from small datasets.  Finally, in the  most general types of models the structure of an object may be  specified by a grammar instead of being fixed in advance.  The number  of parts that make up an object may be variable and there may be  choice among different parts that can go in a particular place.  All  of these aspects make hierarchical models incredibly expressive.<br/><br/>Algorithms for object recognition typically search over large spaces encoding the pose of an object, or over correspondences between model  features and features extracted from an image.  The PI will develop  efficient optimization algorithms for solving these problems.  This  will be accomplished by exploiting the structure of the search spaces  defined by general classes of hierarchical models.<br/><br/>Broader significance and importance: Object recognition has many  important practical applications, including in robotics, human-computer interaction, image retrieval, security systems and medical image analysis.  Research in object recognition can also play an important role in our understanding of human perception and   <br/>intelligence.  The proposed research will draw upon ideas from diverse  areas such as computer vision, theoretical computer science, natural  language understanding and mathematics.<br/><br/>URL: http://people.cs.uchicago.edu/~pff/hierarchical<br/><br/>"
"1054133","CAREER: Toward a General Framework for Words and Pictures","IIS","ROBUST INTELLIGENCE","06/01/2011","06/03/2013","Tamara Berg","NY","SUNY at Stony Brook","Continuing grant","Jie Yang","06/30/2014","$269,605.00","","tlberg@cs.unc.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7495","1045, 1187","$0.00","Pictures convey a visual description of the world directly to their viewers. Computer vision strives to design algorithms to extract the underlying world state captured in the camera's eye, with an overarching goal of general computational image understanding. To date much vision research has approached image understanding by focusing on object detection, only one perspective on the image understanding problem.  This project looks at an additional, complimentary way to collect information about the visual world -- by directly analyzing the enormous amount of visually descriptive text on the web to reveal what information is useful to attach to, and extract from pictures. This project presents a comprehensive research program geared toward modeling and exploiting the complimentary nature of words and pictures. One main goal is studying the connection between text and images to learn about depiction -- communication of meaning through pictures. This goal is addressed through 3 broad challenges: 1) Developing a richer vocabulary to describe the information provided by depiction. 2) Developing image representations that can visually capture this more nuanced vocabulary. 3) Constructing a comprehensive joint words and pictures framework. <br/><br/>This project has direct significance to many concrete tasks that access images on the internet including: image search, browsing, and organization, as well as commercial applications such as product search, and societally important applications such as web assistance for the blind. Additionally, outputs of this project, including progress toward a natural vocabulary and structure for visual description, have great potential for cross-cutting impact in both the computer vision and natural language communities."
"1137687","3rd USA-Sino Summer School in Vision, Learning, Pattern Recognition, VLPR 2011","IIS","Robust Intelligence","05/01/2011","05/05/2011","Qi Tian","TX","University of Texas at San Antonio","Standard Grant","Jie Yang","04/30/2013","$50,000.00","","qitian@cs.utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","7495","7495, 7556","$0.00","This grant supports American students and researchers to participate in the USA-Sino Summer School in Vision, Learning, and Pattern Recognition. The event brings together a high-quality team of leading American and Chinese researchers in computer vision and multimedia to offer a one-week educational and cultural exchange program to students and junior scholars from both US and China. The summer school provides a venue for the participants to explore a variety of aspects and applications on how to address the challenges in large-scale visual data/media acquisition, processing/computing, understanding, and search. The international interactions can foster new understanding and new collaborations in science, education, and culture."
"1134072","Support for Workshop on Advances in Language and Vision","IIS","ROBUST INTELLIGENCE","03/01/2011","11/08/2011","Trevor Darrell","CA","University of California-Berkeley","Standard Grant","Jie Yang","02/29/2012","$28,720.00","","trevor@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","7495","$0.00","This project supports travel expenses for participants at the workshop on advances in language and vision. In the past few years, great progress has been made in the fields of language and computer vision in developing technologies of extracting semantic content from text and imagery respectively. Each field has desires to adapt methods from the other, but often looking to the past literature rather than the current state of the art. This workshop makes significant scientific progress in multimodal representations and methods by bringing together the top researchers in both fields. The well organized brainstorming and discussion sessions contribute new ideas to this emerging area.  The outcome of the workshop provides some guidelines for targeted research in this interdisciplinary area, including anticipated fundamental scientific advances, possible large-scale challenge problems, the needs and prospects for available datasets, and connections to significant applications and their associated long-term economic impact and other societal benefits."
"1116923","RI: Small: Indoor Visual Navigation and Recognition for the Blind Using a Motion Sensing Input Device","IIS","ROBUST INTELLIGENCE","08/15/2011","10/30/2014","Robert Fergus","NY","New York University","Continuing grant","Jie Yang","07/31/2015","$449,994.00","","fergus@cs.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7495","7495, 7923","$0.00","In this project the goal is to develop a computer vision system to assist visually impaired people when navigating in indoor environments. A novel feature of the vision system is that it uses a motion sensing input device to gather both an image and depth map of the scene as input. By learning joint representations that combine both depth and intensity information, powerful features can be learned that give a dramatic improvement over existing scene understanding algorithms (which rely on intensity information alone). The availability of depth information also allows the recovery of the room geometry and permits the construction of new types of 3D priors on the locations of objects, not currently possible with existing approaches. The output of the vision system will be communicated to the visually impaired person via a number of possible methods: (i) a tactile hand-grip on a cane; (ii) a wearable pad embedded with actuators and (iii) the BrainPort sensor which has an array of tiny actuators that are placed on the tongue.<br/><br/>The expected results of the project are: (i) a large dataset of indoor scenes with depth maps and dense labels; (ii) new open-source algorithms for fusing depth and intensity information to aid scene understanding and (iii) a prototype vision-based assistive device for visually impaired people. The project aims to assist the approximately 2.5 million people in the US are blind or partially sighted."
"1064412","G&V: Medium: Collaborative Research: A Unified Approach to Material Appearance Modeling","IIS","HCC-Human-Centered Computing, GRAPHICS & VISUALIZATION","06/01/2011","06/06/2014","Holly Rushmeier","CT","Yale University","Continuing Grant","Ephraim Glinert","05/31/2016","$406,000.00","","rushmeier@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7367, 7453","7453, 7924, 9251","$0.00","Realistic image synthesis techniques from computer graphics enable the use of simulation in a wide variety of important fields including architecture, industrial design and communication, military, medical, and emergency training, cultural heritage preservation, film production, and gaming.  Realistic modeling of material appearance is an essential component of the image synthesis process.  Current approaches to material modeling include analytical modeling, numerical simulation, and image-based capture.  Each approach has distinct advantages and limitations, and different ranges of applicability.  The lack of unity makes material modeling difficult and has limited the useful application of computer graphics image synthesis.  This transformative research will change the way materials are modeled in computer graphics systems.  Rather than using disparate models as at present, this project will unify these approaches into a common physical and perceptual framework that will serve as the basis for a rich set of tools for material modeling that are physically accurate, phenomenologically expressive, computationally efficient, and easy to use.  This work should enable the use of computer-aided material design methods in a wide range of economically and culturally important applications.  Creating this framework will involve three subprojects.<br/><br/>Development of a material simulation testbed:  In this subproject a suite of tools for material simulation will be developed that includes both Monte Carlo and deterministic algorithms.  Different classes of materials (paints, metals, textiles) will be modeled, and different numerical methods will be tested and compared.  The resulting simulation tools and a database of the simulated materials will be distributed.<br/><br/>Unification of analytical, simulation, and image-based capture material modeling methods:  In this subproject the analytical models that represent general classes of materials will be unified with simulation and image-based capture data that represent specific material instances.  In the first part of this subproject simulation and capture data will be fit with a range of analytical models, considering both individual materials and ""families"" of materials generated by progressively changing the parameters of the simulation models.  In the second part of this project methods for inferring the microstructures of materials measured using image-based capture methods will be developed.  The approach will be to identify the class of a material and then vary the parameters of an appropriate simulation model to best reproduce the captured data.  The results of this subproject will be expressive and efficient analytical material models that are physically grounded because they are based on captured data and rigorous simulations.<br/><br/>Development of perceptually-based material design tools:  An important criterion for material modeling is usability.  Material designers need to be able to easily specify and visualize material appearance properties.  This requires consideration of the human factors in material modeling.  In this subproject a series of psychophysical experiments on material perception will be conducted and the results will be used to derive perceptually-based material models with meaningful parameters.  How image properties affect the visual fidelity of rendered materials will also be investigated.  These findings will then be used to develop effective and easy-to-use interfaces for computer-aided material design.<br/><br/>Broader Impacts:  Better methods for material modeling and rendering will lead to improved capability and productivity in fields such as architecture, industrial design and communication, training, cultural heritage, and entertainment.  The project will build a material appearance community that stretches across academic and commercial boundaries to include computer graphics, computer vision and human vision researchers along with a range of industrial collaborators, and which focuses on developing effective solutions to real-world problems.  The research will engage and train groups of students at 3 universities for scientific/technical careers that require working in interdisciplinary teams and partnering with coworkers in remote locations."
"1064427","G&V: Medium: Collaborative Research: A Unified Approach to Material Appearance Modeling","IIS","GRAPHICS & VISUALIZATION","06/01/2011","04/01/2014","Sumanta Pattanaik","FL","The University of Central Florida Board of Trustees","Continuing Grant","Ephraim Glinert","05/31/2017","$400,000.00","","sumant@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7453","7453, 7924","$0.00","Realistic image synthesis techniques from computer graphics enable the use of simulation in a wide variety of important fields including architecture, industrial design and communication, military, medical, and emergency training, cultural heritage preservation, film production, and gaming.  Realistic modeling of material appearance is an essential component of the image synthesis process.  Current approaches to material modeling include analytical modeling, numerical simulation, and image-based capture.  Each approach has distinct advantages and limitations, and different ranges of applicability.  The lack of unity makes material modeling difficult and has limited the useful application of computer graphics image synthesis.  This transformative research will change the way materials are modeled in computer graphics systems.  Rather than using disparate models as at present, this project will unify these approaches into a common physical and perceptual framework that will serve as the basis for a rich set of tools for material modeling that are physically accurate, phenomenologically expressive, computationally efficient, and easy to use.  This work should enable the use of computer-aided material design methods in a wide range of economically and culturally important applications.  Creating this framework will involve three subprojects.<br/><br/>Development of a material simulation testbed:  In this subproject a suite of tools for material simulation will be developed that includes both Monte Carlo and deterministic algorithms.  Different classes of materials (paints, metals, textiles) will be modeled, and different numerical methods will be tested and compared.  The resulting simulation tools and a database of the simulated materials will be distributed.<br/><br/>Unification of analytical, simulation, and image-based capture material modeling methods:  In this subproject the analytical models that represent general classes of materials will be unified with simulation and image-based capture data that represent specific material instances.  In the first part of this subproject simulation and capture data will be fit with a range of analytical models, considering both individual materials and ""families"" of materials generated by progressively changing the parameters of the simulation models.  In the second part of this project methods for inferring the microstructures of materials measured using image-based capture methods will be developed.  The approach will be to identify the class of a material and then vary the parameters of an appropriate simulation model to best reproduce the captured data.  The results of this subproject will be expressive and efficient analytical material models that are physically grounded because they are based on captured data and rigorous simulations.<br/><br/>Development of perceptually-based material design tools:  An important criterion for material modeling is usability.  Material designers need to be able to easily specify and visualize material appearance properties.  This requires consideration of the human factors in material modeling.  In this subproject a series of psychophysical experiments on material perception will be conducted and the results will be used to derive perceptually-based material models with meaningful parameters.  How image properties affect the visual fidelity of rendered materials will also be investigated.  These findings will then be used to develop effective and easy-to-use interfaces for computer-aided material design.<br/><br/>Broader Impacts:  Better methods for material modeling and rendering will lead to improved capability and productivity in fields such as architecture, industrial design and communication, training, cultural heritage, and entertainment.  The project will build a material appearance community that stretches across academic and commercial boundaries to include computer graphics, computer vision and human vision researchers along with a range of industrial collaborators, and which focuses on developing effective solutions to real-world problems.  The research will engage and train groups of students at 3 universities for scientific/technical careers that require working in interdisciplinary teams and partnering with coworkers in remote locations."
"1064410","G&V: Medium: Collaborative Research: A Unified Approach to Material Appearance Modeling","IIS","GRAPHICS & VISUALIZATION","06/01/2011","04/22/2014","James Ferwerda","NY","Rochester Institute of Tech","Continuing grant","Ephraim P. Glinert","05/31/2016","$398,810.00","","jaf@cis.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7453","7453, 7924","$0.00","Realistic image synthesis techniques from computer graphics enable the use of simulation in a wide variety of important fields including architecture, industrial design and communication, military, medical, and emergency training, cultural heritage preservation, film production, and gaming.  Realistic modeling of material appearance is an essential component of the image synthesis process.  Current approaches to material modeling include analytical modeling, numerical simulation, and image-based capture.  Each approach has distinct advantages and limitations, and different ranges of applicability.  The lack of unity makes material modeling difficult and has limited the useful application of computer graphics image synthesis.  This transformative research will change the way materials are modeled in computer graphics systems.  Rather than using disparate models as at present, this project will unify these approaches into a common physical and perceptual framework that will serve as the basis for a rich set of tools for material modeling that are physically accurate, phenomenologically expressive, computationally efficient, and easy to use.  This work should enable the use of computer-aided material design methods in a wide range of economically and culturally important applications.  Creating this framework will involve three subprojects.<br/><br/>Development of a material simulation testbed:  In this subproject a suite of tools for material simulation will be developed that includes both Monte Carlo and deterministic algorithms.  Different classes of materials (paints, metals, textiles) will be modeled, and different numerical methods will be tested and compared.  The resulting simulation tools and a database of the simulated materials will be distributed.<br/><br/>Unification of analytical, simulation, and image-based capture material modeling methods:  In this subproject the analytical models that represent general classes of materials will be unified with simulation and image-based capture data that represent specific material instances.  In the first part of this subproject simulation and capture data will be fit with a range of analytical models, considering both individual materials and ""families"" of materials generated by progressively changing the parameters of the simulation models.  In the second part of this project methods for inferring the microstructures of materials measured using image-based capture methods will be developed.  The approach will be to identify the class of a material and then vary the parameters of an appropriate simulation model to best reproduce the captured data.  The results of this subproject will be expressive and efficient analytical material models that are physically grounded because they are based on captured data and rigorous simulations.<br/><br/>Development of perceptually-based material design tools:  An important criterion for material modeling is usability.  Material designers need to be able to easily specify and visualize material appearance properties.  This requires consideration of the human factors in material modeling.  In this subproject a series of psychophysical experiments on material perception will be conducted and the results will be used to derive perceptually-based material models with meaningful parameters.  How image properties affect the visual fidelity of rendered materials will also be investigated.  These findings will then be used to develop effective and easy-to-use interfaces for computer-aided material design.<br/><br/>Broader Impacts:  Better methods for material modeling and rendering will lead to improved capability and productivity in fields such as architecture, industrial design and communication, training, cultural heritage, and entertainment.  The project will build a material appearance community that stretches across academic and commercial boundaries to include computer graphics, computer vision and human vision researchers along with a range of industrial collaborators, and which focuses on developing effective solutions to real-world problems.  The research will engage and train groups of students at 3 universities for scientific/technical careers that require working in interdisciplinary teams and partnering with coworkers in remote locations."
"1117509","RI: Small: Exploiting Correlated Sparsity Pattern Change in Dynamic Vision Problems","IIS","ROBUST INTELLIGENCE","09/01/2011","08/22/2011","Namrata Vaswani","IA","Iowa State University","Standard Grant","Jie Yang","08/31/2016","$204,395.00","","namrata@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7495","7923","$0.00","This project develops a new framework to solve a large class of dynamic vision problems by exploiting correlated sparsity pattern change in the appropriate domain. The focus is on high-dimensional visual tracking problems such as deformable contour tracking or target tracking in the presence of significant illumination changes. These are difficult because of the high dimensionality and because the observation models are highly nonlinear and/or non-Gaussian due to clutter, occlusions or low contrast. However, in most such problems, even though the state (e.g., contour deformation or illumination) is high-dimensional, at any given time, most change occurs in only a few principal directions. In a long sequence, this set of directions can gradually change over time. Most existing methods need a set of past state estimates to estimate this change on-the-fly while tracking noisy or nonlinear systems. The research team provides a completely new solution to this difficult problem by re-interpreting it as a problem of ""recursively reconstructing sparse state sequences with slow time-varying sparsity patterns"" and tapping into ideas from their ongoing recursive sparse recovery work.<br/><br/>The research of this project enriches the knowledge base of computer vision and can be applied to many different applications such as medical image analysis and video surveillance. The project provides research opportunities for graduate students and involves undergraduate students, including under-represented minorities, through summer, senior design projects and REU projects."
"1110223","STTR Phase I: Use of Serious Games to Improve Learning Outcomes in Engineering Programs","IIP","STTR PHASE I","07/01/2011","06/19/2012","Stephen Lynch","CA","Toolwire, Inc","Standard Grant","Glenn H. Larsen","12/31/2012","$165,000.00","P. Raju","slynch@toolwire.com","6150 Stoneridge Mall Road","Pleasanton","CA","945883238","6172756656","ENG","1505","1505, 8031, SMET","$0.00","This Small Business Technology Transfer Phase 1 project combines the game-based learning scenario development and business entrepreneurship expertise of Toolwire Inc., with the instructional material development and evaluation expertise of the Laboratory for Innovative Technology and Engineering Education (LITEE) at Auburn University. They worked together to develop a pilot version of an engineering design Smart Scenario that included information on the Challenger STS 51-L case study. This was implemented in introductory engineering classes and an evaluation showed that the students perceived that they learned the subject matter deeply due to the gaming nature of this pilot. This evaluation reinforces results from earlier research that shows that serious games have the ability to improve student engagement, positively affect learning, and help retain students in engineering. This project will design and develop serious games in the four topics of engineering design, communications, ethics, and industrial safety for use in introductory engineering classes. These games will be implemented in introductory engineering classes. The evaluation methodology will use a Presage-Pedagogy-Process-Product-model using control and experimental classes. Data will be collected based on constructs designed to measure the variables in the 4-P model. The intellectual merit of this proposal is that it develops, implements, and tests a set of games that emphasize developing capabilities in addition to content, brings real-world problems into classrooms, uses gaming-technologies to mediate learning, includes assessment methodology in the games, and provides students multiple iterations to learn the concepts, thereby changing the approach to both teaching and learning in an introductory course.<br/><br/>The broader impact/commercial potential of this project is its ability to directly provide massively effective parallel education in engineering by being scalable to different levels of classes, using compelling scenarios that lead to deep learning, triggering brain chemistry through games that better engages students in learning while being available anytime, anywhere through secure, web-based services. This combination of highly engaging ?active learning by doing? on advanced technology platforms achieves lower costs of delivery, greater long-term adoption of material in workforce development, improved user experiences and lower dropout rates. These serious games produce continuous experiential learning cycles for both students and teachers alike in both classroom and workplace contexts."
"1062405","ABI Innovation: Modeling the Drosophila Brain with Single-neuron Resolution using Computer Vision Methods","DBI","ADVANCES IN BIO INFORMATICS","05/01/2011","04/22/2011","Gavriil Tsechpenakis","IN","Indiana University","Standard Grant","Anne Haake","04/30/2014","$318,893.00","","gtsechpe@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","BIO","1165","1165, 9179","$0.00","The project is funded to model the morphology of neurons and how they develop and form synapses in the Central Nervous System (CNS) using computer vision approaches. By exploiting the morphological stereotypes and unparalleled single-cell resolution of Drosophila motor neurons, this project will yield the first motor neuron map in the Drosophila CNS, serving as a case study and a step towards modeling the entire brain. The first complete map of a whole brain is that of C. elegans. Its virtue, unchallenged thus far, is resolution down to the level of single visually identified neurons. Because the connectivity of individual neurons ultimately determines brain function, the creation of neural maps at this level in other more complex model organisms is critical for advancing our understanding of brain development. The immediate goal of this project is to create a neural map of a defined population of motor neurons in a healthy Drosophila brain at the single-neuron level. It is intended to be a resource for anyone who wants to ""navigate"" a model brain in vivo.  To achieve this goal, this project includes the following tasks. (a) Modeling the individual neuron morphology with automated image-based motor neuron classification. To meet this objective, a new framework will be developed for three-dimensional partitioning of the neurons? morphological compartments, namely soma (cell body), axon and dendrites. (b) Classifying the neuron morphology from the partitioned volumes. The approach consists of a novel possibilistic semi-supervised clustering framework, robust enough to discriminate between different neuron morphologies, and to handle the intra-class variability and inter-class similarities. (c) Identifying and tracing morphology-specific motor neurons in part of the Drosophila CNS by inferring knowledge of individual neuron modeling, and constructing a prototype motor circuit.<br/><br/>All findings, methods, developed algorithms (open source codes), publications and data will be disseminated through the project's website, as soon as they become available and ready for the public: http://web.mac.com/gavriil/Gavriil_Tsechpenakis/neurovision. This work has immediate extensions to (a) the completion of a model motor circuit, by including all (approximately 400) manually identified motor neurons within a single Drosophila CNS hemisegment; (b) the identification of more types of neurons, for scaling up the findings of this work and modeling more types of neuron circuitries in the brain. The long-term goal is to create a complete model brain that will allow for comprehensive analysis of brain development after mutation, and the assessment of changes in brain connectivity patterns resulting from drug treatments, disease, or aging and stress."
"1117079","AF:Small:RUI:New directions in Fourier analysis, noise sensitivity, and learning theory","CCF","Algorithmic Foundations","06/01/2011","06/03/2011","Karl Wimmer","PA","Duquesne University","Standard Grant","Balasubramanian Kalyanasundaram","05/31/2015","$231,480.00","","wimmerk@duq.edu","Room 310 Administration Building","Pittsburgh","PA","152193016","4123961537","CSE","7796","7923, 7926, 9229","$0.00","One of the major concerns that practitioners have about theoretical machine learning is the focus on distributions where the attributes are independent: in other words, knowing one attribute gives no information about any others.  As an example, while it is plausible that height and eye color are independent, it is much less believable that height and weight are independent.  Thus, the output given by any algorithm that is based on the assumption that the attributes of a person (such as height and weight) are independent cannot be trusted. The goal of this project is to extend what we know about the theory of such problems while removing some of the mathematically convenient assumptions such as independence.  <br/><br/>The tools used focus on discrete Fourier analysis, but involve many other techniques from mathematics such as functional analysis and representation theory of finite groups.  One recurring technique is the application of the ""noise sensitivity"" method, which quantifies the complexity of a function based on how similar the value of the function is on some input to the values of that input's neighbors.  In many cases, the goal is to show that the Fourier spectrum of certain classes of functions is predictable; often, this predictability is a key component of algorithms for machine learning. <br/><br/>The broader goal of this project is to discover new connections between mathematics and computer science with a special focus on questions motivated by machine learning. Answers to the underlying questions would be useful to theoreticians and could lead to better applied machine learning algorithms.  Also, the mathematical questions raised are interesting independently of the machine learning connection.  The problems considered in this project will provide an invigorating research opportunity for undergraduate and Master's students."
"1065025","G&V: Medium: Collaborative Research: Large Data Visualization Using An Interactive Machine Learning Framework","IIS","GRAPHICS & VISUALIZATION","06/01/2011","03/30/2011","Raghu Machiraju","OH","Ohio State University","Standard Grant","Ephraim P. Glinert","05/31/2015","$542,002.00","Han-Wei Shen","machiraju.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7453","7924","$0.00","Abstract - Machiraju, Rangarajan, and Thompson<br/><br/>As computer power continues to increase, the complexity of simulations also increases thereby producing datasets of unprecedented size.  Without effective analysis tools, results from these large-scale simulations cannot be utilized to their fullest extent. This research addresses the problem of large-data visualization and exploration by employing interactive multi-scale machine learning, which exploits an efficient feature-based, multi-resolution representation of the data. The investigators are leveraging methods from the field of machine learning to perform two distinct tasks: identify regions of interest and enhance robustness of feature detection algorithms. The primary outcome of this effort is the realization of a framework for exploring large datasets. Further, this work is introducing a large body of work in machine learning to the field of visualization. Successful completion of this research will help overcome the brittleness of existing visualization methods and foster expedient discovery in many areas of science and engineering.<br/><br/>The multi-resolution techniques developed here will employ a two-fold strategy. First, semi-supervised learning based on training with the domain expert is used to develop strategies for selective spatial and temporal refinement of the data.  A classifier is constructed to tag the output of the coarse resolution feature detection (i.e. regions) as either interesting or not interesting. Then at the finest scale, interesting local data chunks containing features of interest are identified for further analysis. Second, several local feature detection algorithms, or weak classifiers, are combined into a single, more robust compound classifier using adaptive boosting, or AdaBoost, and a data adaptive variant called CAVIAR that facilitates validated feature detection. Ideally, the compound classifier combines the best of all weak classifiers as they respond to the underlying physical signal. This research is demonstrating the effectiveness of these methods by applying existing local detection algorithms for visualizing vortices in turbulent flow fields."
"1065107","G&V: Medium: Collaborative Research: Large Data Visualization Using An Interactive Machine Learning Framework","IIS","GRAPHICS & VISUALIZATION","06/01/2011","04/03/2014","David Thompson","MS","Mississippi State University","Continuing grant","Ephraim Glinert","05/31/2015","$280,258.00","Dibbon Walters","dst@ae.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","CSE","7453","7924, 9150","$0.00","Abstract - Machiraju, Rangarajan, and Thompson<br/><br/>As computer power continues to increase, the complexity of simulations also increases thereby producing datasets of unprecedented size.  Without effective analysis tools, results from these large-scale simulations cannot be utilized to their fullest extent. This research addresses the problem of large-data visualization and exploration by employing interactive multi-scale machine learning, which exploits an efficient feature-based, multi-resolution representation of the data. The investigators are leveraging methods from the field of machine learning to perform two distinct tasks: identify regions of interest and enhance robustness of feature detection algorithms. The primary outcome of this effort is the realization of a framework for exploring large datasets. Further, this work is introducing a large body of work in machine learning to the field of visualization. Successful completion of this research will help overcome the brittleness of existing visualization methods and foster expedient discovery in many areas of science and engineering.<br/><br/>The multi-resolution techniques developed here will employ a two-fold strategy. First, semi-supervised learning based on training with the domain expert is used to develop strategies for selective spatial and temporal refinement of the data.  A classifier is constructed to tag the output of the coarse resolution feature detection (i.e. regions) as either interesting or not interesting. Then at the finest scale, interesting local data chunks containing features of interest are identified for further analysis. Second, several local feature detection algorithms, or weak classifiers, are combined into a single, more robust compound classifier using adaptive boosting, or AdaBoost, and a data adaptive variant called CAVIAR that facilitates validated feature detection. Ideally, the compound classifier combines the best of all weak classifiers as they respond to the underlying physical signal. This research is demonstrating the effectiveness of these methods by applying existing local detection algorithms for visualizing vortices in turbulent flow fields."
"1065081","G&V: Medium: Collaborative Research: Large Data Visualization Using An Interactive Machine Learning Framework","IIS","GRAPHICS & VISUALIZATION","06/01/2011","03/12/2014","Anand Rangarajan","FL","University of Florida","Continuing grant","Ephraim Glinert","05/31/2015","$303,444.00","","anand@cise.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7453","7924","$0.00","Abstract - Machiraju, Rangarajan, and Thompson<br/><br/>As computer power continues to increase, the complexity of simulations also increases thereby producing datasets of unprecedented size.  Without effective analysis tools, results from these large-scale simulations cannot be utilized to their fullest extent. This research addresses the problem of large-data visualization and exploration by employing interactive multi-scale machine learning, which exploits an efficient feature-based, multi-resolution representation of the data. The investigators are leveraging methods from the field of machine learning to perform two distinct tasks: identify regions of interest and enhance robustness of feature detection algorithms. The primary outcome of this effort is the realization of a framework for exploring large datasets. Further, this work is introducing a large body of work in machine learning to the field of visualization. Successful completion of this research will help overcome the brittleness of existing visualization methods and foster expedient discovery in many areas of science and engineering.<br/><br/>The multi-resolution techniques developed here will employ a two-fold strategy. First, semi-supervised learning based on training with the domain expert is used to develop strategies for selective spatial and temporal refinement of the data.  A classifier is constructed to tag the output of the coarse resolution feature detection (i.e. regions) as either interesting or not interesting. Then at the finest scale, interesting local data chunks containing features of interest are identified for further analysis. Second, several local feature detection algorithms, or weak classifiers, are combined into a single, more robust compound classifier using adaptive boosting, or AdaBoost, and a data adaptive variant called CAVIAR that facilitates validated feature detection. Ideally, the compound classifier combines the best of all weak classifiers as they respond to the underlying physical signal. This research is demonstrating the effectiveness of these methods by applying existing local detection algorithms for visualizing vortices in turbulent flow fields."
"1142382","Travel Support for Workshop on Modeling, Simulation and Visual Analysis of Large Crowds","IIS","GRAPHICS & VISUALIZATION","09/01/2011","06/29/2011","Dinesh Manocha","NC","University of North Carolina at Chapel Hill","Standard Grant","Jie Yang","02/28/2013","$8,000.00","Saad Ali","dm@cs.umd.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7453","7453, 7556","$0.00","This travel grant supports USA participants to the workshop on ""Modeling, Simulation, and Visual Analysis of Large Crowds"" in November 2011 at 13th International Conference on Computer Vision. This is a multi-disciplinary workshop that addresses challenges related to visual analysis and simulation of large crowds. The workshop engages complimentary viewpoints from different areas including computer vision, computer graphics, physics-based simulation, and evacuation dynamics to develop additional insight into crowd analysis, modeling and simulation problem. This grant partially supports the travel of invited speakers and participants, especially from computer graphics, simulation, and pedestrian dynamics. The final proceedings of the workshop along with speaker slides would be made available via WWW."
"1111125","HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks","IIS","Info Integration & Informatics, HCC-Human-Centered Computing","08/15/2011","04/11/2012","Holly Yanco","MA","University of Massachusetts Lowell","Standard Grant","Ephraim Glinert","07/31/2015","$416,903.00","","holly@cs.uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","7364, 7367","7364, 7367, 7925, 9251","$0.00","This research involves collaboration among investigators at three institutions.  The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks.  To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot.  Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs.  In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.<br/><br/>The Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space.  The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions.  Speech is a natural though demanding way to use natural language to communicate with a robot.  To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information.  This project will answer three scientific questions.<br/><br/>(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? <br/>(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? <br/>3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?<br/><br/>To these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely.  To inform the design process, the PIs will conduct focus groups with potential users.  They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.<br/><br/>Broader Impacts:  To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively.  It must also be able to communicate effectively with other agents, and particularly with people.  This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research.  Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability.  This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age).  It will also support telepresence applications such as telecommuting, telemedicine and search and rescue.  The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues."
"1112612","RUI: Large-scale Algorithm Analysis and GPU Implementations for Compressed Sensing and Matrix Completion","DMS","COMPUTATIONAL MATHEMATICS, EPSCoR Co-Funding","08/01/2011","07/25/2011","Jeffrey Blanchard","IA","Grinnell College","Standard Grant","Rosemary Renaut","07/31/2016","$160,080.00","","blanchaj@grinnell.edu","1121 Park Street","Grinnell","IA","501121690","6412694983","MPS","1271, 9150","9150, 9229, 9263","$0.00","This project considers the fusion of two timely research topics: algorithms for compressed sensing and matrix completion, and their implementation using graphical processing units (GPUs). Compressed sensing is a relatively new paradigm in signal processing where the acts of acquiring a signal and compressing the measurements are combined into a single operation. The number of compressed measurements acquired is proportional to the information content of the signal rather than, as is traditional, equal to the ambient dimension of the signal.  Although the number of measurements is significantly reduced resulting in an undetermined system of equations, low-complexity greedy algorithms can be guaranteed to reconstruct an accurate approximation to the measured signal provided that the underlying signal was sparse, i.e. had only a few important components. Matrix completion similarly exploits the simplicity of the target matrix having only a few independent columns; in other words, one recovers a low rank matrix from a limited number of measurements. Typical applications include compressive radar, geophysical data analysis, medical imaging, and computer vision. The data sets from these applications are typically, however, at least an order of magnitude beyond the currently available simulation levels.  By employing the computational power of GPUs this project provides a platform for overcoming computational barriers and the necessary large-scale testing on problems up to three orders of magnitude beyond current empirical testing regimes.<br/><br/><br/><br/>Traditionally, a signal is measured by acquiring every component in the signal and then compressing the signal with an appropriate computational algorithm.  For example, digital cameras capture an image with a huge number of pixels and then a compression scheme such as JPEG is used to reduce the size of the digital image for storage or dissemination.  In many cases, the costs and challenges associated with taking measurements are considerable.  In compressed sensing and matrix completion, the measurement process is altered in order to reduce the number of measurements but the signal reconstruction process is necessarily more difficult.  Compressed sensing and matrix completion transfer the workload from the measurement process to computational resources dedicated to the signal reconstruction.  A typical example in medical imaging is magnetic resonance imaging (MRI) where the time required to obtain a diagnostic level MRI causes unnecessary discomfort for patients and even pediatric sedation.  Compressed sensing MRI has demonstrated the ability to produce diagnostic caliber images in a fraction of the time.  The increased computational burden requires fast, efficient algorithms and many such algorithms have been introduced or updated for compressed sensing.  The observed performance of these algorithms is substantially superior to their pessimistic theoretical guarantees, but testing of these algorithms has been constrained by their imposed computational burden. In this project, the PI and collaborators  develop software capable of providing near real-time signal reconstruction from compressed measurements through development of new techniques, and by exploiting the computational performance gains offered by new architectures with graphical processing units. The resulting software validation is aimed to provide practioners with guidance on algorithm choice most appropriate to the application.  Undergraduate students at the PI's institution have the opportunity to participate in the PI's research  and are exposed to the challenges presented, but gains to be achieved, when exploiting new scientific computing architectures."
"1111323","HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks","IIS","Info Integration & Informatics, HCC-Human-Centered Computing","08/15/2011","08/11/2011","Matthias Scheutz","MA","Tufts University","Standard Grant","Ephraim Glinert","07/31/2014","$385,310.00","","matthias.scheutz@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7364, 7367","7364, 7367, 7925","$0.00","This research involves collaboration among investigators at three institutions.  The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks.  To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot.  Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs.  In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.<br/><br/>The Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space.  The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions.  Speech is a natural though demanding way to use natural language to communicate with a robot.  To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information.  This project will answer three scientific questions.<br/><br/>(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? <br/>(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? <br/>3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?<br/><br/>To these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely.  To inform the design process, the PIs will conduct focus groups with potential users.  They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.<br/><br/>Broader Impacts:  To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively.  It must also be able to communicate effectively with other agents, and particularly with people.  This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research.  Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability.  This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age).  It will also support telepresence applications such as telecommuting, telemedicine and search and rescue.  The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues."
"1111494","HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks","IIS","Info Integration & Informatics, HCC-Human-Centered Computing","08/15/2011","03/16/2015","Benjamin Kuipers","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Ephraim Glinert","07/31/2016","$693,265.00","","kuipers@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7364, 7367","7364, 7367, 7925","$0.00","This research involves collaboration among investigators at three institutions.  The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks.  To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot.  Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs.  In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.<br/><br/>The Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space.  The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions.  Speech is a natural though demanding way to use natural language to communicate with a robot.  To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information.  This project will answer three scientific questions.<br/><br/>(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? <br/>(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? <br/>3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?<br/><br/>To these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely.  To inform the design process, the PIs will conduct focus groups with potential users.  They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.<br/><br/>Broader Impacts:  To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively.  It must also be able to communicate effectively with other agents, and particularly with people.  This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research.  Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability.  This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age).  It will also support telepresence applications such as telecommuting, telemedicine and search and rescue.  The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues."
"1116303","CGV: Small: Collaborative Research: Sparse Reconstruction and Frequency Analysis for Computer Graphics Rendering and Imaging","IIS","GRAPHICS & VISUALIZATION","10/01/2011","06/27/2011","Fredo Durand","MA","Massachusetts Institute of Technology","Standard Grant","Ephraim P. Glinert","09/30/2014","$250,000.00","","fredo@graphics.lcs.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7453","7453, 7923","$0.00","A broad range of problems in computer graphics rendering, appearance acquisition, and imaging, involve sampling, reconstruction, and integration of high-dimensional (4D-8D) signals.  Real-time rendering of glossy materials and intricate lighting effects like caustics, for example, can require pre-computing the response of the scene to different light and viewing directions, which is often a 6D dataset.  Similarly, image-based appearance acquisition of facial details, car paint, or glazed wood requires us to take images from different light and view directions.  Even offline rendering of visual effects like motion blur from a fast-moving car, or depth of field, involves high-dimensional sampling across time and lens aperture.  The same problems are also common in computational imaging applications such as light field cameras.  While the PIs and others have made significant progress in subsequent analysis and compact representation for some of these problems, the initial full dataset must almost always still be acquired or computed by brute force which is prohibitively expensive, taking hours to days of computation and acquisition time, as well as being a challenge for memory usage and storage.<br/><br/>The PIs' goal in this project is to make fundamental contributions that enable dramatically sparser sampling and reconstruction of these signals, before the full dataset is acquired or simulated.  The key idea is to exploit the structure of the data that often lies in lower-frequency, sparse, or low-dimensional spaces.  Their recent collaboration on a Fourier analysis of motion blur has shown that the frequency spectrum of dynamic scenes is sheared into a narrow wedge in the space-time domain.  This enables novel sheared (not axis-aligned) filters and a sparse sampling.  The PIs will build upon these preliminary results to develop a unified framework for frequency analysis and sparse data reconstruction of visual appearance in computer graphics.  To these ends, they will first lay the theoretical foundations, including a novel frequency analysis of Monte Carlo integration and 5D space-time analysis of light fields.  They will then develop efficient practical algorithms for a variety of problem domains, including sparse reconstruction of light transport matrices for relighting, sheared sampling and denoising for offline shadow rendering, time-coherent compressive sampling for appearance acquisition, and new approaches to computational photography and imaging.<br/><br/>Broader Impacts:  From a theoretical perspective, this project will develop a fundamental signal-processing analysis of light transport and appearance and imaging datasets, which will provide the foundation for further work not just in computer graphics but in signal-processing, computer vision, and image analysis as well.  Project outcomes will apply to diverse sets of problems and will lead to transformative advances across the spectrum of rendering and imaging applications.  The PIs will leverage existing collaborations with industry to transition the new technologies to practical production use.  Outreach to K-12 students and the public will be enabled by a new science popularization blog that will leverage the public's excitement for advances in digital photography to introduce novel technical concepts, as well as by events such as the Computer Science Education Day for high school students at UC-Berkeley.  The new algorithms and datasets resulting from this work will be made available to the research community; moreover, imaging algorithms will be released in open-source format to work with consumer digital and cell-phone cameras."
"1115587","Solving Polynomial Systems by the Polyhedral Homotopy","DMS","COMPUTATIONAL MATHEMATICS","09/15/2011","09/06/2011","Tien-Yien Li","MI","Michigan State University","Standard Grant","Junping Wang","08/31/2015","$240,000.00","","li@math.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","1271","9263","$0.00","A group led by the PI has successfully developed a software package, HOM4PS-2.0, implementing the  ""polyhedral homotopy continuation"" method for solving polynomial systems. The solver leads existing software packages for solving polynomial systems in speed by a large margin. The essence of the proposed project is the further development in all aspects of the solver HOM4PS-2.0. In particular, for the need of solving larger polynomial systems, a major aspect of the project is the advanced development of the parallel version of the solver. The landscape of computation hardware is quite different from even a decade ago. Developments in new processor design and network technology have allowed supercomputers and computer clusters to grow larger and faster than ever, including new ideas such as cycle scavenging, grid computing, virtual supercomputers, multiple cores, and GPUs (graphics processing units). The proposed project will investigate and implement versions of HOM4PS-2.0 that take optimum advantage of heterogeneous computing platforms, with special emphasis on clusters, cloud computing, multicore and GPUs. In addition we plan to find ways to implement highly serial parts of the original algorithm, such as mixed volume computation and path-jumping detection on parallel architectures. The proposed project intends to fully incorporate all the cutting-edge parallel computing technologies in our solver for solving larger and larger polynomial systems. <br/><br/>The problem of solving polynomial systems arises very frequently in various fields of science and engineering, such as, formula construction, geometric intersection, inverse kinematics, robotics, computer vision and the computation of equilibrium states of chemical reaction equations, etc. Science and engineering problems pose an increasing demand for solving larger and larger polynomial systems. To deal with such large systems, more computing resources are needed to greatly enlarge the capability of our solver, HOM4PS-2.0. For this purpose the parallelization of the original algorithms becomes inevitably essential. Computational technology is experiencing a major sea change in which one either rides the wave or goes under. To embrace this challenge, the core of the project is to fully incorporate the cutting-edge parallel computing technologies for solving larger and larger polynomial systems. The ultimate goal is a more powerful suite of high-quality software package which will provide the scientific community a reliable source for solving polynomial systems in practice."
"1108631","Union of Subspaces and Manifold Data Modeling: Theory, Algorithms, Testing, and Applications","DMS","APPLIED MATHEMATICS, COMM & INFORMATION FOUNDATIONS","10/01/2011","09/19/2011","Akram Aldroubi","TN","Vanderbilt University","Standard Grant","Michael H. Steuerwalt","09/30/2015","$268,368.00","","akram.aldroubi@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","MPS","1266, 7797","7569, 7936, 9150","$0.00","Aldroubi<br/>DMS-1108631<br/><br/>     There is a growing interest in computer science, engineering, and mathematics for modeling signals in terms of union of subspaces and manifolds.  Subspace segmentation and clustering of high-dimensional data drawn from a union of subspaces are especially important with many practical applications in computer vision, image and signal processing, communications, and information theory.  For example, recent paradigms for reconstructing signals assume models that consist of union of subspaces, e.g., the reconstruction of signals with finite rates of innovation.  Another example is in compressed sampling where signals are assumed to be sparse in some basis or in some dictionary.  This assumption implies that the signals live in a union of subspaces.  The subspace clustering problem in computer vision is yet another important example in which data are drawn from a union of low-dimensional subspaces.  Thus, a mathematical framework for finding such models from observed data is fundamental.  In this project the investigator develops a mathematical framework together with algorithms for data modeling in terms of union of subspaces and manifolds.  The mathematical theory is connected to the geometry of Hilbert and Banach spaces, topology, nonlinear approximation, optimization, and probability.<br/><br/>     The investigator develops a mathematical framework and algorithms for describing data by representing them as composed of components that live in restricted sets of the data space, for instance, in subspaces or manifolds.  The theory and methods developed by the investigator unify, extend, and complement some of the techniques used in sampling theory, subspace clustering, and the dictionary design problem.  The applications are fundamental to many problems in engineering and biomedicine, including motion tracking in videos, data classification and segmentation such as face recognition, and brain morphology.  The project is supported by the Division of Mathematical Sciences and the Division of Computing and Communication Foundations."
"1064416","AF: Medium: Collaborative Research: Optimality in Homology - Algorithms and Applications","CCF","Algorithmic Foundations","08/01/2011","04/30/2014","Tamal Dey","OH","Ohio State University","Continuing Grant","jack snoeyink","07/31/2016","$442,276.00","","dey.831@gmail.com","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7796","7924, 7929, 9216, 9218","$0.00","Many applications in science and engineering encounter the problem of<br/>identifying and processing topologically interesting features in the<br/>digital representation of a geometry or data. Such features often need<br/>to be optimal with respect to some metric (measurement). It is<br/>recognized that homology groups from algebraic topology play an<br/>essential role in these computations. Although the study of structural<br/>properties of the homology groups has a rich history in mathematics,<br/>their computations in combination with geometry are not that well<br/>studied. The principal investigators (PIs) propose to study these<br/>fundamental questions thoroughly, along with their connections to<br/>practical problems from science and engineering.<br/>Intellectual merit: Efficient solutions of the optimality questions in<br/>homology computations require both mathematical and algorithmic<br/>developments. The PIs bring aboard these required expertise. Apart<br/>from the synergistic effect of the proposed study on mathematics and<br/>theoretical computer science, the close ties with various applications<br/>in science and engineering will play a synergistic role between<br/>computational fields such as computer graphics, computer vision,<br/>sensor networks, computer aided design, and scientific fields such as<br/>biology, physics, chemistry, and others.<br/>Broader impacts: Optimization of aspects of homology groups provides<br/>important insights in many scientific and engineering applications<br/>ranging from tunnels in protein molecules to voids in large<br/>machines. Solutions of such problems can aid in the manufacturing of<br/>better machines, designing of new drugs, and rapid modeling of<br/>customized objects. The educational impact of this project is in a<br/>large synergy between mathematics and computer science motivated by<br/>real applications. Course notes, internet distributions, and software<br/>systems developed through the project will enable the scientific<br/>community to study challenging problems in geometry, topology, and<br/>algorithms. Graduate students supported by the project will develop<br/>skills in mathematics and theoretical computer science and also in<br/>writing robust, efficient, and user-friendly software."
"1054996","CAREER: Theorem, Algorithm, and Applications of Computational Quasiconformal Geometry","CCF","Algorithmic Foundations, EPSCoR Co-Funding","06/01/2011","03/21/2011","Miao Jin","LA","University of Louisiana at Lafayette","Standard Grant","jack snoeyink","09/30/2017","$419,780.00","","mxj9809@louisiana.edu","104 E University Ave","Lafayette","LA","705032014","3374825811","CSE","7796, 9150","1045, 1187, 7929, 9150, 9218, HPCC","$0.00","Quasiconformal geometry has showed its power and flexibility in complex analysis, differential equations, function theory, and topology.  Computational quasiconformal geometry focuses on algorithmic study of quasiconformal geometry theory, which links very pure areas of abstract mathematics to concrete engineering applications.  This project addresses a number of fundamental engineering problems where quasiconformal geometry can provide a key insight, including building a variational framework of computing the optimal diffeomorphism between surfaces with general topologies, building a theoretically well sound framework to model shape space of surfaces, and building anisotropic models which are widely observed in various areas including wireless sensor networks, computer graphics, and solid mechanics.  Expected results include the exploration of computational theorems, new models, and novel geometric algorithms with provable performance guarantee.<br/><br/>This interdisciplinary project provides the bridge between quasiconformal geometry and applications in broad engineering fields by identifying important geometric problems in computer graphics, computer vision, geometric modeling, and wireless sensor networks as well as supplying computational theorems and efficient algorithmic solutions based on quasiconformal geometry theory.  It is expected that the exploration will reveal key insights of fundamental problems in those fields."
"1115242","CGV: Small: Collaborative Research: Sparse Reconstruction and Frequency Analysis for Computer Graphics Rendering and Imaging","IIS","GRAPHICS & VISUALIZATION","10/01/2011","06/27/2011","Ravi Ramamoorthi","CA","University of California-Berkeley","Standard Grant","Ephraim Glinert","09/30/2015","$250,000.00","","ravir@cs.ucsd.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7453","7453, 7923","$0.00","A broad range of problems in computer graphics rendering, appearance acquisition, and imaging, involve sampling, reconstruction, and integration of high-dimensional (4D-8D) signals.  Real-time rendering of glossy materials and intricate lighting effects like caustics, for example, can require pre-computing the response of the scene to different light and viewing directions, which is often a 6D dataset.  Similarly, image-based appearance acquisition of facial details, car paint, or glazed wood requires us to take images from different light and view directions.  Even offline rendering of visual effects like motion blur from a fast-moving car, or depth of field, involves high-dimensional sampling across time and lens aperture.  The same problems are also common in computational imaging applications such as light field cameras.  While the PIs and others have made significant progress in subsequent analysis and compact representation for some of these problems, the initial full dataset must almost always still be acquired or computed by brute force which is prohibitively expensive, taking hours to days of computation and acquisition time, as well as being a challenge for memory usage and storage.<br/><br/>The PIs' goal in this project is to make fundamental contributions that enable dramatically sparser sampling and reconstruction of these signals, before the full dataset is acquired or simulated.  The key idea is to exploit the structure of the data that often lies in lower-frequency, sparse, or low-dimensional spaces.  Their recent collaboration on a Fourier analysis of motion blur has shown that the frequency spectrum of dynamic scenes is sheared into a narrow wedge in the space-time domain.  This enables novel sheared (not axis-aligned) filters and a sparse sampling.  The PIs will build upon these preliminary results to develop a unified framework for frequency analysis and sparse data reconstruction of visual appearance in computer graphics.  To these ends, they will first lay the theoretical foundations, including a novel frequency analysis of Monte Carlo integration and 5D space-time analysis of light fields.  They will then develop efficient practical algorithms for a variety of problem domains, including sparse reconstruction of light transport matrices for relighting, sheared sampling and denoising for offline shadow rendering, time-coherent compressive sampling for appearance acquisition, and new approaches to computational photography and imaging.<br/><br/>Broader Impacts:  From a theoretical perspective, this project will develop a fundamental signal-processing analysis of light transport and appearance and imaging datasets, which will provide the foundation for further work not just in computer graphics but in signal-processing, computer vision, and image analysis as well.  Project outcomes will apply to diverse sets of problems and will lead to transformative advances across the spectrum of rendering and imaging applications.  The PIs will leverage existing collaborations with industry to transition the new technologies to practical production use.  Outreach to K-12 students and the public will be enabled by a new science popularization blog that will leverage the public's excitement for advances in digital photography to introduce novel technical concepts, as well as by events such as the Computer Science Education Day for high school students at UC-Berkeley.  The new algorithms and datasets resulting from this work will be made available to the research community; moreover, imaging algorithms will be released in open-source format to work with consumer digital and cell-phone cameras."
"1144938","EAGER: Recurring Pattern Discovery","IIS","GRAPHICS & VISUALIZATION, ROBUST INTELLIGENCE","09/01/2011","07/28/2014","Yanxi Liu","PA","Pennsylvania State Univ University Park","Standard Grant","Jie Yang","08/31/2015","$158,000.00","","yanxi@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7453, 7495","7495, 7916, 9251","$0.00","Similar yet visually non-identical objects form recurring patterns that are ubiquitous in the world we live in. Thus an automatic recurring pattern detection algorithm can serve as a stepping stone towards robust higher level machine intelligence. The recognition of such recurring patterns is especially relevant for computer vision since it can lead to saliency detection, image segmentation, image compression and super-resolution, image retrieval and semantically meaningful organization of unlabeled data. This project explores automatic recurring pattern discovery from domain independent images and videos to capture, robustly and flexibly, varying mid-level visual cues emerging from any cluttered background. The work leads to effective and efficient object discovery and scene interpretation. The research team develops an un-supervised method for discovering recurring patterns in a single or multiple images . The key property is the nature of recurring without knowing what recurs. Differing from previous feature- or object-level pairwise-matching-based approaches, recurring pattern discovery from real images is formulated as a joint, 2-dimensional feature assignment optimization problem where multiple objects and multiple feature clusters are considered simultaneously. <br/><br/>The project disseminates the results through publications and sharing data with other researchers. The research of this project contributes to the understanding and capturing of recurring patterns in higher spatial dimensions and spatiotemporal domains. Besides computer vision and computer graphics, many other research fields can also benefit from this research."
"1065463","NeTS:  Medium:  Visual MIMO Networks","CNS","Special Projects - CNS, Networking Technology and Syst","04/01/2011","08/12/2013","Marco Gruteser","NJ","Rutgers University New Brunswick","Continuing grant","Thyagarajan Nandagopal","03/31/2015","$685,000.00","Narayan Mandayam, Kristin Dana","gruteser@winlab.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1714, 7363","7363, 7924","$0.00","The increasingly ubiquitous use of cameras creates an exciting novel opportunity to build camera-based optical wireless networks. Optical wireless is not only a potential low-cost alternative where it can take advantage of existing cameras and light emitting devices, but it's highly directional transmissions can present advantages over radio-frequency (RF) based wireless communications. While the optical channel differs fundamentally from the RF channel, this project recognizes that it also allows multiple spatially separated channels between an array of transmitter elements and the array of camera pixels, akin to an RF multiple-input multiple-output (MIMO) system. This inter-disciplinary project therefore brings together expertise in the areas of mobile networks, communications, and computer vision to analyze, design, and prototype a network stack for such visual MIMO communications. This stack addresses the fundamentally different visual channel and receiver constraints through innovative visual signal acquisition, tracking, interference cancellation, and modulation techniques at the physical layer as well as vision-aware link and MAC layer protocols.<br/><br/>Visual MIMO networks can potentially support applications ranging from secure communication between cell phones, over localization of 911 callers through surveillance cameras, to interference-free car-to-car communications. The project also makes an experimental visual MIMO testbed available to the research community at large.  In addition to publications, the project takes advantage of WINLAB's biannual industry meetings to disseminate results and provides a variety of appealing educational activities involving K-12 and undergraduate students."
"1111798","SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution","CCF","Software & Hardware Foundation, EPSCoR Co-Funding","08/01/2011","09/22/2015","Jeanine Cook","NM","New Mexico State University","Continuing Grant","Almadena Chtchelkanova","07/31/2016","$899,906.00","","jcook@nmsu.edu","Corner of Espina St. & Stewart","Las Cruces","NM","880038002","5756461590","CSE","7798, 9150","7925, 7942, 9150","$0.00","The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.<br/>Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems."
"1111888","SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution","CCF","Software & Hardware Foundation","08/01/2011","07/02/2014","Andrew Lumsdaine","IN","Indiana University","Continuing Grant","Almadena Chtchelkanova","07/31/2016","$1,100,000.00","","al75@uw.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7798","7925, 7942","$0.00","The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.<br/><br/>Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, a graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems."
"1160602","SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution","CCF","Software & Hardware Foundation","08/01/2011","07/02/2014","Thomas Sterling","IN","Indiana University","Continuing Grant","Almadena Chtchelkanova","07/31/2015","$700,000.00","","tron@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7798","7925, 7942, 9150","$0.00","The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.<br/>Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, a graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems."
"1111676","SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution","CCF","Software & Hardware Foundation","08/01/2011","08/10/2011","Thomas Sterling","LA","Louisiana State University","Continuing Grant","Almadena Chtchelkanova","11/30/2011","$525,000.00","","tron@indiana.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7798","7925, 7942, 9150","$0.00","The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.<br/>Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, a graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems."
"1064429","AF: Medium: Collaborative Research: Optimality in Homology - Algorithms and Applications","CCF","ALGORITHMIC FOUNDATIONS","08/01/2011","06/27/2014","Anil Hirani","IL","University of Illinois at Urbana-Champaign","Continuing grant","Jack Snoeyink","07/31/2016","$302,583.00","","hirani@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7924, 7929, 9216, 9218","$0.00","Many applications in science and engineering encounter the problem of<br/>identifying and processing topologically interesting features in the<br/>digital representation of a geometry or data. Such features often need<br/>to be optimal with respect to some metric (measurement). It is<br/>recognized that homology groups from algebraic topology play an<br/>essential role in these computations. Although the study of structural<br/>properties of the homology groups has a rich history in mathematics,<br/>their computations in combination with geometry are not that well<br/>studied. The principal investigators (PIs) propose to study these<br/>fundamental questions thoroughly, along with their connections to<br/>practical problems from science and engineering.<br/>Intellectual merit: Efficient solutions of the optimality questions in<br/>homology computations require both mathematical and algorithmic<br/>developments. The PIs bring aboard these required expertise. Apart<br/>from the synergistic effect of the proposed study on mathematics and<br/>theoretical computer science, the close ties with various applications<br/>in science and engineering will play a synergistic role between<br/>computational fields such as computer graphics, computer vision,<br/>sensor networks, computer aided design, and scientific fields such as<br/>biology, physics, chemistry, and others.<br/><br/>Optimization of aspects of homology groups provides<br/>important insights in many scientific and engineering applications<br/>ranging from tunnels in protein molecules to voids in large<br/>machines. Solutions of such problems can aid in the manufacturing of<br/>better machines, designing of new drugs, and rapid modeling of<br/>customized objects. The educational impact of this project is in a<br/>large synergy between mathematics and computer science motivated by<br/>real applications. Course notes, internet distributions, and software<br/>systems developed through the project will enable the scientific<br/>community to study challenging problems in geometry, topology, and<br/>algorithms. Graduate students supported by the project will develop<br/>skills in mathematics and theoretical computer science and also in<br/>writing robust, efficient, and user-friendly software."
"1064600","AF: Medium: Collaborative Research: Optimality in Homology - Algorithms and Applications","CCF","ALGORITHMIC FOUNDATIONS","08/01/2011","05/06/2015","Bala Krishnamoorthy","WA","Washington State University","Continuing grant","Jack Snoeyink","07/31/2016","$276,121.00","","bkrishna@math.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7796","7924, 7929, 9218, 9251","$0.00","Many applications in science and engineering encounter the problem of<br/>identifying and processing topologically interesting features in the<br/>digital representation of a geometry or data. Such features often need<br/>to be optimal with respect to some metric (measurement). It is<br/>recognized that homology groups from algebraic topology play an<br/>essential role in these computations. Although the study of structural<br/>properties of the homology groups has a rich history in mathematics,<br/>their computations in combination with geometry are not that well<br/>studied. The principal investigators (PIs) propose to study these<br/>fundamental questions thoroughly, along with their connections to<br/>practical problems from science and engineering.<br/><br/>Intellectual merit: Efficient solutions of the optimality questions in<br/>homology computations require both mathematical and algorithmic<br/>developments. The PIs bring aboard these required expertise. Apart<br/>from the synergistic effect of the proposed study on mathematics and<br/>theoretical computer science, the close ties with various applications<br/>in science and engineering will play a synergistic role between<br/>computational fields such as computer graphics, computer vision,<br/>sensor networks, computer aided design, and scientific fields such as<br/>biology, physics, chemistry, and others.<br/><br/>Broader impacts: Optimization of aspects of homology groups provides<br/>important insights in many scientific and engineering applications<br/>ranging from tunnels in protein molecules to voids in large<br/>machines. Solutions of such problems can aid in the manufacturing of<br/>better machines, designing of new drugs, and rapid modeling of<br/>customized objects. The educational impact of this project is in a<br/>large synergy between mathematics and computer science motivated by<br/>real applications. Course notes, internet distributions, and software<br/>systems developed through the project will enable the scientific<br/>community to study challenging problems in geometry, topology, and<br/>algorithms. Graduate students supported by the project will develop<br/>skills in mathematics and theoretical computer science and also in<br/>writing robust, efficient, and user-friendly software."
"1114922","CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING","DMS","STATISTICS","07/01/2011","02/25/2011","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Haiyan Cai","06/30/2012","$15,000.00","","kass@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","Case Studies in Bayesian Statistics and Machine Learning continues the tradition of  the workshop series Case Studies in Bayesian Statistics with a meeting October 14-15, 2011. The usual format for meetings and workshops in statistics and computer science emphasize methods over applications, which often stifles discussion about the impact of the methods on the substantive problem. The unique format of the case studies workshop series has allowed for substantive discussion of application-specific issues, most importantly a narrative of how the substantive scientific problem demanded either new methods or the novel application of existing approaches, the obstacles in the real problem that the researchers encountered, and the solutions that resulted.  <br/> <br/>Statistics and machine learning provide essential methodologies throughout the sciences, yet the connection between the science and the data analytic problem formulation is rarely emphasized in traditional conferences. This workshop fosters cross-pollination of ideas from the statistics  and computer science ommunities  and promotes work that takes on important challenges in scientific investigation.  The case studies highlight the way novel application of statistical machine learning methods are used to answer a scientific question. The workshop especially supports efforts by young investigators, in part by including a session of presentations exclusively by younger investigators."
"1117489","SHB: Small: Machine Learning Models for Blood Glucose Prediction in Diabetes Management","IIS","Information Technology Researc, Info Integration & Informatics, Smart and Connected Health","08/01/2011","07/08/2015","Cynthia Marling","OH","Ohio University","Standard Grant","Sylvia Spengler","07/31/2016","$389,999.00","Razvan Bunescu, Frank Schwartz","marling@ohio.edu","108 CUTLER HL","ATHENS","OH","457012979","7405932857","CSE","1640, 7364, 8018","1640, 7364, 7923, 8018, 9251","$0.00","The goal of this project is to develop machine learning models that leverage underutilized data and account for individual patient differences to improve diabetes management. Diabetes is a chronic disease, which must be treated and managed over a lifetime. In type 1 diabetes, the pancreas does not produce insulin, an essential hormone needed to convert food into energy. The disease is treated with insulin and managed by monitoring and controlling blood glucose levels. Good blood glucose control is key to avoiding serious diabetic complications, but achieving and maintaining good blood glucose control is difficult. Patients are highly individual in their responses to treatment and to life events that affect blood glucose levels. Large volumes of blood glucose data are collected automatically, but automated analysis is lacking. Patients do not always know when problems are impending; problems occurring while patients are asleep are especially dangerous.  Machine learning models that predict blood glucose levels would enable or facilitate new applications of direct benefit to patients, including: alerts to immediately notify patients of impending problems; decision support systems recommending actions to prevent problems; and educational simulations showing the effects of different treatment choices or lifestyle options on blood glucose levels.<br/><br/>The task of blood glucose prediction is approached as a time series forecasting problem. Blood glucose is predicted based on a patient's prior blood glucose levels, insulin data, meal data, exercise data, sleep patterns and work schedules. Batch and incremental time series regression models, including support vector machines and neural networks, are being investigated. To account for individual patient differences, separate models are trained for each patient. However, transfer learning may enable data from multiple patients to aid in building models for patients with limited historical data. Models are sought that are robust in the face of imperfect data, including missing life events, inaccurately recorded life events, and noisy glucose sensors. Disjoint sets of training and testing data extracted from non-overlapping time intervals are used to build models that are compared against baselines such as autoregressive integrated moving average models. Standard metrics for comparing models, such as root mean square error and coefficient of determination, will be supplemented with domain dependent measures of goodness, including the Clarke error grid.<br/><br/>This work aims to improve the overall health and wellbeing of the nearly two million Americans with type 1 diabetes. Predicting blood glucose problems in advance gives patients time to take steps to prevent the predicted problems from occurring. This improves blood glucose control, which is known to reduce the risk of serious diabetic complications, including blindness, amputations, kidney disease, strokes, heart attacks, and death from severe hypoglycemia. In addition, this research project forms the cornerstone of a new Smart Health and Wellbeing Laboratory at Ohio University. This new laboratory is designed to promote further interdisciplinary research among computer scientists and health care professionals as well as to attract more women to careers in computer science. Additional information about the project and the laboratory is available at http://oucsace.cs.ohiou.edu/~marling/shb.html."
"1135660","CPS:Medium:Quantitative Visual Sensing of Dynamic Behaviors for Home-based Progressive Rehabilitation","CNS","Information Technology Researc","12/01/2011","08/30/2011","Yun Fu","NY","SUNY at Buffalo","Standard Grant","Sylvia Spengler","02/28/2013","$1,200,000.00","Venkat Krovi, Dan Ramsey","y.fu@neu.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1640","1640, 7752, 7918, 7924","$0.00","The objective of this research is to develop a comprehensive theoretical and experimental cyber-physical framework to enable intelligent human-environment interaction capabilities by a synergistic combination of computer vision and robotics. Specifically, the approach is applied to examine individualized remote rehabilitation with an intelligent, articulated, and adjustable lower limb orthotic brace to manage Knee Osteoarthritis, where a visual-sensing/dynamical-systems perspective is adopted to: (1) track and record patient/device interactions with internet-enabled commercial-off-the-shelf computer-vision-devices; (2) abstract the interactions into parametric and composable low-dimensional manifold representations; (3) link to quantitative biomechanical assessment of the individual patients; (4)  facilitate development of  individualized user models and exercise regimen; and (5) aid the progressive parametric refinement of exercises and adjustment of bracing devices. This research and its results will enable us to understand underlying human neuro-musculo-skeletal and locomotion principles by merging notions of quantitative data acquisition, and lower-order modeling coupled with individualized feedback. Beyond efficient representation, the quantitative visual models offer the potential to capture fundamental underlying physical, physiological, and behavioral mechanisms grounded on biomechanical assessments, and thereby afford insights into the generative hypotheses of human actions.<br/><br/>Knee osteoarthritis is an important public health issue, because of high costs associated with treatments. The ability to leverage a quantitative paradigm, both in terms of diagnosis and prescription, to improve mobility and reduce pain in patients would be a significant benefit. Moreover, the home-based rehabilitation setting offers not only immense flexibility, but also access to a significantly greater portion of the patient population. The project is also integrated with extensive educational and outreach activities to serve a variety of communities."
"1065251","RI: Medium: Interactive Transfer Learning in Dynamic Environments","IIS","Robust Intelligence","09/01/2011","06/18/2013","Jaime Carbonell","PA","Carnegie-Mellon University","Continuing grant","Hector Munoz-Avila","08/31/2014","$1,048,227.00","Avrim Blum","jgc@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7924","$0.00","Machine learning (ML) has witnessed tremendous success both in establishing firm theoretical foundations and reaching out to major applications ranging from the scientific (e.g. computational biology) to the practical (e.g. financial fraud detection, spam detection).  However the reach of machine learning has been hampered by an underlying inductive framework that largely has not evolved from using only labeled instances of concepts (e.g. emails and yes/no labels on whether they are spam) and its overly simple view of the role of the user or subject matter expert (SME) as a mere provider of the labels for the training instances. However, when instructing humans, teachers provide richer information: Why is an instance of a concept a good positive example? What are key differences between instances belonging to different classes? Which properties are transient and which are invariant?  Where should the learner focus attention?  What does the current learning task have in common with previously acquired concepts or processes? Answers to such questions not only enrich the learning process, but they also can effectively reduce the hypothesis space and provide significant speed ups in learning than can be achieved with use of class membership feedback only.<br/><br/>The aim of this project is to bring this kind of richer interaction into the realm of machine learning by developing frameworks as well as machine learning methods that can take advantage of fuller mixed-initiative communication. In particular, this project aims to develop ML algorithms that can exploit information from SME's such as (1) identification of landmark instances; (2) proposing rules of thumb; (3) providing feedback on similarity of instances; and (4) transfer of similarity measures themselves. This project brings to bear four streams of research: (1) algorithms based on similarity functions and landmark instances; (2) active and ""pro-active"" learning; (3) Bayesian active transfer learning; and (4) learning to cope with temporal evolution in the underlying data distribution. In order to reach practical results, this project focuses on challenges where these new methods are both most needed and likely to prove most effective, such as learning in dynamic environments with concept drift, and where potential for long-term transfer learning is present. Broader impacts include more effective learning by incorporating scientific domain knowledge in eScience, for instance in computational proteomics.  Educational and research-community outreach includes participation of graduates and undergraduates from Howard University, for instance in yearly research gatherings involving all students on the project, and reusable open-source methods and data sets."
"1115293","Polynomial Optimization and Convex Algebraic Geometry","DMS","COMPUTATIONAL MATHEMATICS","07/01/2011","06/06/2011","Rekha Thomas","WA","University of Washington","Standard Grant","Junping Wang","09/30/2014","$342,819.00","","thomas@math.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1271","9263","$0.00","This study focuses on problems from polynomial<br/>optimization and convex algebraic geometry. The latter is a new<br/>research area that concerns convex sets and convex hulls of sets that<br/>are described algebraically and arise in optimization.  The key tool is the use of efficient algorithms in  semidefinite programming, a branch of convex optimization  that is used in polynomial optimization. The<br/>first set of questions studies the general phenomenon of when a given<br/>convex body is the linear projection of a slice (by an affine plane)<br/>of a closed convex cone. This phenomena is central to all<br/>lift-and-project methods for discrete and polynomial optimization. This study<br/>provides a uniform view of all lift-and-project<br/>methods via new notions of cone factorizations of certain operators<br/>associated to the convex body. The investigator and collaborators have<br/>recently constructed a new hierarchy of convex relaxations for<br/>algebraic sets called theta bodies. Various open questions about these<br/>bodies are posed. The methods<br/>from polynomial optimization and convex algebraic geometry can be applied to problems<br/>from computer vision. Here the application is primarily to object reconstruction from<br/>images taken by multiple cameras.<br/><br/>The work of the PI with her collaborators improvies our understanding of the algebraic and geometric<br/>structures that underlie optimization problems that involve<br/>polynomials. Such problems have a wide array of applications and admit<br/>methods from both the algebraic and analytic sides of<br/>mathematics. This research considers both improvements in our understanding of the theoretical aspects of polynomial optimization, and the application of these methods to problems in computer vision."
"1108845","Beyond Boredom: Modeling and Promoting Engagement during Complex Learning","DRL","REAL","09/01/2011","08/22/2011","Sidney D'Mello","TN","University of Memphis","Standard Grant","James S. Dietz","08/31/2012","$1,083,868.00","Arthur Graesser","sidney.dmello@gmail.com","Administration 315","Memphis","TN","381523370","9016783251","EHR","7625","9150, 9177, 9178, SMET","$0.00","The core research question of this proposal is how interactions between learners (i.e., individual differences), instructional materials (i.e., the text), and learning activities (i.e., the task) modulate engagement during learning of critical thinking skills and scientific reasoning. The proposed research will address this goal by: (a) systematically investigating the mechanisms that facilitate or hinder engagement, and (b) leveraging these insights towards the development of interventions that promote persistent and productive engagement trajectories during deep learning.   The work will be conducted at the University of Memphis.  The research subjects will be undergraduate students. <br/><br/>The research design includes four experiments in which learning gains and self-reported engagement, physiological arousal, eye gaze patterns, and facial features will be tracked while learners study instructional texts. Comprehending these texts for mastery requires active engagement as learners generate inferences, understand causality, identify problems, discriminate the quality of experimental designs, and ask diagnostic questions. Analyses will be conducted using nonlinear time series analysis techniques, such as recurrence quantification analysis. The project evaluation will include an expert advisory committee that will be used to critically review the investigators' findings and interpretations.  In addition, the investigators will develop and validate a web-based computer program that dynamically tailors both the instructional text and the learning activity to the needs and learning styles of individual learners to enhance engagement. The proposed research will balance the theoretical goal of theory building and model testing via systematic experimentation with the practical goal of developing innovative advanced learning technologies that aspire to promote engagement and learning of difficult subject matter.<br/><br/>This research is important in the STEM education field's ongoing efforts to increase engagement and the productivity of learning of STEM subject matter.  If the research is successful, the derivative knowledge and tools will be significant contributions and could be applied widely in other intelligent tutoring systems, other instructional technologies, and in our understanding of student learning in general.  This research is potentially transformative in two ways.  First, it will provide a detailed understanding in real time of engagement at the micro, relational level of student, task, and materials. Second, the creation of an intelligent tutoring system based on these findings holds the possibility of being able to 'correct' low levels of student engagement on a moment-to-moment basis and therefore boost learning productivity while increasing student satisfaction and engagement with the experience.  Dissemination will include the public availability of the technological tools as well as contributions to the scholarly literature."
"1116584","RI: Small: Endowing Graph-Based Image Segmentation with Global 'Advice': Applications to Diffusion Tensor Images","IIS","ROBUST INTELLIGENCE","09/01/2011","04/16/2012","Vikas Singh","WI","University of Wisconsin-Madison","Standard Grant","Jie Yang","08/31/2015","$363,146.00","","VSINGH@CS.WISC.EDU","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7495","7923, 9251","$0.00","The primary goal of this project is to enable identification and segmentation of specific structures of interest from imaging data (such as DTI MR brain images). These regions may be small and inconspicuous with poor contrast, therefore, direct application of classical unsupervised segmentation (designed to extract ""salient"" regions) is problematic. The alternative pursued here is a system to leverage expert-like high level advice within the image segmentation process: to do this, the underlying engine is endowed with global constraints encoding (a) effort already expended by the user in segmenting similar images in the past, as well as (b) aggregate knowledge from a cohort of similar images. The key algorithmic component is the design of mechanisms to translate such constraints (as best as possible) to a combinatorial framework so that the resultant models can be optimized efficiently for high resolution 3-D imaging data. This research produces the methodology and accompanying software for this important image analysis task. <br/><br/>The project has broad scientific impact. Wide distribution of code produced from this research can enable improvements in various computer vision and medical imaging problems where image segmentation is a key step. Additionally, the algorithms developed here have applications in other problems such as object recognition and image categorization. The project is also well suited to involve undergraduate and graduate students from a diverse spectrum of backgrounds in cutting edge inter-disciplinary computer vision and image processing research."
"1102440","RUI: Complex Structures, Hyperbolic Invariants, Infinitesimal Currents and Intersection Numbers for Deformation Spaces","DMS","ANALYSIS PROGRAM","08/15/2011","05/14/2013","Dragomir Saric","NY","CUNY Queens College","Continuing grant","Bruce P. Palka","07/31/2014","$153,999.00","","dragomir.saric@qc.cuny.edu","65 30 Kissena Blvd","Flushing","NY","113671575","7189975400","MPS","1281","9229","$0.00","The Principal Investigator studies Teichmuller spaces of surfaces, both closed and open. The Teichmuller space of a surface is the space of all possible shapes of the surface, where a shape of the surface is the hyperbolic metric on a surface up to isometries homotopic to the identity. Therefore the invariants of hyperbolic metrics on a surface are used in the study of Teichmuller spaces. The PI?s approach is to first consider the Teichmuller space of the hyperbolic plane called the universal Teichmuller space as this space contains all other Teichmuller spaces. An invariant called a shear associated to an ideal triangulation of the hyperbolic plane is used to parameterize the universal Teichmuller space. The PI intends to continue his study of the universal Teichmuller space and related Teichmuller spaces in terms of these invariants called shears. In particular, the PI intends to describe the Weil-Petersson metric on the Teichmuller space of a finitely punctured closed surface in terms of shears on ideal triangulations of the surface, where triangulations can be both locally finite and locally infinite. He also intends to implement these formulas on the computer with the help of some undergraduate and masters students from Queens College. Another direction in applying shear invariants to the Teichmuller spaces is to find a parameterization of Takhtajan-Teo Teichmuller space in terms of shears and to find a formula for the Weil-Petersson metric in this space. This direction has possible applications to Sharon-Mumford?s approach to two-dimensional shape analysis in Computer Vision. The Quasifuchsian space of a closed surface supports a Weil-Petersson metric as well which is defined by taking the second partial derivative of the product of the Hausdorff dimension of the limit quasicircle and the Sullivan-Paterson measure. The PI intends to investigate the infinitesimal Sullivan-Paterson measures and their intersection numbers to obtain another expression for the Weil-Petersson metric on the Quasifuchsian space similar to the situation of the Fuchsian (Teichmuller) space.<br/><br/>Riemann surfaces are two-dimensional objects which locally look like open subsets of a plane and that have transition maps which preserve angles. Each Riemann surface supports a unique hyperbolic metric in its class of conformal metrics. The PI studies the variations of hyperbolic metrics on a surface thought of as a single space of metrics called the Teichmuller space. The Teichmuller space is of interest in complex analysis, low-dimensional topology, dynamics, differential geometry and physics. One aspect of the project is related to the Computer Vision given by the approach of Sharon-Mumford as well as to the mathematical physics in the approach of Nag-Sullivan and Takhtajan-Teo. The project is building tools for study of the universal Teichmuller space and it has a potential for applications to the above mentioned fields. Another part of the project involves undergraduate and masters students from Queens College. The students participating in the project will be exposed to an active research agenda thus contributing to the human resource development in the sciences and engineering."
"1119290","Enhancing Games with Assessment and Metacognitive Emphases (EGAME)","DRL","DISCOVERY RESEARCH K-12","09/01/2011","09/06/2016","Douglas Clark","TN","Vanderbilt University","Continuing grant","Arlene M. de Strulle","08/31/2017","$3,249,201.00","Gautam Biswas, Pratim Sengupta, Jim Minstrell, Daniel White","doug.clark@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","EHR","7645","9177, SMET","$0.00","This development and research project from Vanderbilt University, Facet Innovations, and Filament Games, designs, develops, and tests a digital game-based learning environment for supporting, assessing and analyzing middle school students' conceptual knowledge in learning physics, specifically Newtonian mechanics. This research integrates work from prior findings and refines computer assisted testing and Hidden Markov Modeling to develop a new methodology to engage students in deep learning while diagnosing and scaffolding the learning of Newtonian mechanics.<br/><br/>The project uses a randomized experimental 2 x 1 design comparing a single control condition to a single experimental condition with multiple iterations to test the impact of the game on the learning of Newtonian physics. Using designed based research with teachers and students, the researchers are iteratively developing and testing the interactions and knowledge acquisition of students through interviews, pre and post tests and stealth assessment.  Student learner action logs are recorded during game-play along with randomized student interviews.  Students' explanations and game-play data are collected and analyzed for changes in domain understanding using pre-post tests assessment. <br/><br/>The project will afford the validation of EGAME as an enabler of new knowledge in the fields of cognition, conceptual change, computer adaptive testing and Hidden Markov Modeling as 90 to 300 middle school students learn Newtonian mechanics, and other science content in game-based learning and design. The design of this digital game platform encompasses a very flexible environment that will be accessible to a diverse group of audiences, and have a transformational affect that will advance theory, design and practice in game-based learning environments."
"1108896","In Touch with Molecules: Extending Learning with Cyber-enabled Tangibles","DRL","REAL","09/01/2011","01/04/2012","Jodi Davenport","CA","WestEd","Standard Grant","Celestine Pea","08/31/2016","$999,630.00","Arthur Olson, Matt Silberglitt","jdavenp@wested.org","730 Harrison Street","San Francisco","CA","941071242","4156153136","EHR","7625","9177, SMET","$0.00","The purpose this project is to examine the capabilities of cyber-enabled tangible tools in real educational settings with five teachers (three high schools and two college) and 450 high school and college students. The project evaluates the use of cyber-enabled tangible tools to enhance student's understanding of three difficult topics in molecular biology: (1) protein structure, (2) DNA, and (3) viruses. As core concepts of molecular biology instruction in the U.S., students often have difficulties understanding how molecular interactions result in structures that give rise to function. The intent of this project is to help students understand mappings between structure, function, and emergent properties and processes. Hence, the cyber-enabled tangible tools will test whether technology enhances and improves how difficult subject matter is taught. Additionally, the study provides information about what areas of content can be effectively conveyed with these technologies and for which levels of students (high school or college).<br/><br/>The broader impact of the project are potentially transformative improvements in medicine, pharmacology, agriculture, ecology education. The tools promote computer-human interactions at the atomic, molecular, and organ levels. <br/><br/>Hand-held objects with markers, similar to barcodes, use a web cam to track and illustrate different properties and features on a computer screen. Research shows that as molecular biology grows in prominence, there is greater need for materials and scaffolds that promote deep learning and motivation. To help meet those needs, the project brings together leaders from top research entities, researchers in cognition and student learning, scientists, science educators, teachers, and high school and college students to investigate how cyber-enabled tangibles promote different and improved ways to learn core concepts in molecular biology."
"1115185","The 2011 Machine Learning Summer School at Purdue University","IIS","STATISTICS, Info Integration & Informatics, Robust Intelligence","03/01/2011","06/09/2011","Vishwanathan Swaminathan","IN","Purdue University","Standard Grant","Todd Leen","02/29/2012","$33,600.00","Jennifer Neville, Luo Si, Sergey Kirshner, Tao Wang","vishy@ucsc.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1269, 7364, 7495","7364, 7495, 7556","$0.00","This award provides support to students and other young researchers for travel and accommodation to the Machine Learning Summer School 2011 to be held in Purdue University, June 13-24, 2011.  World class speakers from academia and industry are delivering tutorial style lectures over a two week period, and these presentations are being recorded and made available over the Internet. The Summer School is suitable for participants with different backgrounds. Individuals without previous knowledge are able to learn more about the theory and practice of Machine Learning, while those wishing to broaden their expertise in this area find the advanced courses particularly useful. The participating students are able to network with international experts."
"1054057","CAREER:  Statistical Models and Classification of Time-Varying Shape","IIS","ROBUST INTELLIGENCE, EPSCoR Co-Funding","06/01/2011","04/16/2012","Preston Thomas Fletcher","UT","University of Utah","Standard Grant","Jie Yang","05/31/2017","$412,961.00","","tomfletcher@virginia.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7495, 9150","1045, 1187, 7495, 9150, 9251","$0.00","This project develops nonlinear statistical models and classification procedures for time-varying shape and investigates their application to biomedical image analysis problems. In biology and medicine it is often critical to understand processes that change the shape of anatomy. For example, a neuroscientist studying the development of the infant brain would be interested in how neurodevelopment is different in healthy children versus those with Autism. An evolutionary biologist studying how a species has evolved to adapt to its environment would be interested in studying changes in the shape of bones found in the fossil record. The challenge in this modeling problem is that shape and shape variations are highly nonlinear and high-dimensional, and standard linear statistics cannot be applied. Therefore,  the ability to model and understand changes in shape depends on the development of new regression models for data in nonlinear spaces. The research activities of this project include: (1) developing statistical models for dealing with time-varying shape using least-squares principles in shape manifolds, (2) investigating new classification methods for shape sequences, and (3) validating the methodology using synthetic data and testing its efficacy for neuroimaging applications in Alzheimer's disease and Autism. In addition to the significant impact to computer vision, biology, and medicine,  this project is combining differential geometry, statistics, and computing within the undergraduate and graduate computer science curriculum."
"1117716","RI: Small: Learning the Relationship between the Anatomy and Spatial Hearing","IIS","Robust Intelligence","09/01/2011","09/07/2011","Ramani Duraiswami","MD","University of Maryland College Park","Standard Grant","Todd Leen","08/31/2013","$164,999.00","Hal Daume, Dmitry Zotkin","ramani@umiacs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7923","$0.00","To apply machine learning to problems in the physical world, one needs models/algorithms that are faithful to physics.  We consider understanding how the anatomical structure of the body and ears leads to the remarkable ability to localize a sound source in a complex and noisy environment that is innate in most animals and humans. The cues used in localization arise from the process of the acoustic wave scattering off the complex-shaped listener's body and ears. Numerically, these changes in the sound spectrum are characterized by the head-related transfer function (HRTF). Every person's body is unique, and the HRTF is highly individual. It is possible to measure the HRTF; however, the measurement requires specialized hardware and is tedious. There has been considerable interest in convenient methods to obtaining the HRTF.  We propose to develop a framework to perform machine learning to establish a relationship between the anatomy and HRTF. An HRTF database with 100 subjects, along with their anthropometric measurements, is available. A novel LMA (Learning of Multiple Attributes) algorithm will be developed. The key properties of this algorithm are that it can incorporate physical constraints into the learning and predict complex structured outputs in continuous spaces.  The algorithm will find the low-dimensional manifold in high-dimensional HRTF space and to map the manifold structure to anatomical parameters. <br/><br/>The research will create novel machine learning algorithms that are able to incorporate physics based constraints, and these will find application in other problems. HRTF generation from simple body measurements will allow introduction of personalized spatial audio into fields such as human-computer interaction, consumer electronics, auditory assistive devices for the vision-impaired, robotics, entertainment, education, and surveillance. Training of K-16 and graduate students in the proposed research will add to the nations talent pool."
"1018829","AF: Small: Learning in Worst-Case Noise Models","CCF","ALGORITHMS","09/01/2011","05/19/2016","Adam Klivans","TX","University of Texas at Austin","Standard Grant","jack snoeyink","09/30/2016","$499,864.00","","klivans@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7926","9150, 9218, HPCC","$0.00","Machine Learning algorithms are ubiquitous in computer science with important applications to data-mining, classification, and ranking.  These algorithms are typically applied to data sets that contain a sizable fraction of noisy training examples.  This project focuses on developing learning algorithms that can succeed in the presence of noisy data sets that have been corrupted in a potentially adversarial or malicious manner.  Algorithms that can tolerate these types of worst-case noise are critical for the depolyment of complex machine learning systems, as real-world data sets (for example, data related to spam detection) are often noisy in unpredictable ways.  Previous work on learning in the presence of noise focused on models with strong assumptions on how the noise is applied (e.g., independently for each data point).<br/><br/>The intellectual merit of this project lies in understanding the computational complexity of optimization problems associated with learning in worst-case noise models.  More specifically, the project will design algorithms that can find a classifier whose error is competitive with the best function from a large class of concepts.  In order to design these algorithms, the project will prove new structural results on how well classes of Boolean functions can be approximated with respect to a variety of well-studied probability distributions.  Additionally, the project will explore hardness results for learning functions with respect to adversarial noise via reductions to notoriously difficult problems in cryptography and computational complexity.<br/><br/>The broader impact of this project is the potential to realize more powerful classification tools in a variety of application areas in the sciences such as computational biology (e.g., protein detection) and linguistics (e.g., text categorization).  Additionally, the PI will develop a new graduate course that furthers the relationship between computational and statistical methods in machine learning theory."
"1115703","AF: Small:  The Boundary of Learnability for Monotone Boolean Functions","CCF","ALGORITHMIC FOUNDATIONS","09/01/2011","06/16/2011","Rocco Servedio","NY","Columbia University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2014","$350,000.00","","rocco@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7796, 7923, 7926","$0.00","Machine learning is a dynamic and rapidly growing research area that plays an important role in many applications over a diverse range of areas including scientific discovery, search technology, finance, natural language, and more.  An important goal in machine learning theory is to understand which types of binary classification rules (i.e. Boolean functions) can be efficiently learned from labeled data, and which cannot. This proposal describes a detailed program of theoretical research on understanding the learnability of different types of monotone Boolean functions from uniform random examples. Monotone functions are highly natural from a learning point of view; they are also a central class of functions in computational complexity theory and the analysis of Boolean functions, and the study of their learnability has close connections to these areas.<br/><br/>Recent years have seen exciting advances both on efficient algorithms and on hardness results for learning monotone functions. The PI believes that building on this progress, a fine-grained understanding of the boundary between learnable and unlearnable classes of monotone functions may be within reach. More precisely, the PI will work to show that monotone DNF formulas (depth-2 circuits) are efficiently learnable, while monotone depth-3 circuits are not. Establishing this would be a landmark in our understanding of the learnability of this important class of Boolean functions.<br/><br/>On the positive side the PI will work on a range of intermediate problems, leading up to the goal of obtaining a poly(n)-time algorithm for learning arbitrary poly(n)-term monotone DNF formulas:<br/><br/>* Learning Monotone Decision Trees Better. The PI will analyze a widely used machine learning heuristic for decision tree induction and work to show that it is in fact an efficient algorithm for learning poly(n)-size monotone decision trees.<br/><br/>* Learning Monotone CDNF. Using results and techniques from discrete Fourier analysis of Boolean functions, the PI will work to obtain a polynomial time algorithm for monotone Boolean functions whose CNF (Conjunctive Normal Form) size and DNF (Disjunctive Normal Form) size are both polynomial in n (a broader class than poly(n)-size monotone decision trees).<br/><br/>* Learning Monotone DNF Formulas. The PI has developed an algorithm for learning monotone DNF formulas with a subpolynomial number of terms; using different techniques he has also given a poly(n)-time algorithm that can learn random poly(n)-size monotone DNF formulas. The PI will work to unify these two approaches to obtain a single, more powerful, algorithm for learning monotone DNF.<br/><br/>* Other approaches. The PI will study other approaches that may be useful for monotone function learning  problems: 1) analyzing the distribution of ""Fourier weight"" in monotone functions; 2) applying specialized boosting algorithms to learn monotone functions; and 3) using conjectures in Fourier analysis of Boolean functions as tools toward learning results.<br/><br/>Building on his recent work, the PI will also work to establish two types of negative results for learning monotone functions: cryptographic hardness results, and lower bounds for Strong Statistical Query learning. The goal in both cases is to show that learning depth-3 monotone circuits is hard; techniques for monotone hardness amplification in complexity theory are expected to play a role in both of these directions."
"1125098","Collaborative Research: CDI-Type II: BirdCast: Novel Machine Learning Methods for Understanding Continent-Scale Bird Migration","IIS","Info Integration & Informatics, CDI TYPE II","09/01/2011","08/05/2015","Steven Kelling","NY","Cornell University","Standard Grant","Sylvia Spengler","09/30/2016","$1,309,845.00","Wesley Hochachka, Andrew Farnsworth","stk2@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364, 7751","7364, 7721, 7751, 9251","$0.00","An interdisciplinary team of computer scientists, statisticians, and ornithologists will develop novel computer science methods and apply them to the challenge of understanding the annual migration of birds across North America, which is one of the most complex and dynamic natural phenomena on the planet. While direct observation of migrating birds is limited to a handful of birds  wearing tracking devices, other sources of data provide partial information about migration that, when appropriately combined, will provide insight into migration at a scale previously unimaginable. These sources include a continent-wide network of volunteer bird watchers, night flight calls captured by a network of acoustic monitoring stations, continent-scale weather patterns gathered by a network of weather stations, and clouds of migrating birds detected at night by WSR-88D weather radar stations. To analyze these data, the team  will develop two innovative machine learning techniques-Collective Graphical Models (CGMs) and Semi-Parametric Latent Process Models (SLPMs). The resulting model will be able to identify the complex conditions governing the dynamics of migration behavior including the choice of migratory pathways, the factors that influence when birds migrate, and the speed and duration of each night's movements. CGMs greatly extend the scope of phenomena that can be captured with graphical models. Under suitable conditions, a CGM is able to recover a model of the behavior of individuals using only collective observations.<br/><br/>For BirdCast, it will construct a model of individual bird dynamics from the collective observations provided by birders, acoustic and weather stations, and weather radar. Once the model is constructed, it will be applied to live data feeds (bird sightings, acoustic detections, radar detections, and weather forecasts) to predict bird migration in real time. SLPMs are an extension of latent process models, such as the CGM for bird migration, in which the dynamics of a process is represented by latent variables that are observed only indirectly. In an SLPM, the conditional probability distribution of each variable is modeled using flexible, non-parametric methods from machine learning, such as boosted regression trees. Introducing such flexible methods such as CGMs and SLPMs into latent variable models raises difficult challenges for model fitting and validation. Preventing over-fitting will require the creation of novel information regularization and latent model cross-validation methods to enforce latent variable semantics.<br/><br/>The proposed work will allow, for the first time, real-time predictions of bird migrations: when they migrate, where they migrate, and how far they will be flying. Accurate models of migration have broad application for basic research by allowing researchers to understand behavioral aspects of migration, how migration timing and pathways respond to variation in climatic conditions, and whether linkages exist between annual variation in migration timing and subsequent inter-annual changes in population size.<br/><br/>BirdCast will expand opportunities for the public to participate in the gathering of data and its analysis.  The existing data set has more than 60 million observations, and the size is growing exponentially. Last year, volunteers contributed more than 1.3 million hours observing birds.  Student engagement in the research is significant as well."
"1059284","II-EN: Computing research infrastructure for constraint optimization, machine learning, and dynamical models for computational sustainability","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2011","06/03/2011","Carla Gomes","NY","Cornell University","Standard Grant","Vasant G. Honavar","06/30/2012","$378,016.00","Jon Conrad, David Shmoys, Kenneth Rosenberg","gomes@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7359","7359","$0.00","II-EN: Computing research infrastructure for constraint optimization, machine learning, and dynamical models for Computational Sustainability<br/><br/>High performance computing infrastructure will be acquired to support the research, outreach, and educational activities of researchers engaged by the Institute for Computational Sustainability  (ICS) in a number of  research projects which have significant computational needs. Computational sustainability  is a new interdisciplinary field that aims to apply techniques from computer science, applied mathematics and related disciplines to help the balancing of environmental, economic, and societal needs for sustainable development. Computational sustainability research brings together computational sciences and a variety of other disciplines to focus on developing computational models, methods, and tools for supporting the design of sustainable policies, practices, products, and tools. ICS has established a number of significant research projects in areas ranging from biodiversity conservation, to natural resource management, poverty mapping, and material discovery for fuel cell technology. <br/><br/>Computational sustainability research is already leading to foundational contributions in several areas of Computer Science. Computational Sustainability presents decision and optimization problems with a mixture of continuous and discrete variables in highly dynamic and uncertain environments, pushing the boundaries of the current state-of-the art of computer science. ICS research focuses on integrating techniques from constraint reasoning, optimization, dynamical systems, machine learning, and data mining, in order to obtain effective dynamic decision theoretic models to address sustainability problems. The computing infrastructure requested  would allow ICS researchers including 21 faculty, 53 students (including 24 undergraduates), and over a 100 collaborators  to scale up their work to larger  problems than they would have been otherwise be able to solve. <br/><br/>ICS research has direct impacts on policy makers and practitioners engaged in sustainability work. For example, working in collaboration with the Laboratory of Ornithology at Cornell, ICS researchers have provided computational analysis for the U.S. Department of the Interior's 2011 State of the Birds report. In collaboration with the U.S. Forest Service, ICS research is improving the design of wildlife corridors for species such as grizzly bears, wolverines, and lynx. ICS is working with The Conservation Fund to develop conservation plans that will be used by government and conservation agencies. In collaboration with the Cornell Fuel Cell Institute, ICS members are developing automated tools to support the discovery of new materials for fuel cell technology. ICS is also building a vibrant computational sustainability research community through conferences, workshops, lectures, and online discussions. Combined with ICS education and outreach activities, these research efforts promote teaching, training, and the advancement of women and underrepresented minorities in computer science. Additional information regarding the ICS and its efforts to meet the critical societal, environmental, and economic needs for knowledge, methods, and tools that advance computational sustainability efforts can be found at the ICS website: www.cis.cornell.edu/ics"
"1125228","Collaborative Research: CDI-Type II: BirdCast: Novel Machine Learning Methods for Understanding Continent-Scale Bird Migration","IIS","Info Integration & Informatics, CDI TYPE II","09/01/2011","09/09/2012","Thomas Dietterich","OR","Oregon State University","Standard Grant","Sylvia Spengler","08/31/2016","$1,000,865.00","","tgd@cs.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7364, 7751","7721, 7751, 9251","$0.00","An interdisciplinary team of computer scientists, statisticians, and ornithologists will develop novel computer science methods and apply them to the challenge of understanding the annual migration of birds across North America, which is one of the most complex and dynamic natural phenomena on the planet. While direct observation of migrating birds is limited to a handful of birds  wearing tracking devices, other sources of data provide partial information about migration that, when appropriately combined, will provide insight into migration at a scale previously unimaginable. These sources include a continent-wide network of volunteer bird watchers, night flight calls captured by a network of acoustic monitoring stations, continent-scale weather patterns gathered by a network of weather stations, and clouds of migrating birds detected at night by WSR-88D weather radar stations. To analyze these data, the team  will develop two innovative machine learning techniques-Collective Graphical Models (CGMs) and Semi-Parametric Latent Process Models (SLPMs). The resulting model will be able to identify the complex conditions governing the dynamics of migration behavior including the choice of migratory pathways, the factors that influence when birds migrate, and the speed and duration of each night's movements. CGMs greatly extend the scope of phenomena that can be captured with graphical models. Under suitable conditions, a CGM is able to recover a model of the behavior of individuals using only collective observations.<br/><br/>For BirdCast, it will construct a model of individual bird dynamics from the collective observations provided by birders, acoustic and weather stations, and weather radar. Once the model is constructed, it will be applied to live data feeds (bird sightings, acoustic detections, radar detections, and weather forecasts) to predict bird migration in real time. SLPMs are an extension of latent process models, such as the CGM for bird migration, in which the dynamics of a process is represented by latent variables that are observed only indirectly. In an SLPM, the conditional probability distribution of each variable is modeled using flexible, non-parametric methods from machine learning, such as boosted regression trees. Introducing such flexible methods such as CGMs and SLPMs into latent variable models raises difficult challenges for model fitting and validation. Preventing over-fitting will require the creation of novel information regularization and latent model cross-validation methods to enforce latent variable semantics.<br/><br/>The proposed work will allow, for the first time, real-time predictions of bird migrations: when they migrate, where they migrate, and how far they will be flying. Accurate models of migration have broad application for basic research by allowing researchers to understand behavioral aspects of migration, how migration timing and pathways respond to variation in climatic conditions, and whether linkages exist between annual variation in migration timing and subsequent inter-annual changes in population size.<br/><br/>BirdCast will expand opportunities for the public to participate in the gathering of data and its analysis.  The existing data set has more than 60 million observations, and the size is growing exponentially. Last year, volunteers contributed more than 1.3 million hours observing birds.  Student engagement in the research is significant as well."
"1043903","EMSW21 - RTG: STATISTICS AND MACHINE LEARNING FOR SCIENTIFIC INFERENCE","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","07/15/2011","07/01/2015","Robert Kass","PA","Carnegie-Mellon University","Continuing grant","Gabor Szekely","06/30/2017","$2,250,982.00","William Eddy, Kathryn Roeder, Larry Wasserman, Christopher Genovese","kass@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 7335","7301","$0.00","Statistics curricula have required excessive up-front investment in statistical theory, which many quantitatively-capable students in ``big science'' fields initially perceive to be unnecessary.  A research training program at Carnegie Mellon exposes students to cross-disciplinary research early, showing them the scientific importance of ideas from statistics and machine learning, and the intellectual depth of the subject. Graduate students receive instruction and mentored feedback on cross-disciplinary interaction, communication skills, and teaching. Postdoctoral fellows become productive researchers who understand the diverse roles and responsibilities they will face as faculty or members of a research laboratory.<br/><br/>The statistical needs of the scientific establishment are huge, and growing rapidly, making the current rate of workforce production dangerously inadequate.  The research training program in the Department of Statistics at Carnegie Mellon University trains undergraduates, graduate students, and postdoctoral fellows in an integrated environment that emphasizes the application of statistical and machine learning methods in scientific research. The program builds on existing connections with computational neuroscience, computational biology, and astrophysics.  Carnegie Mellon is recruiting students from a broad spectrum of quantitative disciplines, with emphasis on computer science.  Carnegie Mellon already has an unusually large undergraduate statistics program. New efforts will strengthen the training of these students, and attract additional highly capable students to be part of the pipeline entering the mathematical sciences."
"1055062","CAREER: Large Vocabulary Gesture Recognition for Everyone: Gesture Modeling and Recognition Tools for System Builders and Users","IIS","HCC-Human-Centered Computing","04/01/2011","05/11/2016","Vassilis Athitsos","TX","University of Texas at Arlington","Continuing Grant","Ephraim Glinert","03/31/2017","$651,563.00","","athitsos@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7367","1045, 7218, 7233, 7367, 9251","$0.00","The PI's goal in this project is to develop new methods for automatically annotating, recognizing, and indexing large vocabularies of gestures, and to use these methods to create an integrated set of tools for sign language recognition.  Current state-of-the-art methods for recognizing large vocabularies of gestures have significant limitations that impact both system design and the user experience.  Many methods assume the existence of a near-perfect hand detector/tracker; that is a limiting assumption, which prevents deployment of these methods in complex real-world settings where such accuracy is unachievable.   In the absence of perfect hand detectors, system design may involve a large investment in manual annotation of training videos (e.g., specifying hand locations), so as to provide sufficiently clean information to training modules.   The user experience is affected by the limited accuracy and robustness of existing applications.   In this research the PI will address these issues by explicitly designing recognition and indexing methods that require neither perfect hand detectors nor extensive manual annotations, thus making it substantially easier to deploy accurate and efficient gesture recognition systems in real-world settings.  The PI will achieve these objectives through theoretical advances in the current state of the art in computer vision, pattern recognition, and database indexing.  The unifying theme in the project is the integration of low-level tracking modules that produce imperfect output, with recognition and indexing methods that are designed to take as input this imperfect output from the tracking modules.  Novel articulated tracking methods will be developed that utilize probabilistic graph models to provide fully automatic long-term tracking, while improving upon the excessive time complexity that probabilistic graph models currently incur.   New methods will be designed for extracting and exploiting information from hand appearance.  As these novel modeling and recognition methods will violate standard assumptions made by existing indexing methods, new indexing methods will be formulated which will improve the efficiency of search in large databases of dynamic gestures and static hand shapes within the proposed framework.<br/><br/>Broader Impacts:  Project outcomes will significantly improve the ability of sign language users around the world to search databases of sign language videos and to perform tasks such as looking up the meaning of an unknown sign or retrieving occurrences of a sign of interest in videos of continuous signing.  These search tools will have an impact in educational settings, facilitating both learning a sign language and accessing arbitrary information available in a sign language.  To these ends, the PI will make his software freely available to the public online.  He will also work with experts in American Sign Language to implement key applications using his tools, which will be made available to Deaf students.  The PI will furthermore develop a publicly available package of gesture recognition source code, applications, and datasets that will help student researchers at all levels engage in gesture recognition research.   As an additional outreach activity intended to attract young people to careers in science, the PI will co-organize summer camps that educate junior high and high school students in computer science."
"1115417","III: Small: RUI: Improving Data Quality and Data Mining Using Noisy Micro-Outsourcing","IIS","Info Integration & Informatics, EPSCoR Co-Funding","08/01/2011","07/20/2018","Victor Sheng","AR","University of Central Arkansas","Continuing Grant","Sylvia Spengler","07/31/2019","$355,628.00","","victor.sheng@ttu.edu","201 Donaghey Avenue","Conway","AR","720350001","5014505061","CSE","7364, 9150","7364, 7923, 9150, 9229, 9232, 9251","$0.00","Machine learning currently offers one of the most cost-effective approaches to building predictive models (e.g., classifiers for categorizing the millions of messages, news articles, and blogs that are generated every day). However, the effective use of machine learning methods in such settings is limited by the availability of a training corpus (i.e., a representative set of instances that have been labeled with the correponding categories).  In domains where labeled data are scarce or expensive to acquire, there is an urgent need for cost-effective approaches to selectively acquiring labels for data samples used to train predictive models using machine learning. <br/><br/>This project explores novel techniques that take advantage of the low cost of micro-outsourcing using systems such as Amazon's mechanical Turk, to engage a large number of workers from around the world for acquiring the labels of instances to be used to construct the training corpus. There is currently little understanding of how to utilize the multiple noisy labels obtained using micro-outsourcing. There is a need for advanced techniques for taking advantage of the low cost of micro-outsourcing in order to improve data quality and the quality of models built from the available data. It explores novel approaches for utilizing multiple labels given to an instance by different labelers. It also extends active learning techniques for active selection of samples to be labeled to take into account the multi-sets of labels that have been already obtained from a pool of labelers. <br/><br/>Advances in techniques for active selection of data instances to be labeled in a micro-outsourcing setting can significantly improve the quality of data used to build predictive models in a broad range of applications, including gene annotation, image annotation, text classification, sentiment analysis, and recommender systems, where unlabeled data are plentiful yet labeled data are sparse. The project will provide research opportunities for students at University of Central Arkansas, a primarily undergraduate institution and help expand the STEM pipeline. Additional information about the project can be found at: http://sun0.cs.uca.edu/~ssheng/."
"1134863","Planning Grant: Joining I/UCRC Safety, Security, and Rescue Research Center","IIP","INDUSTRY/UNIV COOP RES CENTERS","08/01/2011","08/20/2012","Jing Xiao","NC","University of North Carolina at Charlotte","Standard Grant","Lawrence A. Hornak","08/31/2013","$13,000.00","Srinivas Akella, Jianping Fan, Sonya Hardin","jxiao2@wpi.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","ENG","5761","1049, 5761, 8039, 8042","$0.00","The University of North Carolina at Charoltte is planning to join the Industry/University Cooperative Research Center (I/UCRC) entitled ""Center for Safety, Security, and Rescue Research Center (SSR-RC)"" which is a multi- university center comprised of the University of Minnesota (lead institution), the University of Denver and the University of Pennsylvania. The Center focuses on conducting integrative, multi-disciplinary research in autonomous systems to improve homeland security and emergency response. <br/><br/>The proposed site will enable industry-oriented and application-grounded research projects, and will pose unusual opportunities for leveraging the research strengths and diverse expertise of the faculty in robotics, computer vision, multimedia data processing, networking, interactive visualization, cyber security, and nursing care in the College of Computing and Informatics (CCI), College of Engineering (COE), and College of Health and Human Services (CHHS) at UNC Charlotte. The proposed new site will, therefore, align its mission with the existing centers mission on SSR and also extend the scope of SSR to healthcare and manufacturing, focusing on injury prevention, patient care, and assistive care for the elderly. <br/><br/>The proposed site will develop SSR technologies useful for key industries, and facilitate fast technology transfer. The partnerships forged via the proposed site with industries will directly impact economic development and job creation. The PIs state they would create opportunities for student training through the IUCRC site's industrial research projects; and broaden participation via existing STAR Alliance, REU Site programs and outreach to high schools and the general community at large."
"1065618","RI: Medium: Approximation Algorithms for Probabilistic Graphical Models with Constraints","IIS","ROBUST INTELLIGENCE","03/15/2011","01/03/2012","Rina Dechter","CA","University of California-Irvine","Continuing grant","Hector Munoz-Avila","02/29/2016","$1,089,282.00","Alexander Ihler","dechter@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","7924","$0.00","The goal of this project is to create the next generation of approximate inference techniques and algorithms for probabilistic graphical models. Probabilistic graphical models are employed throughout science and engineering to solve difficult problems, including automated reasoning and decision making, computational biology and genetics, computer vision, data mining, and social network analysis. However, these real-world problems are now of such considerable size that most existing techniques are uneven in their performance in that they typically work well on some problems and not others, and often require sets of choices and customizations that must be made with little guidance or automation.<br/><br/>This project brings together separate but complementary streams of research to develop new algorithms to manage models containing mixtures of probabilistic and deterministic relations and mixtures of graph-based and context-sensitive relationships. This project aims to advance the state of the art of probabilistic reasoning in the presence of deterministic constraints by developing new approximate inference techniques for graphical models, for instance, by exploiting the rich structure of graphical models that is largely neglected by most sampling techniques. This project aims to create improved frameworks for probabilistic graphical models by improving both sampling and message-passing algorithms for approximate inference and developing hybrid approaches that exploit the advantages of each. The frameworks will be used to provide automated guidance for selecting parameters to optimize the inherent tradeoffs between complexity and accuracy as well as provide meaningful bounds on results and accuracy. This project will use the fruits of its research to improve education, both at the undergraduate and graduate level, for instance by developing a new undergraduate course in graphical models, and by posting course materials online. In addition, the project will post open source code on the web."
"1117980","CIF: Small: Nonlinear Matrix and Tensor Completion with Applications in Systems Biology","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2011","07/06/2011","Olgica Milenkovic","IL","University of Illinois at Urbana-Champaign","Standard Grant","John Cozzens","06/30/2016","$478,005.00","Pierre Moulin, Ely Kerman","milenkov@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7936, 9218, HPCC","$0.00","Many experiments in molecular biology designed for discovering cellular functions and organization, or estimating the efficiency of drug treatments, are prohibitively expensive to be systematically conducted on large scale systems. It is therefore of paramount importance to develop analytical methods for predicting which experiments are likely to provide the most informative outcomes of biological tests, based only on a small sample of incomplete and noisy measurements. Frequently, the incomplete data are random or pseudo-random samples of a matrix, or more generally, a multidimensional array (tensor). This suggests the use of a new algorithmic and analytic paradigm, termed low-rank matrix and tensor completion, for solving the inference problems at hand. <br/><br/>This research involves the development of novel tensor and nonlinear completion methods for inference of protein-protein interaction networks and design of synthetic lethality experiments. The methods used represent a combination of information-theoretic and algorithmic approaches centered around constrained optimization on Grassman manifolds. This research program has the potential to benefit many branches of molecular biology, neuroscience, computer vision, control, economics and signal processing in terms of answering fundamental inference questions for sparse systems and reducing the cost and time associated with system testing and experimental design. It will also provide unprecedented opportunities to graduate students in the electrical engineering and mathematics departments at UIUC to explore state-of-the art technologies and research problems at the intersection of molecular biology, bio-informatics, and signal processing."
"1219638","RI:Small:Collaborative Proposal: Computational Framework of Robust Intelligent System for Mental State Identification and Human Performance Prediction with Biofeedback","IIS","ROBUST INTELLIGENCE","06/30/2011","09/11/2013","Wanpracha Chaovalitwongse","WA","University of Washington","Continuing grant","Kenneth C. Whang","07/31/2013","$174,231.00","","artchao@uark.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7923, 9215, HPCC","$0.00","This project will integrate new cognitive models of behavioral data based on queueing theory with new machine learning techniques for analyzing neurophysiological data, specifically electroencephalogram (EEG), in order to provide a deeper and more complete understanding of mental states as well as more accurate prediction of human performance. In cognitive modeling, a new brain network architecture for human performance and mental workload, called Queuing Network-Model Human Processor (QN-MHP), will be further improved. QN-MHP with a new human-like small-scale knowledge system will be used to model the increase of myelination in the brain in cognitive development and predict human performance, in terms of subjective risk perception and confidence. In machine learning, new spatio-temporal (pattern-based) classification techniques will be developed for multidimensional time series data and used to identify human mental states (e.g., fully awake, fatigue, distracted, anger) from EEG data. The integrated framework will result in a robust intelligent system that uses machine learning to identify mental states and the queueing model of that mental state to predict the human performance as well as provide a human operator with feedback. A mind-driven intelligent transportation system will be developed as a case study in this project, where a certain type of feedback will be designed to help drivers avoid accidents and to improve system safety. This system can also be applied to other human-machine systems that require full or partial attention of human operators (e.g., in aviation, military, or manufacturing settings)."
"1137211","EFRI-M3C: Development of New Algorithmic Models and Tools to Enhance Neural Adaptation in Brain Computer Interface Systems.","EFMA","EFRI Research Projects","09/01/2011","05/19/2016","Daniel Moran","MO","Washington University","Standard Grant","Radhakisan Baheti","08/31/2016","$1,992,456.00","Kilian Weinberger, Eric Leuthardt","dmoran@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","ENG","7633","7633","$0.00","Human interaction with machines has always relied on some form of muscle movement to translate the brain's desired action to the machine (e.g. turning a knob). The objective of our project is to eliminate the need for muscle transformations in the man-machine interface. Using a new brain-computer  interface (BCI) technology (electrocorticography or ECoG) pioneered by the research team, we will  develop novel decoding algorithms to control the force/torque inputs to an external device directly.  Likewise, by designing machine learning algorithms to identify and incorporate neural plasticity in the  decoding schemes will allow the BCI to evolve over time. Finally, by combining brain signals from multiple areas to identify various brain states, we can identify and change the effectors to be controlled. <br/><br/> Intellectual Merit: All previous BCI studies decoded only kinematic signals to control computer cursors as well as robotic limbs. While kinematic control is a natural extension for disabled individuals trying to regain function lost by paralysis or amputation, a direct interface between the brain and the machine allows for much more elegant interaction. Thus, rather than controlling a robotic arm through the  use of imagined self limb movements (a proxy of intention), one rather controls the device as if it was a  part of their own body. For instance, mapping brain activity to kinetic parameters such as a robot?s torque motor allows the individual to directly control the forceful interactions within the system.<br/><br/> Broader Impact: One advantage of ECoG is that while it is an invasive recording technology,  the electrodes can be placed epidurally which significantly reduces the risk profile for implantation. In the long term, this should allow ECoG-based BCIs to be accepted as a viable implant in able-bodied humans. Developing a safe and effective BCI modality for the general public will fundamentally change how humans interact with machines. No longer will humans require muscle activity as an intermediary to interact with machines. A whole new field of man-machine interfacing will be initiated where the brain builds complex internal models of the machine's dynamics (instead of musculoskeletal dynamics) for accurate and direct control of the machine's effectors."
"1116012","RI: Small: A Region-Based Approach to Reconstructing Urban Scenes","IIS","ROBUST INTELLIGENCE","08/01/2011","03/19/2013","Minh Do","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jie Yang","07/31/2015","$450,000.00","","minhdo@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7495","7923","$0.00","Recently, reconstructing full and detailed 3D models for large-scale urban environments has become the crucial technology for many applications and offers a natural platform for different services. Although conventional structure from motion (SFM) techniques have been engineered to their maturity,  they normally do not utilize rich global structures of urban scenes, including regularity, symmetry, and self-symmetry; and the very repetitive shapes and textures ubiquitous in urban scenes make detecting and matching features extremely challenging for the conventional techniques. <br/><br/>This project develops a novel approach for inferring highly accurate 3D geometry from an individual or multiple 2D images of an urban scene. It takes full advantage of the rich global symmetry and regularity in urban scenes by leveraging powerful computational tools from modern high-dimensional convex optimization. The developed method can accurately recover the regular 3D geometry and 2D texture of the scene directly from the raw image pixels/regions without relying on extracting any intermediate local features. The research includes developing a set of useful tools and a full system that can significantly improve the efficiency and scalability of 3D modeling of large urban scenes and give significantly more compact representation of the 3D geometry and 2D appearance, enabling online real-time rendering and visualization.<br/><br/>Research results from this project can be easily integrated into and significantly improve the current computer vision course on 3D reconstruction. The associated technologies can be useful for a very wide range of commercial applications such as online or mobile visual search, visual guidance, navigation, or surveillance, virtual tourism, and augmented reality etc."
"1160626","Mori Dream Spaces and Rational Curves","DMS","ALGEBRA,NUMBER THEORY,AND COM","09/01/2011","09/26/2011","Ana-Maria Castravet","OH","Ohio State University","Standard Grant","Tie Luo","07/31/2013","$62,679.00","","a.castravet@neu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1264","","$0.00","The project aims at understanding different aspects of the geometry of algebraic varieties and their moduli. There are two main topics:<br/>(1) Effective and ample cones of moduli spaces of stable curves. This sequence of projects is focused on the Grothendieck-Knudsen moduli space of stable rational curves. The goal is to investigate the Mori Dream Space structure of the moduli space; in particular, give modular interpretations for its birational contractions and give a presentation for its total coordinate ring. A new point of view is the interpretation of the moduli space as a Brill-Noether locus of a reducible curve associated to new combinatorial structures called hypertrees. (2) A study of higher Fano varieties using minimal dominating families of rational curves. The main focus is on the classification of 2-Fano varieties and generalizations of Tsen's theorem.<br/><br/>The broader context of the project is the area of algebraic geometry, one of the oldest and currently one of the most active branches of mathematics, with widespread applications throughout mathematics and reaching into physics and engineering. Algebraic geometry is the study of algebraic varieties, which are geometric objects defined by the zeros of systems of polynomial equations. The variation of algebraic varieties is captured by the so-called moduli spaces, which are themselves varieties with a very rich structure. The project aims at revealing the intriguing structure of various moduli spaces of curves (which are fundamental in many areas of mathematics and in theoretical physics). The project impacts arithmetic and computational algebraic geometry, areas which have increasing applications in coding theory, robotics, computer vision, phylogenetics, statistics, etc."
"1054911","CAREER: Learning- and Incentives-Based Techniques for Aggregating Community-Generated Data","IIS","Info Integration & Informatics, Robust Intelligence, Algorithmic Foundations","06/01/2011","04/08/2015","Jennifer Vaughan","CA","University of California-Los Angeles","Continuing grant","Maria Zemankova","03/31/2015","$238,627.00","","jenn@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364, 7495, 7796","1045, 1187, 7364, 7926","$0.00","The Internet has led to the availability of novel sources of data on the preferences, behaviors, and beliefs of massive communities of users. <br/>Both researchers and engineers are eager to aggregate and interpret this data. However, websites sometimes fail to incentivize high-quality contributions, leading to variable quality data. Furthermore, assumptions made by traditional theories of learning break down in these settings.<br/><br/>This project seeks to create foundational machine learning models and algorithms to address and explain the issues that arise when aggregating local beliefs across large communities, and to advance the state-of-the-art understanding of how to motivate high quality contributions. The research can be split into three directions:<br/><br/>1. Developing mathematical foundations and algorithms for learning from community-labeled data.  This direction involves developing learning models for data from disparate (potentially self-interested or<br/>malicious) sources and using insight from these models to design efficient learning algorithms.<br/><br/>2. Understanding and designing better incentives for crowdsourcing. This direction involves modeling crowdsourcing contributions to determine which features to include in systems to encourage the highest quality contributions.<br/><br/>3. Introducing novel economically-motivated mechanisms for opinion aggregation. This involves formalizing the properties a prediction market should satisfy and making use of ideas from machine learning and optimization to derive tractable market mechanisms satisfying these properties.<br/><br/>This research will have clear impact on industry, especially for web-based crowdsourcing. The PI will pursue her long-term goal of attracting and retaining women in computer science via her involvement in workshops and mentoring programs.  Results will be disseminated at http://www.cs.ucla.edu/~jenn/projects/CAREER.html.<br/>"
"1117631","RI: Small: Learning and Inference with Perfect Graphs","IIS","Robust Intelligence","08/01/2011","06/15/2013","Tony Jebara","NY","Columbia University","Continuing Grant","Todd Leen","07/31/2014","$456,417.00","Maria Chudnovsky","jebara@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7495","7495, 7923, 9251","$0.00","This proposal investigates new computational and combinatorial tools for learning from data and performing inference about data. As such, it serves as a bridge between the machine learning community and the combinatorics community. The latter field develops efficient algorithms or shows when an efficient solution to a particular problem exists. The project will support graduate students in the intersection of these two fields to combine recent theoretical results with practical data problems. The team will produce a website of tutorials, data sets, downloadable code, interactive visual examples and course materials to allow better integration across the two fields. The combinatorics community has identified a family of inputs called perfect graphs where otherwise hard problems are efficiently solvable. This proposal will investigate how to bridge this powerful theoretical result to the area of machine learning and formally characterize which learning and inference problems are efficiently solvable.<br/><br/>More specifically, the team will investigate data learning problems (such as clustering a data set into subgroups that are self-similar or finding anomalies in a database) and statistical inference problems (computing the most likely or most typical outcome from observed measurements). These problems will be compiled or represented as graphs and networks. Then, these graphs and networks can be more formally diagnosed using perfect graph theory to determine if the instance of the problem is easy (or hard) and to provide efficient solutions via exact algorithms on perfect graphs. These solvers will be implemented using message passing or convex programming."
"1201666","Image Processing Using PDE on Image Features and Image Databases","DMS","COMPUTATIONAL MATHEMATICS","08/01/2011","10/24/2011","Arthur Szlam","NY","CUNY City College","Standard Grant","Leland Jameson","06/30/2013","$58,681.00","","aszlam@courant.nyu.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","MPS","1271","0000, 9263, OTHR","$0.00","In recent years the heat equation on a weighted graph  has been used to attack problems in image processing, including denoising, segmentation, and inpainting.<br/>In many cases the data used to build the graph is the set of patches or feature responses from a single image; and even then, patches or filter responses are usually only compared with their spatial<br/>neighborhoods.   On the other hand, it has become practical to<br/>manipulate large collections of images, and using the statistics of large collections of images has become an important image processing<br/>tool.   The PI will investigate how to use larger databases of<br/>images and image features in a PDE framework. For some of the proposed work, it will be necessary to invent new theory to lift notions of the smoothness of a surface embedded in Euclidean space to maps from the discrete grid into a weighted graph; and perhaps further to maps from a more general weighted graph into a weighted graph.  Other proposed work will try to make use of (and improve) recent methods for sparse feature extraction, and solidify the theory underlying the NL-means method of Buades, Coll, and Morel.<br/><br/><br/>A large class of popular image processing techniques making use of the theory of partial differential equations operate locally; that is, the behavior of each step of these algorithms at a given pixel in an image is determined solely by the neighboring pixel values.<br/>In recent years, it has become possible to manipulate large databases of images; and such databases of images have become available from many sources, including the world wide web. The PI will work towards extending the local techniques to make use of these large databases. The long term goal is image processing techniques which understand and utilize the content and context of images; such techniques would have many important applications, for example in medical imaging, hperspectral imaging, and computer vision in general.<br/>"
"1138599","EAGER: Collecting Training Videos for Location Estimation with Mechanical Turk","IIS","Info Integration & Informatics","09/01/2011","07/14/2011","Gerald Friedland","CA","International Computer Science Institute","Standard Grant","Sylvia Spengler","08/31/2013","$50,000.00","","fractor@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7364","7364, 7916","$0.00","Location-based services are rapidly gaining traction in the online world as they allow highly personalized services and easier retrieval and organization of multimedia. However, such services require accurate geolocation information (geo-tags) to be associated with the multimedia data e.g., videos. Because only a small fraction of available video data is geo-tagged. Hence, there is a growing interest in systems that estimate the geolocation of a given video automatically that does not include geo-location metadata.  While machine learning offers a potential approach to training automatic location estimators, it requires a standardized training corpus of geo-tagged videos. Automatic collection of videos introduces a bias toward videos that are easily processible by machines  and towards geographical locations that are over-represented in current corpora. Hence there is a need for carefully curated standard data sets. <br/><br/>This EArly-concept Grants for Exploratory Research   (EAGER) project explores a novel, somewhat high risk, approach to collecting such an annotated  training corpus of geo-tagged videos using Mechanical Turk  (http://www.mturk.com), a ""marketplace for work"" for engaging workers with the desired expertise from around the world to work on a specific task, in this case, participating in a game that involves annotating videos with geolocation metadata e.g., GPS coordinates. The user interface for the game will allow participants to estimate the location of videos by clicking on a map. The knowledge gained from this EAGER would set the stage for more comprehensive geotagged multimedia data collection efforts. The resulting data sets and benchmarks will be made available to the research community to enable detailed and systematic comparative analysis of alternative methods (e.g., machine learning algorithms for predicting geolocation information from videos). <br/><br/>The availability of standardized geo-tagged multimedia data sets will help drive advances in machine learning techniques for geo-location prediction. The resulting advances in geo-tagging multimedia data would enable intelligent location based services and a variety of domains including law enforcement, personalized and location-aware media retrieval, for a variety of applications including journalistic and criminal investigations."
"1062909","REU Site: MedIX: Medical Informatics Experiences in Undergraduate Research","IIS","RSCH EXPER FOR UNDERGRAD SITES","02/01/2011","01/27/2011","Daniela Raicu","IL","DePaul University","Standard Grant","William Bainbridge","01/31/2015","$325,538.00","Jacob Furst","draicu@cti.depaul.edu","1 East Jackson Boulevard","Chicago","IL","606042287","3123627595","CSE","1139","9102, 9250","$0.00","The Medical Informatics (MedIX) program is hosted by the Medical Informatics Laboratory at DePaul University and the Imaging Research Institute at the University of Chicago. The program runs for three summers, with eight students doing research for ten weeks each summer under the supervision of four dedicated and experienced mentors. The MedIX program aims to 1) encourage talented undergraduates to pursue graduate education, especially undergraduates underrepresented in information technology, and b) expose students to interdisciplinary research, especially at the border of information technology and medicine.  All of the students' projects are inspired by state-of-the-art research ideas in imaging informatics and by novel data mining and computer vision algorithms that these research ideas require in order to be realized in practice.  Ultimately, each project has the long-term potential to increase the quality of healthcare available to people everywhere.  <br/><br/>Faculty will conduct tutorials, lectures, and demonstrations on imaging informatics, require frequent presentations of student work, encourage students to publish, and conduct biweekly meetings of students, mentors, and teams.  Students work as part of faculty-undergraduate teams on new problems; they participate in defining the direction of their research, and they are strongly encouraged to publish meaningful results. The intended impact is to continue the increase in enrollment of undergraduates into graduate school, especially with respect to underrepresented students and interdisciplinary research. Further, the emphasis on recruiting underrepresented populations can ultimately help improve the diversity of university faculty in information technology and medical informatics."
"1116631","RI: Small: Collaborative Research: Visual Attributes for Identification and Search in Images","IIS","ROBUST INTELLIGENCE","07/01/2011","06/30/2011","David Jacobs","MD","University of Maryland College Park","Standard Grant","Jie Yang","06/30/2015","$250,000.00","","djacobs@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7923","$0.00","The automatic identification in images of people, places, objects, and especially object categories is a central and ongoing challenge within computer vision. This project addresses this problem using low-level image features to learn intermediate representations, ones in which objects in images are labeled with an extensive list of highly descriptive visual attributes. This work demonstrates this approach in three domains: faces, plant species, and architecture. In each domain, it develops techniques for deriving visual attribute vocabularies, training attribute detectors, and building compositional models to automatically label attributes in images.<br/><br/>The project is making four fundamental contributions to the use of visual attributes. 1) It is developing new methods by which automatic systems and humans can interact to select domain-appropriate attribute vocabularies and label large image collections. 2) It is developing compositional models that capture dependencies between attributes. This provides more accurate attribute detection and enables inference of global properties of objects. 3) Using compositional models, the project is developing new, localizable attributes that capture the geometric relations between object parts and landmarks. 4) The project is designing algorithms that combine attributes to identify objects, search through image vast collections, and automatically annotate image databases.<br/><br/>Not only is this research generating large datasets of labeled images that should help catalyze new research, it is also demonstrating the feasibility of new systems for analyzing images in specialized domains such as faces, plants, and architecture. For example, the project develops new software applications for analyzing and searching images of faces as well as free mobile apps for plant species identification."
"1054127","CAREER: Toward Discovering the 3D Geometrical and Semantic Structure of Objects and Scenes","IIS","Robust Intelligence","01/15/2011","06/10/2013","Silvio Savarese","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Jie Yang","03/31/2014","$300,402.00","","ssilvio@stanford.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495","1045","$0.00","This project develops a novel framework for jointly understanding the 3D spatial and semantic structure of complex scenes from images. The state-of-the-art computer vision methods deal with these two tasks separately. Methods for object recognition typically describe the scene as a list of class labels, but are unable to account for the 3D spatial structure. Methods for scene 3D modeling produce accurate metric reconstructions but are unable to infer the semantic content of its components. This project seeks to fill this gap and creates the foundations for a new framework for coherently describing objects, object components and their 3D spatial arrangement in the scene's physical space.  The research of this project makes two main contributions. First, novel models for representing the intrinsic multi-view nature of object categories and for measuring critical object geometrical attributes are explored. Second, a new coherent probabilistic formulation that is capable to use these measurements for simultaneously estimating the most likely 3D configuration of scene elements and the critical semantic phenomena of the scene are investigated. This research has potential to play a transformative role in many strategic areas such as autonomous navigation, robotics, and 3D automatic modeling of urban environments. Moreover, it is crucial in designing technology for assisting people with reduced functional capabilities.  The project integrates research and education by involving undergraduates or high school students in projects whose primary application goal is to develop technology for people with disabilities."
"1116140","RI: CGV: Small: Multiview Reconstruction and Calibration Using Differential Geometry of Curve Fragments and Surface Patches","IIS","GRAPHICS & VISUALIZATION, Robust Intelligence","08/01/2011","07/29/2011","Benjamin Kimia","RI","Brown University","Standard Grant","Jie Yang","07/31/2014","$450,000.00","","Benjamin_Kimia@Brown.Edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7453, 7495","7453, 7495, 7923, 9150","$0.00","This project investigates the use of image curve fragments to augment the use of isolated features in multi-view calibration and reconstruction tasks.  The research team develops infrastructure based on differential geometry that utilizes curves and surfaces beyond lines and planes to correlate structure in multiple images of a scene: a pair of corresponding curve fragments in two views initiates a candidate 3D curve fragment whose presence can be validated in additional views. This results in a 3D curve sketch. A significant advantage of the 3D curve sketch over an unorganized cloud of point reconstruction is that it can correlate with image curve structure in novel views without referring back to the original views. This allows both incremental reconstruction (incorporating one additional view at a time) and simultaneous reconstruction from numerous views. Calibration methods are also being explored by using curve fragments under a RANSAC regime by using differential geometry in three views or more, and differential geometry together with appearance in two views.  In a similar vein the correlation of surface patches by matching observable intensity local form in images is being investigated as a method for reconstruction of local surface patches. The project provides a core technology for many applications in the computer vision field. The developed technology can be also applied to other fields such as archaeology and art."
"1117170","RI: Small: Collaborative Research: Visual Attributes for Identification and Search in Images","IIS","ROBUST INTELLIGENCE","07/01/2011","06/30/2011","Peter Belhumeur","NY","Columbia University","Standard Grant","Jie Yang","06/30/2014","$249,349.00","","belhumeur@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7495","7495, 7923","$0.00","The automatic identification in images of people, places, objects, and especially object categories is a central and ongoing challenge within computer vision. This project addresses this problem using low-level image features to learn intermediate representations, ones in which objects in images are labeled with an extensive list of highly descriptive visual attributes. This work demonstrates this approach in three domains: faces, plant species, and architecture. In each domain, it develops techniques for deriving visual attribute vocabularies, training attribute detectors, and building compositional models to automatically label attributes in images.<br/><br/>The project is making four fundamental contributions to the use of visual attributes. 1) It is developing new methods by which automatic systems and humans can interact to select domain-appropriate attribute vocabularies and label large image collections. 2) It is developing compositional models that capture dependencies between attributes. This provides more accurate attribute detection and enables inference of global properties of objects. 3) Using compositional models, the project is developing new, localizable attributes that capture the geometric relations between object parts and landmarks. 4) The project is designing algorithms that combine attributes to identify objects, search through image vast collections, and automatically annotate image databases.<br/><br/>Not only is this research generating large datasets of labeled images that should help catalyze new research, it is also demonstrating the feasibility of new systems for analyzing images in specialized domains such as faces, plants, and architecture. For example, the project develops new software applications for analyzing and searching images of faces as well as free mobile apps for plant species identification."
"1116360","HCC: Small: Assistive Social Situational Awareness Aids for Individuals with Disabilities","IIS","HCC-Human-Centered Computing","09/01/2011","05/01/2014","Sethuraman Panchanathan","AZ","Arizona State University","Continuing Grant","Ephraim Glinert","08/31/2015","$515,284.00","Terri Hedgpeth, Artemio Ramirez, Vineeth Nallure Balasubramanian, Troy McDaniel","panch@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7367","7367, 7923, 9251","$0.00","The PI's goal in this project is to enable a quantum leap towards next-generation social assistive aids that enrich the lived social experiences of individuals with visual impairments.  Social interaction is a central component of the human experience.  The ability to communicate effectively with fellow individuals is a fundamental necessity for professional success as well as personal fulfillment.  But nonverbal cues (including prosody, environment attributes, the appearance of communicators and their physical movements) account for a substantial and important part of the information conveyed during social interactions.  As a consequence, the more than 1.3 million individuals in the United States (and 37 million worldwide) who are legally blind have only a limited experience of social interaction.  This ""social disability"" often isolates them from their social environments.  Existing assistive technologies are focused on problems such as navigation, reading text, and access to everyday appliances as well as to computers and the Internet, whereas little or no work has been devoted to real-time accessibility to social and behavioral cues.  Providing real-time access to nonverbal communication cues to visually impaired users poses fundamental challenges in several related fields including affective computing, human communication engineering, behavioral modeling, machine learning, human-machine interaction, multimodal interfaces, usability engineering, multimedia computing, and assistive technology design and development<br/><br/>As a first step towards practical and viable social assistive solutions, the PI will focus in this project on the design and development of a social situational awareness assistive prototype for dyadic (one-on-one) interactions, with an emphasis on head/face-based nonverbal cues.  The research will be accomplished through the following specific objectives: (1) Design and development of a dyadic interpersonal mediation interface; (2) Extraction and understanding of nonverbal communication cues; (3) Visuo-haptic sensory substitution for delivering high-bandwidth socio-behavioral data; and (4) Evaluation of the social assistive prototype system in dyadic interaction scenarios representing real-world conditions.  The project draws upon intellectual synergies among the team members, who are experts in human-centered multimedia computing, human-computer interfaces and machine intelligence (Panchanathan, Computer Science); assistive technology design and usability engineering (Hedgpeth, Disability Resources Center); and human communication modeling and socio-behavioral analysis (Ramirez, Human Communication).  The work will build on the team's past successes in developing assistive technologies that have been designed, prototyped, deployed and tested for individuals who are visually impaired.  Project outcomes will be evaluated through the Arizona State University Disability Resource Center and the Arizona Center for the Blind and Visually Impaired.<br/><br/>Broader Impacts:  This project will pioneer the development of next-generation social assistive aids for individuals with visual impairments and thus will have a significant impact on their lives.   The research to these ends will result in the advancement of computational thinking within and at the confluence of the component disciplines, namely socio-behavioral computing (through the introduction of novel methodologies for computational analysis and evaluation of human communication dynamics in general and social behavior in dyadic interactions, specifically), human-computer interfaces (through the design of novel interfaces that deliver high-bandwidth social data), machine intelligence (through the study of algorithms that elicit various levels of interaction semantics) and assistive technology/usability (through the development and evaluation of social assistive prototypes).  The concepts and technologies developed will also provide pathways to technologies for individuals with other disabilities, such as autism, dementia, and (in the most general sense) a very large portion of society.  The methodologies developed will provide a wealth of data and information that will be made publicly accessible to promote and catalyze further research in the component disciplines."
"1124240","DIP: Exploiting Longitudinal Electroencephalogram (EEG) Input in a Reading Tutor","IIS","Cyberlearn & Future Learn Tech","09/01/2011","11/29/2012","David Mostow","PA","Carnegie-Mellon University","Standard Grant","christopher hoadley","08/31/2015","$1,382,000.00","Kai-Min Chang","mostow@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8020","8045, 8842, 9251","$0.00","Automated (and human) tutors are limited in their ability to infer what is going on in students' heads based on their observable behavior. The proposed work addresses this limitation by investigating how EEG input from a commercially-available device can be used as evidence about students' mental states. In particular, the project focuses on adding EEG-enhanced feedback to Project LISTEN's Reading Tutor, an intelligent tutoring system that helps children learn to read. The project seeks to answer two questions: (1) How can we use EEG to detect mental states that predict, indicate, or reflect student learning? (2) How can we use such detection to improve student learning? Analysis to answer these questions and to enhance the capabilities of the Reading Tutor draws on existing tools to explore annotate, and mine EEG data logged by the Reading Tutor. The research aims to tell us more about how to use EEG to identify mental states that predict learning and to use machine learning to make an intelligent tutoring system better, and it may also add to what is known about sources of reading difficulties. Expected technological contributions of this work include advances in relating EEG data to children's behavior, cognition, engagement, and learning and advances in elucidating how intelligent tutors can robustly exploit noisy EEG input to better assist learning. <br/><br/>The technological innovation in this project is particularly important for those children who need extra help with sounding out, word recognition, and/or making simple inferences needed for understanding."
"1116208","CIF: Small: Fast Stagewise Learning of Sparse Hierarchical Data Representations","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2011","06/30/2011","Peter Ramadge","NJ","Princeton University","Standard Grant","John Cozzens","06/30/2015","$372,000.00","","ramadge@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7797","7923, 7936, 9218, HPCC","$0.00","Increasingly, real-world data has many dimensions or features but rather than filling out all dimensions equally, the data distributed on or near a surface of much lower intrinsic dimension. Examples include fMRI data and natural image and video data sets.  This research will push the boundary of what is currently possible in the analysis of such large data sets by incorporating hierarchical structure into what is known as a ""sparse dictionary representation"" of the data.  The results will include both basic intellectual contributions to machine learning methods and computational advancements that will aid the investigation of complex real world data. <br/> <br/>A fundamental problem in learning the structure of complex data is how to effectively extract a set of features that reflects the underlying structure of the data. In many applications, including face recognition and object recognition, sparse dictionary representations have proved effective for this purpose. However, since solving large-scale sparse representation problems is very expensive, the method is generally limited to problems of moderate scale. This research reformulates the method into an incremental, multi-stage, hierarchical dictionary learning process. This approach incrementally extracts information from the data and uses this to refine the data representation in an organized hierarchical fashion. This enables the building of large-scale dictionaries in a computationally efficient way. The method hence extends the power of sparse dictionary representation methods to a wider variety of real world applications. It also has the flexibility to incorporate an existing state-of-the-art sparse coding algorithm as the basic solver and hence can extend the functionality of existing sparse coding algorithms to multi-stage, hierarchical dictionary learning."
"1052736","Collaborative Research: Communication, Perturbation, and Early Development","BCS","DS -Developmental Sciences, CDI TYPE I","06/15/2011","06/14/2011","Daniel Messinger","FL","University of Miami","Standard Grant","Laura Namy","05/31/2015","$202,002.00","","dmessinger@miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","SBE","1698, 7750","1698, 7750, 7752","$0.00","Young infants typically form lasting, emotional attachments to their caregivers. The strength and type of these attachments are related to emotional well-being and cognitive development. This project will explore how face-to-face interactions between infants and adults contribute to this important aspect of child development.<br/><br/>During early interactions, infants and parents form expectations about one another. Will a smile be answered with a bigger smile, for example, or with no smile at all? If the parent is asked to stop interacting and just look at her infant, will the infant smile or vocalize in an attempt to repair the interaction?  Do these early patterns of interaction predict the infant's later security of attachment--their ability to be comforted after a brief separation from the parent? To answer these questions, seventy-five infants and their mothers will participate in a standard ""Face-To-Face/Still-Face""<br/>procedure at four months. Their security of attachment will then be assessed at<br/>12 months.<br/><br/>It is difficult to measure early interactive behavior-and human behavior more generally-objectively and efficiently. To address this challenge, the project's interdisciplinary team of psychological and computer scientists will implement automated, quantitative measurements of behavior in the Face-To-Face/Still-Face procedure. Automated facial image analysis and pattern recognition approaches will be used to produce objective, continuous measurements of infant and mother facial expression, head motion, gaze direction, and vocalizations. Precise measurement of this multimodal suite of infant and mother behaviors will be used to tackle a fundamental scientific problem: Modeling the structure of early interaction and its relation to later development.<br/><br/>This is a promising approach to understanding threats to typical development and learning associated with risk factors such as maternal depression and disorders such as autism. To maximize the project's impact, the team will make a database of audiovisual recordings, automated measurements, and pattern recognition and modeling software available to other scientists."
"1127163","SBIR Phase II:  An Interactive Music Analysis, Re-synthesis and Distribution Engine","IIP","SBIR Phase II","12/01/2011","04/20/2012","Charles Spencer","GA","ZOOZ Mobile","Standard Grant","Benaiah Schrag","11/30/2013","$499,960.00","","charles@zoozmobile.com","325 Trowbridge Walk","Atlanta","GA","303506876","7703788115","ENG","5373","5373, 8032","$0.00","This Small Business Innovation Research Phase II project aims to extend work in developing computational music systems that will automatically analyze and re-synthesize digital music for the purpose of transforming a linear and passive music listening practice into an interactive, expressive and creative music experiences. Building on the work in phase I, which focused on segmentation and user interaction, the company propose to extend the work in the following areas: Utilizing machine-learning techniques to extract instrumental content of musical segments; Developing automatic compositional techniques that would sequence annotated musical segments to create musically meaningful compositions; Developing visualization techniques for representing musical compositions; Develop a set of applications utilizing our technology, and implement a Cloud based service that would support seamless interaction with these applications.  The intellectual merit of the project lies in the fundamental contribution to human knowledge in the areas for music perception and analysis, machine learning, automatic composition, user interaction, and visualization. The project will advance current knowledge in areas such as music information retrieval, music perception, machine learning, automatic composition, signal processing, visualization and cloud computing. The proposed research would shed light on broader concepts such as human and artificial creativity and expression and the feasibility of utilizing artificial music intelligence as an enabler of novel forms of music creativity for children, novices and experts.<br/><br/>The project will lead to broad impact in the public sphere by creating engaging and rewarding musical experience for users at all skill levels. Zooz?s music intelligence engine will allow even those who believe they are not musically inclined to become engaged in expressive and creative musical experiences. As part of the project, we will continue to conduct workshops with educational and musical institutions where children and novices will interact and create music using the Zooz engine. High visibility public concerts will be conducted to bring the technology to the public eye. From a business perspective, the broad impact of the project is in providing a novel solution to the significant problems faced by the music industry today. The industry, which has suffered from a significant annual drop in music sales, is looking for new ways to monetize their content by engaging fans with music games, personalization tools and cloud-based musical interaction. Zooz Mobile will address these needs by providing an intelligent system that will allow fans to interact, personalize and share their favorite music in the Cloud in novel and expressive manners."
"1065106","AF: Medium: Theory and Practice of Optimal Meshing","CCF","Algorithmic Foundations","04/01/2011","03/31/2011","Gary Miller","PA","Carnegie-Mellon University","Standard Grant","jack snoeyink","03/31/2015","$772,857.00","","glmiller@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7924, 7929, 9218, HPCC","$0.00","Meshing has been a cornerstone for simulations using the finite<br/>element method.  But more recently it has had applications wherever<br/>one needs to define a function over a domain, such as, graphics,<br/>computer aided design, robotics, and even machine learning. Algorithms<br/>will be designed to efficiently work in any fixed dimension with<br/>guarantees on output size and quality.<br/><br/>At present, no theory exists to formulate and produce optimal meshes<br/>in the presence of small input angles even for the 2D case.  The<br/>research will find efficient algorithms 2D and higher dimension that<br/>generate optimal size Delaunay triangulations using only simplices<br/>with no angles approaching 180 degrees.<br/><br/>Machine learning applications need a meshing algorithm that runs in<br/>polynomial time for meshing n points in log n dimensions.<br/>Historically, meshing algorithms return a set of space-filling<br/>simplices.  Even good aspect ratio simplices have too small a volume<br/>and return a mesh that is of super polynomial size.  Thus, new<br/>algorithms will be developed that handle atomic objects that are have<br/>much larger volume than simplices.<br/><br/>The results from this project are eminently practical and have broad<br/>impact on the Sciences, Engineering, Manufacturing, and Machine<br/>Learning.  In particular, meshing is an enabling technology for<br/>designing efficient windmills and cars, and simulations of earth<br/>quakes and medial devices.  One goal is to incorporated techniques<br/>from this research into our first generation 3D code that we made<br/>available on the web. In addition, this material will be incorporated<br/>into classes taught at CMU, lectures, and papers presented at<br/>conferences."
"1118971","Algorithms for Threat Detection in Sensor Systems for Analyzing Chemical and Biological Systems Based on Compressive Sensing and L1 Related Optimization","DMS","","08/15/2011","07/28/2011","Stanley Osher","CA","University of California-Los Angeles","Standard Grant","Leland Jameson","09/30/2017","$1,198,663.00","Andrea Bertozzi","sjo@math.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","K591","6877","$0.00","The investigators intend to generate new and effective mathematical algorithms and methodologies in sensor systems for the detection of chemical and biological materials. Next, they intend to transfer this technology directly to those working towards reducing the threat to the homeland of biological and chemical attack. The new techniques they will use come primarily from information science, image science and physics, involving harmonic analysis, machine learning, optimization and partial differential equations. In particular they intend to provide useful algorithms for multi-component aerosol unmixing for active sensing using LiDAR and for mixtures of vapors in passive sensing. They will use ideas and algorithms recently developed, broadly speaking, from compressive sensing and L1 related optimization which were applied to hyperspectral imaging (recently used by Navy SEALS in the Bin Laden take down), unmixing, template matching, anomaly detection, clustering, change detection and endmember computation. They will improve relevant classical learning techniques, such as support vector machine, using their optimization techniques. They will also use ideas from machine learning with nonlocal means with prior information, in order to segment and identify objects in data collected from all sorts of sensors. Finally, they will factor in physics, such as plume dissipation, as part of the prior information needed to do spatial segmentation and identification.<br/><br/>The US government has been developing laser-based sensors for locating and classifying aerosols in the atmosphere at safe standoff ranges for more than a decade. There is a need to distinguish aerosols of biological origin from indifferent materials such as smoke and dust. Often, mixtures of aerosols are present and it is important to decide whether a threat exists.  This project is intended to resolve data containing such a mixture into their separate components. Some success has already been obtained here by the investigators. This is an example of what this work concerns. A chemical and/or biological contamination might occur on the ground or in the air. The problem is to determine the presence of and concentration of chemical and biological threats and to track the dynamics of the cloud.  The research done here is relevant to all the sensor modalities used in this type of threat detection. These include state-of-the-art LiDAR sensors, infrared radiometry and hyperpectral spensors. Plume tracking through the atmosphere is particularly important in a potential threat situation. The type of work proposed here is basic to our nation's security, given the threat posed by chemical and biological WMD's."
"1137466","Research Initiation Award:Secure measurement-based IP geolocation for Cloud Auditing","HRD","Hist Black Colleges and Univ","09/01/2011","08/13/2014","Sachin Shetty","TN","Tennessee State University","Standard Grant","Claudia Rankins","08/31/2015","$238,835.00","","sshetty@odu.edu","3500 John A. Merritt Blvd.","nashville","TN","372091561","6159637631","EHR","1594","9150, 9178","$0.00","Tennessee State University's Research Initiation Award entitled - Secure measurement-based IP Geolocation for Cloud Auditing - will enhance the cyber security research program at the university by investigating novel security problems in cloud auditing.  The educational goal involves the integration of cloud auditing research in select undergraduate courses. Cloud computing is one of the most enticing technologies due to its scalable, flexible, and cost-efficient access to computing resources. However, the increased concentration of business data and computing power scales security risks as well. One of the recent security concerns is attributed to the lack of sufficient transparency in the operations of the cloud provider, leading to difficulties in cloud auditing. <br/><br/>This project investigates a critical network mapping and measurement (NMM) technique, Secure IP Geolocation, to facilitate reliable cloud auditing. The overarching goal is to develop a methodology based on machine learning that will determine the geolocations of the cloud nodes containing user data with higher reliability, robustness and sensitivity than the current state-of-the art measurement-based IP geolocation techniques. Three separate but synergistic research goals are proposed: 1) develop a machine learning based approach to estimate the hop distances between arbitrary host pairs in complex network topologies that is accurate, scalable, timely, and does not require a significant measurement infrastructure;  2) develop a single node classifier using delay measurements and hop distances based on machine learning to detect forged latency results by adversarial clients in a cloud network; and  3) develop a fusion classifier which is scalable and independent of the network latency levels by training to pool latency and hop count data from multiple target nodes, allowing sufficient number of training examples at any latency level."
"1117705","III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families","IIS","Info Integration & Informatics","07/01/2011","06/03/2011","Vishwanathan Swaminathan","IN","Purdue University","Standard Grant","Sylvia Spengler","10/31/2015","$248,221.00","","vishy@ucsc.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7364","7923","$0.00","III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families<br/>Swaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz<br/><br/>Machine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms  are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that  are robust to outliers. <br/><br/>The key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases.  For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. <br/><br/>In partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http://learning.stat.purdue.edu/wiki/tentropy/start"
"1118028","III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families","IIS","Info Integration & Informatics","07/01/2011","10/15/2015","Manfred Warmuth","CA","University of California-Santa Cruz","Standard Grant","Sylvia Spengler","07/31/2016","$249,995.00","","manfred@cse.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7364","7923","$0.00","III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families<br/>Swaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz<br/><br/>Machine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms  are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that  are robust to outliers. <br/><br/>The key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases.  For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. <br/><br/>In partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http://learning.stat.purdue.edu/wiki/tentropy/start"
"1053407","CAREER: New Approaches for Ranking in Machine Learning","IIS","Robust Intelligence","09/01/2011","06/10/2015","Cynthia Rudin","MA","Massachusetts Institute of Technology","Continuing grant","Weng-keen Wong","01/31/2017","$480,000.00","","cynthia@cs.duke.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","1045, 1187","$0.00","In numerous industries, decisions are based on large amounts of data, where a ranked list of possible actions determines how limited resources will be spent. Over the last decade, machine learning algorithms for ranking have been designed to address prioritization problems. These algorithms rank a set of objects according to the probability to possess a certain attribute; for example, we might rank a set of manholes in order of their probability to catch fire next year. However, current algorithms solve ranking problems approximately rather than exactly, and these approximate algorithms can be slow; furthermore they do not take into account many application-specific problems.<br/><br/>The goals of this project include: <br/><br/>I) Finding exact solutions to ranking problems by developing a toolbox of algorithmic techniques based on mixed-integer optimization technology. <br/><br/>II) Finding solutions faster by showing a fundamental equivalence of ranking problems to easier classification problems that can be solved an order of magnitude faster. <br/><br/>III) Developing frameworks for new structured problems. The first framework pertains to ranking problems that have a graph structure that are relevant to the energy domain. The second framework handles a sequential prediction problem arising from recommender systems, with applications also in the medical domain.<br/><br/>Through collaboration with industry, the proposed methods are being applied in several different areas, including the prevention of serious events (fires and explosions) on NYC's electrical grid."
"1128817","Convex optimization methods for system identification and graphical  modeling of time series","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2011","07/29/2013","Lieven Vandenberghe","CA","University of California-Los Angeles","Continuing grant","Radhakisan Baheti","08/31/2016","$378,761.00","","vandenbe@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","ENG","7607","092E","$0.00","Objectives<br/><br/>The project aims at developing new methods for dynamical system modeling, based on convex optimization formulations and recent algorithms for large-scale non-smooth optimization.  A first component of the project addresses the estimation and topology selection of graphical models of time series.  A graphical model provides a graph representation of relations between random variables, for example, conditional dependence.  These relations can be translated into sparsity constraints on the parameters of the model.  A fundamental challenge in the estimation of a graphical model is the selection of a sparse graph topology from observed data.  The project aims at developing methods for sparse topology selection via non-smooth convex regularizations.  The main application that motivates this work is connectivity analysis from functional magnetic resonance imaging time series. A second part is concerned with new methods for system identification based on convex algorithms for low-rank approximation of structured matrices.  This work requires the formulation of system identification problems as constrained rank optimization problems and the development of large-scale algorithms for convex relaxations of the rank optimization problems.<br/><br/>Intellectual Merit<br/><br/>The project combines techniques from optimization, system theory, and machine learning to address fundamental problems in the modeling of dynamical systems.<br/>Graphical models, an important topic in machine learning, are not widely studied in system identification.  Conversely, system identification can provide tools for modeling dynamical aspects in machine learning problems. The use of convex formulations and fast first-order algorithms will enable an efficient solution of large instances in practical applications.<br/><br/>Broader Impacts<br/><br/>Software implementations of the algorithms developed in the project will be made freely available. The outcomes will be integrated in the graduate optimization sequence in the Electrical Engineering Department at UCLA, in particular an advanced course on large-scale optimization.  Research opportunities will be offered to students via individual study courses and summer internships."
"1131869","Collaborative Research: NSWP--Machine Learning and Data Assimilation for Real-Time Radiation Belt Forecasting","AGS","MAGNETOSPHERIC PHYSICS","07/15/2011","04/24/2014","Xinlin Li","CO","University of Colorado at Boulder","Continuing grant","Janet U. Kozyra","06/30/2016","$170,000.00","","Xinlin.Li@lasp.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","GEO","5750","9196","$0.00","This project is a collaborative effort with scientists at the Los Alamos National Laboratory to develop a real-time model for specifying and forecasting the state of Earth's radiation belt.  The project will utilize data assimilation methods in conjunction with physical models and modern machine learning techniques to examine the acceleration, transport and loss processes for energetic electrons in the radiation belt.  The resulting model will include confidence limits on the system-wide forecasts, making the results useful for operational space weather predicitions.  The project will benefit society by making improved radiation-belt predictions at reduced operational costs and the forecasts will be made freely available online.<br/><br/>The project includes a strong educational and teaching component through the participation of graduate students at the University of Colorado.  In addition the research will be integrated with the Los Alalmos Space Science Outreach (LASSO) program, which offers teacher training to underrepresented groups and individuals with disabilities."
"1106434","EAGER: Unified categories for describing and quantifying scientific research","SMA","SciSIP Infrastructure","02/15/2011","02/15/2011","David Newman","CA","University of California-Irvine","Standard Grant","Joshua Rosenbloom","01/31/2013","$162,774.00","","newman@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","SBE","8075","7626, 7916","$0.00","This project develops state-of-the-art machine learning methods to describe and quantify scientific research. The particular approach taken is to develop new topic models that can learn underlying research categories across a wide variety of text data sources including NSF and NIH grant awards, scientific publications, and US patents. The important innovation is the building of the technology to permit feedback from domain experts and end users. The approach is potentially transformative in that it can potentially overcomes some of the known limitations of current topic modeling approaches by improving the quality and utility of topics across diverse data sources. <br/><br/><br/>The web-based tool displays and manipulates learned topics so that users can apply it to create comprehensive overviews of scientific funding, research, and production, including the answers to questions like:<br/>- What types of science are funded by NSF, NIH and other agencies?<br/>- What types of science are produced by funded investigators?<br/>- What types of science are described in US patents?<br/><br/>The tool also provide users with answers to more complex questions about the science of science and the relationship between funding and scientific achievements, tracking trends, describing funding programs, and identifying funding overlap (across agencies, or even within agencies).<br/> <br/><br/>Intellectual Merit: The proposed research advances techniques and methods for tracking scientific<br/>research in several ways. First, it advances the development of unsupervised statistical topic models to categorize, describe, and measure scientific research. Second, it addresses known problems with topic modeling for this type of application, such as improving the coherence of all topics, and making topics transcend different types of document collections (grants, publications, patents). Users can more directly measure impacts of funding as a result of being able to make use of unified topics from grants, publications, and patents. Third, the research develops evaluation frameworks that shift the focus from machine learning metrics to the needs of domain experts and end users. <br/><br/>Broader Impacts: This work has an array of broader impacts. It creates useful data for funding agency staff, researchers, interested public, government bodies, media and other stakeholders. The web based tool allows users to create custom-based data sets, tailored to their particular needs. Such data sets allow users to answer an array of science of science policy questions. The knowledge created in this work supports initiatives such as STAR METRICS to document the value of investments in scientific research."
"1101389","ICES: Large: Economic Foundations of Digital Privacy","CCF","Inter Com Sci Econ Soc S (ICE)","09/01/2011","11/05/2012","AARON ROTH","PA","University of Pennsylvania","Standard Grant","Tracy Kimbrel","08/31/2016","$997,993.00","Michael Kearns, Sham Kakade, Mallesh Pai","aaroth@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8052","7752, 7924","$0.00","In the last decade private data has become a commodity - it is gathered, bought and sold, and contributes to the primary business of many Internet and information technology companies. At the same time, various formalizations of the notion of ""privacy"" have been developed and studied by computer scientists. Nevertheless, to date we lack a theory for the economics of digital privacy, and this project aims to close this important gap.<br/><br/>Concretely, the project will develop a theory to address the following questions:<br/><br/>- How should a market for private data be structured? How can one design an auction that accommodates issues specific to private data analysis: that the buyer of private data often wishes to buy from a representative sample from the population; and that individuals' value for their privacy can itself be a very sensitive piece of information?<br/><br/>- How should other markets be structured to properly account for participants' concerns about privacy? How should privacy be modeled in auction settings, and how should markets be designed to address issues relating to utility for privacy?<br/><br/>- Studying economic interactions necessitates studying learning - but what is the cost of privacy on agent learning? How does the incomplete information that is the necessary result of privacy-preserving mechanisms affect how individuals engaged in a dynamic interaction can learn and coordinate, and how do perturbed measurements affect learning dynamics in games? How can market research be conducted both usefully and privately?<br/><br/>Our investigation of these questions will blend models and methods from several relevant fields, including computer science, economics, algorithmic game theory and machine learning.<br/><br/>This project directly addresses one of the most important tensions that the Internet era has thrust upon society: the tension between the tremendous societal and commercial value of private and potentially sensitive data about individual citizens, and the interests and rights of those individuals to control their data. Despite the attention and controversy this tension has evoked, there is no comprehensive and coherent science for understanding it. Furthermore, science (rather than technology alone) is required, since the technological and social factors underlying data privacy are undergoing perpetual change. Within the field of computer science, the recently introduced subfield of privacy preserving computation has pointed the way to potential advances. This project aims to both broaden and deepen these directions."
"1115956","SHB: Small: Computational Algorithms for Predictive Health Assessment","IIS","INFORMATION TECHNOLOGY RESEARC","09/01/2011","09/01/2011","Mihail Popescu","MO","University of Missouri-Columbia","Standard Grant","Sylvia J. Spengler","12/31/2014","$150,000.00","Marjorie Skubic, Richelle Koopman","popescum@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","1640","7923, 8018","$0.00","Abstract: SHB: Small: Computational Algorithms for Predictive Health Assessment<br/>This project leverages ongoing work at Tiger Place (TP), University of Missouri (MU) in the use of sensor technology for in-home health assessment.  The TP team has deployed sensor networks in the homes of seniors, with a wide range of sensor types and analysis approaches. They are integrating their sensor networks with an in-house nursing electronic health record (EHR) and investigating health context-aware computational algorithms for health and wellbeing assessments. The proposed project has the following objectives: (1) Integrate the sensor network with an EHR developed in-house to provide automatic health context for comprehensive algorithm development; (2) Investigate algorithms for identifying health patterns based on sensor data and contextual health information such as chronic conditions and medication changes that are provided by the EHR data; and (3) Investigate the possibility of predicting physiological changes such as blood pressure based on sensor data. A variety of machine learning methods are investigated for predictive health assessment. There are two potential difficulties that this research tackles: ground truth uncertainty and data unbalance. To address these problems the project is developing two new machine learning methods: a fuzzy extension of multiple instance learning and a sensor firing sequence similarity based method for recognizing pattern changes. Existing sensor data is used as a starting point to develop the proposed methods, along with simulated data for more diverse testing scenarios. The integrated combination of the sensor network and EHR is expected provide a unique, rich dataset in which to investigate health context-aware algorithms."
"1144227","EAGER: Automated High Speed Object Category Modeling and Model Based Recognition, Segmentation, Clustering, and Classification","IIS","","08/15/2011","07/20/2011","Narendra Ahuja","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jie Yang","07/31/2013","$266,276.00","","ahuja@vision.ai.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","K565","7916","$0.00","This project explores new directions to solving the following problem. Given an image, determine whether and where specific objects, or objects from a specific category, appear in the image. Visual category is defined as earlier, namely, as a collection of objects which share characteristic features that are visually similar, and occur in similar configurations. The visual nature of objects sought is communicated through (training) data containing them, and estimated using machine learning. The approach consists of two main parts. First, it learns whether a given set of previously unseen images (including videos), say supplied by a user, contains any dominant themes, namely, subimages, that occur frequently and look similar. Second, given a set of categories automatically inferred during training and a new test image, the approach recognizes all occurrences in the image of the learned categories. It delineates each such object in the image, and labels it with its category name. Both learning and subsequent recognition do not require human supervision. The approach learns and recognizes categories as image hierarchies. The impact of the project includes accurate high-speed extraction of image regions, image representation by connected segmentation tree, robust image matching, unsupervised extraction of hierarchical category models, efficient recognition of a large number of categories, unsupervised estimation of perceptually salient, relevance weights of subcategory detections to category recognition, and generalization of the proposed approach to extraction of texture elements. More broadly, the proposed approach is useful for applications in search engines, surveillance, video analytics, monitoring and data mining."
"0938430","Individual Award for Excellence in Physics Mentoring","DUE","PAESMEM Pres Awrds Excell Ment","06/01/2011","05/26/2011","Richard Cardenas","TX","St Mary's University San Antonio","Standard Grant","Martha James","05/31/2013","$10,000.00","","rcardenas@stmarytx.edu","One Camino Santa Maria","San Antonio","TX","782285433","2104363720","EHR","1593","9178, SMET","$0.00","Dr. Richard Cardenas has served as the chair of the Physics and Earth Science department at St. Mary?s University since 2004. He facilitated the formation of the Society of Physics Students in 2002 and presently serves as the principal adviser to this group. In addition, Dr. Cardenas initiated the Fiesta of Physics Program in 2003, and since its inception the program has provided science education outreach to more than 15,000 families who live in the area surrounding St. Mary's University in San Antonio, Texas. This program impacts low income, minority families who otherwise may not consider a science career or understand the role the natural world plays in their day-to-day lives. As a primary mentoring activity, St. Mary's students help develop the programming and conduct the experiments that they take to the various schools in the surrounding area. Over the past six years, Dr. Cardenas has personally mentored over 60 physics majors, and over 500 St. Mary?s students, primarily from the sciences but also from non-science majors, have participated in the Fiesta of Physics program. In order to prepare and teach the science experiments in Fiesta of Physics, students must have a good understanding of basic physics and other science principles and achieve deep learning that transcends the superficial memorization that often serves as knowledge. Einstein once stated that ""if you can?t explain relativity to a six-year old, you don't understand it"". Fiesta of Physics students have taken Einstein's observation to heart as they prepare to explain a variety of science principles to many six through ten-year olds and their families, many of whom indicated that they would like to become scientists in the future. In addition, the St. Mary's students develop greater curiosity and interest in science themselves, and over 80% of physics majors go on to attend graduate school to seek Ph.Ds. Many others go on to teach in area high schools. This Fiesta Physics Program reaches thousands of children and their families who have earlier not been interested in science and who are often taught by teachers who lack education and training in science fields. Frequently, science teachers recognize their limitations and actively recruit Dr. Cardenas to bring the Fiesta of Physics to their schools. Due to resource limitations, Fiesta of Physics has limited its programming to the Edgewood Independent School District, one of the poorest school districts in Texas. Currently, there is a waiting list of schools outside the Edgewood District that want to bring Fiesta of Physics to their students. To try and accommodate more schools, twice a year Fiesta of Physics provides programming through an on-campus extravaganza for all schools that wish to participate. Science demonstrations have helped the school children on the state-mandated achievement test. The Spring 2009 program had over 300 attendees most of whom were fourth or fifth graders. Increasingly, non-science majors who want to participate in community education outreach are being drawn to the program."
"1118041","III: Small: Automated Event Classification and Decision Making in Massive Data Streams","IIS","Info Integration & Informatics","08/01/2011","07/22/2011","Stanislav Djorgovski","CA","California Institute of Technology","Standard Grant","Sylvia Spengler","07/31/2014","$499,982.00","","george@astro.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1798, 7364","1206, 7569, 7923","$0.00","As the exponential growth of data volumes and complexity continues in all sciences (and indeed all other fields of the modern society, economy, commerce, security, etc.), there is a growing need for powerful new tools and methodologies which can help us extract knowledge and understanding from these massive data sets and data streams.  The newly gained knowledge is often used to guide our actions, and in science that typically means follow-up studies and measurements, as the research cycle continues.  As the data rates and volume increase, it becomes necessary to take humans out of the loop, and develop automated methods for time-critical knowledge extraction and optimized response to anomalous or interesting events found by the data processing pipelines.  This proposal is to develop a system that will be an example of a new generation of scientific experiments and methods that involve real-time mining of massive data streams, and dynamical follow-up strategies.   The system would be developed and validated in the context of real scientific situations from the emerging field of time-domain astronomy.  A new generation of synoptic sky surveys covers the sky repeatedly, detecting variable or transient phenomena, over a broad range of astrophysics, from the Solar system and stellar evolution, to cosmology and extreme relativistic objects; from extrasolar planets to gamma-ray bursts and supernovae as probes of the dark energy.  As we explore the observable parameter space, there is a real possibility of discovery of new types of objects and phenomena.  <br/><br/>The system will enable exciting new astrophysics, and facilitate discovery.  The key to this is a fully automated classification and prioritization of the transient events, and their follow-up observations.  This poses some interesting challenges for applied computer science, especially in the area of Machine Learning, including an automated classification where only a sparse, incomplete, and heterogeneous data are available, and contextual information and domain expertise must be folded in the process.  The process must be dynamic, incorporating new data as they become available, and revising the classifications accordingly.  The system would then generate automatically decisions for an optimal follow-up of the most interesting events, given the available limited assets and resources.  This project will aid the entire astronomical community in developing new scientific strategies and procedures in the era of large synoptic sky surveys, facilitate data sharing and re-use, and stimulate further development of Virtual Observatory capabilities.  The methods and experiences gained here will be described in the open literature, so that they may find a broader use outside astronomy, wherever similar time-critical situations occur, thus fostering constructive new synergies between applied computer science and other domains.  The proposers will train undergraduate and graduate students and postdocs, in the methods of scientific computing and computational thinking, and develop effective EPO materials, touching on both the new science and computation.<br/><br/>The challenges posed by the knowledge extraction in the era of data abundance become even sharper in the time-critical situations where we mine the information from massive data streams, especially when the phenomena under study are short-lived, and/or a rapid follow-up reaction is needed.  Potentially interesting phenomena and events must be identified, classified, and prioritized in real time, typically using some combination of the new measurements, and existing archival data and models.  Then an optimal decision has to be made as to what is the best follow-up that will provide the essential new information in any given individual case; this can be critical if the follow-up assets are scarce or costly.  If the time scales are short, and data rates large, the implication is that humans should be taken out of the loop, and that the classification, prioritization, and follow-up decision process must be fully automated.  Machine learning (ML) and machine intelligence tools become a necessity.  This proposal is to develop a novel, ML-based system for a real-time classification and prioritization of transient events, using the newly emerging field of time-domain astronomy and synoptic sky surveys as a scientific testbed. The classification problem here is different from the usual situations: the data are sparse and/or incomplete, heterogeneous, and evolving as the new measurements come in; the decision process has to take into account the uncertainties of the classification process, and the available assets; and so on.  While the sky surveys detect transient cosmic events, the scientific returns come from their directed follow-up.  It is essential to be able to classify and prioritize interesting events, especially as we move from the present Terascale data streams and tens of candidate events per night, to the future Petascale data regime, with literally millions of candidates, only a handful of which can be followed.  Given the problem of data incompleteness and sparsity, the proposers will explore the use of Bayesian techniques that can operate on a set of expert-developed and ML-based priors, using the currently best available data.  Some of the methodological challenges include incorporation of the contextual information and human expertise and optimal combination of separate classifier outputs, as well as new methods developed in this project.  All of the algorithmic developments will be done keeping the robustness and scalability in mind, and tested on real scientific use cases."
"1118061","SHB: Small: Robustly Detecting Clinical Laboratory Errors","IIS","Information Technology Researc","12/15/2011","01/31/2017","Steven Kazmierczak","OR","Oregon Health & Science University","Standard Grant","Sylvia Spengler","05/31/2017","$500,000.00","Deniz Erdogmus","kazmierc@ohsu.edu","3181 S W Sam Jackson Park Rd","Portland","OR","972393098","5034947784","CSE","1640","1640, 7923, 8018","$0.00","Hospital clinical laboratory tests are a major source of medical information used to diagnose, treat, and monitor patients.  Such test errors lead to delays, additional clinical evaluation, additional expense, and sometimes to erroneous treatments that increase risk to patients.  One recent study suggests that errors in measured total blood calcium concentration due to instrument mis-calibration alone cost from $60M to $199M annually in the US. However, the vast majority of clinical laboratory errors do not originate in instrument mis-calibration. Clinical laboratory errors affect about 0.5% of samples collected.  Of those, approximately 75% of clinical laboratory test errors originate during sample collection, transport, and storage before samples reach the analysis instruments i.e., the pre-analytic phase.  However the quality control measures standard in hospital clinical test labs only monitor instrument calibration and are therefore completely blind to sample faults introduced in the pre-analytic phase, where most errors originate.  Data derived from patient samples, rather than instrumentation calibration checks, holds the key to detect faults introduced in the pre-analytic phase.  Current methods are either so insensitive to errors that they do not detect sample faults reliably, or they routinely flag normal samples as being faulty.<br/><br/>This project brings together an interdisciplinary team of researchers  from Oregon Health and Science University and Northeastern University with expertise in machine learning, signal processing, and laboratory medicine to develop and apply statistical machine learning technology to reliably detect errors in hospital clinical laboratory tests, using data derived from patient samples.  The primary obstacle to developing reliable statistical detectors for lab errors is the cost of labeling samples combined with the low error rate.  Developing and evaluating any automated error-detection algorithm requires a sufficient number of samples, both faulty and non-faulty. Determining which tests are faulty requires review of the tests and other patient data (e.g. charts) by a clinical lab expert - a time-consuming and economically unfeasible prospect given the low fault rate.  The project addresses this challenge through active learning paradigms used to select, with emphasis on rare classes, subsets of the data for labeling by human experts.  The project focuses on chronic kidney disease because of its medical importance and large data repository at Oregon Health and Science University. This research will provide algorithms for clinical lab error detection that will extend to tests used in other disease entities (for example diabetes and heart failure).  <br/><br/>Ultimately, the error-detection algorithms developed from this research will make their way into clinical laboratory information systems and further into commercialization and thus deployment on a scale significant enough to have widespread positive impact on laboratory costs patient risk.  The project provides cross-disciplinary training in statistical pattern recognition and clinical laboratory science for graduate and undergraduate students. Additional information about the project can be found at:  http://www.bme.ogi.edu/~tleen/LabErrorDetect/."
"1149463","SHF: Small: Collaborative Research: Correlation Mining and its Applications in Test Cost Reduction, Yield Enhancement, and Performance Calibration in Analog/RF Circuits","CCF","SOFTWARE & HARDWARE FOUNDATION, DES AUTO FOR MICRO & NANO SYST","09/01/2011","08/30/2011","Yiorgos Makris","TX","University of Texas at Dallas","Standard Grant","Sankar Basu","08/31/2012","$78,406.00","","yiorgos.makris@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7798, 7945","7923, 7945, 9216, 9217, 9218, 9251, HPCC","$0.00","This project seeks to improve the quality and reliability of Analog/Radio-Frequency (RF) integrated electronic circuits (ICs) by developing an intelligent system for systematically exploring the wealth of information generated throughout their production lifetime and applying it towards improving the effectiveness of their design, manufacturing, and testing. While a large amount of data is made available through extensive design simulations and measurements on actual fabricated circuits, there currently exists a striking lack of formal methods to efficiently extract meaningful information from this data. The research activities that will be carried out through this project aim to fill this void by developing correlation mining methods based on the most recent developments in the fields of machine learning and data mining. Ultimately, using data from actual IC productions provided by industrial partners (i.e. IBM and Texas Instruments), the objective of this project is to demonstrate the impact that such correlations can have on reducing the cost of testing, enhancing the yield of the production and enabling post-manufacturing calibration of analog/RF circuits. <br/><br/>This project will facilitate the cost-effective realization of robust electronic circuits and systems, thus enabling more reliable computing and promoting technology trustworthiness. The proposed research is complemented by educational and outreach activities, including the development of a new graduate-level course on applications of Machine-Learning in Computer Aided Design and Test and the involvement of graduate, undergraduate and high-school students in research with the groups of the Principal Investigators, the industrial partners, and the research laboratory of the international collaborator."
"1127567","SBIR Phase II:  Software to Automate the Detection of Websites that are Fraudulent or Otherwise Harmful to Consumers","IIP","STTR Phase II, SBIR Phase II","09/01/2011","09/25/2012","Michael Lai","CA","GGL Projects, Inc.","Standard Grant","Glenn H. Larsen","02/28/2014","$600,000.00","","fastlane@sitejabber.com","3150 18th Street","San Francisco","CA","941102076","4158945806","ENG","1591, 5373","169E, 5373, 8032","$0.00","This Small Business Innovation Research (SBIR) Phase II project will develop software to automatically detect a broad spectrum of websites that are fraudulent or otherwise harmful to consumers. Much work has been done on specific software capable of detecting websites hosting malware or engaged in phishing. However, software does not yet exist which can detect a broader array of harmful websites, including those selling counterfeits, selling illegal drugs, and hosting weight-loss scams, to name just a few. The challenge in doing this involves selecting the right features of fraudulent sites which in isolation or combination are good indictors of a site's harmfulness. Using these features, a machine learning classifier can be trained using data on known harmful websites. Unknown websites can then be run through the classifier to evaluate their potential for harm. Additional challenges involve gathering sufficient data to properly train the classifier, making the classifier general enough to detect a range of harmful sites while still maintaining accuracy, and updating the classifier in real-time such that it can improve with ongoing human feedback and additional data.<br/><br/>The principal impact of this project is the protection of consumers from online fraud. Today, consumers lack reliable resources to evaluate unfamiliar websites. Most use familiar sites like Amazon or take a gamble on Google search results. These gambles frequently result in fraud. It is believed that there are now over 250 million websites and $100 billion lost yearly to online fraud. While the statistics cover many types of fraud, examples of risky sites include online counterfeiters, pharmacies, and retailers. The software developed in this project will greatly improve transparency around websites and protect millions from fraud.  The technical achievements in this project involve the use of a vector space model in converting non-discrete features of fraudulent sites into useful data that can be inputted into a machine learning classifier. Additionally, this technology will include innovative feature choices, access to high-quality data, and the creation of a general classifier capable of improving itself in real-time and detecting a broad array of heretofore undetectable fraudulent sites."
"1131872","Collaborative Research: NSWP--Machine Learning and Data Assimilation for Real-Time Radiation Belt Forecasting","AGS","MAGNETOSPHERIC PHYSICS","07/15/2011","06/17/2019","Humberto Godinez","NM","Department of Energy Albuquerque Operations Office","Interagency Agreement","Michael Wiltberger","07/31/2016","$264,133.00","","hgodinez@lanl.gov","PENNSYLVANIA & H ST SE","Albuquerque","NM","871150000","","GEO","5750","9150, 9196","$0.00",""
"1117740","III: Small:  Uncovering the Myths of Unlikelihood: Granger Graphical Models for Anomaly Detection in Multivariate Time-Series Data","IIS","Info Integration & Informatics","08/01/2011","07/22/2011","Yan Liu","CA","University of Southern California","Standard Grant","Sylvia Spengler","07/31/2016","$499,999.00","","yanliu.cs@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7364","7923","$0.00","The project aims to develop effective approaches to anomaly detection from high-dimensional time series data, motivated by applications such as oil drilling, semiconductor fabrication, and railroad operation.<br/>The proposed approach takes advantage of Granger Graphical models, which uncover the temporal dependencies between variables, to efficiently compute a robust correlation anomaly score for each variable and obtain insights regarding the causes of anomalies. The project develops effective approaches to addresses several specific challenges that arise in real-world applications of anomaly detection, including (1) nonlinear temporal dependencies; (2) hidden variables; and (3) massive amounts of data. The resulting algorithms will be evaluated on two real production systems: an oil-field mechanical system and a semi-conductor fabrication system.<br/><br/>The project is expected to advance the state of the art in anomaly detection for high-dimensional time series data that arise in many application domains. It offers research-based training opportunities at the intersection of machine learning, data mining, and intelligent production management, as well as operational research in general.Workshops and mini-courses will be organized to introduce advanced machine learning techniques to students, practitioners, and researchers in production management. The anomaly detection code and data sets will be freely disseminated to the broader research and educational community. Additional information about the project can be found at: http://www-bcf.usc.edu/~liu32/ggm.htm."
"1146740","IIS: Workshop on Population Health Data Measurement, Representation, and Predictive Modeling.","IIS","Info Integration & Informatics, Smart and Connected Health","09/01/2011","08/10/2012","Bruce Schatz","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sylvia Spengler","08/31/2013","$87,457.00","","schatz@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7364, 8018","7364, 7556, 8018","$0.00","The workshop brings together a group of scientists with complementary expertise in health informatics, population health, and computer science to identify the research challenges and opportunities in  population health data measurement, representation, and predictive modeling.  <br/><br/>Despite great advances in measurement, computing, and communication technologies,  the health data measurement relies on legacy practices (e.g., phone surveys). In contrast,  billions of persons worldwide have mobile devices, such as cell phones and music players, which contain measurement sensors and are increasingly network aware. If these devices could be appropriately employed for population health data measurement, they could revolutionize the acquisition and use of population health  data. However, there are many research challenges that need to be addressed in order to make widespread use of actionable population health data. <br/><br/>The workshop  is organized around a vision of actionable data for population health.  It covers a broad range of questions such as: What data should be recorded to measure everyday health? How should this data be most helpfully collected? How should the sensor data be classified into actionable data? How should the diverse sources be judged for quality? How should this data be mined and correlated? How can population data be transformed into usable knowledge? How should this data be used to develop practical health systems? How can multiple knowledge sources be integrated for multiple users? How can existing data (medical records and clinical trials) be leveraged using model-based inference to support customized decision making and refine predictive models? What is the impact of this new data on health quality and cost? <br/><br/>Workshop participants include experts in the areas of health informatics, knowledge representation and inference, machine learning and data mining. Thw workshop aims to increase the awareness of research challenges and opportunities in health informatics in general, and population data measurement, representation, and predictive modeling in particular, among researchers in data mining, knowledge representation and inference, machine learning, text analysis, human-computer interaction, social networks and social media, semantic web, decision theory. It also aims to make researchers in health informatics, public health, and related areas better aware of the state of the art informatics approaches that could be leveraged to develop the next generation health informatics infrastructure. The workshop results, including new research challenges and opportunities in discovery informatics, will be broadly disseminated through the workshop report, publications by workshop participants, and outreach efforts through follow-up activities that engage the research community."
"1124794","CDI-Type I: A Unified Probabilistic Model of Astronomical Imaging","IIS","CDI TYPE I","09/01/2011","09/01/2011","David Hogg","NY","New York University","Standard Grant","Sylvia J. Spengler","08/31/2016","$675,000.00","Robert Fergus","david.hogg@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7750","7721","$0.00","Overview: The astrophysics community has produced a Petabyte of of imaging data in a wide range of wavelength channels in the last decade, and is planning to produce a thousand times more in the next.<br/>At the same time, the computer science machine learning community has developed powerful methods for extracting knowledge scalably from large, heterogenous data sets.  This project is to construct a model--a detailed quantitative explanation--for every pixel of every digital astronomical image ever taken by any telescope in the world, including those from amateurs and hobbyists.<br/><br/>Technical description: The proposed model is a justified approximate probabilistic model, making extensive use of non-parametric Bayesian methods.  The model will be hierarchical in nature, with the higher layers capturing regularities among stars and galaxies, and the lower layers will accurately model the image formation process, incorporating all the various noise processes.  The internal parameters of the model will contain the best possible astronomical catalog given the input data; no current astronomical catalog is built using either hierarchical probabilistic inference, or built from the union of all available data.  Science will be enabled by this catalog as it has been enabled by all previous astronomical catalogs: it will contain the position, brightness, temperature, parallax, and proper motion of every star and position, intensity, and morphology of every galaxy, even for sources for which there is evidence in the collection of data but not (sufficiently) in any individual image.  Other internal parameters of the model will contain a quantitative description of calibration properties for all the image-generating hardware.  All these products will help to refine and extend an existing astrophysics software and services for calibration and automated data processing.<br/><br/>Broader impacts: This project provides unique opportunities in citizen science since it leverages  images taken by amateur and hobbyist astronomers in creating astronomical knowledge by offering them opportunities to contribute in exactly the same fashion as professional astronomers.  The project draws together two disparate fields: machine learning and astronomy, and thus has the potential to create a new sub-area of ""inferential astronomy"". The project offers enhanced opportunities for research-based advanced interdisciplinary training for postdoctoral, graduate and undergraduate researchers.  All code created for this project will be released under the open-source license and all model components, parameters, and other internals will be freely disseminated to the wider research community through the project websites at: <br/>http://Astrometry.net/ and http://thetractor.org/ ."
"1116447","CIF: Small: Collaborative Research: Compressed Sensing for Coherent Designs under Gaussian/Non-Gaussian Noise","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2011","06/23/2011","Yiyuan She","FL","Florida State University","Standard Grant","John Cozzens","06/30/2015","$251,000.00","","yshe@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7797","7923, 7936","$0.00","The recent explosion of large amounts of high dimensional data in science, engineering, and society demands new technologies to recover sparse signals from high dimensional noisy observations.  To address it, this project aims to develop efficient and robust methods for analyzing high-dimensional data, which have wide applications in signal processing, communication, computational biology, machine learning,   image/video coding, sensor networks, social science, etc.<br/><br/>Recently,  compressed  sensing  (CS)  has  attracted  a  good  deal  of  attention  from computer science, engineering, and statistics communities. However, the CS recovery only makes sense when the features are weakly correlated. A single rogue outlier may break down the reconstruction completely. The computational procedures cannot meet the challenge of ultrahigh dimensional problems in terms of statistical accuracy, algorithmic stability and computation expediency.  To address these challenges, the investigators develop novel nonconvex regularization techniques to attain prediction accuracy and model parsimony for coherent statistical models that go much beyond Gaussianity. Theoretical analysis of its performance in estimation, prediction, and sparsity  recovery  is  conducted.  A  class  of  simple  algorithms  feasible  for  solving essentially any nonconvex penalized generalized linear models is developed, together with a randomization technique of nonmarginal feature screening for ultra-high dimensional data. Furthermore, the investigators explicitly study the critical effects of outliers and develop a robust CS for handling high leverage points and gross outliers. A unified   framework   that   applies   to   small-sample-size-high-dimension   problems   is provided for simultaneous variable selection and outlier identification under Gaussian/non-Gaussian noise. Finally, this project involves rich motivating examples and widespread applications in various areas including spectral analysis, network topology and dynamics modeling, graphical models, computational biology, machine learning, and image compression, as an essential component."
"1125676","BRIGE: Computational Identification of Gene Regulatory Networks in Microalgae","CBET","BRIGE-Broad Partic in Eng","08/15/2011","08/10/2011","Haiyan Hu","FL","The University of Central Florida Board of Trustees","Standard Grant","Alexander Leonessa","07/31/2014","$174,654.00","","haihu@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","ENG","7741","008E","$0.00","PI: Hu, Haiyan<br/>Proposal Number: 1125676<br/><br/>The research and education goals of the project are to: (1) propose a computational framework to systematically study gene regulation in microalgae towards in-silico modeling and bioengineering applications; (2) educate college students and general public about microalgae gene regulation; and (3) expose women and girls to interdisciplinary science and engineering through mentoring and outreach.<br/>Research Activities: The research objective is to create novel computational approach to perform genome-wide identification of DNA regulatory elements and their patterns in microalgal model organism C. reinhardtii. The planned activities include: (1) genome-wide identification of DNA regulatory regions in C. reinhardtii by creating new strategy to measure sequence conservation; (2) identification of candidates for DNA regulatory elements via novel machine learning algorithms; and (3) identification of interacting DNA regulatory elements in C. reinhardtii through frequent pattern mining and statistical modeling. The longer-term goal of this project is to develop statistical and computational algorithms to model gene regulatory network of microalgae, and to integrate gene regulation information into in-silico modeling of microalgae for microalgae engineering.<br/>Education Activities: The educational objectives are to introduce students at multiple levels to the exciting area of bioinformatics; disseminate the knowledge obtained from the proposed study and develop outreach activities to attract more girls and women into science and to broaden participation of underrepresented groups. The planned activities include: graduate/undergraduate mentoring, curriculum development, and outreaching/mentoring women and girls by collaborating with the UCF office of Undergraduate Research and National Girls Collaborative Project. The education activities will be tightly integrated with the research activities. A combination of metrics will be employed to evaluate the education activities.<br/>Intellectual Merit: Understanding how genes are transcriptionally regulated in microalgae is an important problem in both biology and microalgae engineering. The proposed work aims to advance our understanding of gene regulation in microalgae by computationally identifying DNA regulatory elements at the genome-scale in microalgae model organism C. reinhardtii. There is as yet no broadly applicable method and no systematic study to comprehensively identify DNA regulatory elements and characterize gene regulatory mechanisms in C. reinhardtii. By creating novel computational algorithms such as alignment-free methods to identify regulatory regions in the entire C. reinhardtii genome and enumerative Gibbs sampling approach to de novo identify DNA regulatory elements, the proposed work will be able to systematically discover DNA regulatory signals in C. reinhardtii, and will lay the ground for genomescale gene regulatory network construction in C. reinhardtii and other microalgal organisms in the near future. The gene regulatory information gained from the proposed research has the promise to facilitate integrative in-silico modeling of microalgae and microalgae bioengineering in the subsequent research. The prior work on data integration and knowledge discovery from large scale biological data, machine learning and data mining techniques, and software development put the applicant in a unique position to perform the proposed research.<br/>Broader Impacts: The proposed research will have great impact on education at multiple levels. The research will be incorporated into the graduate and undergraduate education by graduate/undergraduate mentoring and curriculum development. The knowledge resulted from the proposed research will be disseminated to the research community and the public to enhance scientific understanding through a website. In addition, mentoring and outreach for women and girls will create a positive cycle in attracting more women into interdisciplinary science."
"1054009","CAREER:  A Scalable, Declarative, Imprecise Database Management System","IIS","Info Integration & Informatics","05/01/2011","06/07/2013","Christopher Re","WI","University of Wisconsin-Madison","Continuing grant","Frank Olken","09/30/2013","$400,155.00","","chrismre@cs.stanford.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7364","1045, 1187, 7364","$0.00","The unprecedented amounts of data available to individuals, companies, governments, and scientists promises to revolutionize the way entertainment, business, governance, and science operate. And while data are cheap and plentiful, much of this data is lower quality than the precise data that has been managed for the last 30 years. Building an application that processes this imprecise data is difficult: it requires that developers handle both standard data management challenges (e.g., concurrency and scalability), while at the same time coping with imprecise and incomplete data, which is typically done using statistical or machine learning techniques (e.g., interpolation and classification). The Hazy project addresses this challenge by building a system that integrates the paradigms of relational database management systems with statistical machine learning techniques. This project conducts the following major tasks: (I) designing a language to integrate these techniques with standard SQL, (II) proposing an algebra to implement this language along with support for automatic optimization (similar to a standard RDBMS), and (III) discovering techniques to efficiently maintain the statistical models as the underlying data are changed or updated. The end goal is a system that makes it as easy to develop scalable applications that use imprecise data as it is to develop their precise counterparts. Hazy allows users to process larger amounts of data with more sophisticated statistical processing than ever before. In turn, this enables new applications in a divese set of areas, such as life and physical science sensing applications, health-care and environmental monitoring, and enterprise-based and Web-based information extraction.<br/><br/>The research of this project is used to develop the data and infrastructure for new practicum-style courses that are under development at the University of Wisconsin-Madison. In addition, this infrastructure will be used as part of an outreach effort to enable high school students to gain access to data analysis tools. The source code of Hazy is released into open source and the results are disseminated on the project Web site (http://www.cs.wisc.edu/hazy/)."
"1143995","EAGER: Computer Architectures and Algorithms for Adaptive Human Computer Interfaces","IIS","Robust Intelligence","09/01/2011","07/22/2011","Yoav Freund","CA","University of California-San Diego","Standard Grant","Todd Leen","08/31/2013","$150,000.00","Ryan Kastner","yfreund@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7916","$0.00","The ease of use of a human computer interface depends critically on the latency of the system and its ability to adapt to the environment. Differences in lighting, visual appearance and user behavior can significantly alter the input data.  Furthermore, the reaction time must be less than 100 milliseconds to appear instantaneous to the user.  Achieving high accuracy demands a system that adapts to these changing characteristics while processing a significant amount of data in a short amount of time.<br/><br/>We propose a computer architecture for adaptive real-time signal processing systems that combines a general purpose processor with custom hardware. The custom hardware performs the low-level, high-throughput signal processing on the raw signals and feeds them to the processor which performs the high level signal processing and decision making. The processor also executes machine learning algorithms that change the parameters of the low-level processing to adapt them to the current statistical properties of the data.<br/><br/>This project will develop a human-computer interface based on audio and video sensors that allows a user to interact with the computer through gestures and voice alone.  This requires research advances in computer architecture, embedded systems, signal processing, machine learning and human-computer interaction. The major research challenge is in the integration of knowledge from the different areas to create a functional system. This system will serve as a prototype for novel human computer interactions and will be a foundation for future collaboration between the different fields."
"1125210","CDI-Type II: GLOBE: Evolving New Global Workflows for Land Change Science","CNS","SPECIAL PROJECTS - CISE, Computer Systems Research (CSR, CDI TYPE II","09/15/2011","08/30/2017","Erle Ellis","MD","University of Maryland Baltimore County","Standard Grant","Marilyn McClure","08/31/2018","$1,982,753.00","Timothy Finin, Penny Rheingans, Tim Oates, Wayne Lutters","ece@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","1714, 7354, 7751","7354, 7721, 7751, 9178, 9251","$0.00","This project focuses on Land Change Science (LCS).<br/><br/>Land Change Science is an emerging field of study, aimed at understanding interactions among human systems and the terrestrial biosphere, atmosphere and other Earth systems as mediated through human use of land. Advances in LCS are needed to better quantify, predict, mediate, and adapt to global climate change, biodiversity loss, and other consequences of land use and land cover change.<br/><br/>Despite vigorous efforts by a broad array of social and natural scientists, the cross-scale synthesis of multidisciplinary observations, models and theories on coupled human and natural systems (CHANS) that are required to advance LCS has yet to emerge. A major obstacle is the tremendous challenge in global integration and synthesis of local and regional CHANS case studies. This project will accelerate the emergence of new global workflows in land change science through GLOBE: an online collaboration environment combining quantitative real-time global relevance assessment, geovisualization, social-computational structures and machine learning algorithms. This will be accomplished in collaboration with international LCS institutions and experts, enabling researchers and institutions to rapidly share, compare, and synthesize local and regional studies by combining these with global datasets for human and environmental variables using a combination of machine learning, advanced visualization, semantic analysis and social networking. <br/><br/>The project has four core objectives that will be achieved through three integrated activities, as follows: <br/><br/>Objective 1: Create an online collaboration environment leveraging real-time global relevance analysis, geovisualization and social-computational knowledge generation towards the generation and sharing of new global workflows for land change science.<br/>Objective 2: Understand how to build effective social media tools organized around structured and informal scientific workflows.<br/>Objective 3: Develop evaluation methods and metrics and use them to demonstrate the utility of workflow-based social media tools in the context of scientists testing LCS hypotheses.<br/>Objective 4: Leverage GLOBE to characterize and optimize global knowledge generation in LCS.<br/><br/>To achieve these goals, this team will engage in the following activities:<br/><br/>Activity 1: Develop the social-computational infrastructure for GLOBE.<br/>Activity 2: Establish GLOBE as a means for social-computational knowledge generation. Characterize, share and optimize knowledge generation workflows for global synthesis and collaboration across CHANS studies and data collections.<br/>Activity 3: Test hypotheses and identify new research opportunities.<br/><br/>To understand anthropogenic global changes in the Earth system, scientists must generalize globally from observations made locally and regionally. This project will make fundamental hypotheses on the nature of human interactions with earth systems more readily testable by scientific methods, enabling major advances in land-change science and theory. Moreover, this project will engage the computing and social sciences in developing interactive online tools for scientific collaboration and data synthesis that will help identify knowledge gaps in LCS science. The tools will result in new ways of visualizing, communicating, connecting, comparing and synthesizing observations and models of land change processes at global, regional and local scales. Empirical investigation of GLOBE in use will advance our understanding of scientific collaboration more generally.<br/><br/>Broader impacts<br/>This project will develop, enhance and support long-term research collaborations across a broad set of scientific disciplines. It will support education and skill building for interdisciplinary collaboration by seasoned faculty, postdoctoral researchers, graduate students and undergraduate students. The project will design, host and disseminate advanced tools for cross-scale data and knowledge sharing, synthesis, and design of globally representative observing systems. By creating a new environment for sharing and integrating local knowledge, data and ideas across the social, biological and geophysical sciences, land change science will have greater potential to inform the sustainable stewardship of earth systems."
"1100008","Harmonic Analysis, Geometric Measure Theory and Applications","DMS","ANALYSIS PROGRAM","06/01/2011","04/01/2011","Raanan Schul","NY","SUNY at Stony Brook","Standard Grant","Bruce P. Palka","05/31/2015","$135,000.00","","schul@math.sunysb.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","MPS","1281","","$0.00","Mathematically speaking, the project deals with Geometric Measure Theory (GMT) and Harmonic Analysis. More specifically, we study questions on the interface of both these fields, and use harmonic analysis techniques to study questions in GMT. We study problems of the form ""Find a biLipschitz map from a significant part of a given metric space to Euclidean space"", ""Characterize biLipschitz images of the Euclidean plane"" or ""When is a metric space built from biLipschitz images of standard pieces and how do we find these pieces?"" On the harmonic analysis side, this is related to questions of the form ""When is a function decomposable into a sum of nice functions, and how do you construct this decomposition?"" This last question is a standard one in Littlewood-Paley and wavelet analysis, and transferring the methods from these rich theories into the setting of GMT yields a significant toolbox. This type of study of GMT is very related to many questions that arise in applications. In many applications one is given a large data set represented as a subset of a metric space, such as a high dimensional Euclidean space, and one seeks to faithfully represent a large portion of this data set as a subset of a low dimensional Euclidean space. Faithfully here, means that one can still perform the same data mining tasks on the image of the data portion. It is because of this connection to data mining that the above task has thus far yielded much attention from computer scientists and applied mathematicians using a wide range of approaches. The framework of dimensionality reduction also includes data compression and data approximation. These have applications in many areas of science; for examples document analysis, face recognition, clustering, machine learning nonlinear image denoising, segmentation and processing.<br/><br/>The project is geared towards a better understanding of the geometry of collections of points in a given space. In many applications one is given data which we think of as points in a metric space, and we want to map them to a low dimensional space which we understand better (such as a low dimensional Euclidean space). This data-mining task is typically called dimensionality reduction, and this framework includes data compression and data approximation. These have applications in many areas of science; for examples document analysis, face recognition, clustering, machine learning nonlinear image denoising, segmentation and processing. A key point is that quite often the data enjoys nice geometric properties and has more structure then a random set of points would have. One can study these geometric structures and use them to perform data mining tasks. There is a rich mathematical theory behind the study of such geometric structures, and this is what we develop."
"1101283","ICES: Small: Collaborative Research: Algorithms and Mechanisms for Pricing, Influencing Dynamics, and Economic Optimization","CCF","Inter Com Sci Econ Soc S (ICE)","09/01/2011","11/09/2015","Maria-Florina Balcan","GA","Georgia Tech Research Corporation","Standard Grant","Tracy J. Kimbrel","08/31/2016","$185,320.00","","ninamf@cs.cmu.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8052","7923","$0.00","The intersection of Computer Science and Economics has become increasingly important to the development of both fields. Today's software often must handle multiple individuals with their own interests in mind, bringing incentive issues to the forefront in algorithm design. Economic problems, especially in electronic commerce, increasingly involve large numbers of goods and buyers as well as unknown and complex market conditions, making algorithms and machine learning of key importance. This project aims to address fundamental questions at the heart of the intersection of these two fields. These include problems of modeling and influencing behavior in systems with large numbers of agents and components, problems of optimization under complex and changing preferences and constraints in electronic commerce, and problems of efficiently computing and estimating basic economic quantities.<br/><br/>This project specifically has three main thrusts. The first is development of algorithms and analysis techniques for positively influencing dynamics in systems with large numbers of interacting agents. For example, if behavior is currently at a poor-quality equilibrium, when can additional information or a few targeted incentives be used to ""nudge"" behavior towards a good equilibrium? This applies not only to self-interested agents but also to components in a distributed system acting on local information (such as sensors in a sensor network). The second thrust is development of algorithms for efficiently computing or estimating important economic quantities. This includes approximately computing Nash equilibria in large interactions, and learning submodular functions and other common valuation classes from observations of behavior or experimentation. The third thrust is developing mathematical frameworks for understanding and solving problems of pricing and resource allocation in settings with unknown and changing market conditions. These frameworks are crucial for next-generation markets for resources such as computing power and network bandwidth."
"1065154","HCC: Medium: Bringing Brain-Computer Interfaces into Mainstream HCI","IIS","HCC-Human-Centered Computing","06/01/2011","04/14/2013","Robert Jacob","MA","Tufts University","Continuing Grant","Ephraim Glinert","05/31/2015","$935,524.00","Sergio Fantini, Matthias Scheutz","jacob@cs.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7367","7367, 7924","$0.00","Brain-computer interfaces (BCI) have made dramatic progress in recent years.  Their main application to date has been for the physically disabled population, where they typically serve as the sole input means.  Recent results on the real-time measurement and machine learning classification of functional near infrared spectroscopy (fNIRS) brain data lead to this project, in which the PI and his team will develop and evaluate brain measurement technology as input to adaptable user interfaces for the larger population.  In this case, brain input is used as a way to obtain more information about the user and their context in an effortless and direct way from their brain activity, which is then used to adapt the user interface in real time.  To accomplish this a multi-modal dual task interface between humans and robots will be introduced, which will serve as a particularly sensitive testbed for evaluating the efficacy of these new interfaces.<br/><br/>The project will create and study these new user interfaces in domains where the effect on task performance of introducing the brain input to the interface can be measured objectively.  They are most useful in demanding, high-performance, multitasking situations.  Carefully calibrated multitasking applications scenarios from the team's research in Human-Robot Interaction will be employed. <br/><br/>The project will also advance the range of fNIRS brain measurements that can be applied to user interfaces. It will study a recently identified fNIRS signal obtained from the phase relationships among different regions of the scalp at low frequencies (0.1 Hz), as well as a wider range of sensor placement locations than previously examined.  As these are developed into usable measurements for real-time signals with machine learning and other analysis approaches, they will be incorporated into new user interfaces.<br/><br/>Broader Impacts:  The target of the research is adaptive interfaces for non-disabled users, where brain measurement is an additional source of user input.  However, as the work proceeds toward making this into a more robust technology project outcomes will have promise for physically-challenged users, and ultimately they promise to improve the lives of people with severe motor disabilities."
"1143921","EAGER:  Automatic Document and Record Disposition and Retention","IIS","Info Integration & Informatics","08/01/2011","07/23/2011","C. Lee Giles","PA","Pennsylvania State Univ University Park","Standard Grant","Maria Zemankova","07/31/2015","$200,000.00","","giles@ist.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7364","7364, 7916","$0.00","Record and document retention (document disposition) has become a serious problem for both organizations and individuals since most documents now created are digital. Digital documents offer both problems and advantages. Digital documents are easily versioned, copied and disseminated. Thus, there can be several similar copies or versions of important or relevant documents in many locations. Document or record disposition can be applied to or is needed by individuals, organizations and domains (such as law, science, policy, etc.) for effective information management over long periods of time. This problem is of epic proportions and is becoming a major problem in organizations and for individuals throughout the world where effective record disposition is either required by law or by the organization or by practical limitations in systems.<br/><br/>This exploratory project investigates possible automatic document disposition methods based on algorithms for text inspection, mining, and search. The challenges lie in finding scalable, adaptable algorithms that can be used in several if not all application domains. In addition, variability in users presents many problems. A disposition method or procedure may vary depending on the user, organization and domain (e.g., law, health records, etc.). The approach explored in this project applies and extends machine learning methods to these problems since these methods adapt to variability in data, areas and domains. Using such approaches, automated disposition methods can be readily applied to these different areas such as science, email and legal records. This research lays the groundwork for adaptive methods for a variety of domains in terms of applicability, performance and scalability. this proof-of-concept project initially focuses on the Enron email data set that is publicly available and is be used to demonstrate the feasibility of the approach since email can be considered a special case of document disposition. If successful, other disposition domains such as science and government data will be explored. This work will show the viability of developing and applying machine learning methods to an important and diverse problem domain.  <br/><br/>The results from this exploratory project together with insights gathered from methods used in large scale document search are expected to yield understanding as to how we can better manage our digital past and the rapidly expanding digital future. The results are expected to introduce this important problem to other researchers and document disposition professionals and lead to collaborations with industry. Data and research results will be made available through a publicly available website (http://clgiles.ist.psu.edu/disposeseer/) and research papers will be published and presented in appropriate venues.  The project provides research experience for graduate and undergraduate students."
"1130109","ICML 2011 Proposal for Student Poster Program and Travel Scholarships","IIS","Info Integration & Informatics, Robust Intelligence","04/01/2011","03/31/2011","Hal Daume","MD","University of Maryland College Park","Standard Grant","Jie Yang","03/31/2012","$20,000.00","","hal@umiacs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7364, 7495","7364, 7495","$0.00","The project supports graduate student participation in the 28th International Conference on Machine Learning (ICML 2011). Specifically, the project supports travel to the conference for those who might not otherwise be able to attend for financial reasons, and organizes a student poster-presentation program that will facilitate one-on-one discussions and other mentoring with the world's leading researchers in machine learning. Students are exposed to state-of-the-art work by other researchers and have the opportunity to attend tutorials on material that is not taught at their home institutions. Participating students receive feedback from senior researchers beyond their institutional and national boundaries. Furthermore, participation in the poster session and conference helps to integrate these students into the research community and represents a natural integration of research and education."
"1126267","IPGA: Characterization, Modeling, Prediction, and Visualization of the Plant Transcriptome","IOS","Plant Genome Research Resource","09/01/2011","08/16/2011","Volker Brendel","IA","Iowa State University","Standard Grant","Diane Okamuro","03/31/2012","$1,499,996.00","Shailesh Lal, Karin Dorman, Shannon Schlueter","vbrendel@indiana.edu","1138 Pearson","AMES","IA","500112207","5152945225","BIO","7577","1228, 1329, 9109, 9150, 9178, 9179, BIOT","$0.00","PI: Volker P. Brendel (Iowa State University) <br/><br/>CoPIs: Karin Dorman (Iowa State University), Shannon Schlueter (University of North Carolina - Charlotte) and Shailesh Lal (Oakland University) <br/><br/>Senior Personnel: Jon Duvick and Yasser El-Manzalawy (Iowa State University) <br/><br/>The premise of this project is that the scale of sequence and other data accumulation in plant genomics necessitates the development of novel, highly automated, scalable, comprehensive, and accurate approaches to genome annotation. The depth of transcript data accumulating for many plant species under numerous experimental conditions provide unprecedented evidence for the evaluation of all aspects of transcription, including precise mapping of transcription start sites as well as dominant and alternative splice sites. This project engages a team of experts in a wide range of fields, including genomics, molecular biology, bioinformatics, statistics, machine learning, high performance computing, and software engineering to jointly work toward a solution for accurately predicting the expressed protein-coding gene transcriptome from plant genome sequences. Successful completion of the project will result in the deployment of (1) software that implements the novel prediction algorithms, (2) visualization and data access portals, and (3) a cyberinfrastructure environment implementation of the developed tools for distributed computing, sharing of protocols, and analysis provenance recording. In the long run, the project seeks to explore the extent to which genomic biology can transition from a largely descriptive to a highly predictive science driven by quantitative measurements, with algorithms and computation as the domain-adapted language. <br/><br/>The project will generate standardized, accurate protein-coding gene structure annotation for 25 plant genomes from a wide range of the phylogenetic spectrum. Initial emphasis will be on improved annotation of recently sequenced genomes, which will benefit the entire community of researchers working on these important crops. The anticipated algorithms for transcriptome prediction will be essential to the analysis of the thousands of complete plant genome sequences likely to become available within the next few years. Through the development of reliable gold standard annotations and the dissemination of training and test sets for algorithmic development, a larger community of computational data analysts, in particular from the machine learning community, will be engaged. All software developed and data generated in this research is freely available through project Web sites, in particular www.plantgdb.org. The project's plan for integration of research and education will train a new generation of scientists to work on genomics data with the broad range of interdisciplinary approaches represented by the project team."
"1117012","CIF: Small: Collaborative Research: Compressed Sensing for Coherent Designs under Gaussian/Non-Gaussian Noise","CCF","COMM & INFORMATION FOUNDATIONS, SIGNAL PROCESSING","07/01/2011","06/05/2013","Dapeng Wu","FL","University of Florida","Standard Grant","John Cozzens","06/30/2015","$237,225.00","","wu@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7797, 7936","7923, 7936, 9251","$0.00","The recent explosion of large amounts of high dimensional data in science, engineering, and society demands new technologies to recover sparse signals from high dimensional noisy observations. To address it, this project aims to develop efficient and robust methods for analyzing high-dimensional data, which have wide applications in signal processing, communication, computational biology, machine learning, image/video coding, sensor networks, social science, etc.<br/><br/>Recently, compressed sensing (CS) has attracted a good deal of attention from computer science, engineering, and statistics communities. However, the CS recovery only makes sense when the features are weakly correlated. A single rogue outlier may break down the reconstruction completely. The computational procedures cannot meet the challenge of ultrahigh dimensional problems in terms of statistical accuracy, algorithmic stability and computation expediency. To address these challenges, the investigators develop novel nonconvex regularization techniques to attain prediction accuracy and model parsimony for coherent statistical models that go much beyond Gaussianity. Theoretical analysis of its performance in estimation, prediction, and sparsity recovery is conducted. A class of simple algorithms feasible for solving essentially any nonconvex penalized generalized linear models is developed, together with a randomization technique of nonmarginal feature screening for ultra-high dimensional data. Furthermore, the investigators explicitly study the critical effects of outliers and develop a robust CS for handling high leverage points and gross outliers. A unified framework that applies to small-sample-size-high-dimension problems is provided for simultaneous variable selection and outlier identification under Gaussian/non-Gaussian noise. Finally, this project involves rich motivating examples and widespread applications in various areas including spectral analysis, network topology and dynamics modeling, graphical models, computational biology, machine learning, and image compression, as an essential component."
"1116656","RI: Small: Intelligent Autonomous Video Quality Agents","IIS","ROBUST INTELLIGENCE, COMM & INFORMATION FOUNDATIONS","09/01/2011","08/29/2011","Alan Bovik","TX","University of Texas at Austin","Standard Grant","Jie Yang","08/31/2015","$499,944.00","Joydeep Ghosh","bovik@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495, 7797","7923, 7936","$0.00","Determining the perceptual quality of video transmitted through complex networks and viewed on heterogeneous platforms, from cell phones to Internet-based television, is a key problem for the YouTube generation. It is also central to a variety of vision applications including face detection, face recognition and surveillance. Video is subject to numerous distortions: blur, noise, compression, packet/frame drops, etc. Quality assessment is non-trivial when an undistorted video is not available, and unsolved for multiple distortion types and in distributed, non-stationary viewing environments.<br/><br/>This project designs and creates intelligent video ""quality agents"" that learn how to determine perceptual video quality in heterogeneous networks, and assesses its impact on decision tasks such as face detection and recognition, all without the benefit of reference videos. It uses statistical properties of natural scenes, perceptual principles, machine learning, and intelligent adaptive agent collectives to handle videos simultaneously impaired by multiple distortion types. A primary application is novel face-salient quality assessment agents and quality-aware face detection algorithms. Multiple, co-operative video and face quality agents are trained using active learning based feedback mechanisms on mobile devices. This project yields adaptive, robust video Quality of Service assessment in real-life networks and provides new insights into human visual quality perception and visual distortion detection. The research team also creates two large, unique video quality databases: (a) A Mobile Video Quality Database of raw and distorted mobile videos and (b) A Distorted Face Database of undistorted and distorted face images, as gold standards for research and development in this area."
"1117153","III: Small: Network Learning for Integrative Cancer Genomics","IIS","Info Integration & Informatics","09/01/2011","06/18/2013","Rui Kuang","MN","University of Minnesota-Twin Cities","Standard Grant","Sylvia Spengler","08/31/2015","$500,000.00","","kuan0009@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7364","7923","$0.00","New large-scale DNA sequencing and array technologies now provide a promising way to study the molecular mechanisms of cancer by generating enormous information measuring aberrations in cancer genome. The genomic information can potentially guide drug design on targeted molecules, and improve clinical decisions in cancer treatment. One of the main obstacles to further progress is to elucidate multiple complex molecular indicators of cancers from the enormous genomic data. This proposal tackles the problem with network-based machine-learning theoretical frameworks and methods that can model the underlying biological mechanisms for an integrative study of cancer genomic information and relevant biomedical knowledge. As a proof of concept, the developed methods will be applied to study chemoresistance in ovarian cancer treatment. <br/><br/>This proposal aims at creating a general computation-driven approach for guiding cancer genomics research and improving genomics-based clinical decisions in cancer treatment. The research activities described in the proposal will deliver a collection of effective and efficient computational tools to utilize heterogeneous genomic data combined with biomedical knowledge for clinical practices. The study of the ovarian cancer data will help reveal the crucial pathways driving chemoresistance, and provide useful prediction tools and drug targets for ovarian cancer treatment. This proposal will also integrate the latest research development in computational cancer genomics into new courses in several training programs to prepare students for their future professions to meet the need of workforce in the growing biomedical and health informatics industry in the upper midwest region. The education plan will also have a focus on recruiting students in minority and under-represented groups in computer science and information technology.<br/><br/>To achieve the goals, the components of the research plan are 1) to formulate graph kernels and subgraph mining algorithms that can integrate various types of cancer genome aberrations to improve cancer outcome predictions and to discover cancer-causative genome aberration patterns; 2) to formulate semi-supervised matrix factorization methods with Laplacian constraints for predicting novel cancer phenotype and gene associations for identifying potential drug targets, utilizing known relations in phenotype, gene and their association networks; 3) to study the chemoresistance in ovarian cancer treatment to reveal the crucial pathways driving the resistance, and develop useful prediction tools and drug targets for ovarian cancer treatment; 4) to release the developed methods in both software packages and webtools for public use in academia. The two major components of the education plan are: 1) to offer a two-week course, titled Cure Cancer with Computers, in the summer academy of the BioSMART program for Minnesota high school students, and 2) to create a new course Computational Genomics in Biomedical Informatics to support two graduate programs for training students in biomedical/health informatics with knowledge in genomics and computer science."
"1143635","RI: EAGER: Exploratory Research on Acquiring and Adapting Sentence Planning Resources for Generating with Discourse Combinatory Categorial Grammar","IIS","ROBUST INTELLIGENCE","09/01/2011","07/14/2011","Michael White","OH","Ohio State University","Standard Grant","Tatiana D. Korelsky","08/31/2014","$149,937.00","","mwhite@ling.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 7916","$0.00","Natural Language Generation (NLG) systems aim to improve the accessibility and impact of information by turning data into coherent and fluent text or speech, automatically.  Developing high-quality NLG systems, however, remains a difficult and costly undertaking, in large part because bridging the gap between content planning and surface realization---a task known as \textit{sentence planning}---continues to require extensive knowledge engineering.<br/><br/>This Early Grant for Exploratory Research investigates ways of bridging this gap by employing machine learning together with Discourse Combinatory Categorial Grammar (DCCG).  Using a restaurant recommendation application as a proof-of-concept, the project explores methods of (1) adapting previous work on acquiring lexicalized grammar entries for semantic parsing to learn mappings from domain-general semantic dependency representations to application-specific representations of messages; (2) extending the approach to learn rules for combining messages; (3) employing the acquired resources to map content plans to disjunctive logical forms (DLFs), which compactly specify the range of possible realizations of the selected content; and (4) improving the efficiency of realizing DLFs with OpenCCG through grammar specialization.<br/><br/>The project will evaluate the success of these novel methods and assess the portability of the approach.  By demonstrating methods for radically simplifying the construction of NLG systems, the project promises to transform the way NLG systems are built, from today's knowledge-intensive approach to one that relies primarily on assembling a parallel corpus of input-output pairs.  Ultimately, it will facilitate the development of generation components in data-to-text systems as well as dialogue systems, including ones for the visually impaired."
"1116115","CIF: Small: The Informational Limit of Eigen-Analysis Based Dimensionality Reduction, Learning and Classification","CCF","Comm & Information Foundations","08/01/2011","07/16/2011","Rajesh Nadakuditi","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","John Cozzens","07/31/2015","$278,353.00","","rajnrao@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7797","7923, 7936","$0.00","We are confronted with richer, more detailed and more forms of data than ever before. Eigen-analysis based techniques constitute a powerful class of algorithms for discovering statistical signatures in mountains of data. Such techniques are used widely for the detection, estimation and classification of weak signals in a variety of applications that span the breadth of science and engineering such as gene microarray analysis, and climate science among others. This research will uncover the fundamental limits of these techniques and develop new techniques that can tease out weaker signals from large, noisy datasets.  The results of this research will be disseminated broadly to advance relevant technology and clarify both advantages and limitations of eigen-analysis based dimensionality reduction.<br/><br/>Eigen-analysis based dimensionality reduction techniques are ubiquitous in statistical signal processing and machine learning applications. Their popularity is due to a sound theoretical justification, near-optimal computational complexity and strong performance guarantees in regimes where the techniques are known to work well. This research will provide a deeper understanding of when eigen-analysis based dimensionality reduction techniques fail so that these limitations can be potentially overcome. The project focuses on high-dimensional, noisy settings and uses random matrix theory as an enabling mathematical tool in this endeavor. The informational limits developed will provide a principled way of comparing eigen-analysis based algorithms with other techniques, and allow for an objective quantification of any performance losses due to fast implementations, measured in terms of how weak the signal or statistical signature is that can be identified. The research is interdisciplinary - powerful, new tools from random matrix theory are used to prove theorems and establish sharp, asymptotic performance bounds."
"1064628","SHB: Medium: Collaborative Research: Crafting a Human-Centric Environment to Support Human Health Needs","IIS","Smart and Connected Health","09/01/2011","08/27/2011","Diane Cook","WA","Washington State University","Standard Grant","Sylvia Spengler","08/31/2016","$703,636.00","","cook@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","8018","7924, 8018","$0.00","Researchers and providers alike are recognizing that human-centric smart environments can provide health monitoring services and support aging in place through adaptive interventions. The need for the development of such technologies is underscored by the aging of the population, the cost of formal health care, and the importance that individuals place on remaining independent in their own homes. The goal of this project is to design, implement, and evaluate in-home techniques for generating reports of activities and social interactions that are useful for monitoring well being and for automating intervention strategies for persons with dementia.  The plan is to design machine learning techniques that make effective use of sensor data to perform automated activity monitoring and prompting-based interventions that are beneficial for the residents as well as for their caregivers and family. The environment is human-centric because it learns information about its human residents and uses this information to provide activity-aware monitoring and intervention services. By transforming everyday environments into smart environments, many older adults with cognitive and physical impairment can lead independent lives in their own homes. A key component of this project is an evaluation of the technologies in actual homes with volunteer older adults and thus will assess the technologies for acceptance with the target population. <br/><br/>This project addresses NSF?s Smart Health and Wellbeing goal of leveraging computational expertise leading to fundamental advances in the development of algorithms to create improvements in safe, effective, and patient-centered health and wellness services. The development of a Gerontechnology class is focused on training  students to design and use these technologies. This effort includes REU and IGERT students in the research project, which involves students from underrepresented groups in this multidisciplinary, collaborative effort.  To facility community-wide use, comparison and collaboration, all of our datasets, tools, and course materials will be disseminated from our project web page."
"1144258","EAGER: A Multi-User Communication and Information Theoretic Approach to the Sparse Signal Recovery Problem","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2011","07/21/2011","Bhaskar Rao","CA","University of California-San Diego","Standard Grant","John Cozzens","03/31/2014","$297,230.00","Young-Han Kim","brao@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7916, 7936","$0.00","This research project examines the theoretical, algorithmic, and computational issues that arise in compressed sensing (CS) and signal processing problems where there is a need to compute solutions to problems in which the solution vector has many zeros. In addition to the exciting compressed sensing area, this research will benefit numerous signal processing applications where the sparsity constraint on the solution vector naturally arises. Brain imaging techniques such as Magnetoencephalography (MEG) and Electroencephalography (EEG) are currently important examples. Sparse communication channels with large delay spread, high resolution spectral analysis, and direction of arrival estimation, are other important examples. An effective solution to this problem will have significant impact, by providing new and valuable tools to the practicing signal processing engineer. In addition, the tools will be of interest to researchers in cognitive science, neuroscience, and machine learning where sparsity issues naturally arise, such as sparse coding of signals in the brain or learning from data which is often assumed to lie on a low dimensional manifold. <br/><br/>This project provides a comprehensive and tighter integration of the compressed sensing field and multi-user information theory. This makes it possible to utilize the rich results available in network information theory which have been successfully applied to the implementation of communication systems. The theoretical tools necessary to enable this integration are being developed by the investigators. This research enables significant advances in both theory and practice in the CS field. The information theoretic insights are leveraged to provide insights on performance limits and guidance on practical CS-based system design. The implementation experience gained from communication systems will be translated to practical algorithm development and efficient CS-based system design."
"1117127","SHB: Small: Computing Robot Motions for Home Healthcare Assistance","IIS","Smart and Connected Health","09/01/2011","08/30/2011","Ron Alterovitz","NC","University of North Carolina at Chapel Hill","Standard Grant","Sylvia J. Spengler","08/31/2015","$350,000.00","Dinesh Manocha","ron@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8018","7923, 8018","$0.00","Millions of Americans are unable to independently perform activities of daily living (ADLs) such as dressing and grooming, and this number is rising rapidly as America?s aging and disabled population increases. This research focuses on designing and developing new algorithms and software systems that would help enable personal robots to autonomously compute safe motions to assist disabled and elderly individuals with ADLs. The proposed framework uses kinesthetic demonstrations to teach the robot desirable motion trajectories to accomplish several specific ADL assistance tasks. Based on these demonstrations, the research focuses on developing new computational methods to extract task constraints for desirable motion trajectories using learning methods based on Gaussian mixture models in conjunction with machine learning and 3D registration methods. A key element of this project  involves investigation of methods to deformably register and generalize the motion trajectories and task constraints across individuals of different shapes and sizes. In order to generate safe plans in dynamic real-world settings, the proposed research investigates new highly parallel algorithms that effectively utilize the power of modern general purpose graphics processing units (GPUs) for real-time planning in uncertain environments. The framework is evaluated using articulated mannequin testbeds.<br/><br/>This project brings together an interdisciplinary team with computer science, robotics, and occupational therapy expertise. The project integrates research with education through community outreach activities. In the long term, the methods developed in the proposed research could have broad societal benefits by helping enable personal robots to assist disabled and elderly individuals with ADLs, allowing them to safely stay in their homes rather than moving to costly institutions."
"1106586","Dimension Reduction for Non-Regular Statistical Models with Applications","DMS","STATISTICS","08/15/2011","07/29/2011","Chunming Zhang","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","07/31/2014","$100,000.00","","cmzhang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","The proposal aims to develop new statistical theory and methodology on dimension reduction for high-dimensional non-regular models which allow for discontinuity with respect to a subset of the parameters or covariates. Such models arise naturally from applications in various fields, such as statistics, biostatistics, climate, marketing research, management, economics and finance. They can capture many important features of the data structure and association between the explanatory and response variables which either low-dimensional or regular models alone cannot duplicate. This proposal focuses primarily on threshold models, an important class of non-regular models which has a wide variety of applications in statistics, biostatistics, and economics. While the literature on threshold models for low-dimensional data is comprehensive, the statistical theory and methods for threshold models applied to high-dimensional data are undeveloped due to four central challenges: (I) statistical nonregularities of the estimation, (II) increasing dimensionality, (III) unknown or incomplete distributions of response variables, (IV) computational difficulties. By introducing penalization techniques, a number of related research topics are proposed for investigation. New tools for statistical inference and computational algorithms of non-regular models applied to large and high-dimensional data, for example the brain imaging data, will be developed.<br/><br/>These new developments will allow scientists to efficiently analyze data with substantially increased flexibility, interpretability and reduced modeling biases. In addition, the investigator will integrate new mathematical, probabilistic and computational tools with those in sciences and engineering. Dissemination of these developments will enhance new knowledge discoveries, and strengthen interdisciplinary collaborations. The research will also serve an educational purpose through multi-disciplinary courses on the contemporary state-of-the-art data mining and machine learning, and benefit the training and learning of undergraduate, graduate students and underrepresented minorities."
"1064641","IRES: U.S.-India Summer Research Experience in Nondestructive Evaluation Technologies for Michigan State University Students at the Indian Institute of Technology-Madras, Chennai.","OISE","IRES Track I: IRES Sites (IS)","05/01/2011","02/02/2016","Lalita Udpa","MI","Michigan State University","Standard Grant","Cassandra Dudka","04/30/2017","$145,856.00","Satish Udpa, Robert McGough","udpal@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","O/D","7727","5919, 5976, 7639","$0.00","OISE-1064641<br/>Lalita Udpa  <br/><br/>This International Research Experience for Students (IRES) award to US PI Lalita Udpa, Michigan State University (MSU) is for international research collaborations in electrical, computer, and mechanical engineering with Professors Kavitha Arunachalam and Krishnan Balasubramaniam, Indian Institute of Technology-Madras (IITM), Chennai, India.  The research focuses on the assessment of infrastructure integrity or Structural Health Monitoring (SHM) using cutting-edge technologies in Non-Destructive Evaluation (NDE).  The IITM houses an NDE Center with state-of-the art facilities for research using advanced ultrasonic, electro-magnetic, radiography, thermal and optical tools that will be available to MSU graduate and undergraduate students to conduct hypotheses-driven, multidisciplinary research in NDE.<br/><br/>NDE techniques are used to inspect and assess the structural integrity of a variety of systems such as nuclear power plants, natural gas pipelines, aircraft, bridges, dams and other segments of the civilian and defense infrastructure.  As many of these are known to have far exceeded their designed lifetime, there is increasing demand for reliable and accurate assessments of their structural integrity.  Monitoring is accomplished using a variety of modalities that rely on different energy sources such as electromagnetic, ultrasound, thermal, and x-ray.  The underlying physics of each inspection technology is diverse as are the methods for analyzing the data.   Consequently, NDE provides a rich area for student research that will focus on novel developments in computational modeling, signal processing, machine learning algorithms for solving the inverse problem, and development of novel NDE sensors and systems.  Projects in multiple NDE modalities of mutual interest to MSU and IITM will be selected with the goal of developing new technologies. <br/><br/>This award offers opportunities for US students to work in an international research environment in collaboration with leading Indian scientists.  They will also have access to the research laboratories of General Electric and General Motors in Bangalore, where they will learn about   global operations of US companies and the advantages of international collaboration.  This project will attract high caliber domestic students who will be encouraged to do graduate studies and doctoral degrees.  Special outreach will be made to recruit minority and underrepresented students thru MSU and national programs.  A number of student theses and dissertations are anticipated from these projects plus numerous journal articles and joint presentations at scientific meetings and conferences.  This experience will expand students? academic horizons and encourage engagement in global collaboration."
"1064460","SHB: Medium: Collaborative Research: Crafting a Human-Centric Environment to Support Human Health Needs","IIS","Smart and Connected Health","09/01/2011","03/09/2012","Mario Di Francesco","TX","University of Texas at Arlington","Standard Grant","Sylvia J. Spengler","12/31/2013","$300,453.00","","mariodf@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","8018","7924, 8018","$0.00","Researchers and providers alike are recognizing that human-centric smart environments can provide health monitoring services and support aging in place through adaptive interventions. The need for the development of such technologies is underscored by the aging of the population, the cost of formal health care, and the importance that individuals place on remaining independent in their own homes. The goal of this project is to design, implement, and evaluate in-home techniques for generating reports of activities and social interactions that are useful for monitoring well being and for automating intervention strategies for persons with dementia.  The plan is to design machine learning techniques that make effective use of sensor data to perform automated activity monitoring and prompting-based interventions that are beneficial for the residents as well as for their caregivers and family. The environment is human-centric because it learns information about its human residents and uses this information to provide activity-aware monitoring and intervention services. By transforming everyday environments into smart environments, many older adults with cognitive and physical impairment can lead independent lives in their own homes. A key component of this project is an evaluation of the technologies in actual homes with volunteer older adults and thus will assess the technologies for acceptance with the target population. <br/><br/>This project addresses NSF?s Smart Health and Wellbeing goal of leveraging computational expertise leading to fundamental advances in the development of algorithms to create improvements in safe, effective, and patient-centered health and wellness services. Through design of a Gerontechnology class we are training  students to design and use these technologies. This effort includes REU and IGERT students in the research project, which involves students from underrepresented groups in this multidisciplinary, collaborative effort.  To facility community-wide use, comparison and collaboration, all of our datasets, tools, and course materials will be disseminated from our project web page."
"1126008","MRI: Acquisition of an Acoustical Measurement System for Structural Health Monitoring Research and Teaching at the University of Maryland Eastern Shore","CMMI","MAJOR RESEARCH INSTRUMENTATION","09/01/2011","08/18/2011","Yuanwei Jin","MD","University of Maryland Eastern Shore","Standard Grant","Joanne D. Culbertson","12/31/2014","$94,182.00","Payam Matin","yjin@umes.edu","Backbone Road","Princess Anne","MD","218531295","4106516714","ENG","1189","039E, 1189","$0.00","This Major Research Instrumentation (MRI) award provides funding for the acquisition of an acoustical measurement system. The acquisition of the measurement system enables a new program for structural health monitoring for environmental changes on Maryland's Eastern Shore. It facilitates research of structural health monitoring using guided acoustic waves that can propagate a long distance in solid materials such as bars, plates and pipes. Specific research objectives include: (1) investigation of a time reversal Lamb wave communication system using steel pipes or bars as communication channels; (2) research of new methods for assessing material stress behaviors based on the Hopkinson' bar testing using strain sensors; and (3) development of novel approaches to extracting baseline free and baseline dependent defect features using integrated signal processing and machine learning techniques. The projects that will be supported by this equipment will enable understanding of the characteristics of the local civil structures and new design of SHM systems specific to the Eastern Shore's coastal environment. <br/><br/>If successful, the acquisition of this equipment will allow the University of Maryland Eastern Shore (UMES) to facilitate a new program to address the significant aging infrastructure problem facing the Eastern Shore area, but more widely the nation. It will enhance the quality of student research and instruction and provide faculty with the necessary tools to pursue aspects of their research which are presently not available to them. Each year, two undergraduate students will be intensely trained to operate this instrument. In addition, laboratory projects utilizing this equipment will be incorporated into several upper level undergraduate courses and senior design capstone projects affecting a minimum of thirty students per year. This instrument will also enhance collaboration with Salisbury University and the Northrop Grumman Technical Service facility, also located on Maryland's Eastern Shore."
"1149465","TC: Small: THWART: Trojan Hardware in Wireless ICs - Analysis and Remedies for Trust","CNS","SPECIAL PROJECTS - CISE, TRUSTWORTHY COMPUTING","07/01/2011","04/22/2014","Yiorgos Makris","TX","University of Texas at Dallas","Standard Grant","Nina Amla","08/31/2014","$455,809.00","","yiorgos.makris@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1714, 7795","7923, 9178, 9251","$0.00","Towards enhancing trustworthiness of wireless integrated circuits, this project investigates the problem of hardware Trojans in the analog/RF domain. Hardware Trojans are maliciously-intended modifications to fabricated integrated circuits, making them capable of additional functionality which is unknown to the designer and user, but which can be exploited by the perpetrator after chip deployment to sabotage or incapacitate it, or to steal sensitive information. The motivation for this research is two-fold: First, partly because of design outsourcing and migration of fabrication to low-cost areas across the globe, and partly because of increased reliance on external intellectual property and design automation software, the integrated circuit supply chain is now considered far more vulnerable to such malicious modifications than ever before. Second, wireless integrated circuits constitute an indispensable part of modern electronic systems and their ability to communicate data (possibly encrypted) over public channels makes them a prime attack candidate. To address this problem, this project focuses on (i) delineating the threat and potential impact of hardware Trojans in wireless cryptographic ICs, (ii) elucidating the shortcomings of existing test methods in exposing them, (iii) developing preventive countermeasures for obfuscating the chip design and complicating the development of hardware Trojans, and (iv) devising efficient hardware Trojan detection methods based on statistical analysis and machine learning. The anticipated impact of this research lies in the attainment of a better understanding of the hardware Trojan threat and in the development of appropriate remedies, thus enabling secure deployment of wireless integrated circuits and fostering technology trustworthiness."
"1101215","ICES: Small: Collaborative Research:  Algorithms and Mechanisms for Pricing, Influencing Dynamics, and Economic Optimization","CCF","Inter Com Sci Econ Soc S (ICE)","09/01/2011","04/18/2011","Avrim Blum","PA","Carnegie-Mellon University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2014","$199,594.00","","avrim@ttic.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8052","7923","$0.00","The intersection of Computer Science and Economics has become increasingly important to the development of both fields.  Today's software often must handle multiple individuals with their own interests in mind, bringing incentive issues to the forefront in algorithm design.  Economic problems, especially in electronic commerce, increasingly involve large numbers of goods and buyers as well as unknown and complex market conditions, making algorithms and machine learning of key importance.  This project aims to address fundamental questions at the heart of the intersection of these two fields.  These include problems of modeling and influencing behavior in systems with large numbers of agents and components, problems of optimization under complex and changing preferences and constraints in electronic commerce, and problems of efficiently computing and estimating basic economic quantities.<br/><br/>This project specifically has three main thrusts. The first is development of algorithms and analysis techniques for positively influencing dynamics in systems with large numbers of interacting agents.  For example, if behavior is currently at a poor-quality equilibrium, when can additional information or few targeted incentives be used ""nudge"" behavior towards a good equilibrium?  This applies not only to self-interested agents but also to components in a distributed system acting on local information (such as sensors in a sensor network).  The second thrust is development of algorithms for efficiently computing or estimating important economic quantities. This includes approximately computing Nash equilibria in large interactions, and learning submodular functions and other common valuation classes from observations of behavior or experimentation. The third thrust is developing mathematical frameworks for understanding and solving problems of pricing and resource allocation in settings with unknown and changing market conditions.  These frameworks are crucial for next-generation markets of resources such as computing power and network bandwidth."
"1152008","Workshop: A Conversation Between AI and OR on Sequential Decision Making,held at Rutgers University, Spring 2012.","CMMI","OPERATIONS RESEARCH, ROBUST INTELLIGENCE","10/01/2011","08/18/2011","Warren Powell","NJ","Princeton University","Standard Grant","Sheldon Jacobson","12/31/2013","$42,610.00","","powell@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","ENG","5514, 7495","072E, 073E, 077E, 5514, 7495, 7556","$0.00","Sequential decision making in the presence of uncertainty arises in problems that span transportation and logistics, energy, health and military operations.  Real-world instances of these problems are fundamentally intractable, beyond the reach of our most powerful computers.  Parallel research among communities such as operations research and computer science has produced a diversity of algorithmic strategies that offer unique features, but significant language and notational barriers have limited the sharing of ideas.  This workshop will consist of a series of conversations so that leading professionals, post-docs and students from both communities will better learn the languages of both communities.  A major activity will be the design of challenge problems that benefit from the combined skills of both communities.  For example, the problem of optimizing fleets of UAVs is a problem class that would be solved in very different ways by each community.  Computer scientists tend to solve these as swarms of loosely coordinated, independent agents.  Operations researchers have the tools to optimize these fleets using large-scale optimization, reflecting the perspective of a single controller.  <br/><br/>If successful, the workshop will lead to avenues for improved collaboration between the operations research and computer science communities, including a roadmap for future collaboration between the two research communities. Ultimately this should lead to increased capabilities for solving problems that have been intractable up to now by exploiting the unique skills that each community brings, most prominently the computational power of machine learning and mathematical programming."
"1054541","CAREER:The Symbiosis of Graphical Models and Games","IIS","ROBUST INTELLIGENCE","06/01/2011","08/17/2015","Luis Ortiz","NY","SUNY at Stony Brook","Continuing grant","Hector Munoz-Avila","07/31/2016","$407,322.00","","leortiz@umich.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7495","1045, 1187, 7495, 9251","$0.00","Many natural, social and engineered systems exhibit or facilitate complex behavior. Such behavior often results from the deliberate actions of, and interactions between, a large number of individuals. The need to study behavior in complex systems of network-structured interactions in large populations promotes interest in computational game-theoretic models.<br/><br/>Graphical games build on classical models in game theory, as well as compact, structured representations in probabilistic graphical models. The result is a practical and computationally amenable model to handle networked large-population systems.<br/><br/>The creation of technology for scientists and policy makers to study and work with large real-world complex systems of (strategic) interactions requires further advances in graphical games and models. The project seeks to fill knowledge gaps by advancing computational aspects of game theory, graphical models and machine learning, and laying the foundation for a systematic two-way knowledge transfer between computational game theory and graphical models.<br/><br/>The research program strengthens the connection between graphical models and game theory by casting probabilistic inference problems as equilibrium computation, creating algorithms to learn games from behavioral data, and characterizing equilibrium structure and computation.<br/><br/>The educational program includes the infusion of research results into general education, at all levels, via development of new courses and integration into existing ones; and a concerted effort to bridge the Departments of Economics and Computer Science at Stony Brook. Collaborations through the Center for Game Theory in Economics and the International Summer Festival on Game Theory, held annually at Stony Brook, serve as conduits for outreach and dissemination."
"1117707","RI: Small: Algebraic and Spectral Structure of Data in High Dimension","IIS","ROBUST INTELLIGENCE","07/01/2011","04/07/2015","Mikhail Belkin","OH","Ohio State University","Standard Grant","Hector Munoz-Avila","12/31/2015","$450,000.00","","mbelkin@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7923","$0.00","Obtaining information from data is one of the most fundamental problems of modern science and technology. The aim of machine learning is to develop algorithms to automatically extract useful information from complex, high-dimensional data. Making progress toward this aim requires developing an understanding of the aspects of data, which are amenable to analysis and can be learned using computationally efficient methods. In particular, modeling non-linear structures in high-dimensional data has become one of the very challenging and active lines of research, which has seen significant progress over the last ten years.<br/><br/>The goal of this project is to develop and analyze new mathematical representations for data, based on spectral and algebraic methods. We will explore how different structures in the data, such as cluster, manifold or parametric model structures, are reflected in their spectral and algebraic properties and how they can be extracted algorithmically from data, paying particular attention to the issues of high dimensionality and non-linearity. These insights will be used to build better and more adaptive algorithms for inference and data analysis tasks. <br/><br/>We will also analyze experimentally and theoretically properties of these algorithms, when data deviates from the posited model structure. This is a key issue in practical applications, which nearly always involve uncertainty and noise."
"1100735","A Bayesian Approach for Modeling and Simulation of Non-Stationary Ground Motions","CMMI","Structural and Architectural E","09/01/2011","03/09/2011","Jale Tezcan","IL","Southern Illinois University at Carbondale","Standard Grant","Y. Grace Hsuan","08/31/2015","$269,027.00","Qiang (Shawn) Cheng","jale@siu.edu","Ofc. of Sponsored Projects Admin","Carbondale","IL","629014308","6184534540","ENG","1637","039E, 040E, 043E, 1576, 9102","$0.00","The research objective of this award is to develop a probabilistic ground motion model that takes into account the time-varying characteristics of earthquake ground motions to better describe their damage potential.   This research will result in new methodologies to identify the most relevant seismological features affecting the severity of ground motions, to describe the manner in which seismic energy is distributed in the joint time-frequency domain, and to quantify the prediction uncertainty in a unified and structured manner. The research approach is to use a combination of signal processing, feature selection, and machine learning tools that will be customized for ground motions. Deliverables include new methodologies and software tools for analysis and simulation of ground motions, research reports and publications, and education of undergraduate and graduate students. <br/><br/>Successful completion of this project will help establish improved guidelines for selection and simulation of ground motions to be used in seismic design of structures, which in turn will help reduce the cost of damage due to future earthquake occurrences.  The methodology established in this project will be applicable to other real world processes such as wind and ocean waves.  The research findings will be disseminated to broader audiences through journal articles, conferences, and a project website.  All the software tools developed in this project will be made available to engineers, researchers, educators and students. Graduate and undergraduate students in engineering and computer science departments will benefit from this project through enriched curricula and research experience. Two graduate students will be trained to perform the research tasks.  Students at K-12 levels will be exposed to earthquake engineering through outreach activities."
"1116886","III: Small: Exploring Social and Behavioral Contexts for Information Retrieval","IIS","Info Integration & Informatics","09/01/2011","05/02/2014","Hongyuan Zha","GA","Georgia Tech Research Corporation","Continuing grant","Sylvia Spengler","08/31/2015","$508,025.00","","zha@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364","7364, 7923, 9251","$0.00","Web Search engines have become indispensible for people from all walks of life for locating information on  a broad range of topics.  Data from user's interactions with the search engines, web pages, and among each other  provide a rich source of information for understanding and profiling users. This project develops a novel probabilistic framework and the associated machine learning algorithms for modeling users and their behavior on the web to improve the ranking of results returned in response to user queries. <br/><br/>This project addresses three closely related technical topics in behavioral and social data modeling, covering the underlying theory, algorithm development and evaluation on real-world data: 1) Developing rich  models of user interactions with search engines, information sources,  with each other that can handle multiple types of relationships among the entities; 2) Exploiting the resulting models of user behavior to improve the ranking of web pages returned in response to a user query; 3) Modeling the temporal dynamics of user behavior to detect and respond tp changes in the social environment of the user or context-dependent changes in the user's behavior. The resulting algorithms will be evaluated using (i) real-world data available in the public domain (ii) real-world data sets from industrial collaborators and (iii) simulated data that preserve the relevant statistical properties of their real-world counterparts.<br/><br/>The project advances the current state of the art in information retrieval, with potentially large impact on web search and related applications. Collaborations with industry leaders in Web search and e-commerce such as Yahoo!, Microsoft, Tencent and Alibaba can potentially lead to significant impact on information retrieval, web search and ranking, recommender systems, and related areas. The project offers enhanced research-based training opportunities for graduate and undergraduate students at Georgia Tech. Additional information about the project can be found at: http://www.cc.gatech.edu/~zha/socialL2R.html"
"1117597","NeTS: Small: Collaborative Research: Understanding Traffic Dynamics in Cellular Data Networks and Applications to Resource Management","CNS","Networking Technology and Syst","08/01/2011","08/03/2011","Milind Buddhikot","NJ","Lucent Technologies Bell Laboratories","Standard Grant","Thyagarajan Nandagopal","07/31/2015","$103,774.00","","milind.buddhikot@alcatel-lucent.com","600 Mountain Avenue","Murray Hill","NJ","079740636","4699914503","CSE","7363","7923","$0.00","Broadband cellular networks are emerging to be the most common means for mobile data access worldwide. Predictions from industry analysts indicate that the volume of data through cellular data networks will increase exponentially in near future. Understanding of the mobile data traffic via measurement and analysis is critical for the development of resource management techniques for these networks. While spectrum resources are of great concern, this project specifically focuses on the energy required to operate the cellular network infrastructure, specifically base stations. The project undertakes a significant modeling exercise with two goals. One goal is intellectual, driven towards understanding the spatio-temporal dynamics of mobile traffic and discovering possible structure or relationships. The project uses state-of-the-art machine learning tools to develop models using large-scale data collected directly from the operator's networks. Such modeling will bring new insights that in turn will help to deploy and manage future generation cellular data networks. The second goal is utilitarian. Here, techniques are developed to predict base station loads for use in resource management, specifically energy. Algorithms are designed to exploit energy-optimization opportunities to turn off specific network resources based on the forecasted load.<br/><br/>The project has significant broader impact. It develops technologies to appreciably reduce energy consumption in cellular networks. Overall, this exercise will both reduce cost, and contribute to the environment. The project also contributes to several green initiatives in both institutions and to the education and training of graduate students."
"1203986","CAREER: Adaptive Power Management for Multiprocessor System-on-Chip","CNS","Computer Systems Research (CSR","08/23/2011","06/08/2013","Qinru Qiu","NY","Syracuse University","Continuing grant","M. Mimi McClure","05/31/2015","$199,714.00","","qiqiu@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7354","1045, 1187, 7354, 9102, 9216, 9218, HPCC","$0.00","CAREER: Adaptive Power Management for Multiprocessor System-on-Chip<br/><br/>Multiprocessor System-on-Chip (MPSoC) is becoming a major VLSI system design platform due to its advantages in low design cost and high performance. However, power consumption in MPSoC is a crucial factor that is limiting the growth of system performance and functionality. The complexity of the MPSoC hardware and software imposes new challenges and requirements for research in system-level power management.<br/>An effective power manager must be aware of the status of the hardware, the application and the working environment and be able to adapt to the changes. It should be able to work robustly even if the perfect system information is not available. As the number of components that can be power controlled increases, it is increasingly difficult to perform power management in a centralized manner. A hierarchical and distributed power management method is more suitable for MPSoC platforms. Finally, resource management, power management, and thermal management are inter-correlated tasks and it is desirable for them to be optimized simultaneously.<br/>This research project addresses the above mentioned challenges by investigating the theoretical foundation and the applied framework of adaptive power management for the next generation MPSoC. This project consists of four research components: (1) investigate online modeling techniques for runtime workload prediction and hardware performance/power characterization; (2) research new optimization techniques for adaptive resource and power management in a partially observable system; (3) model the distributed power management problem as a multi-agent cooperative game and develop control policy using game theory; (4) develop a unified and standard platform for modeling, optimization and evaluation of power-managed MPSoC.<br/>The educational components of this project will introduce the students to the implementation and optimization techniques of system-level power management and provide students unique hands-on experience with MPSoC design and optimization. <br/>"
"1059312","RUI: CRI: CI-ADDO-EN: Collaborative Research: MASC: A Community Resource For and By the People","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2011","01/15/2015","Nancy Ide","NY","Vassar College","Standard Grant","Tatiana Korelsky","06/30/2015","$190,801.00","","ide@vassar.edu","124 Raymond Avenue","Poughkeepsie","NY","126040657","8454377092","CSE","1714, 7359","9229, 9251","$0.00","The Manually Annotated Sub-Corpus (MASC) is a shared corpus that supports research across<br/>several disciplines: linguistics, computational linguistics, psycholinguistics, sociolinguistics and<br/>machine learning. It includes a wide variety of present-day American English texts annotated for<br/>several linguistic phenomena. Because MASC provides a unique resource, considerable<br/>community momentum has grown up around it. This project builds upon this momentum to<br/>enable the corpus to grow on its own, and to address the need for additional annotations. The<br/>major activities are to : (1) provide web-based mechanisms to facilitate community contribution<br/>and use of MASC annotations; (2) develop means to more fully automate the annotation<br/>validation process; (3) extend the WordNet annotations to cover adjectives, to support research<br/>on evaluation of ?subjective? annotations and harmonization of WordNet with other resources;<br/>(3) promote use of MASC and new annotations by diverse groups, by sponsoring shared tasks<br/>that exploit the corpus? unique characteristics and supporting beta-testers of software, data, and<br/>annotations; and (4) aggressively develop an ?Open Language Data? community around MASC<br/>through workshops, tutorials, and active participation in relevant community activities.<br/><br/>MASC provides an unparalleled resource for training and testing of tools for natural language<br/>processing, which can enable a major leap in the productivity of NLP research and ultimately<br/>impact the way people use and interact with computers. It is the first fully open, communitydriven<br/>resource in the field. All data and annotations are freely distributed in a manner that<br/>permits immediate and easy accessibility for users around the globe."
"1049308","Corpora of Non-Linguistic Symbol Systems, and Statistical Analysis","BCS","Linguistics, HCC-Human-Centered Computing, Robust Intelligence","04/01/2011","07/01/2013","Steven Bedrick","OR","Oregon Health & Science University","Standard Grant","Joan Maling","09/30/2013","$130,140.00","","bedricks@ohsu.edu","3181 S W Sam Jackson Park Rd","Portland","OR","972393098","5034947784","SBE","1311, 7367, 7495","1311, 7495, 9251, SMET","$0.00","Throughout the millenia, humans have used graphical symbols. 5000 years ago, in Mesopotamia, a system of symbols was codified into what was to become the world's first writing system. But in addition to writing systems, many symbol systems have been developed that, though they do communicate information, do not encode language. Such non-linguistic symbol systems include familiar examples such as mathematical symbology, European heraldry, or scouting merit badges, as well as less familiar ones such as Mesopotamian deity symbols or Dakota winter counts.  Like writing, such systems are an important part of the cultures that created them.<br/><br/>Now suppose one has a symbol system whose interpretation is unknown. Short of deciphering it as a writing system, or otherwise providing a rigorous testable interpretation, are there any methods that can determine whether one is dealing with writing or a non-linguistic system? Some recent high-profile papers have indeed claimed to provide statistical evidence that a couple of ancient systems were linguistic. But, one problem with that work is that in order to determine which of two categories an unknown system belongs to, it is very useful to have a large set of examples of the two categories in question. In the case of writing systems, we now have many electronic corpora from hundreds of languages, both ancient and modern. But electronic corpora of non-linguistic systems are few.<br/><br/>This project will fill this void by developing and releasing to the public electronic corpora of a range of non-linguistic systems, including those named above as well as several others.  It will also investigate statistical and machine learning methods that might help in distinguishing written language from other graphical communication. And this in turn will lead to a better understanding of a fundamental question about humanity: what sets language apart from other forms of communication?"
"1138800","Supporting US-Based Students to Attend the 2011 IEEE International Conference on Data Mining (ICDM 2011)","IIS","Info Integration & Informatics","07/01/2011","06/29/2011","Wei Wang","NC","University of North Carolina at Chapel Hill","Standard Grant","Maria Zemankova","06/30/2012","$24,000.00","Diane Cook, Wei Ding","weiwang@cs.ucla.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7364","7364, 7556","$0.00","This grant provides international travel support for U.S. based graduate student participants to attend the 2011 International Conference on Data Mining (ICDM 2011), which will be held in Vancouver, Canada, on December 11-14, 2011 (http://icdm2011.cs.ualberta.ca/). ICDM has established itself as the world's premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences. The conference covers all aspects of data mining, including algorithms, software and systems, and applications, as well as related areas such as data management, machine learning and bioinformatics. The conference proceedings are published by IEEE. The conference seeks to continuously advance the state-of-the-art in data mining. With the growth of the Web, the Internet, and data intensive technologies such as Sensor Networks, and Bioinformatics, Data Mining is an extremely important area in Information Technology. Besides the technical program, the conference features workshops, tutorials, panels, data mining contest, and starting this year, the Ph.D. forum.<br/><br/>A strong representation of U.S. researchers at the Conference is useful in maintaining U.S. competitiveness in this important area. The total number of ICDM participants in the past has been in excess of 500, with a majority of the participants from the U.S., then Europe and Asia. It is expected to provide scholarships to 16 U.S. based graduate student participants. This grant will partially support the travel costs for the U.S. based graduate student participants. <br/><br/>More information of the Ph.D. forum can be found at http://icdm2011.cs.ualberta.ca/phd-forum.php. The award results will be announced at http://icdm2011.cs.ualberta.ca/travel-grants.php."
"1058202","Neural Population Coding in the Brain","PHY","PHYSICS OF LIVING SYSTEMS","09/15/2011","09/09/2011","Vijay Balasubramanian","PA","University of Pennsylvania","Standard Grant","Krastan Blagoev","08/31/2016","$300,000.00","","vijay@physics.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","7246","9183","$0.00","This project explores how information processing by neural circuits is organized to use the resources of the brain efficiently.  The proposed theoretical studies apply fundamentally new approaches to analyzing the organization of cortical maps, by investigating how emerging principles of efficient design pertain to the computational mechanisms employed by the central brain. One aim studies how the ""place map"" in the hippocampus (where individual cells are tuned to fire in particular locations of an environment) should be organized to efficiently support general goal-directed navigation.     A second aim studies how the ""shape map"" in Inferotemporal Cortex (where individual cells are tuned to fire in response to particular visual shapes) should be organized to efficiently support shape perception, given the distribution of shapes in natural visual scenes.  These theoretical studies will lead to directly testable predictions of the distribution of tuning curves in cortical area IT and hippocampus. In this way, the theory will provide a lever for further experimental exploration of the architecture of form vision and spatial navigation in the brain.  Population codes also involve interactions between the different neurons, but techniques are not yet available to comprehensively study these interactions in cortex.   Thus, a third aim uses the retina (a piece of the central brain that has projected out into the eye) as a model system to theoretically and experimentally ask two basic, and as yet unanswered questions:  (a) Do neural networks adapt their interactions to stimulus statistics and noise as predicted by optimization theory?; (b) Is noise, as measured from single neurons, simply a mis-reading of correlated activity?     By asking and answering these questions, this project will also explain key aspects of how the retina prepares visual input for central processing.  Knowledge of how retinal circuits respond to natural and synthetic stimuli will be useful in designing effective prosthetic devices. <br/><br/>This project strengthens the research connections between disciplines by bringing together analytical and theoretical methods from physics and machine learning with experimental techniques from neuroscience.  Students and postdocs who thus develop proficiency with both biological and quantitative physical techniques will be better able to cope with scientific and industrial challenges of coming decades.  The educational component of this proposal also addresses this national need directly by developing pedagogical materials for a course on ""Theoretical and Computational Neuroscience"".  The PI will give presentations to K-8 and high school students and to the general public with a view to broadening public knowledge of the field.  Outreach to historically disadvantaged communities will be carried out through established programs at Penn.  Finally, the PI is active in organizing lecture series and conferences that engage physicists to work within quantitative systems neuroscience."
"1116430","NeTS: Small: Target Monitoring with Low-Cost Sensor Networks","CNS","Networking Technology and Syst","09/01/2011","06/12/2012","Duc Tran","MA","University of Massachusetts Boston","Standard Grant","Thyagarajan Nandagopal","08/31/2015","$383,887.00","Bridget Benson","duc.tran@umb.edu","100 Morrissey Boulevard","Dorchester","MA","021253300","6172875370","CSE","7363","7923","$0.00","Most research on target monitoring using sensor networks requires the geographic locations of the sensors to be known. However, in practice, it is not always possible for every sensor node to have a built-in GPS-like capability. On the other hand, applying a localization procedure to all the sensor nodes can be expensive for large networks. Even if the location information of many sensors can be computed, it may never be used because targets usually appear very sparse. <br/><br/>This project, therefore, tackles the target monitoring problem in the context of low-cost sensor networks, where a sensor node does not need built-in capability to determine its location. Instead, hopcount information serves as the primary source of information to track the target. The challenges result from the unknown mobility and nature of the target and the coarseness of the hopcount data. The matter is more sophisticated if there are multiple moving targets and it is not known whether they move in groups or independently.<br/><br/>The research objective of the project is to design and implement a novel solution framework to address these challenges. This framework, based on a machine-learning approach, can be applied to a wide range of sensor networks which can have modest resource capacities or be deployed in non-conventional physical settings such as under the water or on wet ground. Building on the research, an education plan is proposed that seeks to transfer knowledge to younger generations, improve student enrollment and retention, and encourage active participation from students of under-represented groups."
"1250687","Collaborative Research: Mathematical Programming for Streaming Data","CMMI","OPERATIONS RESEARCH","08/31/2011","07/17/2012","Laurent El Ghaoui","CA","University of California-Berkeley","Standard Grant","Sheldon Jacobson","05/31/2013","$150,473.00","","elghaoui@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","5514","073E, 9147, MANU","$0.00","A large amount of data is now easily accessible in real-time in a streaming fashion: news, traffic, temperature or other physical measurements sent by sensors on cell phones. Applying statistical and machine learning methods to these streaming data sets represents tremendous opportunities for a better real-time understanding of complex physical, social or economic phenomena. These algorithms could be used, for example, to understand trends in how news media cover certain topics, and how these trends evolve over time, or to track incidents in transportation networks.<br/><br/>Unfortunately, most algorithms for large-scale data analysis are not designed for streaming data; typically, adding data points (representing, say, today's batch of news articles from the Associated Press) requires re-solving the entire problem. In addition, many of these algorithms require the whole data set under consideration to be stored in one place. These constraints make classical methods impractical for modern, live data sets.<br/><br/>This project's focus is on optimization algorithms designed to work in online mode, allowing for faster, possibly real-time, updating of solutions when new data or constraints are added to the problem. Efficient online algorithms are currently known for just a few special cases. Using homotopy methods and related ideas, this work will seek to allow online updating for a host of modern data analysis problems. A special emphasis will be put on problems involving sparsity or grouping constraints; such constraints are important for example to understand how a few key features in the data set that explain most of the changes in the data. These new online algorithms will be amenable to distributed implementations to allow for parts of the data to be stored on different servers.<br/><br/>These methods will be applied to streaming news data coming from major US media, and also to the problem of online detection, which arises when tracking some important signal over, say, a communication network, in an online fashion."
"1148012","III: EAGER: Discovering Spontaneous Social Events","IIS","Info Integration & Informatics","09/01/2011","08/08/2011","Charles Dyer","WI","University of Wisconsin-Madison","Standard Grant","Sylvia Spengler","08/31/2014","$151,868.00","Xiaojin Zhu","dyer@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7364","7364, 7916","$0.00","Real-world social events provide a convenient and intuitive way to organize social media content for individuals. Current approaches to event detection from social media: assume that the events to be monitored (and their social media signatures) are known a priori; focus largely on text data and fail to take advantage of other forms of media e.g., images. <br/><br/>Against this background, this project explores a novel approach to discovering spontaneous, a priori unspecified, social events through joint Bayesian non parametric modeling of multi-modal data (including text and images) and using the events thus discovered to foster new social links. The resulting tools for event discovery will be tested in an application involving discovery of wild animal disease outbreaks from twitter text messages and images posted by individuals. <br/><br/>The project brings together an interdisciplinary team of researchers with expertise in image analysis, text mining, and machine learning to advance the state of the art in detection of spontaneous, a priori unspecified events (as they emerge) from social media data. It is expected to yield new scalable nonparametric Bayesian approaches to joint modeling of image and text data, and more generally multi-modal social media data. The resulting tools could potentially transform the way in which people use social media data by empowering them to discover and participate in real world events even as they emerge."
"1111109","AF: Large: Collaborative Research: Algebraic Graph Algorithms: The Laplacian and Beyond","CCF","Algorithmic Foundations","09/01/2011","06/24/2011","Jonathan Kelner","MA","Massachusetts Institute of Technology","Standard Grant","Tracy Kimbrel","08/31/2017","$969,956.00","","kelner@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7925, 7926, 7928","$0.00","This project will exploit algebraic properties of operators associated with graphs in an integrated set of research and educational activities designed to develop new mathematical and algorithmic techniques; apply these to the solution of real-world problems and longstanding theoretical questions in mathematics, computer science, biology, and physics; and make these techniques broadly known and accessible to students, researchers, and practitioners in many fields.<br/><br/>This research has its origins in spectral graph theory, which studies how the eigenvalues and eigenvectors of the graph Laplacian (and other related matrices) interact with the combinatorial structure of the graph.  Spectral graph theory has been one of the great success stories in both the theory and practice of algorithm design. It has led to fundamental advances in graph partitioning, web search (notably including Google's PageRank algorithm), the understanding of random processes and the algorithms derived from them, the construction of error correcting codes, derandomization, convex optimization, machine learning, and many others.<br/><br/>While the eigenvalues and eigenvectors of the Laplacian capture a striking amount of the structure of the graph, they certainly do not capture all of it.  Recent work by the principal investigators and other researchers suggests that theoretical computer scientists have only scratched the surface of what can be done if they are willing to broaden their investigation, extending it to study more general algebraic properties of the Laplacian than just its eigenvalue structure, and more general operators than just the Laplacian.<br/><br/>Under this award, the principal investigators will build a research program across the three universities involved in this proposal to develop such a theory and its applications. This initiative has the potential to provide transformative advances in a range of theoretical and applied areas of computer science, including:<br/><br/>* Faster algorithms for fundamental graph problems, such as Maximum Flow, Minimum Cut, Minimum Cost Flow, Multicommodity Flow, approximating Sparsest Cut, generating random spanning trees, and constructing low-stretch spaning trees.<br/><br/>* Better algorithms for the analysis of data, with potential applications to the Unique Games Conjecture.<br/><br/>* Faster algorithms for solving broad classes of important linear systems, both sequentially and in parallel.<br/><br/>* Faster distributed algorithms for information dissemination in networks.<br/><br/>* A spectral and algebraic graph theory for directed graphs, based on ideas from differential geometry.<br/><br/>* Novel quantum algorithms for a large class of problems that appear to be hard for classical computers.<br/><br/>* New techniques for problems in Quantum Physics based on ideas developed in Computer Science and Combinatorics.<br/><br/>The principal investigators will also work to disseminate these techniques by developing courses, training undergraduate and graduate students, and introducing these ideas to scientists in other fields."
"1053486","CAREER: Scalable Computational Models for Multicellular Systems Biology","DBI","ADVANCES IN BIO INFORMATICS","04/01/2011","04/23/2015","Curtis Huttenhower","MA","Harvard University","Continuing grant","Jennifer Weller","03/31/2016","$853,592.00","","chuttenh@hsph.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","BIO","1165","1045, 1187, 1228, 9179","$0.00","We enter life as a composite of some 200 cell types orchestrated into a single metazoan organism; within days, these are joined by trillions of bacterial, archaeal, and eukaryotic microbes resident on every surface of our bodies. While decades of microbial ecology and metazoan cellular biology have detailed many aspects of these entities, we have only recently begun to bridge models of unicellular organisms in monoculture with complex multicellular systems at the molecular level. The goal of this research is thus to develop computational methodology to model the molecular behavior of multicellular systems, particularly microbial communities and their interactions with metazoan tissues, by taking advantage of large experimental data repositories. The project focuses on characterizing the biological roles of gene products in such systems and on translating data from controlled experimental contexts so as to apply in multi-cell-type and multi-species moieties.  This will require the development of data mining algorithms capable of efficiently leveraging thousands of experimental datasets from diverse organisms to model key aspects of multicellular biology: multi-species communities, cell types and lineages, and their structure and distribution within a community or tissue. For this purpose, the project will develop satellite models for machine learning in which core properties and parameters are modified on an as-needed basis. Predictions from these models will be experimentally validated by characterization of the organisms in and functional activity of the oral and gut microbiota and of individual under-characterized microbes and microbial interactions.  Open-source and online implementations of developed tools will be available through the laboratory web site at http://huttenhower.sph.harvard.edu.<br/><br/>This project will provide a general framework for genomic data mining in multicellular systems made up of multiple species or cell types, with a simple interface for summarizing thousands of genome-scale datasets. The educational component will include an expansion of the Program in Quantitative Genomics, which includes a newly-developed computational biology curriculum, outreach through the Stanford <br/>South Africa Biomedical Informatics program, and ongoing collaborations with the Harvard University LS/HHMI and International Society for Computational Biology high school outreach programs.  This will establish solid foundations in training and in computational methodology for understanding multicellular systems and interactions by mining large biological data collections."
"1117216","AF: Small: Optimization Algorithms for Multi-Armed Bandit Problems","CCF","ALGORITHMIC FOUNDATIONS","09/01/2011","06/30/2011","Sudipto Guha","PA","University of Pennsylvania","Standard Grant","Rahul Shah","08/31/2015","$380,000.00","","sudipto@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7796","7923, 7926","$0.00","Computation in many emerging applications, such as online advertising and large scale social, media or sensor networks, is increasingly moving away from a simplistic view of computing a fixed function on a well defined input. Increasingly we are realizing that the input and the function are only the means to an end; and are inexact and imprecise at best.  In many of these applications, the cost of realizing the input, in possibly distributed and dynamic/interactive environments, is a significant fraction of the cost of the computation itself.  The emerging theme has been (i) to formulate models (very often probabilistic) of the input, (ii) precompute strategies that probe or realize few pieces of the input, and (iii) execute the strategies while making small adjustments as the data is incrementally made available. Moreover all these three stages are interleaved and the optimization is often repetitive. The overall process corresponds to repeatedly adapting to the short run behavior of the input which is reset often, starting from an initial and aggregate model of the long term behavior. Thus the task is to design and analyze algorithms that span and adapt to multiple scales of time.<br/><br/>Similar problems which encode the tradeoffs between exploration and exploitation has classically been modeled by the Multi-Armed Bandit problem, where the arms correspond to the available choices. However, these emerging domains differ in several critical aspects. This proposal seeks to extend the optimization and analysis of Multi-Armed Bandit problems in a number of novel dimensions, specifically in terms of nonlinear and subadditive objective functions, noisy and error-prone feedbacks, lack of centrality and entangled feedbacks, implementation barriers of budgets or policies, and dynamic behavior. These extensions are connected, and progress on these problems would lead to a wealth of new results and more importantly, new techniques, in optimization as well as in bandit literature.<br/><br/>In addition to the development of new algorithmic and analysis ideas, the proposal would train graduate students to develop simultaneous expertise in theoretical computer science, machine learning and statistics, as well as stochastic control. Moreover the crosscutting aspect of the research would be disseminated through tutorials, surveys and monographs."
"1115206","AF: Small: Algorithms for Genetics: Epistatic Interactions, Haplotype Assembly, and Selection Signatures","CCF","ALGORITHMIC FOUNDATIONS","10/01/2011","06/08/2011","Vineet Bafna","CA","University of California-San Diego","Standard Grant","Mitra Basu","09/30/2015","$445,000.00","Glenn Tesler","vbafna@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7796","7923, 7931","$0.00","Algorithms for genetics: epistatic interactions, haplotype assembly, and selection signatures<br/><br/>Variation in our DNA (often inherited) can have important functional consequences, including susceptibility to diseases. However, much of variation is due to random drift and may have no functional consequence. Identifying the small subset of variations that are functionally important is key to a deeper understanding of the genetic basis of diseases and other phenotypes, and is the mainstay of statistical genetics and other fields.  However, rapidly falling costs of genome sequencing implies that genomes of entire populations will be completely sequenced. The availability of tremendous amounts of genetic data, and the complexity of relations between genotypes and phenotypes changes the nature of inference problems from statistical to computational, and demands the use of algorithmic (combinatorial and machine learning) techniques. In this proposal, the PIs propose specific goals in three broad areas, which involve the use of algorithmic techniques in solving problems in genetics. <br/><br/>1. Epistatic interactions and geometric embedding: Epistatic interactions where two distant loci interact to jointly mediate the phenotype often confound analyses. However, with millions of loci, testing all pairs for interactions is computationally intractable. The PIs propose to develop fast algorithms for this problem. The approach depends upon the development of a metric embedding that maps the genotypes at a locus to a point in a high dimensional Euclidean metric, such that interacting pairs have small Euclidean distances. This metric embedding is novel, and allows the use of geometric algorithms for fast detection of epistasis.<br/>2. Haplotype assembly: Haplotyping refers to the separation of the maternal and paternal chromosomes. Successful resolution has great impact in improving the efficacy of genetic association, and in understanding the genetic history of the population. The PIs propose the use of modern strobe-sequencing technologies and single genome amplification to dramatically expand the length of achievable haplotypes. One of the formulated problems maps naturally to connectivity in a new class of random graphs.<br/>3. Pooled selection: The PIs propose the identification of regions under genetic selection, using next generation sequencing data. Specifically, the proposed tests work on pooled DNA, and partially sampled DNA, and employ a combination of techniques from population genetics and combinatorial optimization.<br/><br/>Broader Impact and Intellectual Merit<br/>The great promise of genomics is that our complete sequence will be an integral part of our medical record, and the major health prognostics will be informed by variation. However, the early research in correlating genotypes and phenotypes is stymied by lack of analysis tools. The problems addressed here are central to the domain and will clearly add to the toolkit of geneticists and biologists. The research also contributes directly to the CISE-CCF mission of developing novel algorithms for Computational Biology, as the proposed problems are uniquely at the intersection of algorithmic and genetics, and open new avenues of research in Computer Science. <br/><br/>Dissemination and outreach will continue through the length of the project contributing to the broader impact of this research. It will include invited and contributed presentations, publications, classroom projects, and collaborations. Software will be freely available as source-code, or web-tools, for academic, research and non-commercial purposes adding to the infrastructure of genetic analyses tools."
"1111257","AF: Large: Collaborative Research: Algebraic Graph Algorithms: The Laplacian and Beyond","CCF","ALGORITHMIC FOUNDATIONS","09/01/2011","06/24/2011","Daniel Spielman","CT","Yale University","Standard Grant","Tracy J. Kimbrel","08/31/2016","$772,845.00","Nicholas Read","spielman@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7796","7925, 7926, 7928","$0.00","This project will exploit algebraic properties of operators associated with graphs in an integrated set of research and educational activities designed to develop new mathematical and algorithmic techniques; apply these to the solution of real-world problems and longstanding theoretical questions in mathematics, computer science, biology, and physics; and make these techniques broadly known and accessible to students, researchers, and practitioners in many fields. <br/><br/>This research has its origins in spectral graph theory, which studies how the eigenvalues and eigenvectors of the graph Laplacian (and other related matrices) interact with the combinatorial structure of the graph. Spectral graph theory has been one of the great success stories in both the theory and practice of algorithm design. It has led to fundamental advances in graph partitioning, web search (notably including Google's PageRank algorithm), the understanding of random processes and the algorithms derived from them, the construction of error correcting codes, derandomization, convex optimization, machine learning, and many others. <br/><br/>While the eigenvalues and eigenvectors of the Laplacian capture a striking amount of the structure of the graph, they certainly do not capture all of it. Recent work by the principal investigators and other researchers suggests that theoretical computer scientists have only scratched the surface of what can be done if they are willing to broaden their investigation, extending it to study more general algebraic properties of the Laplacian than just its eigenvalue structure, and more general operators than just the Laplacian. <br/><br/>Under this award, the principal investigators will build a research program across the three universities involved in this proposal to develop such a theory and its applications. This initiative has the potential to provide transformative advances in a range of theoretical and applied areas of computer science, including: <br/><br/>* Faster algorithms for fundamental graph problems, such as Maximum Flow, Minimum Cut, Minimum Cost Flow, Multicommodity Flow, approximating Sparsest Cut, generating random spanning trees, and constructing low-stretch spaning trees. <br/><br/>* Better algorithms for the analysis of data, with potential applications to the Unique Games Conjecture. <br/><br/>* Faster algorithms for solving broad classes of important linear systems, both sequentially and in parallel. <br/><br/>* Faster distributed algorithms for information dissemination in networks. <br/><br/>* A spectral and algebraic graph theory for directed graphs, based on ideas from differential geometry. <br/><br/>* Novel quantum algorithms for a large class of problems that appear to be hard for classical computers. <br/><br/>* New techniques for problems in Quantum Physics based on ideas developed in Computer Science and Combinatorics. <br/><br/>The principal investigators will also work to disseminate these techniques by developing courses, training undergraduate and graduate students, and introducing these ideas to scientists in other fields."
"1113793","SBIR Phase I: A Robust Caller-ID Alternative for Securing Telephony Based Transactions","IIP","SMALL BUSINESS PHASE I","07/01/2011","11/29/2011","Vijay Balasubramaniyan","GA","Telineage, Inc.","Standard Grant","Glenn H. Larsen","06/30/2012","$175,000.00","","vijay@telineage.com","742 CHARLES ALLEN DRIVE NE APT 1","Atlanta","GA","303083741","9082650096","ENG","5371","5371, 6850, 8032, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I research project will explore technologies that will enable an improvement in trust in the converged telephony infrastructure without negatively impacting user experience. Telephony has long been viewed as a trusted communications medium and a variety of transactions conducted over the telephone depend on such trust. For example, to combat credit card and other financial fraud, banks rely on their ability to securely communicate with their customers via the phone. Unfortunately, while the convergence of traditional telephony with cellular and IP networks offers many benefits, it has opened it to additional security threats. Call metadata such as caller identifier can now be easily manipulated and attacks including voice phishing have already resulted in financial losses for banks. The intellectual merit of this research project lies in demonstrating that the source and intermediary networks in a phone call introduce artifacts in the audio that can be used to uniquely identify its source. The project will investigate features that capture key call artifacts and apply machine learning techniques to fingerprint call sources. A key goal will be the validation of the preliminary results at the scale that will arise in real-world deployments.<br/><br/>The broader impact of the project will come from the development of a secure Caller-ID alternative with applicability both in the enterprise setting as well as at the consumer end. Because the proposed approach only relies on analysis of audio at the receiving end, it requires no changes to be made to the telephony infrastructure. This is especially advantageous not only because of the complex and diverse nature of this infrastructure, but also because unlike a cryptographic solution both ends of a call do not need to participate in the process. The ubiquitous nature of the telephone and possible erosion of trust in this medium will have serious consequences for both businesses and citizens. The success of this project will help maintain this trust and thus it will have broad commercial and societal impact."
"1106817","Collaborative Research:  Statistical Methods for Analyzing Complexity and Growth of Large Biological and Information Networks","DMS","STATISTICS","07/01/2011","06/30/2011","David Banks","NC","Duke University","Standard Grant","Gabor Szekely","06/30/2013","$90,000.00","","banks@stat.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","MPS","1269","","$0.00","Every network dataset poses unique challenges, but there is a growing toolkit of methods that can be adapted to specific situations. This research will extend that toolkit by continuing the development of machine learning ideas that perform aggressive local variable selection to fit local metrics, thus allowing nearby nodes to have similar models for edge formation, but distant nodes to have very different models.  Also, the research will provide a new approach to goodness-of-fit assessment for network models, based upon minimum description length inference.<br/><br/>Network modeling has emerged as a critical methodology across many fields of science, including biochemistry, sociology, and Internet communication.  Important applications include social interactions leading to fission in baboon troops, biochemical knowledge derived from protein-protein interactions, and insight into the growth and structure of the Wikipedia.  This research will develop novel statistical models for network growth and new ways to assess how well they explain a given dataset."
"1054960","CAREER:  An Integrated Framework for Multimodal Music Search and Discovery","IIS","Info Integration & Informatics","02/01/2011","04/23/2015","Gert Lanckriet","CA","University of California-San Diego","Continuing Grant","Maria Zemankova","01/31/2017","$550,000.00","","gert@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7364","1045, 1187","$0.00","A revolution in music production and distribution has made millions of songs instantly available to virtually anyone, on the Internet. However, a listener looking for ""dark electronica with cello"" or ""music like U2's"", without knowing a relevant artist or song name, or a musicologist wanting to search through large amounts of unknown ethnic music, would face serious challenges. Novel music search and discovery technologies are required to help users find the desired content.<br/><br/>The non-text-based, multimodal character of Internet-wide information about music (audio clips, lyrics, web documents, images, band networks, etc.) poses a new and difficult challenge to existing database technology that depends on unimodal, text-based data-structures. This project addresses two fundamental research questions at the core of addressing this challenge: (1) The automated annotation of (non-text-based) audio content with descriptive keywords; and (2) the automated integration of the heterogeneous content of multimodal databases, to improve music search and discovery on the Internet or in a personal database. The resulting architecture leverages the automation and scalability of machine learning with the effectiveness of human computation, engaging music professionals or enthusiasts around the world.<br/><br/>The research addresses questions at the core of multimedia information retrieval in general, enabling the design of a new generation of expressive and flexible retrieval systems for multimodal databases, with applications to music discovery, video retrieval, indexing multimedia content on the home PC, etc.<br/><br/>The results of this project, including a software library and annotated music data sets, will be incorporated in ongoing education and outreach activities and disseminated via the project website (http://cosmal.ucsd.edu/~gert/CAREER.html) to enhance research and education in music information retrieval."
"1134669","EAGER:  Local Control Strategies for Predicting Emergent Behavior  and Cooperative Control in Real Time under Minimal Communication","CMMI","CONTROL SYSTEMS","09/01/2011","03/09/2012","Suhada Jayasuriya","FL","The University of Central Florida Board of Trustees","Standard Grant","George Chiu","02/28/2014","$176,000.00","","suhada@drexel.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","ENG","1632","030E, 031E, 034E, 036E, 1057, 116E, 7916, 9178, 9251, CVIS","$0.00","The objective of this Early-Concept Grants for Exploratory Research (EAGER) project is to initiate exploratory work in three areas inspired by the remarkable ability of avian species such as birds, bats and insects  to gracefully and rapidly fly through extremely cluttered and complex environments.: (i) research the large data base on  bats and other avian species to identify a set of key experiments, (ii) use information gathered to guide the innovation of new control principles in a few specific scenarios including formation flying and search missions, (iii) investigate purely local control to explain a few selected swarm behaviors studied by conservation biologists and use the evolving ideas as a platform to  (a) help better understand emergent behavior, and (b) synthesize simple data based algorithms for controlling the motion of collectives under realistic constraints. Specific approaches to be followed will involve (a) treating collectives as continua, (b) homotopy methods for collision free motion, and (c) pointwise optimization leading to finite dimensional searches that eliminates the need for solving complex two-point boundary value problems. <br/><br/>The successful completion of this research will show that avian and other biological inspirations could lead to a shift in paradigm in navigation and control. A  key impact of this work is expected to be a better understanding of how vision and sonar are used by biological systems to navigate in highly complex, unstructured and cluttered environments. The current state of the art of image processing using ideas of machine learning is simply too difficult for real time computations and new approaches are needed. If successful this research can answer questions such as: How can simple error correction ideas be adopted for vision based navigation? Can parallel algorithms be developed with guarantees of convergence time that are based on biological principles? Additionally, a heterogeneous mobile agent platform where bio-inspired controls are used will be developed which can be used to inspire pre-college and undergraduate students to take up careers in engineering. The results will be disseminated through publication of papers, conference presentations and a workshop to be organized in bio-inspired control design."
"1116289","SHF: Small: Synthesizing Human-Readable Documentation","CCF","Software & Hardware Foundation","09/01/2011","07/19/2011","Westley Weimer","VA","University of Virginia Main Campus","Standard Grant","Sol Greenspan","08/31/2016","$470,955.00","","weimerw@umich.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7798","7923, 7944","$0.00","Developing and maintaining software is a key challenge in computer science,<br/>with failures costing up to one half of one percent of the US GDP each<br/>year. Most code is retained and evolved, rather than created from scratch,<br/>and professional software developers spend over three-fourths of their time<br/>trying to understand existing code. Understandability and documentation<br/>have become key components of software quality, yet they remain poorly<br/>understood by both researchers and practitioners. In a future where the<br/>software engineering focus shifts from implementation to design and<br/>composition concerns, program understandability will become even more<br/>important. This research develops tools and techniques for mechanically<br/>generating documentation to help make programs easier to understand.<br/><br/>The research follows the insight that modern analysis techniques can form<br/>rich descriptive models of programs that are both precise and succinct.<br/>Human-readable documentation can then be synthesized from such models.<br/>The approach applies to large programs across multiple application domains.<br/>The research focuses on documenting how code should be used correctly, a<br/>critical aspect in an era of components-of-the-shelf development, as well<br/>as documenting how code has changed and evolved over time, a key<br/>part of software maintenance. The research leverages program analysis<br/>techniques, machine learning, and textual synthesis, with results<br/>disseminated through academic publication; the education, training and<br/>mentoring of students; as well as freely-available, open-source tools."
"1131291","CRCNS Data Sharing: An open data repository for cognitive neuroscience: The OpenfMRI Project","OAC","Data Cyberinfrastructure","09/01/2011","08/27/2011","Deanna Barch","MO","Washington University","Standard Grant","Robert Chadduck","08/31/2014","$101,395.00","","dbarch@artsci.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7726","7327","$0.00","Functional magnetic resonance imaging (fMRI) has become the most common tool for cognitive neuroscience, because it provides a safe, non-invasive, and powerful means to image human brain function. Based on recent rates of publication, there are currently more than 2000 fMRI studies being performed every year worldwide.  The aggregation of data across multiple studies can provide the ability to answer questions that cannot be answered based on a single study. For example, using datasets from multiple domains one can start to investigate to what degree a region is selectively engaged in relation to a particular mental process, as opposed to being generally engaged across a broad range of tasks and processes. In addition, it provides the ability to integrate across specific tasks to obtain stronger empirical generalizations about mind-brain relationships, and to better understand the nature of individual variability across different measures. Recent work in neuroimaging analysis has focused on the application of methods such as machine learning techniques to understand the coding of information at the macroscopic level, and network analysis techniques to understand the interactions inherent in large-scale neural systems. The availability of a large testbed of high-quality fMRI data from published studies would also provide an important resource for the development of these and other new analytic techniques for fMRI data.  However, sharing of raw fMRI data is challenging due to the large size of the datasets and the complexity of the associated metadata, and there is currently no infrastructure for the open sharing of new fMRI datasets.<br/><br/>This project, OpenfMRI, will provide a new infrastructure for the broad dissemination of raw data within cognitive neuroscience, addressing a critical need by providing an open data sharing resource for neuroimaging.  The initial project is already online at http://www.openfmri.org with a limited number of datasets.  The full project will greatly expand this repository by providing access to a large number of fMRI datasets from several prominent neuroimaging labs, spanning across a broad range of cognitive domains. Utilizing the substantial computational resources of the Texas Advanced Computing Center, the project will also perform standard fMRI analyses on all data in the repository using a common analysis pipeline, thus providing directly comparable analysis results for all of the studies in the database.  The OpenfMRI project will support the development of infrastructural elements to make sharing of data by additional investigators more straightforward.<br/><br/>The repository of data that will be created by the OpenfMRI project will also serve as an important resource for teaching by providing students with the ability to replicate the analyses from published studies using the same data. By providing any researcher in the world with the ability to acquire large fMRI datasets, it will also provide all researchers with the ability to work with the same state-of-the-art datasets, regardless of institution. By creating the infrastructure for open sharing of research data, the project will also enhance the impact of other NSF-funded neuroimaging research projects by providing an infrastructure that can be used to make their data available. The planned work has the potential to benefit society by improving education, health, and human productivity through an increased understanding of mental function and its relationship to brain function."
"1054389","CAREER: An Axiomatic Basis for Statistical Privacy","CNS","SPECIAL PROJECTS - CISE, TRUSTWORTHY COMPUTING, Secure &Trustworthy Cyberspace","02/01/2011","04/14/2015","Daniel Kifer","PA","Pennsylvania State Univ University Park","Continuing grant","Dan Cosley","12/31/2017","$437,501.00","","duk17@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1714, 7795, 8060","1045, 1187, 7795, 9178, 9251","$0.00","Statistical privacy is the art of releasing the datasets that provide useful information about population trends without revealing private information about any individual. Recent high-profile attacks on datasets released by AOL and Netflix demonstrate the need for rigorous application-specific privacy definitions to guide the anonymization of data. The goal of this project is to develop modular components, called privacy axioms, that can be chained together to create customized privacy definitions and anonymized data for statistical privacy applications. Such modularity can enable data curators without extensive expertise in statistical privacy to release anonymized data while providing privacy guarantees that are more interpretable and reliable.<br/><br/>Intellectual merit: this project is designed to provide a unifying framework for statistical privacy that can bring about a deeper understanding of privacy issues and provide guidance for the safe anonymization and release of sensitive data. In addition to theoretical developments, this research plan also targets specific existing applications at Penn State and the U.S. Census Bureau.<br/><br/>Broader impact: the systematic approach to privacy pursued by this project can enable access to and analysis of anonymized data in domains where access to data is otherwise heavily restricted. <br/>This project aims to build upon the investigator's prior experience with outreach programs such as the Summer Research Opportunities Program (SROP) by involving undergraduates in the proposed research. To prepare students for future work that requires analysis of anonymized data, this research is also being integrated into machine learning courses at Penn State.<br/><br/>For further information see the project web site at the URL:<br/>http://www.cse.psu.edu/~dkifer/axiomatizingprivacy.html"
"1106980","Collaborative proposal: Statistical methods for analyzing complexity and growth of large biological and information networks","DMS","STATISTICS","07/01/2011","06/30/2011","Edoardo Airoldi","MA","Harvard University","Standard Grant","Gabor J. Szekely","06/30/2014","$75,000.00","","airoldi@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","Every network dataset poses unique challenges, but there is a growing toolkit of methods that can be adapted to specific situations. This research will extend that toolkit by continuing the development of machine learning ideas that perform aggressive local variable selection to fit local metrics, thus allowing nearby nodes to have similar models for edge formation, but distant nodes to have very different models. Also, the research will provide a new approach to goodness-of-fit assessment for network models, based upon minimum description length inference. <br/><br/>Network modeling has emerged as a critical methodology across many fields of science, including biochemistry, sociology, and Internet communication. Important applications include social interactions leading to fission in baboon troops, biochemical knowledge derived from protein-protein interactions, and insight into the growth and structure of the Wikipedia. This research will develop novel statistical models for network growth and new ways to assess how well they explain a given dataset."
"1131338","CRCNS Data Sharing: An open data repository for cognitive neuroscience: The Open fMRI Project","OAC","DATANET","09/01/2011","08/27/2011","Jason Mitchell","MA","Harvard University","Standard Grant","Robert Chadduck","08/31/2014","$101,535.00","","jason_mitchell@harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7726","7327","$0.00","Functional magnetic resonance imaging (fMRI) has become the most common tool for cognitive neuroscience, because it provides a safe, non-invasive, and powerful means to image human brain function. Based on recent rates of publication, there are currently more than 2000 fMRI studies being performed every year worldwide.  The aggregation of data across multiple studies can provide the ability to answer questions that cannot be answered based on a single study. For example, using datasets from multiple domains one can start to investigate to what degree a region is selectively engaged in relation to a particular mental process, as opposed to being generally engaged across a broad range of tasks and processes. In addition, it provides the ability to integrate across specific tasks to obtain stronger empirical generalizations about mind-brain relationships, and to better understand the nature of individual variability across different measures. Recent work in neuroimaging analysis has focused on the application of methods such as machine learning techniques to understand the coding of information at the macroscopic level, and network analysis techniques to understand the interactions inherent in large-scale neural systems. The availability of a large testbed of high-quality fMRI data from published studies would also provide an important resource for the development of these and other new analytic techniques for fMRI data.  However, sharing of raw fMRI data is challenging due to the large size of the datasets and the complexity of the associated metadata, and there is currently no infrastructure for the open sharing of new fMRI datasets.<br/><br/>This project, OpenfMRI, will provide a new infrastructure for the broad dissemination of raw data within cognitive neuroscience, addressing a critical need by providing an open data sharing resource for neuroimaging.  The initial project is already online at http://www.openfmri.org with a limited number of datasets.  The full project will greatly expand this repository by providing access to a large number of fMRI datasets from several prominent neuroimaging labs, spanning across a broad range of cognitive domains. Utilizing the substantial computational resources of the Texas Advanced Computing Center, the project will also perform standard fMRI analyses on all data in the repository using a common analysis pipeline, thus providing directly comparable analysis results for all of the studies in the database.  The OpenfMRI project will support the development of infrastructural elements to make sharing of data by additional investigators more straightforward.<br/><br/>The repository of data that will be created by the OpenfMRI project will also serve as an important resource for teaching by providing students with the ability to replicate the analyses from published studies using the same data. By providing any researcher in the world with the ability to acquire large fMRI datasets, it will also provide all researchers with the ability to work with the same state-of-the-art datasets, regardless of institution. By creating the infrastructure for open sharing of research data, the project will also enhance the impact of other NSF-funded neuroimaging research projects by providing an infrastructure that can be used to make their data available. The planned work has the potential to benefit society by improving education, health, and human productivity through an increased understanding of mental function and its relationship to brain function."
"1131801","CRCNS Data Sharing: An open data repository for cognitive neuroscience: The OpenfMRI Project","OAC","DATANET","09/01/2011","08/27/2011","Tor Wager","CO","University of Colorado at Boulder","Standard Grant","Robert Chadduck","08/31/2014","$101,537.00","","torwager@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7726","7327","$0.00","Functional magnetic resonance imaging (fMRI) has become the most common tool for cognitive neuroscience, because it provides a safe, non-invasive, and powerful means to image human brain function. Based on recent rates of publication, there are currently more than 2000 fMRI studies being performed every year worldwide.  The aggregation of data across multiple studies can provide the ability to answer questions that cannot be answered based on a single study. For example, using datasets from multiple domains one can start to investigate to what degree a region is selectively engaged in relation to a particular mental process, as opposed to being generally engaged across a broad range of tasks and processes. In addition, it provides the ability to integrate across specific tasks to obtain stronger empirical generalizations about mind-brain relationships, and to better understand the nature of individual variability across different measures. Recent work in neuroimaging analysis has focused on the application of methods such as machine learning techniques to understand the coding of information at the macroscopic level, and network analysis techniques to understand the interactions inherent in large-scale neural systems. The availability of a large testbed of high-quality fMRI data from published studies would also provide an important resource for the development of these and other new analytic techniques for fMRI data.  However, sharing of raw fMRI data is challenging due to the large size of the datasets and the complexity of the associated metadata, and there is currently no infrastructure for the open sharing of new fMRI datasets.<br/><br/>This project, OpenfMRI, will provide a new infrastructure for the broad dissemination of raw data within cognitive neuroscience, addressing a critical need by providing an open data sharing resource for neuroimaging.  The initial project is already online at http://www.openfmri.org with a limited number of datasets.  The full project will greatly expand this repository by providing access to a large number of fMRI datasets from several prominent neuroimaging labs, spanning across a broad range of cognitive domains. Utilizing the substantial computational resources of the Texas Advanced Computing Center, the project will also perform standard fMRI analyses on all data in the repository using a common analysis pipeline, thus providing directly comparable analysis results for all of the studies in the database.  The OpenfMRI project will support the development of infrastructural elements to make sharing of data by additional investigators more straightforward.<br/><br/>The repository of data that will be created by the OpenfMRI project will also serve as an important resource for teaching by providing students with the ability to replicate the analyses from published studies using the same data. By providing any researcher in the world with the ability to acquire large fMRI datasets, it will also provide all researchers with the ability to work with the same state-of-the-art datasets, regardless of institution. By creating the infrastructure for open sharing of research data, the project will also enhance the impact of other NSF-funded neuroimaging research projects by providing an infrastructure that can be used to make their data available. The planned work has the potential to benefit society by improving education, health, and human productivity through an increased understanding of mental function and its relationship to brain function."
"1046521","SBIR Phase I: An Automated Data Mining System for Analysis of Microcirculation Videos","IIP","SMALL BUSINESS PHASE I","01/01/2011","11/19/2010","Roya Hakimzadeh","VA","Signal Processing Technologies LLC","Standard Grant","Errol Arkilic","06/30/2011","$150,000.00","","contact@signalprocessingtechnologies.com","11910 Drystack Ct","Glen Allen","VA","230595831","8043855913","ENG","5371","5371, 6850, 9139, HPCC","$0.00","This Small Business Innovation Research Phase I project focused on real-time automated data mining of medical videos. These videos are of increasing importance, particularly in applications such as real-time decision making based on video recordings of networks of blood vessels. The proposed research addresses the real-time knowledge discovery from large volumes of complex video signals, for the generation of predictive models. <br/><br/>If successfully deployed, the technology enabled by this effort will extend an area of emerging clinical significance. The effort proposes to specialize a tool towards assessment of tissue oxygenation using real-time automated processing of microcirculation videos. The real-time and fully automated nature of image processing and machine learning methods proposed by this project, places this technology in a strong position in the market, both as a standalone system and as a software package included with imaging hardware."
"1127482","SBIR Phase II: System for Location-Based Mobile Consumer Analytics","IIP","STTR PHASE II","09/01/2011","08/18/2011","Thaddeus Fulford-Jones","MA","Cadio Inc","Standard Grant","Steven Konsek","08/31/2013","$498,395.00","","thaddeus@locately.com","38 Ossipee Rd, Suite 2","Somerville","MA","021441610","6174478340","ENG","1591","5373, 8032","$0.00","This Small Business Innovation Research Phase II project aims to improve data mining technologies for location analytics. This project will focus on the analysis of semi-continuous GPS and/or WiFi-based location data generated by consumer mobile devices. The anticipated improvements would allow consumer insights professionals and advertising effectiveness researchers to better detect emergent patterns and to draw stronger inferences about consumer behaviors, preferences, and lifestyle attributes. The enhanced data mining system would utilize state-of-the-art pattern recognition and machine learning techniques to dynamically process and interpret location and other types of data. If successful, this research will impact the state-of-the-art in location analytics.<br/><br/>This research has the potential to meet the need of consumer insights professionals to better understand how consumers behave, without the use of lengthy surveys. In a broader sense, this research aims to accelerate progress in the emerging field of location analytics. This research can lead to the creation of a location analytics dashboard, similar to existing dashboards for web analytics. Most web analytics dashboards measure metrics such as site visits, page views and time spent for given online properties; analogously, the location analytics dashboard would measure visits by real consumers to physical locations. Such a location analytics dashboard could be offered on a subscription basis to companies that depend on consumer behaviors in the physical world ? including retailers, hotel/resort chains, restaurants, and travel companies. Such a dashboard would address a broad range of market research opportunities, from shopper loyalty research to store sitting to marketing effectiveness measurement. Additional future impacts of the proposed effort include the ability to integrate location analytics data into Geographic Information Systems for improved public safety, municipal planning and transit systems design."
"1125684","BRIGE: Modeling metabolism in embryonic stem cell growth and differentiation","CBET","Engineering of Biomed Systems, BROAD PARTIC IN ENG (BRIGE)","09/01/2011","09/04/2013","Mark Styczynski","GA","Georgia Tech Research Corporation","Standard Grant","alexander leonessa","08/31/2014","$182,127.00","","Mark.Styczynski@chbe.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","5345, 7741","004E, 010E, 017E, 138E, 7218","$0.00","PI: Styczynski, Mark<br/>Proposal Number: 1125684<br/><br/>The research objective of this proposal is to measure and model the metabolism of embryonic stem cells during proliferation and differentiation. The PI's group is dedicated to understanding and controlling metabolism by studying and modeling metabolite concentrations. Here, we apply metabolite profiling and machine-learning techniques to create the first-ever descriptive models of metabolism in embryonic stem cell differentiation. We will also study the regulatory potential of metabolites we identify as significantly correlated with cellular differentiation.<br/><br/>Intellectual Merit<br/>Stem cells are poised to make a revolutionary impact on modern medicine. Given recent successes, attention has begun to turn to the looming biomanufacturing problem inherent in developing stem cell treatments. Difficulties expanding stem cell populations from small numbers and controlling their differentiation are such significant roadblocks to scale-up that stem cell therapies may be technically or economically infeasible on a large scale. One promising route for controlling stem cell behavior during scale-up is monitoring and manipulating metabolism and metabolic signals in these cells -yet there has been little research in this area. This transformative research plan will help fill this knowledge gap, specifically addressing three key issues in the field of stem cell culture and engineering. By performing a pioneering longitudinal metabolomics study, we will (1) establish an initial dataset and models demonstrating the importance of metabolism in stem cell expansion and differentiation, and that can be used in guiding culture scale-up. By identifying metabolites capable of promoting specific differentiation lineages, we will (2) enable more precise control of ES cells during expansion and differentiation, also facilitating scale-up. Finally, our extracellular metabolite profiling techniques and models will (3) provide non-invasive, non-destructive methods for stem cell culture monitoring and quality control that are capable of detecting changes in cell state long before morphological changes are evident.<br/><br/>Broader Impacts<br/>By expanding capabilities in controlling stem cell fate using our systems-level analysis of metabolism, we will enable development of novel and more complex stem cell engineering therapeutics that can save or greatly improve the lives of people facing debilitating diseases. The direct application of our research results to the scale-up of stem cell culture technologies to industrial production levels will circumvent a critical economic roadblock in the field, enabling the development of therapeutic candidates into products that can heal not just those who are rich or fortunate, but anyone facing such a disease regardless of socioeconomic status. Such systems level analysis also has the potential to make a significant impact on future engineers; in this vein, the PI proposes a broadening participation program with a primary focus of encouraging female participation in engineering. The centerpiece of this effort is the development of an activity and event for Girl Scout troops that stokes interest in engineering and introduces them to the systems-level mindset that defines engineering. The PI will collaborate with a local teacher to develop this activity and align it with Georgia state educational standards. Female undergraduates and graduate students will help develop and implement this activity. Other key activities planned by the PI include continuing work with an all-female dormitory, collaboration with the campus Society of Women Engineers chapter in developing outreach activities, and recruiting and mentoring female students. Additionally, the PI's group will host underrepresented minority undergraduate students through an REU program; the first and third aims have been formulated to easily integrate undergraduate researchers in a rewarding project."
"1117775","CIF: Small: Kernel Trick Compressive Sensing","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2011","08/08/2011","Shannon Hughes","CO","University of Colorado at Boulder","Standard Grant","John Cozzens","01/31/2016","$425,845.00","","shannon.hughes@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7797","7923, 7936","$0.00","The goal of this project is to develop methods to acquire real-world images and video from far fewer measurements than is possible in the current state-of-the-art. Such methods could be used to greatly reduce the cost of imaging in situations where taking measurements of an image/video is currently very expensive. This is the case, for example, in magnetic resonance imaging (MRI), commonly used in both medicine and neuroscientific research. These methods could also make possible the acquisition of much more detailed information from the same imaging resources in areas such as remote sensing, geoscience, or astronomy.<br/><br/> To achieve this goal, the project will exploit manifold models for signal structure that have previously proved intractable in the compressive sensing literature. While manifold models can efficiently express many signals in terms of dramatically fewer parameters than the usual Fourier/wavelet models, allowing for complete reconstruction of these signals from dramatically fewer measurements, the complexity of manifold models has to date stood as an obstacle to their use in efficient data acquisition. This project will employ an elegant framework, based on the kernel trick commonly used in kernel methods in machine learning, to permit the use of manifold models for compressive sensing with little to no increase in computational complexity."
"1059246","RUI: CRI: CI-ADDO-EN: Collaborative Research: MASC: A Community Resource For and By the People","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2011","05/14/2014","Rebecca Passonneau","NY","Columbia University","Standard Grant","Tatiana Korelsky","12/31/2014","$86,183.00","","rjp49@cse.psu.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7359","9229","$0.00","The Manually Annotated Sub-Corpus (MASC) is a shared corpus that supports research across<br/>several disciplines: linguistics, computational linguistics, psycholinguistics, sociolinguistics and<br/>machine learning. It includes a wide variety of present-day American English texts annotated for<br/>several linguistic phenomena. Because MASC provides a unique resource, considerable<br/>community momentum has grown up around it. This project builds upon this momentum to<br/>enable the corpus to grow on its own, and to address the need for additional annotations. The<br/>major activities are to : (1) provide web-based mechanisms to facilitate community contribution<br/>and use of MASC annotations; (2) develop means to more fully automate the annotation<br/>validation process; (3) extend the WordNet annotations to cover adjectives, to support research<br/>on evaluation of ?subjective? annotations and harmonization of WordNet with other resources;<br/>(3) promote use of MASC and new annotations by diverse groups, by sponsoring shared tasks<br/>that exploit the corpus? unique characteristics and supporting beta-testers of software, data, and<br/>annotations; and (4) aggressively develop an ?Open Language Data? community around MASC<br/>through workshops, tutorials, and active participation in relevant community activities.<br/><br/>MASC provides an unparalleled resource for training and testing of tools for natural language<br/>processing, which can enable a major leap in the productivity of NLP research and ultimately<br/>impact the way people use and interact with computers. It is the first fully open, communitydriven<br/>resource in the field. All data and annotations are freely distributed in a manner that<br/>permits immediate and easy accessibility for users around the globe."
"1140230","CAREER: A Networking Approach to Host-based Intrusion Detection","CNS","Special Projects - CNS, TRUSTWORTHY COMPUTING, Secure &Trustworthy Cyberspace","05/25/2011","04/14/2014","Raheem Beyah","GA","Georgia Tech Research Corporation","Continuing Grant","Jeremy Epstein","05/31/2015","$322,526.00","","rbeyah@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1714, 7795, 8060","1045, 1187, 7434, 9102, 9178, 9218, 9251, HPCC","$0.00","CAREER: A Networking Approach to Host-based Intrusion Detection<br/>Proposal# 0844144<br/>Raheem A. Beyah<br/>Georgia State University<br/><br/>Award Abstract<br/>Day by day, threats to the cyber infrastructure are becoming more complex and, in response, so too are defense mechanisms.  One approach to securing nodes is to place a defense mechanism (e.g., intrusion detection system) on the node. This brings many challenges, with the most significant being that potential vulnerabilities in the defense mechanism can provide an additional avenue through which the host can be compromised. To address these challenges, this research investigates completely decoupling the defense mechanisms from the host, while continuing to provide insight about malicious activity as if the defense mechanisms resided on the host. This requires the development of new algorithms and the application of various techniques (e.g., statistical, machine learning, signal processing) to extract from a node?s network traffic characteristics that enable the inference of the state of its hardware components. Over the course of this project, a combination of experimentation and simulation will lead to the development of empirical and analytic models. The models will be used to develop network-based defense systems that provide capabilities similar to those provided by mechanisms traditionally considered host-based. This work leverages the concept of information leakage to bridge the computer architecture, computer networking, and network security fields. This project also seeks to broaden participation of groups traditionally underrepresented in the areas of science, technology, engineering, and mathematics (STEM). Accordingly, through a summer academy, the PI is actively engaging underrepresented middle school students by using current technology to convey abstract computer architecture and computer networking concepts. <br/>"
"1111019","Social Identity in Online Microfinance and Public Goods Provision","BCS","SOCIAL-COMPUTATIONAL SYSTEMS","09/15/2011","09/25/2012","Yan Chen","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Betty Tuller","08/31/2015","$500,000.00","Qiaozhu Mei","yanchen@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","SBE","7953","7953","$0.00","This project represents an interdisciplinary research effort to investigate the effects of social identity on lender behavior in an online microfinance community (kiva.org) and the selection of social identities. Social identity research in economics predicts that a salient group identity and group competition increases individual contribution to public goods.<br/><br/>The researchers will apply social identity and social preference theory to analyze the Kiva lending data at the individual and team level. Results from the analysis will yield insights to one of the most important yet unresolved problems in social identity research, the selection of identity and the formation of norms within a social group. This work represents the first attempt to bridge text and network data mining techniques with social identity theory. Novel text mining and machine learning models will be developed that advance the state-of-the-art of mining of online communities.<br/><br/>More than one billion people globally live in absolute poverty, most also excluded from the formal banking sector. To alleviate poverty, microfinance programs emerge in many parts of the world to provide small loans to the poor. Currently about 10 million households are served by microfinance programs. This research investigates incentives to increase lender participation and lending activities on Kiva. Preliminary analysis indicates that the lending teams program is effective in increasing loan activities. Results from the analysis will benefit Kiva and other microfinance programs. Increased participation and lending activities will help realize the World Bank's goal of serving 100 million poor households."
"1064948","SHB: Medium: Collaborative Research: Novel Computational Techniques for Cardiovascular Risk Stratification","IIS","Information Technology Researc","09/01/2011","08/07/2012","Satinder Baveja","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sylvia Spengler","08/31/2016","$562,415.00","","baveja@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1640","1640, 7924, 8018","$0.00","The project assesses patient cardiovascular risk and matches patients to the treatments most likely to be effective. The project addresses this problem through sophisticated computational methods that identify new markers of disease, improve the ability to measure both new and existing markers, and construct personalized models that can provide highly accurate assessments of individual risk. The core focus of the research addresses the poor performance of existing tools for cardiovascular decision support through advanced methods at the intersection of machine learning, data mining, signal processing, and applied algorithms; with the research guided by knowledge of cardiac pathophysiology.<br/><br/>This project impacts patient care for a disease that causes roughly one death every 38 seconds in the United States and imposes a burden of over half a trillion dollars in the U. S. each year. More generally, many of the ideas explored here (e.g., personalization of risk models) extends to a wide variety of other disorders in a straightforward manner and leads to wide improvements in outcomes while controlling costs. The research also strengthens interdisciplinary research in EECs and medicine throughout the computer science research community."
"1115839","TC: Small: New Directions in Side Channel Attacks and Countermeasures","CNS","Special Projects - CNS, TRUSTWORTHY COMPUTING","08/01/2011","05/06/2013","Inyoung Kim","VA","Virginia Polytechnic Institute and State University","Standard Grant","Deborah Shands","07/31/2015","$437,558.00","Patrick Schaumont","inyoungk@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1714, 7795","7795, 7923, 9102, 9178, 9251","$0.00","This project develops new and promising techniques in the area of side-channel attacks and their corresponding countermeasures. In a side-channel attack, an attacker captures the implementation effects of cryptography, such as power consumption and execution time. A distinctive feature of a side-channel analysis (SCA) attack is that it can reveal a small part of the secret-key. Hence, side-channel attacks avoid the brute-force complexity of cryptanalysis. Using novel side-channel estimation techniques based on Bayesian statistics, the project develops more powerful side-channel attacks. The development of novel side-channel analysis techniques is crucial in order to obtain the best possible countermeasures. The project also develops novel software-oriented countermeasures that more flexible and general than traditional hardware-oriented side-channel countermeasures. The efficiency of side-channel attacks and side-channel countermeasures are evaluated using hardware and software  prototyping.  The project combines advanced statistical techniques with advanced computer engineering, building synergy between Statistics and Computer Engineering. In the field of Statistics, the Bayesian matching technique can be used for variable selection, a technique that is applicable to related problems in biostatistics, machine learning, data mining, genomics, and other areas with high dimensional data. Project results will be disseminated by distributing open-source prototype implementations, measurement data, and in open publications. A formal training program within the Laboratory for Interdisciplinary Statistical Analysis (LISA) at Virginia Tech is developed to distribute the results of this project to students."
"1067488","Neuro-Marker Discovery for Accurate Localization of the Sub-Thalamic Nucleus for Deep Brain Stimulation","CBET","Engineering of Biomed Systems","07/15/2011","07/13/2011","Nuri Ince","MN","University of Minnesota-Twin Cities","Standard Grant","Kaiming Ye","07/31/2013","$331,222.00","Aviva Abosch","nfince@central.uh.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","5345","004E, 137E, 7237","$0.00","1067488<br/>Ince<br/><br/>Deep brain stimulation (DBS) has become the most common surgical treatment in the U.S. for patients with Parkinson?s disease (PD), and involves the stereotactic implantation of a DBS electrode within the subthalamic nucleus (STN) of the brain. The clinical efficacy of DBS depends critically on accurate localization of the STN. Currently, the initial surgical trajectory to STN is determined by preoperative stereotactic magnetic resonance imaging (MRI) of the brain. The ultimate location of the DBS electrode is then modified during surgery by data obtained from electrophysiological recordings--in the form of single-unit neuronal activity (SUA), derived from multiple microelectrode recording tracks.<br/>However, optimal placement of DBS electrodes within STN remains challenging due to current limitations of stereotactic imaging, poor isolation of SUA during microelectrode recordings, and anatomical differences in the location of deep brain structures between human subjects. Consequently, new techniques are required which can automatically localize or provide additional evidence to the neurosurgeon about STN localization. Microelectrodes can also be used to record local field potentials (LFPs) which, in contrast to SUA recordings, represent aggregate activity from populations of neurons surrounding the electrode tip. However, to be clinically useful, patterns in LFP data need to be translated into another modality so that they can be interpreted by the clinician.<br/>The intellectual merit of this project resides in testing the hypothesis that LFP data recorded intraoperatively can be used to identify STN location. Specifically, this project will record LFPs from both micro- and macro-electrodes at consecutive depths, as these electrodes are advanced to STN. Recorded neural data will be processed offline, using state-of-the-art signal processing and machine learning methods to identify novel neuro-markers which will be used for the  optimization of electrode placement in STN. <br/>This research project will identify and validate the use of specific LFP-derived data that correspond to optimal electrode positioning. The results of this interdisciplinary project will enable the development of a new technology for fusing microelectrode recordings with computational intelligence to localize STN during DBS surgery. The proposed research tools will provide valuable data for designing a new DBS surgery system that could be implemented by surgeons around the world. Such a system is expected to reduce the duration of the surgical procedure by enhancing STN localization, reduce the procedural hemorrhage rate by decreasing the number of microelectrode recording passes needed, and significantly decrease the rate of sub-optimal DBS electrode positioning, hence improving efficacy of stimulation. Moreover, the proposed efforts aim to address the nation?s current talent shortage in science and engineering majors in the field of neurotechnology. The interdisciplinary nature of the project offers a great environment for the education of graduate students with a concentration in instrumentation for use in the healthcare industry and the burgeoning field of neuromodulation."
"1110904","Collaborative Research: Discovering and Exploiting Latent Communities in Social Media","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2011","08/09/2011","Scott Kiesling","PA","University of Pittsburgh","Standard Grant","William Bainbridge","07/31/2016","$202,195.00","","kiesling@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7953","7953","$0.00","Despite the prevalence of social network platforms and apps in nowadays daily life, existing research on social media takes the terms ""social"" and ""media"" separately, and fails to address important needs for intelligently managing and utilizing social media, such as finding the information that users want, situating information in a social context that gives it meaning, and providing order and structure to an intricate and intertwined network of relationships. This interdisciplinary project will provide a holistic view of social media by combining socially intelligent language processing with linguistically motivated social network analysis. Specifically, the project will: (a) discover sociolinguistic communities and identify the demographic and sociological factors that underlie community membership; (b) discover cross-community linguistic variation at various levels and develop new computational tools for dialectometric and sociolinguistic analysis and for prediction of user interests and trends; and (c) recommend content and social connections across community boundaries, which will help people to broaden their perspectives with new information, opinions, and social relationships.<br/><br/>Intellectual merit:  The project will lead to (a) new modeling formalisms that jointly incorporate linguistic information with social network metadata; (b) a new computational methodology for sociolinguistic investigation from raw text; and (c) flexible models of linguistic variation that model temporal dynamics and move beyond simplistic bag-of-words approaches to higher-order phenomena such as multi-word expressions, syntax, and joint orthographic variation. <br/><br/>Broader impacts: The project will lead to advancements in basic research in statistical machine learning, social sciences, and language technology.  It will also bring innovations and practical applications in all these areas, such as software that reasons intelligently about community structures and linguistic patterns and conventions in social media. The findings will benefit a wide range of needs, such as personalized information service and intelligence and security operations, which require precise and timely understanding of social-cultural events and trends. The project will also provide undergraduate research opportunities and outreach to high school students through summer programs."
"1116610","SHF: Small: Integrating Compiler and Architecture Design to Boost Timing Speculation","CCF","Software & Hardware Foundation, HIGH-PERFORMANCE COMPUTING","09/01/2011","05/17/2013","Russell Joseph","IL","Northwestern University","Standard Grant","Almadena Chtchelkanova","08/31/2016","$501,349.00","Robert Findler","rjoseph@eecs.northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7798, 7942","7329, 7923, 7942, 9251","$0.00","This project examines the art and science of co-designing compilers and hardware for circuit-level timing speculation thereby enabling reliable, high-performance computer systems. Timing speculation, an exciting, forward-looking hardware design alternative, allows designers to abandon pessimistic design guidelines and consequently extract more performance from the silicon. Unfortunately, timing speculation has been under-served by contemporary compiler technologies. This research develops novel design paradigms that allow the compiler and architecture to be jointly optimized to build highly effective timing speculative systems, a significant industrial and societal benefit. Outreach efforts motivate students to pursue graduate study in computer science and engineering. <br/><br/>Given the promise of timing speculation but the prevailing downward focus of existing work, targeted code generation addresses neglected portions of the system stack and offers enticing possibilities.  This research develops timing-aware compilation that applies instruction-level error rate models to analyze and optimize instruction sequences. By generating binaries specifically targeted for timing speculation, the compiler aims to significantly reduce incidence of timing errors which demand dynamic correction. This extends the reach of timing speculation by reducing recovery cost. This allows systems to operate at higher clock frequencies or enables better energy-efficiency through lower supply voltages. The research develops compact timing error rate models through machine learning techniques and determines how existing flow analysis present in modern compilers could be used to drive these error rate models. The compiler extends existing code optimizations to include the estimated impact of timing errors generated during analysis phases. The compiler technology is being used to examine new design paradigms for integrated timing speculative systems which include co-designed and co-optimized architectures and compilers."
"1065079","SHB: Medium: Collaborative Research: Novel Computational Techniques for Cardiovascular Risk Stratification","IIS","Information Technology Researc","09/01/2011","08/26/2011","John Guttag","MA","Massachusetts Institute of Technology","Standard Grant","Wendy Nilsen","08/31/2015","$637,585.00","","guttag@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1640","1640, 7924, 8018","$0.00","The project assesses patient cardiovascular risk and matches patients to the treatments most likely to be effective. The project addresses this problem through sophisticated computational methods that identify new markers of disease, improve the ability to measure both new and existing markers, and construct personalized models that can provide highly accurate assessments of individual risk. The core focus of the research addresses the poor performance of existing tools for cardiovascular decision support through advanced methods at the intersection of machine learning, data mining, signal processing, and applied algorithms; with the research guided by knowledge of cardiac pathophysiology. <br/><br/>This project impacts patient care for a disease that causes roughly one death every 38 seconds in the United States and imposes a burden of over half a trillion dollars in the U. S. each year. More generally, many of the ideas explored here (e.g., personalization of risk models) extends to a wide variety of other disorders in a straightforward manner and leads to wide improvements in outcomes while controlling costs. The research also strengthens interdisciplinary research in EECs and medicine throughout the computer science research community."
"1117719","NeTS: Small: Collaborative Research: Understanding Traffic Dynamics in Cellular Data Networks and Applications to Resource Management","CNS","Networking Technology and Syst","08/01/2011","08/03/2011","Samir Das","NY","SUNY at Stony Brook","Standard Grant","Thyagarajan Nandagopal","07/31/2015","$320,426.00","Himanshu Gupta, Luis Ortiz","samir@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7363","7923","$0.00","Broadband cellular networks are emerging to be the most common means for mobile data access worldwide. Predictions from industry analysts indicate that the volume of data through cellular data networks will increase exponentially in near future. Understanding of the mobile data traffic via measurement and analysis is critical for the development of resource management techniques for these networks. While spectrum resources are of great concern, this project specifically focuses on the ?energy? required to operate the cellular network infrastructure, specifically base stations. The project undertakes a significant modeling exercise with two goals. One goal is ?intellectual,? driven towards understanding the spatio-temporal dynamics of mobile traffic and discovering possible structure or relationships. The project uses state-of-the-art machine learning tools to develop models using large-scale data collected directly from the operators? networks. Such modeling will bring new insights that in turn will help to deploy and manage future generation cellular data networks. The second goal is ?utilitarian.? Here, techniques are developed to predict base station loads for use in resource management, specifically energy. Algorithms are designed to exploit energy-optimization opportunities to turn off specific network resources based on the forecasted load.<br/><br/>The project has significant broader impact. It develops technologies to appreciably reduce energy consumption in cellular networks. Overall, this exercise will both reduce cost, and contribute to the environment. The project also contributes to several 'green? initiatives in both institutions and to the education and training of graduate students."
"1130086","CRCNS Data Sharing: An open data repository for cognitive neuroscience: The OpenfMRI Project","OAC","Data Cyberinfrastructure","09/01/2011","08/27/2011","Anthony Wagner","CA","Stanford University","Standard Grant","Robert Chadduck","08/31/2015","$101,535.00","","awagner@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7726","7327","$0.00","Functional magnetic resonance imaging (fMRI) has become the most common tool for cognitive neuroscience, because it provides a safe, non-invasive, and powerful means to image human brain function. Based on recent rates of publication, there are currently more than 2000 fMRI studies being performed every year worldwide.  The aggregation of data across multiple studies can provide the ability to answer questions that cannot be answered based on a single study. For example, using datasets from multiple domains one can start to investigate to what degree a region is selectively engaged in relation to a particular mental process, as opposed to being generally engaged across a broad range of tasks and processes. In addition, it provides the ability to integrate across specific tasks to obtain stronger empirical generalizations about mind-brain relationships, and to better understand the nature of individual variability across different measures. Recent work in neuroimaging analysis has focused on the application of methods such as machine learning techniques to understand the coding of information at the macroscopic level, and network analysis techniques to understand the interactions inherent in large-scale neural systems. The availability of a large testbed of high-quality fMRI data from published studies would also provide an important resource for the development of these and other new analytic techniques for fMRI data.  However, sharing of raw fMRI data is challenging due to the large size of the datasets and the complexity of the associated metadata, and there is currently no infrastructure for the open sharing of new fMRI datasets.<br/><br/>This project, OpenfMRI, will provide a new infrastructure for the broad dissemination of raw data within cognitive neuroscience, addressing a critical need by providing an open data sharing resource for neuroimaging.  The initial project is already online at http://www.openfmri.org with a limited number of datasets.  The full project will greatly expand this repository by providing access to a large number of fMRI datasets from several prominent neuroimaging labs, spanning across a broad range of cognitive domains. Utilizing the substantial computational resources of the Texas Advanced Computing Center, the project will also perform standard fMRI analyses on all data in the repository using a common analysis pipeline, thus providing directly comparable analysis results for all of the studies in the database.  The OpenfMRI project will support the development of infrastructural elements to make sharing of data by additional investigators more straightforward.<br/><br/>The repository of data that will be created by the OpenfMRI project will also serve as an important resource for teaching by providing students with the ability to replicate the analyses from published studies using the same data. By providing any researcher in the world with the ability to acquire large fMRI datasets, it will also provide all researchers with the ability to work with the same state-of-the-art datasets, regardless of institution. By creating the infrastructure for open sharing of research data, the project will also enhance the impact of other NSF-funded neuroimaging research projects by providing an infrastructure that can be used to make their data available. The planned work has the potential to benefit society by improving education, health, and human productivity through an increased understanding of mental function and its relationship to brain function."
"1134990","Funding Support for US Students Attending KDD 2011","IIS","Info Integration & Informatics, Robust Intelligence","04/01/2011","04/20/2011","Yan Liu","CA","University of Southern California","Standard Grant","Sylvia Spengler","03/31/2012","$20,000.00","","yanliu.cs@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7364, 7495","7364, 7495, 7556","$0.00","The project supports graduate student participation in the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2011). Specifically, the project supports travel to the conference for students who might not otherwise be able to attend the conference because of financial reasons. The recipients of student travel fellowships will have an opportunity to attend the conference, associated workshops and tutorials on emerging research topics, as well as a doctoral consortium  that provides a venue for the students to have one-on-one discussions and receive mentoring from some of the leading researchers in data mining, machine learning, and related topics. The project offers natural integration of research and education and contributes to the training of a new generation of researchers in an area of growing importance and impact in not only Computer Science but also a variety of data-driven disciplines."
"1111270","AF: Large: Collaborative Research: Algebraic Graph Algorithms: The Laplacian and Beyond","CCF","Algorithmic Foundations","09/01/2011","06/24/2011","Shanghua Teng","CA","University of Southern California","Standard Grant","Tracy Kimbrel","08/31/2017","$724,700.00","","shanghua@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796","7925, 7926, 7928","$0.00","This project will exploit algebraic properties of operators associated with graphs in an integrated set of research and educational activities designed to develop new mathematical and algorithmic techniques; apply these to the solution of real-world problems and longstanding theoretical questions in mathematics, computer science, biology, and physics; and make these techniques broadly known and accessible to students, researchers, and practitioners in many fields. <br/><br/>This research has its origins in spectral graph theory, which studies how the eigenvalues and eigenvectors of the graph Laplacian (and other related matrices) interact with the combinatorial structure of the graph. Spectral graph theory has been one of the great success stories in both the theory and practice of algorithm design. It has led to fundamental advances in graph partitioning, web search (notably including Google's PageRank algorithm), the understanding of random processes and the algorithms derived from them, the construction of error correcting codes, derandomization, convex optimization, machine learning, and many others. <br/><br/>While the eigenvalues and eigenvectors of the Laplacian capture a striking amount of the structure of the graph, they certainly do not capture all of it. Recent work by the principal investigators and other researchers suggests that theoretical computer scientists have only scratched the surface of what can be done if they are willing to broaden their investigation, extending it to study more general algebraic properties of the Laplacian than just its eigenvalue structure, and more general operators than just the Laplacian. <br/><br/>Under this award, the principal investigators will build a research program across the three universities involved in this proposal to develop such a theory and its applications. This initiative has the potential to provide transformative advances in a range of theoretical and applied areas of computer science, including: <br/><br/>* Faster algorithms for fundamental graph problems, such as Maximum Flow, Minimum Cut, Minimum Cost Flow, Multicommodity Flow, approximating Sparsest Cut, generating random spanning trees, and constructing low-stretch spaning trees. <br/><br/>* Better algorithms for the analysis of data, with potential applications to the Unique Games Conjecture. <br/><br/>* Faster algorithms for solving broad classes of important linear systems, both sequentially and in parallel. <br/><br/>* Faster distributed algorithms for information dissemination in networks. <br/><br/>* A spectral and algebraic graph theory for directed graphs, based on ideas from differential geometry. <br/><br/>* Novel quantum algorithms for a large class of problems that appear to be hard for classical computers. <br/><br/>* New techniques for problems in Quantum Physics based on ideas developed in Computer Science and Combinatorics. <br/><br/>The principal investigators will also work to disseminate these techniques by developing courses, training undergraduate and graduate students, and introducing these ideas to scientists in other fields."
"1111142","Collaborative Research: Discovering and Exploiting Latent Communities in Social Media","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2011","09/23/2013","Eric Xing","PA","Carnegie-Mellon University","Standard Grant","William Bainbridge","07/31/2015","$547,805.00","Jacob Eisenstein","epxing@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7953","7953","$0.00","Despite the prevalence of social network platforms and apps in nowadays daily life, existing research on social media takes the terms ""social"" and ""media"" separately, and fails to address important needs for intelligently managing and utilizing social media, such as finding the information that users want, situating information in a social context that gives it meaning, and providing order and structure to an intricate and intertwined network of relationships. This interdisciplinary project will provide a holistic view of social media by combining socially intelligent language processing with linguistically motivated social network analysis. Specifically, the project will: (a) discover sociolinguistic communities and identify the demographic and sociological factors that underlie community membership; (b) discover cross-community linguistic variation at various levels and develop new computational tools for dialectometric and sociolinguistic analysis and for prediction of user interests and trends; and (c) recommend content and social connections across community boundaries, which will help people to broaden their perspectives with new information, opinions, and social relationships.<br/><br/>Intellectual merit:  The project will lead to (a) new modeling formalisms that jointly incorporate linguistic information with social network metadata; (b) a new computational methodology for sociolinguistic investigation from raw text; and (c) flexible models of linguistic variation that model temporal dynamics and move beyond simplistic bag-of-words approaches to higher-order phenomena such as multi-word expressions, syntax, and joint orthographic variation. <br/><br/>Broader impacts: The project will lead to advancements in basic research in statistical machine learning, social sciences, and language technology.  It will also bring innovations and practical applications in all these areas, such as software that reasons intelligently about community structures and linguistic patterns and conventions in social media. The findings will benefit a wide range of needs, such as personalized information service and intelligence and security operations, which require precise and timely understanding of social-cultural events and trends. The project will also provide undergraduate research opportunities and outreach to high school students through summer programs."
"1106570","Bayesian methods for structure detection in analysis of object data","DMS","STATISTICS","06/01/2011","04/17/2013","Subhashis Ghoshal","NC","North Carolina State University","Continuing grant","Gabor Szekely","05/31/2015","$250,000.00","","subhashis_ghoshal@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","MPS","1269","","$0.00","Complex data objects such as images, functional data, random sets, shapes, graphs and trees are nowadays commonly used to represent information present in observations. Bayesian methods are well suited for detecting meaningful structures hidden underneath these complex data sets. The structure may come in the form of sparsity, setting many parameter values to a null value like zero, or by making adjacent values equal, thus effectively reducing the number of parameters to handle. This project will provide definitive guidelines for constructing prior distributions on the parameters controlling the distributions of object data, and for computing the resulting posterior distributions in an efficient manner. The main idea of the research is to use an auxiliary stochastic process to control ties in object data. The project connects diverse concepts such as multi-scale modeling, feature sharing, multiple testing, random geometry, machine learning and nonparametric Bayesian paradigm, and synthesizes these different concepts into a powerful approach for analyzing object data. The project also stimulates the development of computing strategies that exploit structural niceties such as conditional conjugacy, thus providing fast and accurate computational approaches.<br/><br/>This project develops methodologies that will provide a foundation for finding structures in collections of data, with a wide range of applications in astronomy, medical sciences, engineering, finance, and various other fields. The research will help process astronomical images in a more accurate and efficient manner, and thus help identify events in distant supernova remnants and other astronomical bodies.  The research will also have a significant impact in medical imaging, by providing a method of accurate processing of scans of sensitive organs with very little exposure to harmful rays. The project will impact human resource development in the form of graduate student advising. The project will have subprojects that will be suitable topics for undergraduate research projects. Efforts will be made to involve students from under-represented groups to promote diversity."
"1127975","Scalable, approximate dynamic programming algorithms for high-dimensional storage portfolios","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2011","07/22/2011","Warren Powell","NJ","Princeton University","Standard Grant","Paul Werbos","08/31/2014","$360,000.00","","powell@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","ENG","7607","155E, 1653","$0.00","Abstract<br/>The objective of this research is to develop methods to design and control heterogeneous portfolios of energy storage devices for the power grid.  The approach of this research is to use approximate dynamic programming to develop optimal control policies, that will then be used to understand the economic value of different technologies in the context of a complete power grid.  <br/>Intellectual merit<br/>The intellectual merit of the project is the development of scalable algorithmic technologies for solving high-dimensional stochastic optimization problems arising in energy storage.  We propose to use the framework of approximate dynamic programming coupled with tools from machine learning and convex optimization.  We exploit convexity which makes it possible to construct effective approximations that scale to handle large numbers of storage devices. <br/>Broader impacts<br/>The broader impacts of the research will be: 1) The research will guide the design of storage devices so that they meet the specific needs of the power grid in the presence of large supplies of intermittent energy such as wind and solar. 2) Renewable energy, coupled with appropriately designed storage, should dramatically reduce the need for coal. 3) The research, including the approximate dynamic programming models and algorithms, will be made available using a special website with datasets, software, published research and working papers, and downloadable presentations.  The results will be integrated in courses at Princeton University, and presented at conferences and workshops to a broad community spanning energy systems and economics, as well as the algorithmic communities."
"1105634","Dimension Reduction, Model Selection and Classification in Functional Data Analysis.","DMS","STATISTICS","09/01/2011","08/30/2012","Yehua Li","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","08/31/2014","$119,999.00","","yehuali@iastate.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","Functional data analysis aims to model and analyze data sets where a datum is a random function, e.g. a curve or a high dimensional image. Due to the fast growth of modern data collection methods, such data sets become more and more prevalent in many biological, medical and industrial applications. Functional data are viewed as infinite dimensional vectors in a functional space, and are usually observed on discrete points and measured with error. Due to the infinite dimensional nature of functional data, dimension reduction is essential for visualizing, modeling and making inference on these data. In the proposed project, the investigator will study new, computationally efficient dimension reduction methods for functional data based on spline approximations, and use asymptotic theory to develop new statistical devices for model selection and inference. The investigator will also study classification problems in functional data, by combining the proposed dimension reduction techniques with modern machine learning methods.<br/><br/>The proposed research is motivated by data from colon carcinogenesis experiments, hypertension studies, AIDS clinical trials and functional magnetic resonance imaging experiments. The proposed project will benefit the society by advancing knowledge in these scientific fields. To achieve broader dissemination of the research results, the investigator will provide free and user friendly software to all scientific researchers. A new course on functional data analysis will be developed in the investigator's institute. The new course aims to nurture the ability of students to analyze real and innovative data sets and help them gain deeper understanding of modern statistical methods and theory."
"1118756","Second Midwest Conference on Mathematical Methods for Images and Surfaces","DMS","COMPUTATIONAL MATHEMATICS","07/01/2011","06/22/2011","Guowei Wei","MI","Michigan State University","Standard Grant","Junping Wang","06/30/2012","$20,000.00","Yang Wang","wei@math.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","1271","7556, 9263","$0.00","This award provides support for a two-day Midwest Conference on Mathematical Methods for Images and Surfaces, to be held on August 27-28, 2011 on the campus of Michigan State University. This meeting was originally planned to bring together researchers of all career stages who work or are interested in mathematical methods for images and surfaces. However, after looking at the schedules and participants of many other conferences we feel that there is a strong need to reach out to researchers in their early careers, particularly in their training period. We have decided to make this conference one that focuses more on bringing together junior researchers.  We hope that this conference will serve as a venue for junior researchers to connect with one another and with a group of more senior researchers who have a track record of willingness to mentor junior faculties. Among invited 15 speakers, 6 are female, 11 are junior researchers (including PhD student who has done outstanding research work), and 5 are experts from molecular biology, computer science, mechanical engineering, physiology and radiology. This conference has no invited plenary speakers and all talks are of equal length, which provides an opportunity for young researchers to meet each other in a relaxed yet scientifically rigorous setting. The award will be used entirely for supporting graduate students, postdocs, junior faculty, and researchers from under-represented groups. The research areas represented at the conference span a diverse array of topics in mathematical images and surfaces. Example topics include but are not limited to level set methods, Mumford-Shah functional,  higher-order curvature flows, geometric flows for biomedical surface generations, differential geometry based multiscale modeling, partial differential equation transform, surface free energy minimization, wavelet and multiresolution analysis, compressed sensing, cellular imaging and cellular image analysis, biomedical imaging, molecular imaging, bioluminescence imaging,  fluorescent imaging, PET imaging,  ultrasound imaging, MRI, the analysis of protein, virus and membrane surfaces, image segmentation, computational anatomy, pattern recognition, machine learning, and video analysis and processing.  This conference will provide a forum to exchange new ideas and results in images and surfaces and foster interdisciplinary collaborations. Further information can be found at our conference web site: http://www.mth.msu.edu/MCMM2/index.html."
"1131441","CRCNS Data Sharing: An open data repository for cognitive neuroscience: The OpenfMRI Project","OAC","COGNEURO, CRCNS, DATANET","09/01/2011","08/27/2011","Russell Poldrack","TX","University of Texas at Austin","Standard Grant","Robert Chadduck","07/31/2015","$743,856.00","","poldrack@stanford.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1699, 7327, 7726","7327","$0.00","Functional magnetic resonance imaging (fMRI) has become the most common tool for cognitive neuroscience, because it provides a safe, non-invasive, and powerful means to image human brain function. Based on recent rates of publication, there are currently more than 2000 fMRI studies being performed every year worldwide.  The aggregation of data across multiple studies can provide the ability to answer questions that cannot be answered based on a single study. For example, using datasets from multiple domains one can start to investigate to what degree a region is selectively engaged in relation to a particular mental process, as opposed to being generally engaged across a broad range of tasks and processes. In addition, it provides the ability to integrate across specific tasks to obtain stronger empirical generalizations about mind-brain relationships, and to better understand the nature of individual variability across different measures. Recent work in neuroimaging analysis has focused on the application of methods such as machine learning techniques to understand the coding of information at the macroscopic level, and network analysis techniques to understand the interactions inherent in large-scale neural systems. The availability of a large testbed of high-quality fMRI data from published studies would also provide an important resource for the development of these and other new analytic techniques for fMRI data.  However, sharing of raw fMRI data is challenging due to the large size of the datasets and the complexity of the associated metadata, and there is currently no infrastructure for the open sharing of new fMRI datasets.<br/><br/>This project, OpenfMRI, will provide a new infrastructure for the broad dissemination of raw data within cognitive neuroscience, addressing a critical need by providing an open data sharing resource for neuroimaging.  The initial project is already online at http://www.openfmri.org with a limited number of datasets.  The full project will greatly expand this repository by providing access to a large number of fMRI datasets from several prominent neuroimaging labs, spanning across a broad range of cognitive domains. Utilizing the substantial computational resources of the Texas Advanced Computing Center, the project will also perform standard fMRI analyses on all data in the repository using a common analysis pipeline, thus providing directly comparable analysis results for all of the studies in the database.  The OpenfMRI project will support the development of infrastructural elements to make sharing of data by additional investigators more straightforward.<br/><br/>The repository of data that will be created by the OpenfMRI project will also serve as an important resource for teaching by providing students with the ability to replicate the analyses from published studies using the same data. By providing any researcher in the world with the ability to acquire large fMRI datasets, it will also provide all researchers with the ability to work with the same state-of-the-art datasets, regardless of institution. By creating the infrastructure for open sharing of research data, the project will also enhance the impact of other NSF-funded neuroimaging research projects by providing an infrastructure that can be used to make their data available. The planned work has the potential to benefit society by improving education, health, and human productivity through an increased understanding of mental function and its relationship to brain function."
"1117536","NeTS: Small: Understanding, Managing and Trouble-Shooting the Evolving Cellular Data Networks","CNS","Networking Technology and Syst","09/01/2011","07/30/2011","Zhi-Li Zhang","MN","University of Minnesota-Twin Cities","Standard Grant","Thyagarajan Nandagopal","08/31/2016","$300,000.00","","zhzhang@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7363","7363, 7923","$0.00","With the wide adoption of smart phones and other mobile devices, cellular mobile data traffic has grown tremendously in the past few years. This rapid growth in cellular data traffic, coupled with the increasing demands for mobile data services, has exerted enormous competitive pressure on cellular network service providers. How to effectively manage a large-scale cellular data network is therefore a daunting challenge that is not only important to cellular network service providers, but also to mobile data users and emerging mobile applications and services. In this project, the PI sets out to develop a data-driven, statistical machine learning-based framework to identify, analyze and understand various challenging issues in cellular data networks, and develop mechanisms and tools for effectively managing and trouble-shooting constantly evolving cellular data networks. The PI leverages vast amounts of heterogeneous sources of data obtained from operational cellular data networks, and designs statistical inference techniques to analyze, mine and correlate various sources of real-world data to uncover and identify various key factors that contribute to network performance issues and affect user experiences.<br/><br/>This research project brings benefits not only to cellular service providers, but also to millions of users who are increasingly dependent on mobile voice and data services. It also provides valuable insights to inform further development of emerging mobile and cloud computing. Research outcomes will be disseminated through publications and outreach activities, as well as technology transfer."
"1129871","Exploiting Submodularity in Integer Programming","CMMI","OPERATIONS RESEARCH","08/15/2011","07/12/2011","Shabbir Ahmed","GA","Georgia Tech Research Corporation","Standard Grant","Georgia-Ann Klutke","07/31/2016","$250,000.00","","sahmed@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","5514","072E, 073E, 077E","$0.00","The research objective of this project is to develop mixed integer linear programming (MILP)approaches for mixed integer nonlinear programming (MINLP) problems involving nonlinear functions of binary variables by exploiting submodularity of the nonlinear functions. Existing MINLP approaches for these problems ignore the discreteness of the binary variables when dealing with the nonlinearity of the underlying functions. On the other hand, combinatorial algorithms designed specifically for dealing with submodular functions are inapplicable because of the various additional problem-specific side constraints present in these problems. Our approach will address the nonlinearity directly in the space of the binary variables by developing strong MILP reformulations of the nonlinear functions by exploiting submodularity along with other problem specific structure. The developed MILP schemes will be enhanced by integrating specialized combinatorial algorithms for submodular optimization. The proposed approaches will be specialized and tested on various classes of stochastic combinatorial optimization problems involving risk averse objectives where submodularity arises naturally from the concavity of risk aversion. <br/><br/>If successful, the results from this research will advance the general area of MINLP by providing tools for effective linearization of nonlinear functions of binary variables. These tools can be embedded in modeling and solution software and hence impact various application areas including capital budgeting, combinatorial auctions, revenue management, and machine learning, that give rise to problems with submodular substructures. The project will require close integration of integer programming, submodular optimization, stochastic programming, and convex optimization methods; and new developments at the interface of these areas are anticipated. Specifically, new algorithmic techniques for various classes of stochastic programming models in combinatorial settings will be developed. More generally, results from this project can establish a novel framework for integrating specialized algorithms for combinatorial optimization problems within MILP approaches for general problems where the specific combinatorial problems arise as substructures."
"1101570","ICES: Small: Heuristic Mechanism Design","CCF","Inter Com Sci Econ Soc S (ICE)","05/01/2011","04/27/2011","David Parkes","MA","Harvard University","Standard Grant","Tracy J. Kimbrel","10/31/2015","$359,920.00","","parkes@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","8052","7923, 9218, HPCC","$0.00","Computational mechanism design (CMD) seeks to understand how to promote desirable outcomes in multi-agent systems, despite private information, self-interest and limited computational resources. CMD finds application in many settings; e.g., in the public sector for wireless spectrum and airport landing rights, in Internet advertising, in expressive sourcing in the supply chain, in allocating resources in computational systems. A key concept is strategyproofness: the mechanism's outcome should be robust against manipulations through misreports of private information held by participants.<br/><br/>In meeting the demands for CMD in these rich domains, we often need to bridge from the theory of economic mechanism design to the practice of deployable, computational mechanisms. The broad goal of this project is to leverage scalable, heuristic optimization algorithms, making them applicable in settings with self-interest. Rather than seeking provably optimal but possibly inapplicable mechanisms (either without complexity considerations, as in economic theory, or with worst-case complexity considerations, as is commonplace in theoretical computer science), we propose a new computational agenda.<br/><br/>Provable guarantees are often unavailable when search algorithms are applied to real-world optimization problems. Still, heuristic search algorithms are widely employed, and find good empirical success. We seek something analogous to this for settings in which inputs are distributed to participants, each self-interested and willing to misreport inputs in order to improve the outcome in their favor. Rather than looking for optimal mechanisms amongst the class of polynomial-time algorithms, we seek to employ search algorithms with excellent empirical performance despite worst-case exponential run-time (if run-to-completion.)<br/><br/>Specific topics of interest include: (a) automatic self-correction, to apply online sensitivity analysis to automatically correct the outcome of an algorithm, allowing the algorithm to be coupled with payments and made strategyproof; (b) metrics for approximate strategyproofness, to enable design without solving for equilibrium; and (c) automatic generation of payment rules through the use of machine learning, by imposing appropriate structure on the hypothesis space.<br/><br/>Successful progress will provide new and fundamental methodologies with which to develop incentive-aligned mechanisms (e.g., for resource and task allocation) that enjoy excellent empirical properties and are able to scale to real-world domains. The theory of mechanism design has already provided broad societal impact, in enabling the auctioning of public resources such as wireless spectrum and power generation capacity, and in driving revenue to internet businesses by enabling efficient advertising. A new framework for heuristic mechanism design will enable a new generation of mechanisms for large-scale coordination and resource allocation amongst people, firms and organizations, with the promise of broad applications to electronic commerce (including mobile commerce), cloud computing, and across the supply chain."
"1106717","Bayesian Variable Selection and Grouping","DMS","STATISTICS","08/15/2011","01/07/2014","Sounak Chakraborty","MO","University of Missouri-Columbia","Standard Grant","Gabor J. Szekely","07/31/2015","$121,279.00","Fei Liu","chakrabortys@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","MPS","1269","","$0.00","The recent advancements in science and technology present a huge challenge of handling large amount of data. Often the main objective is to retain the relevant features or covariates in the data and to filter out the redundant variables. In the statistical framework, this is known as variable selection. The problem becomes extremely difficult when there are a large number of covariates in comparison to the available sample size. Despite the great deal of research effort on variable selection, knowledge on modeling the dependence between the important variables is very limited and urgently needed in many fields. Penalization based techniques like Lasso can be used for variable selection purpose, but it cannot detect any grouping or dependency structure between the covariates. While methods like Elastic-Net and Octagonal Shrinkage and Clustering Algorithm for Regression (OSCAR) may be used to incorporate grouping, one apparent major drawback for such methods is that they are not based on any probabilistic framework. The project aims to develop probabilistic models or priors incorporating the dependency relationship, for simultaneous variable selection and grouping of closely related variables. The developed model will automate the process as much as possible using highly flexible Bayesian models characterized by a special dependency structure. The special dependency structure is formulated through an extension of the Laplace matrices of graphs (graph Laplacian) used in the machine learning and pattern recognition literature for finding good clusters. The proposed prior distribution for the graph Laplacian allows conjugacy and thereby greatly simplifies the computation. The graph Laplacian prior proposed in this research is very useful for small and moderately high dimension data sets. For data sets with a massive number of predictors, explicit modeling of the pair wise dependence through graph Laplacian is infeasible. Therefore, another goal in this research is to build a coherent Bayesian model which is capable of reducing the dimension and at the same time detecting the clusters of the nonzero coefficients through the graph Laplacian prior formulation (on the reduced dimension data set) for very high dimensional data sets. The proposed Bayesian Variable Selection and Grouping methods would be developed under continuous response data, as well as binary and count response data framework.<br/><br/>Due to enormous progress of computer technology, explosion of the internet based information, and emerging fields in biological sciences, high-dimensional complex data sets are now very common in our life. The first big challenge handling such data sets is to identify important features or covariates in them that are directly related and most important to the desired response or outcomes. In statistics this is commonly referred to as variable selection. This is the first objective of this project. Second goal of this project is to find any grouping pattern among the selected variables and enhance the understanding of how the features or covariates are related among themselves. The investigator proposes a new methodological framework to address these challenges. To account for any related uncertainty the proposed methodology is based on probabilistic or Bayesian framework. The practical implementation of the proposed models is done by developing fast computer algorithms, which are able to handle data sets of any size. The proposed work has enormous potential for real life applications, especially in the field of computer science, engineering, genetics, marketing research, and medicine. For example, the methods can be used to detect active genes and study the relationship between those active genes in biology. Another example arises in marketing segmentation for targeting a smaller market and helping the decision makers to effectively reach all customers. The principal investigator will distribute freely available and easy to use software along with a short tutorial, which will allow researchers from other disciplines to address their own scientific questions using the proposed methods. Based on the ideas developed in this project the principal investigator will develop new graduate courses, enhance the existing courses, and actively involve students in the research. This will train the current generation of students to deal with future challenges."
"1101212","Travel Support for the 8th Workshop on Bayesian Nonparametrics (BNP 2011)","DMS","STATISTICS","06/01/2011","02/22/2011","Abel Rodriguez","CA","University of California-Santa Cruz","Standard Grant","Gabor J. Szekely","05/31/2012","$8,000.00","","abel@soe.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","MPS","1269","7556","$0.00","Travel Support for the 8th Workshop on Bayesian Nonparametrics (BNP 2011)<br/><br/>This proposal seeks to secure travel support for junior U.S. researchers (graduate students, postdoctoral <br/>researchers and junior faculty within three years of completion of their terminal degree) to participate in the 8th Workshop on Bayesian Nonparametrics, to be held in Veracruz, Mexico, from June 26 to 30, 2011. The objective of the meeting is to bring together experts and young talented scientists devoted to the study and application of Bayesian nonparametric techniques. The workshop is organized under the auspices of the International Society for Bayesian Analysis (ISBA) and counts with a scientific committee formed by renowned international scientists. The meeting will include four tutorials on special topics, a series of invited and contributed talks, and a contributed poster session. This is the 8th in series of (mostly) biannual meetings held internationally since 1997, and is the premier venue for dissemination of research in Bayesian nonparametrics. <br/><br/>Bayesian nonparametric (BNP) methods constitute an extremely important area of research in the statistical <br/>sciences, which has recently generated enormous interest and has become one of the fastest growing areas of <br/>research within Bayesian statistics. Applications of BNP methods include areas as diverse as genetics, finance, sociology and machine learning. Providing support for junior researchers who do not have access to other sources of funding to attend the most important international gathering of scientists working on one the fastest growing areas of statistical sciences is key to maintaining the current leadership of American institutions in this field. The conference will include a series of activities specially designed to maximize the active participation of young researchers and to provide them with as many opportunities for interaction with other young researchers and with more senior colleagues. In addition, the conference will provide opportunities for these young researchers to disseminate widely the results of their work, not only through contributed talks and posters, but also by facilitating the publication of peer-reviewed papers. Due to the location of the meeting, and the opportunity for travel award recipients to participate for free in the Mexican Workshop on Bayesian Statistics (TAMEB). It is expected that the workshop will attract junior Hispanic/Latino U.S. researchers, providing them the opportunity to engage colleagues from Latin American and the rest of the world."
"1106588","Conformally invariant statistical mechanics models","DMS","PROBABILITY","07/01/2011","05/13/2011","Clement Hongler","NY","Columbia University","Standard Grant","Tomek Bartoszynski","06/30/2014","$127,673.00","","ch2812@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1263","","$0.00","Recently, spectacular progress in the field of conformally invariant processes has been achieved, in particular with the introduction of the SLE processes. Thanks to these advances, it appears now possible to understand at an unprecedented level of resolution a wide class of models, and to construct their universal limits. The Ising model, which is probably the most studied model for phase transitions, has seen recent breakthroughs, and its further investigation seems very promising: it is one of the models that offer the richest behaviors and is among the best understood from a mathematical point of view. A unified picture, describing the limit of this model as random curves and fields, capturing subtle geometry and boundary conditions effects, seems to be rigorously obtainable. This picture will also deepen the comprehension of other models and offer many applications.<br/><br/>Phase transitions are abrupt changes in the nature of systems: vapor that suddently condensates into water, metals that gain supraconductivity, social networks whose activity explode once critical mass has been reached. A major question is to understand how such global-scale phase transitions occur in large systems. We plan to develop tools and techniques to study models where random macroscopic geometries arise. Such models have found applications for instance in chemistry, image processing, ecology, economics or machine learning. Thanks to new ideas and methods introduced in the recent years, it now appears possible to give a more complete description of the phase transition of such models, which will hopefully become useful tools for both theoretical and applied researchers."
"1042468","BPC-AE: Scaling the STARS Alliance: A National Community for Broadening Participation through Regional Partnerships","CNS","SPECIAL PROJECTS - CISE, COLLABORATIVE RESEARCH, Computing Ed for 21st Century, BROADENING PARTIC IN COMPUTING, ","01/01/2011","08/28/2014","Jamie Payton","NC","University of North Carolina at Charlotte","Continuing grant","Janice E. Cuny","12/31/2017","$4,049,367.00","Tiffany Barnes, Heather Richter Lipford","payton@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","1714, 7298, 7382, 7482, L122","5926, 5977, 7482, 9178, 9251","$0.00","The University of North Carolina at Charlotte (UNC Charlotte) proposes a second extension to the STARS (Students & Technology in Academia, Research, and Service) Alliance. STARS is a  national community that uses regional partnerships to broaden the participation of women, under-represented minorities, and persons with disabilities in computing.  Its signature and catalyzing program is the STARS Leadership Corps, which provides students with mentoring, professional development and opportunities for research and service, resulting in higher computing student diversity, enrollments, and graduation rates. The extension will scale the Alliance from its current size of 20 universities and colleges to 50, each the center of their own regional alliance of K-12 schools, community colleges, industries, and community organizations. The scaled alliance will focus on <br/><br/>-  Institutionalizing the STARS Leadership Corps (SLC) as a multi-year curricular program that catalyzes regional partnerships through the tiered participation of students, professionals, and educators in research and civic engagement. <br/>-    Enhancing the STARS Celebration, a national forum to connect regional communities for collaboration on the SLC, STARS demonstration projects, and other BPC initiatives. <br/>-    Creating the STARS Social Network with pioneering technology to support a vibrant community of practice integrated with mobile games and CONNECT technology. <br/>-    Demonstrating a new Tiered Research Experiences for Undergraduates (TREU) program<br/>-    Scaling the STARS Mentoring and Pair Programming demonstration projects <br/>-    Creating an SLC Practices Collection within the BPC Digital Library at BPCPortal.org <br/><br/>Finally, the extension will spawn the STARS Institute, a nonprofit that  will sustain the annual STARS Celebration and Community Meeting."
"1122732","SIFTER: A Systems Biology Platform for Protein Function Prediction","OAC","CI Fellowships","09/01/2011","08/25/2011","Ameet Talwalkar","CA","Talwalkar               Ameet          S","Fellowship","Sushil Prasad","08/31/2014","$240,000.00","","","","Oakland","CA","946181554","","CSE","7696","","$0.00","Proteins are key biomolecules involved in virtually all processes within cells,<br/>e.g., metabolism, cell signaling, immune response, etc., and knowledge of<br/>protein function is vital to obtain a basic understanding of cellular activity.<br/>Due to recent advances in nucleotide sequencing technology, the number of<br/>available genomic sequences is doubling in size roughly every 12 months, an<br/>incredibly fast pace vastly exceeding Moore's law. Experimental technologies<br/>required to decipher protein function have not progressed nearly as fast. In<br/>fact, although there are roughly 10 million protein sequences in the<br/>comprehensive Uniprot database, only 0.2% have experimentally validated<br/>function annotations. This sequence-function gap is rapidly expanding, and the<br/>development of computational methods is of crucial importance to effectively<br/>utilize this deluge of sequence data.<br/><br/>In this work, we develop SIFTER, a large-scale, systems biology platform to<br/>accurately predict protein function from high-throughput data.  Building upon a<br/>promising phylogenomic-based prototype, we incorporate interaction networks<br/>into our model to improve performance. Interaction data intrinsically couples<br/>the thousands to millions of proteins within such networks, and we use<br/>variational inference and parallelized implementations to address this<br/>challenging computational problem.  We also explore techniques for function<br/>prediction based on low-rank matrix factorization, and along the way, introduce<br/>novel sampling-based approaches to speed up computation.  Additionally, we<br/>develop algorithms to quantify uncertainty in SIFTER's predictions to<br/>help guide future experimental work.  These novel algorithms are large-scale<br/>extensions to classical bootstrap sampling and are generally applicable to any<br/>problem involving massive data.  Finally, we evaluate SIFTER in<br/>collaboration with experimental biologists, allowing us to pinpoint relevant<br/>use cases and resulting in an effective method with widespread impact within<br/>the biomedical community."
